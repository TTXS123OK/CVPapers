# Arxiv Papers in cs.CV on 2016-08-09
### End-to-End Localization and Ranking for Relative Attributes
- **Arxiv ID**: http://arxiv.org/abs/1608.02676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02676v1)
- **Published**: 2016-08-09 02:19:37+00:00
- **Updated**: 2016-08-09 02:19:37+00:00
- **Authors**: Krishna Kumar Singh, Yong Jae Lee
- **Comment**: Appears in European Conference on Computer Vision (ECCV), 2016
- **Journal**: None
- **Summary**: We propose an end-to-end deep convolutional network to simultaneously localize and rank relative visual attributes, given only weakly-supervised pairwise image comparisons. Unlike previous methods, our network jointly learns the attribute's features, localization, and ranker. The localization module of our network discovers the most informative image region for the attribute, which is then used by the ranking module to learn a ranking model of the attribute. Our end-to-end framework also significantly speeds up processing and is much faster than previous methods. We show state-of-the-art ranking results on various relative attribute datasets, and our qualitative localization results clearly demonstrate our network's ability to learn meaningful image patches.



### A Factorization Approach to Inertial Affine Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1608.02680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1608.02680v1)
- **Published**: 2016-08-09 02:55:41+00:00
- **Updated**: 2016-08-09 02:55:41+00:00
- **Authors**: Roberto Tron
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of reconstructing a 3-D scene from a moving camera with high frame rate using the affine projection model. This problem is traditionally known as Affine Structure from Motion (Affine SfM), and can be solved using an elegant low-rank factorization formulation. In this paper, we assume that an accelerometer and gyro are rigidly mounted with the camera, so that synchronized linear acceleration and angular velocity measurements are available together with the image measurements. We extend the standard Affine SfM algorithm to integrate these measurements through the use of image derivatives.



### Deeply Semantic Inductive Spatio-Temporal Learning
- **Arxiv ID**: http://arxiv.org/abs/1608.02693v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/1608.02693v1)
- **Published**: 2016-08-09 05:48:51+00:00
- **Updated**: 2016-08-09 05:48:51+00:00
- **Authors**: Jakob Suchan, Mehul Bhatt, Carl Schultz
- **Comment**: Accepted for publication at ILP 2016: 26th International Conference
  on Inductive Logic Programming 4th - 6th September 2016, London. Keywords:
  Spatio-Temporal Learning; Dynamic Visuo-Spatial Imagery; Declarative Spatial
  Reasoning; Inductive Logic Programming; AI and Art
- **Journal**: None
- **Summary**: We present an inductive spatio-temporal learning framework rooted in inductive logic programming. With an emphasis on visuo-spatial language, logic, and cognition, the framework supports learning with relational spatio-temporal features identifiable in a range of domains involving the processing and interpretation of dynamic visuo-spatial imagery. We present a prototypical system, and an example application in the domain of computing for visual arts and computational cognitive science.



### Steerable Principal Components for Space-Frequency Localized Images
- **Arxiv ID**: http://arxiv.org/abs/1608.02702v2
- **DOI**: 10.1137/16M1085334
- **Categories**: **cs.CV**, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1608.02702v2)
- **Published**: 2016-08-09 06:35:09+00:00
- **Updated**: 2018-08-09 06:12:48+00:00
- **Authors**: Boris Landa, Yoel Shkolnisky
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a fast and accurate method for obtaining steerable principal components from a large dataset of images, assuming the images are well localized in space and frequency. The obtained steerable principal components are optimal for expanding the images in the dataset and all of their rotations. The method relies upon first expanding the images using a series of two-dimensional Prolate Spheroidal Wave Functions (PSWFs), where the expansion coefficients are evaluated using a specially designed numerical integration scheme. Then, the expansion coefficients are used to construct a rotationally-invariant covariance matrix which admits a block-diagonal structure, and the eigen-decomposition of its blocks provides us with the desired steerable principal components. The proposed method is shown to be faster then existing methods, while providing appropriate error bounds which guarantee its accuracy.



### Mean Box Pooling: A Rich Image Representation and Output Embedding for the Visual Madlibs Task
- **Arxiv ID**: http://arxiv.org/abs/1608.02717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1608.02717v1)
- **Published**: 2016-08-09 08:24:02+00:00
- **Updated**: 2016-08-09 08:24:02+00:00
- **Authors**: Ashkan Mokarian, Mateusz Malinowski, Mario Fritz
- **Comment**: Accepted to BMVC'16
- **Journal**: None
- **Summary**: We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs.



### OnionNet: Sharing Features in Cascaded Deep Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1608.02728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1608.02728v1)
- **Published**: 2016-08-09 08:59:47+00:00
- **Updated**: 2016-08-09 08:59:47+00:00
- **Authors**: Martin Simonovsky, Nikos Komodakis
- **Comment**: Accepted to BMVC 2016
- **Journal**: None
- **Summary**: The focus of our work is speeding up evaluation of deep neural networks in retrieval scenarios, where conventional architectures may spend too much time on negative examples. We propose to replace a monolithic network with our novel cascade of feature-sharing deep classifiers, called OnionNet, where subsequent stages may add both new layers as well as new feature channels to the previous ones. Importantly, intermediate feature maps are shared among classifiers, preventing them from the necessity of being recomputed. To accomplish this, the model is trained end-to-end in a principled way under a joint loss. We validate our approach in theory and on a synthetic benchmark. As a result demonstrated in three applications (patch matching, object detection, and image retrieval), our cascade can operate significantly faster than both monolithic networks and traditional cascades without sharing at the cost of marginal decrease in precision.



### Convolutional Oriented Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1608.02755v1
- **DOI**: 10.1007/978-3-319-46448-0_35
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02755v1)
- **Published**: 2016-08-09 10:37:52+00:00
- **Updated**: 2016-08-09 10:37:52+00:00
- **Authors**: Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez, Luc Van Gool
- **Comment**: ECCV 2016 Camera Ready
- **Journal**: None
- **Summary**: We present Convolutional Oriented Boundaries (COB), which produces multiscale oriented contours and region hierarchies starting from generic image classification Convolutional Neural Networks (CNNs). COB is computationally efficient, because it requires a single CNN forward pass for contour detection and it uses a novel sparse boundary representation for hierarchical segmentation; it gives a significant leap in performance over the state-of-the-art, and it generalizes very well to unseen categories and datasets. Particularly, we show that learning to estimate not only contour strength but also orientation provides more accurate results. We perform extensive experiments on BSDS, PASCAL Context, PASCAL Segmentation, and MS-COCO, showing that COB provides state-of-the-art contours, region hierarchies, and object proposals in all datasets.



### Deep Convolution Networks for Compression Artifacts Reduction
- **Arxiv ID**: http://arxiv.org/abs/1608.02778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02778v1)
- **Published**: 2016-08-09 12:11:51+00:00
- **Updated**: 2016-08-09 12:11:51+00:00
- **Authors**: Ke Yu, Chao Dong, Chen Change Loy, Xiaoou Tang
- **Comment**: 13 pages, 19 figures, an extension of our ICCV 2015 paper
- **Journal**: None
- **Summary**: Lossy compression introduces complex compression artifacts, particularly blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restore sharpened images that are accompanied with ringing effects. Inspired by the success of deep convolutional networks (DCN) on superresolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. To meet the speed requirement of real-world applications, we further accelerate the proposed baseline model by layer decomposition and joint use of large-stride convolutional and deconvolutional layers. This also leads to a more general CNN framework that has a close relationship with the conventional Multi-Layer Perceptron (MLP). Finally, the modified network achieves a speed up of 7.5 times with almost no performance loss compared to the baseline model. We also demonstrate that a deeper model can be effectively trained with features learned in a shallow network. Following a similar "easy to hard" idea, we systematically investigate three practical transfer settings and show the effectiveness of transfer learning in low-level vision problems. Our method shows superior performance than the state-of-the-art methods both on benchmark datasets and a real-world use case.



### Camera Pose Estimation from Lines using Plücker Coordinates
- **Arxiv ID**: http://arxiv.org/abs/1608.02824v1
- **DOI**: 10.5244/C.29.45
- **Categories**: **cs.CV**, 68T45, I.4.8; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/1608.02824v1)
- **Published**: 2016-08-09 14:58:34+00:00
- **Updated**: 2016-08-09 14:58:34+00:00
- **Authors**: Bronislav Přibyl, Pavel Zemčík, Martin Čadík
- **Comment**: 12 pages, 5 figures, In Proceedings of the British Machine Vision
  Conference (BMVC 2015), pages 45.1-45.12. BMVA Press, September 2015
- **Journal**: None
- **Summary**: Correspondences between 3D lines and their 2D images captured by a camera are often used to determine position and orientation of the camera in space. In this work, we propose a novel algebraic algorithm to estimate the camera pose. We parameterize 3D lines using Pl\"ucker coordinates that allow linear projection of the lines into the image. A line projection matrix is estimated using Linear Least Squares and the camera pose is then extracted from the matrix. An algebraic approach to handle mismatched line correspondences is also included. The proposed algorithm is an order of magnitude faster yet comparably accurate and robust to the state-of-the-art, it does not require initialization, and it yields only one solution. The described method requires at least 9 lines and is particularly suitable for scenarios with 25 and more lines, as also shown in the results.



### Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator
- **Arxiv ID**: http://arxiv.org/abs/1608.02833v5
- **DOI**: 10.1007/978-3-319-69456-6_12
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1608.02833v5)
- **Published**: 2016-08-09 15:21:33+00:00
- **Updated**: 2017-08-12 02:58:13+00:00
- **Authors**: Mundher Al-Shabi, Wooi Ping Cheah, Tee Connie
- **Comment**: To be appear in LNAI
- **Journal**: None
- **Summary**: Deriving an effective facial expression recognition component is important for a successful human-computer interaction system. Nonetheless, recognizing facial expression remains a challenging task. This paper describes a novel approach towards facial expression recognition task. The proposed method is motivated by the success of Convolutional Neural Networks (CNN) on the face recognition problem. Unlike other works, we focus on achieving good accuracy while requiring only a small sample data for training. Scale Invariant Feature Transform (SIFT) features are used to increase the performance on small data as SIFT does not require extensive training data to generate useful features. In this paper, both Dense SIFT and regular SIFT are studied and compared when merged with CNN features. Moreover, an aggregator of the models is developed. The proposed approach is tested on the FER-2013 and CK+ datasets. Results demonstrate the superiority of CNN with Dense SIFT over conventional CNN and CNN with SIFT. The accuracy even increased when all the models are aggregated which generates state-of-art results on FER-2013 and CK+ datasets, where it achieved 73.4% on FER-2013 and 99.1% on CK+.



### Residual Networks of Residual Networks: Multilevel Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1608.02908v2
- **DOI**: 10.1109/TCSVT.2017.2654543
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02908v2)
- **Published**: 2016-08-09 18:37:26+00:00
- **Updated**: 2017-03-05 04:47:53+00:00
- **Authors**: Ke Zhang, Miao Sun, Tony X. Han, Xingfang Yuan, Liru Guo, Tao Liu
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology 2017
- **Journal**: None
- **Summary**: A residual-networks family with hundreds or even thousands of layers dominates major image recognition tasks, but building a network by simply stacking residual blocks inevitably limits its optimization ability. This paper proposes a novel residual-network architecture, Residual networks of Residual networks (RoR), to dig the optimization ability of residual networks. RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping. In particular, RoR adds level-wise shortcut connections upon original residual networks to promote the learning capability of residual networks. More importantly, RoR can be applied to various kinds of residual networks (ResNets, Pre-ResNets and WRN) and significantly boost their performance. Our experiments demonstrate the effectiveness and versatility of RoR, where it achieves the best performance in all residual-network-like structures. Our RoR-3-WRN58-4+SD models achieve new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59%, respectively. RoR-3 models also achieve state-of-the-art results compared to ResNets on ImageNet data set.



### Deep Convolutional Neural Networks for Microscopy-Based Point of Care Diagnostics
- **Arxiv ID**: http://arxiv.org/abs/1608.02989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1608.02989v1)
- **Published**: 2016-08-09 22:04:46+00:00
- **Updated**: 2016-08-09 22:04:46+00:00
- **Authors**: John A. Quinn, Rose Nakasi, Pius K. B. Mugagga, Patrick Byanyima, William Lubega, Alfred Andama
- **Comment**: Presented at 2016 Machine Learning and Healthcare Conference (MLHC
  2016), Los Angeles, CA
- **Journal**: None
- **Summary**: Point of care diagnostics using microscopy and computer vision methods have been applied to a number of practical problems, and are particularly relevant to low-income, high disease burden areas. However, this is subject to the limitations in sensitivity and specificity of the computer vision methods used. In general, deep learning has recently revolutionised the field of computer vision, in some cases surpassing human performance for other object recognition tasks. In this paper, we evaluate the performance of deep convolutional neural networks on three different microscopy tasks: diagnosis of malaria in thick blood smears, tuberculosis in sputum samples, and intestinal parasite eggs in stool samples. In all cases accuracy is very high and substantially better than an alternative approach more representative of traditional medical imaging techniques.




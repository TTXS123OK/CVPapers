# Arxiv Papers in cs.CV on 2016-04-29
### Crowd Counting via Weighted VLAD on Dense Attribute Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1604.08660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08660v1)
- **Published**: 2016-04-29 00:55:41+00:00
- **Updated**: 2016-04-29 00:55:41+00:00
- **Authors**: Biyun Sheng, Chunhua Shen, Guosheng Lin, Jun Li, Wankou Yang, Changyin Sun
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Crowd counting is an important task in computer vision, which has many applications in video surveillance. Although the regression-based framework has achieved great improvements for crowd counting, how to improve the discriminative power of image representation is still an open problem. Conventional holistic features used in crowd counting often fail to capture semantic attributes and spatial cues of the image. In this paper, we propose integrating semantic information into learning locality-aware feature sets for accurate crowd counting. First, with the help of convolutional neural network (CNN), the original pixel space is mapped onto a dense attribute feature map, where each dimension of the pixel-wise feature indicates the probabilistic strength of a certain semantic class. Then, locality-aware features (LAF) built on the idea of spatial pyramids on neighboring patches are proposed to explore more spatial context and local information. Finally, the traditional VLAD encoding method is extended to a more generalized form in which diverse coefficient weights are taken into consideration. Experimental results validate the effectiveness of our presented method.



### Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1604.08671v2
- **DOI**: 10.1109/TIP.2017.2750403
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08671v2)
- **Published**: 2016-04-29 02:33:17+00:00
- **Updated**: 2016-07-18 03:38:35+00:00
- **Authors**: Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu, Zongming Guo, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we consider the image super-resolution (SR) problem. The main challenge of image SR is to recover high-frequency details of a low-resolution (LR) image that are important for human perception. To address this essentially ill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual~(DEGREE) network to progressively recover the high-frequency details. Different from most of existing methods that aim at predicting high-resolution (HR) images directly, DEGREE investigates an alternative route to recover the difference between a pair of LR and HR images by recurrent residual learning. DEGREE further augments the SR process with edge-preserving capability, namely the LR image and its edge map can jointly infer the sharp edge details of the HR image during the recurrent recovery process. To speed up its training convergence rate, by-pass connections across multiple layers of DEGREE are constructed. In addition, we offer an understanding on DEGREE from the view-point of sub-band frequency decomposition on image signal and experimentally demonstrate how DEGREE can recover different frequency bands separately. Extensive experiments on three benchmark datasets clearly demonstrate the superiority of DEGREE over well-established baselines and DEGREE also provides new state-of-the-arts on these datasets.



### Top-push Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1604.08683v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08683v2)
- **Published**: 2016-04-29 04:14:09+00:00
- **Updated**: 2016-07-07 15:10:37+00:00
- **Authors**: Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng
- **Comment**: In IEEE CVPR 2016
- **Journal**: None
- **Summary**: Most existing person re-identification (re-id) models focus on matching still person images across disjoint camera views. Since only limited information can be exploited from still images, it is hard (if not impossible) to overcome the occlusion, pose and camera-view change, and lighting variation problems. In comparison, video-based re-id methods can utilize extra space-time information, which contains much more rich cues for matching to overcome the mentioned problems. However, we find that when using video-based representation, some inter-class difference can be much more obscure than the one when using still-image based representation, because different people could not only have similar appearance but also have similar motions and actions which are hard to align. To solve this problem, we propose a top-push distance learning model (TDL), in which we integrate a top-push constrain for matching video features of persons. The top-push constraint enforces the optimization on top-rank matching in re-id, so as to make the matching model more effective towards selecting more discriminative features to distinguish different persons. Our experiments show that the proposed video-based re-id framework outperforms the state-of-the-art video-based re-id methods.



### Single Image 3D Interpreter Network
- **Arxiv ID**: http://arxiv.org/abs/1604.08685v2
- **DOI**: 10.1007/978-3-319-46466-4_22
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.08685v2)
- **Published**: 2016-04-29 04:52:46+00:00
- **Updated**: 2016-10-04 19:35:54+00:00
- **Authors**: Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman
- **Comment**: ECCV 2016 (oral). The first two authors contributed equally to this
  work
- **Journal**: None
- **Summary**: Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an end-to-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technical innovations. First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as 3D rendering and image retrieval.



### Towards Conceptual Compression
- **Arxiv ID**: http://arxiv.org/abs/1604.08772v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.08772v1)
- **Published**: 2016-04-29 11:02:52+00:00
- **Updated**: 2016-04-29 11:02:52+00:00
- **Authors**: Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.



### Effective Backscatter Approximation for Photometry in Murky Water
- **Arxiv ID**: http://arxiv.org/abs/1604.08789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08789v1)
- **Published**: 2016-04-29 12:14:10+00:00
- **Updated**: 2016-04-29 12:14:10+00:00
- **Authors**: Chourmouzios Tsiotsios, Maria E. Angelopoulou, Andrew J. Davison, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Shading-based approaches like Photometric Stereo assume that the image formation model can be effectively optimized for the scene normals. However, in murky water this is a very challenging problem. The light from artificial sources is not only reflected by the scene but it is also scattered by the medium particles, yielding the backscatter component. Backscatter corresponds to a complex term with several unknown variables, and makes the problem of normal estimation hard. In this work, we show that instead of trying to optimize the complex backscatter model or use previous unrealistic simplifications, we can approximate the per-pixel backscatter signal directly from the captured images. Our method is based on the observation that backscatter is saturated beyond a certain distance, i.e. it becomes scene-depth independent, and finally corresponds to a smoothly varying signal which depends strongly on the light position with respect to each pixel. Our backscatter approximation method facilitates imaging and scene reconstruction in murky water when the illumination is artificial as in Photometric Stereo. Specifically, we show that it allows accurate scene normal estimation and offers potentials like single image restoration. We evaluate our approach using numerical simulations and real experiments within both the controlled environment of a big water-tank and real murky port-waters.



### Mesh Interest Point Detection Based on Geometric Measures and Sparse Refinement
- **Arxiv ID**: http://arxiv.org/abs/1604.08806v3
- **DOI**: 10.1109/MMSP.2016.7813369
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08806v3)
- **Published**: 2016-04-29 12:48:43+00:00
- **Updated**: 2018-02-19 12:22:47+00:00
- **Authors**: Xinyu Lin, Ce Zhu, Yipeng Liu
- **Comment**: 17 pages
- **Journal**: 2016 IEEE 18th International Workshop on Multimedia Signal
  Processing (MMSP)
- **Summary**: Three dimensional (3D) interest point detection plays a fundamental role in 3D computer vision and graphics. In this paper, we introduce a new method for detecting mesh interest points based on geometric measures and sparse refinement (GMSR). The key point of our approach is to calculate the 3D interest point response function using two intuitive and effective geometric properties of the local surface on a 3D mesh model, namely Euclidean distances between the neighborhood vertices to the tangent plane of a vertex and the angles of normal vectors of them. The response function is defined in multi-scale space and can be utilized to effectively distinguish 3D interest points from edges and flat areas. Those points with local maximal 3D interest point response value are selected as the candidates of 3D interest points. Finally, we utilize an $\ell_0$ norm based optimization method to refine the candidates of 3D interest points by constraining its quality and quantity. Numerical experiments demonstrate that our proposed GMSR based 3D interest point detector outperforms current several state-of-the-art methods for different kinds of 3D mesh models.



### Improved Dense Trajectory with Cross Streams
- **Arxiv ID**: http://arxiv.org/abs/1604.08826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08826v1)
- **Published**: 2016-04-29 13:39:40+00:00
- **Updated**: 2016-04-29 13:39:40+00:00
- **Authors**: Katsunori Ohnishi, Masatoshi Hidaka, Tatsuya Harada
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: Improved dense trajectories (iDT) have shown great performance in action recognition, and their combination with the two-stream approach has achieved state-of-the-art performance. It is, however, difficult for iDT to completely remove background trajectories from video with camera shaking. Trajectories in less discriminative regions should be given modest weights in order to create more discriminative local descriptors for action recognition. In addition, the two-stream approach, which learns appearance and motion information separately, cannot focus on motion in important regions when extracting features from spatial convolutional layers of the appearance network, and vice versa. In order to address the above mentioned problems, we propose a new local descriptor that pools a new convolutional layer obtained from crossing two networks along iDT. This new descriptor is calculated by applying discriminative weights learned from one network to a convolutional layer of the other network. Our method has achieved state-of-the-art performance on ordinal action recognition datasets, 92.3% on UCF101, and 66.2% on HMDB51.



### Convolutional Neural Networks for Attribute-based Active Authentication on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1604.08865v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08865v2)
- **Published**: 2016-04-29 15:03:09+00:00
- **Updated**: 2016-07-08 13:11:31+00:00
- **Authors**: Pouya Samangouei, Rama Chellappa
- **Comment**: Accepted in BTAS 2016
- **Journal**: None
- **Summary**: We present a Deep Convolutional Neural Network (DCNN) architecture for the task of continuous authentication on mobile devices. To deal with the limited resources of these devices, we reduce the complexity of the networks by learning intermediate features such as gender and hair color instead of identities. We present a multi-task, part-based DCNN architecture for attribute detection that performs better than the state-of-the-art methods in terms of accuracy. As a byproduct of the proposed architecture, we are able to explore the embedding space of the attributes extracted from different facial parts, such as mouth and eyes, to discover new attributes. Furthermore, through extensive experimentation, we show that the attribute features extracted by our method outperform the previously presented attribute-based method and a baseline LBP method for the task of active authentication. Lastly, we demonstrate the effectiveness of the proposed architecture in terms of speed and power consumption by deploying it on an actual mobile device.



### Faster R-CNN Features for Instance Search
- **Arxiv ID**: http://arxiv.org/abs/1604.08893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.08893v1)
- **Published**: 2016-04-29 16:00:24+00:00
- **Updated**: 2016-04-29 16:00:24+00:00
- **Authors**: Amaia Salvador, Xavier Giro-i-Nieto, Ferran Marques, Shin'ichi Satoh
- **Comment**: DeepVision Workshop in CVPR 2016
- **Journal**: None
- **Summary**: Image representations derived from pre-trained Convolutional Neural Networks (CNNs) have become the new state of the art in computer vision tasks such as instance retrieval. This work explores the suitability for instance retrieval of image- and region-wise representations pooled from an object detection CNN such as Faster R-CNN. We take advantage of the object proposals learned by a Region Proposal Network (RPN) and their associated CNN features to build an instance search pipeline composed of a first filtering stage followed by a spatial reranking. We further investigate the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with the Oxford Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results.



### Multi-Atlas Segmentation using Partially Annotated Data: Methods and Annotation Strategies
- **Arxiv ID**: http://arxiv.org/abs/1605.00029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.00029v1)
- **Published**: 2016-04-29 21:34:29+00:00
- **Updated**: 2016-04-29 21:34:29+00:00
- **Authors**: Lisa M. Koch, Martin Rajchl, Wenjia Bai, Christian F. Baumgartner, Tong Tong, Jonathan Passerat-Palmbach, Paul Aljabar, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-atlas segmentation is a widely used tool in medical image analysis, providing robust and accurate results by learning from annotated atlas datasets. However, the availability of fully annotated atlas images for training is limited due to the time required for the labelling task. Segmentation methods requiring only a proportion of each atlas image to be labelled could therefore reduce the workload on expert raters tasked with annotating atlas images. To address this issue, we first re-examine the labelling problem common in many existing approaches and formulate its solution in terms of a Markov Random Field energy minimisation problem on a graph connecting atlases and the target image. This provides a unifying framework for multi-atlas segmentation. We then show how modifications in the graph configuration of the proposed framework enable the use of partially annotated atlas images and investigate different partial annotation strategies. The proposed method was evaluated on two Magnetic Resonance Imaging (MRI) datasets for hippocampal and cardiac segmentation. Experiments were performed aimed at (1) recreating existing segmentation techniques with the proposed framework and (2) demonstrating the potential of employing sparsely annotated atlas data for multi-atlas segmentation.



### Deep Convolutional Neural Networks on Cartoon Functions
- **Arxiv ID**: http://arxiv.org/abs/1605.00031v2
- **DOI**: 10.1109/ISIT.2016.7541482
- **Categories**: **cs.LG**, cs.CV, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.00031v2)
- **Published**: 2016-04-29 21:40:16+00:00
- **Updated**: 2018-02-12 13:47:49+00:00
- **Authors**: Philipp Grohs, Thomas Wiatowski, Helmut BÃ¶lcskei
- **Comment**: This is a slightly updated version of the paper published in the ISIT
  proceedings. Specifically, we corrected errors in the arguments on the volume
  of tubes. Note that this correction does not affect the main statements of
  the paper
- **Journal**: Proc. of IEEE International Symposium on Information Theory
  (ISIT), Barcelona, Spain, pp. 1163-1167, July 2016
- **Summary**: Wiatowski and B\"olcskei, 2015, proved that deformation stability and vertical translation invariance of deep convolutional neural network-based feature extractors are guaranteed by the network structure per se rather than the specific convolution kernels and non-linearities. While the translation invariance result applies to square-integrable functions, the deformation stability bound holds for band-limited functions only. Many signals of practical relevance (such as natural images) exhibit, however, sharp and curved discontinuities and are, hence, not band-limited. The main contribution of this paper is a deformation stability result that takes these structural properties into account. Specifically, we establish deformation stability bounds for the class of cartoon functions introduced by Donoho, 2001.




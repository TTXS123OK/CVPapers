# Arxiv Papers in cs.CV on 2016-04-18
### Fully Convolutional Recurrent Network for Handwritten Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1604.04953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04953v1)
- **Published**: 2016-04-18 01:28:28+00:00
- **Updated**: 2016-04-18 01:28:28+00:00
- **Authors**: Zecheng Xie, Zenghui Sun, Lianwen Jin, Ziyong Feng, Shuye Zhang
- **Comment**: 6 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: This paper proposes an end-to-end framework, namely fully convolutional recurrent network (FCRN) for handwritten Chinese text recognition (HCTR). Unlike traditional methods that rely heavily on segmentation, our FCRN is trained with online text data directly and learns to associate the pen-tip trajectory with a sequence of characters. FCRN consists of four parts: a path-signature layer to extract signature features from the input pen-tip trajectory, a fully convolutional network to learn informative representation, a sequence modeling layer to make per-frame predictions on the input sequence and a transcription layer to translate the predictions into a label sequence. The FCRN is end-to-end trainable in contrast to conventional methods whose components are separately trained and tuned. We also present a refined beam search method that efficiently integrates the language model to decode the FCRN and significantly improve the recognition results.   We evaluate the performance of the proposed method on the test sets from the databases CASIA-OLHWDB and ICDAR 2013 Chinese handwriting recognition competition, and both achieve state-of-the-art performance with correct rates of 96.40% and 95.00%, respectively.



### Deep Aesthetic Quality Assessment with Semantic Information
- **Arxiv ID**: http://arxiv.org/abs/1604.04970v3
- **DOI**: 10.1109/TIP.2017.2651399
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.04970v3)
- **Published**: 2016-04-18 03:16:56+00:00
- **Updated**: 2016-10-21 07:46:54+00:00
- **Authors**: Yueying Kao, Ran He, Kaiqi Huang
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Human beings often assess the aesthetic quality of an image coupled with the identification of the image's semantic content. This paper addresses the correlation issue between automatic aesthetic quality assessment and semantic recognition. We cast the assessment problem as the main task among a multi-task deep model, and argue that semantic recognition task offers the key to address this problem. Based on convolutional neural networks, we employ a single and simple multi-task framework to efficiently utilize the supervision of aesthetic and semantic labels. A correlation item between these two tasks is further introduced to the framework by incorporating the inter-task relationship learning. This item not only provides some useful insight about the correlation but also improves assessment accuracy of the aesthetic task. Particularly, an effective strategy is developed to keep a balance between the two tasks, which facilitates to optimize the parameters of the framework. Extensive experiments on the challenging AVA dataset and Photo.net dataset validate the importance of semantic recognition in aesthetic quality assessment, and demonstrate that multi-task deep models can discover an effective aesthetic representation to achieve state-of-the-art results.



### Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1604.04994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.04994v2)
- **Published**: 2016-04-18 05:39:32+00:00
- **Updated**: 2017-07-13 02:38:16+00:00
- **Authors**: Xiu-Shen Wei, Jian-Hao Luo, Jianxin Wu, Zhi-Hua Zhou
- **Comment**: IEEE Transactions on Image Processing (TIP), 2017, 26(6): 2868-2881
- **Journal**: None
- **Summary**: Deep convolutional neural network models pre-trained for the ImageNet classification task have been successfully adopted to tasks in other domains, such as texture description and object proposal generation, but these tasks require annotations for images in the new domain. In this paper, we focus on a novel and challenging task in the pure unsupervised setting: fine-grained image retrieval. Even with image labels, fine-grained images are difficult to classify, let alone the unsupervised retrieval task. We propose the Selective Convolutional Descriptor Aggregation (SCDA) method. SCDA firstly localizes the main object in fine-grained images, a step that discards the noisy background and keeps useful deep descriptors. The selected descriptors are then aggregated and dimensionality reduced into a short feature vector using the best practices we found. SCDA is unsupervised, using no image label or bounding box annotation. Experiments on six fine-grained datasets confirm the effectiveness of SCDA for fine-grained image retrieval. Besides, visualization of the SCDA features shows that they correspond to visual attributes (even subtle ones), which might explain SCDA's high mean average precision in fine-grained retrieval. Moreover, on general image retrieval datasets, SCDA achieves comparable retrieval results with state-of-the-art general image retrieval approaches.



### LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1604.05000v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1604.05000v3)
- **Published**: 2016-04-18 05:59:50+00:00
- **Updated**: 2016-07-26 16:46:43+00:00
- **Authors**: Zhen Li, Yukang Gan, Xiaodan Liang, Yizhou Yu, Hui Cheng, Liang Lin
- **Comment**: 17 pages, accepted by ECCV
- **Journal**: None
- **Summary**: Semantic labeling of RGB-D scenes is crucial to many intelligent applications including perceptual robotics. It generates pixelwise and fine-grained label maps from simultaneously sensed photometric (RGB) and depth channels. This paper addresses this problem by i) developing a novel Long Short-Term Memorized Context Fusion (LSTM-CF) Model that captures and fuses contextual information from multiple channels of photometric and depth data, and ii) incorporating this model into deep convolutional neural networks (CNNs) for end-to-end training. Specifically, contexts in photometric and depth channels are, respectively, captured by stacking several convolutional layers and a long short-term memory layer; the memory layer encodes both short-range and long-range spatial dependencies in an image along the vertical direction. Another long short-term memorized fusion layer is set up to integrate the contexts along the vertical direction from different channels, and perform bi-directional propagation of the fused vertical contexts along the horizontal direction to obtain true 2D global contexts. At last, the fused contextual representation is concatenated with the convolutional features extracted from the photometric channels in order to improve the accuracy of fine-scale semantic labeling. Our proposed model has set a new state of the art, i.e., 48.1% and 49.4% average class accuracy over 37 categories (2.2% and 5.4% improvement) on the large-scale SUNRGBD dataset and the NYUDv2dataset, respectively.



### Most Likely Separation of Intensity and Warping Effects in Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1604.05027v2
- **DOI**: 10.1137/16M1070980
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05027v2)
- **Published**: 2016-04-18 08:15:27+00:00
- **Updated**: 2017-03-15 10:54:43+00:00
- **Authors**: Line Kühnel, Stefan Sommer, Akshay Pai, Lars Lau Raket
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a class of mixed-effects models for joint modeling of spatially correlated intensity variation and warping variation in 2D images. Spatially correlated intensity variation and warp variation are modeled as random effects, resulting in a nonlinear mixed-effects model that enables simultaneous estimation of template and model parameters by optimization of the likelihood function. We propose an algorithm for fitting the model which alternates estimation of variance parameters and image registration. This approach avoids the potential estimation bias in the template estimate that arises when treating registration as a preprocessing step. We apply the model to datasets of facial images and 2D brain magnetic resonance images to illustrate the simultaneous estimation and prediction of intensity and warp effects.



### End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1604.05091v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1604.05091v2)
- **Published**: 2016-04-18 11:15:56+00:00
- **Updated**: 2016-04-19 14:09:26+00:00
- **Authors**: Peter Ondruska, Julie Dequaire, Dominic Zeng Wang, Ingmar Posner
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present a novel end-to-end framework for tracking and classifying a robot's surroundings in complex, dynamic and only partially observable real-world environments. The approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations, along with their identity in both visible and occluded areas. To achieve this we first train the network using unsupervised Deep Tracking, a recently proposed theoretical framework for end-to-end space occupancy prediction. We show that by learning to track on a large amount of unsupervised data, the network creates a rich internal representation of its environment which we in turn exploit through the principle of inductive transfer of knowledge to perform the task of it's semantic classification. As a result, we show that only a small amount of labelled data suffices to steer the network towards mastering this additional task. Furthermore we propose a novel recurrent neural network architecture specifically tailored to tracking and semantic classification in real-world robotics applications. We demonstrate the tracking and classification performance of the method on real-world data collected at a busy road junction. Our evaluation shows that the proposed end-to-end framework compares favourably to a state-of-the-art, model-free tracking solution and that it outperforms a conventional one-shot training scheme for semantic classification.



### Pixel-level Encoding and Depth Layering for Instance-level Semantic Labeling
- **Arxiv ID**: http://arxiv.org/abs/1604.05096v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05096v2)
- **Published**: 2016-04-18 11:24:39+00:00
- **Updated**: 2016-07-14 14:46:35+00:00
- **Authors**: Jonas Uhrig, Marius Cordts, Uwe Franke, Thomas Brox
- **Comment**: Accepted at GCPR 2016. Includes supplementary material
- **Journal**: None
- **Summary**: Recent approaches for instance-aware semantic labeling have augmented convolutional neural networks (CNNs) with complex multi-task architectures or computationally expensive graphical models. We present a method that leverages a fully convolutional network (FCN) to predict semantic labels, depth and an instance-based encoding using each pixel's direction towards its corresponding instance center. Subsequently, we apply low-level computer vision techniques to generate state-of-the-art instance segmentation on the street scene datasets KITTI and Cityscapes. Our approach outperforms existing works by a large margin and can additionally predict absolute distances of individual instances from a monocular image as well as a pixel-level semantic labeling.



### Using Self-Contradiction to Learn Confidence Measures in Stereo Vision
- **Arxiv ID**: http://arxiv.org/abs/1604.05132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05132v1)
- **Published**: 2016-04-18 13:26:46+00:00
- **Updated**: 2016-04-18 13:26:46+00:00
- **Authors**: Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof
- **Comment**: This paper was accepted to the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2016. The copyright was transfered to IEEE
  (https://www.ieee.org). The official version of the paper will be made
  available on IEEE Xplore (R) (http://ieeexplore.ieee.org). This version of
  the paper also contains the supplementary material, which will not appear
  IEEE Xplore (R)
- **Journal**: None
- **Summary**: Learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision. However, acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction, active sensing devices and/or synthetic scenes. To overcome this problem, we propose a new, flexible, and scalable way for generating training data that only requires a set of stereo images as input. The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm. This enables us to generate a huge amount of training data in a fully automated manner. Among other experiments, we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data.



### ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1604.05144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05144v1)
- **Published**: 2016-04-18 13:46:23+00:00
- **Updated**: 2016-04-18 13:46:23+00:00
- **Authors**: Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun
- **Comment**: accepted by CVPR 2016
- **Journal**: None
- **Summary**: Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most user-friendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup



### Pieces-of-parts for supervoxel segmentation with global context: Application to DCE-MRI tumour delineation
- **Arxiv ID**: http://arxiv.org/abs/1604.05210v1
- **DOI**: 10.1016/j.media.2016.03.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05210v1)
- **Published**: 2016-04-18 15:31:22+00:00
- **Updated**: 2016-04-18 15:31:22+00:00
- **Authors**: Benjamin Irving, James M Franklin, Bartlomiej W Papiez, Ewan M Anderson, Ricky A Sharma, Fergus V Gleeson, Sir Michael Brady, Julia A Schnabel
- **Comment**: accepted for publication in the Journal of Medical Image Analysis
- **Journal**: None
- **Summary**: Rectal tumour segmentation in dynamic contrast-enhanced MRI (DCE-MRI) is a challenging task, and an automated and consistent method would be highly desirable to improve the modelling and prediction of patient outcomes from tissue contrast enhancement characteristics - particularly in routine clinical practice. A framework is developed to automate DCE-MRI tumour segmentation, by introducing: perfusion-supervoxels to over-segment and classify DCE-MRI volumes using the dynamic contrast enhancement characteristics; and the pieces-of-parts graphical model, which adds global (anatomic) constraints that further refine the supervoxel components that comprise the tumour. The framework was evaluated on 23 DCE-MRI scans of patients with rectal adenocarcinomas, and achieved a voxelwise area-under the receiver operating characteristic curve (AUC) of 0.97 compared to expert delineations. Creating a binary tumour segmentation, 21 of the 23 cases were segmented correctly with a median Dice similarity coefficient (DSC) of 0.63, which is close to the inter-rater variability of this challenging task. A sec- ond study is also included to demonstrate the method's generalisability and achieved a DSC of 0.71. The framework achieves promising results for the underexplored area of rectal tumour segmentation in DCE-MRI, and the methods have potential to be applied to other DCE-MRI and supervoxel segmentation problems



### Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging
- **Arxiv ID**: http://arxiv.org/abs/1604.05225v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1604.05225v3)
- **Published**: 2016-04-18 16:09:04+00:00
- **Updated**: 2016-12-07 19:57:02+00:00
- **Authors**: Jiren Jin, Hideki Nakayama
- **Comment**: International Conference on Pattern Recognition (ICPR) 2016, Cancun,
  Mexico (oral presentation)
- **Journal**: None
- **Summary**: Automatic image annotation has been an important research topic in facilitating large scale image management and retrieval. Existing methods focus on learning image-tag correlation or correlation between tags to improve annotation accuracy. However, most of these methods evaluate their performance using top-k retrieval performance, where k is fixed. Although such setting gives convenience for comparing different methods, it is not the natural way that humans annotate images. The number of annotated tags should depend on image contents. Inspired by the recent progress in machine translation and image captioning, we propose a novel Recurrent Image Annotator (RIA) model that forms image annotation task as a sequence generation problem so that RIA can natively predict the proper length of tags according to image contents. We evaluate the proposed model on various image annotation datasets. In addition to comparing our model with existing methods using the conventional top-k evaluation measures, we also provide our model as a high quality baseline for the arbitrary length image tagging task. Moreover, the results of our experiments show that the order of tags in training phase has a great impact on the final annotation performance.



### Can Boosting with SVM as Week Learners Help?
- **Arxiv ID**: http://arxiv.org/abs/1604.05242v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1604.05242v2)
- **Published**: 2016-04-18 17:05:00+00:00
- **Updated**: 2016-04-22 23:03:27+00:00
- **Authors**: Dinesh Govindaraj
- **Comment**: Work done in 2009
- **Journal**: None
- **Summary**: Object recognition in images involves identifying objects with partial occlusions, viewpoint changes, varying illumination, cluttered backgrounds. Recent work in object recognition uses machine learning techniques SVM-KNN, Local Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, we want to utilize SVM as week learners in AdaBoost. Experiments are done with classifiers like near- est neighbor, k-nearest neighbor, Support vector machines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariant descriptors and Pyramid his- togram of gradient descriptors. AdaBoost is trained with set of week classifier as SVMs, each with kernel distance function on different descriptors. Results shows AdaBoost with SVM outperform other methods for Object Categorization dataset.



### Learning Dense Correspondence via 3D-guided Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/1604.05383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1604.05383v1)
- **Published**: 2016-04-18 23:50:40+00:00
- **Updated**: 2016-04-18 23:50:40+00:00
- **Authors**: Tinghui Zhou, Philipp Krähenbühl, Mathieu Aubry, Qixing Huang, Alexei A. Efros
- **Comment**: To appear in CVPR 2016 (oral presentation)
- **Journal**: None
- **Summary**: Discriminative deep learning approaches have shown impressive results for problems where human-labeled ground truth is plentiful, but what about tasks where labels are difficult or impossible to obtain? This paper tackles one such problem: establishing dense visual correspondence across different object instances. For this task, although we do not know what the ground-truth is, we know it should be consistent across instances of that category. We exploit this consistency as a supervisory signal to train a convolutional neural network to predict cross-instance correspondences between pairs of images depicting objects of the same category. For each pair of training images we find an appropriate 3D CAD model and render two synthetic views to link in with the pair, establishing a correspondence flow 4-cycle. We use ground-truth synthetic-to-synthetic correspondences, provided by the rendering engine, to train a ConvNet to predict synthetic-to-real, real-to-real and real-to-synthetic correspondences that are cycle-consistent with the ground-truth. At test time, no CAD models are required. We demonstrate that our end-to-end trained ConvNet supervised by cycle-consistency outperforms state-of-the-art pairwise matching methods in correspondence-related tasks.




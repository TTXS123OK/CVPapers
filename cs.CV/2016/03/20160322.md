# Arxiv Papers in cs.CV on 2016-03-22
### Input Aggregated Network for Face Video Representation
- **Arxiv ID**: http://arxiv.org/abs/1603.06655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06655v1)
- **Published**: 2016-03-22 01:27:50+00:00
- **Updated**: 2016-03-22 01:27:50+00:00
- **Authors**: Zhen Dong, Su Jia, Chi Zhang, Mingtao Pei
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep neural network has shown promising performance in face image recognition. The inputs of most networks are face images, and there is hardly any work reported in literature on network with face videos as input. To sufficiently discover the useful information contained in face videos, we present a novel network architecture called input aggregated network which is able to learn fixed-length representations for variable-length face videos. To accomplish this goal, an aggregation unit is designed to model a face video with various frames as a point on a Riemannian manifold, and the mapping unit aims at mapping the point into high-dimensional space where face videos belonging to the same subject are close-by and others are distant. These two units together with the frame representation unit build an end-to-end learning system which can learn representations of face videos for the specific tasks. Experiments on two public face video datasets demonstrate the effectiveness of the proposed network.



### Learning Representations for Automatic Colorization
- **Arxiv ID**: http://arxiv.org/abs/1603.06668v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06668v3)
- **Published**: 2016-03-22 04:08:01+00:00
- **Updated**: 2017-08-13 17:50:50+00:00
- **Authors**: Gustav Larsson, Michael Maire, Gregory Shakhnarovich
- **Comment**: ECCV 2016 (Project page:
  http://people.cs.uchicago.edu/~larsson/colorization/)
- **Journal**: None
- **Summary**: We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.



### Implementation of a FPGA-Based Feature Detection and Networking System for Real-time Traffic Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1603.06669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06669v1)
- **Published**: 2016-03-22 04:09:34+00:00
- **Updated**: 2016-03-22 04:09:34+00:00
- **Authors**: Jieshi Chen, Benjamin Carrion Schafer, Ivan Wang-Hei Ho
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing demand of real-time traffic monitoring nowadays, software-based image processing can hardly meet the real-time data processing requirement due to the serial data processing nature. In this paper, the implementation of a hardware-based feature detection and networking system prototype for real-time traffic monitoring as well as data transmission is presented. The hardware architecture of the proposed system is mainly composed of three parts: data collection, feature detection, and data transmission. Overall, the presented prototype can tolerate a high data rate of about 60 frames per second. By integrating the feature detection and data transmission functions, the presented system can be further developed for various VANET application scenarios to improve road safety and traffic efficiency. For example, detection of vehicles that violate traffic rules, parking enforcement, etc.



### Stitching Stabilizer: Two-frame-stitching Video Stabilization for Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1603.06678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06678v1)
- **Published**: 2016-03-22 05:42:31+00:00
- **Updated**: 2016-03-22 05:42:31+00:00
- **Authors**: Masaki Satoh
- **Comment**: 17 pages. For a supplemental video, see
  https://www.youtube.com/watch?v=LmyPXfGZRb0
- **Journal**: None
- **Summary**: In conventional electronic video stabilization, the stabilized frame is obtained by cropping the input frame to cancel camera shake. While a small cropping size results in strong stabilization, it does not provide us satisfactory results from the viewpoint of image quality, because it narrows the angle of view. By fusing several frames, we can effectively expand the area of input frames, and achieve strong stabilization even with a large cropping size. Several methods for doing so have been studied. However, their computational costs are too high for embedded systems such as smartphones.   We propose a simple, yet surprisingly effective algorithm, called the stitching stabilizer. It stitches only two frames together with a minimal computational cost. It can achieve real-time processes in embedded systems, for Full HD and 30 FPS videos. To clearly show the effect, we apply it to hyperlapse. Using several clips, we show it produces more strongly stabilized and natural results than the existing solutions from Microsoft and Instagram.



### Image Super-Resolution Based on Sparsity Prior via Smoothed $l_0$ Norm
- **Arxiv ID**: http://arxiv.org/abs/1603.06680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06680v1)
- **Published**: 2016-03-22 06:30:19+00:00
- **Updated**: 2016-03-22 06:30:19+00:00
- **Authors**: Mohammad Rostami, Zhou Wang
- **Comment**: Proceedings of the 2011 Symposium on Advanced Intelligent Systems
- **Journal**: None
- **Summary**: In this paper we aim to tackle the problem of reconstructing a high-resolution image from a single low-resolution input image, known as single image super-resolution. In the literature, sparse representation has been used to address this problem, where it is assumed that both low-resolution and high-resolution images share the same sparse representation over a pair of coupled jointly trained dictionaries. This assumption enables us to use the compressed sensing theory to find the jointly sparse representation via the low-resolution image and then use it to recover the high-resolution image. However, sparse representation of a signal over a known dictionary is an ill-posed, combinatorial optimization problem. Here we propose an algorithm that adopts the smoothed $l_0$-norm (SL0) approach to find the jointly sparse representation. Improved quality of the reconstructed image is obtained for most images in terms of both peak signal-to-noise-ratio (PSNR) and structural similarity (SSIM) measures.



### Convolution in Convolution for Network in Network
- **Arxiv ID**: http://arxiv.org/abs/1603.06759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06759v1)
- **Published**: 2016-03-22 12:33:11+00:00
- **Updated**: 2016-03-22 12:33:11+00:00
- **Authors**: Yanwei Pang, Manli Sun, Xiaoheng Jiang, Xuelong Li
- **Comment**: A method of Convolutional Neural Networks
- **Journal**: None
- **Summary**: Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and $ 1\times 1 $ convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method.



### Fully Convolutional Attention Networks for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.06765v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06765v4)
- **Published**: 2016-03-22 12:45:20+00:00
- **Updated**: 2017-03-21 02:08:15+00:00
- **Authors**: Xiao Liu, Tian Xia, Jiang Wang, Yi Yang, Feng Zhou, Yuanqing Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained recognition is challenging due to its subtle local inter-class differences versus large intra-class variations such as poses. A key to address this problem is to localize discriminative parts to extract pose-invariant features. However, ground-truth part annotations can be expensive to acquire. Moreover, it is hard to define parts for many fine-grained classes. This work introduces Fully Convolutional Attention Networks (FCANs), a reinforcement learning framework to optimally glimpse local discriminative regions adaptive to different fine-grained domains. Compared to previous methods, our approach enjoys three advantages: 1) the weakly-supervised reinforcement learning procedure requires no expensive part annotations; 2) the fully-convolutional architecture speeds up both training and testing; 3) the greedy reward strategy accelerates the convergence of the learning. We demonstrate the effectiveness of our method with extensive experiments on four challenging fine-grained benchmark datasets, including CUB-200-2011, Stanford Dogs, Stanford Cars and Food-101.



### Energy-Efficient ConvNets Through Approximate Computing
- **Arxiv ID**: http://arxiv.org/abs/1603.06777v1
- **DOI**: 10.1109/WACV.2016.7477614
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06777v1)
- **Published**: 2016-03-22 13:20:36+00:00
- **Updated**: 2016-03-22 13:20:36+00:00
- **Authors**: Bert Moons, Bert De Brabandere, Luc Van Gool, Marian Verhelst
- **Comment**: Published in IEEE Winter Conference on Applications of Computer
  Vision (WACV 2016)
- **Journal**: None
- **Summary**: Recently ConvNets or convolutional neural networks (CNN) have come up as state-of-the-art classification and detection algorithms, achieving near-human performance in visual detection. However, ConvNet algorithms are typically very computation and memory intensive. In order to be able to embed ConvNet-based classification into wearable platforms and embedded systems such as smartphones or ubiquitous electronics for the internet-of-things, their energy consumption should be reduced drastically. This paper proposes methods based on approximate computing to reduce energy consumption in state-of-the-art ConvNet accelerators. By combining techniques both at the system- and circuit level, we can gain energy in the systems arithmetic: up to 30x without losing classification accuracy and more than 100x at 99% classification accuracy, compared to the commonly used 16-bit fixed point number format.



### Con-Patch: When a Patch Meets its Context
- **Arxiv ID**: http://arxiv.org/abs/1603.06812v3
- **DOI**: 10.1109/TIP.2016.2576402
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06812v3)
- **Published**: 2016-03-22 14:44:28+00:00
- **Updated**: 2016-06-09 14:14:58+00:00
- **Authors**: Yaniv Romano, Michael Elad
- **Comment**: Accepted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Measuring the similarity between patches in images is a fundamental building block in various tasks. Naturally, the patch-size has a major impact on the matching quality, and on the consequent application performance. Under the assumption that our patch database is sufficiently sampled, using large patches (e.g. 21-by-21) should be preferred over small ones (e.g. 7-by-7). However, this "dense-sampling" assumption is rarely true; in most cases large patches cannot find relevant nearby examples. This phenomenon is a consequence of the curse of dimensionality, stating that the database-size should grow exponentially with the patch-size to ensure proper matches. This explains the favored choice of small patch-size in most applications.   Is there a way to keep the simplicity and work with small patches while getting some of the benefits that large patches provide? In this work we offer such an approach. We propose to concatenate the regular content of a conventional (small) patch with a compact representation of its (large) surroundings - its context. Therefore, with a minor increase of the dimensions (e.g. with additional 10 values to the patch representation), we implicitly/softly describe the information of a large patch. The additional descriptors are computed based on a self-similarity behavior of the patch surrounding.   We show that this approach achieves better matches, compared to the use of conventional-size patches, without the need to increase the database-size. Also, the effectiveness of the proposed method is tested on three distinct problems: (i) External natural image denoising, (ii) Depth image super-resolution, and (iii) Motion-compensated frame-rate up-conversion.



### Object Recognition and Identification Using ESM Data
- **Arxiv ID**: http://arxiv.org/abs/1607.01355v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.01355v1)
- **Published**: 2016-03-22 15:03:59+00:00
- **Updated**: 2016-03-22 15:03:59+00:00
- **Authors**: E. Taghavi, D. Song, R. Tharmarasa, T. Kirubarajan, Anne-Claire Boury-Brisset, Bhashyam Balaji
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition and identification of unknown targets is a crucial task in surveillance and security systems. Electronic Support Measures (ESM) are one of the most effective sensors for identification, especially for maritime and air--to--ground applications. In typical surveillance systems multiple ESM sensors are usually deployed along with kinematic sensors like radar. Different ESM sensors may produce different types of reports ready to be sent to the fusion center. The focus of this paper is to develop a new architecture for target recognition and identification when non--homogeneous ESM and possibly kinematic reports are received at the fusion center. The new fusion architecture is evaluated using simulations to show the benefit of utilizing different ESM reports such as attributes and signal level ESM data.



### Multi-velocity neural networks for gesture recognition in videos
- **Arxiv ID**: http://arxiv.org/abs/1603.06829v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.06829v1)
- **Published**: 2016-03-22 15:26:26+00:00
- **Updated**: 2016-03-22 15:26:26+00:00
- **Authors**: Otkrist Gupta, Dan Raviv, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new action recognition deep neural network which adaptively learns the best action velocities in addition to the classification. While deep neural networks have reached maturity for image understanding tasks, we are still exploring network topologies and features to handle the richer environment of video clips. Here, we tackle the problem of multiple velocities in action recognition, and provide state-of-the-art results for gesture recognition, on known and new collected datasets. We further provide the training steps for our semi-supervised network, suited to learn from huge unlabeled datasets with only a fraction of labeled examples.



### A Selection of Giant Radio Sources from NVSS
- **Arxiv ID**: http://arxiv.org/abs/1603.06895v2
- **DOI**: 10.3847/0067-0049/224/2/18
- **Categories**: **astro-ph.GA**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1603.06895v2)
- **Published**: 2016-03-22 18:18:16+00:00
- **Updated**: 2016-05-10 01:29:20+00:00
- **Authors**: D. D. Proctor
- **Comment**: 20 pages of text, 6 figures, 22 pages tables, total 55 pages. The
  stub for Table 6 is followed by the complete machine readable file. To be
  published in The Astrophysical Journal Supplement. Revision 1: Corrected
  typos, references updated/corrected, addition to acknowledgments. Five
  candidates identified as SNR (Thanks to D. A. Green)
- **Journal**: None
- **Summary**: Results of the application of pattern recognition techniques to the problem of identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are presented and issues affecting the process are explored. Decision-tree pattern recognition software was applied to training set source pairs developed from known NVSS large angular size radio galaxies. The full training set consisted of 51,195 source pairs, 48 of which were known GRS for which each lobe was primarily represented by a single catalog component. The source pairs had a maximum separation of 20 arc minutes and a minimum component area of 1.87 square arc minutes at the 1.4 mJy level. The importance of comparing resulting probability distributions of the training and application sets for cases of unknown class ratio is demonstrated. The probability of correctly ranking a randomly selected (GRS, non-GRS) pair from the best of the tested classifiers was determined to be 97.8 +/- 1.5%. The best classifiers were applied to the over 870,000 candidate pairs from the entire catalog. Images of higher ranked sources were visually screened and a table of over sixteen hundred candidates, including morphological annotation, is presented. These systems include doubles and triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped systems, and core-jets and resolved cores. While some resolved lobe systems are recovered with this technique, generally it is expected that such systems would require a different approach.



### Stacked Hourglass Networks for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1603.06937v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06937v2)
- **Published**: 2016-03-22 19:56:42+00:00
- **Updated**: 2016-07-26 19:19:37+00:00
- **Authors**: Alejandro Newell, Kaiyu Yang, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.



### Knowledge Transfer for Scene-specific Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1603.06987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06987v2)
- **Published**: 2016-03-22 21:19:42+00:00
- **Updated**: 2016-07-26 01:05:52+00:00
- **Authors**: Lamberto Ballan, Francesco Castaldo, Alexandre Alahi, Francesco Palmieri, Silvio Savarese
- **Comment**: Accepted to ECCV 2016
- **Journal**: None
- **Summary**: When given a single frame of the video, humans can not only interpret the content of the scene, but also they are able to forecast the near future. This ability is mostly driven by their rich prior knowledge about the visual world, both in terms of (i) the dynamics of moving agents, as well as (ii) the semantic of the scene. In this work we exploit the interplay between these two key elements to predict scene-specific motion patterns. First, we extract patch descriptors encoding the probability of moving to the adjacent patches, and the probability of being in that particular patch or changing behavior. Then, we introduce a Dynamic Bayesian Network which exploits this scene specific knowledge for trajectory prediction. Experimental results demonstrate that our method is able to accurately predict trajectories and transfer predictions to a novel scene characterized by similar elements.



### Multi-Scale Convolutional Neural Networks for Time Series Classification
- **Arxiv ID**: http://arxiv.org/abs/1603.06995v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.06995v4)
- **Published**: 2016-03-22 21:37:33+00:00
- **Updated**: 2016-05-11 04:48:21+00:00
- **Authors**: Zhicheng Cui, Wenlin Chen, Yixin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Time series classification (TSC), the problem of predicting class labels of time series, has been around for decades within the community of data mining and machine learning, and found many important applications such as biomedical engineering and clinical prediction. However, it still remains challenging and falls short of classification accuracy and efficiency. Traditional approaches typically involve extracting discriminative features from the original time series using dynamic time warping (DTW) or shapelet transformation, based on which an off-the-shelf classifier can be applied. These methods are ad-hoc and separate the feature extraction part with the classification part, which limits their accuracy performance. Plus, most existing methods fail to take into account the fact that time series often have features at different time scales. To address these problems, we propose a novel end-to-end neural network model, Multi-Scale Convolutional Neural Networks (MCNN), which incorporates feature extraction and classification in a single framework. Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at different scales and frequencies, leading to superior feature representation. MCNN is also computationally efficient, as it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods on a large number of benchmark datasets, and show that MCNN advances the state-of-the-art by achieving superior accuracy performance than other leading methods.



### Active Detection and Localization of Textureless Objects in Cluttered Environments
- **Arxiv ID**: http://arxiv.org/abs/1603.07022v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1603.07022v1)
- **Published**: 2016-03-22 22:55:03+00:00
- **Updated**: 2016-03-22 22:55:03+00:00
- **Authors**: Marco Imperoli, Alberto Pretto
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an active object detection and localization framework that combines a robust untextured object detection and 3D pose estimation algorithm with a novel next-best-view selection strategy. We address the detection and localization problems by proposing an edge-based registration algorithm that refines the object position by minimizing a cost directly extracted from a 3D image tensor that encodes the minimum distance to an edge point in a joint direction/location space. We face the next-best-view problem by exploiting a sequential decision process that, for each step, selects the next camera position which maximizes the mutual information between the state and the next observations. We solve the intrinsic intractability of this solution by generating observations that represent scene realizations, i.e. combination samples of object hypothesis provided by the object detector, while modeling the state by means of a set of constantly resampled particles. Experiments performed on different real world, challenging datasets confirm the effectiveness of the proposed methods.



### MOON: A Mixed Objective Optimization Network for the Recognition of Facial Attributes
- **Arxiv ID**: http://arxiv.org/abs/1603.07027v2
- **DOI**: 10.1007/978-3-319-46454-1_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.07027v2)
- **Published**: 2016-03-22 23:21:26+00:00
- **Updated**: 2016-10-21 16:56:07+00:00
- **Authors**: Ethan Rudd, Manuel GÃ¼nther, Terrance Boult
- **Comment**: Post-print of manuscript accepted to the European Conference on
  Computer Vision (ECCV) 2016
  http://link.springer.com/chapter/10.1007%2F978-3-319-46454-1_2
- **Journal**: None
- **Summary**: Attribute recognition, particularly facial, extracts many labels for each image. While some multi-task vision problems can be decomposed into separate tasks and stages, e.g., training independent models for each task, for a growing set of problems joint optimization across all tasks has been shown to improve performance. We show that for deep convolutional neural network (DCNN) facial attribute extraction, multi-task optimization is better. Unfortunately, it can be difficult to apply joint optimization to DCNNs when training data is imbalanced, and re-balancing multi-label data directly is structurally infeasible, since adding/removing data to balance one label will change the sampling of the other labels. This paper addresses the multi-label imbalance problem by introducing a novel mixed objective optimization network (MOON) with a loss function that mixes multiple task objectives with domain adaptive re-weighting of propagated loss. Experiments demonstrate that not only does MOON advance the state of the art in facial attribute recognition, but it also outperforms independently trained DCNNs using the same data. When using facial attributes for the LFW face recognition task, we show that our balanced (domain adapted) network outperforms the unbalanced trained network.




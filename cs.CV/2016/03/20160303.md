# Arxiv Papers in cs.CV on 2016-03-03
### PCANet: An energy perspective
- **Arxiv ID**: http://arxiv.org/abs/1603.00944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.00944v1)
- **Published**: 2016-03-03 01:26:25+00:00
- **Updated**: 2016-03-03 01:26:25+00:00
- **Authors**: Jiasong Wu, Shijie Qiu, Youyong Kong, Longyu Jiang, Lotfi Senhadji, Huazhong Shu
- **Comment**: 37 pages, 23 figures, 16 tables
- **Journal**: None
- **Summary**: The principal component analysis network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various databases. However, the explanation of the PCANet is lacked. In this paper, we try to explain why PCANet works well from energy perspective point of view based on a set of experiments. The impact of various parameters on the error rate of PCANet is analyzed in depth. It was found that this error rate is correlated with the logarithm of energy of image. The proposed energy explanation approach can be used as a testing method for checking if every step of the constructed networks is necessary.



### Cellular Automata Segmentation of the Boundary between the Compacta of Vertebral Bodies and Surrounding Structures
- **Arxiv ID**: http://arxiv.org/abs/1603.00960v1
- **DOI**: 10.1117/12.2209039
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1603.00960v1)
- **Published**: 2016-03-03 03:35:07+00:00
- **Updated**: 2016-03-03 03:35:07+00:00
- **Authors**: Jan Egger, Christopher Nimsky
- **Comment**: 6 pages, 5 figures, 1 table, 42 references
- **Journal**: SPIE Medical Imaging Conference 2016, Paper 9787-52
- **Summary**: Due to the aging population, spinal diseases get more and more common nowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white women and 13% for white men in the United States. Thus the numbers of surgical spinal procedures are also increasing with the aging population and precise diagnosis plays a vital role in reducing complication and recurrence of symptoms. Spinal imaging of vertebral column is a tedious process subjected to interpretation errors. In this contribution, we aim to reduce time and error for vertebral interpretation by applying and studying the GrowCut-algorithm for boundary segmentation between vertebral body compacta and surrounding structures. GrowCut is a competitive region growing algorithm using cellular automata. For our study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans were first manually outlined by neurosurgeons. Then, the vertebral bodies were segmented in the medical images by a GrowCut-trained physician using the semi-automated GrowCut-algorithm. Afterwards, results of both segmentation processes were compared using the Dice Similarity Coefficient (DSC) and the Hausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of 18.91+/-7.2 voxel, respectively. In addition, the times have been measured during the manual and the GrowCut segmentations, showing that a GrowCut-segmentation - with an average time of less than six minutes (5.77+/-0.73) - is significantly shorter than a pure manual outlining.



### Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via User-Defined Templates
- **Arxiv ID**: http://arxiv.org/abs/1603.00961v1
- **DOI**: 10.1117/12.2216226
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1603.00961v1)
- **Published**: 2016-03-03 03:39:59+00:00
- **Updated**: 2016-03-03 03:39:59+00:00
- **Authors**: Tobias Lüddemann, Jan Egger
- **Comment**: 6 pages, 4 figures, 1 table, 43 references
- **Journal**: SPIE Medical Imaging Conference 2016, Paper 9784-113
- **Summary**: Among all types of cancer, gynecological malignancies belong to the 4th most frequent type of cancer among women. Besides chemotherapy and external beam radiation, brachytherapy is the standard procedure for the treatment of these malignancies. In the progress of treatment planning, localization of the tumor as the target volume and adjacent organs of risks by segmentation is crucial to accomplish an optimal radiation distribution to the tumor while simultaneously preserving healthy tissue. Segmentation is performed manually and represents a time-consuming task in clinical daily routine. This study focuses on the segmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecological brachytherapy. The proposed segmentation method uses an interactive, graph-based segmentation scheme with a user-defined template. The scheme creates a directed two dimensional graph, followed by the minimal cost closed set computation on the graph, resulting in an outlining of the rectum. The graphs outline is dynamically adapted to the last calculated cut. Evaluation was performed by comparing manual segmentations of the rectum/sigmoid colon to results achieved with the proposed method. The comparison of the algorithmic to manual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%, in comparison to 83.97+/-8.08% for the comparison of two manual segmentations of the same physician. Utilizing the proposed methodology resulted in a median time of 128 seconds per dataset, compared to 300 seconds needed for pure manual segmentation.



### Self-localization from Images with Small Overlap
- **Arxiv ID**: http://arxiv.org/abs/1603.00993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.00993v1)
- **Published**: 2016-03-03 06:39:37+00:00
- **Updated**: 2016-03-03 06:39:37+00:00
- **Authors**: Tanaka Kanji
- **Comment**: 8 pages, 9 figures, Draft of a paper submitted to an International
  Conference
- **Journal**: None
- **Summary**: With the recent success of visual features from deep convolutional neural networks (DCNN) in visual robot self-localization, it has become important and practical to address more general self-localization scenarios. In this paper, we address the scenario of self-localization from images with small overlap. We explicitly introduce a localization difficulty index as a decreasing function of view overlap between query and relevant database images and investigate performance versus difficulty for challenging cross-view self-localization tasks. We then reformulate the self-localization as a scalable bag-of-visual-features (BoVF) scene retrieval and present an efficient solution called PCA-NBNN, aiming to facilitate fast and yet discriminative correspondence between partially overlapping images. The proposed approach adopts recent findings in discriminativity preserving encoding of DCNN features using principal component analysis (PCA) and cross-domain scene matching using naive Bayes nearest neighbor distance metric (NBNN). We experimentally demonstrate that the proposed PCA-NBNN framework frequently achieves comparable results to previous DCNN features and that the BoVF model is significantly more efficient. We further address an important alternative scenario of "self-localization from images with NO overlap" and report the result.



### Automatic learning of gait signatures for people identification
- **Arxiv ID**: http://arxiv.org/abs/1603.01006v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1603.01006v2)
- **Published**: 2016-03-03 08:07:14+00:00
- **Updated**: 2016-06-14 16:07:07+00:00
- **Authors**: F. M. Castro, M. J. Marin-Jimenez, N. Guil, N. Perez de la Blanca
- **Comment**: Proof of concept paper. Technical report on the use of ConvNets (CNN)
  for gait recognition. Data and code:
  http://www.uco.es/~in1majim/research/cnngaitof.html
- **Journal**: None
- **Summary**: This work targets people identification in video based on the way they walk (i.e. gait). While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e. optical flow components). We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset. The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e. 80x60 pixels).



### Photographic dataset: random peppercorns
- **Arxiv ID**: http://arxiv.org/abs/1603.01046v1
- **DOI**: None
- **Categories**: **physics.data-an**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.01046v1)
- **Published**: 2016-03-03 10:24:07+00:00
- **Updated**: 2016-03-03 10:24:07+00:00
- **Authors**: Teemu Helenius, Samuli Siltanen
- **Comment**: None
- **Journal**: None
- **Summary**: This is a photographic dataset collected for testing image processing algorithms. The idea is to have sets of different but statistically similar images. In this work the images show randomly distributed peppercorns. The dataset is made available at www.fips.fi/photographic_dataset.php .



### A novel and automatic pectoral muscle identification algorithm for mediolateral oblique (MLO) view mammograms using ImageJ
- **Arxiv ID**: http://arxiv.org/abs/1603.01056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01056v1)
- **Published**: 2016-03-03 11:20:23+00:00
- **Updated**: 2016-03-03 11:20:23+00:00
- **Authors**: Chao Wang
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Pectoral muscle identification is often required for breast cancer risk analysis, such as estimating breast density. Traditional methods are overwhelmingly based on manual visual assessment or straight line fitting for the pectoral muscle boundary, which are inefficient and inaccurate since pectoral muscle in mammograms can have curved boundaries.   This paper proposes a novel and automatic pectoral muscle identification algorithm for MLO view mammograms. It is suitable for both scanned film and full field digital mammograms. This algorithm is demonstrated using a public domain software ImageJ. A validation of this algorithm has been performed using real-world data and it shows promising result.



### Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain Decoding
- **Arxiv ID**: http://arxiv.org/abs/1603.01067v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1603.01067v1)
- **Published**: 2016-03-03 12:06:00+00:00
- **Updated**: 2016-03-03 12:06:00+00:00
- **Authors**: Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural
- **Comment**: 13 pages, 10 figures, submitted to JSTSP Special Issue on Advanced
  Signal Processing in Brain Networks
- **Journal**: None
- **Summary**: We represent the sequence of fMRI (Functional Magnetic Resonance Imaging) brain volumes recorded during a cognitive stimulus by a graph which consists of a set of local meshes. The corresponding cognitive process, encoded in the brain, is then represented by these meshes each of which is estimated assuming a linear relationship among the voxel time series in a predefined locality. First, we define the concept of locality in two neighborhood systems, namely, the spatial and functional neighborhoods. Then, we construct spatially and functionally local meshes around each voxel, called seed voxel, by connecting it either to its spatial or functional p-nearest neighbors. The mesh formed around a voxel is a directed sub-graph with a star topology, where the direction of the edges is taken towards the seed voxel at the center of the mesh. We represent the time series recorded at each seed voxel in terms of linear combination of the time series of its p-nearest neighbors in the mesh. The relationships between a seed voxel and its neighbors are represented by the edge weights of each mesh, and are estimated by solving a linear regression equation. The estimated mesh edge weights lead to a better representation of information in the brain for encoding and decoding of the cognitive tasks. We test our model on a visual object recognition and emotional memory retrieval experiments using Support Vector Machines that are trained using the mesh edge weights as features. In the experimental analysis, we observe that the edge weights of the spatial and functional meshes perform better than the state-of-the-art brain decoding models.



### First Steps Toward Camera Model Identification with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.01068v2
- **DOI**: 10.1109/LSP.2016.2641006
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1603.01068v2)
- **Published**: 2016-03-03 12:10:47+00:00
- **Updated**: 2017-09-26 09:29:28+00:00
- **Authors**: Luca Bondi, Luca Baroffio, David Güera, Paolo Bestagini, Edward J. Delp, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting the camera model used to shoot a picture enables to solve a wide series of forensic problems, from copyright infringement to ownership attribution. For this reason, the forensic community has developed a set of camera model identification algorithms that exploit characteristic traces left on acquired images by the processing pipelines specific of each camera model. In this paper, we investigate a novel approach to solve camera model identification problem. Specifically, we propose a data-driven algorithm based on convolutional neural networks, which learns features characterizing each camera model directly from the acquired pictures. Results on a well-known dataset of 18 camera models show that: (i) the proposed method outperforms up-to-date state-of-the-art algorithms on classification of 64x64 color image patches; (ii) features learned by the proposed network generalize to camera models never used for training.



### What is the right way to represent document images?
- **Arxiv ID**: http://arxiv.org/abs/1603.01076v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01076v3)
- **Published**: 2016-03-03 12:46:51+00:00
- **Updated**: 2016-12-02 16:38:25+00:00
- **Authors**: Gabriela Csurka, Diane Larlus, Albert Gordo, Jon Almazan
- **Comment**: None
- **Journal**: None
- **Summary**: In this article we study the problem of document image representation based on visual features. We propose a comprehensive experimental study that compares three types of visual document image representations: (1) traditional so-called shallow features, such as the RunLength and the Fisher-Vector descriptors, (2) deep features based on Convolutional Neural Networks, and (3) features extracted from hybrid architectures that take inspiration from the two previous ones.   We evaluate these features in several tasks (i.e. classification, clustering, and retrieval) and in different setups (e.g. domain transfer) using several public and in-house datasets. Our results show that deep features generally outperform other types of features when there is no domain shift and the new task is closely related to the one used to train the model. However, when a large domain or task shift is present, the Fisher-Vector shallow features generalize better and often obtain the best results.



### Elastic Net Hypergraph Learning for Image Clustering and Semi-supervised Classification
- **Arxiv ID**: http://arxiv.org/abs/1603.01096v1
- **DOI**: 10.1109/TIP.2016.2621671
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01096v1)
- **Published**: 2016-03-03 13:37:23+00:00
- **Updated**: 2016-03-03 13:37:23+00:00
- **Authors**: Qingshan Liu, Yubao Sun, Cantian Wang, Tongliang Liu, Dacheng Tao
- **Comment**: 13 pages, 12 figures
- **Journal**: None
- **Summary**: Graph model is emerging as a very effective tool for learning the complex structures and relationships hidden in data. Generally, the critical purpose of graph-oriented learning algorithms is to construct an informative graph for image clustering and classification tasks. In addition to the classical $K$-nearest-neighbor and $r$-neighborhood methods for graph construction, $l_1$-graph and its variants are emerging methods for finding the neighboring samples of a center datum, where the corresponding ingoing edge weights are simultaneously derived by the sparse reconstruction coefficients of the remaining samples. However, the pair-wise links of $l_1$-graph are not capable of capturing the high order relationships between the center datum and its prominent data in sparse reconstruction. Meanwhile, from the perspective of variable selection, the $l_1$ norm sparse constraint, regarded as a LASSO model, tends to select only one datum from a group of data that are highly correlated and ignore the others. To simultaneously cope with these drawbacks, we propose a new elastic net hypergraph learning model, which consists of two steps. In the first step, the Robust Matrix Elastic Net model is constructed to find the canonically related samples in a somewhat greedy way, achieving the grouping effect by adding the $l_2$ penalty to the $l_1$ constraint. In the second step, hypergraph is used to represent the high order relationships between each datum and its prominent samples by regarding them as a hyperedge. Subsequently, hypergraph Laplacian matrix is constructed for further analysis. New hypergraph learning algorithms, including unsupervised clustering and multi-class semi-supervised classification, are then derived. Extensive experiments on face and handwriting databases demonstrate the effectiveness of the proposed method.



### Automated quantification of one-dimensional nanostructure alignment on surfaces
- **Arxiv ID**: http://arxiv.org/abs/1607.07297v1
- **DOI**: 10.1088/0957-4484/27/23/235701
- **Categories**: **physics.ins-det**, cond-mat.mtrl-sci, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1607.07297v1)
- **Published**: 2016-03-03 20:04:14+00:00
- **Updated**: 2016-03-03 20:04:14+00:00
- **Authors**: Jianjin Dong, Irene A. Goldthorpe, Nasser Mohieddin Abukhdeir
- **Comment**: 14 pages, 6 figures, submitted to Nanotechnology
- **Journal**: None
- **Summary**: A method for automated quantification of the alignment of one-dimensional nanostructures from microscopy imaging is presented. Nanostructure alignment metrics are formulated and shown to able to rigorously quantify the orientational order of nanostructures within a two-dimensional domain (surface). A complementary image processing method is also presented which enables robust processing of microscopy images where overlapping nanostructures might be present. Scanning electron microscopy (SEM) images of nanowire-covered surfaces are analyzed using the presented methods and it is shown that past single parameter alignment metrics are insufficient for highly aligned domains. Through the use of multiple parameter alignment metrics, automated quantitative analysis of SEM images is shown to be possible and the alignment characteristics of different samples are able to be rigorously compared using a similarity metric. The results of this work provide researchers in nanoscience and nanotechnology with a rigorous method for the determination of structure/property relationships where alignment of one-dimensional nanostructures is significant.



### HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.01249v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01249v3)
- **Published**: 2016-03-03 20:30:53+00:00
- **Updated**: 2017-12-06 01:34:20+00:00
- **Authors**: Rajeev Ranjan, Vishal M. Patel, Rama Chellappa
- **Comment**: Accepted in Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2) Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.



### Decision Forests, Convolutional Networks and the Models in-Between
- **Arxiv ID**: http://arxiv.org/abs/1603.01250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1603.01250v1)
- **Published**: 2016-03-03 20:41:47+00:00
- **Updated**: 2016-03-03 20:41:47+00:00
- **Authors**: Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, Antonio Criminisi
- **Comment**: Microsoft Research Technical Report
- **Journal**: None
- **Summary**: This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.



### Modular Decomposition and Analysis of Registration based Trackers
- **Arxiv ID**: http://arxiv.org/abs/1603.01292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.01292v2)
- **Published**: 2016-03-03 21:50:09+00:00
- **Updated**: 2016-03-25 18:39:12+00:00
- **Authors**: Abhineet Singh, Ankush Roy, Xi Zhang, Martin Jagersand
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new way to study registration based trackers by decomposing them into three constituent sub modules: appearance model, state space model and search method. It is often the case that when a new tracker is introduced in literature, it only contributes to one or two of these sub modules while using existing methods for the rest. Since these are often selected arbitrarily by the authors, they may not be optimal for the new method. In such cases, our breakdown can help to experimentally find the best combination of methods for these sub modules while also providing a framework within which the contributions of the new tracker can be clearly demarcated and thus studied better. We show how existing trackers can be broken down using the suggested methodology and compare the performance of the default configuration chosen by the authors against other possible combinations to demonstrate the new insights that can be gained by such an approach. We also present an open source system that provides a convenient interface to plug in a new method for any sub module and test it against all possible combinations of methods for the other two sub modules while also serving as a fast and efficient solution for practical tracking requirements.




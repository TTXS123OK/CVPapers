# Arxiv Papers in cs.CV on 2016-03-15
### Pushing the Limits of Deep CNNs for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1603.04525v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04525v2)
- **Published**: 2016-03-15 01:55:14+00:00
- **Updated**: 2016-06-06 06:36:15+00:00
- **Authors**: Qichang Hu, Peng Wang, Chunhua Shen, Anton van den Hengel, Fatih Porikli
- **Comment**: Fixed some typos
- **Journal**: None
- **Summary**: Compared to other applications in computer vision, convolutional neural networks have under-performed on pedestrian detection. A breakthrough was made very recently by using sophisticated deep CNN models, with a number of hand-crafted features, or explicit occlusion handling mechanism. In this work, we show that by re-using the convolutional feature maps (CFMs) of a deep convolutional neural network (DCNN) model as image features to train an ensemble of boosted decision models, we are able to achieve the best reported accuracy without using specially designed learning algorithms. We empirically identify and disclose important implementation details. We also show that pixel labelling may be simply combined with a detector to boost the detection performance. By adding complementary hand-crafted features such as optical flow, the DCNN based detector can be further improved. We set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from $11.7\%$ to $8.9\%$, a relative improvement of $24\%$. We also achieve a comparable result to the state-of-the-art approaches on the KITTI dataset.



### Object Contour Detection with a Fully Convolutional Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1603.04530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.04530v1)
- **Published**: 2016-03-15 02:11:49+00:00
- **Updated**: 2016-03-15 02:11:49+00:00
- **Authors**: Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang
- **Comment**: Accepted by CVPR2016 as spotlight
- **Journal**: None
- **Summary**: We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same super-categories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates ($\sim$1660 per image).



### Revealing the Hidden Patterns of News Photos: Analysis of Millions of News Photos Using GDELT and Deep Learning-based Vision APIs
- **Arxiv ID**: http://arxiv.org/abs/1603.04531v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1603.04531v2)
- **Published**: 2016-03-15 02:23:53+00:00
- **Updated**: 2016-03-24 04:44:12+00:00
- **Authors**: Haewoon Kwak, Jisun An
- **Comment**: Presented in the first workshop on NEws and publiC Opinion (NECO'16,
  www.neco.io, colocated with ICWSM'16), Cologne, Germany, 2016
- **Journal**: None
- **Summary**: In this work, we analyze more than two million news photos published in January 2016. We demonstrate i) which objects appear the most in news photos; ii) what the sentiments of news photos are; iii) whether the sentiment of news photos is aligned with the tone of the text; iv) how gender is treated; and v) how differently political candidates are portrayed. To our best knowledge, this is the first large-scale study of news photo contents using deep learning-based vision APIs.



### Learning Domain-Invariant Subspace using Domain Features and Independence Maximization
- **Arxiv ID**: http://arxiv.org/abs/1603.04535v2
- **DOI**: 10.1109/TCYB.2016.2633306
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.04535v2)
- **Published**: 2016-03-15 02:56:22+00:00
- **Updated**: 2017-06-22 01:39:22+00:00
- **Authors**: Ke Yan, Lu Kou, David Zhang
- **Comment**: Accepted
- **Journal**: None
- **Summary**: Domain adaptation algorithms are useful when the distributions of the training and the test data are different. In this paper, we focus on the problem of instrumental variation and time-varying drift in the field of sensors and measurement, which can be viewed as discrete and continuous distributional change in the feature space. We propose maximum independence domain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns a subspace which has maximum independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is also designed to project samples according to their backgrounds so as to improve the adaptation. The proposed algorithms are flexible and fast. Their effectiveness is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision. They can greatly enhance the practicability of sensor systems, as well as extend the application scope of existing domain adaptation algorithms by uniformly handling different kinds of distributional change.



### Effective Computer Model For Recognizing Nationality From Frontal Image
- **Arxiv ID**: http://arxiv.org/abs/1603.04550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04550v1)
- **Published**: 2016-03-15 04:30:34+00:00
- **Updated**: 2016-03-15 04:30:34+00:00
- **Authors**: Bat-Erdene Batsukh, Ganbat Tsend
- **Comment**: 5 pages in IJASCSE Volume 5, Issue 3 (March 2016)
- **Journal**: None
- **Summary**: We are introducing new effective computer model for extracting nationality from frontal image candidate using face part color, size and distances based on deep research. Determining face part size, color, and distances is depending on a variety of factors including image quality, lighting condition, rotation angle, occlusion and facial emotion. Therefore, first we need to detect a face on the image then convert an image into the real input. After that, we can determine image candidate gender, face shape, key points and face parts. Finally, we will return the result, based on the comparison of sizes and distances with the sample measurement table database. While we were measuring samples, there were big differences between images by their gender and face shapes. Input images must be the frontal face image that has smooth lighting and does not have any rotation angle. The model can be used in military, police, defense, healthcare, and technology sectors. Finally, Computer can distinguish nationality from the face image.



### Classification with Repulsion Tensors: A Case Study on Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1603.04588v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.2; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1603.04588v1)
- **Published**: 2016-03-15 08:31:04+00:00
- **Updated**: 2016-03-15 08:31:04+00:00
- **Authors**: Hawren Fang
- **Comment**: 25 pages, 8 figures, 6 tables, unpublished manuscript
- **Journal**: None
- **Summary**: We consider dimensionality reduction methods for face recognition in a supervised setting, using an image-as-matrix representation. A common procedure is to project image matrices into a smaller space in which the recognition is performed. These methods are often called "two-dimensional" in the literature and there exist counterparts that use an image-as-vector representation. When two face images are close to each other in the input space they may remain close after projection - but this is not desirable in the situation when these two images are from different classes, and this often affects the recognition performance. We extend a previously developed `repulsion Laplacean' technique based on adding terms to the objective function with the goal or creation a repulsion energy between such images in the projected space. This scheme, which relies on a repulsion graph, is generic and can be incorporated into various two-dimensional methods. It can be regarded as a multilinear generalization of the repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp. 2392--2402]. Experimental results demonstrate that the proposed methodology offers significant recognition improvement relative to the underlying two-dimensional methods.



### Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1603.04595v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1603.04595v2)
- **Published**: 2016-03-15 08:56:33+00:00
- **Updated**: 2016-04-14 14:11:18+00:00
- **Authors**: Olivier Morère, Jie Lin, Antoine Veillard, Vijay Chandrasekhar, Tomaso Poggio
- **Comment**: Image Instance Retrieval, CNN, Invariant Representation, Hashing,
  Unsupervised Learning, Regularization. arXiv admin note: text overlap with
  arXiv:1601.02093
- **Journal**: None
- **Summary**: The goal of this work is the computation of very compact binary hashes for image instance retrieval. Our approach has two novel contributions. The first one is Nested Invariance Pooling (NIP), a method inspired from i-theory, a mathematical theory for computing group invariant transformations with feed-forward neural networks. NIP is able to produce compact and well-performing descriptors with visual representations extracted from convolutional neural networks. We specifically incorporate scale, translation and rotation invariances but the scheme can be extended to any arbitrary sets of transformations. We also show that using moments of increasing order throughout nesting is important. The NIP descriptors are then hashed to the target code size (32-256 bits) with a Restricted Boltzmann Machine with a novel batch-level regularization scheme specifically designed for the purpose of hashing (RBMH). A thorough empirical evaluation with state-of-the-art shows that the results obtained both with the NIP descriptors and the NIP+RBMH hashes are consistently outstanding across a wide range of datasets.



### Scalable Image Retrieval by Sparse Product Quantization
- **Arxiv ID**: http://arxiv.org/abs/1603.04614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04614v1)
- **Published**: 2016-03-15 09:53:32+00:00
- **Updated**: 2016-03-15 09:53:32+00:00
- **Authors**: Qingqun Ning, Jianke Zhu, Zhiyuan Zhong, Steven C. H. Hoi, Chun Chen
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional feature indexing and retrieval is the crux of large-scale image retrieval. A recent promising technique is Product Quantization, which attempts to index high-dimensional image features by decomposing the feature space into a Cartesian product of low dimensional subspaces and quantizing each of them separately. Despite the promising results reported, their quantization approach follows the typical hard assignment of traditional quantization methods, which may result in large quantization errors and thus inferior search performance. Unlike the existing approaches, in this paper, we propose a novel approach called Sparse Product Quantization (SPQ) to encoding the high-dimensional feature vectors into sparse representation. We optimize the sparse representations of the feature vectors by minimizing their quantization errors, making the resulting representation is essentially close to the original data in practice. Experiments show that the proposed SPQ technique is not only able to compress data, but also an effective encoding technique. We obtain state-of-the-art results for ANN search on four public image datasets and the promising results of content-based image retrieval further validate the efficacy of our proposed method.



### Image Co-localization by Mimicking a Good Detector's Confidence Score Distribution
- **Arxiv ID**: http://arxiv.org/abs/1603.04619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04619v2)
- **Published**: 2016-03-15 10:21:47+00:00
- **Updated**: 2016-07-23 11:09:05+00:00
- **Authors**: Yao Li, Linqiao Liu, Chunhua Shen, Anton van den Hengel
- **Comment**: Accepted to Proc. European Conf. Computer Vision 2016
- **Journal**: None
- **Summary**: Given a set of images containing objects from the same category, the task of image co-localization is to identify and localize each instance. This paper shows that this problem can be solved by a simple but intriguing idea, that is, a common object detector can be learnt by making its detection confidence scores distributed like those of a strongly supervised detector. More specifically, we observe that given a set of object proposals extracted from an image that contains the object of interest, an accurate strongly supervised object detector should give high scores to only a small minority of proposals, and low scores to most of them. Thus, we devise an entropy-based objective function to enforce the above property when learning the common object detector. Once the detector is learnt, we resort to a segmentation approach to refine the localization. We show that despite its simplicity, our approach outperforms state-of-the-art methods.



### Modeling Time Series Similarity with Siamese Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1603.04713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04713v1)
- **Published**: 2016-03-15 14:57:44+00:00
- **Updated**: 2016-03-15 14:57:44+00:00
- **Authors**: Wenjie Pei, David M. J. Tax, Laurens van der Maaten
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Traditional techniques for measuring similarities between time series are based on handcrafted similarity measures, whereas more recent learning-based approaches cannot exploit external supervision. We combine ideas from time-series modeling and metric learning, and study siamese recurrent networks (SRNs) that minimize a classification loss to learn a good similarity measure between time series. Specifically, our approach learns a vectorial representation for each time series in such a way that similar time series are modeled by similar representations, and dissimilar time series by dissimilar representations. Because it is a similarity prediction models, SRNs are particularly well-suited to challenging scenarios such as signature recognition, in which each person is a separate class and very few examples per class are available. We demonstrate the potential merits of SRNs in within-domain and out-of-domain classification experiments and in one-shot learning experiments on tasks such as signature, voice, and sign language recognition.



### A Neural Approach to Blind Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1603.04771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04771v2)
- **Published**: 2016-03-15 17:35:26+00:00
- **Updated**: 2016-08-01 18:53:00+00:00
- **Authors**: Ayan Chakrabarti
- **Comment**: ECCV 2016. Project page at http://www.ttic.edu/chakrabarti/ndeblur
- **Journal**: None
- **Summary**: We present a new method for blind motion deblurring that uses a neural network trained to compute estimates of sharp image patches from observations that are blurred by an unknown motion kernel. Instead of regressing directly to patch intensities, this network learns to predict the complex Fourier coefficients of a deconvolution filter to be applied to the input patch for restoration. For inference, we apply the network independently to all overlapping patches in the observed image, and average its outputs to form an initial estimate of the sharp image. We then explicitly estimate a single global blur kernel by relating this estimate to the observed image, and finally perform non-blind deconvolution with this kernel. Our method exhibits accuracy and robustness close to state-of-the-art iterative methods, while being much faster when parallelized on GPU hardware.



### Revisiting Batch Normalization For Practical Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1603.04779v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1603.04779v4)
- **Published**: 2016-03-15 17:44:32+00:00
- **Updated**: 2016-11-08 06:11:30+00:00
- **Authors**: Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.



### Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/1603.04833v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1603.04833v1)
- **Published**: 2016-03-15 19:40:34+00:00
- **Updated**: 2016-03-15 19:40:34+00:00
- **Authors**: Debapriya Maji, Anirban Santara, Pabitra Mitra, Debdoot Sheet
- **Comment**: None
- **Journal**: None
- **Summary**: Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning for reliable detection of blood vessels in fundus color images. An ensemble of deep convolutional neural networks is trained to segment vessel and non-vessel areas of a color fundus image. During inference, the responses of the individual ConvNets of the ensemble are averaged to form the final segmentation. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with maximum average accuracy of 94.7\% and area under ROC curve of 0.9283.



### Hierarchical image simplification and segmentation based on Mumford-Shah-salient level line selection
- **Arxiv ID**: http://arxiv.org/abs/1603.04838v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04838v2)
- **Published**: 2016-03-15 19:51:10+00:00
- **Updated**: 2016-05-17 14:38:04+00:00
- **Authors**: Yongchao Xu, Thierry Géraud, Laurent Najman
- **Comment**: Pattern Recognition Letters, Elsevier, 2016
- **Journal**: None
- **Summary**: Hierarchies, such as the tree of shapes, are popular representations for image simplification and segmentation thanks to their multiscale structures. Selecting meaningful level lines (boundaries of shapes) yields to simplify image while preserving intact salient structures. Many image simplification and segmentation methods are driven by the optimization of an energy functional, for instance the celebrated Mumford-Shah functional. In this paper, we propose an efficient approach to hierarchical image simplification and segmentation based on the minimization of the piecewise-constant Mumford-Shah functional. This method conforms to the current trend that consists in producing hierarchical results rather than a unique partition. Contrary to classical approaches which compute optimal hierarchical segmentations from an input hierarchy of segmentations, we rely on the tree of shapes, a unique and well-defined representation equivalent to the image. Simply put, we compute for each level line of the image an attribute function that characterizes its persistence under the energy minimization. Then we stack the level lines from meaningless ones to salient ones through a saliency map based on extinction values defined on the tree-based shape space. Qualitative illustrations and quantitative evaluation on Weizmann segmentation evaluation database demonstrate the state-of-the-art performance of our method.



### Efficient Global Point Cloud Alignment using Bayesian Nonparametric Mixtures
- **Arxiv ID**: http://arxiv.org/abs/1603.04868v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04868v3)
- **Published**: 2016-03-15 20:02:38+00:00
- **Updated**: 2016-11-22 03:46:37+00:00
- **Authors**: Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud alignment is a common problem in computer vision and robotics, with applications ranging from 3D object recognition to reconstruction. We propose a novel approach to the alignment problem that utilizes Bayesian nonparametrics to describe the point cloud and surface normal densities, and branch and bound (BB) optimization to recover the relative transformation. BB uses a novel, refinable, near-uniform tessellation of rotation space using 4D tetrahedra, leading to more efficient optimization compared to the common axis-angle tessellation. We provide objective function bounds for pruning given the proposed tessellation, and prove that BB converges to the optimum of the cost function along with providing its computational complexity. Finally, we empirically demonstrate the efficiency of the proposed approach as well as its robustness to real-world conditions such as missing data and partial overlap.



### Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1603.04871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04871v1)
- **Published**: 2016-03-15 20:10:48+00:00
- **Updated**: 2016-03-15 20:10:48+00:00
- **Authors**: Zhicheng Yan, Hao Zhang, Yangqing Jia, Thomas Breuel, Yizhou Yu
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: State-of-the-art results of semantic segmentation are established by Fully Convolutional neural Networks (FCNs). FCNs rely on cascaded convolutional and pooling layers to gradually enlarge the receptive fields of neurons, resulting in an indirect way of modeling the distant contextual dependence. In this work, we advocate the use of spatially recurrent layers (i.e. ReNet layers) which directly capture global contexts and lead to improved feature representations. We demonstrate the effectiveness of ReNet layers by building a Naive deep ReNet (N-ReNet), which achieves competitive performance on Stanford Background dataset. Furthermore, we integrate ReNet layers with FCNs, and develop a novel Hybrid deep ReNet (H-ReNet). It enjoys a few remarkable properties, including full-image receptive fields, end-to-end training, and efficient network execution. On the PASCAL VOC 2012 benchmark, the H-ReNet improves the results of state-of-the-art approaches Piecewise, CRFasRNN and DeepParsing by 3.6%, 2.3% and 0.2%, respectively, and achieves the highest IoUs for 13 out of the 20 object classes.



### First Person Action-Object Detection with EgoNet
- **Arxiv ID**: http://arxiv.org/abs/1603.04908v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1603.04908v3)
- **Published**: 2016-03-15 22:29:03+00:00
- **Updated**: 2017-06-10 18:04:17+00:00
- **Authors**: Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Unlike traditional third-person cameras mounted on robots, a first-person camera, captures a person's visual sensorimotor object interactions from up close. In this paper, we study the tight interplay between our momentary visual attention and motor action with objects from a first-person camera. We propose a concept of action-objects---the objects that capture person's conscious visual (watching a TV) or tactile (taking a cup) interactions. Action-objects may be task-dependent but since many tasks share common person-object spatial configurations, action-objects exhibit a characteristic 3D spatial distance and orientation with respect to the person.   We design a predictive model that detects action-objects using EgoNet, a joint two-stream network that holistically integrates visual appearance (RGB) and 3D spatial layout (depth and height) cues to predict per-pixel likelihood of action-objects. Our network also incorporates a first-person coordinate embedding, which is designed to learn a spatial distribution of the action-objects in the first-person data. We demonstrate EgoNet's predictive power, by showing that it consistently outperforms previous baseline approaches. Furthermore, EgoNet also exhibits a strong generalization ability, i.e., it predicts semantically meaningful objects in novel first-person datasets. Our method's ability to effectively detect action-objects could be used to improve robots' understanding of human-object interactions.




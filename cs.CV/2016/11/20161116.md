# Arxiv Papers in cs.CV on 2016-11-16
### Low-rank Bilinear Pooling for Fine-Grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.05109v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05109v2)
- **Published**: 2016-11-16 01:10:41+00:00
- **Updated**: 2016-11-30 01:30:12+00:00
- **Authors**: Shu Kong, Charless Fowlkes
- **Comment**: None
- **Journal**: None
- **Summary**: Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of fine-grained classification tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a low-rank bilinear classifier. The resulting classifier can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned.   To further compress the model, we propose classifier co-decomposition that factorizes the collection of bilinear classifiers into a common factor and compact per-class terms. The co-decomposition idea can be deployed through two convolutional layers and trained in an end-to-end architecture. We suggest a simple yet effective initialization that avoids explicitly first training and factorizing the larger bilinear classifiers. Through extensive experiments, we show that our model achieves state-of-the-art performance on several public datasets for fine-grained classification trained with only category labels. Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model, and three orders smaller than the standard bilinear CNN model.



### Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact CNN Representations
- **Arxiv ID**: http://arxiv.org/abs/1611.05113v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05113v3)
- **Published**: 2016-11-16 01:33:51+00:00
- **Updated**: 2019-07-01 12:17:00+00:00
- **Authors**: Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, Ondrej Chum
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. We perform diffusion through a sparse linear system solver, yielding practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval.



### The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives
- **Arxiv ID**: http://arxiv.org/abs/1611.05118v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1611.05118v2)
- **Published**: 2016-11-16 02:16:09+00:00
- **Updated**: 2017-05-07 20:26:24+00:00
- **Authors**: Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daumé III, Larry Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the "gutters" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called "closure". While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.



### Learning To Score Olympic Events
- **Arxiv ID**: http://arxiv.org/abs/1611.05125v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05125v3)
- **Published**: 2016-11-16 02:56:24+00:00
- **Updated**: 2017-05-18 05:55:24+00:00
- **Authors**: Paritosh Parmar, Brendan Tran Morris
- **Comment**: CVPR 2017 - CVSports Workshop
- **Journal**: None
- **Summary**: Estimating action quality, the process of assigning a "score" to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small -- typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR, ii) LSTM, and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of Olympic events {diving, vault, figure skating}. While the SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback.



### Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning
- **Arxiv ID**: http://arxiv.org/abs/1611.05128v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05128v4)
- **Published**: 2016-11-16 03:00:40+00:00
- **Updated**: 2017-04-18 19:49:29+00:00
- **Authors**: Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze
- **Comment**: Published as a conference paper at CVPR 2017
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or amount of computation, we find that they do not necessarily result in lower energy consumption, and therefore do not serve as a good metric for energy cost estimation.   To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses energy consumption estimation of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements that target realistic battery-powered system setups. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in output feature maps instead of filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is further globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. Finally, we show that pruning the AlexNet with a reduced number of target classes can greatly decrease the number of weights but the energy reduction is limited.   Energy modeling tool and energy-aware pruned models available at http://eyeriss.mit.edu/energy.html



### Cost-Sensitive Deep Learning with Layer-Wise Cost Estimation
- **Arxiv ID**: http://arxiv.org/abs/1611.05134v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05134v2)
- **Published**: 2016-11-16 03:31:25+00:00
- **Updated**: 2020-10-03 03:10:16+00:00
- **Authors**: Yu-An Chung, Shao-Wen Yang, Hsuan-Tien Lin
- **Comment**: None
- **Journal**: None
- **Summary**: While deep neural networks have succeeded in several visual applications, such as object recognition, detection, and localization, by reaching very high classification accuracies, it is important to note that many real-world applications demand varying costs for different types of misclassification errors, thus requiring cost-sensitive classification algorithms. Current models of deep neural networks for cost-sensitive classification are restricted to some specific network structures and limited depth. In this paper, we propose a novel framework that can be applied to deep neural networks with any structure to facilitate their learning of meaningful representations for cost-sensitive classification problems. Furthermore, the framework allows end-to-end training of deeper networks directly. The framework is designed by augmenting auxiliary neurons to the output of each hidden layer for layer-wise cost estimation, and including the total estimation loss within the optimization objective. Experimental results on public benchmark visual data sets with two cost information settings demonstrate that the proposed framework outperforms state-of-the-art cost-sensitive deep learning models.



### S3Pool: Pooling with Stochastic Spatial Sampling
- **Arxiv ID**: http://arxiv.org/abs/1611.05138v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.05138v1)
- **Published**: 2016-11-16 04:17:52+00:00
- **Updated**: 2016-11-16 04:17:52+00:00
- **Authors**: Shuangfei Zhai, Hui Wu, Abhishek Kumar, Yu Cheng, Yongxi Lu, Zhongfei Zhang, Rogerio Feris
- **Comment**: None
- **Journal**: None
- **Summary**: Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two-step procedure: first, a pooling window (e.g., $2\times 2$) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for \emph{learning} (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models. Experimental code is available at https://github.com/Shuangfei/s3pool.



### Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering
- **Arxiv ID**: http://arxiv.org/abs/1611.05148v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05148v3)
- **Published**: 2016-11-16 05:19:50+00:00
- **Updated**: 2017-06-28 02:45:32+00:00
- **Authors**: Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, Hanning Zhou
- **Comment**: 8 pages, 6 figures, accepted by IJCAI2017
- **Journal**: None
- **Summary**: Clustering is among the most fundamental tasks in computer vision and machine learning. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into observables. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 4 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training. Lastly, VaDE is a flexible and extensible framework for unsupervised generative clustering, more general mixture models than GMM can be easily plugged in.



### One-Shot Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.05198v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05198v4)
- **Published**: 2016-11-16 09:58:37+00:00
- **Updated**: 2017-04-13 08:08:55+00:00
- **Authors**: Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, Luc Van Gool
- **Comment**: CVPR 2017 camera ready. Code:
  http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/
- **Journal**: None
- **Summary**: This paper tackles the task of semi-supervised video object segmentation, i.e., the separation of an object from the background in a video, given the mask of the first frame. We present One-Shot Video Object Segmentation (OSVOS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one-shot). Although all frames are processed independently, the results are temporally coherent and stable. We perform experiments on two annotated video segmentation databases, which show that OSVOS is fast and improves the state of the art by a significant margin (79.8% vs 68.0%).



### Will People Like Your Image? Learning the Aesthetic Space
- **Arxiv ID**: http://arxiv.org/abs/1611.05203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05203v2)
- **Published**: 2016-11-16 10:13:06+00:00
- **Updated**: 2017-12-04 08:36:39+00:00
- **Authors**: Katharina Schwarz, Patrick Wieschollek, Hendrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: Rating how aesthetically pleasing an image appears is a highly complex matter and depends on a large number of different visual factors. Previous work has tackled the aesthetic rating problem by ranking on a 1-dimensional rating scale, e.g., incorporating handcrafted attributes. In this paper, we propose a rather general approach to automatically map aesthetic pleasingness with all its complexity into an "aesthetic space" to allow for a highly fine-grained resolution. In detail, making use of deep learning, our method directly learns an encoding of a given image into this high-dimensional feature space resembling visual aesthetics. Additionally to the mentioned visual factors, differences in personal judgments have a large impact on the likeableness of a photograph. Nowadays, online platforms allow users to "like" or favor certain content with a single click. To incorporate a huge diversity of people, we make use of such multi-user agreements and assemble a large data set of 380K images (AROD) with associated meta information and derive a score to rate how visually pleasing a given photo is. We validate our derived model of aesthetics in a user study. Further, without any extra data labeling or handcrafted features, we achieve state-of-the art accuracy on the AVA benchmark data set. Finally, as our approach is able to predict the aesthetic quality of any arbitrary image or video, we demonstrate our results on applications for resorting photo collections, capturing the best shot on mobile devices and aesthetic key-frame extraction from videos.



### Deep Variational Inference Without Pixel-Wise Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1611.05209v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.05209v1)
- **Published**: 2016-11-16 10:20:10+00:00
- **Updated**: 2016-11-16 10:20:10+00:00
- **Authors**: Siddharth Agrawal, Ambedkar Dukkipati
- **Comment**: None
- **Journal**: None
- **Summary**: Variational autoencoders (VAEs), that are built upon deep neural networks have emerged as popular generative models in computer vision. Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate, leading to tremendous progress. However, there have been limited efforts to replace pixel-wise reconstruction, which have known shortcomings. In this work, we use real-valued non-volume preserving transformations (real NVP) to exactly compute the conditional likelihood of the data given the latent distribution. We show that a simple VAE with this form of reconstruction is competitive with complicated VAE structures, on image modeling tasks. As part of our model, we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers.



### Joint Network based Attention for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.05215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05215v1)
- **Published**: 2016-11-16 10:40:30+00:00
- **Updated**: 2016-11-16 10:40:30+00:00
- **Authors**: Yemin Shi, Yonghong Tian, Yaowei Wang, Tiejun Huang
- **Comment**: 8 pages, 5 figures, JNA
- **Journal**: None
- **Summary**: By extracting spatial and temporal characteristics in one network, the two-stream ConvNets can achieve the state-of-the-art performance in action recognition. However, such a framework typically suffers from the separately processing of spatial and temporal information between the two standalone streams and is hard to capture long-term temporal dependence of an action. More importantly, it is incapable of finding the salient portions of an action, say, the frames that are the most discriminative to identify the action. To address these problems, a \textbf{j}oint \textbf{n}etwork based \textbf{a}ttention (JNA) is proposed in this study. We find that the fully-connected fusion, branch selection and spatial attention mechanism are totally infeasible for action recognition. Thus in our joint network, the spatial and temporal branches share some information during the training stage. We also introduce an attention mechanism on the temporal domain to capture the long-term dependence meanwhile finding the salient portions. Extensive experiments are conducted on two benchmark datasets, UCF101 and HMDB51. Experimental results show that our method can improve the action recognition performance significantly and achieves the state-of-the-art results on both datasets.



### Learning long-term dependencies for action recognition with a biologically-inspired deep network
- **Arxiv ID**: http://arxiv.org/abs/1611.05216v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05216v3)
- **Published**: 2016-11-16 10:49:43+00:00
- **Updated**: 2017-03-19 08:27:24+00:00
- **Authors**: Yemin Shi, Yonghong Tian, Yaowei Wang, Tiejun Huang
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Despite a lot of research efforts devoted in recent years, how to efficiently learn long-term dependencies from sequences still remains a pretty challenging task. As one of the key models for sequence learning, recurrent neural network (RNN) and its variants such as long short term memory (LSTM) and gated recurrent unit (GRU) are still not powerful enough in practice. One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of both feedforward and feedback connections. To address this problem, this paper proposes a biologically-inspired deep network, called shuttleNet\footnote{Our code is available at \url{https://github.com/shiyemin/shuttlenet}}. Technologically, the shuttleNet consists of several processors, each of which is a GRU while associated with multiple groups of cells and states. Unlike traditional RNNs, all processors inside shuttleNet are loop connected to mimic the brain's feedforward and feedback connections, in which they are shared across multiple pathways in the loop connection. Attention mechanism is then employed to select the best information flow pathway. Extensive experiments conducted on two benchmark datasets (i.e UCF101 and HMDB51) show that we can beat state-of-the-art methods by simply embedding shuttleNet into a CNN-RNN framework.



### A Combinatorial Solution to Non-Rigid 3D Shape-to-Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1611.05241v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1611.05241v2)
- **Published**: 2016-11-16 12:08:13+00:00
- **Updated**: 2017-05-18 14:59:07+00:00
- **Authors**: Florian Bernard, Frank R. Schmidt, Johan Thunberg, Daniel Cremers
- **Comment**: 10 pages, 7 figures
- **Journal**: CVPR 2017
- **Summary**: We propose a combinatorial solution for the problem of non-rigidly matching a 3D shape to 3D image data. To this end, we model the shape as a triangular mesh and allow each triangle of this mesh to be rigidly transformed to achieve a suitable matching to the image. By penalising the distance and the relative rotation between neighbouring triangles our matching compromises between image and shape information. In this paper, we resolve two major challenges: Firstly, we address the resulting large and NP-hard combinatorial problem with a suitable graph-theoretic approach. Secondly, we propose an efficient discretisation of the unbounded 6-dimensional Lie group SE(3). To our knowledge this is the first combinatorial formulation for non-rigid 3D shape-to-image matching. In contrast to existing local (gradient descent) optimisation methods, we obtain solutions that do not require a good initialisation and that are within a bound of the optimal solution. We evaluate the proposed method on the two problems of non-rigid 3D shape-to-shape and non-rigid 3D shape-to-image registration and demonstrate that it provides promising results.



### Deep Transfer Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1611.05244v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05244v2)
- **Published**: 2016-11-16 12:14:09+00:00
- **Updated**: 2016-11-22 12:16:51+00:00
- **Authors**: Mengyue Geng, Yaowei Wang, Tao Xiang, Yonghong Tian
- **Comment**: 12 pages, 2 figures
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) poses a unique challenge to deep learning: how to learn a deep model with millions of parameters on a small training set of few or no labels. In this paper, a number of deep transfer learning models are proposed to address the data sparsity problem. First, a deep network architecture is designed which differs from existing deep Re-ID models in that (a) it is more suitable for transferring representations learned from large image classification datasets, and (b) classification loss and verification loss are combined, each of which adopts a different dropout strategy. Second, a two-stepped fine-tuning strategy is developed to transfer knowledge from auxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel unsupervised deep transfer learning model is developed based on co-training. The proposed models outperform the state-of-the-art deep Re-ID models by large margins: we achieve Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03, Market1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model (45.1\%) beats most supervised models.



### Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation
- **Arxiv ID**: http://arxiv.org/abs/1611.05250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05250v2)
- **Published**: 2016-11-16 12:25:08+00:00
- **Updated**: 2017-04-10 11:53:26+00:00
- **Authors**: Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes Totz, Zehan Wang, Wenzhe Shi
- **Comment**: Changes: * Uploaded Vid4 results (footnote 1). * Added references
  [14, 29] as spatial-transformer prior art. * Fixed typos
- **Journal**: None
- **Summary**: Convolutional neural networks have enabled accurate image super-resolution in real-time. However, recent attempts to benefit from temporal correlations in video super-resolution have been limited to naive or inefficient architectures. In this paper, we introduce spatio-temporal sub-pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real-time speed. Specifically, we discuss the use of early fusion, slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames. We also propose a novel joint motion compensation and video super-resolution algorithm that is orders of magnitude more efficient than competing methods, relying on a fast multi-resolution spatial transformer module that is end-to-end trainable. These contributions provide both higher accuracy and temporally more consistent videos, which we confirm qualitatively and quantitatively. Relative to single-frame models, spatio-temporal networks can either reduce the computational cost by 30% whilst maintaining the same quality or provide a 0.2dB gain for a similar computational cost. Results on publicly available datasets demonstrate that the proposed algorithms surpass current state-of-the-art performance in both accuracy and efficiency.



### Temporal Convolutional Networks for Action Segmentation and Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.05267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05267v1)
- **Published**: 2016-11-16 13:19:19+00:00
- **Updated**: 2016-11-16 13:19:19+00:00
- **Authors**: Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, Gregory D. Hager
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We introduce a new class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.



### DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification
- **Arxiv ID**: http://arxiv.org/abs/1611.05271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05271v1)
- **Published**: 2016-11-16 13:36:45+00:00
- **Updated**: 2016-11-16 13:36:45+00:00
- **Authors**: Shu Zhang, Ran He, Tieniu Tan
- **Comment**: 10pages, submitted to CVPR 17
- **Journal**: None
- **Summary**: MeshFace photos have been widely used in many Chinese business organizations to protect ID face photos from being misused. The occlusions incurred by random meshes severely degenerate the performance of face verification systems, which raises the MeshFace verification problem between MeshFace and daily photos. Previous methods cast this problem as a typical low-level vision problem, i.e. blind inpainting. They recover perceptually pleasing clear ID photos from MeshFaces by enforcing pixel level similarity between the recovered ID images and the ground-truth clear ID images and then perform face verification on them. Essentially, face verification is conducted on a compact feature space rather than the image pixel space. Therefore, this paper argues that pixel level similarity and feature level similarity jointly offer the key to improve the verification performance. Based on this insight, we offer a novel feature oriented blind face inpainting framework. Specifically, we implement this by establishing a novel DeMeshNet, which consists of three parts. The first part addresses blind inpainting of the MeshFaces by implicitly exploiting extra supervision from the occlusion position to enforce pixel level similarity. The second part explicitly enforces a feature level similarity in the compact feature space, which can explore informative supervision from the feature space to produce better inpainting results for verification. The last part copes with face alignment within the net via a customized spatial transformer module when extracting deep facial features. All the three parts are implemented within an end-to-end network that facilitates efficient optimization. Extensive experiments on two MeshFace datasets demonstrate the effectiveness of the proposed DeMeshNet as well as the insight of this paper.



### Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1611.05301v1
- **DOI**: 10.1016/j.cag.2017.12.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05301v1)
- **Published**: 2016-11-16 14:57:19+00:00
- **Updated**: 2016-11-16 14:57:19+00:00
- **Authors**: Tu Bui, Leonardo Ribeiro, Moacir Ponti, John Collomosse
- **Comment**: submitted to CVPR2017 on 15Nov16
- **Journal**: Computers & Graphics, vol 71, pages 77-87 (2017)
- **Summary**: We propose and evaluate several triplet CNN architectures for measuring the similarity between sketches and photographs, within the context of the sketch based image retrieval (SBIR) task. In contrast to recent fine-grained SBIR work, we study the ability of our networks to generalise across diverse object categories from limited training data, and explore in detail strategies for weight sharing, pre-processing, data augmentation and dimensionality reduction. We exceed the performance of pre-existing techniques on both the Flickr15k category level SBIR benchmark by $18\%$, and the TU-Berlin SBIR benchmark by $\sim10 \mathcal{T}_b$, when trained on the 250 category TU-Berlin classification dataset augmented with 25k corresponding photographs harvested from the Internet.



### Guidefill: GPU Accelerated, Artist Guided Geometric Inpainting for 3D Conversion
- **Arxiv ID**: http://arxiv.org/abs/1611.05319v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 68W10, 65M15
- **Links**: [PDF](http://arxiv.org/pdf/1611.05319v3)
- **Published**: 2016-11-16 15:30:58+00:00
- **Updated**: 2017-07-31 19:40:09+00:00
- **Authors**: L. Robert Hocking, Russell MacKenzie, Carola-Bibiane Schoenlieb
- **Comment**: None
- **Journal**: None
- **Summary**: The conversion of traditional film into stereo 3D has become an important problem in the past decade. One of the main bottlenecks is a disocclusion step, which in commercial 3D conversion is usually done by teams of artists armed with a toolbox of inpainting algorithms. A current difficulty in this is that most available algorithms are either too slow for interactive use, or provide no intuitive means for users to tweak the output. In this paper we present a new fast inpainting algorithm based on transporting along automatically detected splines, which the user may edit. Our algorithm is implemented on the GPU and fills the inpainting domain in successive shells that adapt their shape on the fly. In order to allocate GPU resources as efficiently as possible, we propose a parallel algorithm to track the inpainting interface as it evolves, ensuring that no resources are wasted on pixels that are not currently being worked on. Theoretical analysis of the time and processor complexiy of our algorithm without and with tracking (as well as numerous numerical experiments) demonstrate the merits of the latter. Our transport mechanism is similar to the one used in coherence transport, but improves upon it by corrected a "kinking" phenomena whereby extrapolated isophotes may bend at the boundary of the inpainting domain. Theoretical results explaining this phenomena and its resolution are presented. Although our method ignores texture, in many cases this is not a problem due to the thin inpainting domains in 3D conversion. Experimental results show that our method can achieve a visual quality that is competitive with the state-of-the-art while maintaining interactive speeds and providing the user with an intuitive interface to tweak the results.



### A Semi-supervised Framework for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1611.05321v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05321v3)
- **Published**: 2016-11-16 15:33:12+00:00
- **Updated**: 2017-06-24 08:24:44+00:00
- **Authors**: Wenhu Chen, Aurelien Lucchi, Thomas Hofmann
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art approaches for image captioning require supervised training data consisting of captions with paired image data. These methods are typically unable to use unsupervised data such as textual data with no corresponding images, which is a much more abundant commodity. We here propose a novel way of using such textual data by artificially generating missing visual information. We evaluate this learning approach on a newly designed model that detects visual concepts present in an image and feed them to a reviewer-decoder architecture with an attention mechanism. Unlike previous approaches that encode visual concepts using word embeddings, we instead suggest using regional image features which capture more intrinsic information. The main benefit of this architecture is that it synthesizes meaningful thought vectors that capture salient image properties and then applies a soft attentive decoder to decode the thought vectors and generate image captions. We evaluate our model on both Microsoft COCO and Flickr30K datasets and demonstrate that this model combined with our semi-supervised learning method can largely improve performance and help the model to generate more accurate and diverse captions.



### Image Credibility Analysis with Effective Domain Transferred Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.05328v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1611.05328v1)
- **Published**: 2016-11-16 15:45:19+00:00
- **Updated**: 2016-11-16 15:45:19+00:00
- **Authors**: Zhiwei Jin, Juan Cao, Jiebo Luo, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous fake images spread on social media today and can severely jeopardize the credibility of online content to public. In this paper, we employ deep networks to learn distinct fake image related features. In contrast to authentic images, fake images tend to be eye-catching and visually striking. Compared with traditional visual recognition tasks, it is extremely challenging to understand these psychologically triggered visual patterns in fake images. Traditional general image classification datasets, such as ImageNet set, are designed for feature learning at the object level but are not suitable for learning the hyper-features that would be required by image credibility analysis. In order to overcome the scarcity of training samples of fake images, we first construct a large-scale auxiliary dataset indirectly related to this task. This auxiliary dataset contains 0.6 million weakly-labeled fake and real images collected automatically from social media. Through an AdaBoost-like transfer learning algorithm, we train a CNN model with a few instances in the target training set and 0.6 million images in the collected auxiliary set. This learning algorithm is able to leverage knowledge from the auxiliary set and gradually transfer it to the target task. Experiments on a real-world testing set show that our proposed domain transferred CNN model outperforms several competing baselines. It obtains superiror results over transfer learning methods based on the general ImageNet set. Moreover, case studies show that our proposed method reveals some interesting patterns for distinguishing fake and authentic images.



### Unsupervised Learning of Important Objects from First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1611.05335v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05335v3)
- **Published**: 2016-11-16 15:53:12+00:00
- **Updated**: 2017-08-02 14:57:04+00:00
- **Authors**: Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: A first-person camera, placed at a person's head, captures, which objects are important to the camera wearer. Most prior methods for this task learn to detect such important objects from the manually labeled first-person data in a supervised fashion. However, important objects are strongly related to the camera wearer's internal state such as his intentions and attention, and thus, only the person wearing the camera can provide the importance labels. Such a constraint makes the annotation process costly and limited in scalability.   In this work, we show that we can detect important objects in first-person images without the supervision by the camera wearer or even third-person labelers. We formulate an important detection problem as an interplay between the 1) segmentation and 2) recognition agents. The segmentation agent first proposes a possible important object segmentation mask for each image, and then feeds it to the recognition agent, which learns to predict an important object mask using visual semantics and spatial features.   We implement such an interplay between both agents via an alternating cross-pathway supervision scheme inside our proposed Visual-Spatial Network (VSN). Our VSN consists of spatial ("where") and visual ("what") pathways, one of which learns common visual semantics while the other focuses on the spatial location cues. Our unsupervised learning is accomplished via a cross-pathway supervision, where one pathway feeds its predictions to a segmentation agent, which proposes a candidate important object segmentation mask that is then used by the other pathway as a supervisory signal. We show our method's success on two different important object datasets, where our method achieves similar or better results as the supervised methods.



### Backtracking Spatial Pyramid Pooling (SPP)-based Image Classifier for Weakly Supervised Top-down Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.05345v3
- **DOI**: 10.1109/TIP.2018.2864891
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05345v3)
- **Published**: 2016-11-16 16:23:51+00:00
- **Updated**: 2018-08-14 17:39:11+00:00
- **Authors**: Hisham Cholakkal, Jubin Johnson, Deepu Rajan
- **Comment**: 14 pages, 7 figures
- **Journal**: H. Cholakkal, J. Johnson, D. Rajan, "Backtracking Spatial Pyramid
  Pooling (SPP)-based Image Classifier for Weakly Supervised Top-down Salient
  Object Detection", in IEEE Transactions on Image processing, August 2018
- **Summary**: Top-down saliency models produce a probability map that peaks at target locations specified by a task/goal such as object detection. They are usually trained in a fully supervised setting involving pixel-level annotations of objects. We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence/absence of an object in an image. First, the probabilistic contribution of each image region to the confidence of a CNN-based image classifier is computed through a backtracking strategy to produce top-down saliency. From a set of saliency maps of an image produced by fast bottom-up saliency approaches, we select the best saliency map suitable for the top-down task. The selected bottom-up saliency map is combined with the top-down saliency map. Features having high combined saliency are used to train a linear SVM classifier to estimate feature saliency. This is integrated with combined saliency and further refined through a multi-scale superpixel-averaging of saliency map. We evaluate the performance of the proposed weakly supervised topdown saliency and achieve comparable performance with fully supervised approaches. Experiments are carried out on seven challenging datasets and quantitative results are compared with 40 closely related approaches across 4 different applications.



### Lip Reading Sentences in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1611.05358v2
- **DOI**: 10.1109/CVPR.2017.367
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05358v2)
- **Published**: 2016-11-16 16:53:46+00:00
- **Updated**: 2017-01-30 22:46:20+00:00
- **Authors**: Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos.   Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.   The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is available.



### Am I a Baller? Basketball Performance Assessment from First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1611.05365v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05365v4)
- **Published**: 2016-11-16 17:01:27+00:00
- **Updated**: 2017-08-02 15:10:27+00:00
- **Authors**: Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method to assess a basketball player's performance from his/her first-person video. A key challenge lies in the fact that the evaluation metric is highly subjective and specific to a particular evaluator. We leverage the first-person camera to address this challenge. The spatiotemporal visual semantics provided by a first-person view allows us to reason about the camera wearer's actions while he/she is participating in an unscripted basketball game. Our method takes a player's first-person video and provides a player's performance measure that is specific to an evaluator's preference.   To achieve this goal, we first use a convolutional LSTM network to detect atomic basketball events from first-person videos. Our network's ability to zoom-in to the salient regions addresses the issue of a severe camera wearer's head movement in first-person videos. The detected atomic events are then passed through the Gaussian mixtures to construct a highly non-linear visual spatiotemporal basketball assessment feature. Finally, we use this feature to learn a basketball assessment model from pairs of labeled first-person basketball videos, for which a basketball expert indicates, which of the two players is better.   We demonstrate that despite not knowing the basketball evaluator's criterion, our model learns to accurately assess the players in real-world games. Furthermore, our model can also discover basketball events that contribute positively and negatively to a player's performance.



### Neural Style Representations and the Large-Scale Classification of Artistic Style
- **Arxiv ID**: http://arxiv.org/abs/1611.05368v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1611.05368v1)
- **Published**: 2016-11-16 17:04:04+00:00
- **Updated**: 2016-11-16 17:04:04+00:00
- **Authors**: Jeremiah Johnson
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: Proceedings of the Future Technologies Conference, 2017
- **Summary**: The artistic style of a painting is a subtle aesthetic judgment used by art historians for grouping and classifying artwork. The recently introduced `neural-style' algorithm substantially succeeds in merging the perceived artistic style of one image or set of images with the perceived content of another. In light of this and other recent developments in image analysis via convolutional neural networks, we investigate the effectiveness of a `neural-style' representation for classifying the artistic style of paintings.



### Fast On-Line Kernel Density Estimation for Active Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1611.05369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.05369v1)
- **Published**: 2016-11-16 17:04:35+00:00
- **Updated**: 2016-11-16 17:04:35+00:00
- **Authors**: Anthony D. Rhodes, Max H. Quinn, Melanie Mitchell
- **Comment**: arXiv admin note: text overlap with arXiv:1607.00548
- **Journal**: None
- **Summary**: A major goal of computer vision is to enable computers to interpret visual situations---abstract concepts (e.g., "a person walking a dog," "a crowd waiting for a bus," "a picnic") whose image instantiations are linked more by their common spatial and semantic structure than by low-level visual similarity. In this paper, we propose a novel method for prior learning and active object localization for this kind of knowledge-driven search in static images. In our system, prior situation knowledge is captured by a set of flexible, kernel-based density estimations---a situation model---that represent the expected spatial structure of the given situation. These estimations are efficiently updated by information gained as the system searches for relevant objects, allowing the system to use context as it is discovered to narrow the search.   More specifically, at any given time in a run on a test image, our system uses image features plus contextual information it has discovered to identify a small subset of training images---an importance cluster---that is deemed most similar to the given test image, given the context. This subset is used to generate an updated situation model in an on-line fashion, using an efficient multipole expansion technique.   As a proof of concept, we apply our algorithm to a highly varied and challenging dataset consisting of instances of a "dog-walking" situation. Our results support the hypothesis that dynamically-rendered, context-based probability models can support efficient object localization in visual situations. Moreover, our approach is general enough to be applied to diverse machine learning paradigms requiring interpretable, probabilistic representations generated from partially observed data.



### Fully-adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/1611.05377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1611.05377v1)
- **Published**: 2016-11-16 17:31:44+00:00
- **Updated**: 2016-11-16 17:31:44+00:00
- **Authors**: Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by hand-designed network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space subject to designer's bias, which can be both error-prone and tedious. In this work, we propose a principled approach for designing compact multi-task deep learning architectures. Our approach starts with a thin network and dynamically widens it in a greedy manner during training using a novel criterion that promotes grouping of similar tasks together. Our Extensive evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-the-art accuracy from strong baselines by much more expensive models.



### Dynamic Attention-controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-set Sample Weighting
- **Arxiv ID**: http://arxiv.org/abs/1611.05396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05396v2)
- **Published**: 2016-11-16 18:18:07+00:00
- **Updated**: 2017-04-04 17:45:43+00:00
- **Authors**: Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber, Xiao-Jun Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting for attention-controlled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art.



### VisualBackProp: efficient visualization of CNNs
- **Arxiv ID**: http://arxiv.org/abs/1611.05418v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05418v3)
- **Published**: 2016-11-16 19:45:54+00:00
- **Updated**: 2017-05-19 23:50:46+00:00
- **Authors**: Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Larry Jackel, Urs Muller, Karol Zieba
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper into the network. The technique we propose was developed as a debugging tool for CNN-based systems for steering self-driving cars and is therefore required to run in real-time, i.e. it was designed to require less computations than a forward propagation. This makes the presented visualization method a valuable debugging tool which can be easily used during both training and inference. We furthermore justify our approach with theoretical arguments and theoretically confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively contribute to the prediction. Our theoretical findings stand in agreement with the experimental results. The empirical evaluation shows the plausibility of the proposed approach on the road video data as well as in other applications and reveals that it compares favorably to the layer-wise relevance propagation approach, i.e. it obtains similar visualization results and simultaneously achieves order of magnitude speed-ups.



### Associative Embedding: End-to-End Learning for Joint Detection and Grouping
- **Arxiv ID**: http://arxiv.org/abs/1611.05424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05424v2)
- **Published**: 2016-11-16 20:04:28+00:00
- **Updated**: 2017-06-09 16:13:48+00:00
- **Authors**: Alejandro Newell, Zhiao Huang, Jia Deng
- **Comment**: Added results on MS-COCO and updated results on MPII
- **Journal**: None
- **Summary**: We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to both multi-person pose estimation and instance segmentation and report state-of-the-art performance for multi-person pose on the MPII and MS-COCO datasets.



### Aggregated Residual Transformations for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1611.05431v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05431v2)
- **Published**: 2016-11-16 20:34:42+00:00
- **Updated**: 2017-04-11 01:53:41+00:00
- **Authors**: Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He
- **Comment**: Accepted to CVPR 2017. Code and models:
  https://github.com/facebookresearch/ResNeXt
- **Journal**: None
- **Summary**: We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.



### Convolutional Gated Recurrent Networks for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1611.05435v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05435v2)
- **Published**: 2016-11-16 20:46:38+00:00
- **Updated**: 2016-11-21 21:47:17+00:00
- **Authors**: Mennatullah Siam, Sepehr Valipour, Martin Jagersand, Nilanjan Ray
- **Comment**: arXiv admin note: text overlap with arXiv:1606.00487
- **Journal**: None
- **Summary**: Semantic segmentation has recently witnessed major progress, where fully convolutional neural networks have shown to perform well. However, most of the previous work focused on improving single image segmentation. To our knowledge, no prior work has made use of temporal video information in a recurrent network. In this paper, we introduce a novel approach to implicitly utilize temporal data in videos for online semantic segmentation. The method relies on a fully convolutional network that is embedded into a gated recurrent architecture. This design receives a sequence of consecutive video frames and outputs the segmentation of the last frame. Convolutional gated recurrent networks are used for the recurrent part to preserve spatial connectivities in the image. Our proposed method can be applied in both online and batch segmentation. This architecture is tested for both binary and semantic video segmentation tasks. Experiments are conducted on the recent benchmarks in SegTrack V2, Davis, CityScapes, and Synthia. Using recurrent fully convolutional networks improved the baseline network performance in all of our experiments. Namely, 5% and 3% improvement of F-measure in SegTrack2 and Davis respectively, 5.7% improvement in mean IoU in Synthia and 3.5% improvement in categorical mean IoU in CityScapes. The performance of the RFCN network depends on its baseline fully convolutional network. Thus RFCN architecture can be seen as a method to improve its baseline segmentation network by exploiting spatiotemporal information in videos.



### Self-calibration-based Approach to Critical Motion Sequences of Rolling-shutter Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1611.05476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05476v1)
- **Published**: 2016-11-16 21:50:11+00:00
- **Updated**: 2016-11-16 21:50:11+00:00
- **Authors**: Eisuke Ito, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we consider critical motion sequences (CMSs) of rolling-shutter (RS) SfM. Employing an RS camera model with linearized pure rotation, we show that the RS distortion can be approximately expressed by two internal parameters of an "imaginary" camera plus one-parameter nonlinear transformation similar to lens distortion. We then reformulate the problem as self-calibration of the imaginary camera, in which its skew and aspect ratio are unknown and varying in the image sequence. In the formulation, we derive a general representation of CMSs. We also show that our method can explain the CMS that was recently reported in the literature, and then present a new remedy to deal with the degeneracy. Our theoretical results agree well with experimental results; it explains degeneracies observed when we employ naive bundle adjustment, and how they are resolved by our method.



### Probabilistic Fluorescence-Based Synapse Detection
- **Arxiv ID**: http://arxiv.org/abs/1611.05479v1
- **DOI**: 10.1371/journal.pcbi.1005493
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1611.05479v1)
- **Published**: 2016-11-16 22:01:31+00:00
- **Updated**: 2016-11-16 22:01:31+00:00
- **Authors**: Anish K. Simhal, Cecilia Aguerrebere, Forrest Collman, Joshua T. Vogelstein, Kristina D. Micheva, Richard J. Weinberg, Stephen J. Smith, Guillermo Sapiro
- **Comment**: Current awaiting peer review
- **Journal**: None
- **Summary**: Brain function results from communication between neurons connected by complex synaptic networks. Synapses are themselves highly complex and diverse signaling machines, containing protein products of hundreds of different genes, some in hundreds of copies, arranged in precise lattice at each individual synapse. Synapses are fundamental not only to synaptic network function but also to network development, adaptation, and memory. In addition, abnormalities of synapse numbers or molecular components are implicated in most mental and neurological disorders. Despite their obvious importance, mammalian synapse populations have so far resisted detailed quantitative study. In human brains and most animal nervous systems, synapses are very small and very densely packed: there are approximately 1 billion synapses per cubic millimeter of human cortex. This volumetric density poses very substantial challenges to proteometric analysis at the critical level of the individual synapse. The present work describes new probabilistic image analysis methods for single-synapse analysis of synapse populations in both animal and human brains.



### Semantic Regularisation for Recurrent Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1611.05490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05490v1)
- **Published**: 2016-11-16 22:39:59+00:00
- **Updated**: 2016-11-16 22:39:59+00:00
- **Authors**: Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The "CNN-RNN" design pattern is increasingly widely applied in a variety of image annotation tasks including multi-label classification and captioning. Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This leaves the RNN overstretched with two jobs: predicting the visual concepts and modelling their correlations for generating structured annotation output. Importantly this makes the end-to-end training of the CNN and RNN slow and ineffective due to the difficulty of back propagating gradients through the RNN to train the CNN. We propose a simple modification to the design pattern that makes learning more effective and efficient. Specifically, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN. Regularising the interface can partially or completely decouple the learning problems, allowing each to be more effectively trained and jointly training much more efficient. Extensive experiments show that state-of-the art performance is achieved on multi-label classification as well as image captioning.



### On the Exploration of Convolutional Fusion Networks for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1611.05503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05503v1)
- **Published**: 2016-11-16 23:33:38+00:00
- **Updated**: 2016-11-16 23:33:38+00:00
- **Authors**: Yu Liu, Yanming Guo, Michael S. Lew
- **Comment**: 23rd International Conference on MultiMedia Modeling (MMM 2017)
- **Journal**: None
- **Summary**: Despite recent advances in multi-scale deep representations, their limitations are attributed to expensive parameters and weak fusion modules. Hence, we propose an efficient approach to fuse multi-scale deep representations, called convolutional fusion networks (CFN). Owing to using 1$\times$1 convolution and global average pooling, CFN can efficiently generate the side branches while adding few parameters. In addition, we present a locally-connected fusion module, which can learn adaptive weights for the side branches and form a discriminatively fused feature. CFN models trained on the CIFAR and ImageNet datasets demonstrate remarkable improvements over the plain CNNs. Furthermore, we generalize CFN to three new tasks, including scene recognition, fine-grained recognition and image retrieval. Our experiments show that it can obtain consistent improvements towards the transferring tasks.



### Deep Feature Interpolation for Image Content Changes
- **Arxiv ID**: http://arxiv.org/abs/1611.05507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1611.05507v2)
- **Published**: 2016-11-16 23:56:27+00:00
- **Updated**: 2017-06-19 17:20:31+00:00
- **Authors**: Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah Snavely, Kavita Bala, Kilian Weinberger
- **Comment**: First two authors contributed equally. Accepted by CVPR 2017. Code at
  https://github.com/paulu/deepfeatinterp
- **Journal**: None
- **Summary**: We propose Deep Feature Interpolation (DFI), a new data-driven baseline for automatic high-resolution image transformation. As the name suggests, it relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, DFI can perform high-level semantic transformations like "make older/younger", "make bespectacled", "add smile", among others, surprisingly well - sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as DFI requires no specialized network architecture or even any deep network to be trained for these tasks. DFI therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging in the rise of deep learning.




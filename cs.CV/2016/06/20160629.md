# Arxiv Papers in cs.CV on 2016-06-29
### Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections
- **Arxiv ID**: http://arxiv.org/abs/1606.08921v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08921v3)
- **Published**: 2016-06-29 00:32:28+00:00
- **Updated**: 2016-08-30 08:50:51+00:00
- **Authors**: Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang
- **Comment**: 17 pages. A journal extension of the version at arXiv:1603.09056
- **Journal**: None
- **Summary**: Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results.



### LCrowdV: Generating Labeled Videos for Simulation-based Crowd Behavior Learning
- **Arxiv ID**: http://arxiv.org/abs/1606.08998v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.08998v2)
- **Published**: 2016-06-29 08:30:44+00:00
- **Updated**: 2016-07-04 05:33:48+00:00
- **Authors**: Ernest Cheung, Tsan Kwong Wong, Aniket Bera, Xiaogang Wang, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel procedural framework to generate an arbitrary number of labeled crowd videos (LCrowdV). The resulting crowd video datasets are used to design accurate algorithms or training models for crowded scene understanding. Our overall approach is composed of two components: a procedural simulation framework for generating crowd movements and behaviors, and a procedural rendering framework to generate different videos or images. Each video or image is automatically labeled based on the environment, number of pedestrians, density, behavior, flow, lighting conditions, viewpoint, noise, etc. Furthermore, we can increase the realism by combining synthetically-generated behaviors with real-world background videos. We demonstrate the benefits of LCrowdV over prior lableled crowd datasets by improving the accuracy of pedestrian detection and crowd behavior classification algorithms. LCrowdV would be released on the WWW.



### De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1606.08999v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.08999v1)
- **Published**: 2016-06-29 08:33:28+00:00
- **Updated**: 2016-06-29 08:33:28+00:00
- **Authors**: Yin-Hsi Kuo, Winston H. Hsu
- **Comment**: Accepted for publication in IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)
- **Journal**: None
- **Summary**: Due to the prevalence of mobile devices, mobile search becomes a more convenient way than desktop search. Different from the traditional desktop search, mobile visual search needs more consideration for the limited resources on mobile devices (e.g., bandwidth, computing power, and memory consumption). The state-of-the-art approaches show that bag-of-words (BoW) model is robust for image and video retrieval; however, the large vocabulary tree might not be able to be loaded on the mobile device. We observe that recent works mainly focus on designing compact feature representations on mobile devices for bandwidth-limited network (e.g., 3G) and directly adopt feature matching on remote servers (cloud). However, the compact (binary) representation might fail to retrieve target objects (images, videos). Based on the hashed binary codes, we propose a de-hashing process that reconstructs BoW by leveraging the computing power of remote servers. To mitigate the information loss from binary codes, we further utilize contextual information (e.g., GPS) to reconstruct a context-aware BoW for better retrieval results. Experiment results show that the proposed method can achieve competitive retrieval accuracy as BoW while only transmitting few bits from mobile devices.



### Scene Text Detection via Holistic, Multi-Channel Prediction
- **Arxiv ID**: http://arxiv.org/abs/1606.09002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09002v2)
- **Published**: 2016-06-29 08:45:17+00:00
- **Updated**: 2016-07-05 11:22:49+00:00
- **Authors**: Cong Yao, Xiang Bai, Nong Sang, Xinyu Zhou, Shuchang Zhou, Zhimin Cao
- **Comment**: 10 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Recently, scene text detection has become an active research topic in computer vision and document analysis, because of its great importance and significant challenge. However, vast majority of the existing methods detect text within local regions, typically through extracting character, word or line level candidates followed by candidate aggregation and false positive elimination, which potentially exclude the effect of wide-scope and long-range contextual cues in the scene. To take full advantage of the rich information available in the whole natural image, we propose to localize text in a holistic manner, by casting scene text detection as a semantic segmentation problem. The proposed algorithm directly runs on full images and produces global, pixel-wise prediction maps, in which detections are subsequently formed. To better make use of the properties of text, three types of information regarding text region, individual characters and their relationship are estimated, with a single Fully Convolutional Network (FCN) model. With such predictions of text properties, the proposed algorithm can simultaneously handle horizontal, multi-oriented and curved text in real-world natural images. The experiments on standard benchmarks, including ICDAR 2013, ICDAR 2015 and MSRA-TD500, demonstrate that the proposed algorithm substantially outperforms previous state-of-the-art approaches. Moreover, we report the first baseline result on the recently-released, large-scale dataset COCO-Text.



### Geometry in Active Learning for Binary and Multi-class Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1606.09029v4
- **DOI**: 10.1016/j.cviu.2019.01.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09029v4)
- **Published**: 2016-06-29 10:17:32+00:00
- **Updated**: 2019-04-04 08:28:29+00:00
- **Authors**: Ksenia Konyushkova, Raphael Sznitman, Pascal Fua
- **Comment**: Extension of our previous paper arXiv:1508.04955
- **Journal**: Published in "Computer Vision and Image Understanding" journal,
  1077-3142, 2019
- **Summary**: We propose an active learning approach to image segmentation that exploits geometric priors to speed up and streamline the annotation process. It can be applied for both background-foreground and multi-class segmentation tasks in 2D images and 3D image volumes. Our approach combines geometric smoothness priors in the image space with more traditional uncertainty measures to estimate which pixels or voxels are the most informative, and thus should to be annotated next. For multi-class settings, we additionally introduce two novel criteria for uncertainty. In the 3D case, we use the resulting uncertainty measure to select voxels lying on a planar patch, which makes batch annotation much more convenient for the end user compared to the setting where voxels are randomly distributed in a volume. The planar patch is found using a branch-and-bound algorithm that looks for a 2D patch in a 3D volume where the most informative instances are located. We evaluate our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on regular images of horses and faces. We demonstrate a substantial performance increase over other approaches thanks to the use of geometric priors.



### Resolution- and throughput-enhanced spectroscopy using high-throughput computational slit
- **Arxiv ID**: http://arxiv.org/abs/1606.09072v2
- **DOI**: 10.1364/OL.41.004352
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1606.09072v2)
- **Published**: 2016-06-29 13:07:24+00:00
- **Updated**: 2016-09-10 04:20:52+00:00
- **Authors**: Farnoud Kazemzadeh, Alexander Wong
- **Comment**: 11 pages, 2 figures
- **Journal**: None
- **Summary**: There exists a fundamental tradeoff between spectral resolution and the efficiency or throughput for all optical spectrometers. The primary factors affecting the spectral resolution and throughput of an optical spectrometer are the size of the entrance aperture and the optical power of the focusing element. Thus far collective optimization of the above mentioned has proven difficult. Here, we introduce the concept of high-throughput computational slits (HTCS), a numerical technique for improving both the effective spectral resolution and efficiency of a spectrometer. The proposed HTCS approach was experimentally validated using an optical spectrometer configured with a 200 um entrance aperture, test, and a 50 um entrance aperture, control, demonstrating improvements in spectral resolution of the spectrum by ~ 50% over the control spectral resolution and improvements in efficiency of > 2 times over the efficiency of the largest entrance aperture used in the study while producing highly accurate spectra.



### A spectral-spatial fusion model for robust blood pulse waveform extraction in photoplethysmographic imaging
- **Arxiv ID**: http://arxiv.org/abs/1606.09118v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1606.09118v1)
- **Published**: 2016-06-29 14:31:09+00:00
- **Updated**: 2016-06-29 14:31:09+00:00
- **Authors**: Robert Amelard, David A Clausi, Alexander Wong
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Photoplethysmographic imaging is a camera-based solution for non-contact cardiovascular monitoring from a distance. This technology enables monitoring in situations where contact-based devices may be problematic or infeasible, such as ambulatory, sleep, and multi-individual monitoring. However, extracting the blood pulse waveform signal is challenging due to the unknown mixture of relevant (pulsatile) and irrelevant pixels in the scene. Here, we design and implement a signal fusion framework, FusionPPG, for extracting a blood pulse waveform signal with strong temporal fidelity from a scene without requiring anatomical priors (e.g., facial tracking). The extraction problem is posed as a Bayesian least squares fusion problem, and solved using a novel probabilistic pulsatility model that incorporates both physiologically derived spectral and spatial waveform priors to identify pulsatility characteristics in the scene. Experimental results show statistically significantly improvements compared to the FaceMeanPPG method ($p<0.001$) and DistancePPG ($p<0.001$) methods. Heart rates predicted using FusionPPG correlated strongly with ground truth measurements ($r^2=0.9952$). FusionPPG was the only method able to assess cardiac arrhythmia via temporal analysis.



### Optimising The Input Window Alignment in CD-DNN Based Phoneme Recognition for Low Latency Processing
- **Arxiv ID**: http://arxiv.org/abs/1606.09163v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.09163v1)
- **Published**: 2016-06-29 15:51:44+00:00
- **Updated**: 2016-06-29 15:51:44+00:00
- **Authors**: Akash Kumar Dhaka, Giampiero Salvi
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction.



### Object Boundary Detection and Classification with Image-level Labels
- **Arxiv ID**: http://arxiv.org/abs/1606.09187v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.09187v3)
- **Published**: 2016-06-29 17:08:58+00:00
- **Updated**: 2017-06-25 12:50:55+00:00
- **Authors**: Jing Yu Koh, Wojciech Samek, Klaus-Robert MÃ¼ller, Alexander Binder
- **Comment**: 12 pages, 2 figures, accepted for GCPR 2017 - 39th German Conference
  on Pattern Recognition
- **Journal**: None
- **Summary**: Semantic boundary and edge detection aims at simultaneously detecting object edge pixels in images and assigning class labels to them. Systematic training of predictors for this task requires the labeling of edges in images which is a particularly tedious task. We propose a novel strategy for solving this task, when pixel-level annotations are not available, performing it in an almost zero-shot manner by relying on conventional whole image neural net classifiers that were trained using large bounding boxes. Our method performs the following two steps at test time. Firstly it predicts the class labels by applying the trained whole image network to the test images. Secondly, it computes pixel-wise scores from the obtained predictions by applying backprop gradients as well as recent visualization algorithms such as deconvolution and layer-wise relevance propagation. We show that high pixel-wise scores are indicative for the location of semantic boundaries, which suggests that the semantic boundary problem can be approached without using edge labels during the training phase.



### Learning Concept Taxonomies from Multi-modal Data
- **Arxiv ID**: http://arxiv.org/abs/1606.09239v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1606.09239v1)
- **Published**: 2016-06-29 19:52:53+00:00
- **Updated**: 2016-06-29 19:52:53+00:00
- **Authors**: Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan, Eric P. Xing
- **Comment**: To appear in ACL 2016
- **Journal**: None
- **Summary**: We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.



### How smart does your profile image look? Estimating intelligence from social network profile images
- **Arxiv ID**: http://arxiv.org/abs/1606.09264v3
- **DOI**: 10.1145/3018661.3018663
- **Categories**: **cs.CV**, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1606.09264v3)
- **Published**: 2016-06-29 20:03:55+00:00
- **Updated**: 2016-12-11 21:16:17+00:00
- **Authors**: Xingjie Wei, David Stillwell
- **Comment**: None
- **Journal**: None
- **Summary**: Profile images on social networks are users' opportunity to present themselves and to affect how others judge them. We examine what Facebook images say about users' perceived and measured intelligence. 1,122 Facebook users completed a matrices intelligence test and shared their current Facebook profile image. Strangers also rated the images for perceived intelligence. We use automatically extracted image features to predict both measured and perceived intelligence. Intelligence estimation from images is a difficult task even for humans, but experimental results show that human accuracy can be equalled using computing methods. We report the image features that predict both measured and perceived intelligence, and highlight misleading features such as "smiling" and "wearing glasses" that are correlated with perceived but not measured intelligence. Our results give insights into inaccurate stereotyping from profile images and also have implications for privacy, especially since in most social networks profile images are public by default.



### Multiphase Segmentation For Simultaneously Homogeneous and Textural Images
- **Arxiv ID**: http://arxiv.org/abs/1606.09281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1606.09281v1)
- **Published**: 2016-06-29 20:52:10+00:00
- **Updated**: 2016-06-29 20:52:10+00:00
- **Authors**: Duy Hoang Thai, Lucas Mentch
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation remains an important problem in image processing. For homogeneous (piecewise smooth) images, a number of important models have been developed and refined over the past several decades. However, these models often fail when applied to the substantially larger class of natural images that simultaneously contain regions of both texture and homogeneity. This work introduces a bi-level constrained minimization model for simultaneous multiphase segmentation of images containing both homogeneous and textural regions. We develop novel norms defined in different functional Banach spaces for the segmentation which results in a non-convex minimization. Finally, we develop a generalized notion of segmentation delving into approximation theory and demonstrating that a more refined decomposition of these images results in multiple meaningful components. Both theoretical results and demonstrations on natural images are provided.



### Learning without Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1606.09282v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1606.09282v3)
- **Published**: 2016-06-29 20:54:04+00:00
- **Updated**: 2017-02-14 22:32:30+00:00
- **Authors**: Zhizhong Li, Derek Hoiem
- **Comment**: Conference version appears in ECCV 2016; updated with journal version
- **Journal**: None
- **Summary**: When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.




# Arxiv Papers in cs.CV on 2016-05-30
### Stochastic Function Norm Regularization of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.09085v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.09085v3)
- **Published**: 2016-05-30 01:49:18+00:00
- **Updated**: 2019-08-30 14:38:32+00:00
- **Authors**: Amal Rannen Triki, Matthew B. Blaschko
- **Comment**: arXiv admin note: text overlap with arXiv:1710.06703
- **Journal**: None
- **Summary**: Deep neural networks have had an enormous impact on image analysis. State-of-the-art training methods, based on weight decay and DropOut, result in impressive performance when a very large training set is available. However, they tend to have large problems overfitting to small data sets. Indeed, the available regularization methods deal with the complexity of the network function only indirectly. In this paper, we study the feasibility of directly using the $L_2$ function norm for regularization. Two methods to integrate this new regularization in the stochastic backpropagation are proposed. Moreover, the convergence of these new algorithms is studied. We finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets (MNIST and CIFAR10). The obtained results demonstrate very clear improvement, especially in the context of small sample regimes with data laying in a low dimensional manifold. Source code of the method can be found at \url{https://github.com/AmalRT/DNN_Reg}.



### Image segmentation based on the hybrid total variation model and the K-means clustering strategy
- **Arxiv ID**: http://arxiv.org/abs/1605.09116v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09116v1)
- **Published**: 2016-05-30 06:50:31+00:00
- **Updated**: 2016-05-30 06:50:31+00:00
- **Authors**: Baoli Shi, Zhi-Feng Pang, Jing Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of image segmentation highly relies on the original inputting image. When the image is contaminated by some noises or blurs, we can not obtain the efficient segmentation result by using direct segmentation methods. In order to efficiently segment the contaminated image, this paper proposes a two step method based on the hybrid total variation model with a box constraint and the K-means clustering method. In the first step, the hybrid model is based on the weighted convex combination between the total variation functional and the high-order total variation as the regularization term to obtain the original clustering data. In order to deal with non-smooth regularization term, we solve this model by employing the alternating split Bregman method. Then, in the second step, the segmentation can be obtained by thresholding this clustering data into different phases, where the thresholds can be given by using the K-means clustering method. Numerical comparisons show that our proposed model can provide more efficient segmentation results dealing with the noise image and blurring image.



### Control of Memory, Active Perception, and Action in Minecraft
- **Arxiv ID**: http://arxiv.org/abs/1605.09128v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.09128v1)
- **Published**: 2016-05-30 07:40:13+00:00
- **Updated**: 2016-05-30 07:40:13+00:00
- **Authors**: Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee
- **Comment**: ICML 2016
- **Journal**: None
- **Summary**: In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.



### Hyperspectral Image Classification with Support Vector Machines on Kernel Distribution Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1605.09136v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1605.09136v1)
- **Published**: 2016-05-30 08:26:28+00:00
- **Updated**: 2016-05-30 08:26:28+00:00
- **Authors**: Gianni Franchi, Jesus Angulo, Dino Sejdinovic
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for pixel classification in hyperspectral images, leveraging on both the spatial and spectral information in the data. The introduced method relies on a recently proposed framework for learning on distributions -- by representing them with mean elements in reproducing kernel Hilbert spaces (RKHS) and formulating a classification algorithm therein. In particular, we associate each pixel to an empirical distribution of its neighbouring pixels, a judicious representation of which in an RKHS, in conjunction with the spectral information contained in the pixel itself, give a new explicit set of features that can be fed into a suite of standard classification techniques -- we opt for a well-established framework of support vector machines (SVM). Furthermore, the computational complexity is reduced via random Fourier features formalism. We study the consistency and the convergence rates of the proposed method and the experiments demonstrate strong performance on hyperspectral data with gains in comparison to the state-of-the-art results.



### Going Deeper for Multilingual Visual Sentiment Detection
- **Arxiv ID**: http://arxiv.org/abs/1605.09211v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09211v1)
- **Published**: 2016-05-30 12:57:44+00:00
- **Updated**: 2016-05-30 12:57:44+00:00
- **Authors**: Brendan Jou, Shih-Fu Chang
- **Comment**: technical report, 7 pages
- **Journal**: None
- **Summary**: This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology (MVSO). The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release, adjective-noun pair (ANP) detectors were trained for the six languages using an AlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet, providing comparable or better performance with reduced network parameter cost.   In addition, since the image pool in MVSO can be corrupted by user noise from social interactions, we partitioned out a sub-corpus of MVSO images based on tag-restricted queries for higher fidelity labels. We show that as a result of these higher fidelity labels, higher performing AlexNet-styled ANP detectors can be trained using the tag-restricted image subset as compared to the models in full corpus. We release all these newly trained models for public research use along with the list of tag-restricted images from the MVSO dataset.



### k2-means for fast and accurate large scale clustering
- **Arxiv ID**: http://arxiv.org/abs/1605.09299v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09299v1)
- **Published**: 2016-05-30 16:17:45+00:00
- **Updated**: 2016-05-30 16:17:45+00:00
- **Authors**: Eirikur Agustsson, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: We propose k^2-means, a new clustering method which efficiently copes with large numbers of clusters and achieves low energy solutions. k^2-means builds upon the standard k-means (Lloyd's algorithm) and combines a new strategy to accelerate the convergence with a new low time complexity divisive initialization. The accelerated convergence is achieved through only looking at k_n nearest clusters and using triangle inequality bounds in the assignment step while the divisive initialization employs an optimal 2-clustering along a direction. The worst-case time complexity per iteration of our k^2-means is O(nk_nd+k^2d), where d is the dimension of the n data points and k is the number of clusters and usually n << k << k_n. Compared to k-means' O(nkd) complexity, our k^2-means complexity is significantly lower, at the expense of slightly increasing the memory complexity by O(nk_n+k^2). In our extensive experiments k^2-means is order(s) of magnitude faster than standard methods in computing accurate clusterings on several standard datasets and settings with hundreds of clusters and high dimensional data. Moreover, the proposed divisive initialization generally leads to clustering energies comparable to those achieved with the standard k-means++ initialization, while being significantly faster.



### Synthesizing the preferred inputs for neurons in neural networks via deep generator networks
- **Arxiv ID**: http://arxiv.org/abs/1605.09304v5
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1605.09304v5)
- **Published**: 2016-05-30 16:22:54+00:00
- **Updated**: 2016-11-23 18:41:12+00:00
- **Authors**: Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune
- **Comment**: 29 pages, 35 figures, NIPS camera-ready
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).



### Parametric Exponential Linear Unit for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1605.09332v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1605.09332v4)
- **Published**: 2016-05-30 17:16:40+00:00
- **Updated**: 2018-01-10 15:18:48+00:00
- **Authors**: Ludovic Trottier, Philippe GiguÃ¨re, Brahim Chaib-draa
- **Comment**: 16th IEEE International Conference On Machine Learning And
  Applications, 2017
- **Journal**: None
- **Summary**: Object recognition is an important task for improving the ability of visual systems to perform complex scene understanding. Recently, the Exponential Linear Unit (ELU) has been proposed as a key component for managing bias shift in Convolutional Neural Networks (CNNs), but defines a parameter that must be set by hand. In this paper, we propose learning a parameterization of ELU in order to learn the proper activation shape at each layer in the CNNs. Our results on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN, Overfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU (PELU) has better performances than the non-parametric ELU. We have observed as much as a 7.28% relative error improvement on ImageNet with the NiN network, with only 0.0003% parameter increase. Our visual examination of the non-linear behaviors adopted by Vgg using PELU shows that the network took advantage of the added flexibility by learning different activations at different layers.



### Learning the image processing pipeline
- **Arxiv ID**: http://arxiv.org/abs/1605.09336v1
- **DOI**: 10.1109/TIP.2017.2713942
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1605.09336v1)
- **Published**: 2016-05-30 17:28:02+00:00
- **Updated**: 2016-05-30 17:28:02+00:00
- **Authors**: Haomiao Jiang, Qiyuan Tian, Joyce Farrell, Brian Wandell
- **Comment**: None
- **Journal**: None
- **Summary**: Many creative ideas are being proposed for image sensor designs, and these may be useful in applications ranging from consumer photography to computer vision. To understand and evaluate each new design, we must create a corresponding image processing pipeline that transforms the sensor data into a form that is appropriate for the application. The need to design and optimize these pipelines is time-consuming and costly. We explain a method that combines machine learning and image systems simulation that automates the pipeline design. The approach is based on a new way of thinking of the image processing pipeline as a large collection of local linear filters. We illustrate how the method has been used to design pipelines for novel sensor architectures in consumer photography applications.



### End-to-End Instance Segmentation with Recurrent Attention
- **Arxiv ID**: http://arxiv.org/abs/1605.09410v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1605.09410v5)
- **Published**: 2016-05-30 20:40:20+00:00
- **Updated**: 2017-07-13 00:53:33+00:00
- **Authors**: Mengye Ren, Richard S. Zemel
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves competitive results on the CVPPP, KITTI, and Cityscapes datasets.



### Blind Modulation Classification based on MLP and PNN
- **Arxiv ID**: http://arxiv.org/abs/1605.09441v1
- **DOI**: 10.1109/SCES.2012.6199042
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1605.09441v1)
- **Published**: 2016-05-30 23:00:32+00:00
- **Updated**: 2016-05-30 23:00:32+00:00
- **Authors**: Harishchandra Dubey, Nandita, Ashutosh Kumar Tiwari
- **Comment**: 6 Pages, 12 Figures, 12 Tables
- **Journal**: Harishchandra Dubey, Nandita and A. K. Tiwari, "Blind modulation
  classification based on MLP and PNN," Engineering and Systems (SCES), 2012
  Students Conference on, llahabad, Uttar Pradesh, 2012, pp. 1-6
- **Summary**: In this work, a pattern recognition system is investigated for blind automatic classification of digitally modulated communication signals. The proposed technique is able to discriminate the type of modulation scheme which is eventually used for demodulation followed by information extraction. The proposed system is composed of two subsystems namely feature extraction sub-system (FESS) and classifier sub-system (CSS). The FESS consists of continuous wavelet transform (CWT) for feature generation and principal component analysis (PCA) for selection of the feature subset which is rich in discriminatory information. The CSS uses the selected features to accurately classify the modulation class of the received signal. The proposed technique uses probabilistic neural network (PNN) and multilayer perceptron forward neural network (MLPFN) for comparative study of their recognition ability. PNN have been found to perform better in terms of classification accuracy as well as testing and training time than MLPFN. The proposed approach is robust to presence of phase offset and additive Gaussian noise.




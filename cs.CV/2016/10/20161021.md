# Arxiv Papers in cs.CV on 2016-10-21
### Short-term prediction of localized cloud motion using ground-based sky imagers
- **Arxiv ID**: http://arxiv.org/abs/1610.06666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06666v1)
- **Published**: 2016-10-21 04:28:35+00:00
- **Updated**: 2016-10-21 04:28:35+00:00
- **Authors**: Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler
- **Comment**: Accepted in Proc. TENCON 2016 - 2016 IEEE Region 10 Conference
- **Journal**: None
- **Summary**: Fine-scale short-term cloud motion prediction is needed for several applications, including solar energy generation and satellite communications. In tropical regions such as Singapore, clouds are mostly formed by convection; they are very localized, and evolve quickly. We capture hemispherical images of the sky at regular intervals of time using ground-based cameras. They provide a high resolution and localized cloud images. We use two successive frames to compute optical flow and predict the future location of clouds. We achieve good prediction accuracy for a lead time of up to 5 minutes.



### Detecting Rainfall Onset Using Sky Images
- **Arxiv ID**: http://arxiv.org/abs/1610.06667v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/1610.06667v1)
- **Published**: 2016-10-21 04:31:03+00:00
- **Updated**: 2016-10-21 04:31:03+00:00
- **Authors**: Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, Stefan Winkler
- **Comment**: Accepted in Proc. TENCON 2016 - 2016 IEEE Region 10 Conference
- **Journal**: None
- **Summary**: Ground-based sky cameras (popularly known as Whole Sky Imagers) are increasingly used now-a-days for continuous monitoring of the atmosphere. These imagers have higher temporal and spatial resolutions compared to conventional satellite images. In this paper, we use ground-based sky cameras to detect the onset of rainfall. These images contain additional information about cloud coverage and movement and are therefore useful for accurate rainfall nowcast. We validate our results using rain gauge measurement recordings and achieve an accuracy of 89% for correct detection of rainfall onset.



### Scalable Pooled Time Series of Big Video Data from the Deep Web
- **Arxiv ID**: http://arxiv.org/abs/1610.06669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06669v1)
- **Published**: 2016-10-21 04:43:52+00:00
- **Updated**: 2016-10-21 04:43:52+00:00
- **Authors**: Chris Mattmann, Madhav Sharan
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: We contribute a scalable implementation of Ryoo et al's Pooled Time Series algorithm from CVPR 2015. The updated algorithm has been evaluated on a large and diverse dataset of approximately 6800 videos collected from a crawl of the deep web related to human trafficking on DARPA's MEMEX effort. We describe the properties of Pooled Time Series and the motivation for using it to relate videos collected from the deep web. We highlight issues that we found while running Pooled Time Series on larger datasets and discuss solutions for those issues. Our solution centers are re-imagining Pooled Time Series as a Hadoop-based algorithm in which we compute portions of the eventual solution in parallel on large commodity clusters. We demonstrate that our new Hadoop-based algorithm works well on the 6800 video dataset and shares all of the properties described in the CVPR 2015 paper. We suggest avenues of future work in the project.



### Multi-view metric learning for multi-instance image classification
- **Arxiv ID**: http://arxiv.org/abs/1610.06671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06671v1)
- **Published**: 2016-10-21 04:46:53+00:00
- **Updated**: 2016-10-21 04:46:53+00:00
- **Authors**: Dewei Li, Yingjie Tian
- **Comment**: None
- **Journal**: None
- **Summary**: It is critical and meaningful to make image classification since it can help human in image retrieval and recognition, object detection, etc. In this paper, three-sides efforts are made to accomplish the task. First, visual features with bag-of-words representation, not single vector, are extracted to characterize the image. To improve the performance, the idea of multi-view learning is implemented and three kinds of features are provided, each one corresponds to a single view. The information from three views is complementary to each other, which can be unified together. Then a new distance function is designed for bags by computing the weighted sum of the distances between instances. The technique of metric learning is explored to construct a data-dependent distance metric to measure the relationships between instances, meanwhile between bags and images, more accurately. Last, a novel approach, called MVML, is proposed, which optimizes the joint probability that every image is similar with its nearest image. MVML learns multiple distance metrics, each one models a single view, to unifies the information from multiple views. The method can be solved by alternate optimization iteratively. Gradient ascent and positive semi-definite projection are utilized in the iterations. Distance comparisons verified that the new bag distance function is prior to previous functions. In model evaluation, numerical experiments show that MVML with multiple views performs better than single view condition, which demonstrates that our model can assemble the complementary information efficiently and measure the distance between images more precisely. Experiments on influence of parameters and instance number validate the consistency of the method.



### Multispectral image denoising with optimized vector non-local mean filter
- **Arxiv ID**: http://arxiv.org/abs/1610.06688v1
- **DOI**: 10.1016/j.dsp.2016.07.017
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1610.06688v1)
- **Published**: 2016-10-21 07:34:57+00:00
- **Updated**: 2016-10-21 07:34:57+00:00
- **Authors**: Ahmed Ben Said, Rachid Hadjidj, Kamel Eddine Melkemi, Sebti Foufou
- **Comment**: 30 pages, 17 figures, journal paper
- **Journal**: None
- **Summary**: Nowadays, many applications rely on images of high quality to ensure good performance in conducting their tasks. However, noise goes against this objective as it is an unavoidable issue in most applications. Therefore, it is essential to develop techniques to attenuate the impact of noise, while maintaining the integrity of relevant information in images. We propose in this work to extend the application of the Non-Local Means filter (NLM) to the vector case and apply it for denoising multispectral images. The objective is to benefit from the additional information brought by multispectral imaging systems. The NLM filter exploits the redundancy of information in an image to remove noise. A restored pixel is a weighted average of all pixels in the image. In our contribution, we propose an optimization framework where we dynamically fine tune the NLM filter parameters and attenuate its computational complexity by considering only pixels which are most similar to each other in computing a restored pixel. Filter parameters are optimized using Stein's Unbiased Risk Estimator (SURE) rather than using ad hoc means. Experiments have been conducted on multispectral images corrupted with additive white Gaussian noise and PSNR and similarity comparison with other approaches are provided to illustrate the efficiency of our approach in terms of both denoising performance and computation complexity.



### Model-based Outdoor Performance Capture
- **Arxiv ID**: http://arxiv.org/abs/1610.06740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06740v1)
- **Published**: 2016-10-21 11:06:49+00:00
- **Updated**: 2016-10-21 11:06:49+00:00
- **Authors**: Nadia Robertini, Dan Casas, Helge Rhodin, Hans-Peter Seidel, Christian Theobalt
- **Comment**: 3DV 2016
- **Journal**: None
- **Summary**: We propose a new model-based method to accurately reconstruct human performances captured outdoors in a multi-camera setup. Starting from a template of the actor model, we introduce a new unified implicit representation for both, articulated skeleton tracking and nonrigid surface shape refinement. Our method fits the template to unsegmented video frames in two stages - first, the coarse skeletal pose is estimated, and subsequently non-rigid surface shape and body pose are jointly refined. Particularly for surface shape refinement we propose a new combination of 3D Gaussians designed to align the projected model with likely silhouette contours without explicit segmentation or edge detection. We obtain reconstructions of much higher quality in outdoor settings than existing methods, and show that we are on par with state-of-the-art methods on indoor scenes for which they were designed



### Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of Convolutional Neural Networks Approaches
- **Arxiv ID**: http://arxiv.org/abs/1610.06756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06756v1)
- **Published**: 2016-10-21 12:14:50+00:00
- **Updated**: 2016-10-21 12:14:50+00:00
- **Authors**: Erik Rodner, Marcel Simon, Robert B. Fisher, Joachim Denzler
- **Comment**: BMVC 2016 Paper
- **Journal**: None
- **Summary**: In this paper, we study the sensitivity of CNN outputs with respect to image transformations and noise in the area of fine-grained recognition. In particular, we answer the following questions (1) how sensitive are CNNs with respect to image transformations encountered during wild image capture?; (2) how can we predict CNN sensitivity?; and (3) can we increase the robustness of CNNs with respect to image degradations? To answer the first question, we provide an extensive empirical sensitivity analysis of commonly used CNN architectures (AlexNet, VGG19, GoogleNet) across various types of image degradations. This allows for predicting CNN performance for new domains comprised by images of lower quality or captured from a different viewpoint. We also show how the sensitivity of CNN outputs can be predicted for single images. Furthermore, we demonstrate that input layer dropout or pre-filtering during test time only reduces CNN sensitivity for high levels of degradation.   Experiments for fine-grained recognition tasks reveal that VGG19 is more robust to severe image degradations than AlexNet and GoogleNet. However, small intensity noise can lead to dramatic changes in CNN performance even for VGG19.



### Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies
- **Arxiv ID**: http://arxiv.org/abs/1610.06781v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1610.06781v4)
- **Published**: 2016-10-21 13:36:25+00:00
- **Updated**: 2017-12-19 04:59:03+00:00
- **Authors**: Fangyi Zhang, JÃ¼rgen Leitner, Michael Milford, Peter Corke
- **Comment**: Australasian Conference on Robotics and Automation (ACRA) 2017,
  Student Paper Award Finalist
- **Journal**: The proceedings of the Australasian Conference on Robotics and
  Automation (ACRA) 2017
- **Summary**: While deep learning has had significant successes in computer vision thanks to the abundance of visual data, collecting sufficiently large real-world datasets for robot learning can be costly. To increase the practicality of these techniques on real robots, we propose a modular deep reinforcement learning method capable of transferring models trained in simulation to a real-world robotic task. We introduce a bottleneck between perception and control, enabling the networks to be trained independently, but then merged and fine-tuned in an end-to-end manner to further improve hand-eye coordination. On a canonical, planar visually-guided robot reaching task a fine-tuned accuracy of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 pixels), showing the potential for more complicated and broader applications. Our method provides a technique for more efficient learning and transfer of visuo-motor policies for real robotic systems without relying entirely on large real-world robot datasets.



### Deep Models for Engagement Assessment With Scarce Label Information
- **Arxiv ID**: http://arxiv.org/abs/1610.06815v1
- **DOI**: 10.1109/THMS.2016.2608933
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06815v1)
- **Published**: 2016-10-21 15:05:16+00:00
- **Updated**: 2016-10-21 15:05:16+00:00
- **Authors**: Feng Li, Guangfan Zhang, Wei Wang, Roger Xu, Tom Schnell, Jonathan Wen, Frederic McKenzie, Jiang Li
- **Comment**: 9 pages, 8 figures and 3 tables
- **Journal**: None
- **Summary**: Task engagement is defined as loadings on energetic arousal (affect), task motivation, and concentration (cognition). It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overfitting. In this paper, we proposed two deep models (i.e., a deep classifier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were fine-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training.



### Exploitation of Semantic Keywords for Malicious Event Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.06903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06903v2)
- **Published**: 2016-10-21 19:33:46+00:00
- **Updated**: 2017-12-01 15:29:39+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Joel Levis, Heesung Kwon, James Michaelis, Michael Kolodny
- **Comment**: None
- **Journal**: None
- **Summary**: Learning an event classifier is challenging when the scenes are semantically different but visually similar. However, as humans, we typically handle such tasks painlessly by adding our background semantic knowledge. Motivated by this observation, we aim to provide an empirical study about how additional information such as semantic keywords can boost up the discrimination of such events. To demonstrate the validity of this study, we first construct a novel Malicious Crowd Dataset containing crowd images with two events, benign and malicious, which look visually similar. Note that the primary focus of this paper is not to provide the state-of-the-art performance on this dataset but to show the beneficial aspects of using semantically-driven keyword information. By leveraging crowd-sourcing platforms, such as Amazon Mechanical Turk, we collect semantic keywords associated with images and then subsequently identify a subset of keywords (e.g. police, fire, etc.) unique to specific events. We first show that by using recently introduced attention models, a naive CNN-based event classifier actually learns to primarily focus on local attributes associated with the discriminant semantic keywords identified by the Turks. We further show that incorporating the keyword-driven information into early- and late-fusion approaches can significantly enhance malicious event classification.



### Review of Action Recognition and Detection Methods
- **Arxiv ID**: http://arxiv.org/abs/1610.06906v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06906v2)
- **Published**: 2016-10-21 19:36:49+00:00
- **Updated**: 2016-11-01 16:06:47+00:00
- **Authors**: Soo Min Kang, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, action recognition refers to the act of classifying an action that is present in a given video and action detection involves locating actions of interest in space and/or time. Videos, which contain photometric information (e.g. RGB, intensity values) in a lattice structure, contain information that can assist in identifying the action that has been imaged. The process of action recognition and detection often begins with extracting useful features and encoding them to ensure that the features are specific to serve the task of action recognition and detection. Encoded features are then processed through a classifier to identify the action class and their spatial and/or temporal locations. In this report, a thorough review of various action recognition and detection algorithms in computer vision is provided by analyzing the two-step process of a typical action recognition and detection algorithm: (i) extraction and encoding of features, and (ii) classifying features into action classes. In efforts to ensure that computer vision-based algorithms reach the capabilities that humans have of identifying actions irrespective of various nuisance variables that may be present within the field of view, the state-of-the-art methods are reviewed and some remaining problems are addressed in the final chapter.



### Enhanced Object Detection via Fusion With Prior Beliefs from Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1610.06907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06907v1)
- **Published**: 2016-10-21 19:38:45+00:00
- **Updated**: 2016-10-21 19:38:45+00:00
- **Authors**: Yilun Cao, Hyungtae Lee, Heesung Kwon
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a novel fusion method that can enhance object detection performance by fusing decisions from two different types of computer vision tasks: object detection and image classification. In the proposed work, the class label of an image obtained from image classification is viewed as prior knowledge about existence or non-existence of certain objects. The prior knowledge is then fused with the decisions of object detection to improve detection accuracy by mitigating false positives of an object detector that are strongly contradicted with the prior knowledge. A recently introduced novel fusion approach called dynamic belief fusion (DBF) is used to fuse the detector output with the classification prior. Experimental results show that the detection performance of all the detection algorithms used in the proposed work is improved on benchmark datasets via the proposed fusion framework.



### Automatic Image De-fencing System
- **Arxiv ID**: http://arxiv.org/abs/1610.06924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.06924v1)
- **Published**: 2016-10-21 19:59:41+00:00
- **Updated**: 2016-10-21 19:59:41+00:00
- **Authors**: Krishna Kanth Nakka
- **Comment**: Master Thesis, EE IIT KGP, May 2015. arXiv admin note: text overlap
  with arXiv:1405.3531 by other authors
- **Journal**: None
- **Summary**: Tourists and Wild-life photographers are often hindered in capturing their cherished images or videos by a fence that limits accessibility to the scene of interest. The situation has been exacerbated by growing concerns of security at public places and a need exists to provide a tool that can be used for post-processing such fenced videos to produce a de-fenced image. There are several challenges in this problem, we identify them as Robust detection of fence/occlusions and Estimating pixel motion of background scenes and Filling in the fence/occlusions by utilizing information in multiple frames of the input video. In this work, we aim to build an automatic post-processing tool that can efficiently rid the input video of occlusion artifacts like fences. Our work is distinguished by two major contributions. The first is the introduction of learning based technique to detect the fences patterns with complicated backgrounds. The second is the formulation of objective function and further minimization through loopy belief propagation to fill-in the fence pixels. We observe that grids of Histogram of oriented gradients descriptor using Support vector machines based classifier significantly outperforms detection accuracy of texels in a lattice. We present results of experiments using several real-world videos to demonstrate the effectiveness of the proposed fence detection and de-fencing algorithm.




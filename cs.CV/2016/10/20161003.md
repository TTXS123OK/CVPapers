# Arxiv Papers in cs.CV on 2016-10-03
### Near-Infrared Coloring via a Contrast-Preserving Mapping Model
- **Arxiv ID**: http://arxiv.org/abs/1610.00382v1
- **DOI**: 10.1109/TIP.2017.2724241
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00382v1)
- **Published**: 2016-10-03 01:08:20+00:00
- **Updated**: 2016-10-03 01:08:20+00:00
- **Authors**: Chang-Hwan Son, Xiao-Ping Zhang
- **Comment**: 12 pages
- **Journal**: IEEE Transactions on Image Processing, vol. 26, no. 11, pp.
  5381-5394, Nov. 2017
- **Summary**: Near-infrared gray images captured together with corresponding visible color images have recently proven useful for image restoration and classification. This paper introduces a new coloring method to add colors to near-infrared gray images based on a contrast-preserving mapping model. A naive coloring method directly adds the colors from the visible color image to the near-infrared gray image; however, this method results in an unrealistic image because of the discrepancies in brightness and image structure between the captured near-infrared gray image and the visible color image. To solve the discrepancy problem, first we present a new contrast-preserving mapping model to create a new near-infrared gray image with a similar appearance in the luminance plane to the visible color image, while preserving the contrast and details of the captured near-infrared gray image. Then based on the proposed contrast-preserving mapping model, we develop a method to derive realistic colors that can be added to the newly created near-infrared gray image. Experimental results show that the proposed method can not only preserve the local contrasts and details of the captured near-infrared gray image, but transfers the realistic colors from the visible color image to the newly created near-infrared gray image. Experimental results also show that the proposed approach can be applied to near-infrared denoising.



### Rain Removal via Shrinkage-Based Sparse Coding and Learned Rain Dictionary
- **Arxiv ID**: http://arxiv.org/abs/1610.00386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00386v1)
- **Published**: 2016-10-03 02:03:06+00:00
- **Updated**: 2016-10-03 02:03:06+00:00
- **Authors**: Chang-Hwan Son, Xiao-Ping Zhang
- **Comment**: 17 pages
- **Journal**: Journal of Imaging Science and Technology, vol. 64, no. 3, pp.
  30501-1-30501-17(17), May 2020
- **Summary**: This paper introduces a new rain removal model based on the shrinkage of the sparse codes for a single image. Recently, dictionary learning and sparse coding have been widely used for image restoration problems. These methods can also be applied to the rain removal by learning two types of rain and non-rain dictionaries and forcing the sparse codes of the rain dictionary to be zero vectors. However, this approach can generate unwanted edge artifacts and detail loss in the non-rain regions. Based on this observation, a new approach for shrinking the sparse codes is presented in this paper. To effectively shrink the sparse codes in the rain and non-rain regions, an error map between the input rain image and the reconstructed rain image is generated by using the learned rain dictionary. Based on this error map, both the sparse codes of rain and non-rain dictionaries are used jointly to represent the image structures of objects and avoid the edge artifacts in the non-rain regions. In the rain regions, the correlation matrix between the rain and non-rain dictionaries is calculated. Then, the sparse codes corresponding to the highly correlated signal-atoms in the rain and non-rain dictionaries are shrunk jointly to improve the removal of the rain structures. The experimental results show that the proposed shrinkage-based sparse coding can preserve image structures and avoid the edge artifacts in the non-rain regions, and it can remove the rain structures in the rain regions. Also, visual quality evaluation confirms that the proposed method outperforms the conventional texture and rain removal methods.



### Seeing into Darkness: Scotopic Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1610.00405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00405v1)
- **Published**: 2016-10-03 05:03:45+00:00
- **Updated**: 2016-10-03 05:03:45+00:00
- **Authors**: Bo Chen, Pietro Perona
- **Comment**: 23 pages, 6 figures
- **Journal**: None
- **Summary**: Images are formed by counting how many photons traveling from a given set of directions hit an image sensor during a given time interval. When photons are few and far in between, the concept of `image' breaks down and it is best to consider directly the flow of photons. Computer vision in this regime, which we call `scotopic', is radically different from the classical image-based paradigm in that visual computations (classification, control, search) have to take place while the stream of photons is captured and decisions may be taken as soon as enough information is available. The scotopic regime is important for biomedical imaging, security, astronomy and many other fields. Here we develop a framework that allows a machine to classify objects with as few photons as possible, while maintaining the error rate below an acceptable threshold. A dynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of this framework. We propose and study an algorithm to optimize the tradeoff of a convolutional network directly from lowlight images and evaluate on simulated images from standard datasets. Surprisingly, scotopic systems can achieve comparable classification performance as traditional vision systems while using less than 0.1% of the photons in a conventional image. In addition, we demonstrate that our algorithms work even when the illuminance of the environment is unknown and varying. Last, we outline a spiking neural network coupled with photon-counting sensors as a power-efficient hardware realization of scotopic algorithms.



### On the Empirical Effect of Gaussian Noise in Under-sampled MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1610.00410v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1610.00410v1)
- **Published**: 2016-10-03 05:28:06+00:00
- **Updated**: 2016-10-03 05:28:06+00:00
- **Authors**: Patrick Virtue, Michael Lustig
- **Comment**: 24 pages, 7 figures
- **Journal**: None
- **Summary**: In Fourier-based medical imaging, sampling below the Nyquist rate results in an underdetermined system, in which linear reconstructions will exhibit artifacts. Another consequence of under-sampling is lower signal to noise ratio (SNR) due to fewer acquired measurements. Even if an oracle provided the information to perfectly disambiguate the underdetermined system, the reconstructed image could still have lower image quality than a corresponding fully sampled acquisition because of the reduced measurement time. The effects of lower SNR and the underdetermined system are coupled during reconstruction, making it difficult to isolate the impact of lower SNR on image quality. To this end, we present an image quality prediction process that reconstructs fully sampled, fully determined data with noise added to simulate the loss of SNR induced by a given under-sampling pattern. The resulting prediction image empirically shows the effect of noise in under-sampled image reconstruction without any effect from an underdetermined system.   We discuss how our image quality prediction process can simulate the distribution of noise for a given under-sampling pattern, including variable density sampling that produces colored noise in the measurement data. An interesting consequence of our prediction model is that we can show that recovery from underdetermined non-uniform sampling is equivalent to a weighted least squares optimization that accounts for heterogeneous noise levels across measurements.   Through a series of experiments with synthetic and in vivo datasets, we demonstrate the efficacy of the image quality prediction process and show that it provides a better estimation of reconstruction image quality than the corresponding fully-sampled reference image.



### Rain structure transfer using an exemplar rain image for synthetic rain image generation
- **Arxiv ID**: http://arxiv.org/abs/1610.00427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00427v1)
- **Published**: 2016-10-03 06:58:43+00:00
- **Updated**: 2016-10-03 06:58:43+00:00
- **Authors**: Chang-Hwan Son, Xiao-Ping Zhang
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: This letter proposes a simple method of transferring rain structures of a given exemplar rain image into a target image. Given the exemplar rain image and its corresponding masked rain image, rain patches including rain structures are extracted randomly, and then residual rain patches are obtained by subtracting those rain patches from their mean patches. Next, residual rain patches are selected randomly, and then added to the given target image along a raster scanning direction. To decrease boundary artifacts around the added patches on the target image, minimum error boundary cuts are found using dynamic programming, and then blending is conducted between overlapping patches. Our experiment shows that the proposed method can generate realistic rain images that have similar rain structures in the exemplar images. Moreover, it is expected that the proposed method can be used for rain removal. More specifically, natural images and synthetic rain images generated via the proposed method can be used to learn classifiers, for example, deep neural networks, in a supervised manner.



### Video Pixel Networks
- **Arxiv ID**: http://arxiv.org/abs/1610.00527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.00527v1)
- **Published**: 2016-10-03 13:06:40+00:00
- **Updated**: 2016-10-03 13:06:40+00:00
- **Authors**: Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.



### A Survey of Multi-View Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1610.01206v5
- **DOI**: 10.1109/TKDE.2018.2872063
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1610.01206v5)
- **Published**: 2016-10-03 17:14:15+00:00
- **Updated**: 2018-10-24 02:34:53+00:00
- **Authors**: Yingming Li, Ming Yang, Zhongfei Zhang
- **Comment**: Accepted by IEEE Transactions on Knowledge and Data Engineering
- **Journal**: None
- **Summary**: Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then from the perspective of representation fusion we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.



### Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance Scenario
- **Arxiv ID**: http://arxiv.org/abs/1610.00660v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1610.00660v1)
- **Published**: 2016-10-03 18:22:03+00:00
- **Updated**: 2016-10-03 18:22:03+00:00
- **Authors**: Samik Banerjee, Sukhendu Das
- **Comment**: 13 pages, 15 figures, 4 tables. Kernel Selection, Surveillance,
  Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination
- **Journal**: None
- **Summary**: Face Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under surveillance conditions. For Support Vector Machine classification, the selection of appropriate kernel has been a widely discussed issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three real-world surveillance face datasets : FR\_SURV, SCface and ChokePoint. Results have been shown using Rank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin.



### Deep Visual Foresight for Planning Robot Motion
- **Arxiv ID**: http://arxiv.org/abs/1610.00696v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1610.00696v2)
- **Published**: 2016-10-03 19:54:17+00:00
- **Updated**: 2017-03-13 00:18:49+00:00
- **Authors**: Chelsea Finn, Sergey Levine
- **Comment**: ICRA 2017. Supplementary video:
  https://sites.google.com/site/robotforesight/
- **Journal**: None
- **Summary**: A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.



### Can Ground Truth Label Propagation from Video help Semantic Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/1610.00731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00731v1)
- **Published**: 2016-10-03 20:29:12+00:00
- **Updated**: 2016-10-03 20:29:12+00:00
- **Authors**: Siva Karthik Mustikovela, Michael Ying Yang, Carsten Rother
- **Comment**: To appear at ECCV 2016 Workshop on Video Segmentation
- **Journal**: None
- **Summary**: For state-of-the-art semantic segmentation task, training convolutional neural networks (CNNs) requires dense pixelwise ground truth (GT) labeling, which is expensive and involves extensive human effort. In this work, we study the possibility of using auxiliary ground truth, so-called \textit{pseudo ground truth} (PGT) to improve the performance. The PGT is obtained by propagating the labels of a GT frame to its subsequent frames in the video using a simple CRF-based, cue integration framework. Our main contribution is to demonstrate the use of noisy PGT along with GT to improve the performance of a CNN. We perform a systematic analysis to find the right kind of PGT that needs to be added along with the GT for training a CNN. In this regard, we explore three aspects of PGT which influence the learning of a CNN: i) the PGT labeling has to be of good quality; ii) the PGT images have to be different compared to the GT images; iii) the PGT has to be trusted differently than GT. We conclude that PGT which is diverse from GT images and has good quality of labeling can indeed help improve the performance of a CNN. Also, when PGT is multiple folds larger than GT, weighing down the trust on PGT helps in improving the accuracy. Finally, We show that using PGT along with GT, the performance of Fully Convolutional Network (FCN) on Camvid data is increased by $2.7\%$ on IoU accuracy. We believe such an approach can be used to train CNNs for semantic video segmentation where sequentially labeled image frames are needed. To this end, we provide recommendations for using PGT strategically for semantic segmentation and hence bypass the need for extensive human efforts in labeling.



### Real-Time RGB-D based Template Matching Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1610.00748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1610.00748v1)
- **Published**: 2016-10-03 20:57:17+00:00
- **Updated**: 2016-10-03 20:57:17+00:00
- **Authors**: Omid Hosseini jafari, Michael Ying Yang
- **Comment**: published in ICRA 2016
- **Journal**: None
- **Summary**: Pedestrian detection is one of the most popular topics in computer vision and robotics. Considering challenging issues in multiple pedestrian detection, we present a real-time depth-based template matching people detector. In this paper, we propose different approaches for training the depth-based template. We train multiple templates for handling issues due to various upper-body orientations of the pedestrians and different levels of detail in depth-map of the pedestrians with various distances from the camera. And, we take into account the degree of reliability for different regions of sliding window by proposing the weighted template approach. Furthermore, we combine the depth-detector with an appearance based detector as a verifier to take advantage of the appearance cues for dealing with the limitations of depth data. We evaluate our method on the challenging ETH dataset sequence. We show that our method outperforms the state-of-the-art approaches.



### Prediction of Manipulation Actions
- **Arxiv ID**: http://arxiv.org/abs/1610.00759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1610.00759v1)
- **Published**: 2016-10-03 21:23:13+00:00
- **Updated**: 2016-10-03 21:23:13+00:00
- **Authors**: Cornelia Ferm√ºller, Fang Wang, Yezhou Yang, Konstantinos Zampogiannis, Yi Zhang, Francisco Barranco, Michael Pfeiffer
- **Comment**: 15 pages, 12 figures, 6 tables
- **Journal**: None
- **Summary**: Looking at a person's hands one often can tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor's intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded from subjects performing different manipulation actions on the same object, such as "squeezing", "flipping", "washing", "wiping" and "scratching" with a sponge. In psychophysical experiments, we evaluated human observers' skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets showed that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithm to predict what and how a dexterous action is performed.




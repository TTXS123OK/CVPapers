# Arxiv Papers in cs.CV on 2016-12-02
### A Visual Representation for Editing Face Images
- **Arxiv ID**: http://arxiv.org/abs/1612.00522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.00522v1)
- **Published**: 2016-12-02 00:07:38+00:00
- **Updated**: 2016-12-02 00:07:38+00:00
- **Authors**: Jiajun Lu, Kalyan Sunkavalli, Nathan Carr, Sunil Hadap, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach for editing face images, which enables numerous exciting applications including face relighting, makeup transfer and face detail editing. Our face edits are based on a visual representation, which includes geometry, face segmentation, albedo, illumination and detail map. To recover our visual representation, we start by estimating geometry using a morphable face model, then decompose the face image to recover the albedo, and then shade the geometry with the albedo and illumination. The residual between our shaded geometry and the input image produces our detail map, which carries high frequency information that is either insufficiently or incorrectly captured by our shading process. By manipulating the detail map, we can edit face images with reality and identity preserved. Our representation allows various applications. First, it allows a user to directly manipulate various illumination. Second, it allows non-parametric makeup transfer with input face's distinctive identity features preserved. Third, it allows non-parametric modifications to the face appearance by transferring details. For face relighting and detail editing, we evaluate via a user study and our method outperforms other methods. For makeup transfer, we evaluate via an online attractiveness evaluation system, and can reliably make people look younger and more attractive. We also show extensive qualitative comparisons to existing methods, and have significant improvements over previous techniques.



### Photorealistic Facial Texture Inference Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.00523v1)
- **Published**: 2016-12-02 00:14:12+00:00
- **Updated**: 2016-12-02 00:14:12+00:00
- **Authors**: Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: We present a data-driven inference method that can synthesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild. After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area. To extract the fine appearance details from this incomplete input, we introduce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolutional neural network. We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial detail description of the entire face. A complete and photorealistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate successful face reconstructions from a wide range of low resolution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study.



### Object Detection via Aspect Ratio and Context Aware Region-based Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00534v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00534v2)
- **Published**: 2016-12-02 01:20:02+00:00
- **Updated**: 2017-03-22 16:28:24+00:00
- **Authors**: Bo Li, Tianfu Wu, Shuai Shao, Lun Zhang, Rufeng Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Jointly integrating aspect ratio and context has been extensively studied and shown performance improvement in traditional object detection systems such as the DPMs. It, however, has been largely ignored in deep neural network based detection systems. This paper presents a method of integrating a mixture of object models and region-based convolutional networks for accurate object detection. Each mixture component accounts for both object aspect ratio and multi-scale contextual information explicitly: (i) it exploits a mixture of tiling configurations in the RoI pooling to remedy the warping artifacts caused by a single type RoI pooling (e.g., with equally-sized 7 x 7 cells), and to respect the underlying object shapes more; (ii) it "looks from both the inside and the outside of a RoI" by incorporating contextual information at two scales: global context pooled from the whole image and local context pooled from the surrounding of a RoI. To facilitate accurate detection, this paper proposes a multi-stage detection scheme for integrating the mixture of object models, which utilizes the detection results of the model at the previous stage as the proposals for the current in both training and testing. The proposed method is called the aspect ratio and context aware region-based convolutional network (ARC-R-CNN). In experiments, ARC-R-CNN shows very competitive results with Faster R-CNN [41] and R-FCN [10] on two datasets: the PASCAL VOC and the Microsoft COCO. It obtains significantly better mAP performance using high IoU thresholds on both datasets.



### Breast Mass Classification from Mammograms using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.00542v1)
- **Published**: 2016-12-02 02:06:15+00:00
- **Updated**: 2016-12-02 02:06:15+00:00
- **Authors**: Daniel Lévy, Arzav Jain
- **Comment**: NIPS 2016 ML4HC Workshop
- **Journal**: None
- **Summary**: Mammography is the most widely used method to screen breast cancer. Because of its mostly manual nature, variability in mass appearance, and low signal-to-noise ratio, a significant number of breast masses are missed or misdiagnosed. In this work, we present how Convolutional Neural Networks can be used to directly classify pre-segmented breast masses in mammograms as benign or malignant, using a combination of transfer learning, careful pre-processing and data augmentation to overcome limited training data. We achieve state-of-the-art results on the DDSM dataset, surpassing human performance, and show interpretability of our model.



### Unsupervised Human Action Detection by Action Matching
- **Arxiv ID**: http://arxiv.org/abs/1612.00558v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00558v4)
- **Published**: 2016-12-02 03:39:38+00:00
- **Updated**: 2017-05-16 00:56:24+00:00
- **Authors**: Basura Fernando, Sareh Shirazi, Stephen Gould
- **Comment**: IEEE International Conference on Computer Vision and Pattern
  Recognition CVPR 2017 Workshops
- **Journal**: None
- **Summary**: We propose a new task of unsupervised action detection by action matching. Given two long videos, the objective is to temporally detect all pairs of matching video segments. A pair of video segments are matched if they share the same human action. The task is category independent---it does not matter what action is being performed---and no supervision is used to discover such video segments. Unsupervised action detection by action matching allows us to align videos in a meaningful manner. As such, it can be used to discover new action categories or as an action proposal technique within, say, an action detection pipeline. Moreover, it is a useful pre-processing step for generating video highlights, e.g., from sports videos.   We present an effective and efficient method for unsupervised action detection. We use an unsupervised temporal encoding method and exploit the temporal consistency in human actions to obtain candidate action segments. We evaluate our method on this challenging task using three activity recognition benchmarks, namely, the MPII Cooking activities dataset, the THUMOS15 action detection benchmark and a new dataset called the IKEA dataset. On the MPII Cooking dataset we detect action segments with a precision of 21.6% and recall of 11.7% over 946 long video pairs and over 5000 ground truth action segments. Similarly, on THUMOS dataset we obtain 18.4% precision and 25.1% recall over 5094 ground truth action segment pairs.



### Zero-Shot Learning posed as a Missing Data Problem
- **Arxiv ID**: http://arxiv.org/abs/1612.00560v2
- **DOI**: 10.1109/ICCVW.2017.310
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00560v2)
- **Published**: 2016-12-02 03:49:23+00:00
- **Updated**: 2017-02-21 02:37:56+00:00
- **Authors**: Bo Zhao, Botong Wu, Tianfu Wu, Yizhou Wang
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Computer Vision Workshops
  (ICCVW)
- **Summary**: This paper presents a method of zero-shot learning (ZSL) which poses ZSL as the missing data problem, rather than the missing label problem. Specifically, most existing ZSL methods focus on learning mapping functions from the image feature space to the label embedding space. Whereas, the proposed method explores a simple yet effective transductive framework in the reverse way \--- our method estimates data distribution of unseen classes in the image feature space by transferring knowledge from the label embedding space. In experiments, our method outperforms the state-of-the-art on two popular datasets.



### Self-critical Sequence Training for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1612.00563v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.00563v2)
- **Published**: 2016-12-02 04:37:22+00:00
- **Updated**: 2017-11-16 02:38:37+00:00
- **Authors**: Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, Vaibhava Goel
- **Comment**: CVPR 2017 + additional analysis + fixed baseline results, 16 pages
- **Journal**: None
- **Summary**: Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a "baseline" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.



### Guided Open Vocabulary Image Captioning with Constrained Beam Search
- **Arxiv ID**: http://arxiv.org/abs/1612.00576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00576v2)
- **Published**: 2016-12-02 06:50:49+00:00
- **Updated**: 2017-07-19 22:01:27+00:00
- **Authors**: Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould
- **Comment**: EMNLP 2017
- **Journal**: None
- **Summary**: Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.



### PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.00593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00593v2)
- **Published**: 2016-12-02 08:40:40+00:00
- **Updated**: 2017-04-10 22:25:25+00:00
- **Authors**: Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.



### Learning to Search on Manifolds for 3D Pose Estimation of Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/1612.00596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00596v1)
- **Published**: 2016-12-02 08:54:28+00:00
- **Updated**: 2016-12-02 08:54:28+00:00
- **Authors**: Yu Zhang, Chi Xu, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on the challenging problem of 3D pose estimation of a diverse spectrum of articulated objects from single depth images. A novel structured prediction approach is considered, where 3D poses are represented as skeletal models that naturally operate on manifolds. Given an input depth image, the problem of predicting the most proper articulation of underlying skeletal model is thus formulated as sequentially searching for the optimal skeletal configuration. This is subsequently addressed by convolutional neural nets trained end-to-end to render sequential prediction of the joint locations as regressing a set of tangent vectors of the underlying manifolds. Our approach is examined on various articulated objects including human hand, mouse, and fish benchmark datasets. Empirically it is shown to deliver highly competitive performance with respect to the state-of-the-arts, while operating in real-time (over 30 FPS).



### A Point Set Generation Network for 3D Object Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1612.00603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00603v2)
- **Published**: 2016-12-02 09:20:09+00:00
- **Updated**: 2016-12-07 01:12:53+00:00
- **Authors**: Haoqiang Fan, Hao Su, Leonidas Guibas
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.



### Globally Consistent Multi-People Tracking using Motion Patterns
- **Arxiv ID**: http://arxiv.org/abs/1612.00604v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1612.00604v1)
- **Published**: 2016-12-02 09:24:30+00:00
- **Updated**: 2016-12-02 09:24:30+00:00
- **Authors**: Andrii Maksai, Xinchao Wang, Francois Fleuret, Pascal Fua
- **Comment**: 8 pages, 7 figures. 11 pages supplementary
- **Journal**: None
- **Summary**: Many state-of-the-art approaches to people tracking rely on detecting them in each frame independently, grouping detections into short but reliable trajectory segments, and then further grouping them into full trajectories. This grouping typically relies on imposing local smoothness constraints but almost never on enforcing more global constraints on the trajectories. In this paper, we propose an approach to imposing global consistency by first inferring behavioral patterns from the ground truth and then using them to guide the tracking algorithm. When used in conjunction with several state-of-the-art algorithms, this further increases their already good performance. Furthermore, we propose an unsupervised scheme that yields almost similar improvements without the need for ground truth.



### SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.00606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00606v1)
- **Published**: 2016-12-02 09:27:34+00:00
- **Updated**: 2016-12-02 09:27:34+00:00
- **Authors**: Li Yi, Hao Su, Xingwen Guo, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.



### Recognition of Text Image Using Multilayer Perceptron
- **Arxiv ID**: http://arxiv.org/abs/1612.00625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00625v1)
- **Published**: 2016-12-02 10:43:04+00:00
- **Updated**: 2016-12-02 10:43:04+00:00
- **Authors**: Singh Vijendra, Nisha Vasudeva, Hem Jyotsana Parashar
- **Comment**: 2011 IEEE 3rd International Conference on Machine Learning and
  Computing (ICMLC 2011, Singapore, PP 547-550
- **Journal**: None
- **Summary**: The biggest challenge in the field of image processing is to recognize documents both in printed and handwritten format. Optical Character Recognition OCR is a type of document image analysis where scanned digital image that contains either machine printed or handwritten script input into an OCR software engine and translating it into an editable machine readable digital text format. A Neural network is designed to model the way in which the brain performs a particular task or function of interest: The neural network is simulated in software on a digital computer. Character Recognition refers to the process of converting printed Text documents into translated Unicode Text. The printed documents available in the form of books, papers, magazines, etc. are scanned using standard scanners which produce an image of the scanned document. Lines are identifying by an algorithm where we identify top and bottom of line. Then in each line character boundaries are calculated by an algorithm then using these calculation, characters is isolated from the image and then we classify each character by basic back propagation. Each image character is comprised of 30*20 pixels. We have used the Back propagation Neural Network for efficient recognition where the errors were corrected through back propagation and rectified neuron values were transmitted by feed-forward method in the neural network of multiple layers.



### Centrog Feature technique for vehicle type recognition at day and night times
- **Arxiv ID**: http://arxiv.org/abs/1612.00645v1
- **DOI**: 10.5121/ijaia.2016.7604
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00645v1)
- **Published**: 2016-12-02 11:51:50+00:00
- **Updated**: 2016-12-02 11:51:50+00:00
- **Authors**: Martins E. Irhebhude, Philip O. Odion, Darius T. Chinyio
- **Comment**: 14 pages, 8 figures, Journal article
- **Journal**: None
- **Summary**: This work proposes a feature-based technique to recognize vehicle types within day and night times. Support vector machine (SVM) classifier is applied on image histogram and CENsus Transformed histogRam Oriented Gradient (CENTROG) features in order to classify vehicle types during the day and night. Thermal images were used for the night time experiments. Although thermal images suffer from low image resolution, lack of colour and poor texture information, they offer the advantage of being unaffected by high intensity light sources such as vehicle headlights which tend to render normal images unsuitable for night time image capturing and subsequent analysis. Since contour is useful in shape based categorisation and the most distinctive feature within thermal images, CENTROG is used to capture this feature information and is used within the experiments. The experimental results so obtained were compared with those obtained by employing the CENsus TRansformed hISTogram (CENTRIST). Experimental results revealed that CENTROG offers better recognition accuracies for both day and night times vehicle types recognition.



### Voxelwise nonlinear regression toolbox for neuroimage analysis: Application to aging and neurodegenerative disease modeling
- **Arxiv ID**: http://arxiv.org/abs/1612.00667v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, q-bio.NC, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1612.00667v3)
- **Published**: 2016-12-02 12:59:11+00:00
- **Updated**: 2017-04-18 20:12:16+00:00
- **Authors**: Santi Puch, Asier Aduriz, Adrià Casamitjana, Veronica Vilaplana, Paula Petrone, Grégory Operto, Raffaele Cacciaglia, Stavros Skouras, Carles Falcon, José Luis Molinuevo, Juan Domingo Gispert
- **Comment**: 4 pages + 1 page for acknowledgements and references. NIPS 2016
  Workshop on Machine Learning for Health (NIPS ML4HC)
- **Journal**: None
- **Summary**: This paper describes a new neuroimaging analysis toolbox that allows for the modeling of nonlinear effects at the voxel level, overcoming limitations of methods based on linear models like the GLM. We illustrate its features using a relevant example in which distinct nonlinear trajectories of Alzheimer's disease related brain atrophy patterns were found across the full biological spectrum of the disease. The open-source toolbox presented in this paper is available at https://github.com/imatge-upc/VNeAT.



### Identifying and Categorizing Anomalies in Retinal Imaging Data
- **Arxiv ID**: http://arxiv.org/abs/1612.00686v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.00686v1)
- **Published**: 2016-12-02 14:05:49+00:00
- **Updated**: 2016-12-02 14:05:49+00:00
- **Authors**: Philipp Seeböck, Sebastian Waldstein, Sophie Klimscha, Bianca S. Gerendas, René Donner, Thomas Schlegl, Ursula Schmidt-Erfurth, Georg Langs
- **Comment**: Extended Abstract, Accepted for NIPS 2016 Workshop "Machine Learning
  for Health"
- **Journal**: None
- **Summary**: The identification and quantification of markers in medical images is critical for diagnosis, prognosis and management of patients in clinical practice. Supervised- or weakly supervised training enables the detection of findings that are known a priori. It does not scale well, and a priori definition limits the vocabulary of markers to known entities reducing the accuracy of diagnosis and prognosis. Here, we propose the identification of anomalies in large-scale medical imaging data using healthy examples as a reference. We detect and categorize candidates for anomaly findings untypical for the observed data. A deep convolutional autoencoder is trained on healthy retinal images. The learned model generates a new feature representation, and the distribution of healthy retinal patches is estimated by a one-class support vector machine. Results demonstrate that we can identify pathologic regions in images without using expert annotations. A subsequent clustering categorizes findings into clinically meaningful classes. In addition the learned features outperform standard embedding approaches in a classification task.



### Action Recognition with Dynamic Image Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00738v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00738v2)
- **Published**: 2016-12-02 16:33:06+00:00
- **Updated**: 2017-08-19 20:54:07+00:00
- **Authors**: Hakan Bilen, Basura Fernando, Efstratios Gavves, Andrea Vedaldi
- **Comment**: 14 pages, 9 figures, 9 tables
- **Journal**: None
- **Summary**: We introduce the concept of "dynamic image", a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of `rank pooling'. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. When a linear ranking machine is used, the resulting representation is in the form of an image, which we call dynamic because it summarizes the video dynamics in addition of appearance. This is a powerful idea because it allows to convert any video to an image so that existing CNN models pre-trained for the analysis of still images can be immediately extended to videos. We also present an efficient and effective approximate rank pooling operator, accelerating standard rank pooling algorithms by orders of magnitude, and formulate that as a CNN layer. This new layer allows generalizing dynamic images to dynamic feature maps. We demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance.



### A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1612.00799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00799v1)
- **Published**: 2016-12-02 19:25:44+00:00
- **Updated**: 2016-12-02 19:25:44+00:00
- **Authors**: David Vázquez, Jorge Bernal, F. Javier Sánchez, Gloria Fernández-Esparrach, Antonio M. López, Adriana Romero, Michal Drozdzal, Aaron Courville
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. The main limitations of this screening procedure are polyp miss-rate and inability to perform visual assessment of polyp malignancy. These drawbacks can be reduced by designing Decision Support Systems (DSS) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation. Thus, in this paper, we introduce an extended benchmark of colonoscopy image, with the hope of establishing a new strong benchmark for colonoscopy image analysis research. We provide new baselines on this dataset by training standard fully convolutional networks (FCN) for semantic segmentation and significantly outperforming, without any further post-processing, prior results in endoluminal scene segmentation.



### Scribbler: Controlling Deep Image Synthesis with Sketch and Color
- **Arxiv ID**: http://arxiv.org/abs/1612.00835v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.00835v2)
- **Published**: 2016-12-02 20:53:01+00:00
- **Updated**: 2016-12-05 20:06:57+00:00
- **Authors**: Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, James Hays
- **Comment**: 13 pages, 14 figures
- **Journal**: None
- **Summary**: Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.



### Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1612.00837v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.00837v3)
- **Published**: 2016-12-02 20:57:07+00:00
- **Updated**: 2017-05-15 17:58:49+00:00
- **Authors**: Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.   We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).   We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.   Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.



### Procedural Generation of Videos to Train Deep Action Recognition Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.00881v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.00881v2)
- **Published**: 2016-12-02 22:24:24+00:00
- **Updated**: 2017-07-19 10:34:36+00:00
- **Authors**: César Roberto de Souza, Adrien Gaidon, Yohann Cabon, Antonio Manuel López Peña
- **Comment**: Accepted for publication at CVPR 2017. http://adas.cvc.uab.es/phav/
- **Journal**: None
- **Summary**: Deep learning for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate the generation of synthetic training data for action recognition, as it has recently shown promising results for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines. We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for "Procedural Human Action Videos". It contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix synthetic and real videos, even if the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos.



### Parameter Compression of Recurrent Neural Networks and Degradation of Short-term Memory
- **Arxiv ID**: http://arxiv.org/abs/1612.00891v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.00891v2)
- **Published**: 2016-12-02 23:11:10+00:00
- **Updated**: 2017-02-24 18:22:30+00:00
- **Authors**: Jonathan A. Cox
- **Comment**: Accepted to IJCNN 2017. Final camera ready paper
- **Journal**: None
- **Summary**: The significant computational costs of deploying neural networks in large-scale or resource constrained environments, such as data centers and mobile devices, has spurred interest in model compression, which can achieve a reduction in both arithmetic operations and storage memory. Several techniques have been proposed for reducing or compressing the parameters for feed-forward and convolutional neural networks, but less is understood about the effect of parameter compression on recurrent neural networks (RNN). In particular, the extent to which the recurrent parameters can be compressed and the impact on short-term memory performance, is not well understood. In this paper, we study the effect of complexity reduction, through singular value decomposition rank reduction, on RNN and minimal gated recurrent unit (MGRU) networks for several tasks. We show that considerable rank reduction is possible when compressing recurrent weights, even without fine tuning. Furthermore, we propose a perturbation model for the effect of general perturbations, such as a compression, on the recurrent parameters of RNNs. The model is tested against a noiseless memorization experiment that elucidates the short-term memory performance. In this way, we demonstrate that the effect of compression of recurrent parameters is dependent on the degree of temporal coherence present in the data and task. This work can guide on-the-fly RNN compression for novel environments or tasks, and provides insight for applying RNN compression in low-power devices, such as hearing aids.




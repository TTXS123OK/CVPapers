# Arxiv Papers in cs.CV on 2016-12-19
### Parsing Images of Overlapping Organisms with Deep Singling-Out Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.06017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06017v1)
- **Published**: 2016-12-19 00:59:30+00:00
- **Updated**: 2016-12-19 00:59:30+00:00
- **Authors**: Victor Yurchenko, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: This work is motivated by the mostly unsolved task of parsing biological images with multiple overlapping articulated model organisms (such as worms or larvae). We present a general approach that separates the two main challenges associated with such data, individual object shape estimation and object groups disentangling. At the core of the approach is a deep feed-forward singling-out network (SON) that is trained to map each local patch to a vectorial descriptor that is sensitive to the characteristics (e.g. shape) of a central object, while being invariant to the variability of all other surrounding elements. Given a SON, a local image patch can be matched to a gallery of isolated elements using their SON-descriptors, thus producing a hypothesis about the shape of the central element in that patch. The image-level optimization based on integer programming can then pick a subset of the hypotheses to explain (parse) the whole image and disentangle groups of organisms.   While sharing many similarities with existing "analysis-by-synthesis" approaches, our method avoids the need for stochastic search in the high-dimensional configuration space and numerous rendering operations at test-time. We show that our approach can parse microscopy images of three popular model organisms (the C.Elegans roundworms, the Drosophila larvae, and the E.Coli bacteria) even under significant crowding and overlaps between organisms. We speculate that the overall approach is applicable to a wider class of image parsing problems concerned with crowded articulated objects, for which rendering training images is possible.



### Dual Deep Network for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1612.06053v1
- **DOI**: 10.1109/TIP.2017.2669880
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06053v1)
- **Published**: 2016-12-19 06:01:30+00:00
- **Updated**: 2016-12-19 06:01:30+00:00
- **Authors**: Zhizhen Chi, Hongyang Li, Huchuan Lu, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual tracking addresses the problem of identifying and localizing an unknown target in a video given the target specified by a bounding box in the first frame. In this paper, we propose a dual network to better utilize features among layers for visual tracking. It is observed that features in higher layers encode semantic context while its counterparts in lower layers are sensitive to discriminative appearance. Thus we exploit the hierarchical features in different layers of a deep model and design a dual structure to obtain better feature representation from various streams, which is rarely investigated in previous work. To highlight geometric contours of the target, we integrate the hierarchical feature maps with an edge detector as the coarse prior maps to further embed local details around the target. To leverage the robustness of our dual network, we train it with random patches measuring the similarities between the network activation and target appearance, which serves as a regularization to enforce the dual network to focus on target object. The proposed dual network is updated online in a unique manner based on the observation that the target being tracked in consecutive frames should share more similar feature representations than those in the surrounding background. It is also found that for a target object, the prior maps can help further enhance performance by passing message into the output maps of the dual network. Therefore, an independent component analysis with reference algorithm (ICA-R) is employed to extract target context using prior maps as guidance. Online tracking is conducted by maximizing the posterior estimate on the final maps with stochastic and periodic update. Quantitative and qualitative evaluations on two large-scale benchmark data sets show that the proposed algorithm performs favourably against the state-of-the-arts.



### On Random Weights for Texture Generation in One Layer Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.06070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1612.06070v1)
- **Published**: 2016-12-19 08:21:04+00:00
- **Updated**: 2016-12-19 08:21:04+00:00
- **Authors**: Mihir Mongia, Kundan Kumar, Akram Erraqabi, Yoshua Bengio
- **Comment**: ICASSP 2017
- **Journal**: None
- **Summary**: Recent work in the literature has shown experimentally that one can use the lower layers of a trained convolutional neural network (CNN) to model natural textures. More interestingly, it has also been experimentally shown that only one layer with random filters can also model textures although with less variability. In this paper we ask the question as to why one layer CNNs with random filters are so effective in generating textures? We theoretically show that one layer convolutional architectures (without a non-linearity) paired with the an energy function used in previous literature, can in fact preserve and modulate frequency coefficients in a manner so that random weights and pretrained weights will generate the same type of images. Based on the results of this analysis we question whether similar properties hold in the case where one uses one convolution layer with a non-linearity. We show that in the case of ReLu non-linearity there are situations where only one input will give the minimum possible energy whereas in the case of no nonlinearity, there are always infinite solutions that will give the minimum possible energy. Thus we can show that in certain situations adding a ReLu non-linearity generates less variable images.



### X-ray In-Depth Decomposition: Revealing The Latent Structures
- **Arxiv ID**: http://arxiv.org/abs/1612.06096v2
- **DOI**: 10.1007/978-3-319-66179-7_51
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06096v2)
- **Published**: 2016-12-19 09:59:07+00:00
- **Updated**: 2017-03-22 17:12:17+00:00
- **Authors**: Shadi Albarqouni, Javad Fotouhi, Nassir Navab
- **Comment**: Under review at MICCAI 2017
- **Journal**: Medical Image Computing and Computer Assisted Intervention (MICCAI
  2017)
- **Summary**: X-ray radiography is the most readily available imaging modality and has a broad range of applications that spans from diagnosis to intra-operative guidance in cardiac, orthopedics, and trauma procedures. Proper interpretation of the hidden and obscured anatomy in X-ray images remains a challenge and often requires high radiation dose and imaging from several perspectives. In this work, we aim at decomposing the conventional X-ray image into d X-ray components of independent, non-overlapped, clipped sub-volumes using deep learning approach. Despite the challenging aspects of modeling such a highly ill-posed problem, exciting and encouraging results are obtained paving the path for further contributions in this direction.



### Cross-Modal Manifold Learning for Cross-modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1612.06098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06098v1)
- **Published**: 2016-12-19 10:03:58+00:00
- **Updated**: 2016-12-19 10:03:58+00:00
- **Authors**: Sailesh Conjeti, Anees Kazi, Nassir Navab, Amin Katouzian
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new scalable algorithm for cross-modal similarity preserving retrieval in a learnt manifold space. Unlike existing approaches that compromise between preserving global and local geometries, the proposed technique respects both simultaneously during manifold alignment. The global topologies are maintained by recovering underlying mapping functions in the joint manifold space by deploying partially corresponding instances. The inter-, and intra-modality affinity matrices are then computed to reinforce original data skeleton using perturbed minimum spanning tree (pMST), and maximizing the affinity among similar cross-modal instances, respectively. The performance of proposed algorithm is evaluated upon two multimodal image datasets (coronary atherosclerosis histology and brain MRI) for two applications: classification, and regression. Our exhaustive validations and results demonstrate the superiority of our technique over comparative methods and its feasibility for improving computer-assisted diagnosis systems, where disease-specific complementary information shall be aggregated and interpreted across modalities to form the final decision.



### Active and Continuous Exploration with Deep Neural Networks and Expected Model Output Changes
- **Arxiv ID**: http://arxiv.org/abs/1612.06129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06129v1)
- **Published**: 2016-12-19 11:27:33+00:00
- **Updated**: 2016-12-19 11:27:33+00:00
- **Authors**: Christoph Käding, Erik Rodner, Alexander Freytag, Joachim Denzler
- **Comment**: accepted contribution at NIPS 2016 Workshop on Continual Learning and
  Deep Networks
- **Journal**: None
- **Summary**: The demands on visual recognition systems do not end with the complexity offered by current large-scale image datasets, such as ImageNet. In consequence, we need curious and continuously learning algorithms that actively acquire knowledge about semantic concepts which are present in available unlabeled data. As a step towards this goal, we show how to perform continuous active learning and exploration, where an algorithm actively selects relevant batches of unlabeled examples for annotation. These examples could either belong to already known or to yet undiscovered classes. Our algorithm is based on a new generalization of the Expected Model Output Change principle for deep architectures and is especially tailored to deep neural networks. Furthermore, we show easy-to-implement approximations that yield efficient techniques for active selection. Empirical experiments show that our method outperforms currently used heuristics.



### Few-Shot Object Recognition from Machine-Labeled Web Images
- **Arxiv ID**: http://arxiv.org/abs/1612.06152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06152v1)
- **Published**: 2016-12-19 12:25:36+00:00
- **Updated**: 2016-12-19 12:25:36+00:00
- **Authors**: Zhongwen Xu, Linchao Zhu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: With the tremendous advances of Convolutional Neural Networks (ConvNets) on object recognition, we can now obtain reliable enough machine-labeled annotations easily by predictions from off-the-shelf ConvNets. In this work, we present an abstraction memory based framework for few-shot learning, building upon machine-labeled image annotations. Our method takes some large-scale machine-annotated datasets (e.g., OpenImages) as an external memory bank. In the external memory bank, the information is stored in the memory slots with the form of key-value, where image feature is regarded as key and label embedding serves as value. When queried by the few-shot examples, our model selects visually similar data from the external memory bank, and writes the useful information obtained from related external data into another memory bank, i.e., abstraction memory. Long Short-Term Memory (LSTM) controllers and attention mechanisms are utilized to guarantee the data written to the abstraction memory is correlated to the query example. The abstraction memory concentrates information from the external memory bank, so that it makes the few-shot recognition effective. In the experiments, we firstly confirm that our model can learn to conduct few-shot object recognition on clean human-labeled data from ImageNet dataset. Then, we demonstrate that with our model, machine-labeled image annotations are very effective and abundant resources to perform object recognition on novel categories. Experimental results show that our proposed model with machine-labeled annotations achieves great performance, only with a gap of 1% between of the one with human-labeled annotations.



### Crowd collectiveness measure via graph-based node clique learning
- **Arxiv ID**: http://arxiv.org/abs/1612.06170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1612.06170v1)
- **Published**: 2016-12-19 13:26:46+00:00
- **Updated**: 2016-12-19 13:26:46+00:00
- **Authors**: Weiya Ren
- **Comment**: 23 pages. 10 figures. 5 tables
- **Journal**: None
- **Summary**: Collectiveness motions of crowd systems have attracted a great deal of attentions in recently years. In this paper, we try to measure the collectiveness of a crowd system by the proposed node clique learning method. The proposed method is a graph based method, and investigates the influence from one node to other nodes. A node is represented by a set of nodes which named a clique, which is obtained by spreading information from this node to other nodes in graph. Then only nodes with sufficient information are selected as the clique of this node. The motion coherence between two nodes is defined by node cliques comparing. The collectiveness of a node and the collectiveness of the crowd system are defined by the nodes coherence. Self-driven particle (SDP) model and the crowd motion database are used to test the ability of the proposed method in measuring collectiveness.



### An extended Perona-Malik model based on probabilistic models
- **Arxiv ID**: http://arxiv.org/abs/1612.06176v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.06176v1)
- **Published**: 2016-12-19 13:39:45+00:00
- **Updated**: 2016-12-19 13:39:45+00:00
- **Authors**: Lars M. Mescheder, Dirk A. Lorenz
- **Comment**: None
- **Journal**: None
- **Summary**: The Perona-Malik model has been very successful at restoring images from noisy input. In this paper, we reinterpret the Perona-Malik model in the language of Gaussian scale mixtures and derive some extensions of the model. Specifically, we show that the expectation-maximization (EM) algorithm applied to Gaussian scale mixtures leads to the lagged-diffusivity algorithm for computing stationary points of the Perona-Malik diffusion equations. Moreover, we show how mean field approximations to these Gaussian scale mixtures lead to a modification of the lagged-diffusivity algorithm that better captures the uncertainties in the restoration. Since this modification can be hard to compute in practice we propose relaxations to the mean field objective to make the algorithm computationally feasible. Our numerical experiments show that this modified lagged-diffusivity algorithm often performs better at restoring textured areas and fuzzy edges than the unmodified algorithm. As a second application of the Gaussian scale mixture framework, we show how an efficient sampling procedure can be obtained for the probabilistic model, making the computation of the conditional mean and other expectations algorithmically feasible. Again, the resulting algorithm has a strong resemblance to the lagged-diffusivity algorithm. Finally, we show that a probabilistic version of the Mumford-Shah segementation model can be obtained in the same framework with a discrete edge-prior.



### Photo-Quality Evaluation based on Computational Aesthetics: Review of Feature Extraction Techniques
- **Arxiv ID**: http://arxiv.org/abs/1612.06259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06259v1)
- **Published**: 2016-12-19 16:41:24+00:00
- **Updated**: 2016-12-19 16:41:24+00:00
- **Authors**: Dimitris Spathis
- **Comment**: None
- **Journal**: None
- **Summary**: Researchers try to model the aesthetic quality of photographs into low and high- level features, drawing inspiration from art theory, psychology and marketing. We attempt to describe every feature extraction measure employed in the above process. The contribution of this literature review is the taxonomy of each feature by its implementation complexity, considering real-world applications and integration in mobile apps and digital cameras. Also, we discuss the machine learning results along with some unexplored research areas as future work.



### Handwritten Signature Verification Using Hand-Worn Devices
- **Arxiv ID**: http://arxiv.org/abs/1612.06305v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1612.06305v1)
- **Published**: 2016-12-19 18:42:52+00:00
- **Updated**: 2016-12-19 18:42:52+00:00
- **Authors**: Ben Nassi, Alona Levy, Yuval Elovici, Erez Shmueli
- **Comment**: None
- **Journal**: None
- **Summary**: Online signature verification technologies, such as those available in banks and post offices, rely on dedicated digital devices such as tablets or smart pens to capture, analyze and verify signatures. In this paper, we suggest a novel method for online signature verification that relies on the increasingly available hand-worn devices, such as smartwatches or fitness trackers, instead of dedicated ad-hoc devices. Our method uses a set of known genuine and forged signatures, recorded using the motion sensors of a hand-worn device, to train a machine learning classifier. Then, given the recording of an unknown signature and a claimed identity, the classifier can determine whether the signature is genuine or forged. In order to validate our method, it was applied on 1980 recordings of genuine and forged signatures that we collected from 66 subjects in our institution. Using our method, we were able to successfully distinguish between genuine and forged signatures with a high degree of accuracy (0.98 AUC and 0.05 EER).



### Large-Scale Image Retrieval with Attentive Deep Local Features
- **Arxiv ID**: http://arxiv.org/abs/1612.06321v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06321v4)
- **Published**: 2016-12-19 19:35:56+00:00
- **Updated**: 2018-02-03 02:19:16+00:00
- **Authors**: Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, Bohyung Han
- **Comment**: ICCV 2017. Code and dataset available:
  https://github.com/tensorflow/models/tree/master/research/delf
- **Journal**: None
- **Summary**: We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives---in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins. Code and dataset can be found at the project webpage: https://github.com/tensorflow/models/tree/master/research/delf .



### Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images
- **Arxiv ID**: http://arxiv.org/abs/1612.06341v2
- **DOI**: 10.1109/ICCV.2017.594
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06341v2)
- **Published**: 2016-12-19 20:42:43+00:00
- **Updated**: 2017-04-27 17:31:55+00:00
- **Authors**: Aron Yu, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Distinguishing subtle differences in attributes is valuable, yet learning to make visual comparisons remains non-trivial. Not only is the number of possible comparisons quadratic in the number of training images, but also access to images adequately spanning the space of fine-grained visual differences is limited. We propose to overcome the sparsity of supervision problem via synthetically generated images. Building on a state-of-the-art image generation engine, we sample pairs of training images exhibiting slight modifications of individual attributes. Augmenting real training image pairs with these examples, we then train attribute ranking models to predict the relative strength of an attribute in novel pairs of real images. Our results on datasets of faces and fashion images show the great promise of bootstrapping imperfect image generators to counteract sample sparsity for learning to rank.



### Learning Features by Watching Objects Move
- **Arxiv ID**: http://arxiv.org/abs/1612.06370v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.06370v2)
- **Published**: 2016-12-19 20:56:04+00:00
- **Updated**: 2017-04-12 04:28:47+00:00
- **Authors**: Deepak Pathak, Ross Girshick, Piotr Dollár, Trevor Darrell, Bharath Hariharan
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.



### Asynchronous Temporal Fields for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.06371v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06371v2)
- **Published**: 2016-12-19 20:56:40+00:00
- **Updated**: 2017-07-24 09:58:14+00:00
- **Authors**: Gunnar A. Sigurdsson, Santosh Divvala, Ali Farhadi, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Actions are more than just movements and trajectories: we cook to eat and we hold a cup to drink from it. A thorough understanding of videos requires going beyond appearance modeling and necessitates reasoning about the sequence of activities, as well as the higher-level constructs such as intentions. But how do we model and reason about these? We propose a fully-connected temporal CRF model for reasoning over various aspects of activities that includes objects, actions, and intentions, where the potentials are predicted by a deep network. End-to-end training of such structured models is a challenging endeavor: For inference and learning we need to construct mini-batches consisting of whole videos, leading to mini-batches with only a few videos. This causes high-correlation between data points leading to breakdown of the backprop algorithm. To address this challenge, we present an asynchronous variational inference method that allows efficient end-to-end training. Our method achieves a classification mAP of 22.4% on the Charades benchmark, outperforming the state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal localization.



### Feature Encoding in Band-limited Distributed Surveillance Systems
- **Arxiv ID**: http://arxiv.org/abs/1612.06423v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06423v3)
- **Published**: 2016-12-19 21:38:53+00:00
- **Updated**: 2019-04-29 04:09:41+00:00
- **Authors**: Alireza Rahimpour, Ali Taalimi, Hairong Qi
- **Comment**: The 42th International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2017)
- **Journal**: None
- **Summary**: Distributed surveillance systems have become popular in recent years due to security concerns. However, transmitting high dimensional data in bandwidth-limited distributed systems becomes a major challenge. In this paper, we address this issue by proposing a novel probabilistic algorithm based on the divergence between the probability distributions of the visual features in order to reduce their dimensionality and thus save the network bandwidth in distributed wireless smart camera networks. We demonstrate the effectiveness of the proposed approach through extensive experiments on two surveillance recognition tasks.



### Fractal Descriptors of Texture Images Based on the Triangular Prism Dimension
- **Arxiv ID**: http://arxiv.org/abs/1612.06435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06435v1)
- **Published**: 2016-12-19 22:01:59+00:00
- **Updated**: 2016-12-19 22:01:59+00:00
- **Authors**: João B. Florindo, Odemir M. Bruno
- **Comment**: 21 pages, 14 figures
- **Journal**: None
- **Summary**: This work presents a novel descriptor for texture images based on fractal geometry and its application to image analysis. The descriptors are provided by estimating the triangular prism fractal dimension under different scales with a weight exponential parameter, followed by dimensionality reduction using Karhunen-Lo\`{e}ve transform. The efficiency of the proposed descriptors is tested on two well-known texture data sets, that is, Brodatz and Vistex, both for classification and image retrieval. The novel method is also tested concerning invariances in situations when the textures are rotated or affected by Gaussian noise. The obtained results outperform other classical and state-of-the-art descriptors in the literature and demonstrate the power of the triangular descriptors in these tasks, suggesting their use in practical applications of image analysis based on texture features.



### Binary Distance Transform to Improve Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1612.06443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06443v1)
- **Published**: 2016-12-19 22:19:19+00:00
- **Updated**: 2016-12-19 22:19:19+00:00
- **Authors**: Mariane Barros Neiva, Antoine Manzanera, Odemir Martinez Bruno
- **Comment**: 9 pages, 4 figures, WVC 2016 proceedings
- **Journal**: None
- **Summary**: To recognize textures many methods have been developed along the years. However, texture datasets may be hard to be classified due to artefacts such as a variety of scale, illumination and noise. This paper proposes the application of binary distance transform on the original dataset to add information to texture representation and consequently improve recognition. Texture images, usually in grayscale, suffers a binarization prior to distance transform and one of the resulted images are combined with original texture to improve the amount of information. Four datasets are used to evaluate our approach. For Outex dataset, for instance, the proposal outperforms all rates, improvements of an up to 10\%, compared to traditional approach where descriptors are applied on the original dataset, showing the importance of this approach.



### Exploring Structure for Long-Term Tracking of Multiple Objects in Sports Videos
- **Arxiv ID**: http://arxiv.org/abs/1612.06454v1
- **DOI**: 10.1016/j.cviu.2016.12.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06454v1)
- **Published**: 2016-12-19 23:14:26+00:00
- **Updated**: 2016-12-19 23:14:26+00:00
- **Authors**: Henrique Morimitsu, Isabelle Bloch, Roberto M. Cesar-Jr
- **Comment**: This version corresponds to the preprint of the paper accepted for
  CVIU
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for exploiting structural relations to track multiple objects that may undergo long-term occlusion and abrupt motion. We use a model-free approach that relies only on annotations given in the first frame of the video to track all the objects online, i.e. without knowledge from future frames. We initialize a probabilistic Attributed Relational Graph (ARG) from the first frame, which is incrementally updated along the video. Instead of using the structural information only to evaluate the scene, the proposed approach considers it to generate new tracking hypotheses. In this way, our method is capable of generating relevant object candidates that are used to improve or recover the track of lost objects. The proposed method is evaluated on several videos of table tennis, volleyball, and on the ACASVA dataset. The results show that our approach is very robust, flexible and able to outperform other state-of-the-art methods in sports videos that present structural patterns.



### High Performance Software in Multidimensional Reduction Methods for Image Processing with Application to Ancient Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/1612.06457v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.06457v5)
- **Published**: 2016-12-19 23:38:26+00:00
- **Updated**: 2018-07-18 22:44:43+00:00
- **Authors**: Corneliu T. C. Arsene, Stephen Church, Mark Dickinson
- **Comment**: 25 pages; University of Manchester, UK. Paper submitted to Manuscript
  Cultures
- **Journal**: None
- **Summary**: Multispectral imaging is an important technique for improving the readability of written or printed text where the letters have faded, either due to deliberate erasing or simply due to the ravages of time. Often the text can be read simply by looking at individual wavelengths, but in some cases the images need further enhancement to maximise the chances of reading the text. There are many possible enhancement techniques and this paper assesses and compares an extended set of dimensionality reduction methods for image processing. We assess 15 dimensionality reduction methods in two different manuscripts. This assessment was performed both subjectively by asking the opinions of scholars who were experts in the languages used in the manuscripts which of the techniques they preferred and also by using the Davies-Bouldin and Dunn indexes for assessing the quality of the resulted image clusters. We found that the Canonical Variates Analysis (CVA) method which was using a Matlab implementation and we have used previously to enhance multispectral images, it was indeed superior to all the other tested methods. However it is very likely that other approaches will be more suitable in specific circumstance so we would still recommend that a range of these techniques are tried. In particular, CVA is a supervised clustering technique so it requires considerably more user time and effort than a non-supervised technique such as the much more commonly used Principle Component Analysis Approach (PCA). If the results from PCA are adequate to allow a text to be read then the added effort required for CVA may not be justified. For the purposes of comparing the computational times and the image results, a CVA method is also implemented in C programming language and using the GNU (GNUs Not Unix) Scientific Library (GSL) and the OpenCV (OPEN source Computer Vision) computer vision programming library.




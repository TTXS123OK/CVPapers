# Arxiv Papers in cs.CV on 2016-12-08
### Research on the Multiple Feature Fusion Image Retrieval Algorithm based on Texture Feature and Rough Set Theory
- **Arxiv ID**: http://arxiv.org/abs/1612.02493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02493v1)
- **Published**: 2016-12-08 00:08:23+00:00
- **Updated**: 2016-12-08 00:08:23+00:00
- **Authors**: Xiaojie Shi, Yijun Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, we have witnessed the explosive growth of images with complex information and content. In order to effectively and precisely retrieve desired images from a large-scale image database with low time-consuming, we propose the multiple feature fusion image retrieval algorithm based on the texture feature and rough set theory in this paper. In contrast to the conventional approaches that only use the single feature or standard, we fuse the different features with operation of normalization. The rough set theory will assist us to enhance the robustness of retrieval system when facing with incomplete data warehouse. To enhance the texture extraction paradigm, we use the wavelet Gabor function that holds better robustness. In addition, from the perspectives of the internal and external normalization, we re-organize extracted feature with the better combination. The numerical experiment has verified general feasibility of our methodology. We enhance the overall accuracy compared with the other state-of-the-art algorithms.



### Discrete Schroedinger Transform For Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.02498v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1612.02498v1)
- **Published**: 2016-12-08 00:49:18+00:00
- **Updated**: 2016-12-08 00:49:18+00:00
- **Authors**: João B. Florindo, Odemir M. Bruno
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: This work presents a new procedure to extract features of grey-level texture images based on the discrete Schroedinger transform. This is a non-linear transform where the image is mapped as the initial probability distribution of a wave function and such distribution evolves in time following the Schroedinger equation from Quantum Mechanics. The features are provided by statistical moments of the distribution measured at different times. The proposed method is applied to the classification of three databases of textures used for benchmark and compared to other well-known texture descriptors in the literature, such as textons, local binary patterns, multifractals, among others. All of them are outperformed by the proposed method in terms of percentage of images correctly classified. The proposal is also applied to the identification of plant species using scanned images of leaves and again it outperforms other texture methods. A test with images affected by Gaussian and "salt \& pepper" noise is also carried out, also with the best performance achieved by the Schroedinger descriptors.



### Complex Matrix Factorization for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1612.02513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02513v1)
- **Published**: 2016-12-08 02:42:20+00:00
- **Updated**: 2016-12-08 02:42:20+00:00
- **Authors**: Viet-Hang Duong, Yuan-Shan Lee, Bach-Tung Pham, Seksan Mathulaprangsan, Pham The Bao, Jia-Ching Wang
- **Comment**: 4 pages,3 figures,4 tables
- **Journal**: None
- **Summary**: This work developed novel complex matrix factorization methods for face recognition; the methods were complex matrix factorization (CMF), sparse complex matrix factorization (SpaCMF), and graph complex matrix factorization (GraCMF). After real-valued data are transformed into a complex field, the complex-valued matrix will be decomposed into two matrices of bases and coefficients, which are derived from solutions to an optimization problem in a complex domain. The generated objective function is the real-valued function of the reconstruction error, which produces a parametric description. Factorizing the matrix of complex entries directly transformed the constrained optimization problem into an unconstrained optimization problem. Additionally, a complex vector space with N dimensions can be regarded as a 2N-dimensional real vector space. Accordingly, all real analytic properties can be exploited in the complex field. The ability to exploit these important characteristics motivated the development herein of a simpler framework that can provide better recognition results. The effectiveness of this framework will be clearly elucidated in later sections in this paper.



### An Efficient Algorithm for the Piecewise-Smooth Model with Approximately Explicit Solutions
- **Arxiv ID**: http://arxiv.org/abs/1612.02521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02521v1)
- **Published**: 2016-12-08 03:25:59+00:00
- **Updated**: 2016-12-08 03:25:59+00:00
- **Authors**: Huihui Song, Yuhui Zheng, Kaihua Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an efficient approach to image segmentation that approximates the piecewise-smooth (PS) functional in [12] with explicit solutions. By rendering some rational constraints on the initial conditions and the final solutions of the PS functional, we propose two novel formulations which can be approximated to be the explicit solutions of the evolution partial differential equations (PDEs) of the PS model, in which only one PDE needs to be solved efficiently. Furthermore, an energy term that regularizes the level set function to be a signed distance function is incorporated into our evolution formulation, and the time-consuming re-initialization is avoided. Experiments on synthetic and real images show that our method is more efficient than both the PS model and the local binary fitting (LBF) model [4], while having similar segmentation accuracy as the LBF model.



### Contextual Visual Similarity
- **Arxiv ID**: http://arxiv.org/abs/1612.02534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02534v1)
- **Published**: 2016-12-08 05:53:16+00:00
- **Updated**: 2016-12-08 05:53:16+00:00
- **Authors**: Xiaofang Wang, Kris M. Kitani, Martial Hebert
- **Comment**: Submitted to CVPR 2017
- **Journal**: None
- **Summary**: Measuring visual similarity is critical for image understanding. But what makes two images similar? Most existing work on visual similarity assumes that images are similar because they contain the same object instance or category. However, the reason why images are similar is much more complex. For example, from the perspective of category, a black dog image is similar to a white dog image. However, in terms of color, a black dog image is more similar to a black horse image than the white dog image. This example serves to illustrate that visual similarity is ambiguous but can be made precise when given an explicit contextual perspective. Based on this observation, we propose the concept of contextual visual similarity. To be concrete, we examine the concept of contextual visual similarity in the application domain of image search. Instead of providing only a single image for image similarity search (\eg, Google image search), we require three images. Given a query image, a second positive image and a third negative image, dissimilar to the first two images, we define a contextualized similarity search criteria. In particular, we learn feature weights over all the feature dimensions of each image such that the distance between the query image and the positive image is small and their distances to the negative image are large after reweighting their features. The learned feature weights encode the contextualized visual similarity specified by the user and can be used for attribute specific image search. We also show the usefulness of our contextualized similarity weighting scheme for different tasks, such as answering visual analogy questions and unsupervised attribute discovery.



### Query-adaptive Image Retrieval by Deep Weighted Hashing
- **Arxiv ID**: http://arxiv.org/abs/1612.02541v2
- **DOI**: None
- **Categories**: **cs.CV**, H.3.1, H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1612.02541v2)
- **Published**: 2016-12-08 06:20:03+00:00
- **Updated**: 2017-05-09 02:40:20+00:00
- **Authors**: Jian Zhang, Yuxin Peng
- **Comment**: 13 pages, submitted to IEEE Transactions On Multimedia
- **Journal**: None
- **Summary**: Hashing methods have attracted much attention for large scale image retrieval. Some deep hashing methods have achieved promising results by taking advantage of the strong representation power of deep networks recently. However, existing deep hashing methods treat all hash bits equally. On one hand, a large number of images share the same distance to a query image due to the discrete Hamming distance, which raises a critical issue of image retrieval where fine-grained rankings are very important. On the other hand, different hash bits actually contribute to the image retrieval differently, and treating them equally greatly affects the retrieval accuracy of image. To address the above two problems, we propose the query-adaptive deep weighted hashing (QaDWH) approach, which can perform fine-grained ranking for different queries by weighted Hamming distance. First, a novel deep hashing network is proposed to learn the hash codes and corresponding class-wise weights jointly, so that the learned weights can reflect the importance of different hash bits for different image classes. Second, a query-adaptive image retrieval method is proposed, which rapidly generates hash bit weights for different query images by fusing its semantic probability and the learned class-wise weights. Fine-grained image retrieval is then performed by the weighted Hamming distance, which can provide more accurate ranking than the traditional Hamming distance. Experiments on four widely used datasets show that the proposed approach outperforms eight state-of-the-art hashing methods.



### AGA: Attribute Guided Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.02559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02559v2)
- **Published**: 2016-12-08 08:23:09+00:00
- **Updated**: 2017-08-26 19:52:46+00:00
- **Authors**: Mandar Dixit, Roland Kwitt, Marc Niethammer, Nuno Vasconcelos
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We consider the problem of data augmentation, i.e., generating artificial samples to extend a given corpus of training data. Specifically, we propose attributed-guided augmentation (AGA) which learns a mapping that allows to synthesize data such that an attribute of a synthesized sample is at a desired value or strength. This is particularly interesting in situations where little data with no attribute annotation is available for learning, but we have access to a large external corpus of heavily annotated samples. While prior works primarily augment in the space of images, we propose to perform augmentation in feature space instead. We implement our approach as a deep encoder-decoder architecture that learns the synthesis function in an end-to-end manner. We demonstrate the utility of our approach on the problems of (1) one-shot object recognition in a transfer-learning setting where we have no prior knowledge of the new classes, as well as (2) object-based one-shot scene recognition. As external data, we leverage 3D depth and pose information from the SUN RGB-D dataset. Our experiments show that attribute-guided augmentation of high-level CNN features considerably improves one-shot recognition performance on both problems.



### Classification of Neurological Gait Disorders Using Multi-task Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1612.02562v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1612.02562v3)
- **Published**: 2016-12-08 08:43:42+00:00
- **Updated**: 2017-02-01 03:50:17+00:00
- **Authors**: Ioannis Papavasileiou, Wenlong Zhang, Xin Wang, Jinbo Bi, Li Zhang, Song Han
- **Comment**: shorter version of this paper is submitted to CHASE '17 conference
- **Journal**: None
- **Summary**: As our population ages, neurological impairments and degeneration of the musculoskeletal system yield gait abnormalities, which can significantly reduce quality of life. Gait rehabilitative therapy has been widely adopted to help patients maximize community participation and living independence. To further improve the precision and efficiency of rehabilitative therapy, more objective methods need to be developed based on sensory data. In this paper, an algorithmic framework is proposed to provide classification of gait disorders caused by two common neurological diseases, stroke and Parkinson's Disease (PD), from ground contact force (GCF) data. An advanced machine learning method, multi-task feature learning (MTFL), is used to jointly train classification models of a subject's gait in three classes, post-stroke, PD and healthy gait. Gait parameters related to mobility, balance, strength and rhythm are used as features for the classification. Out of all the features used, the MTFL models capture the more important ones per disease, which will help provide better objective assessment and therapy progress tracking. To evaluate the proposed methodology we use data from a human participant study, which includes five PD patients, three post-stroke patients, and three healthy subjects. Despite the diversity of abnormalities, the evaluation shows that the proposed approach can successfully distinguish post-stroke and PD gait from healthy gait, as well as post-stroke from PD gait, with Area Under the Curve (AUC) score of at least 0.96. Moreover, the methodology helps select important gait features to better understand the key characteristics that distinguish abnormal gaits and design personalized treatment.



### Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker
- **Arxiv ID**: http://arxiv.org/abs/1612.02572v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1612.02572v1)
- **Published**: 2016-12-08 09:26:08+00:00
- **Updated**: 2016-12-08 09:26:08+00:00
- **Authors**: James H Cole, Rudra PK Poudel, Dimosthenis Tsagkrasoulis, Matthan WA Caan, Claire Steves, Tim D Spector, Giovanni Montana
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning analysis of neuroimaging data can accurately predict chronological age in healthy people and deviations from healthy brain ageing have been associated with cognitive impairment and disease. Here we sought to further establish the credentials of "brain-predicted age" as a biomarker of individual differences in the brain ageing process, using a predictive modelling approach based on deep learning, and specifically convolutional neural networks (CNN), and applied to both pre-processed and raw T1-weighted MRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted age using a large dataset of healthy adults (N = 2001). Next, we sought to establish the heritability of brain-predicted age using a sample of monozygotic and dizygotic female twins (N = 62). Thirdly, we examined the test-retest and multi-centre reliability of brain-predicted age using two samples (within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were generated and compared to a Gaussian Process Regression (GPR) approach, on all datasets. Input data were grey matter (GM) or white matter (WM) volumetric maps generated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted age represents an accurate, highly reliable and genetically-valid phenotype, that has potential to be used as a biomarker of brain ageing. Moreover, age predictions can be accurately generated on raw T1-MRI data, substantially reducing computation time for novel data, bringing the process closer to giving real-time information on brain health in clinical settings.



### Filter sharing: Efficient learning of parameters for volumetric convolutions
- **Arxiv ID**: http://arxiv.org/abs/1612.02575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02575v1)
- **Published**: 2016-12-08 09:35:13+00:00
- **Updated**: 2016-12-08 09:35:13+00:00
- **Authors**: Rahul Venkataramani, Sheshadri Thiruvenkadam, Prasad Sudhakar, Hariharan Ravishankar, Vivek Vaidya
- **Comment**: 6 pages, 2 figures. Published in NIPS 2016 workshop on Machine
  Learning for Health, December 2016, Barcelona
- **Journal**: None
- **Summary**: Typical convolutional neural networks (CNNs) have several millions of parameters and require a large amount of annotated data to train them. In medical applications where training data is hard to come by, these sophisticated machine learning models are difficult to train. In this paper, we propose a method to reduce the inherent complexity of CNNs during training by exploiting the significant redundancy that is noticed in the learnt CNN filters. Our method relies on finding a small set of filters and mixing coefficients to derive every filter in each convolutional layer at the time of training itself, thereby reducing the number of parameters to be trained. We consider the problem of 3D lung nodule segmentation in CT images and demonstrate the effectiveness of our method in achieving good results with only few training examples.



### Imaging around corners with single-pixel detector by computational ghost imaging
- **Arxiv ID**: http://arxiv.org/abs/1612.07120v1
- **DOI**: 10.1016/j.ijleo.2017.08.057
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1612.07120v1)
- **Published**: 2016-12-08 09:54:20+00:00
- **Updated**: 2016-12-08 09:54:20+00:00
- **Authors**: Bin Bai, Jianbin Liu, Yu Zhou, Songlin Zhang, Yuchen He, Zhuo Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We have designed a single-pixel camera with imaging around corners based on computational ghost imaging. It can obtain the image of an object when the camera cannot look at the object directly. Our imaging system explores the fact that a bucket detector in a ghost imaging setup has no spatial resolution capability. A series of experiments have been designed to confirm our predictions. This camera has potential applications for imaging around corner or other similar environments where the object cannot be observed directly.



### From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur
- **Arxiv ID**: http://arxiv.org/abs/1612.02583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02583v1)
- **Published**: 2016-12-08 10:05:57+00:00
- **Updated**: 2016-12-08 10:05:57+00:00
- **Authors**: Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton van den Hengel, Qinfeng Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Removing pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but the extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach is thus that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.



### Multi-source Transfer Learning with Convolutional Neural Networks for Lung Pattern Analysis
- **Arxiv ID**: http://arxiv.org/abs/1612.02589v1
- **DOI**: 10.1109/JBHI.2016.2636929
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1612.02589v1)
- **Published**: 2016-12-08 10:43:03+00:00
- **Updated**: 2016-12-08 10:43:03+00:00
- **Authors**: Stergios Christodoulidis, Marios Anthimopoulos, Lukas Ebner, Andreas Christe, Stavroula Mougiakakou
- **Comment**: None
- **Journal**: None
- **Summary**: Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis (CAD) systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.



### Scene Flow Estimation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1612.02590v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02590v3)
- **Published**: 2016-12-08 10:44:03+00:00
- **Updated**: 2017-05-21 09:52:02+00:00
- **Authors**: Zike Yan, Xuezhi Xiang
- **Comment**: 51 pages, 12 figures, 10 tables, 108 references
- **Journal**: None
- **Summary**: This paper is the first to review the scene flow estimation field, which analyzes and compares methods, technical challenges, evaluation methodologies and performance of scene flow estimation. Existing algorithms are categorized in terms of scene representation, data source, and calculation scheme, and the pros and cons in each category are compared briefly. The datasets and evaluation protocols are enumerated, and the performance of the most representative methods is presented. A future vision is illustrated with few questions arisen for discussion. This survey presents a general introduction and analysis of scene flow estimation.



### Progressive Tree-like Curvilinear Structure Reconstruction with Structured Ranking Learning and Graph Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1612.02631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02631v1)
- **Published**: 2016-12-08 13:13:01+00:00
- **Updated**: 2016-12-08 13:13:01+00:00
- **Authors**: Seong-Gyun Jeong, Yuliya Tarabalka, Nicolas Nisse, Josiane Zerubia
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel tree-like curvilinear structure reconstruction algorithm based on supervised learning and graph theory. In this work we analyze image patches to obtain the local major orientations and the rankings that correspond to the curvilinear structure. To extract local curvilinear features, we compute oriented gradient information using steerable filters. We then employ Structured Support Vector Machine for ordinal regression of the input image patches, where the ordering is determined by shape similarity to latent curvilinear structure. Finally, we progressively reconstruct the curvilinear structure by looking for geodesic paths connecting remote vertices in the graph built on the structured output rankings. Experimental results show that the proposed algorithm faithfully provides topological features of the curvilinear structures using minimal pixels for various datasets.



### Learning Video Object Segmentation from Static Images
- **Arxiv ID**: http://arxiv.org/abs/1612.02646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02646v1)
- **Published**: 2016-12-08 13:59:18+00:00
- **Updated**: 2016-12-08 13:59:18+00:00
- **Authors**: Anna Khoreva, Federico Perazzi, Rodrigo Benenson, Bernt Schiele, Alexander Sorkine-Hornung
- **Comment**: Submitted to CVPR 2017
- **Journal**: None
- **Summary**: Inspired by recent advances of deep learning in instance segmentation and object tracking, we introduce video object segmentation problem as a concept of guided instance segmentation. Our model proceeds on a per-frame basis, guided by the output of the previous frame towards the object of interest in the next frame. We demonstrate that highly accurate object segmentation in videos can be enabled by using a convnet trained with static images only. The key ingredient of our approach is a combination of offline and online learning strategies, where the former serves to produce a refined mask from the previous frame estimate and the latter allows to capture the appearance of the specific object instance. Our method can handle different types of input annotations: bounding boxes and segments, as well as incorporate multiple annotated frames, making the system suitable for diverse applications. We obtain competitive results on three different datasets, independently from the type of input annotation.



### FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1612.02649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02649v1)
- **Published**: 2016-12-08 14:11:10+00:00
- **Updated**: 2016-12-08 14:11:10+00:00
- **Authors**: Judy Hoffman, Dequan Wang, Fisher Yu, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional models for dense prediction have proven successful for a wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset.



### A fuzzy approach for segmentation of touching characters
- **Arxiv ID**: http://arxiv.org/abs/1612.04862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.04862v1)
- **Published**: 2016-12-08 14:44:31+00:00
- **Updated**: 2016-12-08 14:44:31+00:00
- **Authors**: Giuseppe Airò Farulla, Nadir Murru, Rosaria Rossini
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of correctly segmenting touching characters is an hard task to solve and it is of major relevance in pattern recognition. In the recent years, many methods and algorithms have been proposed; still, a definitive solution is far from being found. In this paper, we propose a novel method based on fuzzy logic. The proposed method combines in a novel way three features for segmenting touching characters that have been already proposed in other studies but have been exploited only singularly so far. The proposed strategy is based on a 3--input/1--output fuzzy inference system with fuzzy rules specifically optimized for segmenting touching characters in the case of Latin printed and handwritten characters. The system performances are illustrated and supported by numerical examples showing that our approach can achieve a reasonable good overall accuracy in segmenting characters even on tricky conditions of touching characters. Moreover, numerical results suggest that the method can be applied to many different datasets of characters by means of a convenient tuning of the fuzzy sets and rules.



### Domain knowledge assisted cyst segmentation in OCT retinal images
- **Arxiv ID**: http://arxiv.org/abs/1612.02675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02675v1)
- **Published**: 2016-12-08 14:59:07+00:00
- **Updated**: 2016-12-08 14:59:07+00:00
- **Authors**: Karthik Gopinath, Jayanthi Sivaswamy
- **Comment**: The paper was accepted as an oral presentation in MICCAI-2015 OPTIMA
  Cyst Segmentation Challenge
- **Journal**: None
- **Summary**: 3D imaging modalities are becoming increasingly popular and relevant in retinal imaging owing to their effectiveness in highlighting structures in sub-retinal layers. OCT is one such modality which has great importance in the context of analysis of cystoid structures in subretinal layers. Signal to noise ratio(SNR) of the images obtained from OCT is less and hence automated and accurate determination of cystoid structures from OCT is a challenging task. We propose an automated method for detecting/segmenting cysts in 3D OCT volumes. The proposed method is biologically inspired and fast aided by the domain knowledge about the cystoid structures. An ensemble learning methodRandom forests is learnt for classification of detected region into cyst region. The method achieves detection and segmentation in a unified setting. We believe the proposed approach with further improvements can be a promising starting point for more robust approach. This method is validated against the training set achieves a mean dice coefficient of 0.3893 with a standard deviation of 0.2987



### Deep Supervision with Shape Concepts for Occlusion-Aware 3D Object Parsing
- **Arxiv ID**: http://arxiv.org/abs/1612.02699v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02699v3)
- **Published**: 2016-12-08 15:33:19+00:00
- **Updated**: 2017-04-20 20:19:33+00:00
- **Authors**: Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D. Hager, Manmohan Chandraker
- **Comment**: Accepted in CVPR 2017
- **Journal**: None
- **Summary**: Monocular 3D object parsing is highly desirable in various scenarios including occlusion reasoning and holistic scene interpretation. We present a deep convolutional neural network (CNN) architecture to localize semantic parts in 2D image and 3D space while inferring their visibility states, given a single RGB image. Our key insight is to exploit domain knowledge to regularize the network by deeply supervising its hidden layers, in order to sequentially infer intermediate concepts associated with the final task. To acquire training data in desired quantities with ground truth 3D shape and relevant concepts, we render 3D object CAD models to generate large-scale synthetic data and simulate challenging occlusion configurations between objects. We train the network only on synthetic data and demonstrate state-of-the-art performances on real image benchmarks including an extended version of KITTI, PASCAL VOC, PASCAL3D+ and IKEA for 2D and 3D keypoint localization and instance segmentation. The empirical results substantiate the utility of our deep supervision scheme by demonstrating effective transfer of knowledge from synthetic data to real images, resulting in less overfitting compared to standard end-to-end training.



### Predicting Ground-Level Scene Layout from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1612.02709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02709v1)
- **Published**: 2016-12-08 16:12:23+00:00
- **Updated**: 2016-12-08 16:12:23+00:00
- **Authors**: Menghua Zhai, Zachary Bessinger, Scott Workman, Nathan Jacobs
- **Comment**: 13 pages including appendix
- **Journal**: None
- **Summary**: We introduce a novel strategy for learning to extract semantically meaningful features from aerial imagery. Instead of manually labeling the aerial imagery, we propose to predict (noisy) semantic features automatically extracted from co-located ground imagery. Our network architecture takes an aerial image as input, extracts features using a convolutional neural network, and then applies an adaptive transformation to map these features into the ground-level perspective. We use an end-to-end learning approach to minimize the difference between the semantic segmentation extracted directly from the ground image and the semantic segmentation predicted solely based on the aerial image. We show that a model learned using this strategy, with no additional training, is already capable of rough semantic labeling of aerial imagery. Furthermore, we demonstrate that by finetuning this model we can achieve more accurate semantic segmentation than two baseline initialization strategies. We use our network to address the task of estimating the geolocation and geoorientation of a ground image. Finally, we show how features extracted from an aerial image can be used to hallucinate a plausible ground-level panorama.



### Joint Hand Detection and Rotation Estimation by Using CNN
- **Arxiv ID**: http://arxiv.org/abs/1612.02742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02742v1)
- **Published**: 2016-12-08 17:53:54+00:00
- **Updated**: 2016-12-08 17:53:54+00:00
- **Authors**: Xiaoming Deng, Ye Yuan, Yinda Zhang, Ping Tan, Liang Chang, Shuo Yang, Hongan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Hand detection is essential for many hand related tasks, e.g. parsing hand pose, understanding gesture, which are extremely useful for robotics and human-computer interaction. However, hand detection in uncontrolled environments is challenging due to the flexibility of wrist joint and cluttered background. We propose a deep learning based approach which detects hands and calibrates in-plane rotation under supervision at the same time. To guarantee the recall, we propose a context aware proposal generation algorithm which significantly outperforms the selective search. We then design a convolutional neural network(CNN) which handles object rotation explicitly to jointly solve the object detection and rotation estimation tasks. Experiments show that our method achieves better results than state-of-the-art detection models on widely-used benchmarks such as Oxford and Egohands database. We further show that rotation estimation and classification can mutually benefit each other.



### A Maximum A Posteriori Estimation Framework for Robust High Dynamic Range Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1612.02761v1
- **DOI**: 10.1109/TIP.2016.2642790
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02761v1)
- **Published**: 2016-12-08 18:33:08+00:00
- **Updated**: 2016-12-08 18:33:08+00:00
- **Authors**: Yuelong Li, Chul Lee, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: High dynamic range (HDR) image synthesis from multiple low dynamic range (LDR) exposures continues to be actively researched. The extension to HDR video synthesis is a topic of significant current interest due to potential cost benefits. For HDR video, a stiff practical challenge presents itself in the form of accurate correspondence estimation of objects between video frames. In particular, loss of data resulting from poor exposures and varying intensity make conventional optical flow methods highly inaccurate. We avoid exact correspondence estimation by proposing a statistical approach via maximum a posterior (MAP) estimation, and under appropriate statistical assumptions and choice of priors and models, we reduce it to an optimization problem of solving for the foreground and background of the target frame. We obtain the background through rank minimization and estimate the foreground via a novel multiscale adaptive kernel regression technique, which implicitly captures local structure and temporal motion by solving an unconstrained optimization problem. Extensive experimental results on both real and synthetic datasets demonstrate that our algorithm is more capable of delivering high-quality HDR videos than current state-of-the-art methods, under both subjective and objective assessments. Furthermore, a thorough complexity analysis reveals that our algorithm achieves better complexity-performance trade-off than conventional methods.



### Feedback Neural Network for Weakly Supervised Geo-Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1612.02766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02766v1)
- **Published**: 2016-12-08 18:46:39+00:00
- **Updated**: 2016-12-08 18:46:39+00:00
- **Authors**: Xianming Liu, Amy Zhang, Tobias Tiecke, Andreas Gros, Thomas S. Huang
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Learning from weakly-supervised data is one of the main challenges in machine learning and computer vision, especially for tasks such as image semantic segmentation where labeling is extremely expensive and subjective. In this paper, we propose a novel neural network architecture to perform weakly-supervised learning by suppressing irrelevant neuron activations. It localizes objects of interest by learning from image-level categorical labels in an end-to-end manner. We apply this algorithm to a practical challenge of transforming satellite images into a map of settlements and individual buildings. Experimental results show that the proposed algorithm achieves superior performance and efficiency when compared with various baseline models.



### 3D Shape Segmentation with Projective Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.02808v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1612.02808v3)
- **Published**: 2016-12-08 20:46:32+00:00
- **Updated**: 2017-11-13 05:46:17+00:00
- **Authors**: Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha Chaudhuri
- **Comment**: This is an updated version of our CVPR 2017 paper. We incorporated
  new experiments that demonstrate ShapePFCN performance under the case of
  consistent *upright* orientation and an additional input channel in our
  rendered images for encoding height from the ground plane (upright axis
  coordinate values). Performance is improved in this setting
- **Journal**: None
- **Summary**: This paper introduces a deep architecture for segmenting 3D objects into their labeled semantic parts. Our architecture combines image-based Fully Convolutional Networks (FCNs) and surface-based Conditional Random Fields (CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are used for efficient view-based reasoning about 3D object parts. Through a special projection layer, FCN outputs are effectively aggregated across multiple views and scales, then are projected onto the 3D object surfaces. Finally, a surface-based CRF combines the projected outputs with geometric consistency cues to yield coherent segmentations. The whole architecture (multi-view FCNs and CRF) is trained end-to-end. Our approach significantly outperforms the existing state-of-the-art methods in the currently largest segmentation benchmark (ShapeNet). Finally, we demonstrate promising segmentation results on noisy 3D shapes acquired from consumer-grade depth cameras.



### Deep TEN: Texture Encoding Network
- **Arxiv ID**: http://arxiv.org/abs/1612.02844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02844v1)
- **Published**: 2016-12-08 21:27:31+00:00
- **Updated**: 2016-12-08 21:27:31+00:00
- **Authors**: Hang Zhang, Jia Xue, Kristin Dana
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Deep Texture Encoding Network (Deep-TEN) with a novel Encoding Layer integrated on top of convolutional layers, which ports the entire dictionary learning and encoding pipeline into a single model. Current methods build from distinct components, using standard encoders with separate off-the-shelf features such as SIFT descriptors or pre-trained CNN features for material recognition. Our new approach provides an end-to-end learning framework, where the inherent visual vocabularies are learned directly from the loss function. The features, dictionaries and the encoding representation for the classifier are all learned simultaneously. The representation is orderless and therefore is particularly useful for material and texture recognition. The Encoding Layer generalizes robust residual encoders such as VLAD and Fisher Vectors, and has the property of discarding domain specific information which makes the learned convolutional features easier to transfer. Additionally, joint training using multiple datasets of varied sizes and class labels is supported resulting in increased recognition performance. The experimental results show superior performance as compared to state-of-the-art methods using gold-standard databases such as MINC-2500, Flickr Material Database, KTH-TIPS-2b, and two recent databases 4D-Light-Field-Material and GTOS. The source code for the complete system are publicly available.



### Exploiting 2D Floorplan for Building-scale Panorama RGBD Alignment
- **Arxiv ID**: http://arxiv.org/abs/1612.02859v1
- **DOI**: 10.1109/CVPR.2017.156
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.02859v1)
- **Published**: 2016-12-08 22:22:14+00:00
- **Updated**: 2016-12-08 22:22:14+00:00
- **Authors**: Erik Wijmans, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel algorithm that utilizes a 2D floorplan to align panorama RGBD scans. While effective panorama RGBD alignment techniques exist, such a system requires extremely dense RGBD image sampling. Our approach can significantly reduce the number of necessary scans with the aid of a floorplan image. We formulate a novel Markov Random Field inference problem as a scan placement over the floorplan, as opposed to the conventional scan-to-scan alignment. The technical contributions lie in multi-modal image correspondence cues (between scans and schematic floorplan) as well as a novel coverage potential avoiding an inherent stacking bias. The proposed approach has been evaluated on five challenging large indoor spaces. To the best of our knowledge, we present the first effective system that utilizes a 2D floorplan image for building-scale 3D pointcloud alignment. The source code and the data will be shared with the community to further enhance indoor mapping research.




# Arxiv Papers in cs.CV on 2016-12-30
### Analysis of the noise in back-projection light field acquisition and its optimization
- **Arxiv ID**: http://arxiv.org/abs/1701.05084v1
- **DOI**: 10.1364/AO.56.000F20
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1701.05084v1)
- **Published**: 2016-12-30 01:09:36+00:00
- **Updated**: 2016-12-30 01:09:36+00:00
- **Authors**: Ni Chen, Zhenbo Ren, Dayan Li, Edmund Y. Lam, Guohai Situ
- **Comment**: None
- **Journal**: None
- **Summary**: Light field reconstruction from images captured by focal plane sweeping can achieve high lateral resolution comparable to the modern camera sensor. This is impossible for the conventional micro-lenslet based light field capture systems. However, the severe defocus noise and the low depth resolution limit its applications. In this paper, we analyze the defocus noise and the depth resolution in the focal plane sweeping based light field reconstruction technique, and propose a method to reduce the defocus noise and improve the depth resolution. Both numerical and experimental results verify the proposed method.



### Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.09401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09401v1)
- **Published**: 2016-12-30 06:32:38+00:00
- **Updated**: 2016-12-30 06:32:38+00:00
- **Authors**: Pichao Wang, Wanqing Li, Chuankun Li, Yonghong Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (ConvNets) have recently shown promising performance in many computer vision tasks, especially image-based recognition. How to effectively apply ConvNets to sequence-based data is still an open problem. This paper proposes an effective yet simple method to represent spatio-temporal information carried in $3D$ skeleton sequences into three $2D$ images by encoding the joint trajectories and their dynamics into color distribution in the images, referred to as Joint Trajectory Maps (JTM), and adopts ConvNets to learn the discriminative features for human action recognition. Such an image-based representation enables us to fine-tune existing ConvNets models for the classification of skeleton sequences without training the networks afresh. The three JTMs are generated in three orthogonal planes and provide complimentary information to each other. The final recognition is further improved through multiply score fusion of the three JTMs. The proposed method was evaluated on four public benchmark datasets, the large NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset and UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the state-of-the-art results.



### Shape Estimation from Defocus Cue for Microscopy Images via Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/1612.09411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09411v1)
- **Published**: 2016-12-30 07:41:25+00:00
- **Updated**: 2016-12-30 07:41:25+00:00
- **Authors**: Arnav Bhavsar
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the usefulness of 3D shape estimation is being realized in microscopic or close-range imaging, as the 3D information can further be used in various applications. Due to limited depth of field at such small distances, the defocus blur induced in images can provide information about the 3D shape of the object. The task of `shape from defocus' (SFD), involves the problem of estimating good quality 3D shape estimates from images with depth-dependent defocus blur. While the research area of SFD is quite well-established, the approaches have largely demonstrated results on objects with bulk/coarse shape variation. However, in many cases, objects studied under microscopes often involve fine/detailed structures, which have not been explicitly considered in most methods. In addition, given that, in recent years, large data volumes are typically associated with microscopy related applications, it is also important for such SFD methods to be efficient. In this work, we provide an indication of the usefulness of the Belief Propagation (BP) approach in addressing these concerns for SFD. BP has been known to be an efficient combinatorial optimization approach, and has been empirically demonstrated to yield good quality solutions in low-level vision problems such as image restoration, stereo disparity estimation etc. For exploiting the efficiency of BP in SFD, we assume local space-invariance of the defocus blur, which enables the application of BP in a straightforward manner. Even with such an assumption, the ability of BP to provide good quality solutions while using non-convex priors, reflects in yielding plausible shape estimates in presence of fine structures on the objects under microscopy imaging.



### Automatic labeling of molecular biomarkers of whole slide immunohistochemistry images using fully convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1612.09420v1
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1612.09420v1)
- **Published**: 2016-12-30 08:27:04+00:00
- **Updated**: 2016-12-30 08:27:04+00:00
- **Authors**: Fahime Sheikhzadeh, Martial Guillaud, Rabab K. Ward
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of quantifying biomarkers in multi-stained tissues, based on color and spatial information. A deep learning based method that can automatically localize and quantify the cells expressing biomarker(s) in a whole slide image is proposed. The deep learning network is a fully convolutional network (FCN) whose input is the true RGB color image of a tissue and output is a map of the different biomarkers. The FCN relies on a convolutional neural network (CNN) that classifies each cell separately according to the biomarker it expresses. In this study, images of immunohistochemistry (IHC) stained slides were collected and used. More than 4,500 RGB images of cells were manually labeled based on the expressing biomarkers. The labeled cell images were used to train the CNN (obtaining an accuracy of 92% in a test set). The trained CNN is then extended to an FCN that generates a map of all biomarkers in the whole slide image acquired by the scanner (instead of classifying every cell image). To evaluate our method, we manually labeled all nuclei expressing different biomarkers in two whole slide images and used theses as the ground truth. Our proposed method for immunohistochemical analysis compares well with the manual labeling by humans (average F-score of 0.96).



### Smart Content Recognition from Images Using a Mixture of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.09506v2
- **DOI**: 10.1007/978-981-10-6451-7_2
- **Categories**: **stat.ML**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1612.09506v2)
- **Published**: 2016-12-30 15:18:39+00:00
- **Updated**: 2017-07-14 09:03:57+00:00
- **Authors**: Tee Connie, Mundher Al-Shabi, Michael Goh
- **Comment**: To be published in LNEE, Code: github.com/mundher/NSFW
- **Journal**: None
- **Summary**: With rapid development of the Internet, web contents become huge. Most of the websites are publicly available, and anyone can access the contents from anywhere such as workplace, home and even schools. Nevertheless, not all the web contents are appropriate for all users, especially children. An example of these contents is pornography images which should be restricted to certain age group. Besides, these images are not safe for work (NSFW) in which employees should not be seen accessing such contents during work. Recently, convolutional neural networks have been successfully applied to many computer vision problems. Inspired by these successes, we propose a mixture of convolutional neural networks for adult content recognition. Unlike other works, our method is formulated on a weighted sum of multiple deep neural network models. The weights of each CNN models are expressed as a linear regression problem learned using Ordinary Least Squares (OLS). Experimental results demonstrate that the proposed model outperforms both single CNN model and the average sum of CNN models in adult content recognition.



### Feedback Networks
- **Arxiv ID**: http://arxiv.org/abs/1612.09508v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09508v3)
- **Published**: 2016-12-30 15:39:45+00:00
- **Updated**: 2017-08-20 07:15:55+00:00
- **Authors**: Amir R. Zamir, Te-Lin Wu, Lin Sun, William Shen, Jitendra Malik, Silvio Savarese
- **Comment**: See a video describing the method at https://youtu.be/MY5Uhv38Ttg and
  the website at http://feedbacknet.stanford.edu/
- **Journal**: None
- **Summary**: Currently, the most successful learning models in computer vision are based on learning successive representations followed by a decision layer. This is usually actualized through feedforward multilayer neural networks, e.g. ConvNets, where each layer forms one of such successive representations. However, an alternative that can achieve the same goal is a feedback based approach in which the representation is formed in an iterative manner based on a feedback received from previous iteration's output.   We establish that a feedback based approach has several fundamental advantages over feedforward: it enables making early predictions at the query time, its output naturally conforms to a hierarchical structure in the label space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning. We observe that feedback networks develop a considerably different representation compared to feedforward counterparts, in line with the aforementioned advantages. We put forth a general feedback based learning architecture with the endpoint results on par or better than existing feedforward networks with the addition of the above advantages. We also investigate several mechanisms in feedback architectures (e.g. skip connections in time) and design choices (e.g. feedback length). We hope this study offers new perspectives in quest for more natural and practical learning models.



### A Joint Speaker-Listener-Reinforcer Model for Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/1612.09542v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1612.09542v2)
- **Published**: 2016-12-30 17:39:19+00:00
- **Updated**: 2017-04-17 20:13:49+00:00
- **Authors**: Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg
- **Comment**: Some typo fixed; comprehension results on refcocog updated; more
  human evaluation results added
- **Journal**: None
- **Summary**: Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer



### A Unified Tensor-based Active Appearance Face Model
- **Arxiv ID**: http://arxiv.org/abs/1612.09548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1612.09548v2)
- **Published**: 2016-12-30 18:08:16+00:00
- **Updated**: 2017-06-13 16:33:25+00:00
- **Authors**: Zhen-Hua Feng, Josef Kittler, William Christmas, Xiao-Jun Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. For each type of face information, namely shape and texture, we construct a unified tensor model capturing all relevant appearance variations. This contrasts with the variation-specific models of the classical tensor AAM. To achieve the unification across pose variations, a strategy for dealing with self-occluded faces is proposed to obtain consistent shape and texture representations of pose-varied faces. In addition, our UT-AAM is capable of constructing the model from an incomplete training dataset, using tensor completion methods. Last, we use an effective cascaded-regression-based method for UT-AAM fitting. With these advancements, the utility of UT-AAM in practice is considerably enhanced. As an example, we demonstrate the improvements in training facial landmark detectors through the use of UT-AAM to synthesise a large number of virtual samples. Experimental results obtained using the Multi-PIE and 300-W face datasets demonstrate the merits of the proposed approach.



### Super-Resolution Reconstruction of Electrical Impedance Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1701.00031v3
- **DOI**: 10.1016/j.compeleceng.2018.05.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.00031v3)
- **Published**: 2016-12-30 23:23:38+00:00
- **Updated**: 2018-05-15 23:43:16+00:00
- **Authors**: Ricardo A. Borsoi, Julio C. C. Aya, Guilherme H. Costa, Jos√© C. M. Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Electrical Impedance Tomography (EIT) systems are becoming popular because they present several advantages over competing systems. However, EIT leads to images with very low resolution. Moreover, the nonuniform sampling characteristic of EIT precludes the straightforward application of traditional image ruper-resolution techniques. In this work, we propose a resampling based Super-Resolution method for EIT image quality improvement. Preliminary results show that the proposed technique can lead to substantial improvements in EIT image resolution, making it more competitive with other technologies.




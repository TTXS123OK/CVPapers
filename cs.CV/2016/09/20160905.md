# Arxiv Papers in cs.CV on 2016-09-05
### Combining Fully Convolutional and Recurrent Neural Networks for 3D Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1609.01006v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01006v2)
- **Published**: 2016-09-05 00:33:26+00:00
- **Updated**: 2016-09-06 13:35:13+00:00
- **Authors**: Jianxu Chen, Lin Yang, Yizhe Zhang, Mark Alber, Danny Z. Chen
- **Comment**: None
- **Journal**: https://papers.nips.cc/paper/2016/hash/4dcf435435894a4d0972046fc566af76-Abstract.html
- **Summary**: Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved state-of-the-art segmentation perfor- mance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on planes orthogonal to 2D image slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a com- bination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results comparing to the known DL-based 3D segmentation approaches.



### Classifying and sorting cluttered piles of unknown objects with robots: a learning approach
- **Arxiv ID**: http://arxiv.org/abs/1609.01044v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1609.01044v1)
- **Published**: 2016-09-05 07:44:40+00:00
- **Updated**: 2016-09-05 07:44:40+00:00
- **Authors**: Janne V. Kujala, Tuomas J. Lukka, Harri Holopainen
- **Comment**: 8 pages, 14 figures (pagination changed in arXiv version); accepted
  for the International Conference on Intelligent Robots and Systems (IROS)
  2016
- **Journal**: None
- **Summary**: We consider the problem of sorting a densely cluttered pile of unknown objects using a robot. This yet unsolved problem is relevant in the robotic waste sorting business.   By extending previous active learning approaches to grasping, we show a system that learns the task autonomously. Instead of predicting just whether a grasp succeeds, we predict the classes of the objects that end up being picked and thrown onto the target conveyor. Segmenting and identifying objects from the uncluttered target conveyor, as opposed to the working area, is easier due to the added structure since the thrown objects will be the only ones present.   Instead of trying to segment or otherwise understand the cluttered working area in any way, we simply allow the controller to learn a mapping from an RGBD image in the neighborhood of the grasp to a predicted result---all segmentation etc. in the working area is implicit in the learned function. The grasp selection operates in two stages: The first stage is hardcoded and outputs a distribution of possible grasps that sometimes succeed. The second stage uses a purely learned criterion to choose the grasp to make from the proposal distribution created by the first stage.   In an experiment, the system quickly learned to make good pickups and predict correctly, in advance, which class of object it was going to pick up and was able to sort the objects from a densely cluttered pile by color.



### A Deep Multi-Level Network for Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1609.01064v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01064v2)
- **Published**: 2016-09-05 08:48:01+00:00
- **Updated**: 2017-07-18 10:04:18+00:00
- **Authors**: Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara
- **Comment**: International Conference on Pattern Recognition (ICPR), 2016
- **Journal**: None
- **Summary**: This paper presents a novel deep architecture for saliency prediction. Current state of the art models for saliency prediction employ Fully Convolutional networks that perform a non-linear combination of features extracted from the last convolutional layer to predict saliency maps. We propose an architecture which, instead, combines features extracted at different levels of a Convolutional Neural Network (CNN). Our model is composed of three main blocks: a feature extraction CNN, a feature encoding network, that weights low and high level feature maps, and a prior learning network. We compare our solution with state of the art saliency models on two public benchmarks datasets. Results show that our model outperforms under all evaluation metrics on the SALICON dataset, which is currently the largest public dataset for saliency prediction, and achieves competitive results on the MIT300 benchmark.



### A max-cut approach to heterogeneity in cryo-electron microscopy
- **Arxiv ID**: http://arxiv.org/abs/1609.01100v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, q-bio.BM
- **Links**: [PDF](http://arxiv.org/pdf/1609.01100v2)
- **Published**: 2016-09-05 11:08:34+00:00
- **Updated**: 2019-10-03 14:48:05+00:00
- **Authors**: Yariv Aizenbud, Yoel Shkolnisky
- **Comment**: None
- **Journal**: None
- **Summary**: The field of cryo-electron microscopy has made astounding advancements in the past few years, mainly due to advancements in electron detectors' technology. Yet, one of the key open challenges of the field remains the processing of heterogeneous data sets, produced from samples containing particles at several different conformational states. For such data sets, the algorithms must include some classification procedure to identify homogeneous groups within the data, so that the images in each group correspond to the same underlying structure. The fundamental importance of the heterogeneity problem in cryo-electron microscopy has drawn many research efforts, and resulted in significant progress in classification algorithms for heterogeneous data sets. While these algorithms are extremely useful and effective in practice, they lack rigorous mathematical analysis and performance guarantees.   In this paper, we attempt to make the first steps towards rigorous mathematical analysis of the heterogeneity problem in cryo-electron microscopy. To that end, we present an algorithm for processing heterogeneous data sets, and prove accuracy and stability bounds for it. We also suggest an extension of this algorithm that combines the classification and reconstruction steps. We demonstrate it on simulated data, and compare its performance to the state-of-the-art algorithm in RELION.



### Deep Retinal Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1609.01103v1
- **DOI**: 10.1007/978-3-319-46723-8_17
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01103v1)
- **Published**: 2016-09-05 11:20:30+00:00
- **Updated**: 2016-09-05 11:20:30+00:00
- **Authors**: Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez, Luc Van Gool
- **Comment**: MICCAI 2016 Camera Ready
- **Journal**: None
- **Summary**: This paper presents Deep Retinal Image Understanding (DRIU), a unified framework of retinal image analysis that provides both retinal vessel and optic disc segmentation. We make use of deep Convolutional Neural Networks (CNNs), which have proven revolutionary in other fields of computer vision such as object detection and image classification, and we bring their power to the study of eye fundus images. DRIU uses a base network architecture on which two set of specialized layers are trained to solve both the retinal vessel and optic disc segmentation. We present experimental validation, both qualitative and quantitative, in four public datasets for these tasks. In all of them, DRIU presents super-human performance, that is, it shows results more consistent with a gold standard than a second human annotator used as control.



### Reflections on Shannon Information: In search of a natural information-entropy for images
- **Arxiv ID**: http://arxiv.org/abs/1609.01117v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT, I.4.1; I.4.2; I.4.5; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1609.01117v1)
- **Published**: 2016-09-05 11:59:47+00:00
- **Updated**: 2016-09-05 11:59:47+00:00
- **Authors**: Kieran G. Larkin
- **Comment**: 47 pages,9 figures, preprint for submission to an image science and
  optics journal
- **Journal**: None
- **Summary**: It is not obvious how to extend Shannon's original information entropy to higher dimensions, and many different approaches have been tried. We replace the English text symbol sequence originally used to illustrate the theory by a discrete, bandlimited signal. Using Shannon's later theory of sampling we derive a new and symmetric version of the second order entropy in 1D. The new theory then naturally extends to 2D and higher dimensions, where by naturally we mean simple, symmetric, isotropic and parsimonious. Simplicity arises from the direct application of Shannon's joint entropy equalities and inequalities to the gradient (del) vector field image embodying the second order relations of the scalar image. Parsimony is guaranteed by halving of the vector data rate using Papoulis' generalized sampling expansion. The new 2D entropy measure, which we dub delentropy, is underpinned by a computable probability density function we call deldensity. The deldensity captures the underlying spatial image structure and pixel co-occurrence. It achieves this because each scalar image pixel value is nonlocally related to the entire gradient vector field. Both deldensity and delentropy are highly tractable and yield many interesting connections and useful inequalities. The new measure explicitly defines a realizable encoding algorithm and a corresponding reconstruction. Initial tests show that delentropy compares favourably with the conventional intensity-based histogram entropy and the compressed data rates of lossless image encoders (GIF, PNG, WEBP, JP2K-LS and JPG-LS) for a selection of images. The symmetric approach may have applications to higher dimensions and problems concerning image complexity measures.



### Towards Automated Melanoma Screening: Exploring Transfer Learning Schemes
- **Arxiv ID**: http://arxiv.org/abs/1609.01228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01228v1)
- **Published**: 2016-09-05 17:31:15+00:00
- **Updated**: 2016-09-05 17:31:15+00:00
- **Authors**: Afonso Menegola, Michel Fornaciali, Ramon Pires, Sandra Avila, Eduardo Valle
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is the current bet for image classification. Its greed for huge amounts of annotated data limits its usage in medical imaging context. In this scenario transfer learning appears as a prominent solution. In this report we aim to clarify how transfer learning schemes may influence classification results. We are particularly focused in the automated melanoma screening problem, a case of medical imaging in which transfer learning is still not widely used. We explored transfer with and without fine-tuning, sequential transfers and usage of pre-trained models in general and specific datasets. Although some issues remain open, our findings may drive future researches.



### UnrealCV: Connecting Computer Vision to Unreal Engine
- **Arxiv ID**: http://arxiv.org/abs/1609.01326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01326v1)
- **Published**: 2016-09-05 21:09:33+00:00
- **Updated**: 2016-09-05 21:09:33+00:00
- **Authors**: Weichao Qiu, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which: (i) an agent can perceive, navigate, and take actions guided by AI algorithms, (ii) properties of the worlds can be modified (e.g., material and reflectance), (iii) physical simulations can be performed, and (iv) algorithms can be learnt and evaluated. But creating realistic virtual worlds is not easy. The game industry, however, has spent a lot of effort creating 3D worlds, which a player can interact with. So researchers can build on these resources to create virtual worlds, provided we can access and modify the internal data structures of the games. To enable this we created an open-source plugin UnrealCV (http://unrealcv.github.io) for a popular game engine Unreal Engine 4 (UE4). We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms.



### Depth Reconstruction and Computer-Aided Polyp Detection in Optical Colonoscopy Video Frames
- **Arxiv ID**: http://arxiv.org/abs/1609.01329v2
- **DOI**: 10.1117/12.2216996
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01329v2)
- **Published**: 2016-09-05 21:12:34+00:00
- **Updated**: 2016-09-10 16:06:39+00:00
- **Authors**: Saad Nadeem, Arie Kaufman
- **Comment**: **The title has been modified to highlight the contributions more
  clearly. The original title is: "Computer-Aided Detection of Polyps in
  Optical Colonoscopy Images". Keywords: Machine learning, computer-aided
  detection, segmentation, endoscopy, colonoscopy, videos, polyp, detection,
  medical imaging, depth maps, 3D, reconstruction, computed tomography, virtual
  colonoscopy, colorectal cancer, SPIE Medical Imaging, 2016
- **Journal**: None
- **Summary**: We present a computer-aided detection algorithm for polyps in optical colonoscopy images. Polyps are the precursors to colon cancer. In the US alone, more than 14 million optical colonoscopies are performed every year, mostly to screen for polyps. Optical colonoscopy has been shown to have an approximately 25% polyp miss rate due to the convoluted folds and bends present in the colon. In this work, we present an automatic detection algorithm to detect these polyps in the optical colonoscopy images. We use a machine learning algorithm to infer a depth map for a given optical colonoscopy image and then use a detailed pre-built polyp profile to detect and delineate the boundaries of polyps in this given image. We have achieved the best recall of 84.0% and the best specificity value of 83.4%.



### Vision-based Engagement Detection in Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/1609.01344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01344v1)
- **Published**: 2016-09-05 22:24:08+00:00
- **Updated**: 2016-09-05 22:24:08+00:00
- **Authors**: Ghassem Tofighi, Kaamraan Raahemifar, Maria Frank, Haisong Gu
- **Comment**: Paper has been published in Digital Media Industry and Academic Forum
  2016 (DMIAF 2016) http://ieee-digitalmediaforum.org/
- **Journal**: None
- **Summary**: User engagement modeling for manipulating actions in vision-based interfaces is one of the most important case studies of user mental state detection. In a Virtual Reality environment that employs camera sensors to recognize human activities, we have to know when user intends to perform an action and when not. Without a proper algorithm for recognizing engagement status, any kind of activities could be interpreted as manipulating actions, called "Midas Touch" problem. Baseline approach for solving this problem is activating gesture recognition system using some focus gestures such as waiving or raising hand. However, a desirable natural user interface should be able to understand user's mental status automatically. In this paper, a novel multi-modal model for engagement detection, DAIA, is presented. using DAIA, the spectrum of mental status for performing an action is quantized in a finite number of engagement states. For this purpose, a Finite State Transducer (FST) is designed. This engagement framework shows how to integrate multi-modal information from user biometric data streams such as 2D and 3D imaging. FST is employed to make the state transition smoothly using combination of several boolean expressions. Our FST true detection rate is 92.3% in total for four different states. Results also show FST can segment user hand gestures more robustly.



### Efficient Volumetric Fusion of Airborne and Street-Side Data for Urban Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1609.01345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.01345v1)
- **Published**: 2016-09-05 22:28:49+00:00
- **Updated**: 2016-09-05 22:28:49+00:00
- **Authors**: András Bódis-Szomorú, Hayko Riemenschneider, Luc Van Gool
- **Comment**: To appear in ICPR 2016
- **Journal**: None
- **Summary**: Airborne acquisition and on-road mobile mapping provide complementary 3D information of an urban landscape: the former acquires roof structures, ground, and vegetation at a large scale, but lacks the facade and street-side details, while the latter is incomplete for higher floors and often totally misses out on pedestrian-only areas or undriven districts. In this work, we introduce an approach that efficiently unifies a detailed street-side Structure-from-Motion (SfM) or Multi-View Stereo (MVS) point cloud and a coarser but more complete point cloud from airborne acquisition in a joint surface mesh. We propose a point cloud blending and a volumetric fusion based on ray casting across a 3D tetrahedralization (3DT), extended with data reduction techniques to handle large datasets. To the best of our knowledge, we are the first to adopt a 3DT approach for airborne/street-side data fusion. Our pipeline exploits typical characteristics of airborne and ground data, and produces a seamless, watertight mesh that is both complete and detailed. Experiments on 3D urban data from multiple sources and different data densities show the effectiveness and benefits of our approach.




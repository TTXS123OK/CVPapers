# Arxiv Papers in cs.CV on 2016-09-15
### Matrix Product State for Higher-Order Tensor Compression and Classification
- **Arxiv ID**: http://arxiv.org/abs/1609.04541v1
- **DOI**: 10.1109/TSP.2017.2703882
- **Categories**: **stat.ML**, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1609.04541v1)
- **Published**: 2016-09-15 09:04:25+00:00
- **Updated**: 2016-09-15 09:04:25+00:00
- **Authors**: Johann A. Bengua, Ho N. Phien, Hoang D. Tuan, Minh N. Do
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: This paper introduces matrix product state (MPS) decomposition as a new and systematic method to compress multidimensional data represented by higher-order tensors. It solves two major bottlenecks in tensor compression: computation and compression quality. Regardless of tensor order, MPS compresses tensors to matrices of moderate dimension which can be used for classification. Mainly based on a successive sequence of singular value decompositions (SVD), MPS is quite simple to implement and arrives at the global optimal matrix, bypassing local alternating optimization, which is not only computationally expensive but cannot yield the global solution. Benchmark results show that MPS can achieve better classification performance with favorable computation cost compared to other tensor compression methods.



### Context Aware Nonnegative Matrix Factorization Clustering
- **Arxiv ID**: http://arxiv.org/abs/1609.04628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/1609.04628v1)
- **Published**: 2016-09-15 13:23:43+00:00
- **Updated**: 2016-09-15 13:23:43+00:00
- **Authors**: Rocco Tripodi, Sebastiano Vascon, Marcello Pelillo
- **Comment**: 6 pages, 3 figures. Full paper accepted to International Conference
  on Pattern Recognition ICPR 2016, Canc\'un, Mexico
- **Journal**: None
- **Summary**: In this article we propose a method to refine the clustering results obtained with the nonnegative matrix factorization (NMF) technique, imposing consistency constraints on the final labeling of the data. The research community focused its effort on the initialization and on the optimization part of this method, without paying attention to the final cluster assignments. We propose a game theoretic framework in which each object to be clustered is represented as a player, which has to choose its cluster membership. The information obtained with NMF is used to initialize the strategy space of the players and a weighted graph is used to model the interactions among the players. These interactions allow the players to choose a cluster which is coherent with the clusters chosen by similar players, a property which is not guaranteed by NMF, since it produces a soft clustering of the data. The results on common benchmarks show that our model is able to improve the performances of many NMF formulations.



### Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1609.04653v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.04653v1)
- **Published**: 2016-09-15 14:01:03+00:00
- **Updated**: 2016-09-15 14:01:03+00:00
- **Authors**: Peter Pinggera, Sebastian Ramos, Stefan Gehrig, Uwe Franke, Carsten Rother, Rudolf Mester
- **Comment**: To be presented at IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2016
- **Journal**: None
- **Summary**: Detecting small obstacles on the road ahead is a critical part of the driving task which has to be mastered by fully autonomous cars. In this paper, we present a method based on stereo vision to reliably detect such obstacles from a moving vehicle. The proposed algorithm performs statistical hypothesis tests in disparity space directly on stereo image data, assessing freespace and obstacle hypotheses on independent local patches. This detection approach does not depend on a global road model and handles both static and moving obstacles. For evaluation, we employ a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic. The proposed approach outperforms all considered baselines in our evaluations on both pixel and object level and runs at frame rates of up to 20 Hz on 2 mega-pixel stereo imagery. Small obstacles down to the height of 5 cm can successfully be detected at 20 m distance at low false positive rates.



### Improving the Accuracy of Stereo Visual Odometry Using Visual Illumination Estimation
- **Arxiv ID**: http://arxiv.org/abs/1609.04705v3
- **DOI**: 10.1007/978-3-319-50115-4_36
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1609.04705v3)
- **Published**: 2016-09-15 15:44:25+00:00
- **Updated**: 2019-08-08 13:17:36+00:00
- **Authors**: Lee Clement, Valentin Peretroukhin, Jonathan Kelly
- **Comment**: In: Kuli\'c D., Nakamura Y., Khatib O., Venture G. (eds) 2016
  International Symposium on Experimental Robotics. ISER 2016. Springer
  Proceedings in Advanced Robotics, vol 1. Springer, Cham
- **Journal**: None
- **Summary**: In the absence of reliable and accurate GPS, visual odometry (VO) has emerged as an effective means of estimating the egomotion of robotic vehicles. Like any dead-reckoning technique, VO suffers from unbounded accumulation of drift error over time, but this accumulation can be limited by incorporating absolute orientation information from, for example, a sun sensor. In this paper, we leverage recent work on visual outdoor illumination estimation to show that estimation error in a stereo VO pipeline can be reduced by inferring the sun position from the same image stream used to compute VO, thereby gaining the benefits of sun sensing without requiring a dedicated sun sensor or the sun to be visible to the camera. We compare sun estimation methods based on hand-crafted visual cues and Convolutional Neural Networks (CNNs) and demonstrate our approach on a combined 7.8 km of urban driving from the popular KITTI dataset, achieving up to a 43% reduction in translational average root mean squared error (ARMSE) and a 59% reduction in final translational drift error compared to pure VO alone.



### Perceptual Quality Prediction on Authentically Distorted Images Using a Bag of Features Approach
- **Arxiv ID**: http://arxiv.org/abs/1609.04757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04757v1)
- **Published**: 2016-09-15 18:06:21+00:00
- **Updated**: 2016-09-15 18:06:21+00:00
- **Authors**: Deepti Ghadiyaram, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Current top-performing blind perceptual image quality prediction models are generally trained on legacy databases of human quality opinion scores on synthetically distorted images. Therefore they learn image features that effectively predict human visual quality judgments of inauthentic, and usually isolated (single) distortions. However, real-world images usually contain complex, composite mixtures of multiple distortions. We study the perceptually relevant natural scene statistics of such authentically distorted images, in different color spaces and transform domains. We propose a bag of feature-maps approach which avoids assumptions about the type of distortion(s) contained in an image, focusing instead on capturing consistencies, or departures therefrom, of the statistics of real world images. Using a large database of authentically distorted images, human opinions of them, and bags of features computed on them, we train a regressor to conduct image quality prediction. We demonstrate the competence of the features towards improving automatic perceptual quality prediction by testing a learned algorithm using them on a benchmark legacy database as well as on a newly introduced distortion-realistic resource called the LIVE In the Wild Image Quality Challenge Database. We extensively evaluate the perceptual quality prediction model and algorithm and show that it is able to achieve good quality prediction power that is better than other leading models.



### Transport-based analysis, modeling, and learning from signal and data distributions
- **Arxiv ID**: http://arxiv.org/abs/1609.04767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04767v1)
- **Published**: 2016-09-15 18:28:50+00:00
- **Updated**: 2016-09-15 18:28:50+00:00
- **Authors**: Soheil Kolouri, Serim Park, Matthew Thorpe, Dejan Slepčev, Gustavo K. Rohde
- **Comment**: None
- **Journal**: None
- **Summary**: Transport-based techniques for signal and data analysis have received increased attention recently. Given their abilities to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications including content-based retrieval, cancer detection, image super-resolution, and statistical machine learning, to name a few, and shown to produce state of the art in several applications. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here we provide an overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications.



### Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1609.04802v5
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1609.04802v5)
- **Published**: 2016-09-15 19:53:07+00:00
- **Updated**: 2017-05-25 11:25:41+00:00
- **Authors**: Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi
- **Comment**: 19 pages, 15 figures, 2 tables, accepted for oral presentation at
  CVPR, main paper + some supplementary material
- **Journal**: None
- **Summary**: Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.



### Visible Light-Based Human Visual System Conceptual Model
- **Arxiv ID**: http://arxiv.org/abs/1609.04830v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1609.04830v5)
- **Published**: 2016-09-15 20:00:56+00:00
- **Updated**: 2019-08-19 08:48:34+00:00
- **Authors**: Lee Prangnell
- **Comment**: Discussion Paper, Department of Computer Science, University of
  Warwick, UK
- **Journal**: None
- **Summary**: There exists a widely accepted set of assertions in the digital image and video coding literature, which are as follows: the Human Visual System (HVS) is more sensitive to luminance (often confused with brightness) than photon energies (often confused with chromaticity and chrominance). Passages similar to the following occur with high frequency in the peer reviewed literature and academic text books: "the HVS is much more sensitive to brightness than colour" and/or "the HVS is much more sensitive to luma than chroma". In this discussion paper, a Visible Light-Based Human Visual System (VL-HVS) conceptual model is discussed. The objectives of VL-HVS are as follows: 1. To provide a deeper theoretical reflection of the fundamental relationship between visible light, the manifestation of colour perception derived from visible light and the physiology of the perception of colour. That is, in terms of the physics of visible light, photobiology and the human subjective interpretation of visible light, it is appropriate to provide comprehensive background information in relation to the natural interactions between visible light, the retinal photoreceptors and the subsequent cortical processing of such. 2. To provide a more wholesome account with respect to colour information in digital image and video processing applications. 3. To recontextualise colour data in the RGB and YCbCr colour spaces, such that novel techniques in digital image and video processing, including quantisation and artifact reduction techniques, may be developed based on both luma and chroma information (not luma data only).



### A Glimpse Far into the Future: Understanding Long-term Crowd Worker Quality
- **Arxiv ID**: http://arxiv.org/abs/1609.04855v2
- **DOI**: 10.1145/2998181.2998248
- **Categories**: **cs.HC**, cs.CV, H.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1609.04855v2)
- **Published**: 2016-09-15 20:47:51+00:00
- **Updated**: 2016-11-01 17:34:10+00:00
- **Authors**: Kenji Hata, Ranjay Krishna, Li Fei-Fei, Michael S. Bernstein
- **Comment**: 10 pages, 11 figures, accepted CSCW 2017
- **Journal**: None
- **Summary**: Microtask crowdsourcing is increasingly critical to the creation of extremely large datasets. As a result, crowd workers spend weeks or months repeating the exact same tasks, making it necessary to understand their behavior over these long periods of time. We utilize three large, longitudinal datasets of nine million annotations collected from Amazon Mechanical Turk to examine claims that workers fatigue or satisfice over these long periods, producing lower quality work. We find that, contrary to these claims, workers are extremely stable in their quality over the entire period. To understand whether workers set their quality based on the task's requirements for acceptance, we then perform an experiment where we vary the required quality for a large crowdsourcing task. Workers did not adjust their quality based on the acceptance threshold: workers who were above the threshold continued working at their usual quality level, and workers below the threshold self-selected themselves out of the task. Capitalizing on this consistency, we demonstrate that it is possible to predict workers' long-term quality using just a glimpse of their quality on the first five tasks.



### Visual Stability Prediction and Its Application to Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1609.04861v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1609.04861v2)
- **Published**: 2016-09-15 21:12:41+00:00
- **Updated**: 2016-09-26 10:19:44+00:00
- **Authors**: Wenbin Li, Aleš Leonardis, Mario Fritz
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1604.00066
- **Journal**: None
- **Summary**: Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel objects and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage.   In this paper, we contrast a more traditional approach of taking a model-based route with explicit 3D representations and physical simulation by an {\em end-to-end} approach that directly predicts stability from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way---bypassing the need for an explicit simulation at run-time.   We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. We first evaluate the approach on synthetic data and compared the results to human judgments on the same stimuli. Further, we extend this approach to reason about future states of such towers that in turn enables successful stacking.




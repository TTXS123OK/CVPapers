# Arxiv Papers in cs.CV on 2020-05-23
### S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.11437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11437v1)
- **Published**: 2020-05-23 00:44:38+00:00
- **Updated**: 2020-05-23 00:44:38+00:00
- **Authors**: Yizhe Zhu, Martin Renqiang Min, Asim Kadav, Hans Peter Graf
- **Comment**: to appear in CVPR2020
- **Journal**: None
- **Summary**: We propose a sequential variational autoencoder to learn disentangled representations of sequential data (e.g., videos and audios) under self-supervision. Specifically, we exploit the benefits of some readily accessible supervisory signals from input data itself or some off-the-shelf functional models and accordingly design auxiliary tasks for our model to utilize these signals. With the supervision of the signals, our model can easily disentangle the representation of an input sequence into static factors and dynamic factors (i.e., time-invariant and time-varying parts). Comprehensive experiments across videos and audios verify the effectiveness of our model on representation disentanglement and generation of sequential data, and demonstrate that, our model with self-supervision performs comparable to, if not better than, the fully-supervised model with ground truth labels, and outperforms state-of-the-art unsupervised models by a large margin.



### Fine-Grain Few-Shot Vision via Domain Knowledge as Hyperspherical Priors
- **Arxiv ID**: http://arxiv.org/abs/2005.11450v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.11450v1)
- **Published**: 2020-05-23 02:10:57+00:00
- **Updated**: 2020-05-23 02:10:57+00:00
- **Authors**: Bijan Haney, Alexander Lavin
- **Comment**: None
- **Journal**: CVPR 2020 Workshop on Fine-Grained Visual Categorization
- **Summary**: Prototypical networks have been shown to perform well at few-shot learning tasks in computer vision. Yet these networks struggle when classes are very similar to each other (fine-grain classification) and currently have no way of taking into account prior knowledge (through the use of tabular data). Using a spherical latent space to encode prototypes, we can achieve few-shot fine-grain classification by maximally separating the classes while incorporating domain knowledge as informative priors. We describe how to construct a hypersphere of prototypes that embed a-priori domain information, and demonstrate the effectiveness of the approach on challenging benchmark datasets for fine-grain classification, with top results for one-shot classification and 5x speedups in training time.



### Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2006.10132v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.10132v2)
- **Published**: 2020-05-23 03:50:27+00:00
- **Updated**: 2020-07-23 15:07:58+00:00
- **Authors**: Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial nets (GANs) have been successfully applied in many fields like image generation, inpainting, super-resolution and drug discovery, etc., by now, the inner process of GANs is far from been understood. To get deeper insight of the intrinsic mechanism of GANs, in this paper, a method for interpreting the latent space of GANs by analyzing the correlation between latent variables and the corresponding semantic contents in generated images is proposed. Unlike previous methods that focus on dissecting models via feature visualization, the emphasis of this work is put on the variables in latent space, i.e. how the latent variables affect the quantitative analysis of generated results. Given a pretrained GAN model with weights fixed, the latent variables are intervened to analyze their effect on the semantic content in generated images. A set of controlling latent variables can be derived for specific content generation, and the controllable semantic content manipulation be achieved. The proposed method is testified on the datasets Fashion-MNIST and UT Zappos50K, experiment results show its effectiveness.



### Learning from Naturalistic Driving Data for Human-like Autonomous Highway Driving
- **Arxiv ID**: http://arxiv.org/abs/2005.11470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.11470v1)
- **Published**: 2020-05-23 04:39:39+00:00
- **Updated**: 2020-05-23 04:39:39+00:00
- **Authors**: Donghao Xu, Zhezhang Ding, Xu He, Huijing Zhao, Mathieu Moze, FranÃ§ois Aioun, Franck Guillemard
- **Comment**: 14 pages, 9 figures. Submitted to T.ITS
- **Journal**: None
- **Summary**: Driving in a human-like manner is important for an autonomous vehicle to be a smart and predictable traffic participant. To achieve this goal, parameters of the motion planning module should be carefully tuned, which needs great effort and expert knowledge. In this study, a method of learning cost parameters of a motion planner from naturalistic driving data is proposed. The learning is achieved by encouraging the selected trajectory to approximate the human driving trajectory under the same traffic situation. The employed motion planner follows a widely accepted methodology that first samples candidate trajectories in the trajectory space, then select the one with minimal cost as the planned trajectory. Moreover, in addition to traditional factors such as comfort, efficiency and safety, the cost function is proposed to incorporate incentive of behavior decision like a human driver, so that both lane change decision and motion planning are coupled into one framework. Two types of lane incentive cost -- heuristic and learning based -- are proposed and implemented. To verify the validity of the proposed method, a data set is developed by using the naturalistic trajectory data of human drivers collected on the motorways in Beijing, containing samples of lane changes to the left and right lanes, and car followings. Experiments are conducted with respect to both lane change decision and motion planning, and promising results are achieved.



### Delving into the Imbalance of Positive Proposals in Two-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11472v1)
- **Published**: 2020-05-23 04:54:59+00:00
- **Updated**: 2020-05-23 04:54:59+00:00
- **Authors**: Zheng Ge, Zequn Jie, Xin Huang, Chengzheng Li, Osamu Yoshie
- **Comment**: None
- **Journal**: None
- **Summary**: Imbalance issue is a major yet unsolved bottleneck for the current object detection models. In this work, we observe two crucial yet never discussed imbalance issues. The first imbalance lies in the large number of low-quality RPN proposals, which makes the R-CNN module (i.e., post-classification layers) become highly biased towards the negative proposals in the early training stage. The second imbalance stems from the unbalanced ground-truth numbers across different testing images, resulting in the imbalance of the number of potentially existing positive proposals in testing phase. To tackle these two imbalance issues, we incorporates two innovations into Faster R-CNN: 1) an R-CNN Gradient Annealing (RGA) strategy to enhance the impact of positive proposals in the early training stage. 2) a set of Parallel R-CNN Modules (PRM) with different positive/negative sampling ratios during training on one same backbone. Our RGA and PRM can totally bring 2.0% improvements on AP on COCO minival. Experiments on CrowdHuman further validates the effectiveness of our innovations across various kinds of object detection tasks.



### Attention-guided Context Feature Pyramid Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11475v1)
- **Published**: 2020-05-23 05:24:50+00:00
- **Updated**: 2020-05-23 05:24:50+00:00
- **Authors**: Junxu Cao, Qi Chen, Jun Guo, Ruichao Shi
- **Comment**: None
- **Journal**: None
- **Summary**: For object detection, how to address the contradictory requirement between feature map resolution and receptive field on high-resolution inputs still remains an open question. In this paper, to tackle this issue, we build a novel architecture, called Attention-guided Context Feature Pyramid Network (AC-FPN), that exploits discriminative information from various large receptive fields via integrating attention-guided multi-path features. The model contains two modules. The first one is Context Extraction Module (CEM) that explores large contextual information from multiple receptive fields. As redundant contextual relations may mislead localization and recognition, we also design the second module named Attention-guided Module (AM), which can adaptively capture the salient dependencies over objects by using the attention mechanism. AM consists of two sub-modules, i.e., Context Attention Module (CxAM) and Content Attention Module (CnAM), which focus on capturing discriminative semantics and locating precise positions, respectively. Most importantly, our AC-FPN can be readily plugged into existing FPN-based models. Extensive experiments on object detection and instance segmentation show that existing models with our proposed CEM and AM significantly surpass their counterparts without them, and our model successfully obtains state-of-the-art results. We have released the source code at https://github.com/Caojunxu/AC-FPN.



### Self-Training for Domain Adaptive Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11487v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11487v1)
- **Published**: 2020-05-23 07:36:23+00:00
- **Updated**: 2020-05-23 07:36:23+00:00
- **Authors**: Yudi Chen, Wei Wang, Yu Zhou, Fei Yang, Dongbao Yang, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Though deep learning based scene text detection has achieved great progress, well-trained detectors suffer from severe performance degradation for different domains. In general, a tremendous amount of data is indispensable to train the detector in the target domain. However, data collection and annotation are expensive and time-consuming. To address this problem, we propose a self-training framework to automatically mine hard examples with pseudo-labels from unannotated videos or images. To reduce the noise of hard examples, a novel text mining module is implemented based on the fusion of detection and tracking results. Then, an image-to-video generation method is designed for the tasks that videos are unavailable and only images can be used. Experimental results on standard benchmarks, including ICDAR2015, MSRA-TD500, ICDAR2017 MLT, demonstrate the effectiveness of our self-training method. The simple Mask R-CNN adapted with self-training and fine-tuned on real data can achieve comparable or even superior results with the state-of-the-art methods.



### AnimGAN: A Spatiotemporally-Conditioned Generative Adversarial Network for Character Animation
- **Arxiv ID**: http://arxiv.org/abs/2005.11489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11489v1)
- **Published**: 2020-05-23 07:47:46+00:00
- **Updated**: 2020-05-23 07:47:46+00:00
- **Authors**: Maryam Sadat Mirzaei, Kourosh Meshgi, Etienne Frigo, Toyoaki Nishida
- **Comment**: Submitted to ICIP 2020
- **Journal**: None
- **Summary**: Producing realistic character animations is one of the essential tasks in human-AI interactions. Considered as a sequence of poses of a humanoid, the task can be considered as a sequence generation problem with spatiotemporal smoothness and realism constraints. Additionally, we wish to control the behavior of AI agents by giving them what to do and, more specifically, how to do it. We proposed a spatiotemporally-conditioned GAN that generates a sequence that is similar to a given sequence in terms of semantics and spatiotemporal dynamics. Using LSTM-based generator and graph ConvNet discriminator, this system is trained end-to-end on a large gathered dataset of gestures, expressions, and actions. Experiments showed that compared to traditional conditional GAN, our method creates plausible, realistic, and semantically relevant humanoid animation sequences that match user expectations.



### Cubical Ripser: Software for computing persistent homology of image and volume data
- **Arxiv ID**: http://arxiv.org/abs/2005.12692v2
- **DOI**: None
- **Categories**: **cs.CV**, math.AT, 55N31 (primary), 68R01 (secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2005.12692v2)
- **Published**: 2020-05-23 08:25:49+00:00
- **Updated**: 2020-06-12 06:44:01+00:00
- **Authors**: Shizuo Kaji, Takeki Sudo, Kazushi Ahara
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Cubical Ripser for computing persistent homology of image and volume data (more precisely, weighted cubical complexes). To our best knowledge, Cubical Ripser is currently the fastest and the most memory-efficient program for computing persistent homology of weighted cubical complexes. We demonstrate our software with an example of image analysis in which persistent homology and convolutional neural networks are successfully combined. Our open-source implementation is available online.



### Deep Learning for Reliable Classification of COVID-19, MERS, and SARS from Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2005.11524v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11524v6)
- **Published**: 2020-05-23 12:22:28+00:00
- **Updated**: 2021-06-01 12:37:22+00:00
- **Authors**: Anas Tahir, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Uzair Khurshid, Farayi Musharavati, M. T. Islam, Serkan Kiranyaz, Muhammad E. H. Chowdhury
- **Comment**: 10 Figures, 4 Tables
- **Journal**: None
- **Summary**: Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly spreading Coronavirus infestation. Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome (MERS), which outbreak in 2002 and 2011, and the current COVID-19 pandemic are all from the same family of coronavirus. This work aims to classify COVID-19, SARS, and MERS chest X-ray (CXR) images using deep Convolutional Neural Networks (CNNs). A unique database was created, so-called QU-COVID-family, consisting of 423 COVID-19, 144 MERS, and 134 SARS CXR images. Besides, a robust COVID-19 recognition system was proposed to identify lung regions using a CNN segmentation model (U-Net), and then classify the segmented lung images as COVID-19, MERS, or SARS using a pre-trained CNN classifier. Furthermore, the Score-CAM visualization method was utilized to visualize classification output and understand the reasoning behind the decision of deep CNNs. Several Deep Learning classifiers were trained and tested; four outperforming algorithms were reported. Original and preprocessed images were used individually and all together as the input(s) to the networks. Two recognition schemes were considered: plain CXR classification and segmented CXR classification. For plain CXRs, it was observed that InceptionV3 outperforms other networks with a 3-channel scheme and achieves sensitivities of 99.5%, 93.1%, and 97% for classifying COVID-19, MERS, and SARS images, respectively. In contrast, for segmented CXRs, InceptionV3 outperformed using the original CXR dataset and achieved sensitivities of 96.94%, 79.68%, and 90.26% for classifying COVID-19, MERS, and SARS images, respectively. All networks showed high COVID-19 detection sensitivity (>96%) with the segmented lung images. This indicates the unique radiographic signature of COVID-19 cases in the eyes of AI, which is often a challenging task for medical doctors.



### ProAlignNet : Unsupervised Learning for Progressively Aligning Noisy Contours
- **Arxiv ID**: http://arxiv.org/abs/2005.11546v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11546v1)
- **Published**: 2020-05-23 14:56:14+00:00
- **Updated**: 2020-05-23 14:56:14+00:00
- **Authors**: VSR Veeravasarapu, Abhishek Goel, Deepak Mittal, Maneesh Singh
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: Contour shape alignment is a fundamental but challenging problem in computer vision, especially when the observations are partial, noisy, and largely misaligned. Recent ConvNet-based architectures that were proposed to align image structures tend to fail with contour representation of shapes, mostly due to the use of proximity-insensitive pixel-wise similarity measures as loss functions in their training processes. This work presents a novel ConvNet, "ProAlignNet" that accounts for large scale misalignments and complex transformations between the contour shapes. It infers the warp parameters in a multi-scale fashion with progressively increasing complex transformations over increasing scales. It learns --without supervision-- to align contours, agnostic to noise and missing parts, by training with a novel loss function which is derived an upperbound of a proximity-sensitive and local shape-dependent similarity metric that uses classical Morphological Chamfer Distance Transform. We evaluate the reliability of these proposals on a simulated MNIST noisy contours dataset via some basic sanity check experiments. Next, we demonstrate the effectiveness of the proposed models in two real-world applications of (i) aligning geo-parcel data to aerial image maps and (ii) refining coarsely annotated segmentation labels. In both applications, the proposed models consistently perform superior to state-of-the-art methods.



### Self-supervised Robust Object Detectors from Partially Labelled Datasets
- **Arxiv ID**: http://arxiv.org/abs/2005.11549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11549v2)
- **Published**: 2020-05-23 15:18:20+00:00
- **Updated**: 2020-06-27 03:56:12+00:00
- **Authors**: Mahdieh Abbasi, Denis Laurendeau, Christian Gagne
- **Comment**: None
- **Journal**: None
- **Summary**: In the object detection task, merging various datasets from similar contexts but with different sets of Objects of Interest (OoI) is an inexpensive way (in terms of labor cost) for crafting a large-scale dataset covering a wide range of objects. Moreover, merging datasets allows us to train one integrated object detector, instead of training several ones, which in turn resulting in the reduction of computational and time costs. However, merging the datasets from similar contexts causes samples with partial labeling as each constituent dataset is originally annotated for its own set of OoI and ignores to annotate those objects that are become interested after merging the datasets. With the goal of training \emph{one integrated robust object detector with high generalization performance}, we propose a training framework to overcome missing-label challenge of the merged datasets. More specifically, we propose a computationally efficient self-supervised framework to create on-the-fly pseudo-labels for the unlabeled positive instances in the merged dataset in order to train the object detector jointly on both ground truth and pseudo labels. We evaluate our proposed framework for training Yolo on a simulated merged dataset with missing rate $\approx\!48\%$ using VOC2012 and VOC2007. We empirically show that generalization performance of Yolo trained on both ground truth and the pseudo-labels created by our method is on average $4\%$ higher than the ones trained only with the ground truth labels of the merged dataset.



### Underwater object detection using Invert Multi-Class Adaboost with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2005.11552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.11552v1)
- **Published**: 2020-05-23 15:30:38+00:00
- **Updated**: 2020-05-23 15:30:38+00:00
- **Authors**: Long Chen, Zhihua Liu, Lei Tong, Zheheng Jiang, Shengke Wang, Junyu Dong, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning based methods have achieved promising performance in standard object detection. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) Objects in real applications are usually small and their images are blurry, and (2) images in the underwater datasets and real applications accompany heterogeneous noise. To address these two problems, we first propose a novel neural network architecture, namely Sample-WeIghted hyPEr Network (SWIPENet), for small object detection. SWIPENet consists of high resolution and semantic rich Hyper Feature Maps which can significantly improve small object detection accuracy. In addition, we propose a novel sample-weighted loss function which can model sample weights for SWIPENet, which uses a novel sample re-weighting algorithm, namely Invert Multi-Class Adaboost (IMA), to reduce the influence of noise on the proposed SWIPENet. Experiments on two underwater robot picking contest datasets URPC2017 and URPC2018 show that the proposed SWIPENet+IMA framework achieves better performance in detection accuracy against several state-of-the-art object detection approaches.



### 3D geometric moment invariants from the point of view of the classical invariant theory
- **Arxiv ID**: http://arxiv.org/abs/2006.05674v1
- **DOI**: 10.30970/ms.58.2.115-132
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05674v1)
- **Published**: 2020-05-23 15:40:53+00:00
- **Updated**: 2020-05-23 15:40:53+00:00
- **Authors**: Leonid Bedratyuk
- **Comment**: 19 pages
- **Journal**: Matematychni Studii, 58(2), 115-132 (2022)
- **Summary**: The aim of this paper is to clear up the problem of the connection between the 3D geometric moments invariants and the invariant theory, considering a problem of describing of the 3D geometric moments invariants as a problem of the classical invariant theory. Using the remarkable fact that the groups $SO(3)$ and $SL(2)$ are locally isomorphic, we reduced the problem of deriving 3D geometric moments invariants to the well-known problem of the classical invariant theory. We give a precise statement of the 3D geometric invariant moments computation, introducing the notions of the algebras of simultaneous 3D geometric moment invariants, and prove that they are isomorphic to the algebras of joint $SL(2)$-invariants of several binary forms. To simplify the calculating of the invariants we proceed from an action of Lie group $SO(3)$ to an action of its Lie algebra $\mathfrak{sl}_2$. The author hopes that the results will be useful to the researchers in the fields of image analysis and pattern recognition.



### Invariant 3D Shape Recognition using Predictive Modular Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.11558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2005.11558v1)
- **Published**: 2020-05-23 16:16:37+00:00
- **Updated**: 2020-05-23 16:16:37+00:00
- **Authors**: Vasileios Petridis
- **Comment**: 17 pages, 2 figures
- **Journal**: None
- **Summary**: In this paper PREMONN (PREdictive MOdular Neural Networks) model/architecture is generalized to functions of two variables and to non-Euclidean spaces. It is presented in the context of 3D invariant shape recognition and texture recognition. PREMONN uses local relation, it is modular and exhibits incremental learning. The recognition process can start at any point on a shape or texture, so a reference point is not needed. Its local relation characteristic enables it to recognize shape and texture even in presence of occlusion. The analysis is mainly mathematical. However, we present some experimental results. The methods presented in this paper can be applied to many problems such as gesture recognition, action recognition, dynamic texture recognition etc.



### Hierarchical Feature Embedding for Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.11576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11576v1)
- **Published**: 2020-05-23 17:52:41+00:00
- **Updated**: 2020-05-23 17:52:41+00:00
- **Authors**: Jie Yang, Jiarou Fan, Yiru Wang, Yige Wang, Weihao Gan, Lin Liu, Wei Wu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Attribute recognition is a crucial but challenging task due to viewpoint changes, illumination variations and appearance diversities, etc. Most of previous work only consider the attribute-level feature embedding, which might perform poorly in complicated heterogeneous conditions. To address this problem, we propose a hierarchical feature embedding (HFE) framework, which learns a fine-grained feature embedding by combining attribute and ID information. In HFE, we maintain the inter-class and intra-class feature embedding simultaneously. Not only samples with the same attribute but also samples with the same ID are gathered more closely, which could restrict the feature embedding of visually hard samples with regard to attributes and improve the robustness to variant conditions. We establish this hierarchical structure by utilizing HFE loss consisted of attribute-level and ID-level constraints. We also introduce an absolute boundary regularization and a dynamic loss weight as supplementary components to help build up the feature embedding. Experiments show that our method achieves the state-of-the-art results on two pedestrian attribute datasets and a facial attribute dataset.



### Revisiting Street-to-Aerial View Image Geo-localization and Orientation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.11592v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11592v2)
- **Published**: 2020-05-23 19:52:24+00:00
- **Updated**: 2020-12-07 02:15:30+00:00
- **Authors**: Sijie Zhu, Taojiannan Yang, Chen Chen
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Street-to-aerial image geo-localization, which matches a query street-view image to the GPS-tagged aerial images in a reference set, has attracted increasing attention recently. In this paper, we revisit this problem and point out the ignored issue about image alignment information. We show that the performance of a simple Siamese network is highly dependent on the alignment setting and the comparison of previous works can be unfair if they have different assumptions. Instead of focusing on the feature extraction under the alignment assumption, we show that improvements in metric learning techniques significantly boost the performance regardless of the alignment. Without leveraging the alignment information, our pipeline outperforms previous works on both panorama and cropped datasets. Furthermore, we conduct visualization to help understand the learned model and the effect of alignment information using Grad-CAM. With our discovery on the approximate rotation-invariant activation maps, we propose a novel method to estimate the orientation/alignment between a pair of cross-view images with unknown alignment information. It achieves state-of-the-art results on the CVUSA dataset.



### One-Shot Unsupervised Cross-Domain Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.11610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11610v1)
- **Published**: 2020-05-23 22:12:20+00:00
- **Updated**: 2020-05-23 22:12:20+00:00
- **Authors**: Antonio D'Innocente, Francesco Cappio Borlino, Silvia Bucci, Barbara Caputo, Tatiana Tommasi
- **Comment**: None
- **Journal**: None
- **Summary**: Despite impressive progress in object detection over the last years, it is still an open challenge to reliably detect objects across visual domains. Although the topic has attracted attention recently, current approaches all rely on the ability to access a sizable amount of target data for use at training time. This is a heavy assumption, as often it is not possible to anticipate the domain where a detector will be used, nor to access it in advance for data acquisition. Consider for instance the task of monitoring image feeds from social media: as every image is created and uploaded by a different user it belongs to a different target domain that is impossible to foresee during training. This paper addresses this setting, presenting an object detection algorithm able to perform unsupervised adaption across domains by using only one target sample, seen at test time. We achieve this by introducing a multi-task architecture that one-shot adapts to any incoming sample by iteratively solving a self-supervised task on it. We further enhance this auxiliary adaptation with cross-task pseudo-labeling. A thorough benchmark analysis against the most recent cross-domain detection methods and a detailed ablation study show the advantage of our method, which sets the state-of-the-art in the defined one-shot scenario.



### Bayesian Neural Networks at Scale: A Performance Analysis and Pruning Study
- **Arxiv ID**: http://arxiv.org/abs/2005.11619v2
- **DOI**: 10.1007/s11227-020-03401-z
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11619v2)
- **Published**: 2020-05-23 23:15:34+00:00
- **Updated**: 2020-05-28 23:18:54+00:00
- **Authors**: Himanshu Sharma, Elise Jennings
- **Comment**: None
- **Journal**: Journal of Super Computing (2020)
- **Summary**: Bayesian neural Networks (BNNs) are a promising method of obtaining statistical uncertainties for neural network predictions but with a higher computational overhead which can limit their practical usage. This work explores the use of high performance computing with distributed training to address the challenges of training BNNs at scale. We present a performance and scalability comparison of training the VGG-16 and Resnet-18 models on a Cray-XC40 cluster. We demonstrate that network pruning can speed up inference without accuracy loss and provide an open source software package, {\it{BPrune}} to automate this pruning. For certain models we find that pruning up to 80\% of the network results in only a 7.0\% loss in accuracy. With the development of new hardware accelerators for Deep Learning, BNNs are of considerable interest for benchmarking performance. This analysis of training a BNN at scale outlines the limitations and benefits compared to a conventional neural network.



### Unsupervised Geometric Disentanglement for Surfaces via CFAN-VAE
- **Arxiv ID**: http://arxiv.org/abs/2005.11622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.11622v2)
- **Published**: 2020-05-23 23:28:10+00:00
- **Updated**: 2020-12-10 01:50:38+00:00
- **Authors**: N. Joseph Tatro, Stefan C. Schonsheck, Rongjie Lai
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: Geometric disentanglement, the separation of latent codes for intrinsic (i.e. identity) and extrinsic(i.e. pose) geometry, is a prominent task for generative models of non-Euclidean data such as 3D deformable models. It provides greater interpretability of the latent space, and leads to more control in generation. This work introduces a mesh feature, the conformal factor and normal feature (CFAN),for use in mesh convolutional autoencoders. We further propose CFAN-VAE, a novel architecture that disentangles identity and pose using the CFAN feature. Requiring no label information on the identity or pose during training, CFAN-VAE achieves geometric disentanglement in an unsupervisedway. Our comprehensive experiments, including reconstruction, interpolation, generation, and identity/pose transfer, demonstrate CFAN-VAE achieves state-of-the-art performance on unsupervised geometric disentanglement. We also successfully detect a level of geometric disentanglement in mesh convolutional autoencoders that encode xyz-coordinates directly by registering its latent space to that of CFAN-VAE.



### RAPiD: Rotation-Aware People Detection in Overhead Fisheye Images
- **Arxiv ID**: http://arxiv.org/abs/2005.11623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.11623v1)
- **Published**: 2020-05-23 23:47:18+00:00
- **Updated**: 2020-05-23 23:47:18+00:00
- **Authors**: Zhihao Duan, M. Ozan Tezcan, Hayato Nakamura, Prakash Ishwar, Janusz Konrad
- **Comment**: CVPR 2020 OmniCV Workshop paper extended version
- **Journal**: None
- **Summary**: Recent methods for people detection in overhead, fisheye images either use radially-aligned bounding boxes to represent people, assuming people always appear along image radius or require significant pre-/post-processing which radically increases computational complexity. In this work, we develop an end-to-end rotation-aware people detection method, named RAPiD, that detects people using arbitrarily-oriented bounding boxes. Our fully-convolutional neural network directly regresses the angle of each bounding box using a periodic loss function, which accounts for angle periodicities. We have also created a new dataset with spatio-temporal annotations of rotated bounding boxes, for people detection as well as other vision tasks in overhead fisheye videos. We show that our simple, yet effective method outperforms state-of-the-art results on three fisheye-image datasets. Code and dataset are available at http://vip.bu.edu/rapid .




# Arxiv Papers in cs.CV on 2020-05-06
### Trajectory Prediction for Autonomous Driving based on Multi-Head Attention with Joint Agent-Map Representation
- **Arxiv ID**: http://arxiv.org/abs/2005.02545v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.02545v3)
- **Published**: 2020-05-06 00:39:45+00:00
- **Updated**: 2020-09-02 22:41:33+00:00
- **Authors**: Kaouther Messaoud, Nachiket Deo, Mohan M. Trivedi, Fawzi Nashashibi
- **Comment**: Revised submission for RA-L
- **Journal**: None
- **Summary**: Predicting the trajectories of surrounding agents is an essential ability for autonomous vehicles navigating through complex traffic scenes. The future trajectories of agents can be inferred using two important cues: the locations and past motion of agents, and the static scene structure. Due to the high variability in scene structure and agent configurations, prior work has employed the attention mechanism, applied separately to the scene and agent configuration to learn the most salient parts of both cues. However, the two cues are tightly linked. The agent configuration can inform what part of the scene is most relevant to prediction. The static scene in turn can help determine the relative influence of agents on each other's motion. Moreover, the distribution of future trajectories is multimodal, with modes corresponding to the agent's intent. The agent's intent also informs what part of the scene and agent configuration is relevant to prediction. We thus propose a novel approach applying multi-head attention by considering a joint representation of the static scene and surrounding agents. We use each attention head to generate a distinct future trajectory to address multimodality of future trajectories. Our model achieves state of the art results on the nuScenes prediction benchmark and generates diverse future trajectories compliant with scene structure and agent configuration.



### AIBench Scenario: Scenario-distilling AI Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2005.03459v4
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03459v4)
- **Published**: 2020-05-06 01:24:25+00:00
- **Updated**: 2021-09-05 13:44:38+00:00
- **Authors**: Wanling Gao, Fei Tang, Jianfeng Zhan, Xu Wen, Lei Wang, Zheng Cao, Chuanxin Lan, Chunjie Luo, Xiaoli Liu, Zihan Jiang
- **Comment**: This paper has been accepted by The 30th International Conference on
  Parallel Architectures and Compilation Techniques (PACT 2021)
- **Journal**: None
- **Summary**: Modern real-world application scenarios like Internet services consist of a diversity of AI and non-AI modules with huge code sizes and long and complicated execution paths, which raises serious benchmarking or evaluating challenges. Using AI components or micro benchmarks alone can lead to error-prone conclusions. This paper presents a methodology to attack the above challenge. We formalize a real-world application scenario as a Directed Acyclic Graph-based model and propose the rules to distill it into a permutation of essential AI and non-AI tasks, which we call a scenario benchmark. Together with seventeen industry partners, we extract nine typical scenario benchmarks. We design and implement an extensible, configurable, and flexible benchmark framework. We implement two Internet service AI scenario benchmarks based on the framework as proxies to two real-world application scenarios. We consider scenario, component, and micro benchmarks as three indispensable parts for evaluating. Our evaluation shows the advantage of our methodology against using component or micro AI benchmarks alone. The specifications, source code, testbed, and results are publicly available from \url{https://www.benchcouncil.org/aibench/scenario/}.



### CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement
- **Arxiv ID**: http://arxiv.org/abs/2005.02551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02551v1)
- **Published**: 2020-05-06 01:38:03+00:00
- **Updated**: 2020-05-06 01:38:03+00:00
- **Authors**: Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Accepted to CVPR2020. Project page:
  https://github.com/hkchengrex/CascadePSP
- **Journal**: None
- **Summary**: State-of-the-art semantic segmentation methods were almost exclusively trained on images within a fixed resolution range. These segmentations are inaccurate for very high-resolution images since using bicubic upsampling of low-resolution segmentation does not adequately capture high-resolution details along object boundaries. In this paper, we propose a novel approach to address the high-resolution segmentation problem without using any high-resolution training data. The key insight is our CascadePSP network which refines and corrects local boundaries whenever possible. Although our network is trained with low-resolution segmentation data, our method is applicable to any resolution even for very high-resolution images larger than 4K. We present quantitative and qualitative studies on different datasets to show that CascadePSP can reveal pixel-accurate segmentation boundaries using our novel refinement module without any finetuning. Thus, our method can be regarded as class-agnostic. Finally, we demonstrate the application of our model to scene parsing in multi-class segmentation.



### Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder
- **Arxiv ID**: http://arxiv.org/abs/2005.02552v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02552v1)
- **Published**: 2020-05-06 01:40:26+00:00
- **Updated**: 2020-05-06 01:40:26+00:00
- **Authors**: Guanlin Li, Shuya Ding, Jun Luo, Chang Liu
- **Comment**: None
- **Journal**: CVPR 2020
- **Summary**: Whereas adversarial training is employed as the main defence strategy against specific adversarial samples, it has limited generalization capability and incurs excessive time complexity. In this paper, we propose an attack-agnostic defence framework to enhance the intrinsic robustness of neural networks, without jeopardizing the ability of generalizing clean samples. Our Feature Pyramid Decoder (FPD) framework applies to all block-based convolutional neural networks (CNNs). It implants denoising and image restoration modules into a targeted CNN, and it also constraints the Lipschitz constant of the classification layer. Moreover, we propose a two-phase strategy to train the FPD-enhanced CNN, utilizing $\epsilon$-neighbourhood noisy images with multi-task and self-supervised learning. Evaluated against a variety of white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain sufficient robustness against general adversarial samples on MNIST, SVHN and CALTECH. In addition, if we further conduct adversarial training, the FPD-enhanced CNNs perform better than their non-enhanced versions.



### Collective Loss Function for Positive and Unlabeled Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.03228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03228v1)
- **Published**: 2020-05-06 03:30:22+00:00
- **Updated**: 2020-05-06 03:30:22+00:00
- **Authors**: Chenhao Xie, Qiao Cheng, Jiaqing Liang, Lihan Chen, Yanghua Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: People learn to discriminate between classes without explicit exposure to negative examples. On the contrary, traditional machine learning algorithms often rely on negative examples, otherwise the model would be prone to collapse and always-true predictions. Therefore, it is crucial to design the learning objective which leads the model to converge and to perform predictions unbiasedly without explicit negative signals. In this paper, we propose a Collectively loss function to learn from only Positive and Unlabeled data (cPU). We theoretically elicit the loss function from the setting of PU learning. We perform intensive experiments on the benchmark and real-world datasets. The results show that cPU consistently outperforms the current state-of-the-art PU learning methods.



### Unsupervised Pre-trained Models from Healthy ADLs Improve Parkinson's Disease Classification of Gait Patterns
- **Arxiv ID**: http://arxiv.org/abs/2005.02589v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02589v2)
- **Published**: 2020-05-06 04:08:19+00:00
- **Updated**: 2020-05-07 00:56:01+00:00
- **Authors**: Anirudh Som, Narayanan Krishnamurthi, Matthew Buman, Pavan Turaga
- **Comment**: Accepted in the 42nd Annual International Conferences of the IEEE
  Engineering in Medicine and Biology Society (EMBC 2020)
- **Journal**: None
- **Summary**: Application and use of deep learning algorithms for different healthcare applications is gaining interest at a steady pace. However, use of such algorithms can prove to be challenging as they require large amounts of training data that capture different possible variations. This makes it difficult to use them in a clinical setting since in most health applications researchers often have to work with limited data. Less data can cause the deep learning model to over-fit. In this paper, we ask how can we use data from a different environment, different use-case, with widely differing data distributions. We exemplify this use case by using single-sensor accelerometer data from healthy subjects performing activities of daily living - ADLs (source dataset), to extract features relevant to multi-sensor accelerometer gait data (target dataset) for Parkinson's disease classification. We train the pre-trained model using the source dataset and use it as a feature extractor. We show that the features extracted for the target dataset can be used to train an effective classification model. Our pre-trained source model consists of a convolutional autoencoder, and the target classification model is a simple multi-layer perceptron model. We explore two different pre-trained source models, trained using different activity groups, and analyze the influence the choice of pre-trained model has over the task of Parkinson's disease classification.



### Subdomain Adaptation with Manifolds Discrepancy Alignment
- **Arxiv ID**: http://arxiv.org/abs/2005.03229v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03229v1)
- **Published**: 2020-05-06 04:18:47+00:00
- **Updated**: 2020-05-06 04:18:47+00:00
- **Authors**: Pengfei Wei, Yiping Ke, Xinghua Qu, Tze-Yun Leong
- **Comment**: None
- **Journal**: None
- **Summary**: Reducing domain divergence is a key step in transfer learning problems. Existing works focus on the minimization of global domain divergence. However, two domains may consist of several shared subdomains, and differ from each other in each subdomain. In this paper, we take the local divergence of subdomains into account in transfer. Specifically, we propose to use low-dimensional manifold to represent subdomain, and align the local data distribution discrepancy in each manifold across domains. A Manifold Maximum Mean Discrepancy (M3D) is developed to measure the local distribution discrepancy in each manifold. We then propose a general framework, called Transfer with Manifolds Discrepancy Alignment (TMDA), to couple the discovery of data manifolds with the minimization of M3D. We instantiate TMDA in the subspace learning case considering both the linear and nonlinear mappings. We also instantiate TMDA in the deep learning framework. Extensive experimental studies demonstrate that TMDA is a promising method for various transfer learning tasks.



### Exploiting Inter-Frame Regional Correlation for Efficient Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.02591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02591v1)
- **Published**: 2020-05-06 04:28:00+00:00
- **Updated**: 2020-05-06 04:28:00+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Kezhi Mao, Jianxiong Yin, Simon See
- **Comment**: 24 pages (exclude reference), 7 figures, 4 tables
- **Journal**: None
- **Summary**: Temporal feature extraction is an important issue in video-based action recognition. Optical flow is a popular method to extract temporal feature, which produces excellent performance thanks to its capacity of capturing pixel-level correlation information between consecutive frames. However, such a pixel-level correlation is extracted at the cost of high computational complexity and large storage resource. In this paper, we propose a novel temporal feature extraction method, named Attentive Correlated Temporal Feature (ACTF), by exploring inter-frame correlation within a certain region. The proposed ACTF exploits both bilinear and linear correlation between successive frames on the regional level. Our method has the advantage of achieving performance comparable to or better than optical flow-based methods while avoiding the introduction of optical flow. Experimental results demonstrate our proposed method achieves the state-of-the-art performances of 96.3% on UCF101 and 76.3% on HMDB51 benchmark datasets.



### Dependency Aware Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2005.02634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02634v1)
- **Published**: 2020-05-06 07:41:22+00:00
- **Updated**: 2020-05-06 07:41:22+00:00
- **Authors**: Kai Zhao, Xin-Yu Zhang, Qi Han, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are typically over-parameterized, bringing considerable computational overhead and memory footprint in inference. Pruning a proportion of unimportant filters is an efficient way to mitigate the inference cost. For this purpose, identifying unimportant convolutional filters is the key to effective filter pruning. Previous work prunes filters according to either their weight norms or the corresponding batch-norm scaling factors, while neglecting the sequential dependency between adjacent layers. In this paper, we further develop the norm-based importance estimation by taking the dependency between the adjacent layers into consideration. Besides, we propose a novel mechanism to dynamically control the sparsity-inducing regularization so as to achieve the desired sparsity. In this way, we can identify unimportant filters and search for the optimal network architecture within certain resource budgets in a more principled manner. Comprehensive experimental results demonstrate the proposed method performs favorably against the existing strong baseline on the CIFAR, SVHN, and ImageNet datasets. The training sources will be publicly available after the review process.



### Incremental Few-Shot Object Detection for Robotics
- **Arxiv ID**: http://arxiv.org/abs/2005.02641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02641v2)
- **Published**: 2020-05-06 08:05:08+00:00
- **Updated**: 2022-03-23 09:05:22+00:00
- **Authors**: Yiting Li, Haiyue Zhu, Sichao Tian, Fan Feng, Jun Ma, Chek Sing Teo, Cheng Xiang, Prahlad Vadakkepat, Tong Heng Lee
- **Comment**: ICRA 2022
- **Journal**: None
- **Summary**: Incremental few-shot learning is highly expected for practical robotics applications. On one hand, robot is desired to learn new tasks quickly and flexibly using only few annotated training samples; on the other hand, such new additional tasks should be learned in a continuous and incremental manner without forgetting the previous learned knowledge dramatically. In this work, we propose a novel Class-Incremental Few-Shot Object Detection (CI-FSOD) framework that enables deep object detection network to perform effective continual learning from just few-shot samples without re-accessing the previous training data. We achieve this by equipping the widely-used Faster-RCNN detector with three elegant components. Firstly, to best preserve performance on the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES) architecture which decouples the representation learning of base and novel categories into different spaces. Secondly, to mitigate the catastrophic forgetting on the accumulated novel classes, we propose a Sequential Model Fusion (SMF) method, which is able to achieve long-term memory without additional storage cost. Thirdly, to promote inter-task class separation in feature space, we propose a novel regularization technique that extends the classification boundary further away from the previous classes to avoid misclassification. Overall, our framework is simple yet effective and outperforms the previous SOTA with a significant margin of 2.4 points in AP performance.



### Deep Recurrent Model for Individualized Prediction of Alzheimer's Disease Progression
- **Arxiv ID**: http://arxiv.org/abs/2005.02643v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02643v2)
- **Published**: 2020-05-06 08:08:00+00:00
- **Updated**: 2020-08-27 11:28:42+00:00
- **Authors**: Wonsik Jung, Eunji Jun, Heung-Il Suk
- **Comment**: 17 pages, 12 figures
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is known as one of the major causes of dementia and is characterized by slow progression over several years, with no treatments or available medicines. In this regard, there have been efforts to identify the risk of developing AD in its earliest time. While many of the previous works considered cross-sectional analysis, more recent studies have focused on the diagnosis and prognosis of AD with longitudinal or time series data in a way of disease progression modeling (DPM). Under the same problem settings, in this work, we propose a novel computational framework that can predict the phenotypic measurements of MRI biomarkers and trajectories of clinical status along with cognitive scores at multiple future time points. However, in handling time series data, it generally faces with many unexpected missing observations. In regard to such an unfavorable situation, we define a secondary problem of estimating those missing values and tackle it in a systematic way by taking account of temporal and multivariate relations inherent in time series data. Concretely, we propose a deep recurrent network that jointly tackles the four problems of (i) missing value imputation, (ii) phenotypic measurements forecasting, (iii) trajectory estimation of the cognitive score, and (iv) clinical status prediction of a subject based on his/her longitudinal imaging biomarkers. Notably, the learnable model parameters of our network are trained in an end-to-end manner with our circumspectly defined loss function. In our experiments over TADPOLE challenge cohort, we measured performance for various metrics and compared our method to competing methods in the literature. Exhaustive analyses and ablation studies were also conducted to better confirm the effectiveness of our method.



### Regularized Pooling
- **Arxiv ID**: http://arxiv.org/abs/2005.03709v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03709v2)
- **Published**: 2020-05-06 09:02:17+00:00
- **Updated**: 2020-08-06 07:10:34+00:00
- **Authors**: Takato Otsuzuki, Hideaki Hayashi, Yuchen Zheng, Seiichi Uchida
- **Comment**: 12 pages, 10 figures, accepted for ICANN 2020
- **Journal**: None
- **Summary**: In convolutional neural networks (CNNs), pooling operations play important roles such as dimensionality reduction and deformation compensation. In general, max pooling, which is the most widely used operation for local pooling, is performed independently for each kernel. However, the deformation may be spatially smooth over the neighboring kernels. This means that max pooling is too flexible to compensate for actual deformations. In other words, its excessive flexibility risks canceling the essential spatial differences between classes. In this paper, we propose regularized pooling, which enables the value selection direction in the pooling operation to be spatially smooth across adjacent kernels so as to compensate only for actual deformations. The results of experiments on handwritten character images and texture images showed that regularized pooling not only improves recognition accuracy but also accelerates the convergence of learning compared with conventional pooling operations.



### Automated Transcription for Pre-Modern Japanese Kuzushiji Documents by Random Lines Erasure and Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.02669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02669v1)
- **Published**: 2020-05-06 09:17:28+00:00
- **Updated**: 2020-05-06 09:17:28+00:00
- **Authors**: Anh Duc Le
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing the full-page of Japanese historical documents is a challenging problem due to the complex layout/background and difficulty of writing styles, such as cursive and connected characters. Most of the previous methods divided the recognition process into character segmentation and recognition. However, those methods provide only character bounding boxes and classes without text transcription. In this paper, we enlarge our previous humaninspired recognition system from multiple lines to the full-page of Kuzushiji documents. The human-inspired recognition system simulates human eye movement during the reading process. For the lack of training data, we propose a random text line erasure approach that randomly erases text lines and distorts documents. For the convergence problem of the recognition system for fullpage documents, we employ curriculum learning that trains the recognition system step by step from the easy level (several text lines of documents) to the difficult level (full-page documents). We tested the step training approach and random text line erasure approach on the dataset of the Kuzushiji recognition competition on Kaggle. The results of the experiments demonstrate the effectiveness of our proposed approaches. These results are competitive with other participants of the Kuzushiji recognition competition.



### CONFIG: Controllable Neural Face Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2005.02671v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02671v3)
- **Published**: 2020-05-06 09:19:46+00:00
- **Updated**: 2020-10-19 10:13:56+00:00
- **Authors**: Marek Kowalski, Stephan J. Garbin, Virginia Estellers, Tadas Baltru≈°aitis, Matthew Johnson, Jamie Shotton
- **Comment**: includes supplementary materials
- **Journal**: None
- **Summary**: Our ability to sample realistic natural images, particularly faces, has advanced by leaps and bounds in recent years, yet our ability to exert fine-tuned control over the generative process has lagged behind. If this new technology is to find practical uses, we need to achieve a level of control over generative networks which, without sacrificing realism, is on par with that seen in computer graphics and character animation. To this end we propose ConfigNet, a neural face model that allows for controlling individual aspects of output images in semantically meaningful ways and that is a significant step on the path towards finely-controllable neural rendering. ConfigNet is trained on real face images as well as synthetic face renders. Our novel method uses synthetic data to factorize the latent space into elements that correspond to the inputs of a traditional rendering pipeline, separating aspects such as head pose, facial expression, hair style, illumination, and many others which are very hard to annotate in real data. The real images, which are presented to the network without labels, extend the variety of the generated images and encourage realism. Finally, we propose an evaluation criterion using an attribute detection network combined with a user study and demonstrate state-of-the-art individual control over attributes in the output images.



### Dual-Sampling Attention Network for Diagnosis of COVID-19 from Community Acquired Pneumonia
- **Arxiv ID**: http://arxiv.org/abs/2005.02690v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02690v2)
- **Published**: 2020-05-06 09:56:51+00:00
- **Updated**: 2020-05-20 03:43:05+00:00
- **Authors**: Xi Ouyang, Jiayu Huo, Liming Xia, Fei Shan, Jun Liu, Zhanhao Mo, Fuhua Yan, Zhongxiang Ding, Qi Yang, Bin Song, Feng Shi, Huan Yuan, Ying Wei, Xiaohuan Cao, Yaozong Gao, Dijia Wu, Qian Wang, Dinggang Shen
- **Comment**: accepted by IEEE Transactions on Medical Imaging, 2020
- **Journal**: None
- **Summary**: The coronavirus disease (COVID-19) is rapidly spreading all over the world, and has infected more than 1,436,000 people in more than 200 countries and territories as of April 9, 2020. Detecting COVID-19 at early stage is essential to deliver proper healthcare to the patients and also to protect the uninfected population. To this end, we develop a dual-sampling attention network to automatically diagnose COVID- 19 from the community acquired pneumonia (CAP) in chest computed tomography (CT). In particular, we propose a novel online attention module with a 3D convolutional network (CNN) to focus on the infection regions in lungs when making decisions of diagnoses. Note that there exists imbalanced distribution of the sizes of the infection regions between COVID-19 and CAP, partially due to fast progress of COVID-19 after symptom onset. Therefore, we develop a dual-sampling strategy to mitigate the imbalanced learning. Our method is evaluated (to our best knowledge) upon the largest multi-center CT data for COVID-19 from 8 hospitals. In the training-validation stage, we collect 2186 CT scans from 1588 patients for a 5-fold cross-validation. In the testing stage, we employ another independent large-scale testing dataset including 2796 CT scans from 2057 patients. Results show that our algorithm can identify the COVID-19 images with the area under the receiver operating characteristic curve (AUC) value of 0.944, accuracy of 87.5%, sensitivity of 86.9%, specificity of 90.1%, and F1-score of 82.0%. With this performance, the proposed algorithm could potentially aid radiologists with COVID-19 diagnosis from CAP, especially in the early stage of the COVID-19 outbreak.



### Drosophila-Inspired 3D Moving Object Detection Based on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.02696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02696v1)
- **Published**: 2020-05-06 10:04:23+00:00
- **Updated**: 2020-05-06 10:04:23+00:00
- **Authors**: Li Wang, Dawei Zhao, Tao Wu, Hao Fu, Zhiyu Wang, Liang Xiao, Xin Xu, Bin Dai
- **Comment**: None
- **Journal**: None
- **Summary**: 3D moving object detection is one of the most critical tasks in dynamic scene analysis. In this paper, we propose a novel Drosophila-inspired 3D moving object detection method using Lidar sensors. According to the theory of elementary motion detector, we have developed a motion detector based on the shallow visual neural pathway of Drosophila. This detector is sensitive to the movement of objects and can well suppress background noise. Designing neural circuits with different connection modes, the approach searches for motion areas in a coarse-to-fine fashion and extracts point clouds of each motion area to form moving object proposals. An improved 3D object detection network is then used to estimate the point clouds of each proposal and efficiently generates the 3D bounding boxes and the object categories. We evaluate the proposed approach on the widely-used KITTI benchmark, and state-of-the-art performance was obtained by using the proposed approach on the task of motion detection.



### ProbaNet: Proposal-balanced Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.02699v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02699v2)
- **Published**: 2020-05-06 10:07:39+00:00
- **Updated**: 2020-05-27 10:32:55+00:00
- **Authors**: Jing Wu, Xiang Zhang, Mingyi Zhou, Ce Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Candidate object proposals generated by object detectors based on convolutional neural network (CNN) encounter easy-hard samples imbalance problem, which can affect overall performance. In this study, we propose a Proposal-balanced Network (ProbaNet) for alleviating the imbalance problem. Firstly, ProbaNet increases the probability of choosing hard samples for training by discarding easy samples through threshold truncation. Secondly, ProbaNet emphasizes foreground proposals by increasing their weights. To evaluate the effectiveness of ProbaNet, we train models based on different benchmarks. Mean Average Precision (mAP) of the model using ProbaNet achieves 1.2$\%$ higher than the baseline on PASCAL VOC 2007. Furthermore, it is compatible with existing two-stage detectors and offers a very small amount of additional computational cost.



### Fast Geometric Surface based Segmentation of Point Cloud from Lidar Data
- **Arxiv ID**: http://arxiv.org/abs/2005.02704v1
- **DOI**: 10.1007/978-3-030-34869-4_45
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.02704v1)
- **Published**: 2020-05-06 10:17:16+00:00
- **Updated**: 2020-05-06 10:17:16+00:00
- **Authors**: Aritra Mukherjee, Sourya Dipta Das, Jasorsi Ghosh, Ananda S. Chowdhury, Sanjoy Kumar Saha
- **Comment**: Accepted to PReMI 2019( Pattern Recognition and Machine Intelligence
  2019). International Conference on Pattern Recognition and Machine
  Intelligence. Springer, Cham, 2019
- **Journal**: None
- **Summary**: Mapping the environment has been an important task for robot navigation and Simultaneous Localization And Mapping (SLAM). LIDAR provides a fast and accurate 3D point cloud map of the environment which helps in map building. However, processing millions of points in the point cloud becomes a computationally expensive task. In this paper, a methodology is presented to generate the segmented surfaces in real time and these can be used in modeling the 3D objects. At first an algorithm is proposed for efficient map building from single shot data of spinning Lidar. It is based on fast meshing and sub-sampling. It exploits the physical design and the working principle of the spinning Lidar sensor. The generated mesh surfaces are then segmented by estimating the normal and considering their homogeneity. The segmented surfaces can be used as proposals for predicting geometrically accurate model of objects in the robots activity environment. The proposed methodology is compared with some popular point cloud segmentation methods to highlight the efficacy in terms of accuracy and speed.



### Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet)
- **Arxiv ID**: http://arxiv.org/abs/2005.02706v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02706v3)
- **Published**: 2020-05-06 10:21:16+00:00
- **Updated**: 2020-09-30 08:14:54+00:00
- **Authors**: Chen-Han Tsai, Nahum Kiryati, Eli Konen, Iris Eshed, Arnaldo Mayer
- **Comment**: 11 pages, 4 figures, Accepted to the Medical Imaging and Deep
  Learning (MIDL) Conference 2020
- **Journal**: Proceedings of Machine Learning Research 121 (2020) 784-794
- **Summary**: Magnetic Resonance Imaging (MRI) is a widely-accepted imaging technique for knee injury analysis. Its advantage of capturing knee structure in three dimensions makes it the ideal tool for radiologists to locate potential tears in the knee. In order to better confront the ever growing workload of musculoskeletal (MSK) radiologists, automated tools for patients' triage are becoming a real need, reducing delays in the reading of pathological cases. In this work, we present the Efficiently-Layered Network (ELNet), a convolutional neural network (CNN) architecture optimized for the task of initial knee MRI diagnosis for triage. Unlike past approaches, we train ELNet from scratch instead of using a transfer-learning approach. The proposed method is validated quantitatively and qualitatively, and compares favorably against state-of-the-art MRNet while using a single imaging stack (axial or coronal) as input. Additionally, we demonstrate our model's capability to locate tears in the knee despite the absence of localization information during training. Lastly, the proposed model is extremely lightweight ($<$ 1MB) and therefore easy to train and deploy in real clinical settings. The code for our model is provided at: https://github.com/mxtsai/ELNet.



### Probabilistic Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/2005.02730v1
- **DOI**: 10.1109/ICIP40778.2020.9190893
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02730v1)
- **Published**: 2020-05-06 11:03:05+00:00
- **Updated**: 2020-05-06 11:03:05+00:00
- **Authors**: Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Uygar Tuna, Jarno Nikkanen, Moncef Gabbouj
- **Comment**: 5 pages, 1 figure
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP)
- **Summary**: In this paper, we propose a novel unsupervised color constancy method, called Probabilistic Color Constancy (PCC). We define a framework for estimating the illumination of a scene by weighting the contribution of different image regions using a graph-based representation of the image. To estimate the weight of each (super-)pixel, we rely on two assumptions: (Super-)pixels with similar colors contribute similarly and darker (super-)pixels contribute less. The resulting system has one global optimum solution. The proposed method achieves competitive performance, compared to the state-of-the-art, on INTEL-TAU dataset.



### Design and Development of a Web-based Tool for Inpainting of Dissected Aortae in Angiography Images
- **Arxiv ID**: http://arxiv.org/abs/2005.02760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02760v1)
- **Published**: 2020-05-06 12:22:21+00:00
- **Updated**: 2020-05-06 12:22:21+00:00
- **Authors**: Alexander Prutsch, Antonio Pepe, Jan Egger
- **Comment**: 9 figures, 14 references
- **Journal**: The 24th Central European Seminar on Computer Graphics (CESCG),
  pp. 1-8, May. 2020
- **Summary**: Medical imaging is an important tool for the diagnosis and the evaluation of an aortic dissection (AD); a serious condition of the aorta, which could lead to a life-threatening aortic rupture. AD patients need life-long medical monitoring of the aortic enlargement and of the disease progression, subsequent to the diagnosis of the aortic dissection. Since there is a lack of 'healthy-dissected' image pairs from medical studies, the application of inpainting techniques offers an alternative source for generating them by doing a virtual regression from dissected aortae to healthy aortae; an indirect way to study the origin of the disease. The proposed inpainting tool combines a neural network, which was trained on the task of inpainting aortic dissections, with an easy-to-use user interface. To achieve this goal, the inpainting tool has been integrated within the 3D medical image viewer of StudierFenster (www.studierfenster.at). By designing the tool as a web application, we simplify the usage of the neural network and reduce the initial learning curve.



### End-to-End Lane Marker Detection via Row-wise Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.08630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.08630v1)
- **Published**: 2020-05-06 12:48:46+00:00
- **Updated**: 2020-05-06 12:48:46+00:00
- **Authors**: Seungwoo Yoo, Heeseok Lee, Heesoo Myeong, Sungrack Yun, Hyoungwoo Park, Janghoon Cho, Duck Hoon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In autonomous driving, detecting reliable and accurate lane marker positions is a crucial yet challenging task. The conventional approaches for the lane marker detection problem perform a pixel-level dense prediction task followed by sophisticated post-processing that is inevitable since lane markers are typically represented by a collection of line segments without thickness. In this paper, we propose a method performing direct lane marker vertex prediction in an end-to-end manner, i.e., without any post-processing step that is required in the pixel-level dense prediction task. Specifically, we translate the lane marker detection problem into a row-wise classification task, which takes advantage of the innate shape of lane markers but, surprisingly, has not been explored well. In order to compactly extract sufficient information about lane markers which spread from the left to the right in an image, we devise a novel layer, which is utilized to successively compress horizontal components so enables an end-to-end lane marker detection system where the final lane marker positions are simply obtained via argmax operations in testing time. Experimental results demonstrate the effectiveness of the proposed method, which is on par or outperforms the state-of-the-art methods on two popular lane marker detection benchmarks, i.e., TuSimple and CULane.



### UST: Unifying Spatio-Temporal Context for Trajectory Prediction in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2005.02790v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.02790v1)
- **Published**: 2020-05-06 13:02:57+00:00
- **Updated**: 2020-05-06 13:02:57+00:00
- **Authors**: Hao He, Hengchen Dai, Naiyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory prediction has always been a challenging problem for autonomous driving, since it needs to infer the latent intention from the behaviors and interactions from traffic participants. This problem is intrinsically hard, because each participant may behave differently under different environments and interactions. This key is to effectively model the interlaced influence from both spatial context and temporal context. Existing work usually encodes these two types of context separately, which would lead to inferior modeling of the scenarios. In this paper, we first propose a unified approach to treat time and space dimensions equally for modeling spatio-temporal context. The proposed module is simple and easy to implement within several lines of codes. In contrast to existing methods which heavily rely on recurrent neural network for temporal context and hand-crafted structure for spatial context, our method could automatically partition the spatio-temporal space to adapt the data. Lastly, we test our proposed framework on two recently proposed trajectory prediction dataset ApolloScape and Argoverse. We show that the proposed method substantially outperforms the previous state-of-the-art methods while maintaining its simplicity. These encouraging results further validate the superiority of our approach.



### Unsupervised Low-light Image Enhancement with Decoupled Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.02818v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02818v2)
- **Published**: 2020-05-06 13:37:08+00:00
- **Updated**: 2022-03-28 05:41:49+00:00
- **Authors**: Wei Xiong, Ding Liu, Xiaohui Shen, Chen Fang, Jiebo Luo
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of enhancing real-world low-light images with significant noise in an unsupervised fashion. Conventional unsupervised learning-based approaches usually tackle the low-light image enhancement problem using an image-to-image translation model. They focus primarily on illumination or contrast enhancement but fail to suppress the noise that ubiquitously exists in images taken under real-world low-light conditions. To address this issue, we explicitly decouple this task into two sub-tasks: illumination enhancement and noise suppression. We propose to learn a two-stage GAN-based framework to enhance the real-world low-light images in a fully unsupervised fashion. To facilitate the unsupervised training of our model, we construct samples with pseudo labels. Furthermore, we propose an adaptive content loss to suppress real image noise in different regions based on illumination intensity. In addition to conventional benchmark datasets, a new unpaired low-light image enhancement dataset is built and used to thoroughly evaluate the performance of our model. Extensive experiments show that our proposed method outperforms the state-of-the-art unsupervised image enhancement methods in terms of both illumination enhancement and noise reduction.



### Preprint: Using RF-DNA Fingerprints To Classify OFDM Transmitters Under Rayleigh Fading Conditions
- **Arxiv ID**: http://arxiv.org/abs/2005.04184v1
- **DOI**: 10.1109/TIFS.2021.3054524
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04184v1)
- **Published**: 2020-05-06 13:53:25+00:00
- **Updated**: 2020-05-06 13:53:25+00:00
- **Authors**: Mohamed Fadul, Donald Reising, T. Daniel Loveless, Abdul Ofoli
- **Comment**: 13 pages, 14 total figures/images, Currently under review by the IEEE
  Transactions on Information Forensics and Security
- **Journal**: IEEE Transactions on Information Forensics and Security 2021
- **Summary**: The Internet of Things (IoT) is a collection of Internet connected devices capable of interacting with the physical world and computer systems. It is estimated that the IoT will consist of approximately fifty billion devices by the year 2020. In addition to the sheer numbers, the need for IoT security is exacerbated by the fact that many of the edge devices employ weak to no encryption of the communication link. It has been estimated that almost 70% of IoT devices use no form of encryption. Previous research has suggested the use of Specific Emitter Identification (SEI), a physical layer technique, as a means of augmenting bit-level security mechanism such as encryption. The work presented here integrates a Nelder-Mead based approach for estimating the Rayleigh fading channel coefficients prior to the SEI approach known as RF-DNA fingerprinting. The performance of this estimator is assessed for degrading signal-to-noise ratio and compared with least square and minimum mean squared error channel estimators. Additionally, this work presents classification results using RF-DNA fingerprints that were extracted from received signals that have undergone Rayleigh fading channel correction using Minimum Mean Squared Error (MMSE) equalization. This work also performs radio discrimination using RF-DNA fingerprints generated from the normalized magnitude-squared and phase response of Gabor coefficients as well as two classifiers. Discrimination of four 802.11a Wi-Fi radios achieves an average percent correct classification of 90% or better for signal-to-noise ratios of 18 and 21 dB or greater using a Rayleigh fading channel comprised of two and five paths, respectively.



### Stochastic Bottleneck: Rateless Auto-Encoder for Flexible Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/2005.02870v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02870v1)
- **Published**: 2020-05-06 14:47:42+00:00
- **Updated**: 2020-05-06 14:47:42+00:00
- **Authors**: Toshiaki Koike-Akino, Ye Wang
- **Comment**: 14 pages, 12 figures, ISIT 2020 accepted
- **Journal**: None
- **Summary**: We propose a new concept of rateless auto-encoders (RL-AEs) that enable a flexible latent dimensionality, which can be seamlessly adjusted for varying distortion and dimensionality requirements. In the proposed RL-AEs, instead of a deterministic bottleneck architecture, we use an over-complete representation that is stochastically regularized with weighted dropouts, in a manner analogous to sparse AE (SAE). Unlike SAEs, our RL-AEs employ monotonically increasing dropout rates across the latent representation nodes such that the latent variables become sorted by importance like in principal component analysis (PCA). This is motivated by the rateless property of conventional PCA, where the least important principal components can be discarded to realize variable rate dimensionality reduction that gracefully degrades the distortion. In contrast, since the latent variables of conventional AEs are equally important for data reconstruction, they cannot be simply discarded to further reduce the dimensionality after the AE model is trained. Our proposed stochastic bottleneck framework enables seamless rate adaptation with high reconstruction performance, without requiring predetermined latent dimensionality at training. We experimentally demonstrate that the proposed RL-AEs can achieve variable dimensionality reduction while achieving low distortion compared to conventional AEs.



### Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent Multi-View Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.03227v1
- **DOI**: 10.1109/TMI.2020.2992546
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03227v1)
- **Published**: 2020-05-06 15:19:15+00:00
- **Updated**: 2020-05-06 15:19:15+00:00
- **Authors**: Hengyuan Kang, Liming Xia, Fuhua Yan, Zhibin Wan, Feng Shi, Huan Yuan, Huiting Jiang, Dijia Wu, He Sui, Changqing Zhang, Dinggang Shen
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging (2020)
- **Summary**: Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread rapidly across the world. Due to the large number of affected patients and heavy labor for doctors, computer-aided diagnosis with machine learning algorithm is urgently needed, and could largely reduce the efforts of clinicians and accelerate the diagnosis process. Chest computed tomography (CT) has been recognized as an informative tool for diagnosis of the disease. In this study, we propose to conduct the diagnosis of COVID-19 with a series of features extracted from CT images. To fully explore multiple features describing CT images from different views, a unified latent representation is learned which can completely encode information from different aspects of features and is endowed with promising class structure for separability. Specifically, the completeness is guaranteed with a group of backward neural networks (each for one type of features), while by using class labels the representation is enforced to be compact within COVID-19/community-acquired pneumonia (CAP) and also a large margin is guaranteed between different types of pneumonia. In this way, our model can well avoid overfitting compared to the case of directly projecting highdimensional features into classes. Extensive experimental results show that the proposed method outperforms all comparison methods, and rather stable performances are observed when varying the numbers of training data.



### High-Contrast Reflection Tomography with Total-Variation Constraints
- **Arxiv ID**: http://arxiv.org/abs/2005.02903v2
- **DOI**: 10.1109/TCI.2020.3038171
- **Categories**: **eess.SP**, cs.CE, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02903v2)
- **Published**: 2020-05-06 15:26:55+00:00
- **Updated**: 2020-12-14 14:44:29+00:00
- **Authors**: Ajinkya Kadu, Hassan Mansour, Petros T. Boufounos
- **Comment**: None
- **Journal**: IEEE Transactions on Computational Imaging, vol. 6, pp. 1523-1536,
  2020
- **Summary**: Inverse scattering is the process of estimating the spatial distribution of the scattering potential of an object by measuring the scattered wavefields around it. In this paper, we consider reflection tomography of high contrast objects that commonly occurs in ground-penetrating radar, exploration geophysics, terahertz imaging, ultrasound, and electron microscopy. Unlike conventional transmission tomography, the reflection regime is severely ill-posed since the measured wavefields contain far less spatial frequency information of the target object. We propose a constrained incremental frequency inversion framework that requires no side information from a background model of the object. Our framework solves a sequence of regularized least-squares subproblems that ensure consistency with the measured scattered wavefield while imposing total-variation and non-negativity constraints. We propose a proximal Quasi-Newton method to solve the resulting subproblem and devise an automatic parameter selection routine to determine the constraint of each subproblem. We validate the performance of our approach on synthetic low-resolution phantoms and with a mismatched forward model test on a high-resolution phantom.



### Automatic Detection and Recognition of Individuals in Patterned Species
- **Arxiv ID**: http://arxiv.org/abs/2005.02905v1
- **DOI**: 10.1007/978-3-319-71273-4_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02905v1)
- **Published**: 2020-05-06 15:29:21+00:00
- **Updated**: 2020-05-06 15:29:21+00:00
- **Authors**: Gullal Singh Cheema, Saket Anand
- **Comment**: 12 pages, ECML-PKDD 2017
- **Journal**: None
- **Summary**: Visual animal biometrics is rapidly gaining popularity as it enables a non-invasive and cost-effective approach for wildlife monitoring applications. Widespread usage of camera traps has led to large volumes of collected images, making manual processing of visual content hard to manage. In this work, we develop a framework for automatic detection and recognition of individuals in different patterned species like tigers, zebras and jaguars. Most existing systems primarily rely on manual input for localizing the animal, which does not scale well to large datasets. In order to automate the detection process while retaining robustness to blur, partial occlusion, illumination and pose variations, we use the recently proposed Faster-RCNN object detection framework to efficiently detect animals in images. We further extract features from AlexNet of the animal's flank and train a logistic regression (or Linear SVM) classifier to recognize the individuals. We primarily test and evaluate our framework on a camera trap tiger image dataset that contains images that vary in overall image quality, animal pose, scale and lighting. We also evaluate our recognition system on zebra and jaguar images to show generalization to other patterned species. Our framework gives perfect detection results in camera trapped tiger images and a similar or better individual recognition performance when compared with state-of-the-art recognition techniques.



### Groupwise Multimodal Image Registration using Joint Total Variation
- **Arxiv ID**: http://arxiv.org/abs/2005.02933v1
- **DOI**: 10.1007/978-3-030-52791-4_15
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02933v1)
- **Published**: 2020-05-06 16:11:32+00:00
- **Updated**: 2020-05-06 16:11:32+00:00
- **Authors**: Mikael Brudfors, Ya√´l Balbastre, John Ashburner
- **Comment**: None
- **Journal**: None
- **Summary**: In medical imaging it is common practice to acquire a wide range of modalities (MRI, CT, PET, etc.), to highlight different structures or pathologies. As patient movement between scans or scanning session is unavoidable, registration is often an essential step before any subsequent image analysis. In this paper, we introduce a cost function based on joint total variation for such multimodal image registration. This cost function has the advantage of enabling principled, groupwise alignment of multiple images, whilst being insensitive to strong intensity non-uniformities. We evaluate our algorithm on rigidly aligning both simulated and real 3D brain scans. This validation shows robustness to strong intensity non-uniformities and low registration errors for CT/PET to MRI alignment. Our implementation is publicly available at https://github.com/brudfors/coregistration-njtv.



### GraCIAS: Grassmannian of Corrupted Images for Adversarial Security
- **Arxiv ID**: http://arxiv.org/abs/2005.02936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02936v2)
- **Published**: 2020-05-06 16:17:12+00:00
- **Updated**: 2020-05-07 15:11:24+00:00
- **Authors**: Ankita Shukla, Pavan Turaga, Saket Anand
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Input transformation based defense strategies fall short in defending against strong adversarial attacks. Some successful defenses adopt approaches that either increase the randomness within the applied transformations, or make the defense computationally intensive, making it substantially more challenging for the attacker. However, it limits the applicability of such defenses as a pre-processing step, similar to computationally heavy approaches that use retraining and network modifications to achieve robustness to perturbations. In this work, we propose a defense strategy that applies random image corruptions to the input image alone, constructs a self-correlation based subspace followed by a projection operation to suppress the adversarial perturbation. Due to its simplicity, the proposed defense is computationally efficient as compared to the state-of-the-art, and yet can withstand huge perturbations. Further, we develop proximity relationships between the projection operator of a clean image and of its adversarially perturbed version, via bounds relating geodesic distance on the Grassmannian to matrix Frobenius norms. We empirically show that our strategy is complementary to other weak defenses like JPEG compression and can be seamlessly integrated with them to create a stronger defense. We present extensive experiments on the ImageNet dataset across four different models namely InceptionV3, ResNet50, VGG16 and MobileNet models with perturbation magnitude set to {\epsilon} = 16. Unlike state-of-the-art approaches, even without any retraining, the proposed strategy achieves an absolute improvement of ~ 4.5% in defense accuracy on ImageNet.



### Attentive Semantic Exploring for Manipulated Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.02958v2
- **DOI**: 10.1109/ICASSP39728.2021.9414225
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02958v2)
- **Published**: 2020-05-06 17:08:56+00:00
- **Updated**: 2020-10-28 06:07:21+00:00
- **Authors**: Zehao Chen, Hua Yang
- **Comment**: None
- **Journal**: 2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2021, pp. 1985-1989
- **Summary**: Face manipulation methods develop rapidly in recent years, whose potential risk to society accounts for the emerging of researches on detection methods. However, due to the diversity of manipulation methods and the high quality of fake images, detection methods suffer from a lack of generalization ability. To solve the problem, we find that segmenting images into semantic fragments could be effective, as discriminative defects and distortions are closely related to such fragments. Besides, to highlight discriminative regions in fragments and to measure contribution to the final prediction of each fragment is efficient for the improvement of generalization ability. Therefore, we propose a novel manipulated face detection method based on Multilevel Facial Semantic Segmentation and Cascade Attention Mechanism. To evaluate our method, we reconstruct two datasets: GGFI and FFMI, and also collect two open-source datasets. Experiments on four datasets verify the advantages of our approach against other state-of-the-arts, especially its generalization ability.



### Generating Memorable Images Based on Human Visual Memory Schemas
- **Arxiv ID**: http://arxiv.org/abs/2005.02969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02969v1)
- **Published**: 2020-05-06 17:23:44+00:00
- **Updated**: 2020-05-06 17:23:44+00:00
- **Authors**: Cameron Kyle-Davidson, Adrian G. Bors, Karla K. Evans
- **Comment**: None
- **Journal**: None
- **Summary**: This research study proposes using Generative Adversarial Networks (GAN) that incorporate a two-dimensional measure of human memorability to generate memorable or non-memorable images of scenes. The memorability of the generated images is evaluated by modelling Visual Memory Schemas (VMS), which correspond to mental representations that human observers use to encode an image into memory. The VMS model is based upon the results of memory experiments conducted on human observers, and provides a 2D map of memorability. We impose a memorability constraint upon the latent space of a GAN by employing a VMS map prediction model as an auxiliary loss. We assess the difference in memorability between images generated to be memorable or non-memorable through an independent computational measure of memorability, and additionally assess the effect of memorability on the realness of the generated images.



### DenoiSeg: Joint Denoising and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.02987v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02987v2)
- **Published**: 2020-05-06 17:42:54+00:00
- **Updated**: 2020-06-10 21:58:18+00:00
- **Authors**: Tim-Oliver Buchholz, Mangal Prakash, Alexander Krull, Florian Jug
- **Comment**: 10 pages, 4 figures, 2 pages supplement (4 figures)
- **Journal**: None
- **Summary**: Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain. Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations. We achieve this by extending Noise2Void, a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense 3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network. The network becomes a denoising expert by seeing all available raw data, while co-learning to segment, even if only a few segmentation labels are available. This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts of synthetic noise are added. This renders the denoising-task non-trivial and unleashes the desired co-learning effect. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables few-shot learning of dense segmentations.



### CovidCTNet: An Open-Source Deep Learning Approach to Identify Covid-19 Using CT Image
- **Arxiv ID**: http://arxiv.org/abs/2005.03059v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03059v3)
- **Published**: 2020-05-06 18:16:59+00:00
- **Updated**: 2020-05-16 00:47:50+00:00
- **Authors**: Tahereh Javaheri, Morteza Homayounfar, Zohreh Amoozgar, Reza Reiazi, Fatemeh Homayounieh, Engy Abbas, Azadeh Laali, Amir Reza Radmard, Mohammad Hadi Gharib, Seyed Ali Javad Mousavi, Omid Ghaemi, Rosa Babaei, Hadi Karimi Mobin, Mehdi Hosseinzadeh, Rana Jahanban-Esfahlan, Khaled Seidi, Mannudeep K. Kalra, Guanglan Zhang, L. T. Chitkushev, Benjamin Haibe-Kains, Reza Malekzadeh, Reza Rawassizadeh
- **Comment**: 5 figures
- **Journal**: None
- **Summary**: Coronavirus disease 2019 (Covid-19) is highly contagious with limited treatment options. Early and accurate diagnosis of Covid-19 is crucial in reducing the spread of the disease and its accompanied mortality. Currently, detection by reverse transcriptase polymerase chain reaction (RT-PCR) is the gold standard of outpatient and inpatient detection of Covid-19. RT-PCR is a rapid method, however, its accuracy in detection is only ~70-75%. Another approved strategy is computed tomography (CT) imaging. CT imaging has a much higher sensitivity of ~80-98%, but similar accuracy of 70%. To enhance the accuracy of CT imaging detection, we developed an open-source set of algorithms called CovidCTNet that successfully differentiates Covid-19 from community-acquired pneumonia (CAP) and other lung diseases. CovidCTNet increases the accuracy of CT imaging detection to 90% compared to radiologists (70%). The model is designed to work with heterogeneous and small sample sizes independent of the CT imaging hardware. In order to facilitate the detection of Covid-19 globally and assist radiologists and physicians in the screening process, we are releasing all algorithms and parametric details in an open-source format. Open-source sharing of our CovidCTNet enables developers to rapidly improve and optimize services, while preserving user privacy and data ownership.



### Detection of Line Artefacts in Lung Ultrasound Images of COVID-19 Patients via Non-Convex Regularization
- **Arxiv ID**: http://arxiv.org/abs/2005.03080v3
- **DOI**: 10.1109/TUFFC.2020.3016092
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2005.03080v3)
- **Published**: 2020-05-06 19:16:04+00:00
- **Updated**: 2020-09-09 15:18:00+00:00
- **Authors**: Oktay Karaku≈ü, Nantheera Anantrasirichai, Amazigh Aguersif, Stein Silva, Adrian Basarab, Alin Achim
- **Comment**: 16 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we present a novel method for line artefacts quantification in lung ultrasound (LUS) images of COVID-19 patients. We formulate this as a non-convex regularisation problem involving a sparsity-enforcing, Cauchy-based penalty function, and the inverse Radon transform. We employ a simple local maxima detection technique in the Radon transform domain, associated with known clinical definitions of line artefacts. Despite being non-convex, the proposed technique is guaranteed to convergence through our proposed Cauchy proximal splitting (CPS) method and accurately identifies both horizontal and vertical line artefacts in LUS images. In order to reduce the number of false and missed detection, our method includes a two-stage validation mechanism, which is performed in both Radon and image domains. We evaluate the performance of the proposed method in comparison to the current state-of-the-art B-line identification method and show a considerable performance gain with 87% correctly detected B-lines in LUS images of nine COVID-19 patients. In addition, owing to its fast convergence, our proposed method is readily applicable for processing LUS image sequences.



### Diagnosing the Environment Bias in Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2005.03086v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03086v1)
- **Published**: 2020-05-06 19:24:33+00:00
- **Updated**: 2020-05-06 19:24:33+00:00
- **Authors**: Yubo Zhang, Hao Tan, Mohit Bansal
- **Comment**: IJCAI 2020 (9 pages; first two authors contributed equally)
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions, explore the given environments, and reach the desired target locations. These step-by-step navigational instructions are crucial when the agent is navigating new environments about which it has no prior knowledge. Most recent works that study VLN observe a significant performance drop when tested on unseen environments (i.e., environments not used in training), indicating that the neural agent models are highly biased towards training environments. Although this issue is considered as one of the major challenges in VLN research, it is still under-studied and needs a clearer explanation. In this work, we design novel diagnosis experiments via environment re-splitting and feature replacement, looking into possible reasons for this environment bias. We observe that neither the language nor the underlying navigational graph, but the low-level visual appearance conveyed by ResNet features directly affects the agent model and contributes to this environment bias in results. According to this observation, we explore several kinds of semantic representations that contain less low-level visual information, hence the agent learned with these features could be better generalized to unseen testing environments. Without modifying the baseline agent model and its training method, our explored semantic features significantly decrease the performance gaps between seen and unseen on multiple datasets (i.e. R2R, R4R, and CVDN) and achieve competitive unseen results to previous state-of-the-art models. Our code and features are available at: https://github.com/zhangybzbo/EnvBiasVLN



### Scale-Equalizing Pyramid Convolution for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2005.03101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03101v1)
- **Published**: 2020-05-06 19:34:56+00:00
- **Updated**: 2020-05-06 19:34:56+00:00
- **Authors**: Xinjiang Wang, Shilong Zhang, Zhuoran Yu, Litong Feng, Wayne Zhang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Feature pyramid has been an efficient method to extract features at different scales. Development over this method mainly focuses on aggregating contextual information at different levels while seldom touching the inter-level correlation in the feature pyramid. Early computer vision methods extracted scale-invariant features by locating the feature extrema in both spatial and scale dimension. Inspired by this, a convolution across the pyramid level is proposed in this study, which is termed pyramid convolution and is a modified 3-D convolution. Stacked pyramid convolutions directly extract 3-D (scale and spatial) features and outperforms other meticulously designed feature fusion modules. Based on the viewpoint of 3-D convolution, an integrated batch normalization that collects statistics from the whole feature pyramid is naturally inserted after the pyramid convolution. Furthermore, we also show that the naive pyramid convolution, together with the design of RetinaNet head, actually best applies for extracting features from a Gaussian pyramid, whose properties can hardly be satisfied by a feature pyramid. In order to alleviate this discrepancy, we build a scale-equalizing pyramid convolution (SEPC) that aligns the shared pyramid convolution kernel only at high-level feature maps. Being computationally efficient and compatible with the head design of most single-stage object detectors, the SEPC module brings significant performance improvement ($>4$AP increase on MS-COCO2017 dataset) in state-of-the-art one-stage object detectors, and a light version of SEPC also has $\sim3.5$AP gain with only around 7% inference time increase. The pyramid convolution also functions well as a stand-alone module in two-stage object detectors and is able to improve the performance by $\sim2$AP. The source code can be found at https://github.com/jshilong/SEPC.



### Deep Learning for Image-based Automatic Dial Meter Reading: Dataset and Baselines
- **Arxiv ID**: http://arxiv.org/abs/2005.03106v2
- **DOI**: 10.1109/IJCNN48605.2020.9207318
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03106v2)
- **Published**: 2020-05-06 19:48:23+00:00
- **Updated**: 2020-05-08 13:51:06+00:00
- **Authors**: Gabriel Salomon, Rayson Laroca, David Menotti
- **Comment**: Accepted for presentation at the 2020 International Joint Conference
  on Neural Networks (IJCNN)
- **Journal**: None
- **Summary**: Smart meters enable remote and automatic electricity, water and gas consumption reading and are being widely deployed in developed countries. Nonetheless, there is still a huge number of non-smart meters in operation. Image-based Automatic Meter Reading (AMR) focuses on dealing with this type of meter readings. We estimate that the Energy Company of Paran\'a (Copel), in Brazil, performs more than 850,000 readings of dial meters per month. Those meters are the focus of this work. Our main contributions are: (i) a public real-world dial meter dataset (shared upon request) called UFPR-ADMR; (ii) a deep learning-based recognition baseline on the proposed dataset; and (iii) a detailed error analysis of the main issues present in AMR for dial meters. To the best of our knowledge, this is the first work to introduce deep learning approaches to multi-dial meter reading, and perform experiments on unconstrained images. We achieved a 100.0% F1-score on the dial detection stage with both Faster R-CNN and YOLO, while the recognition rates reached 93.6% for dials and 75.25% for meters using Faster R-CNN (ResNext-101).



### DeepHist: Differentiable Joint and Color Histogram Layers for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2005.03995v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03995v1)
- **Published**: 2020-05-06 20:07:58+00:00
- **Updated**: 2020-05-06 20:07:58+00:00
- **Authors**: Mor Avi-Aharon, Assaf Arbelle, Tammy Riklin Raviv
- **Comment**: arXiv admin note: text overlap with arXiv:1912.06044
- **Journal**: None
- **Summary**: We present the DeepHist - a novel Deep Learning framework for augmenting a network by histogram layers and demonstrate its strength by addressing image-to-image translation problems. Specifically, given an input image and a reference color distribution we aim to generate an output image with the structural appearance (content) of the input (source) yet with the colors of the reference. The key idea is a new technique for a differentiable construction of joint and color histograms of the output images. We further define a color distribution loss based on the Earth Mover's Distance between the output's and the reference's color histograms and a Mutual Information loss based on the joint histograms of the source and the output images. Promising results are shown for the tasks of color transfer, image colorization and edges $\rightarrow$ photo, where the color distribution of the output image is controlled. Comparison to Pix2Pix and CyclyGANs are shown.



### Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting
- **Arxiv ID**: http://arxiv.org/abs/2005.03119v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03119v1)
- **Published**: 2020-05-06 20:11:46+00:00
- **Updated**: 2020-05-06 20:11:46+00:00
- **Authors**: Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann
- **Comment**: Accepted by ACL 2020
- **Journal**: None
- **Summary**: Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when the images are not available at the testing time.



### Towards Frequency-Based Explanation for Robust CNN
- **Arxiv ID**: http://arxiv.org/abs/2005.03141v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03141v1)
- **Published**: 2020-05-06 21:22:35+00:00
- **Updated**: 2020-05-06 21:22:35+00:00
- **Authors**: Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, Zihao Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses on building connections between the human-understandable input features with models' prediction, overlooking an alternative representation of the input, the frequency components decomposition. In this work, we present an analysis of the connection between the distribution of frequency components in the input dataset and the reasoning process the model learns from the data. We further provide quantification analysis about the contribution of different frequency components toward the model's prediction. We show that the vulnerability of the model against tiny distortions is a result of the model is relying on the high-frequency features, the target features of the adversarial (black and white-box) attackers, to make the prediction. We further show that if the model develops stronger association between the low-frequency component with true labels, the model is more robust, which is the explanation of why adversarially trained models are more robust against tiny distortions.



### NTIRE 2020 Challenge on Image Demoireing: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2005.03155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03155v1)
- **Published**: 2020-05-06 22:05:58+00:00
- **Updated**: 2020-05-06 22:05:58+00:00
- **Authors**: Shanxin Yuan, Radu Timofte, Ales Leonardis, Gregory Slabaugh, Xiaotong Luo, Jiangtao Zhang, Yanyun Qu, Ming Hong, Yuan Xie, Cuihua Li, Dejia Xu, Yihao Chu, Qingyan Sun, Shuai Liu, Ziyao Zong, Nan Nan, Chenghua Li, Sangmin Kim, Hyungjoon Nam, Jisu Kim, Jechang Jeong, Manri Cheon, Sung-Jun Yoon, Byungyeon Kang, Junwoo Lee, Bolun Zheng, Xiaohong Liu, Linhui Dai, Jun Chen, Xi Cheng, Zhenyong Fu, Jian Yang, Chul Lee, An Gia Vien, Hyunkook Park, Sabari Nathan, M. Parisa Beham, S Mohamed Mansoor Roomi, Florian Lemarchand, Maxime Pelcat, Erwan Nogues, Densen Puthussery, Hrishikesh P S, Jiji C V, Ashish Sinha, Xuan Zhao
- **Comment**: None
- **Journal**: CVPRW 2020
- **Summary**: This paper reviews the Challenge on Image Demoireing that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2020. Demoireing is a difficult task of removing moire patterns from an image to reveal an underlying clean image. The challenge was divided into two tracks. Track 1 targeted the single image demoireing problem, which seeks to remove moire patterns from a single image. Track 2 focused on the burst demoireing problem, where a set of degraded moire images of the same scene were provided as input, with the goal of producing a single demoired image as output. The methods were ranked in terms of their fidelity, measured using the peak signal-to-noise ratio (PSNR) between the ground truth clean images and the restored images produced by the participants' methods. The tracks had 142 and 99 registered participants, respectively, with a total of 14 and 6 submissions in the final testing stage. The entries span the current state-of-the-art in image and burst image demoireing problems.




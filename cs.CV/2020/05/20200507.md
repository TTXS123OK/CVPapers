# Arxiv Papers in cs.CV on 2020-05-07
### A Dynamical Perspective on Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2005.03190v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DS, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2005.03190v1)
- **Published**: 2020-05-07 01:00:29+00:00
- **Updated**: 2020-05-07 01:00:29+00:00
- **Authors**: Heng Yang
- **Comment**: Preliminary results, 10 pages, 4 figures
- **Journal**: None
- **Summary**: We provide a dynamical perspective on the classical problem of 3D point cloud registration with correspondences. A point cloud is considered as a rigid body consisting of particles. The problem of registering two point clouds is formulated as a dynamical system, where the dynamic model point cloud translates and rotates in a viscous environment towards the static scene point cloud, under forces and torques induced by virtual springs placed between each pair of corresponding points. We first show that the potential energy of the system recovers the objective function of the maximum likelihood estimation. We then adopt Lyapunov analysis, particularly the invariant set theorem, to analyze the rigid body dynamics and show that the system globally asymptotically tends towards the set of equilibrium points, where the globally optimal registration solution lies in. We conjecture that, besides the globally optimal equilibrium point, the system has either three or infinite "spurious" equilibrium points, and these spurious equilibria are all locally unstable. The case of three spurious equilibria corresponds to generic shape of the point cloud, while the case of infinite spurious equilibria happens when the point cloud exhibits symmetry. Therefore, simulating the dynamics with random perturbations guarantees to obtain the globally optimal registration solution. Numerical experiments support our analysis and conjecture.



### Recognizing Exercises and Counting Repetitions in Real Time
- **Arxiv ID**: http://arxiv.org/abs/2005.03194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03194v1)
- **Published**: 2020-05-07 01:30:48+00:00
- **Updated**: 2020-05-07 01:30:48+00:00
- **Authors**: Talal Alatiah, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence technology has made its way absolutely necessary in a variety of industries including the fitness industry. Human pose estimation is one of the important researches in the field of Computer Vision for the last few years. In this project, pose estimation and deep machine learning techniques are combined to analyze the performance and report feedback on the repetitions of performed exercises in real-time. Involving machine learning technology in the fitness industry could help the judges to count repetitions of any exercise during Weightlifting or CrossFit competitions.



### What comprises a good talking-head video generation?: A Survey and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2005.03201v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03201v1)
- **Published**: 2020-05-07 01:58:05+00:00
- **Updated**: 2020-05-07 01:58:05+00:00
- **Authors**: Lele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Over the years, performance evaluation has become essential in computer vision, enabling tangible progress in many sub-fields. While talking-head video generation has become an emerging research topic, existing evaluations on this topic present many limitations. For example, most approaches use human subjects (e.g., via Amazon MTurk) to evaluate their research claims directly. This subjective evaluation is cumbersome, unreproducible, and may impend the evolution of new research. In this work, we present a carefully-designed benchmark for evaluating talking-head video generation with standardized dataset pre-processing strategies. As for evaluation, we either propose new metrics or select the most appropriate ones to evaluate results in what we consider as desired properties for a good talking-head video, namely, identity preserving, lip synchronization, high video quality, and natural-spontaneous motion. By conducting a thoughtful analysis across several state-of-the-art talking-head generation approaches, we aim to uncover the merits and drawbacks of current methods and point out promising directions for future work. All the evaluation code is available at: https://github.com/lelechen63/talking-head-generation-survey.



### Hierarchical Attention Network for Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.03209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03209v1)
- **Published**: 2020-05-07 02:39:18+00:00
- **Updated**: 2020-05-07 02:39:18+00:00
- **Authors**: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Published in Pattern Recognition Letters
- **Journal**: None
- **Summary**: The temporal segmentation of events is an essential task and a precursor for the automatic recognition of human actions in the video. Several attempts have been made to capture frame-level salient aspects through attention but they lack the capacity to effectively map the temporal relationships in between the frames as they only capture a limited span of temporal dependencies. To this end we propose a complete end-to-end supervised learning approach that can better learn relationships between actions over time, thus improving the overall segmentation performance. The proposed hierarchical recurrent attention framework analyses the input video at multiple temporal scales, to form embeddings at frame level and segment level, and perform fine-grained action segmentation. This generates a simple, lightweight, yet extremely effective architecture for segmenting continuous video streams and has multiple application domains. We evaluate our system on multiple challenging public benchmark datasets, including MERL Shopping, 50 salads, and Georgia Tech Egocentric datasets, and achieves state-of-the-art performance. The evaluated datasets encompass numerous video capture settings which are inclusive of static overhead camera views and dynamic, ego-centric head-mounted camera views, demonstrating the direct applicability of the proposed framework in a variety of settings.



### Deep Learning Framework for Detecting Ground Deformation in the Built Environment using Satellite InSAR data
- **Arxiv ID**: http://arxiv.org/abs/2005.03221v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/2005.03221v2)
- **Published**: 2020-05-07 03:14:00+00:00
- **Updated**: 2020-05-13 03:20:00+00:00
- **Authors**: Nantheera Anantrasirichai, Juliet Biggs, Krisztina Kelevitz, Zahra Sadeghi, Tim Wright, James Thompson, Alin Achim, David Bull
- **Comment**: None
- **Journal**: None
- **Summary**: The large volumes of Sentinel-1 data produced over Europe are being used to develop pan-national ground motion services. However, simple analysis techniques like thresholding cannot detect and classify complex deformation signals reliably making providing usable information to a broad range of non-expert stakeholders a challenge. Here we explore the applicability of deep learning approaches by adapting a pre-trained convolutional neural network (CNN) to detect deformation in a national-scale velocity field. For our proof-of-concept, we focus on the UK where previously identified deformation is associated with coal-mining, ground water withdrawal, landslides and tunnelling. The sparsity of measurement points and the presence of spike noise make this a challenging application for deep learning networks, which involve calculations of the spatial convolution between images. Moreover, insufficient ground truth data exists to construct a balanced training data set, and the deformation signals are slower and more localised than in previous applications. We propose three enhancement methods to tackle these problems: i) spatial interpolation with modified matrix completion, ii) a synthetic training dataset based on the characteristics of real UK velocity map, and iii) enhanced over-wrapping techniques. Using velocity maps spanning 2015-2019, our framework detects several areas of coal mining subsidence, uplift due to dewatering, slate quarries, landslides and tunnel engineering works. The results demonstrate the potential applicability of the proposed framework to the development of automated ground motion analysis systems.



### End-to-End Domain Adaptive Attention Network for Cross-Domain Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2005.03222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03222v1)
- **Published**: 2020-05-07 03:17:43+00:00
- **Updated**: 2020-05-07 03:17:43+00:00
- **Authors**: Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: submitted to IEEE Transactions on Information Forensics and Security
- **Journal**: None
- **Summary**: Person re-identification (re-ID) remains challenging in a real-world scenario, as it requires a trained network to generalise to totally unseen target data in the presence of variations across domains. Recently, generative adversarial models have been widely adopted to enhance the diversity of training data. These approaches, however, often fail to generalise to other domains, as existing generative person re-identification models have a disconnect between the generative component and the discriminative feature learning stage. To address the on-going challenges regarding model generalisation, we propose an end-to-end domain adaptive attention network to jointly translate images between domains and learn discriminative re-id features in a single framework. To address the domain gap challenge, we introduce an attention module for image translation from source to target domains without affecting the identity of a person. More specifically, attention is directed to the background instead of the entire image of the person, ensuring identifying characteristics of the subject are preserved. The proposed joint learning network results in a significant performance improvement over state-of-the-art methods on several benchmark datasets.



### Deeply Supervised Active Learning for Finger Bones Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.03225v1
- **DOI**: 10.1109/EMBC44109.2020.9176662
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03225v1)
- **Published**: 2020-05-07 03:27:40+00:00
- **Updated**: 2020-05-07 03:27:40+00:00
- **Authors**: Ziyuan Zhao, Xiaoyan Yang, Bharadwaj Veeravalli, Zeng Zeng
- **Comment**: Accepted version to be published in the 42nd IEEE Annual
  International Conference of the IEEE Engineering in Medicine and Biology
  Society, EMBC 2020, Montreal, Canada
- **Journal**: 2020 42nd Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC)
- **Summary**: Segmentation is a prerequisite yet challenging task for medical image analysis. In this paper, we introduce a novel deeply supervised active learning approach for finger bones segmentation. The proposed architecture is fine-tuned in an iterative and incremental learning manner. In each step, the deep supervision mechanism guides the learning process of hidden layers and selects samples to be labeled. Extensive experiments demonstrated that our method achieves competitive segmentation results using less labeled samples as compared with full annotation.



### Hierarchical Predictive Coding Models in a Deep-Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2005.03230v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03230v2)
- **Published**: 2020-05-07 03:39:57+00:00
- **Updated**: 2020-09-23 00:42:39+00:00
- **Authors**: Matin Hosseini, Anthony Maida
- **Comment**: None
- **Journal**: None
- **Summary**: Bayesian predictive coding is a putative neuromorphic method for acquiring higher-level neural representations to account for sensory input. Although originating in the neuroscience community, there are also efforts in the machine learning community to study these models. This paper reviews some of the more well known models. Our review analyzes module connectivity and patterns of information transfer, seeking to find general principles used across the models. We also survey some recent attempts to cast these models within a deep learning framework. A defining feature of Bayesian predictive coding is that it uses top-down, reconstructive mechanisms to predict incoming sensory inputs or their lower-level representations. Discrepancies between the predicted and the actual inputs, known as prediction errors, then give rise to future learning that refines and improves the predictive accuracy of learned higher-level representations. Predictive coding models intended to describe computations in the neocortex emerged prior to the development of deep learning and used a communication structure between modules that we name the Rao-Ballard protocol. This protocol was derived from a Bayesian generative model with some rather strong statistical assumptions. The RB protocol provides a rubric to assess the fidelity of deep learning models that claim to implement predictive coding.



### Multi-Target Deep Learning for Algal Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.03232v1
- **DOI**: 10.1109/EMBC44109.2020.9176204
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03232v1)
- **Published**: 2020-05-07 03:40:29+00:00
- **Updated**: 2020-05-07 03:40:29+00:00
- **Authors**: Peisheng Qian, Ziyuan Zhao, Haobing Liu, Yingcai Wang, Yu Peng, Sheng Hu, Jing Zhang, Yue Deng, Zeng Zeng
- **Comment**: Accepted version to be published in the 42nd IEEE Annual
  International Conference of the IEEE Engineering in Medicine and Biology
  Society, EMBC 2020, Montreal, Canada
- **Journal**: 2020 42nd Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC)
- **Summary**: Water quality has a direct impact on industry, agriculture, and public health. Algae species are common indicators of water quality. It is because algal communities are sensitive to changes in their habitats, giving valuable knowledge on variations in water quality. However, water quality analysis requires professional inspection of algal detection and classification under microscopes, which is very time-consuming and tedious. In this paper, we propose a novel multi-target deep learning framework for algal detection and classification. Extensive experiments were carried out on a large-scale colored microscopic algal dataset. Experimental results demonstrate that the proposed method leads to the promising performance on algal detection, class identification and genus identification.



### Adaptive Feature Selection Guided Deep Forest for COVID-19 Classification with Chest CT
- **Arxiv ID**: http://arxiv.org/abs/2005.03264v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03264v1)
- **Published**: 2020-05-07 06:00:02+00:00
- **Updated**: 2020-05-07 06:00:02+00:00
- **Authors**: Liang Sun, Zhanhao Mo, Fuhua Yan, Liming Xia, Fei Shan, Zhongxiang Ding, Wei Shao, Feng Shi, Huan Yuan, Huiting Jiang, Dijia Wu, Ying Wei, Yaozong Gao, Wanchun Gao, He Sui, Daoqiang Zhang, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Chest computed tomography (CT) becomes an effective tool to assist the diagnosis of coronavirus disease-19 (COVID-19). Due to the outbreak of COVID-19 worldwide, using the computed-aided diagnosis technique for COVID-19 classification based on CT images could largely alleviate the burden of clinicians. In this paper, we propose an Adaptive Feature Selection guided Deep Forest (AFS-DF) for COVID-19 classification based on chest CT images. Specifically, we first extract location-specific features from CT images. Then, in order to capture the high-level representation of these features with the relatively small-scale data, we leverage a deep forest model to learn high-level representation of the features. Moreover, we propose a feature selection method based on the trained deep forest model to reduce the redundancy of features, where the feature selection could be adaptively incorporated with the COVID-19 classification model. We evaluated our proposed AFS-DF on COVID-19 dataset with 1495 patients of COVID-19 and 1027 patients of community acquired pneumonia (CAP). The accuracy (ACC), sensitivity (SEN), specificity (SPE) and AUC achieved by our method are 91.79%, 93.05%, 89.95% and 96.35%, respectively. Experimental results on the COVID-19 dataset suggest that the proposed AFS-DF achieves superior performance in COVID-19 vs. CAP classification, compared with 4 widely used machine learning methods.



### Synthetic Image Augmentation for Damage Region Segmentation using Conditional GAN with Structure Edge
- **Arxiv ID**: http://arxiv.org/abs/2005.08628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.5.4; I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2005.08628v1)
- **Published**: 2020-05-07 06:04:02+00:00
- **Updated**: 2020-05-07 06:04:02+00:00
- **Authors**: Takato Yasuno, Michihiro Nakajima, Tomoharu Sekiguchi, Kazuhiro Noda, Kiyoshi Aoyanagi, Sakura Kato
- **Comment**: 4 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2004.10126
- **Journal**: None
- **Summary**: Recently, social infrastructure is aging, and its predictive maintenance has become important issue. To monitor the state of infrastructures, bridge inspection is performed by human eye or bay drone. For diagnosis, primary damage region are recognized for repair targets. But, the degradation at worse level has rarely occurred, and the damage regions of interest are often narrow, so their ratio per image is extremely small pixel count, as experienced 0.6 to 1.5 percent. The both scarcity and imbalance property on the damage region of interest influences limited performance to detect damage. If additional data set of damaged images can be generated, it may enable to improve accuracy in damage region segmentation algorithm. We propose a synthetic augmentation procedure to generate damaged images using the image-to-image translation mapping from the tri-categorical label that consists the both semantic label and structure edge to the real damage image. We use the Sobel gradient operator to enhance structure edge. Actually, in case of bridge inspection, we apply the RC concrete structure with the number of 208 eye-inspection photos that rebar exposure have occurred, which are prepared 840 block images with size 224 by 224. We applied popular per-pixel segmentation algorithms such as the FCN-8s, SegNet, and DeepLabv3+Xception-v2. We demonstrates that re-training a data set added with synthetic augmentation procedure make higher accuracy based on indices the mean IoU, damage region of interest IoU, precision, recall, BF score when we predict test images.



### Multi-view data capture using edge-synchronised mobiles
- **Arxiv ID**: http://arxiv.org/abs/2005.03286v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03286v1)
- **Published**: 2020-05-07 07:13:20+00:00
- **Updated**: 2020-05-07 07:13:20+00:00
- **Authors**: Matteo Bortolon, Paul Chippendale, Stefano Messelodi, Fabio Poiesi
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view data capture permits free-viewpoint video (FVV) content creation. To this end, several users must capture video streams, calibrated in both time and pose, framing the same object/scene, from different viewpoints. New-generation network architectures (e.g. 5G) promise lower latency and larger bandwidth connections supported by powerful edge computing, properties that seem ideal for reliable FVV capture. We have explored this possibility, aiming to remove the need for bespoke synchronisation hardware when capturing a scene from multiple viewpoints, making it possible through off-the-shelf mobiles. We propose a novel and scalable data capture architecture that exploits edge resources to synchronise and harvest frame captures. We have designed an edge computing unit that supervises the relaying of timing triggers to and from multiple mobiles, in addition to synchronising frame harvesting. We empirically show the benefits of our edge computing unit by analysing latencies and show the quality of 3D reconstruction outputs against an alternative and popular centralised solution based on Unity3D.



### Deep Learning based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2005.03293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03293v1)
- **Published**: 2020-05-07 07:30:28+00:00
- **Updated**: 2020-05-07 07:30:28+00:00
- **Authors**: Nirbhay Kumar Tagore, Ayushman Singh, Sumanth Manche, Pratik Chattopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Automated person re-identification in a multi-camera surveillance setup is very important for effective tracking and monitoring crowd movement. In the recent years, few deep learning based re-identification approaches have been developed which are quite accurate but time-intensive, and hence not very suitable for practical purposes. In this paper, we propose an efficient hierarchical re-identification approach in which color histogram based comparison is first employed to find the closest matches in the gallery set, and next deep feature based comparison is carried out using Siamese network. Reduction in search space after the first level of matching helps in achieving a fast response time as well as improving the accuracy of prediction by the Siamese network by eliminating vastly dissimilar elements. A silhouette part-based feature extraction scheme is adopted in each level of hierarchy to preserve the relative locations of the different body structures and make the appearance descriptors more discriminating in nature. The proposed approach has been evaluated on five public data sets and also a new data set captured by our team in our laboratory. Results reveal that it outperforms most state-of-the-art approaches in terms of overall accuracy.



### Knowledge Enhanced Neural Fashion Trend Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2005.03297v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2005.03297v2)
- **Published**: 2020-05-07 07:42:17+00:00
- **Updated**: 2020-09-23 09:14:20+00:00
- **Authors**: Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng Chua
- **Comment**: 8 pages, 9 figures, ICMR 2020
- **Journal**: None
- **Summary**: Fashion trend forecasting is a crucial task for both academia and industry. Although some efforts have been devoted to tackling this challenging task, they only studied limited fashion elements with highly seasonal or simple patterns, which could hardly reveal the real fashion trends. Towards insightful fashion trend forecasting, this work focuses on investigating fine-grained fashion element trends for specific user groups. We first contribute a large-scale fashion trend dataset (FIT) collected from Instagram with extracted time series fashion element records and user information. Further-more, to effectively model the time series data of fashion elements with rather complex patterns, we propose a Knowledge EnhancedRecurrent Network model (KERN) which takes advantage of the capability of deep recurrent neural networks in modeling time-series data. Moreover, it leverages internal and external knowledge in fashion domain that affects the time-series patterns of fashion element trends. Such incorporation of domain knowledge further enhances the deep learning model in capturing the patterns of specific fashion elements and predicting the future trends. Extensive experiments demonstrate that the proposed KERN model can effectively capture the complicated patterns of objective fashion elements, therefore making preferable fashion trend forecast.



### Encoding in the Dark Grand Challenge: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2005.03315v1
- **DOI**: 10.1109/ICMEW46912.2020.9106011
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03315v1)
- **Published**: 2020-05-07 08:22:56+00:00
- **Updated**: 2020-05-07 08:22:56+00:00
- **Authors**: Nantheera Anantrasirichai, Fan Zhang, Alexandra Malyugina, Paul Hill, Angeliki Katsenou
- **Comment**: None
- **Journal**: None
- **Summary**: A big part of the video content we consume from video providers consists of genres featuring low-light aesthetics. Low light sequences have special characteristics, such as spatio-temporal varying acquisition noise and light flickering, that make the encoding process challenging. To deal with the spatio-temporal incoherent noise, higher bitrates are used to achieve high objective quality. Additionally, the quality assessment metrics and methods have not been designed, trained or tested for this type of content. This has inspired us to trigger research in that area and propose a Grand Challenge on encoding low-light video sequences. In this paper, we present an overview of the proposed challenge, and test state-of-the-art methods that will be part of the benchmark methods at the stage of the participants' deliverable assessment. From this exploration, our results show that VVC already achieves a high performance compared to simply denoising the video source prior to encoding. Moreover, the quality of the video streams can be further improved by employing a post-processing image enhancement method.



### A Review of Computer Vision Methods in Network Security
- **Arxiv ID**: http://arxiv.org/abs/2005.03318v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03318v1)
- **Published**: 2020-05-07 08:29:11+00:00
- **Updated**: 2020-05-07 08:29:11+00:00
- **Authors**: Jiawei Zhao, Rahat Masood, Suranga Seneviratne
- **Comment**: None
- **Journal**: None
- **Summary**: Network security has become an area of significant importance more than ever as highlighted by the eye-opening numbers of data breaches, attacks on critical infrastructure, and malware/ransomware/cryptojacker attacks that are reported almost every day. Increasingly, we are relying on networked infrastructure and with the advent of IoT, billions of devices will be connected to the internet, providing attackers with more opportunities to exploit. Traditional machine learning methods have been frequently used in the context of network security. However, such methods are more based on statistical features extracted from sources such as binaries, emails, and packet flows.   On the other hand, recent years witnessed a phenomenal growth in computer vision mainly driven by the advances in the area of convolutional neural networks. At a glance, it is not trivial to see how computer vision methods are related to network security. Nonetheless, there is a significant amount of work that highlighted how methods from computer vision can be applied in network security for detecting attacks or building security solutions. In this paper, we provide a comprehensive survey of such work under three topics; i) phishing attempt detection, ii) malware detection, and iii) traffic anomaly detection. Next, we review a set of such commercial products for which public information is available and explore how computer vision methods are effectively used in those products. Finally, we discuss existing research gaps and future research directions, especially focusing on how network security research community and the industry can leverage the exponential growth of computer vision methods to build much secure networked systems.



### Wavelet Integrated CNNs for Noise-Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.03337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03337v2)
- **Published**: 2020-05-07 09:10:41+00:00
- **Updated**: 2020-07-14 07:51:21+00:00
- **Authors**: Qiufu Li, Linlin Shen, Sheng Guo, Zhihui Lai
- **Comment**: CVPR accepted paper
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are generally prone to noise interruptions, i.e., small image noise can cause drastic changes in the output. To suppress the noise effect to the final predication, we enhance CNNs by replacing max-pooling, strided-convolution, and average-pooling with Discrete Wavelet Transform (DWT). We present general DWT and Inverse DWT (IDWT) layers applicable to various wavelets like Haar, Daubechies, and Cohen, etc., and design wavelet integrated CNNs (WaveCNets) using these layers for image classification. In WaveCNets, feature maps are decomposed into the low-frequency and high-frequency components during the down-sampling. The low-frequency component stores main information including the basic object structures, which is transmitted into the subsequent layers to extract robust high-level features. The high-frequency components, containing most of the data noise, are dropped during inference to improve the noise-robustness of the WaveCNets. Our experimental results on ImageNet and ImageNet-C (the noisy version of ImageNet) show that WaveCNets, the wavelet integrated versions of VGG, ResNets, and DenseNet, achieve higher accuracy and better noise-robustness than their vanilla versions.



### Scene Text Image Super-Resolution in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2005.03341v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03341v3)
- **Published**: 2020-05-07 09:18:59+00:00
- **Updated**: 2020-08-02 03:27:43+00:00
- **Authors**: Wenjia Wang, Enze Xie, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua Shen, Xiang Bai
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Low-resolution text images are often seen in natural scenes such as documents captured by mobile phones. Recognizing low-resolution text images is challenging because they lose detailed content information, leading to poor recognition accuracy. An intuitive solution is to introduce super-resolution (SR) techniques as pre-processing. However, previous single image super-resolution (SISR) methods are trained on synthetic low-resolution images (e.g.Bicubic down-sampling), which is simple and not suitable for real low-resolution text recognition. To this end, we pro-pose a real scene text SR dataset, termed TextZoom. It contains paired real low-resolution and high-resolution images which are captured by cameras with different focal length in the wild. It is more authentic and challenging than synthetic data, as shown in Fig. 1. We argue improv-ing the recognition accuracy is the ultimate goal for Scene Text SR. In this purpose, a new Text Super-Resolution Network termed TSRN, with three novel modules is developed. (1) A sequential residual block is proposed to extract the sequential information of the text images. (2) A boundary-aware loss is designed to sharpen the character boundaries. (3) A central alignment module is proposed to relieve the misalignment problem in TextZoom. Extensive experiments on TextZoom demonstrate that our TSRN largely improves the recognition accuracy by over 13%of CRNN, and by nearly 9.0% of ASTER and MORAN compared to synthetic SR data. Furthermore, our TSRN clearly outperforms 7 state-of-the-art SR methods in boosting the recognition accuracy of LR images in TextZoom. For example, it outperforms LapSRN by over 5% and 8%on the recognition accuracy of ASTER and CRNN. Our results suggest that low-resolution text recognition in the wild is far from being solved, thus more research effort is needed.



### Regression Forest-Based Atlas Localization and Direction Specific Atlas Generation for Pancreas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.03345v1
- **DOI**: 10.1007/978-3-319-46723-8_64
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03345v1)
- **Published**: 2020-05-07 09:20:55+00:00
- **Updated**: 2020-05-07 09:20:55+00:00
- **Authors**: Masahiro Oda, Natsuki Shimizu, Ken'ichi Karasawa, Yukitaka Nimura, Takayuki Kitasaka, Kazunari Misawa, Michitaka Fujiwara, Daniel Rueckert, Kensaku Mori
- **Comment**: Accepted paper as a poster presentation at MICCAI 2016 (International
  Conference on Medical Image Computing and Computer-Assisted Intervention),
  Athens, Greece
- **Journal**: Published in Proceedings of MICCAI 2016, LNCS 9901, pp 556-563
- **Summary**: This paper proposes a fully automated atlas-based pancreas segmentation method from CT volumes utilizing atlas localization by regression forest and atlas generation using blood vessel information. Previous probabilistic atlas-based pancreas segmentation methods cannot deal with spatial variations that are commonly found in the pancreas well. Also, shape variations are not represented by an averaged atlas. We propose a fully automated pancreas segmentation method that deals with two types of variations mentioned above. The position and size of the pancreas is estimated using a regression forest technique. After localization, a patient-specific probabilistic atlas is generated based on a new image similarity that reflects the blood vessel position and direction information around the pancreas. We segment it using the EM algorithm with the atlas as prior followed by the graph-cut. In evaluation results using 147 CT volumes, the Jaccard index and the Dice overlap of the proposed method were 62.1% and 75.1%, respectively. Although we automated all of the segmentation processes, segmentation results were superior to the other state-of-the-art methods in the Dice overlap.



### DMCP: Differentiable Markov Channel Pruning for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.03354v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03354v2)
- **Published**: 2020-05-07 09:39:55+00:00
- **Updated**: 2020-05-08 03:41:52+00:00
- **Authors**: Shaopeng Guo, Yujie Wang, Quanquan Li, Junjie Yan
- **Comment**: CVPR2020 Oral. Code has been released at https://github.com/zx55/dmcp
- **Journal**: None
- **Summary**: Recent works imply that the channel pruning can be regarded as searching optimal sub-structure from unpruned networks. However, existing works based on this observation require training and evaluating a large number of structures, which limits their application. In this paper, we propose a novel differentiable method for channel pruning, named Differentiable Markov Channel Pruning (DMCP), to efficiently search the optimal sub-structure. Our method is differentiable and can be directly optimized by gradient descent with respect to standard task loss and budget regularization (e.g. FLOPs constraint). In DMCP, we model the channel pruning as a Markov process, in which each state represents for retaining the corresponding channel during pruning, and transitions between states denote the pruning process. In the end, our method is able to implicitly select the proper number of channels in each layer by the Markov process with optimized transitions. To validate the effectiveness of our method, we perform extensive experiments on Imagenet with ResNet and MobilenetV2. Results show our method can achieve consistent improvement than state-of-the-art pruning methods in various FLOPs settings. The code is available at https://github.com/zx55/dmcp



### DramaQA: Character-Centered Video Story Understanding with Hierarchical QA
- **Arxiv ID**: http://arxiv.org/abs/2005.03356v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03356v2)
- **Published**: 2020-05-07 09:44:58+00:00
- **Updated**: 2020-12-17 02:59:37+00:00
- **Authors**: Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Minsu Lee, Byoung-Tak Zhang
- **Comment**: 15 pages, 11 figures, accepted to AAAI 2021
- **Journal**: None
- **Summary**: Despite recent progress on computer vision and natural language processing, developing a machine that can understand video story is still hard to achieve due to the intrinsic difficulty of video story. Moreover, researches on how to evaluate the degree of video understanding based on human cognitive process have not progressed as yet. In this paper, we propose a novel video question answering (Video QA) task, DramaQA, for a comprehensive understanding of the video story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story. Our dataset is built upon the TV drama "Another Miss Oh" and it contains 17,983 QA pairs from 23,928 various length video clips, with each QA pair belonging to one of four difficulty levels. We provide 217,308 annotated images with rich character-centered annotations, including visual bounding boxes, behaviors and emotions of main characters, and coreference resolved scripts. Additionally, we suggest Multi-level Context Matching model which hierarchically understands character-centered representations of video to answer questions. We release our dataset and model publicly for research purposes, and we expect our work to provide a new perspective on video story understanding research.



### Self-Supervised Human Depth Estimation from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2005.03358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03358v1)
- **Published**: 2020-05-07 09:45:11+00:00
- **Updated**: 2020-05-07 09:45:11+00:00
- **Authors**: Feitong Tan, Hao Zhu, Zhaopeng Cui, Siyu Zhu, Marc Pollefeys, Ping Tan
- **Comment**: Accepted by IEEE Conference on Computer Vision and Patten Recognition
  (CVPR), 2020
- **Journal**: None
- **Summary**: Previous methods on estimating detailed human depth often require supervised training with `ground truth' depth data. This paper presents a self-supervised method that can be trained on YouTube videos without known depth, which makes training data collection simple and improves the generalization of the learned network. The self-supervised learning is achieved by minimizing a photo-consistency loss, which is evaluated between a video frame and its neighboring frames warped according to the estimated depth and the 3D non-rigid motion of the human body. To solve this non-rigid motion, we first estimate a rough SMPL model at each video frame and compute the non-rigid body motion accordingly, which enables self-supervised learning on estimating the shape details. Experiments demonstrate that our method enjoys better generalization and performs much better on data in the wild.



### Scoring Root Necrosis in Cassava Using Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.03367v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03367v1)
- **Published**: 2020-05-07 10:09:20+00:00
- **Updated**: 2020-05-07 10:09:20+00:00
- **Authors**: Jeremy Francis Tusubira, Benjamin Akera, Solomon Nsumba, Joyce Nakatumba-Nabende, Ernest Mwebaze
- **Comment**: 15 pages, 5 figures. Accepted as a poster in The 1st International
  Workshop and Prize Challenge on Agriculture vision: Challenges &
  Opportunities for Computer Vision In Agriculture in conjunction with IEEE/CVF
  CVPR 2020
- **Journal**: None
- **Summary**: Cassava a major food crop in many parts of Africa, has majorly been affected by Cassava Brown Streak Disease (CBSD). The disease affects tuberous roots and presents symptoms that include a yellow/brown, dry, corky necrosis within the starch-bearing tissues. Cassava breeders currently depend on visual inspection to score necrosis in roots based on a qualitative score which is quite subjective. In this paper we present an approach to automate root necrosis scoring using deep convolutional neural networks with semantic segmentation. Our experiments show that the UNet model performs this task with high accuracy achieving a mean Intersection over Union (IoU) of 0.90 on the test set. This method provides a means to use a quantitative measure for necrosis scoring on root cross-sections. This is done by segmentation and classifying the necrotized and non-necrotized pixels of cassava root cross-sections without any additional feature engineering.



### Vid2Curve: Simultaneous Camera Motion Estimation and Thin Structure Reconstruction from an RGB Video
- **Arxiv ID**: http://arxiv.org/abs/2005.03372v3
- **DOI**: 10.1145/3386569.3392476
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03372v3)
- **Published**: 2020-05-07 10:39:20+00:00
- **Updated**: 2020-05-20 04:57:24+00:00
- **Authors**: Peng Wang, Lingjie Liu, Nenglun Chen, Hung-Kuo Chu, Christian Theobalt, Wenping Wang
- **Comment**: Accepted by SIGGRAPH 2020
- **Journal**: None
- **Summary**: Thin structures, such as wire-frame sculptures, fences, cables, power lines, and tree branches, are common in the real world. It is extremely challenging to acquire their 3D digital models using traditional image-based or depth-based reconstruction methods because thin structures often lack distinct point features and have severe self-occlusion. We propose the first approach that simultaneously estimates camera motion and reconstructs the geometry of complex 3D thin structures in high quality from a color video captured by a handheld camera. Specifically, we present a new curve-based approach to estimate accurate camera poses by establishing correspondences between featureless thin objects in the foreground in consecutive video frames, without requiring visual texture in the background scene to lock on. Enabled by this effective curve-based camera pose estimation strategy, we develop an iterative optimization method with tailored measures on geometry, topology as well as self-occlusion handling for reconstructing 3D thin structures. Extensive validations on a variety of thin structures show that our method achieves accurate camera pose estimation and faithful reconstruction of 3D thin structures with complex shape and topology at a level that has not been attained by other existing reconstruction methods.



### WSMN: An optimized multipurpose blind watermarking in Shearlet domain using MLP and NSGA-II
- **Arxiv ID**: http://arxiv.org/abs/2005.03382v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2005.03382v1)
- **Published**: 2020-05-07 11:14:46+00:00
- **Updated**: 2020-05-07 11:14:46+00:00
- **Authors**: Behrouz Bolourian Haghighi, Amir Hossein Taherinia, Ahad Harati, Modjtaba Rouhani
- **Comment**: None
- **Journal**: None
- **Summary**: Digital watermarking is a remarkable issue in the field of information security to avoid the misuse of images in multimedia networks. Although access to unauthorized persons can be prevented through cryptography, it cannot be simultaneously used for copyright protection or content authentication with the preservation of image integrity. Hence, this paper presents an optimized multipurpose blind watermarking in Shearlet domain with the help of smart algorithms including MLP and NSGA-II. In this method, four copies of the robust copyright logo are embedded in the approximate coefficients of Shearlet by using an effective quantization technique. Furthermore, an embedded random sequence as a semi-fragile authentication mark is effectively extracted from details by the neural network. Due to performing an effective optimization algorithm for selecting optimum embedding thresholds, and also distinguishing the texture of blocks, the imperceptibility and robustness have been preserved. The experimental results reveal the superiority of the scheme with regard to the quality of watermarked images and robustness against hybrid attacks over other state-of-the-art schemes. The average PSNR and SSIM of the dual watermarked images are 38 dB and 0.95, respectively; Besides, it can effectively extract the copyright logo and locates forgery regions under severe attacks with satisfactory accuracy.



### Hypergraph Learning for Identification of COVID-19 with CT Imaging
- **Arxiv ID**: http://arxiv.org/abs/2005.04043v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.04043v1)
- **Published**: 2020-05-07 11:26:32+00:00
- **Updated**: 2020-05-07 11:26:32+00:00
- **Authors**: Donglin Di, Feng Shi, Fuhua Yan, Liming Xia, Zhanhao Mo, Zhongxiang Ding, Fei Shan, Shengrui Li, Ying Wei, Ying Shao, Miaofei Han, Yaozong Gao, He Sui, Yue Gao, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: The coronavirus disease, named COVID-19, has become the largest global public health crisis since it started in early 2020. CT imaging has been used as a complementary tool to assist early screening, especially for the rapid identification of COVID-19 cases from community acquired pneumonia (CAP) cases. The main challenge in early screening is how to model the confusing cases in the COVID-19 and CAP groups, with very similar clinical manifestations and imaging features. To tackle this challenge, we propose an Uncertainty Vertex-weighted Hypergraph Learning (UVHL) method to identify COVID-19 from CAP using CT images. In particular, multiple types of features (including regional features and radiomics features) are first extracted from CT image for each case. Then, the relationship among different cases is formulated by a hypergraph structure, with each case represented as a vertex in the hypergraph. The uncertainty of each vertex is further computed with an uncertainty score measurement and used as a weight in the hypergraph. Finally, a learning process of the vertex-weighted hypergraph is used to predict whether a new testing case belongs to COVID-19 or not. Experiments on a large multi-center pneumonia dataset, consisting of 2,148 COVID-19 cases and 1,182 CAP cases from five hospitals, are conducted to evaluate the performance of the proposed method. Results demonstrate the effectiveness and robustness of our proposed method on the identification of COVID-19 in comparison to state-of-the-art methods.



### Semantic Signatures for Large-scale Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2005.03388v1
- **DOI**: 10.1007/s11042-020-08992-6
- **Categories**: **cs.CV**, eess.IV, H.3; I.4; I.6
- **Links**: [PDF](http://arxiv.org/pdf/2005.03388v1)
- **Published**: 2020-05-07 11:33:10+00:00
- **Updated**: 2020-05-07 11:33:10+00:00
- **Authors**: Li Weng, Valerie Gouet-Brunet, Bahman Soheilian
- **Comment**: 12 pages, 22 figures, Multimedia Tools and Applications (2020)
- **Journal**: None
- **Summary**: Visual localization is a useful alternative to standard localization techniques. It works by utilizing cameras. In a typical scenario, features are extracted from captured images and compared with geo-referenced databases. Location information is then inferred from the matching results. Conventional schemes mainly use low-level visual features. These approaches offer good accuracy but suffer from scalability issues. In order to assist localization in large urban areas, this work explores a different path by utilizing high-level semantic information. It is found that object information in a street view can facilitate localization. A novel descriptor scheme called "semantic signature" is proposed to summarize this information. A semantic signature consists of type and angle information of visible objects at a spatial location. Several metrics and protocols are proposed for signature comparison and retrieval. They illustrate different trade-offs between accuracy and complexity. Extensive simulation results confirm the potential of the proposed scheme in large-scale applications. This paper is an extended version of a conference paper in CBMI'18. A more efficient retrieval protocol is presented with additional experiment results.



### Joint Prediction and Time Estimation of COVID-19 Developing Severe Symptoms using Chest CT Scan
- **Arxiv ID**: http://arxiv.org/abs/2005.03405v1
- **DOI**: 10.1016/j.media.2020.101824
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03405v1)
- **Published**: 2020-05-07 12:16:37+00:00
- **Updated**: 2020-05-07 12:16:37+00:00
- **Authors**: Xiaofeng Zhu, Bin Song, Feng Shi, Yanbo Chen, Rongyao Hu, Jiangzhang Gan, Wenhai Zhang, Man Li, Liye Wang, Yaozong Gao, Fei Shan, Dinggang Shen
- **Comment**: None
- **Journal**: Medical Image Analysis (2020)
- **Summary**: With the rapidly worldwide spread of Coronavirus disease (COVID-19), it is of great importance to conduct early diagnosis of COVID-19 and predict the time that patients might convert to the severe stage, for designing effective treatment plan and reducing the clinicians' workloads. In this study, we propose a joint classification and regression method to determine whether the patient would develop severe symptoms in the later time, and if yes, predict the possible conversion time that the patient would spend to convert to the severe stage. To do this, the proposed method takes into account 1) the weight for each sample to reduce the outliers' influence and explore the problem of imbalance classification, and 2) the weight for each feature via a sparsity regularization term to remove the redundant features of high-dimensional data and learn the shared information across the classification task and the regression task. To our knowledge, this study is the first work to predict the disease progression and the conversion time, which could help clinicians to deal with the potential severe cases in time or even save the patients' lives. Experimental analysis was conducted on a real data set from two hospitals with 422 chest computed tomography (CT) scans, where 52 cases were converted to severe on average 5.64 days and 34 cases were severe at admission. Results show that our method achieves the best classification (e.g., 85.91% of accuracy) and regression (e.g., 0.462 of the correlation coefficient) performance, compared to all comparison methods. Moreover, our proposed method yields 76.97% of accuracy for predicting the severe cases, 0.524 of the correlation coefficient, and 0.55 days difference for the converted time.



### NTIRE 2020 Challenge on Spectral Reconstruction from an RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2005.03412v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03412v1)
- **Published**: 2020-05-07 12:23:56+00:00
- **Updated**: 2020-05-07 12:23:56+00:00
- **Authors**: Boaz Arad, Radu Timofte, Ohad Ben-Shahar, Yi-Tun Lin, Graham Finlayson, Shai Givati, others
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the second challenge on spectral reconstruction from RGB images, i.e., the recovery of whole-scene hyperspectral (HS) information from a 3-channel RGB image. As in the previous challenge, two tracks were provided: (i) a "Clean" track where HS images are estimated from noise-free RGBs, the RGB images are themselves calculated numerically using the ground-truth HS images and supplied spectral sensitivity functions (ii) a "Real World" track, simulating capture by an uncalibrated and unknown camera, where the HS images are recovered from noisy JPEG-compressed RGB images. A new, larger-than-ever, natural hyperspectral image data set is presented, containing a total of 510 HS images. The Clean and Real World tracks had 103 and 78 registered participants respectively, with 14 teams competing in the final testing phase. A description of the proposed methods, alongside their challenge scores and an extensive evaluation of top performing methods is also provided. They gauge the state-of-the-art in spectral reconstruction from an RGB image.



### Kunster -- AR Art Video Maker -- Real time video neural style transfer on mobile devices
- **Arxiv ID**: http://arxiv.org/abs/2005.03415v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03415v1)
- **Published**: 2020-05-07 12:30:48+00:00
- **Updated**: 2020-05-07 12:30:48+00:00
- **Authors**: Wojciech Dudzik, Damian Kosowski
- **Comment**: None
- **Journal**: None
- **Summary**: Neural style transfer is a well-known branch of deep learning research, with many interesting works and two major drawbacks. Most of the works in the field are hard to use by non-expert users and substantial hardware resources are required. In this work, we present a solution to both of these problems. We have applied neural style transfer to real-time video (over 25 frames per second), which is capable of running on mobile devices. We also investigate the works on achieving temporal coherence and present the idea of fine-tuning, already trained models, to achieve stable video. What is more, we also analyze the impact of the common deep neural network architecture on the performance of mobile devices with regard to number of layers and filters present. In the experiment section we present the results of our work with respect to the iOS devices and discuss the problems present in current Android devices as well as future possibilities. At the end we present the qualitative results of stylization and quantitative results of performance tested on the iPhone 11 Pro and iPhone 6s. The presented work is incorporated in Kunster - AR Art Video Maker application available in the Apple's App Store.



### Lifted Regression/Reconstruction Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.03452v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03452v1)
- **Published**: 2020-05-07 13:24:46+00:00
- **Updated**: 2020-05-07 13:24:46+00:00
- **Authors**: Rasmus Kjr Hier, Christopher Zach
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: In this work we propose lifted regression/reconstruction networks (LRRNs), which combine lifted neural networks with a guaranteed Lipschitz continuity property for the output layer. Lifted neural networks explicitly optimize an energy model to infer the unit activations and therefore---in contrast to standard feed-forward neural networks---allow bidirectional feedback between layers. So far lifted neural networks have been modelled around standard feed-forward architectures. We propose to take further advantage of the feedback property by letting the layers simultaneously perform regression and reconstruction. The resulting lifted network architecture allows to control the desired amount of Lipschitz continuity, which is an important feature to obtain adversarially robust regression and classification methods. We analyse and numerically demonstrate applications for unsupervised and supervised learning.



### NTIRE 2020 Challenge on NonHomogeneous Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2005.03457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03457v1)
- **Published**: 2020-05-07 13:29:56+00:00
- **Updated**: 2020-05-07 13:29:56+00:00
- **Authors**: Codruta O. Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu, Radu Timofte, Jing Liu, Haiyan Wu, Yuan Xie, Yanyun Qu, Lizhuang Ma, Ziling Huang, Qili Deng, Ju-Chin Chao, Tsung-Shan Yang, Peng-Wen Chen, Po-Min Hsu, Tzu-Yi Liao, Chung-En Sun, Pei-Yuan Wu, Jeonghyeok Do, Jongmin Park, Munchurl Kim, Kareem Metwaly, Xuelu Li, Tiantong Guo, Vishal Monga, Mingzhao Yu, Venkateswararao Cherukuri, Shiue-Yuan Chuang, Tsung-Nan Lin, David Lee, Jerome Chang, Zhan-Han Wang, Yu-Bang Chang, Chang-Hong Lin, Yu Dong, Hongyu Zhou, Xiangzhen Kong, Sourya Dipta Das, Saikat Dutta, Xuan Zhao, Bing Ouyang, Dennis Estrada, Meiqi Wang, Tianqi Su, Siyi Chen, Bangyong Sun, Vincent Whannou de Dravo, Zhe Yu, Pratik Narang, Aryan Mehra, Navaneeth Raghunath, Murari Mandal
- **Comment**: CVPR Workshops Proceedings 2020
- **Journal**: None
- **Summary**: This paper reviews the NTIRE 2020 Challenge on NonHomogeneous Dehazing of images (restoration of rich details in hazy image). We focus on the proposed solutions and their results evaluated on NH-Haze, a novel dataset consisting of 55 pairs of real haze free and nonhomogeneous hazy images recorded outdoor. NH-Haze is the first realistic nonhomogeneous haze dataset that provides ground truth images. The nonhomogeneous haze has been produced using a professional haze generator that imitates the real conditions of haze scenes. 168 participants registered in the challenge and 27 teams competed in the final testing phase. The proposed solutions gauge the state-of-the-art in image dehazing.



### How Can CNNs Use Image Position for Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2005.03463v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03463v1)
- **Published**: 2020-05-07 13:38:13+00:00
- **Updated**: 2020-05-07 13:38:13+00:00
- **Authors**: Rito Murase, Masanori Suganuma, Takayuki Okatani
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Convolution is an equivariant operation, and image position does not affect its result. A recent study shows that the zero-padding employed in convolutional layers of CNNs provides position information to the CNNs. The study further claims that the position information enables accurate inference for several tasks, such as object recognition, segmentation, etc. However, there is a technical issue with the design of the experiments of the study, and thus the correctness of the claim is yet to be verified. Moreover, the absolute image position may not be essential for the segmentation of natural images, in which target objects will appear at any image position. In this study, we investigate how positional information is and can be utilized for segmentation tasks. Toward this end, we consider {\em positional encoding} (PE) that adds channels embedding image position to the input images and compare PE with several padding methods. Considering the above nature of natural images, we choose medical image segmentation tasks, in which the absolute position appears to be relatively important, as the same organs (of different patients) are captured in similar sizes and positions. We draw a mixed conclusion from the experimental results; the positional encoding certainly works in some cases, but the absolute image position may not be so important for segmentation tasks as we think.



### Text Recognition in the Wild: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2005.03492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03492v3)
- **Published**: 2020-05-07 13:57:04+00:00
- **Updated**: 2020-12-03 07:06:27+00:00
- **Authors**: Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, Tianwei Wang
- **Comment**: Accepted by ACM Computing Surveys
- **Journal**: None
- **Summary**: The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research field in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising in terms of innovation, practicality, and efficiency. This paper aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition; (2) introduce new insights and ideas; (3) provide a comprehensive review of publicly available resources; (4) point out directions for future work. In summary, this literature review attempts to present the entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field, and could be helpful to inspire future research. Related resources are available at our Github repository: https://github.com/HCIILAB/Scene-Text-Recognition.



### Heidelberg Colorectal Data Set for Surgical Data Science in the Sensor Operating Room
- **Arxiv ID**: http://arxiv.org/abs/2005.03501v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03501v5)
- **Published**: 2020-05-07 14:04:29+00:00
- **Updated**: 2021-02-23 14:32:49+00:00
- **Authors**: Lena Maier-Hein, Martin Wagner, Tobias Ross, Annika Reinke, Sebastian Bodenstedt, Peter M. Full, Hellena Hempe, Diana Mindroc-Filimon, Patrick Scholz, Thuy Nuong Tran, Pierangela Bruno, Anna Kisilenko, Benjamin Mller, Tornike Davitashvili, Manuela Capek, Minu Tizabi, Matthias Eisenmann, Tim J. Adler, Janek Grhl, Melanie Schellenberg, Silvia Seidlitz, T. Y. Emmy Lai, Bnyamin Pekdemir, Veith Roethlingshoefer, Fabian Both, Sebastian Bittel, Marc Mengler, Lars Mndermann, Martin Apitz, Annette Kopp-Schneider, Stefanie Speidel, Hannes G. Kenngott, Beat P. Mller-Stich
- **Comment**: Submitted to Nature Scientific Data
- **Journal**: None
- **Summary**: Image-based tracking of medical instruments is an integral part of surgical data science applications. Previous research has addressed the tasks of detecting, segmenting and tracking medical instruments based on laparoscopic video data. However, the proposed methods still tend to fail when applied to challenging images and do not generalize well to data they have not been trained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set - the first publicly available data set enabling comprehensive benchmarking of medical instrument detection and segmentation algorithms with a specific emphasis on method robustness and generalization capabilities. Our data set comprises 30 laparoscopic videos and corresponding sensor data from medical devices in the operating room for three different types of laparoscopic surgery. Annotations include surgical phase labels for all video frames as well as information on instrument presence and corresponding instance-wise segmentation masks for surgical instruments (if any) in more than 10,000 individual frames. The data has successfully been used to organize international competitions within the Endoscopic Vision Challenges 2017 and 2019.



### NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images
- **Arxiv ID**: http://arxiv.org/abs/2005.03560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03560v1)
- **Published**: 2020-05-07 15:50:37+00:00
- **Updated**: 2020-05-07 15:50:37+00:00
- **Authors**: Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte
- **Comment**: CVPR 2020 Workshops proceedings
- **Journal**: None
- **Summary**: Image dehazing is an ill-posed problem that has been extensively studied in the recent years. The objective performance evaluation of the dehazing methods is one of the major obstacles due to the lacking of a reference dataset. While the synthetic datasets have shown important limitations, the few realistic datasets introduced recently assume homogeneous haze over the entire scene. Since in many real cases haze is not uniformly distributed we introduce NH-HAZE, a non-homogeneous realistic dataset with pairs of real hazy and corresponding haze-free images. This is the first non-homogeneous image dehazing dataset and contains 55 outdoor scenes. The non-homogeneous haze has been introduced in the scene using a professional haze generator that imitates the real conditions of hazy scenes. Additionally, this work presents an objective assessment of several state-of-the-art single image dehazing methods that were evaluated using NH-HAZE dataset.



### Noisy Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2005.03566v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03566v3)
- **Published**: 2020-05-07 15:53:52+00:00
- **Updated**: 2021-10-17 14:57:46+00:00
- **Authors**: Xiangxiang Chu, Bo Zhang
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Simplicity is the ultimate sophistication. Differentiable Architecture Search (DARTS) has now become one of the mainstream paradigms of neural architecture search. However, it largely suffers from the well-known performance collapse issue due to the aggregation of skip connections. It is thought to have overly benefited from the residual structure which accelerates the information flow. To weaken this impact, we propose to inject unbiased random noise to impede the flow. We name this novel approach NoisyDARTS. In effect, a network optimizer should perceive this difficulty at each training step and refrain from overshooting, especially on skip connections. In the long run, since we add no bias to the gradient in terms of expectation, it is still likely to converge to the right solution area. We also prove that the injected noise plays a role in smoothing the loss landscape, which makes the optimization easier. Our method features extreme simplicity and acts as a new strong baseline. We perform extensive experiments across various search spaces, datasets, and tasks, where we robustly achieve state-of-the-art results. Our code is available at https://github.com/xiaomi-automl/NoisyDARTS.



### Compressive sensing with un-trained neural networks: Gradient descent finds the smoothest approximation
- **Arxiv ID**: http://arxiv.org/abs/2005.03991v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03991v1)
- **Published**: 2020-05-07 15:57:25+00:00
- **Updated**: 2020-05-07 15:57:25+00:00
- **Authors**: Reinhard Heckel, Mahdi Soltanolkotabi
- **Comment**: arXiv admin note: text overlap with arXiv:1910.14634
- **Journal**: None
- **Summary**: Un-trained convolutional neural networks have emerged as highly successful tools for image recovery and restoration. They are capable of solving standard inverse problems such as denoising and compressive sensing with excellent results by simply fitting a neural network model to measurements from a single image or signal without the need for any additional training data. For some applications, this critically requires additional regularization in the form of early stopping the optimization. For signal recovery from a few measurements, however, un-trained convolutional networks have an intriguing self-regularizing property: Even though the network can perfectly fit any image, the network recovers a natural image from few measurements when trained with gradient descent until convergence. In this paper, we provide numerical evidence for this property and study it theoretically. We show that---without any further regularization---an un-trained convolutional neural network can approximately reconstruct signals and images that are sufficiently structured, from a near minimal number of random measurements.



### Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.03572v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03572v4)
- **Published**: 2020-05-07 16:00:27+00:00
- **Updated**: 2021-07-05 08:21:41+00:00
- **Authors**: Zhaohui Zheng, Ping Wang, Dongwei Ren, Wei Liu, Rongguang Ye, Qinghua Hu, Wangmeng Zuo
- **Comment**: This work has been accepted to IEEE Transactions on Cybernetics. The
  source codes are available at https://github.com/Zzh-tju/CIoU
- **Journal**: None
- **Summary**: Deep learning-based object detection and instance segmentation have achieved unprecedented progress. In this paper, we propose Complete-IoU (CIoU) loss and Cluster-NMS for enhancing geometric factors in both bounding box regression and Non-Maximum Suppression (NMS), leading to notable gains of average precision (AP) and average recall (AR), without the sacrifice of inference efficiency. In particular, we consider three geometric factors, i.e., overlap area, normalized central point distance and aspect ratio, which are crucial for measuring bounding box regression in object detection and instance segmentation. The three geometric factors are then incorporated into CIoU loss for better distinguishing difficult regression cases. The training of deep models using CIoU loss results in consistent AP and AR improvements in comparison to widely adopted $\ell_n$-norm loss and IoU-based loss. Furthermore, we propose Cluster-NMS, where NMS during inference is done by implicitly clustering detected boxes and usually requires less iterations. Cluster-NMS is very efficient due to its pure GPU implementation, and geometric factors can be incorporated to improve both AP and AR. In the experiments, CIoU loss and Cluster-NMS have been applied to state-of-the-art instance segmentation (e.g., YOLACT and BlendMask-RT), and object detection (e.g., YOLO v3, SSD and Faster R-CNN) models. Taking YOLACT on MS COCO as an example, our method achieves performance gains as +1.7 AP and +6.2 AR$_{100}$ for object detection, and +0.9 AP and +3.5 AR$_{100}$ for instance segmentation, with 27.1 FPS on one NVIDIA GTX 1080Ti GPU. All the source code and trained models are available at https://github.com/Zzh-tju/CIoU



### Efficient Exact Verification of Binarized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.03597v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.LO, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2005.03597v2)
- **Published**: 2020-05-07 16:34:30+00:00
- **Updated**: 2020-10-27 04:00:16+00:00
- **Authors**: Kai Jia, Martin Rinard
- **Comment**: To be published in NeurIPS 2020
- **Journal**: None
- **Summary**: Concerned with the reliability of neural networks, researchers have developed verification techniques to prove their robustness. Most verifiers work with real-valued networks. Unfortunately, the exact (complete and sound) verifiers face scalability challenges and provide no correctness guarantees due to floating point errors. We argue that Binarized Neural Networks (BNNs) provide comparable robustness and allow exact and significantly more efficient verification. We present a new system, EEV, for efficient and exact verification of BNNs. EEV consists of two parts: (i) a novel SAT solver that speeds up BNN verification by natively handling the reified cardinality constraints arising in BNN encodings; and (ii) strategies to train solver-friendly robust BNNs by inducing balanced layer-wise sparsity and low cardinality bounds, and adaptively cancelling the gradients. We demonstrate the effectiveness of EEV by presenting the first exact verification results for L-inf-bounded adversarial robustness of nontrivial convolutional BNNs on the MNIST and CIFAR10 datasets. Compared to exact verification of real-valued networks of the same architectures on the same tasks, EEV verifies BNNs hundreds to thousands of times faster, while delivering comparable verifiable accuracy in most cases.



### Seismic Shot Gather Noise Localization Using a Multi-Scale Feature-Fusion-Based Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2005.03626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03626v1)
- **Published**: 2020-05-07 17:23:55+00:00
- **Updated**: 2020-05-07 17:23:55+00:00
- **Authors**: Antonio Jos G. Busson, Srgio Colcher, Ruy Luiz Milidi, Bruno Pereira Dias, Andr Bulco
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based models, such as convolutional neural networks, have advanced various segments of computer vision. However, this technology is rarely applied to seismic shot gather noise localization problem. This letter presents an investigation on the effectiveness of a multi-scale feature-fusion-based network for seismic shot-gather noise localization. Herein, we describe the following: (1) the construction of a real-world dataset of seismic noise localization based on 6,500 seismograms; (2) a multi-scale feature-fusion-based detector that uses the MobileNet combined with the Feature Pyramid Net as the backbone; and (3) the Single Shot multi-box detector for box classification/regression. Additionally, we propose the use of the Focal Loss function that improves the detector's prediction accuracy. The proposed detector achieves an AP@0.5 of 78.67\% in our empirical evaluation.



### Visualisation and knowledge discovery from interpretable models
- **Arxiv ID**: http://arxiv.org/abs/2005.03632v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03632v2)
- **Published**: 2020-05-07 17:37:06+00:00
- **Updated**: 2020-05-08 08:22:02+00:00
- **Authors**: Sreejita Ghosh, Peter Tino, Kerstin Bunte
- **Comment**: Accepted for proceedings of the International Joint Conference on
  Neural Networks (IJCNN) 2020
- **Journal**: None
- **Summary**: Increasing number of sectors which affect human lives, are using Machine Learning (ML) tools. Hence the need for understanding their working mechanism and evaluating their fairness in decision-making, are becoming paramount, ushering in the era of Explainable AI (XAI). In this contribution we introduced a few intrinsically interpretable models which are also capable of dealing with missing values, in addition to extracting knowledge from the dataset and about the problem. These models are also capable of visualisation of the classifier and decision boundaries: they are the angle based variants of Learning Vector Quantization. We have demonstrated the algorithms on a synthetic dataset and a real-world one (heart disease dataset from the UCI repository). The newly developed classifiers helped in investigating the complexities of the UCI dataset as a multiclass problem. The performance of the developed classifiers were comparable to those reported in literature for this dataset, with additional value of interpretability, when the dataset was treated as a binary class problem.



### Deep Learning on Point Clouds for False Positive Reduction at Nodule Detection in Chest CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2005.03654v2
- **DOI**: 10.1007/978-3-030-72610-2_15
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, 68T45 (Primary) 68T05 (Secondary), I.2.10; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2005.03654v2)
- **Published**: 2020-05-07 17:59:54+00:00
- **Updated**: 2020-06-25 08:31:26+00:00
- **Authors**: Ivan Drokin, Elena Ericheva
- **Comment**: None
- **Journal**: In: van der Aalst W.M.P. et al. (eds) Analysis of Images, Social
  Networks and Texts. AIST 2020. Lecture Notes in Computer Science, vol 12602.
  Springer, Cham
- **Summary**: This paper focuses on a novel approach for false-positive reduction (FPR) of nodule candidates in Computer-aided detection (CADe) systems following the suspicious lesions detection stage. Contrary to typical decisions in medical image analysis, the proposed approach considers input data not as a 2D or 3D image, but rather as a point cloud, and uses deep learning models for point clouds. We discovered that point cloud models require less memory and are faster both in training and inference compared to traditional CNN 3D, they achieve better performance and do not impose restrictions on the size of the input image, i.e. no restrictions on the size of the nodule candidate. We propose an algorithm for transforming 3D CT scan data to point cloud. In some cases, the volume of the nodule candidate can be much smaller than the surrounding context, for example, in the case of subpleural localization of the nodule. Therefore, we developed an algorithm for sampling points from a point cloud constructed from a 3D image of the candidate region. The algorithm is able to guarantee the capture of both context and candidate information as part of the point cloud of the nodule candidate. We designed and set up an experiment in creating a dataset from an open LIDC-IDRI database for a feature of the FPR task, and is herein described in detail. Data augmentation was applied both to avoid overfitting and as an upsampling method. Experiments were conducted with PointNet, PointNet++, and DGCNN. We show that the proposed approach outperforms baseline CNN 3D models and resulted in 85.98 FROC versus 77.26 FROC for baseline models. We compare our algorithm with published SOTA and demonstrate that even without significant modifications it works at the appropriate performance level on LUNA2016 and shows SOTA on LIDC-IDRI.



### Learning to Segment Actions from Observation and Narration
- **Arxiv ID**: http://arxiv.org/abs/2005.03684v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03684v2)
- **Published**: 2020-05-07 18:03:57+00:00
- **Updated**: 2020-08-12 03:21:27+00:00
- **Authors**: Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, Aida Nematzadeh
- **Comment**: ACL 2020
- **Journal**: None
- **Summary**: We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.



### A Hand Motion-guided Articulation and Segmentation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.03691v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03691v1)
- **Published**: 2020-05-07 18:32:08+00:00
- **Updated**: 2020-05-07 18:32:08+00:00
- **Authors**: Richard Sahala Hartanto, Ryoichi Ishikawa, Menandro Roxas, Takeshi Oishi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a method for simultaneous articulation model estimation and segmentation of an articulated object in RGB-D images using human hand motion. Our method uses the hand motion in the processes of the initial articulation model estimation, ICP-based model parameter optimization, and region selection of the target object. The hand motion gives an initial guess of the articulation model: prismatic or revolute joint. The method estimates the joint parameters by aligning the RGB-D images with the constraint of the hand motion. Finally, the target regions are selected from the cluster regions which move symmetrically along with the articulation model. Our experimental results show the robustness of the proposed method for the various objects.



### Source-Relaxed Domain Adaptation for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2005.03697v2
- **DOI**: 10.1007/978-3-030-59710-8_48
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03697v2)
- **Published**: 2020-05-07 18:46:01+00:00
- **Updated**: 2021-04-07 20:22:45+00:00
- **Authors**: Mathilde Bateson, Hoel Kervadec, Jose Dolz, Herve Lombaert, Ismail Ben Ayed
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention MICCAI
  2020. Lecture Notes in Computer Science vol 12261. Springer Cham
- **Summary**: Domain adaptation (DA) has drawn high interests for its capacity to adapt a model trained on labeled source data to perform well on unlabeled or weakly labeled target data from a different domain. Most common DA techniques require the concurrent access to the input images of both the source and target domains. However, in practice, it is common that the source images are not available in the adaptation phase. This is a very frequent DA scenario in medical imaging, for instance, when the source and target images come from different clinical sites. We propose a novel formulation for adapting segmentation networks, which relaxes such a constraint. Our formulation is based on minimizing a label-free entropy loss defined over target-domain data, which we further guide with a domain invariant prior on the segmentation regions. Many priors can be used, derived from anatomical information. Here, a class-ratio prior is learned via an auxiliary network and integrated in the form of a Kullback-Leibler (KL) divergence in our overall loss function. We show the effectiveness of our prior-aware entropy minimization in adapting spine segmentation across different MRI modalities. Our method yields comparable results to several state-of-the-art adaptation techniques, even though is has access to less information, the source images being absent in the adaptation phase. Our straight-forward adaptation strategy only uses one network, contrary to popular adversarial techniques, which cannot perform without the presence of the source images. Our framework can be readily used with various priors and segmentation problems.



### Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images
- **Arxiv ID**: http://arxiv.org/abs/2005.03717v2
- **DOI**: 10.1007/978-3-030-58548-8_38
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.03717v2)
- **Published**: 2020-05-07 19:33:06+00:00
- **Updated**: 2020-08-21 15:28:16+00:00
- **Authors**: Kiru Park, Timothy Patten, Markus Vincze
- **Comment**: ECCV 2020 (Spotlight)
- **Journal**: None
- **Summary**: Recent methods for 6D pose estimation of objects assume either textured 3D models or real images that cover the entire range of target poses. However, it is difficult to obtain textured 3D models and annotate the poses of objects in real scenarios. This paper proposes a method, Neural Object Learning (NOL), that creates synthetic images of objects in arbitrary poses by combining only a few observations from cluttered images. A novel refinement step is proposed to align inaccurate poses of objects in source images, which results in better quality images. Evaluations performed on two public datasets show that the rendered images created by NOL lead to state-of-the-art performance in comparison to methods that use 13 times the number of real images. Evaluations on our new dataset show multiple objects can be trained and recognized simultaneously using a sequence of a fixed scene.



### Effective Data Fusion with Generalized Vegetation Index: Evidence from Land Cover Segmentation in Agriculture
- **Arxiv ID**: http://arxiv.org/abs/2005.03743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.03743v1)
- **Published**: 2020-05-07 20:41:20+00:00
- **Updated**: 2020-05-07 20:41:20+00:00
- **Authors**: Hao Sheng, Xiao Chen, Jingyi Su, Ram Rajagopal, Andrew Ng
- **Comment**: CVPR 2020 - Vision for Agriculture;
  https://www.agriculture-vision.com
- **Journal**: None
- **Summary**: How can we effectively leverage the domain knowledge from remote sensing to better segment agriculture land cover from satellite images? In this paper, we propose a novel, model-agnostic, data-fusion approach for vegetation-related computer vision tasks. Motivated by the various Vegetation Indices (VIs), which are introduced by domain experts, we systematically reviewed the VIs that are widely used in remote sensing and their feasibility to be incorporated in deep neural networks. To fully leverage the Near-Infrared channel, the traditional Red-Green-Blue channels, and Vegetation Index or its variants, we propose a Generalized Vegetation Index (GVI), a lightweight module that can be easily plugged into many neural network architectures to serve as an additional information input. To smoothly train models with our GVI, we developed an Additive Group Normalization (AGN) module that does not require extra parameters of the prescribed neural networks. Our approach has improved the IoUs of vegetation-related classes by 0.9-1.3 percent and consistently improves the overall mIoU by 2 percent on our baseline.



### Recognizing Magnification Levels in Microscopic Snapshots
- **Arxiv ID**: http://arxiv.org/abs/2005.03748v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2005.03748v1)
- **Published**: 2020-05-07 20:48:29+00:00
- **Updated**: 2020-05-07 20:48:29+00:00
- **Authors**: Manit Zaveri, Shivam Kalra, Morteza Babaie, Sultaan Shah, Savvas Damskinos, Hany Kashani, H. R. Tizhoosh
- **Comment**: 4 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Recent advances in digital imaging has transformed computer vision and machine learning to new tools for analyzing pathology images. This trend could automate some of the tasks in the diagnostic pathology and elevate the pathologist workload. The final step of any cancer diagnosis procedure is performed by the expert pathologist. These experts use microscopes with high level of optical magnification to observe minute characteristics of the tissue acquired through biopsy and fixed on glass slides. Switching between different magnifications, and finding the magnification level at which they identify the presence or absence of malignant tissues is important. As the majority of pathologists still use light microscopy, compared to digital scanners, in many instance a mounted camera on the microscope is used to capture snapshots from significant field-of-views. Repositories of such snapshots usually do not contain the magnification information. In this paper, we extract deep features of the images available on TCGA dataset with known magnification to train a classifier for magnification recognition. We compared the results with LBP, a well-known handcrafted feature extraction method. The proposed approach achieved a mean accuracy of 96% when a multi-layer perceptron was trained as a classifier.



### Planning from Images with Deep Latent Gaussian Process Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2005.03770v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03770v1)
- **Published**: 2020-05-07 21:29:45+00:00
- **Updated**: 2020-05-07 21:29:45+00:00
- **Authors**: Nathanael Bosch, Jan Achterhold, Laura Leal-Taix, Jrg Stckler
- **Comment**: Accepted for publication at the 2nd Annual Conference on Learning for
  Dynamics and Control (L4DC) 2020, with supplementary material. First two
  authors contributed equally
- **Journal**: None
- **Summary**: Planning is a powerful approach to control problems with known environment dynamics. In unknown environments the agent needs to learn a model of the system dynamics to make planning applicable. This is particularly challenging when the underlying states are only indirectly observable through images. We propose to learn a deep latent Gaussian process dynamics (DLGPD) model that learns low-dimensional system dynamics from environment interactions with visual observations. The method infers latent state representations from observations using neural networks and models the system dynamics in the learned latent space with Gaussian processes. All parts of the model can be trained jointly by optimizing a lower bound on the likelihood of transitions in image space. We evaluate the proposed approach on the pendulum swing-up task while using the learned dynamics model for planning in latent space in order to solve the control problem. We also demonstrate that our method can quickly adapt a trained agent to changes in the system dynamics from just a few rollouts. We compare our approach to a state-of-the-art purely deep learning based method and demonstrate the advantages of combining Gaussian processes with deep learning for data efficiency and transfer learning.



### A Gaussian Process Upsampling Model for Improvements in Optical Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.03780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03780v1)
- **Published**: 2020-05-07 22:13:22+00:00
- **Updated**: 2020-05-07 22:13:22+00:00
- **Authors**: Steven I Reeves, Dongwook Lee, Anurag Singh, Kunal Verma
- **Comment**: 12 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: Optical Character Recognition and extraction is a key tool in the automatic evaluation of documents in a financial context. However, the image data provided to automated systems can have unreliable quality, and can be inherently low-resolution or downsampled and compressed by a transmitting program. In this paper, we illustrate the efficacy of a Gaussian Process upsampling model for the purposes of improving OCR and extraction through upsampling low resolution documents.



### ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.03788v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.03788v6)
- **Published**: 2020-05-07 22:35:04+00:00
- **Updated**: 2021-06-02 12:27:53+00:00
- **Authors**: Xinshao Wang, Yang Hua, Elyor Kodirov, David A. Clifton, Neil M. Robertson
- **Comment**: ProSelfLC is the first method to trust self knowledge progressively
  and adaptively. ProSelfLC redirects and promotes entropy minimisation, which
  is in marked contrast to recent practices of confidence penalty [42, 33, 6]
- **Journal**: CVPR 2021
- **Summary**: To train robust deep neural networks (DNNs), we systematically study several target modification approaches, which include output regularisation, self and non-self label correction (LC). Two key issues are discovered: (1) Self LC is the most appealing as it exploits its own knowledge and requires no extra models. However, how to automatically decide the trust degree of a learner as training goes is not well answered in the literature? (2) Some methods penalise while the others reward low-entropy predictions, prompting us to ask which one is better?   To resolve the first issue, taking two well-accepted propositions--deep neural networks learn meaningful patterns before fitting noise [3] and minimum entropy regularisation principle [10]--we propose a novel end-to-end method named ProSelfLC, which is designed according to learning time and entropy. Specifically, given a data point, we progressively increase trust in its predicted label distribution versus its annotated one if a model has been trained for enough time and the prediction is of low entropy (high confidence). For the second issue, according to ProSelfLC, we empirically prove that it is better to redefine a meaningful low-entropy status and optimise the learner toward it. This serves as a defence of entropy minimisation.   We demonstrate the effectiveness of ProSelfLC through extensive experiments in both clean and noisy settings. The source code is available at https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.   Keywords: entropy minimisation, maximum entropy, confidence penalty, self knowledge distillation, label correction, label noise, semi-supervised learning, output regularisation



### Federated Generative Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.03793v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.03793v3)
- **Published**: 2020-05-07 23:06:49+00:00
- **Updated**: 2020-07-19 05:02:05+00:00
- **Authors**: Chenyou Fan, Ping Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This work studies training generative adversarial networks under the federated learning setting. Generative adversarial networks (GANs) have achieved advancement in various real-world applications, such as image editing, style transfer, scene generations, etc. However, like other deep learning models, GANs are also suffering from data limitation problems in real cases. To boost the performance of GANs in target tasks, collecting images as many as possible from different sources becomes not only important but also essential. For example, to build a robust and accurate bio-metric verification system, huge amounts of images might be collected from surveillance cameras, and/or uploaded from cellphones by users accepting agreements. In an ideal case, utilize all those data uploaded from public and private devices for model training is straightforward. Unfortunately, in the real scenarios, this is hard due to a few reasons. At first, some data face the serious concern of leakage, and therefore it is prohibitive to upload them to a third-party server for model training; at second, the images collected by different kinds of devices, probably have distinctive biases due to various factors, $\textit{e.g.}$, collector preferences, geo-location differences, which is also known as "domain shift". To handle those problems, we propose a novel generative learning scheme utilizing a federated learning framework. Following the configuration of federated learning, we conduct model training and aggregation on one center and a group of clients. Specifically, our method learns the distributed generative models in clients, while the models trained in each client are fused into one unified and versatile model in the center. We perform extensive experiments to compare different federation strategies, and empirically examine the effectiveness of federation under different levels of parallelism and data skewness.



### MLGaze: Machine Learning-Based Analysis of Gaze Error Patterns in Consumer Eye Tracking Systems
- **Arxiv ID**: http://arxiv.org/abs/2005.03795v1
- **DOI**: 10.3390/vision4020025
- **Categories**: **eess.SP**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.03795v1)
- **Published**: 2020-05-07 23:07:02+00:00
- **Updated**: 2020-05-07 23:07:02+00:00
- **Authors**: Anuradha Kar
- **Comment**: https://github.com/anuradhakar49/MLGaze
- **Journal**: None
- **Summary**: Analyzing the gaze accuracy characteristics of an eye tracker is a critical task as its gaze data is frequently affected by non-ideal operating conditions in various consumer eye tracking applications. In this study, gaze error patterns produced by a commercial eye tracking device were studied with the help of machine learning algorithms, such as classifiers and regression models. Gaze data were collected from a group of participants under multiple conditions that commonly affect eye trackers operating on desktop and handheld platforms. These conditions (referred here as error sources) include user distance, head pose, and eye-tracker pose variations, and the collected gaze data were used to train the classifier and regression models. It was seen that while the impact of the different error sources on gaze data characteristics were nearly impossible to distinguish by visual inspection or from data statistics, machine learning models were successful in identifying the impact of the different error sources and predicting the variability in gaze error levels due to these conditions. The objective of this study was to investigate the efficacy of machine learning methods towards the detection and prediction of gaze error patterns, which would enable an in-depth understanding of the data quality and reliability of eye trackers under unconstrained operating conditions. Coding resources for all the machine learning methods adopted in this study were included in an open repository named MLGaze to allow researchers to replicate the principles presented here using data from their own eye trackers.




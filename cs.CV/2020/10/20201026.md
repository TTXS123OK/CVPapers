# Arxiv Papers in cs.CV on 2020-10-26
### Interpreting Uncertainty in Model Predictions For COVID-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2010.13271v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13271v1)
- **Published**: 2020-10-26 01:27:29+00:00
- **Updated**: 2020-10-26 01:27:29+00:00
- **Authors**: Gayathiri Murugamoorthy, Naimul Khan
- **Comment**: Submitted to ISBI 2021
- **Journal**: None
- **Summary**: COVID-19, due to its accelerated spread has brought in the need to use assistive tools for faster diagnosis in addition to typical lab swab testing. Chest X-Rays for COVID cases tend to show changes in the lungs such as ground glass opacities and peripheral consolidations which can be detected by deep neural networks. However, traditional convolutional networks use point estimate for predictions, lacking in capture of uncertainty, which makes them less reliable for adoption. There have been several works so far in predicting COVID positive cases with chest X-Rays. However, not much has been explored on quantifying the uncertainty of these predictions, interpreting uncertainty, and decomposing this to model or data uncertainty. To address these needs, we develop a visualization framework to address interpretability of uncertainty and its components, with uncertainty in predictions computed with a Bayesian Convolutional Neural Network. This framework aims to understand the contribution of individual features in the Chest-X-Ray images to predictive uncertainty. Providing this as an assistive tool can help the radiologist understand why the model came up with a prediction and whether the regions of interest captured by the model for the specific prediction are of significance in diagnosis. We demonstrate the usefulness of the tool in chest x-ray interpretation through several test cases from a benchmark dataset.



### $P^2$ Net: Augmented Parallel-Pyramid Net for Attention Guided Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.14076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.14076v1)
- **Published**: 2020-10-26 02:10:12+00:00
- **Updated**: 2020-10-26 02:10:12+00:00
- **Authors**: Luanxuan Hou, Jie Cao, Yuan Zhao, Haifeng Shen, Jian Tang, Ran He
- **Comment**: Accepted by ICPR2020. arXiv admin note: text overlap with
  arXiv:2003.07516
- **Journal**: None
- **Summary**: We propose an augmented Parallel-Pyramid Net ($P^2~Net$) with feature refinement by dilated bottleneck and attention module. During data preprocessing, we proposed a differentiable auto data augmentation ($DA^2$) method. We formulate the problem of searching data augmentaion policy in a differentiable form, so that the optimal policy setting can be easily updated by back propagation during training. $DA^2$ improves the training efficiency. A parallel-pyramid structure is followed to compensate the information loss introduced by the network. We innovate two fusion structures, i.e. Parallel Fusion and Progressive Fusion, to process pyramid features from backbone network. Both fusion structures leverage the advantages of spatial information affluence at high resolution and semantic comprehension at low resolution effectively. We propose a refinement stage for the pyramid features to further boost the accuracy of our network. By introducing dilated bottleneck and attention module, we increase the receptive field for the features with limited complexity and tune the importance to different feature channels. To further refine the feature maps after completion of feature extraction stage, an Attention Module ($AM$) is defined to extract weighted features from different scale feature maps generated by the parallel-pyramid structure. Compared with the traditional up-sampling refining, $AM$ can better capture the relationship between channels. Experiments corroborate the effectiveness of our proposed method. Notably, our method achieves the best performance on the challenging MSCOCO and MPII datasets.



### Global Image Segmentation Process using Machine Learning algorithm & Convolution Neural Network method for Self- Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2010.13294v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2010.13294v2)
- **Published**: 2020-10-26 02:51:00+00:00
- **Updated**: 2020-11-04 14:35:12+00:00
- **Authors**: Tirumalapudi Raviteja, Rajay Vedaraj . I. S
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: In autonomous Vehicles technology Image segmentation was a major problem in visual perception. This image segmentation process is mainly used in medical applications. Here we adopted an image segmentation process to visual perception tasks for predicting the agents on the surrounding environment, identifying the road boundaries and tracking the line markings. Main objective of the paper is to divide the input images using the image segmentation process and Convolution Neural Network method for efficient results of visual perception. For Sampling assume a local city data-set samples and validation process done in Jupyter Notebook using Python language. We proposed this image segmentation method planning to standard and further the development of state-of-the art methods for visual inspection system understanding. The experimental results achieves 73% mean IOU. Our method also achieves 90 FPS inference speed and using a NVDIA GeForce GTX 1050 GPU.



### AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2010.13302v1
- **DOI**: 10.1007/s11263-020-01398-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13302v1)
- **Published**: 2020-10-26 03:19:46+00:00
- **Updated**: 2020-10-26 03:19:46+00:00
- **Authors**: Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, Wenjun Zeng
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: Occlusion is probably the biggest challenge for human pose estimation in the wild. Typical solutions often rely on intrusive sensors such as IMUs to detect occluded joints. To make the task truly unconstrained, we present AdaFuse, an adaptive multiview fusion method, which can enhance the features in occluded views by leveraging those in visible views. The core of AdaFuse is to determine the point-point correspondence between two views which we solve effectively by exploring the sparsity of the heatmap representation. We also learn an adaptive fusion weight for each camera view to reflect its feature quality in order to reduce the chance that good features are undesirably corrupted by ``bad'' views. The fusion model is trained end-to-end with the pose estimation network, and can be directly applied to new camera configurations without additional adaptation. We extensively evaluate the approach on three public datasets including Human3.6M, Total Capture and CMU Panoptic. It outperforms the state-of-the-arts on all of them. We also create a large scale synthetic dataset Occlusion-Person, which allows us to perform numerical evaluation on the occluded joints, as it provides occlusion labels for every joint in the images. The dataset and code are released at https://github.com/zhezh/adafuse-3d-human-pose.



### Geometrically Matched Multi-source Microscopic Image Synthesis Using Bidirectional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13308v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13308v2)
- **Published**: 2020-10-26 03:30:59+00:00
- **Updated**: 2021-03-27 04:13:20+00:00
- **Authors**: Jun Zhuang, Dali Wang
- **Comment**: Published in MICAD 2021
- **Journal**: None
- **Summary**: Microscopic images from multiple modalities can produce plentiful experimental information. In practice, biological or physical constraints under a given observation period may prevent researchers from acquiring enough microscopic scanning. Recent studies demonstrate that image synthesis is one of the popular approaches to release such constraints. Nonetheless, most existing synthesis approaches only translate images from the source domain to the target domain without solid geometric associations. To embrace this challenge, we propose an innovative model architecture, BANIS, to synthesize diversified microscopic images from multi-source domains with distinct geometric features. The experimental outcomes indicate that BANIS successfully synthesizes favorable image pairs on C. elegans microscopy embryonic images. To the best of our knowledge, BANIS is the first application to synthesize microscopic images that associate distinct spatial geometric features from multi-source domains.



### A Dark and Bright Channel Prior Guided Deep Network for Retinal Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2010.13313v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13313v2)
- **Published**: 2020-10-26 03:53:08+00:00
- **Updated**: 2021-04-20 14:50:47+00:00
- **Authors**: Ziwen Xu, Beiji Zou, Qing Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal image quality assessment is an essential task in the diagnosis of retinal diseases. Recently, there are emerging deep models to grade quality of retinal images. Current state-of-the-arts either directly transfer classification networks originally designed for natural images to quality classification of retinal images or introduce extra image quality priors via multiple CNN branches or independent CNNs. This paper proposes a dark and bright channel prior guided deep network for retinal image quality assessment called GuidedNet. Specifically, the dark and bright channel priors are embedded into the start layer of network to improve the discriminate ability of deep features. In addition, we re-annotate a new retinal image quality dataset called RIQA-RFMiD for further validation. Experimental results on a public retinal image quality dataset Eye-Quality and our re-annotated dataset RIQA-RFMiD demonstrate the effectiveness of the proposed GuidedNet.



### Structural Prior Driven Regularized Deep Learning for Sonar Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.13317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13317v1)
- **Published**: 2020-10-26 04:00:46+00:00
- **Updated**: 2020-10-26 04:00:46+00:00
- **Authors**: Isaac D. Gerg, Vishal Monga
- **Comment**: To appear in TGRS, 2021
- **Journal**: None
- **Summary**: Deep learning has been recently shown to improve performance in the domain of synthetic aperture sonar (SAS) image classification. Given the constant resolution with range of a SAS, it is no surprise that deep learning techniques perform so well. Despite deep learning's recent success, there are still compelling open challenges in reducing the high false alarm rate and enabling success when training imagery is limited, which is a practical challenge that distinguishes the SAS classification problem from standard image classification set-ups where training imagery may be abundant. We address these challenges by exploiting prior knowledge that humans use to grasp the scene. These include unconscious elimination of the image speckle and localization of objects in the scene. We introduce a new deep learning architecture which incorporates these priors with the goal of improving automatic target recognition (ATR) from SAS imagery. Our proposal -- called SPDRDL, Structural Prior Driven Regularized Deep Learning -- incorporates the previously mentioned priors in a multi-task convolutional neural network (CNN) and requires no additional training data when compared to traditional SAS ATR methods. Two structural priors are enforced via regularization terms in the learning of the network: (1) structural similarity prior -- enhanced imagery (often through despeckling) aids human interpretation and is semantically similar to the original imagery and (2) structural scene context priors -- learned features ideally encapsulate target centering information; hence learning may be enhanced via a regularization that encourages fidelity against known ground truth target shifts (relative target position from scene center). Experiments on a challenging real-world dataset reveal that SPDRDL outperforms state-of-the-art deep learning and other competing methods for SAS image classification.



### Deep Sequential Learning for Cervical Spine Fracture Detection on Computed Tomography Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.13336v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13336v4)
- **Published**: 2020-10-26 04:36:29+00:00
- **Updated**: 2021-02-05 17:33:06+00:00
- **Authors**: Hojjat Salehinejad, Edward Ho, Hui-Ming Lin, Priscila Crivellaro, Oleksandra Samorodova, Monica Tafur Arciniegas, Zamir Merali, Suradech Suthiphosuwan, Aditya Bharatha, Kristen Yeom, Muhammad Mamdani, Jefferson Wilson, Errol Colak
- **Comment**: This paper is accepted for presentation at the IEEE International
  Symposium on Biomedical Imaging (ISBI) 2021
- **Journal**: None
- **Summary**: Fractures of the cervical spine are a medical emergency and may lead to permanent paralysis and even death. Accurate diagnosis in patients with suspected fractures by computed tomography (CT) is critical to patient management. In this paper, we propose a deep convolutional neural network (DCNN) with a bidirectional long-short term memory (BLSTM) layer for the automated detection of cervical spine fractures in CT axial images. We used an annotated dataset of 3,666 CT scans (729 positive and 2,937 negative cases) to train and validate the model. The validation results show a classification accuracy of 70.92% and 79.18% on the balanced (104 positive and 104 negative cases) and imbalanced (104 positive and 419 negative cases) test datasets, respectively.



### Robust Pre-Training by Adversarial Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.13337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13337v1)
- **Published**: 2020-10-26 04:44:43+00:00
- **Updated**: 2020-10-26 04:44:43+00:00
- **Authors**: Ziyu Jiang, Tianlong Chen, Ting Chen, Zhangyang Wang
- **Comment**: Neurips 2020
- **Journal**: None
- **Summary**: Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.



### EDNet: Efficient Disparity Estimation with Cost Volume Combination and Attention-based Spatial Residual
- **Arxiv ID**: http://arxiv.org/abs/2010.13338v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13338v4)
- **Published**: 2020-10-26 04:49:44+00:00
- **Updated**: 2021-03-04 05:30:42+00:00
- **Authors**: Songyan Zhang, Zhicheng Wang, Qiang Wang, Jinshuo Zhang, Gang Wei, Xiaowen Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing state-of-the-art disparity estimation works mostly leverage the 4D concatenation volume and construct a very deep 3D convolution neural network (CNN) for disparity regression, which is inefficient due to the high memory consumption and slow inference speed. In this paper, we propose a network named EDNet for efficient disparity estimation. Firstly, we construct a combined volume which incorporates contextual information from the squeezed concatenation volume and feature similarity measurement from the correlation volume. The combined volume can be next aggregated by 2D convolutions which are faster and require less memory than 3D convolutions. Secondly, we propose an attention-based spatial residual module to generate attention-aware residual features. The attention mechanism is applied to provide intuitive spatial evidence about inaccurate regions with the help of error maps at multiple scales and thus improve the residual learning efficiency. Extensive experiments on the Scene Flow and KITTI datasets show that EDNet outperforms the previous 3D CNN based works and achieves state-of-the-art performance with significantly faster speed and less memory consumption.



### Semi supervised segmentation and graph-based tracking of 3D nuclei in time-lapse microscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.13343v1
- **DOI**: 10.1109/ISBI48211.2021.9433831
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13343v1)
- **Published**: 2020-10-26 05:09:44+00:00
- **Updated**: 2020-10-26 05:09:44+00:00
- **Authors**: S. Shailja, Jiaxiang Jiang, B. S. Manjunath
- **Comment**: To be submitted to ISBI 2021
- **Journal**: 2021 IEEE 18th International Symposium on Biomedical Imaging
  (ISBI)
- **Summary**: We propose a novel weakly supervised method to improve the boundary of the 3D segmented nuclei utilizing an over-segmented image. This is motivated by the observation that current state-of-the-art deep learning methods do not result in accurate boundaries when the training data is weakly annotated. Towards this, a 3D U-Net is trained to get the centroid of the nuclei and integrated with a simple linear iterative clustering (SLIC) supervoxel algorithm that provides better adherence to cluster boundaries. To track these segmented nuclei, our algorithm utilizes the relative nuclei location depicting the processes of nuclei division and apoptosis. The proposed algorithmic pipeline achieves better segmentation performance compared to the state-of-the-art method in Cell Tracking Challenge (CTC) 2019 and comparable performance to state-of-the-art methods in IEEE ISBI CTC2020 while utilizing very few pixel-wise annotated data. Detailed experimental results are provided, and the source code is available on GitHub.



### PSF-LO: Parameterized Semantic Features Based Lidar Odometry
- **Arxiv ID**: http://arxiv.org/abs/2010.13355v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13355v3)
- **Published**: 2020-10-26 05:48:26+00:00
- **Updated**: 2021-03-25 04:01:17+00:00
- **Authors**: Guibin Chen, Bosheng Wang, Xiaoliang Wang, Huanjun Deng, Bing Wang, Shuo Zhang
- **Comment**: Accepted in International Conference on Robotics and Automation
  (ICRA) 2021
- **Journal**: None
- **Summary**: Lidar odometry (LO) is a key technology in numerous reliable and accurate localization and mapping systems of autonomous driving. The state-of-the-art LO methods generally leverage geometric information to perform point cloud registration. Furthermore, obtaining point cloud semantic information which can describe the environment more abundantly will help for the registration. We present a novel semantic lidar odometry method based on self-designed parameterized semantic features (PSFs) to achieve low-drift ego-motion estimation for autonomous vehicle in realtime. We first use a convolutional neural network-based algorithm to obtain point-wise semantics from the input laser point cloud, and then use semantic labels to separate the road, building, traffic sign and pole-like point cloud and fit them separately to obtain corresponding PSFs. A fast PSF-based matching enable us to refine geometric features (GeFs) registration, reducing the impact of blurred submap surface on the accuracy of GeFs matching. Besides, we design an efficient method to accurately recognize and remove the dynamic objects while retaining static ones in the semantic point cloud, which are beneficial to further improve the accuracy of LO. We evaluated our method, namely PSF-LO, on the public dataset KITTI Odometry Benchmark and ranked #1 among semantic lidar methods with an average translation error of 0.82% in the test dataset at the time of writing.



### Where to Look and How to Describe: Fashion Image Retrieval with an Attentional Heterogeneous Bilinear Network
- **Arxiv ID**: http://arxiv.org/abs/2010.13357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13357v1)
- **Published**: 2020-10-26 06:01:09+00:00
- **Updated**: 2020-10-26 06:01:09+00:00
- **Authors**: Haibo Su, Peng Wang, Lingqiao Liu, Hui Li, Zhen Li, Yanning Zhang
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Fashion products typically feature in compositions of a variety of styles at different clothing parts. In order to distinguish images of different fashion products, we need to extract both appearance (i.e., "how to describe") and localization (i.e.,"where to look") information, and their interactions. To this end, we propose a biologically inspired framework for image-based fashion product retrieval, which mimics the hypothesized twostream visual processing system of human brain. The proposed attentional heterogeneous bilinear network (AHBN) consists of two branches: a deep CNN branch to extract fine-grained appearance attributes and a fully convolutional branch to extract landmark localization information. A joint channel-wise attention mechanism is further applied to the extracted heterogeneous features to focus on important channels, followed by a compact bilinear pooling layer to model the interaction of the two streams. Our proposed framework achieves satisfactory performance on three image-based fashion product retrieval benchmarks.



### Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2010.13365v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13365v2)
- **Published**: 2020-10-26 06:32:32+00:00
- **Updated**: 2021-10-10 18:23:13+00:00
- **Authors**: Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have made significant advancement, however, they are widely known to be vulnerable to adversarial attacks. Adversarial training is the most widely used technique for improving adversarial robustness to strong white-box attacks. Prior works have been evaluating and improving the model average robustness without class-wise evaluation. The average evaluation alone might provide a false sense of robustness. For example, the attacker can focus on attacking the vulnerable class, which can be dangerous, especially, when the vulnerable class is a critical one, such as "human" in autonomous driving. We propose an empirical study on the class-wise accuracy and robustness of adversarially trained models. We find that there exists inter-class discrepancy for accuracy and robustness even when the training dataset has an equal number of samples for each class. For example, in CIFAR10, "cat" is much more vulnerable than other classes. Moreover, this inter-class discrepancy also exists for normally trained models, while adversarial training tends to further increase the discrepancy. Our work aims to investigate the following questions: (a) is the phenomenon of inter-class discrepancy universal regardless of datasets, model architectures and optimization hyper-parameters? (b) If so, what can be possible explanations for the inter-class discrepancy? (c) Can the techniques proposed in the long tail classification be readily extended to adversarial training for addressing the inter-class discrepancy?



### What is the best data augmentation for 3D brain tumor segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2010.13372v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13372v2)
- **Published**: 2020-10-26 07:00:16+00:00
- **Updated**: 2021-01-16 15:28:23+00:00
- **Authors**: Marco Domenico Cirillo, David Abramian, Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: Training segmentation networks requires large annotated datasets, which in medical imaging can be hard to obtain. Despite this fact, data augmentation has in our opinion not been fully explored for brain tumor segmentation. In this project we apply different types of data augmentation (flipping, rotation, scaling, brightness adjustment, elastic deformation) when training a standard 3D U-Net, and demonstrate that augmentation significantly improves the network's performance in many cases. Our conclusion is that brightness augmentation and elastic deformation work best, and that combinations of different augmentation techniques do not provide further improvement compared to only using one augmentation technique. Our code is available at https://github.com/mdciri/3D-augmentation-techniques.



### Video-based Facial Expression Recognition using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13386v1)
- **Published**: 2020-10-26 07:31:51+00:00
- **Updated**: 2020-10-26 07:31:51+00:00
- **Authors**: Daizong Liu, Hongting Zhang, Pan Zhou
- **Comment**: Accepted by ICPR2020
- **Journal**: None
- **Summary**: Facial expression recognition (FER), aiming to classify the expression present in the facial image or video, has attracted a lot of research interests in the field of artificial intelligence and multimedia. In terms of video based FER task, it is sensible to capture the dynamic expression variation among the frames to recognize facial expression. However, existing methods directly utilize CNN-RNN or 3D CNN to extract the spatial-temporal features from different facial units, instead of concentrating on a certain region during expression variation capturing, which leads to limited performance in FER. In our paper, we introduce a Graph Convolutional Network (GCN) layer into a common CNN-RNN based model for video-based FER. First, the GCN layer is utilized to learn more significant facial expression features which concentrate on certain regions after sharing information between extracted CNN features of nodes. Then, a LSTM layer is applied to learn long-term dependencies among the GCN learned features to model the variation. In addition, a weight assignment mechanism is also designed to weight the output of different nodes for final classification by characterizing the expression intensities in each frame. To the best of our knowledge, it is the first time to use GCN in FER task. We evaluate our method on three widely-used datasets, CK+, Oulu-CASIA and MMI, and also one challenging wild dataset AFEW8.0, and the experimental results demonstrate that our method has superior performance to existing methods.



### Flexible Piecewise Curves Estimation for Photo Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2010.13412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13412v1)
- **Published**: 2020-10-26 08:16:25+00:00
- **Updated**: 2020-10-26 08:16:25+00:00
- **Authors**: Chongyi Li, Chunle Guo, Qiming Ai, Shangchen Zhou, Chen Change Loy
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: This paper presents a new method, called FlexiCurve, for photo enhancement. Unlike most existing methods that perform image-to-image mapping, which requires expensive pixel-wise reconstruction, FlexiCurve takes an input image and estimates global curves to adjust the image. The adjustment curves are specially designed for performing piecewise mapping, taking nonlinear adjustment and differentiability into account. To cope with challenging and diverse illumination properties in real-world images, FlexiCurve is formulated as a multi-task framework to produce diverse estimations and the associated confidence maps. These estimations are adaptively fused to improve local enhancements of different regions. Thanks to the image-to-curve formulation, for an image with a size of 512*512*3, FlexiCurve only needs a lightweight network (150K trainable parameters) and it has a fast inference speed (83FPS on a single NVIDIA 2080Ti GPU). The proposed method improves efficiency without compromising the enhancement quality and losing details in the original image. The method is also appealing as it is not limited to paired training data, thus it can flexibly learn rich enhancement styles from unpaired data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on photo enhancement quantitively and qualitatively.



### Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores
- **Arxiv ID**: http://arxiv.org/abs/2010.13418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13418v2)
- **Published**: 2020-10-26 08:39:37+00:00
- **Updated**: 2021-08-04 13:18:13+00:00
- **Authors**: Aozhi Liu, Lipei Zhang, Yaqi Mei, Baoqiang Han, Zifeng Cai, Zhaohua Zhu, Jing Xiao
- **Comment**: 5 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: One of the challenges of the Optical Music Recognition task is to transcript the symbols of the camera-captured images into digital music notations. Previous end-to-end model which was developed as a Convolutional Recurrent Neural Network does not explore sufficient contextual information from full scales and there is still a large room for improvement. We propose an innovative framework that combines a block of Residual Recurrent Convolutional Neural Network with a recurrent Encoder-Decoder network to map a sequence of monophonic music symbols corresponding to the notations present in the image. The Residual Recurrent Convolutional block can improve the ability of the model to enrich the context information. The experiment results are benchmarked against a publicly available dataset called CAMERA-PRIMUS, which demonstrates that our approach surpass the state-of-the-art end-to-end method using Convolutional Recurrent Neural Network.



### Lane detection in complex scenes based on end-to-end neural network
- **Arxiv ID**: http://arxiv.org/abs/2010.13422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13422v1)
- **Published**: 2020-10-26 08:46:35+00:00
- **Updated**: 2020-10-26 08:46:35+00:00
- **Authors**: Wenbo Liu, Fei Yan, Kuan Tang, Jiyong Zhang, Tao Deng
- **Comment**: Accepted by 2020 China Automation Congress (CAC)
- **Journal**: None
- **Summary**: The lane detection is a key problem to solve the division of derivable areas in unmanned driving, and the detection accuracy of lane lines plays an important role in the decision-making of vehicle driving. Scenes faced by vehicles in daily driving are relatively complex. Bright light, insufficient light, and crowded vehicles will bring varying degrees of difficulty to lane detection. So we combine the advantages of spatial convolution in spatial information processing and the efficiency of ERFNet in semantic segmentation, propose an end-to-end network to lane detection in a variety of complex scenes. And we design the information exchange block by combining spatial convolution and dilated convolution, which plays a great role in understanding detailed information. Finally, our network was tested on the CULane database and its F1-measure with IOU threshold of 0.5 can reach 71.9%.



### Multi-object tracking with self-supervised associating network
- **Arxiv ID**: http://arxiv.org/abs/2010.13424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13424v1)
- **Published**: 2020-10-26 08:48:23+00:00
- **Updated**: 2020-10-26 08:48:23+00:00
- **Authors**: Tae-young Chung, Heansung Lee, Myeong Ah Cho, Suhwan Cho, Sangyoun Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Object Tracking (MOT) is the task that has a lot of potential for development, and there are still many problems to be solved. In the traditional tracking by detection paradigm, There has been a lot of work on feature based object re-identification methods. However, this method has a lack of training data problem. For labeling multi-object tracking dataset, every detection in a video sequence need its location and IDs. Since assigning consecutive IDs to each detection in every sequence is a very labor-intensive task, current multi-object tracking dataset is not sufficient enough to train re-identification network. So in this paper, we propose a novel self-supervised learning method using a lot of short videos which has no human labeling, and improve the tracking performance through the re-identification network trained in the self-supervised manner to solve the lack of training data problem. Despite the re-identification network is trained in a self-supervised manner, it achieves the state-of-the-art performance of MOTA 62.0\% and IDF1 62.6\% on the MOT17 test benchmark. Furthermore, the performance is improved as much as learned with a large amount of data, it shows the potential of self-supervised method.



### A Weakly-Supervised Semantic Segmentation Approach based on the Centroid Loss: Application to Quality Control and Inspection
- **Arxiv ID**: http://arxiv.org/abs/2010.13433v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13433v3)
- **Published**: 2020-10-26 09:08:21+00:00
- **Updated**: 2021-03-04 14:41:06+00:00
- **Authors**: Kai Yao, Alberto Ortiz, Francisco Bonnin-Pascual
- **Comment**: None
- **Journal**: None
- **Summary**: It is generally accepted that one of the critical parts of current vision algorithms based on deep learning and convolutional neural networks is the annotation of a sufficient number of images to achieve competitive performance. This is particularly difficult for semantic segmentation tasks since the annotation must be ideally generated at the pixel level. Weakly-supervised semantic segmentation aims at reducing this cost by employing simpler annotations that, hence, are easier, cheaper and quicker to produce. In this paper, we propose and assess a new weakly-supervised semantic segmentation approach making use of a novel loss function whose goal is to counteract the effects of weak annotations. To this end, this loss function comprises several terms based on partial cross-entropy losses, being one of them the Centroid Loss. This term induces a clustering of the image pixels in the object classes under consideration, whose aim is to improve the training of the segmentation network by guiding the optimization. The performance of the approach is evaluated against datasets from two different industry-related case studies: while one involves the detection of instances of a number of different object classes in the context of a quality control application, the other stems from the visual inspection domain and deals with the localization of images areas whose pixels correspond to scene surface points affected by a specific sort of defect. The detection results that are reported for both cases show that, despite the differences among them and the particular challenges, the use of weak annotations do not prevent from achieving a competitive performance level for both.



### On Embodied Visual Navigation in Real Environments Through Habitat
- **Arxiv ID**: http://arxiv.org/abs/2010.13439v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13439v1)
- **Published**: 2020-10-26 09:19:07+00:00
- **Updated**: 2020-10-26 09:19:07+00:00
- **Authors**: Marco Rosano, Antonino Furnari, Luigi Gulino, Giovanni Maria Farinella
- **Comment**: Published in International Conference on Pattern Recognition (ICPR),
  2020
- **Journal**: None
- **Summary**: Visual navigation models based on deep learning can learn effective policies when trained on large amounts of visual observations through reinforcement learning. Unfortunately, collecting the required experience in the real world requires the deployment of a robotic platform, which is expensive and time-consuming. To deal with this limitation, several simulation platforms have been proposed in order to train visual navigation policies on virtual environments efficiently. Despite the advantages they offer, simulators present a limited realism in terms of appearance and physical dynamics, leading to navigation policies that do not generalize in the real world.   In this paper, we propose a tool based on the Habitat simulator which exploits real world images of the environment, together with sensor and actuator noise models, to produce more realistic navigation episodes. We perform a range of experiments to assess the ability of such policies to generalize using virtual and real-world images, as well as observations transformed with unsupervised domain adaptation approaches. We also assess the impact of sensor and actuation noise on the navigation performance and investigate whether it allows to learn more robust navigation policies. We show that our tool can effectively help to train and evaluate navigation policies on real-world observations without running navigation pisodes in the real world.



### Matthews Correlation Coefficient Loss for Deep Convolutional Networks: Application to Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.13454v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13454v2)
- **Published**: 2020-10-26 09:50:25+00:00
- **Updated**: 2021-02-21 01:06:00+00:00
- **Authors**: Kumar Abhishek, Ghassan Hamarneh
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: The segmentation of skin lesions is a crucial task in clinical decision support systems for the computer aided diagnosis of skin lesions. Although deep learning-based approaches have improved segmentation performance, these models are often susceptible to class imbalance in the data, particularly, the fraction of the image occupied by the background healthy skin. Despite variations of the popular Dice loss function being proposed to tackle the class imbalance problem, the Dice loss formulation does not penalize misclassifications of the background pixels. We propose a novel metric-based loss function using the Matthews correlation coefficient, a metric that has been shown to be efficient in scenarios with skewed class distributions, and use it to optimize deep segmentation models. Evaluations on three skin lesion image datasets: the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset, the DermoFit Image Library, and the PH2 dataset, show that models trained using the proposed loss function outperform those trained using Dice loss by 11.25%, 4.87%, and 0.76% respectively in the mean Jaccard index. The code is available at https://github.com/kakumarabhishek/MCC-Loss.



### Does anatomical contextual information improve 3D U-Net based brain tumor segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2010.13460v3
- **DOI**: 10.3390/diagnostics11071159
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13460v3)
- **Published**: 2020-10-26 09:57:58+00:00
- **Updated**: 2022-03-04 12:13:01+00:00
- **Authors**: Iulian Emil Tampu, Neda Haj-Hosseini, Anders Eklund
- **Comment**: None
- **Journal**: Diagnostics (Basel). 2021 Jun 25;11(7):1159
- **Summary**: Effective, robust, and automatic tools for brain tumor segmentation are needed for the extraction of information useful in treatment planning from magnetic resonance (MR) images. Context-aware artificial intelligence is an emerging concept for the development of deep learning applications for computer-aided medical image analysis. In this work, it is investigated whether the addition of contextual information from the brain anatomy in the form of white matter, gray matter, and cerebrospinal fluid masks and probability maps improves U-Net-based brain tumor segmentation. The BraTS2020 dataset was used to train and test two standard 3D U-Net models that, in addition to the conventional MR image modalities, used the anatomical contextual information as extra channels in the form of binary masks (CIM) or probability maps (CIP). A baseline model (BLM) that only used the conventional MR image modalities was also trained. The impact of adding contextual information was investigated in terms of overall segmentation accuracy, model training time, domain generalization, and compensation for fewer MR modalities available for each subject. Results show that there is no statistically significant difference when comparing Dice scores between the baseline model and the contextual information models, even when comparing performances for high- and low-grade tumors independently. Only in the case of compensation for fewer MR modalities available for each subject did the addition of anatomical contextual information significantly improve the segmentation of the whole tumor. Overall, there is no overall significant improvement in segmentation performance when using anatomical contextual information in the form of either binary masks or probability maps as extra channels.



### VoteNet++: Registration Refinement for Multi-Atlas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.13484v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13484v1)
- **Published**: 2020-10-26 11:16:59+00:00
- **Updated**: 2020-10-26 11:16:59+00:00
- **Authors**: Zhipeng Ding, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-atlas segmentation (MAS) is a popular image segmentation technique for medical images. In this work, we improve the performance of MAS by correcting registration errors before label fusion. Specifically, we use a volumetric displacement field to refine registrations based on image anatomical appearance and predicted labels. We show the influence of the initial spatial alignment as well as the beneficial effect of using label information for MAS performance. Experiments demonstrate that the proposed refinement approach improves MAS performance on a 3D magnetic resonance dataset of the knee.



### Optimization for Medical Image Segmentation: Theory and Practice when evaluating with Dice Score or Jaccard Index
- **Arxiv ID**: http://arxiv.org/abs/2010.13499v1
- **DOI**: 10.1109/TMI.2020.3002417
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13499v1)
- **Published**: 2020-10-26 11:45:55+00:00
- **Updated**: 2020-10-26 11:45:55+00:00
- **Authors**: Tom Eelbode, Jeroen Bertels, Maxim Berman, Dirk Vandermeulen, Frederik Maes, Raf Bisschops, Matthew B. Blaschko
- **Comment**: 15 pages, 14 figures, accepted for publication in IEEE Transactions
  on Medical Imaging (2020)
- **Journal**: None
- **Summary**: In many medical imaging and classical computer vision tasks, the Dice score and Jaccard index are used to evaluate the segmentation performance. Despite the existence and great empirical success of metric-sensitive losses, i.e. relaxations of these metrics such as soft Dice, soft Jaccard and Lovasz-Softmax, many researchers still use per-pixel losses, such as (weighted) cross-entropy to train CNNs for segmentation. Therefore, the target metric is in many cases not directly optimized. We investigate from a theoretical perspective, the relation within the group of metric-sensitive loss functions and question the existence of an optimal weighting scheme for weighted cross-entropy to optimize the Dice score and Jaccard index at test time. We find that the Dice score and Jaccard index approximate each other relatively and absolutely, but we find no such approximation for a weighted Hamming similarity. For the Tversky loss, the approximation gets monotonically worse when deviating from the trivial weight setting where soft Tversky equals soft Dice. We verify these results empirically in an extensive validation on six medical segmentation tasks and can confirm that metric-sensitive losses are superior to cross-entropy based loss functions in case of evaluation with Dice Score or Jaccard Index. This further holds in a multi-class setting, and across different object sizes and foreground/background ratios. These results encourage a wider adoption of metric-sensitive loss functions for medical segmentation tasks where the performance measure of interest is the Dice score or Jaccard index.



### Activation Map Adaptation for Effective Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2010.13500v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13500v2)
- **Published**: 2020-10-26 11:55:13+00:00
- **Updated**: 2022-04-14 15:40:22+00:00
- **Authors**: Zhiyuan Wu, Hong Qi, Yu Jiang, Minghao Zhao, Chupeng Cui, Zongmin Yang, Xinhui Xue
- **Comment**: This is my first paper, which is not well written
- **Journal**: None
- **Summary**: Model compression becomes a recent trend due to the requirement of deploying neural networks on embedded and mobile devices. Hence, both accuracy and efficiency are of critical importance. To explore a balance between them, a knowledge distillation strategy is proposed for general visual representation learning. It utilizes our well-designed activation map adaptive module to replace some blocks of the teacher network, exploring the most appropriate supervisory features adaptively during the training process. Using the teacher's hidden layer output to prompt the student network to train so as to transfer effective semantic information.To verify the effectiveness of our strategy, this paper applied our method to cifar-10 dataset. Results demonstrate that the method can boost the accuracy of the student network by 0.6% with 6.5% loss reduction, and significantly improve its training speed.



### Hierarchical Neural Architecture Search for Deep Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2010.13501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13501v1)
- **Published**: 2020-10-26 11:57:37+00:00
- **Updated**: 2020-10-26 11:57:37+00:00
- **Authors**: Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Tom Drummond, Hongdong Li, Zongyuan Ge
- **Comment**: Accepted at NeurIPS 2020; Xuelian Cheng and Yiran Zhong made equal
  contribution
- **Journal**: None
- **Summary**: To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to enable the network the ability to choose among a set of operations (e.g., convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first end-to-end hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (i.e., feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. The code is available at https://github.com/XuelianCheng/LEAStereo.



### SHARP 2020: The 1st Shape Recovery from Partial Textured 3D Scans Challenge Results
- **Arxiv ID**: http://arxiv.org/abs/2010.13508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13508v1)
- **Published**: 2020-10-26 12:05:56+00:00
- **Updated**: 2020-10-26 12:05:56+00:00
- **Authors**: Alexandre Saint, Anis Kacem, Kseniya Cherenkova, Konstantinos Papadopoulos, Julian Chibane, Gerard Pons-Moll, Gleb Gusev, David Fofi, Djamila Aouada, Bjorn Ottersten
- **Comment**: SHARP workshop, ECCV 2020
- **Journal**: None
- **Summary**: The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is the first edition of a challenge fostering and benchmarking methods for recovering complete textured 3D scans from raw incomplete data. SHARP 2020 is organised as a workshop in conjunction with ECCV 2020. There are two complementary challenges, the first one on 3D human scans, and the second one on generic objects. Challenge 1 is further split into two tracks, focusing, first, on large body and clothing regions, and, second, on fine body details. A novel evaluation metric is proposed to quantify jointly the shape reconstruction, the texture reconstruction and the amount of completed data. Additionally, two unique datasets of 3D scans are proposed, to provide raw ground-truth data for the benchmarks. The datasets are released to the scientific community. Moreover, an accompanying custom library of software routines is also released to the scientific community. It allows for processing 3D scans, generating partial data and performing the evaluation. Results of the competition, analysed in comparison to baselines, show the validity of the proposed evaluation metrics, and highlight the challenging aspects of the task and of the datasets. Details on the SHARP 2020 challenge can be found at https://cvi2.uni.lu/sharp2020/.



### Robust Disentanglement of a Few Factors at a Time
- **Arxiv ID**: http://arxiv.org/abs/2010.13527v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.13527v1)
- **Published**: 2020-10-26 12:34:23+00:00
- **Updated**: 2020-10-26 12:34:23+00:00
- **Authors**: Benjamin Estermann, Markus Marks, Mehmet Fatih Yanik
- **Comment**: The first two authors contributed equally. Code is available at this
  url https://github.com/besterma/robust_disentanglement
- **Journal**: None
- **Summary**: Disentanglement is at the forefront of unsupervised learning, as disentangled representations of data improve generalization, interpretability, and performance in downstream tasks. Current unsupervised approaches remain inapplicable for real-world datasets since they are highly variable in their performance and fail to reach levels of disentanglement of (semi-)supervised approaches. We introduce population-based training (PBT) for improving consistency in training variational autoencoders (VAEs) and demonstrate the validity of this approach in a supervised setting (PBT-VAE). We then use Unsupervised Disentanglement Ranking (UDR) as an unsupervised heuristic to score models in our PBT-VAE training and show how models trained this way tend to consistently disentangle only a subset of the generative factors. Building on top of this observation we introduce the recursive rPU-VAE approach. We train the model until convergence, remove the learned factors from the dataset and reiterate. In doing so, we can label subsets of the dataset with the learned factors and consecutively use these labels to train one model that fully disentangles the whole dataset. With this approach, we show striking improvement in state-of-the-art unsupervised disentanglement performance and robustness across multiple datasets and metrics.



### Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13547v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13547v1)
- **Published**: 2020-10-26 12:57:28+00:00
- **Updated**: 2020-10-26 12:57:28+00:00
- **Authors**: Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, Hao Su
- **Comment**: To appear at NeurIPS 2020
- **Journal**: None
- **Summary**: Current graph neural networks (GNNs) lack generalizability with respect to scales (graph sizes, graph diameters, edge weights, etc..) when solving many graph analysis problems. Taking the perspective of synthesizing graph theory programs, we propose several extensions to address the issue. First, inspired by the dependency of the iteration number of common graph theory algorithms on graph size, we learn to terminate the message passing process in GNNs adaptively according to the computation progress. Second, inspired by the fact that many graph theory algorithms are homogeneous with respect to graph weights, we introduce homogeneous transformation layers that are universal homogeneous function approximators, to convert ordinary GNNs to be homogeneous. Experimentally, we show that our GNN can be trained from small-scale graphs but generalize well to large-scale graphs for a number of basic graph theory problems. It also shows generalizability for applications of multi-body physical simulation and image-based navigation problems.



### Classification of Important Segments in Educational Videos using Multimodal Features
- **Arxiv ID**: http://arxiv.org/abs/2010.13626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13626v1)
- **Published**: 2020-10-26 14:40:23+00:00
- **Updated**: 2020-10-26 14:40:23+00:00
- **Authors**: Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth
- **Comment**: Proceedings of the CIKM 2020 Workshops, October 19 to 20, Galway,
  Ireland
- **Journal**: None
- **Summary**: Videos are a commonly-used type of content in learning during Web search. Many e-learning platforms provide quality content, but sometimes educational videos are long and cover many topics. Humans are good in extracting important sections from videos, but it remains a significant challenge for computers. In this paper, we address the problem of assigning importance scores to video segments, that is how much information they contain with respect to the overall topic of an educational video. We present an annotation tool and a new dataset of annotated educational videos collected from popular online learning platforms. Moreover, we propose a multimodal neural architecture that utilizes state-of-the-art audio, visual and textual features. Our experiments investigate the impact of visual and temporal information, as well as the combination of multimodal features on importance prediction.



### Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer Proxies
- **Arxiv ID**: http://arxiv.org/abs/2010.13636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13636v1)
- **Published**: 2020-10-26 14:52:42+00:00
- **Updated**: 2020-10-26 14:52:42+00:00
- **Authors**: Yuehua Zhu, Muli Yang, Cheng Deng, Wei Liu
- **Comment**: Accepted in NeurIPS 2020 as a spotlight paper. Code can be found at
  https://github.com/YuehuaZhu/ProxyGML
- **Journal**: None
- **Summary**: Deep metric learning plays a key role in various machine learning tasks. Most of the previous works have been confined to sampling from a mini-batch, which cannot precisely characterize the global geometry of the embedding space. Although researchers have developed proxy- and classification-based methods to tackle the sampling issue, those methods inevitably incur a redundant computational cost. In this paper, we propose a novel Proxy-based deep Graph Metric Learning (ProxyGML) approach from the perspective of graph classification, which uses fewer proxies yet achieves better comprehensive performance. Specifically, multiple global proxies are leveraged to collectively approximate the original data points for each class. To efficiently capture local neighbor relationships, a small number of such proxies are adaptively selected to construct similarity subgraphs between these proxies and each data point. Further, we design a novel reverse label propagation algorithm, by which the neighbor relationships are adjusted according to ground-truth labels, so that a discriminative metric space can be learned during the process of subgraph classification. Extensive experiments carried out on widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate the superiority of the proposed ProxyGML over the state-of-the-art methods in terms of both effectiveness and efficiency. The source code is publicly available at https://github.com/YuehuaZhu/ProxyGML.



### SCFusion: Real-time Incremental Scene Reconstruction with Semantic Completion
- **Arxiv ID**: http://arxiv.org/abs/2010.13662v3
- **DOI**: 10.1109/3DV50981.2020.00090
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.13662v3)
- **Published**: 2020-10-26 15:31:52+00:00
- **Updated**: 2021-03-31 08:03:44+00:00
- **Authors**: Shun-Cheng Wu, Keisuke Tateno, Nassir Navab, Federico Tombari
- **Comment**: None
- **Journal**: International Conference on 3D Vision (2020), 801-810
- **Summary**: Real-time scene reconstruction from depth data inevitably suffers from occlusion, thus leading to incomplete 3D models. Partial reconstructions, in turn, limit the performance of algorithms that leverage them for applications in the context of, e.g., augmented reality, robotic navigation, and 3D mapping. Most methods address this issue by predicting the missing geometry as an offline optimization, thus being incompatible with real-time applications. We propose a framework that ameliorates this issue by performing scene reconstruction and semantic scene completion jointly in an incremental and real-time manner, based on an input sequence of depth maps. Our framework relies on a novel neural architecture designed to process occupancy maps and leverages voxel states to accurately and efficiently fuse semantic completion with the 3D global model. We evaluate the proposed approach quantitatively and qualitatively, demonstrating that our method can obtain accurate 3D semantic scene completion in real-time.



### Face Frontalization Based on Robustly Fitting a Deformable Shape Model to 3D Landmarks
- **Arxiv ID**: http://arxiv.org/abs/2010.13676v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13676v2)
- **Published**: 2020-10-26 15:52:50+00:00
- **Updated**: 2021-03-10 10:45:41+00:00
- **Authors**: Zhiqi Kang, Mostafa Sadeghi, Radu Horaud
- **Comment**: None
- **Journal**: None
- **Summary**: Face frontalization consists of synthesizing a frontally-viewed face from an arbitrarily-viewed one. The main contribution of this paper is a robust face alignment method that enables pixel-to-pixel warping. The method simultaneously estimates the rigid transformation (scale, rotation, and translation) and the non-rigid deformation between two 3D point sets: a set of 3D landmarks extracted from an arbitrary-viewed face, and a set of 3D landmarks parameterized by a frontally-viewed deformable face model. An important merit of the proposed method is its ability to deal both with noise (small perturbations) and with outliers (large errors). We propose to model inliers and outliers with the generalized Student's t-probability distribution function, a heavy-tailed distribution that is immune to non-Gaussian errors in the data. We describe in detail the associated expectation-maximization (EM) algorithm that alternates between the estimation of (i) the rigid parameters, (ii) the deformation parameters, and (iii) the Student-t distribution parameters. We also propose to use the zero-mean normalized cross-correlation, between a frontalized face and the corresponding ground-truth frontally-viewed face, to evaluate the performance of frontalization. To this end, we use a dataset that contains pairs of profile-viewed and frontally-viewed faces. This evaluation, based on direct image-to-image comparison, stands in contrast with indirect evaluation, based on analyzing the effect of frontalization on face recognition.



### Deep Low-rank plus Sparse Network for Dynamic MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.13677v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.13677v3)
- **Published**: 2020-10-26 15:55:24+00:00
- **Updated**: 2021-07-20 12:51:35+00:00
- **Authors**: Wenqi Huang, Ziwen Ke, Zhuo-Xu Cui, Jing Cheng, Zhilang Qiu, Sen Jia, Leslie Ying, Yanjie Zhu, Dong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S) decomposition, or robust principal component analysis (PCA), has achieved stunning performance. However, the selection of the parameters of L+S is empirical, and the acceleration rate is limited, which are common failings of iterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many deep learning approaches have been proposed to address these issues, but few of them use a low-rank prior. In this paper, a model-based low-rank plus sparse network, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In particular, we use an alternating linearized minimization method to solve the optimization problem with low-rank and sparse regularization. Learned soft singular value thresholding is introduced to ensure the clear separation of the L component and S component. Then, the iterative steps are unrolled into a network in which the regularization parameters are learnable. We prove that the proposed L+S-Net achieves global convergence under two standard assumptions. Experiments on retrospective and prospective cardiac cine datasets show that the proposed model outperforms state-of-the-art CS and existing deep learning methods and has great potential for extremely high acceleration factors (up to 24x).



### Distributed Multi-Target Tracking in Camera Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13701v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2010.13701v3)
- **Published**: 2020-10-26 16:34:53+00:00
- **Updated**: 2021-04-16 17:57:21+00:00
- **Authors**: Sara Casao, Abel Naya, Ana C. Murillo, Eduardo Montijano
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent works on multi-target tracking with multiple cameras focus on centralized systems. In contrast, this paper presents a multi-target tracking approach implemented in a distributed camera network. The advantages of distributed systems lie in lighter communication management, greater robustness to failures and local decision making. On the other hand, data association and information fusion are more challenging than in a centralized setup, mostly due to the lack of global and complete information. The proposed algorithm boosts the benefits of the Distributed-Consensus Kalman Filter with the support of a re-identification network and a distributed tracker manager module to facilitate consistent information. These techniques complement each other and facilitate the cross-camera data association in a simple and effective manner. We evaluate the whole system with known public data sets under different conditions demonstrating the advantages of combining all the modules. In addition, we compare our algorithm to some existing centralized tracking methods, outperforming their behavior in terms of accuracy and bandwidth usage.



### Using a Supervised Method without supervision for foreground segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.07954v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07954v4)
- **Published**: 2020-10-26 16:42:37+00:00
- **Updated**: 2021-06-20 20:26:33+00:00
- **Authors**: Levi Kassel, Michael Werman
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are a powerful framework for foreground segmentation in video acquired by static cameras, segmenting moving objects from the background in a robust way in various challenging scenarios. The premier methods are those based on supervision requiring a final training stage on a database of tens to hundreds of manually segmented images from the specific static camera. In this work, we propose a method to automatically create an "artificial" database that is sufficient for training the supervised methods so that it performs better than current unsupervised methods. It is based on combining a weak foreground segmenter, compared to the supervised method, to extract suitable objects from the training images and randomly inserting these objects back into a background image. Test results are shown on the test sequences in CDnet.



### ActiveNet: A computer-vision based approach to determine lethargy
- **Arxiv ID**: http://arxiv.org/abs/2010.13714v1
- **DOI**: 10.1145/3430984.3430986
- **Categories**: **cs.CV**, cs.HC, I.2.10; I.4.7; I.4.8; I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2010.13714v1)
- **Published**: 2020-10-26 16:54:03+00:00
- **Updated**: 2020-10-26 16:54:03+00:00
- **Authors**: Aitik Gupta, Aadit Agarwal
- **Comment**: Accepted at The ACM India Joint International Conference on Data
  Science and Management of Data (CoDS-COMAD) 2021
- **Journal**: None
- **Summary**: The outbreak of COVID-19 has forced everyone to stay indoors, fabricating a significant drop in physical activeness. Our work is constructed upon the idea to formulate a backbone mechanism, to detect levels of activeness in real-time, using a single monocular image of a target person. The scope can be generalized under many applications, be it in an interview, online classes, security surveillance, et cetera. We propose a Computer Vision based multi-stage approach, wherein the pose of a person is first detected, encoded with a novel approach, and then assessed by a classical machine learning algorithm to determine the level of activeness. An alerting system is wrapped around the approach to provide a solution to inhibit lethargy by sending notification alerts to individuals involved.



### ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate Dependent Video Quality Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.13715v2
- **DOI**: 10.1109/TIP.2021.3106801
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13715v2)
- **Published**: 2020-10-26 16:54:33+00:00
- **Updated**: 2021-09-27 03:12:23+00:00
- **Authors**: Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing. 30 (2021) 7446 - 7457
- **Summary**: We consider the problem of conducting frame rate dependent video quality assessment (VQA) on videos of diverse frame rates, including high frame rate (HFR) videos. More generally, we study how perceptual quality is affected by frame rate, and how frame rate and compression combine to affect perceived quality. We devise an objective VQA model called Space-Time GeneRalized Entropic Difference (GREED) which analyzes the statistics of spatial and temporal band-pass video coefficients. A generalized Gaussian distribution (GGD) is used to model band-pass responses, while entropy variations between reference and distorted videos under the GGD model are used to capture video quality variations arising from frame rate changes. The entropic differences are calculated across multiple temporal and spatial subbands, and merged using a learned regressor. We show through extensive experiments that GREED achieves state-of-the-art performance on the LIVE-YT-HFR Database when compared with existing VQA models. The features used in GREED are highly generalizable and obtain competitive performance even on standard, non-HFR VQA databases. The implementation of GREED has been made available online: https://github.com/pavancm/GREED



### Demo Abstract: Indoor Positioning System in Visually-Degraded Environments with Millimetre-Wave Radar and Inertial Sensors
- **Arxiv ID**: http://arxiv.org/abs/2010.13750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.13750v1)
- **Published**: 2020-10-26 17:41:25+00:00
- **Updated**: 2020-10-26 17:41:25+00:00
- **Authors**: Zhuangzhuang Dai, Muhamad Risqi U. Saputra, Chris Xiaoxuan Lu, Niki Trigoni, Andrew Markham
- **Comment**: Appear as demo abstract at the ACM Conference on Embedded Networked
  Sensor Systems (SenSys 2020)
- **Journal**: None
- **Summary**: Positional estimation is of great importance in the public safety sector. Emergency responders such as fire fighters, medical rescue teams, and the police will all benefit from a resilient positioning system to deliver safe and effective emergency services. Unfortunately, satellite navigation (e.g., GPS) offers limited coverage in indoor environments. It is also not possible to rely on infrastructure based solutions. To this end, wearable sensor-aided navigation techniques, such as those based on camera and Inertial Measurement Units (IMU), have recently emerged recently as an accurate, infrastructure-free solution. Together with an increase in the computational capabilities of mobile devices, motion estimation can be performed in real-time. In this demonstration, we present a real-time indoor positioning system which fuses millimetre-wave (mmWave) radar and IMU data via deep sensor fusion. We employ mmWave radar rather than an RGB camera as it provides better robustness to visual degradation (e.g., smoke, darkness, etc.) while at the same time requiring lower computational resources to enable runtime computation. We implemented the sensor system on a handheld device and a mobile computer running at 10 FPS to track a user inside an apartment. Good accuracy and resilience were exhibited even in poorly illuminated scenes.



### Handgun detection using combined human pose and weapon appearance
- **Arxiv ID**: http://arxiv.org/abs/2010.13753v4
- **DOI**: 10.1109/ACCESS.2021.3110335
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.13753v4)
- **Published**: 2020-10-26 17:45:12+00:00
- **Updated**: 2021-07-23 09:55:57+00:00
- **Authors**: Jesus Ruiz-Santaquiteria, Alberto Velasco-Mata, Noelia Vallez, Gloria Bueno, Juan A. lvarez-Garca, Oscar Deniz
- **Comment**: 17 pages, 18 figures
- **Journal**: IEEE Access, Volume 9 (2021) 123815 - 123826
- **Summary**: Closed-circuit television (CCTV) systems are essential nowadays to prevent security threats or dangerous situations, in which early detection is crucial. Novel deep learning-based methods have allowed to develop automatic weapon detectors with promising results. However, these approaches are mainly based on visual weapon appearance only. For handguns, body pose may be a useful cue, especially in cases where the gun is barely visible. In this work, a novel method is proposed to combine, in a single architecture, both weapon appearance and human pose information. First, pose keypoints are estimated to extract hand regions and generate binary pose images, which are the model inputs. Then, each input is processed in different subnetworks and combined to produce the handgun bounding box. Results obtained show that the combined model improves the handgun detection state of the art, achieving from 4.23 to 18.9 AP points more than the best previous approach.



### Refactoring Policy for Compositional Generalizability using Self-Supervised Object Proposals
- **Arxiv ID**: http://arxiv.org/abs/2011.00971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.00971v1)
- **Published**: 2020-10-26 17:46:08+00:00
- **Updated**: 2020-10-26 17:46:08+00:00
- **Authors**: Tongzhou Mu, Jiayuan Gu, Zhiwei Jia, Hao Tang, Hao Su
- **Comment**: 34th Conference on Neural Information Processing Systems (NeurIPS
  2020), Vancouver, Canada
- **Journal**: None
- **Summary**: We study how to learn a policy with compositional generalizability. We propose a two-stage framework, which refactorizes a high-reward teacher policy into a generalizable student policy with strong inductive bias. Particularly, we implement an object-centric GNN-based student policy, whose input objects are learned from images through self-supervised learning. Empirically, we evaluate our approach on four difficult tasks that require compositional generalizability, and achieve superior performance compared to baselines.



### Instance Semantic Segmentation Benefits from Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13757v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2010.13757v2)
- **Published**: 2020-10-26 17:47:30+00:00
- **Updated**: 2021-12-04 05:00:33+00:00
- **Authors**: Quang H. Le, Kamal Youcef-Toumi, Dzmitry Tsetserukou, Ali Jahanian
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: In design of instance segmentation networks that reconstruct masks, segmentation is often taken as its literal definition -- assigning each pixel a label. This has led to thinking the problem as a template matching one with the goal of minimizing the loss between the reconstructed and the ground truth pixels. Rethinking reconstruction networks as a generator, we define the problem of predicting masks as a GANs game framework: A segmentation network generates the masks, and a discriminator network decides on the quality of the masks. To demonstrate this game, we show effective modifications on the general segmentation framework in Mask R-CNN. We find that playing the game in feature space is more effective than the pixel space leading to stable training between the discriminator and the generator, predicting object coordinates should be replaced by predicting contextual regions for objects, and overall the adversarial loss helps the performance and removes the need for any custom settings per different data domain. We test our framework in various domains and report on cellphone recycling, autonomous driving, large-scale object detection, and medical glands. We observe in general GANs yield masks that account for crispier boundaries, clutter, small objects, and details, being in domain of regular shapes or heterogeneous and coalescing shapes. Our code for reproducing the results is available publicly.



### GreedyFool: Distortion-Aware Sparse Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2010.13773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13773v1)
- **Published**: 2020-10-26 17:59:07+00:00
- **Updated**: 2020-10-26 17:59:07+00:00
- **Authors**: Xiaoyi Dong, Dongdong Chen, Jianmin Bao, Chuan Qin, Lu Yuan, Weiming Zhang, Nenghai Yu, Dong Chen
- **Comment**: To appear in NeurIPS 2020, code:
  https://github.com/LightDXY/GreedyFool
- **Journal**: None
- **Summary**: Modern deep neural networks(DNNs) are vulnerable to adversarial samples. Sparse adversarial samples are a special branch of adversarial samples that can fool the target model by only perturbing a few pixels. The existence of the sparse adversarial attack points out that DNNs are much more vulnerable than people believed, which is also a new aspect for analyzing DNNs. However, current sparse adversarial attack methods still have some shortcomings on both sparsity and invisibility. In this paper, we propose a novel two-stage distortion-aware greedy-based method dubbed as "GreedyFool". Specifically, it first selects the most effective candidate positions to modify by considering both the gradient(for adversary) and the distortion map(for invisibility), then drops some less important points in the reduce stage. Experiments demonstrate that compared with the start-of-the-art method, we only need to modify $3\times$ fewer pixels under the same sparse perturbation setting. For target attack, the success rate of our method is 9.96\% higher than the start-of-the-art method under the same pixel budget. Code can be found at https://github.com/LightDXY/GreedyFool.



### Wavelet Flow: Fast Training of High Resolution Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2010.13821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13821v1)
- **Published**: 2020-10-26 18:13:43+00:00
- **Updated**: 2020-10-26 18:13:43+00:00
- **Authors**: Jason J. Yu, Konstantinos G. Derpanis, Marcus A. Brubaker
- **Comment**: Manuscript appendix images compressed with JPEG to meet arXiv size
  limits. Visit the project page for PNG versions:
  https://yorkucvil.github.io/Wavelet-Flow
- **Journal**: None
- **Summary**: Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15x faster to train.



### Generative Tomography Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2010.14933v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2010.14933v2)
- **Published**: 2020-10-26 18:22:37+00:00
- **Updated**: 2020-11-25 22:05:56+00:00
- **Authors**: Matteo Ronchetti, Davide Bacciu
- **Comment**: Accepted as a poster for the NeurIPS 2020 Workshop on Deep Learning
  and Inverse Problems
- **Journal**: None
- **Summary**: We propose an end-to-end differentiable architecture for tomography reconstruction that directly maps a noisy sinogram into a denoised reconstruction. Compared to existing approaches our end-to-end architecture produces more accurate reconstructions while using less parameters and time. We also propose a generative model that, given a noisy sinogram, can sample realistic reconstructions. This generative model can be used as prior inside an iterative process that, by taking into consideration the physical model, can reduce artifacts and errors in the reconstructions.



### Application of sequential processing of computer vision methods for solving the problem of detecting the edges of a honeycomb block
- **Arxiv ID**: http://arxiv.org/abs/2010.13837v1
- **DOI**: 10.1088/1742-6596/1679/4/042098
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.13837v1)
- **Published**: 2020-10-26 18:48:46+00:00
- **Updated**: 2020-10-26 18:48:46+00:00
- **Authors**: M V Kubrikov, I A Paulin, M V Saramud, A S Kubrikova
- **Comment**: 10 pages, 13 figures, APITECH-II - 2020
- **Journal**: None
- **Summary**: The article describes the application of the Hough transform to a honeycomb block image. The problem of cutting a mold from a honeycomb block is described. A number of image transformations are considered to increase the efficiency of the Hough algorithm. A method for obtaining a binary image using a simple threshold, a method for obtaining a binary image using Otsu binarization, and the Canny Edge Detection algorithm are considered. The method of binary skeleton (skeletonization) is considered, in which the skeleton is obtained using 2 main morphological operations: Dilation and Erosion. As a result of a number of experiments, the optimal sequence of processing the original image was revealed, which allows obtaining the coordinates of the maximum number of faces. This result allows one to choose the optimal places for cutting a honeycomb block, which will improve the quality of the resulting shapes.



### Peak Detection On Data Independent Acquisition Mass Spectrometry Data With Semisupervised Convolutional Transformers
- **Arxiv ID**: http://arxiv.org/abs/2010.13841v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2010.13841v1)
- **Published**: 2020-10-26 18:55:27+00:00
- **Updated**: 2020-10-26 18:55:27+00:00
- **Authors**: Leon L. Xu, Hannes L. Rst
- **Comment**: None
- **Journal**: None
- **Summary**: Liquid Chromatography coupled to Mass Spectrometry (LC-MS) based methods are commonly used for high-throughput, quantitative measurements of the proteome (i.e. the set of all proteins in a sample at a given time). Targeted LC-MS produces data in the form of a two-dimensional time series spectrum, with the mass to charge ratio of analytes (m/z) on one axis, and the retention time from the chromatography on the other. The elution of a peptide of interest produces highly specific patterns across multiple fragment ion traces (extracted ion chromatograms, or XICs). In this paper, we formulate this peak detection problem as a multivariate time series segmentation problem, and propose a novel approach based on the Transformer architecture. Here we augment Transformers, which are capable of capturing long distance dependencies with a global view, with Convolutional Neural Networks (CNNs), which can capture local context important to the task at hand, in the form of Transformers with Convolutional Self-Attention. We further train this model in a semisupervised manner by adapting state of the art semisupervised image classification techniques for multi-channel time series data. Experiments on a representative LC-MS dataset are benchmarked using manual annotations to showcase the encouraging performance of our method; it outperforms baseline neural network architectures and is competitive against the current state of the art in automated peak detection.



### Enhancing road signs segmentation using photometric invariants
- **Arxiv ID**: http://arxiv.org/abs/2010.13844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13844v1)
- **Published**: 2020-10-26 18:59:06+00:00
- **Updated**: 2020-10-26 18:59:06+00:00
- **Authors**: Tarik Ayaou, Azeddine Beghdadi, Karim Afdel, Abdellah Amghar
- **Comment**: 5 pages, Submitted to the Computing Conference 2021
- **Journal**: None
- **Summary**: Road signs detection and recognition in natural scenes is one of the most important tasksin the design of Intelligent Transport Systems (ITS). However, illumination changes remain a major problem. In this paper, an efficient ap-proach of road signs segmentation based on photometric invariants is proposed. This method is based on color in-formation using a hybrid distance, by exploiting the chro-matic distance and the red and blue ratio, on l Theta Phi color space which is invariant to highlight, shading and shadow changes. A comparative study is performed to demonstrate the robustness of this approach over the most frequently used methods for road sign segmentation. The experimental results and the detailed analysis show the high performance of the algorithm described in this paper.



### Multi-Class Zero-Shot Learning for Artistic Material Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.13850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13850v1)
- **Published**: 2020-10-26 19:04:50+00:00
- **Updated**: 2020-10-26 19:04:50+00:00
- **Authors**: Alexander W Olson, Andreea Cucu, Tom Bock
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) is an extreme form of transfer learning, where no labelled examples of the data to be classified are provided during the training stage. Instead, ZSL uses additional information learned about the domain, and relies upon transfer learning algorithms to infer knowledge about the missing instances. ZSL approaches are an attractive solution for sparse datasets. Here we outline a model to identify the materials with which a work of art was created, by learning the relationship between English descriptions of the subject of a piece and its composite materials. After experimenting with a range of hyper-parameters, we produce a model which is capable of correctly identifying the materials used on pieces from an entirely distinct museum dataset. This model returned a classification accuracy of 48.42% on 5,000 artworks taken from the Tate collection, which is distinct from the Rijksmuseum network used to create and train our model.



### Improved Supervised Training of Physics-Guided Deep Learning Image Reconstruction with Multi-Masking
- **Arxiv ID**: http://arxiv.org/abs/2010.13868v1
- **DOI**: 10.1109/ICASSP39728.2021.9413495
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.13868v1)
- **Published**: 2020-10-26 19:39:32+00:00
- **Updated**: 2020-10-26 19:39:32+00:00
- **Authors**: Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Mehmet Akakaya
- **Comment**: None
- **Journal**: Proceedings of IEEE ICASSP, 2021
- **Summary**: Physics-guided deep learning (PG-DL) via algorithm unrolling has received significant interest for improved image reconstruction, including MRI applications. These methods unroll an iterative optimization algorithm into a series of regularizer and data consistency units. The unrolled networks are typically trained end-to-end using a supervised approach. Current supervised PG-DL approaches use all of the available sub-sampled measurements in their data consistency units. Thus, the network learns to fit the rest of the measurements. In this study, we propose to improve the performance and robustness of supervised training by utilizing randomness by retrospectively selecting only a subset of all the available measurements for data consistency units. The process is repeated multiple times using different random masks during training for further enhancement. Results on knee MRI show that the proposed multi-mask supervised PG-DL enhances reconstruction performance compared to conventional supervised PG-DL approaches.



### Shimon the Robot Film Composer and DeepScore: An LSTM for Generation of Film Scores based on Visual Analysis
- **Arxiv ID**: http://arxiv.org/abs/2011.07953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2011.07953v1)
- **Published**: 2020-10-26 19:41:47+00:00
- **Updated**: 2020-10-26 19:41:47+00:00
- **Authors**: Richard Savery, Gil Weinberg
- **Comment**: Computer Simulation of Musical Creativity, 20th-22nd August,
  University College Dublin
- **Journal**: None
- **Summary**: Composing for a film requires developing an understanding of the film, its characters and the film aesthetic choices made by the director. We propose using existing visual analysis systems as a core technology for film music generation. We extract film features including main characters and their emotions to develop a computer understanding of the film's narrative arc. This arc is combined with visually analyzed director aesthetic choices including pacing and levels of movement. Two systems are presented, the first using a robotic film composer and marimbist to generate film scores in real-time performance. The second software-based system builds on the results from the robot film composer to create narrative driven film scores.



### Comprehensive evaluation of no-reference image quality assessment algorithms on authentic distortions
- **Arxiv ID**: http://arxiv.org/abs/2011.07950v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07950v1)
- **Published**: 2020-10-26 21:25:46+00:00
- **Updated**: 2020-10-26 21:25:46+00:00
- **Authors**: Domonkos Varga
- **Comment**: None
- **Journal**: None
- **Summary**: Objective image quality assessment deals with the prediction of digital images' perceptual quality. No-reference image quality assessment predicts the quality of a given input image without any knowledge or information about its pristine (distortion free) counterpart. Machine learning algorithms are heavily used in no-reference image quality assessment because it is very complicated to model the human visual system's quality perception. Moreover, no-reference image quality assessment algorithms are evaluated on publicly available benchmark databases. These databases contain images with their corresponding quality scores. In this study, we evaluate several machine learning based NR-IQA methods and one opinion unaware method on databases consisting of authentic distortions. Specifically, LIVE In the Wild and KonIQ-10k databases were applied to evaluate the state-of-the-art. For machine learning based methods, appx. 80% were used for training and the remaining 20% were used for testing. Furthermore, average PLCC, SROCC, and KROCC values were reported over 100 random train-test splits. The statistics of PLCC, SROCC, and KROCC values were also published using boxplots. Our evaluation results may be helpful to obtain a clear understanding about the status of state-of-the-art no-reference image quality assessment methods.



### Processing of incomplete images by (graph) convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13914v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13914v1)
- **Published**: 2020-10-26 21:40:03+00:00
- **Updated**: 2020-10-26 21:40:03+00:00
- **Authors**: Tomasz Danel, Marek mieja, ukasz Struski, Przemysaw Spurek, ukasz Maziarka
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the problem of training neural networks from incomplete images without replacing missing values. For this purpose, we first represent an image as a graph, in which missing pixels are entirely ignored. The graph image representation is processed using a spatial graph convolutional network (SGCN) -- a type of graph convolutional networks, which is a proper generalization of classical CNNs operating on images. On one hand, our approach avoids the problem of missing data imputation while, on the other hand, there is a natural correspondence between CNNs and SGCN. Experiments confirm that our approach performs better than analogical CNNs with the imputation of missing values on typical classification and reconstruction tasks.



### Neural Unsigned Distance Fields for Implicit Function Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.13938v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13938v1)
- **Published**: 2020-10-26 22:49:45+00:00
- **Updated**: 2020-10-26 22:49:45+00:00
- **Authors**: Julian Chibane, Aymen Mir, Gerard Pons-Moll
- **Comment**: Neural Information Processing Systems (NeurIPS) 2020
- **Journal**: Neural Information Processing Systems (NeurIPS) 2020
- **Summary**: In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.



### Developing Univariate Neurodegeneration Biomarkers with Low-Rank and Sparse Subspace Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2010.13954v1
- **DOI**: 10.1016/j.media.2020.101877
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13954v1)
- **Published**: 2020-10-26 23:42:43+00:00
- **Updated**: 2020-10-26 23:42:43+00:00
- **Authors**: Gang Wang, Qunxi Dong, Jianfeng Wu, Yi Su, Kewei Chen, Qingtang Su, Xiaofeng Zhang, Jinguang Hao, Tao Yao, Li Liu, Caiming Zhang, Richard J Caselli, Eric M Reiman, Yalin Wang
- **Comment**: Accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: Cognitive decline due to Alzheimer's disease (AD) is closely associated with brain structure alterations captured by structural magnetic resonance imaging (sMRI). It supports the validity to develop sMRI-based univariate neurodegeneration biomarkers (UNB). However, existing UNB work either fails to model large group variances or does not capture AD dementia (ADD) induced changes. We propose a novel low-rank and sparse subspace decomposition method capable of stably quantifying the morphological changes induced by ADD. Specifically, we propose a numerically efficient rank minimization mechanism to extract group common structure and impose regularization constraints to encode the original 3D morphometry connectivity. Further, we generate regions-of-interest (ROI) with group difference study between common subspaces of $A\beta+$ AD and $A\beta-$ cognitively unimpaired (CU) groups. A univariate morphometry index (UMI) is constructed from these ROIs by summarizing individual morphological characteristics weighted by normalized difference between $A\beta+$ AD and $A\beta-$ CU groups. We use hippocampal surface radial distance feature to compute the UMIs and validate our work in the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. With hippocampal UMIs, the estimated minimum sample sizes needed to detect a 25$\%$ reduction in the mean annual change with 80$\%$ power and two-tailed $P=0.05$ are 116, 279 and 387 for the longitudinal $A\beta+$ AD, $A\beta+$ mild cognitive impairment (MCI) and $A\beta+$ CU groups, respectively. Additionally, for MCI patients, UMIs well correlate with hazard ratio of conversion to AD ($4.3$, $95\%$ CI=$2.3-8.2$) within 18 months. Our experimental results outperform traditional hippocampal volume measures and suggest the application of UMI as a potential UNB.



### MELD: Meta-Reinforcement Learning from Images via Latent State Models
- **Arxiv ID**: http://arxiv.org/abs/2010.13957v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.13957v2)
- **Published**: 2020-10-26 23:50:30+00:00
- **Updated**: 2021-01-11 16:09:19+00:00
- **Authors**: Tony Z. Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, Sergey Levine
- **Comment**: Accepted to CoRL 2020. Supplementary material at
  https://sites.google.com/view/meld-lsm/home . 16 pages, 19 figures. V2: add
  funding acknowledgements, reduce file size
- **Journal**: None
- **Summary**: Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can \emph{also} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only $8$ hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.



### Detector Algorithms of Bounding Box and Segmentation Mask of a Mask R-CNN Model
- **Arxiv ID**: http://arxiv.org/abs/2010.13783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.13783v1)
- **Published**: 2020-10-26 23:56:41+00:00
- **Updated**: 2020-10-26 23:56:41+00:00
- **Authors**: Haruhiro Fujita, Masatoshi Itagaki, Yew Kwang Hooi, Kenta Ichikawa, Kazutaka Kawano, Ryo Yamamoto
- **Comment**: 8 pages, 9 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:2010.11464
- **Journal**: None
- **Summary**: Detection performances on bounding box and segmentation mask outputs of Mask R-CNN models are evaluated. There are significant differences in detection performances of bounding boxes and segmentation masks, where the former is constantly superior to the latter. Harmonic values of precisions and recalls of linear cracks, joints, fillings, and shadows are significantly lower in segmentation masks than bounding boxes. Other classes showed similar harmonic values. Discussions are made on different performances of detection metrics of bounding boxes and segmentation masks focusing on detection algorithms of both detectors.




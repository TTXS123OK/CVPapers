# Arxiv Papers in cs.CV on 2020-10-01
### Deformable Kernel Convolutional Network for Video Extreme Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.00154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00154v1)
- **Published**: 2020-10-01 00:22:42+00:00
- **Updated**: 2020-10-01 00:22:42+00:00
- **Authors**: Xuan Xu, Xin Xiong, Jinge Wang, Xin Li
- **Comment**: To appear in ECCVW 2020
- **Journal**: None
- **Summary**: Video super-resolution, which attempts to reconstruct high-resolution video frames from their corresponding low-resolution versions, has received increasingly more attention in recent years. Most existing approaches opt to use deformable convolution to temporally align neighboring frames and apply traditional spatial attention mechanism (convolution based) to enhance reconstructed features. However, such spatial-only strategies cannot fully utilize temporal dependency among video frames. In this paper, we propose a novel deep learning based VSR algorithm, named Deformable Kernel Spatial Attention Network (DKSAN). Thanks to newly designed Deformable Kernel Convolution Alignment (DKC_Align) and Deformable Kernel Spatial Attention (DKSA) modules, DKSAN can better exploit both spatial and temporal redundancies to facilitate the information propagation across different layers. We have tested DKSAN on AIM2020 Video Extreme Super-Resolution Challenge to super-resolve videos with a scale factor as large as 16. Experimental results demonstrate that our proposed DKSAN can achieve both better subjective and objective performance compared with the existing state-of-the-art EDVR on Vid3oC and IntVID datasets.



### A Large Multi-Target Dataset of Common Bengali Handwritten Graphemes
- **Arxiv ID**: http://arxiv.org/abs/2010.00170v3
- **DOI**: 10.1007/978-3-030-86337-1_26
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00170v3)
- **Published**: 2020-10-01 01:51:45+00:00
- **Updated**: 2021-01-13 17:19:52+00:00
- **Authors**: Samiul Alam, Tahsin Reasat, Asif Shahriyar Sushmit, Sadi Mohammad Siddiquee, Fuad Rahman, Mahady Hasan, Ahmed Imtiaz Humayun
- **Comment**: 15 pages, 12 figures, 6 Tables, Submitted to CVPR-21
- **Journal**: None
- **Summary**: Latin has historically led the state-of-the-art in handwritten optical character recognition (OCR) research. Adapting existing systems from Latin to alpha-syllabary languages is particularly challenging due to a sharp contrast between their orthographies. The segmentation of graphical constituents corresponding to characters becomes significantly hard due to a cursive writing system and frequent use of diacritics in the alpha-syllabary family of languages. We propose a labeling scheme based on graphemes (linguistic segments of word formation) that makes segmentation in-side alpha-syllabary words linear and present the first dataset of Bengali handwritten graphemes that are commonly used in an everyday context. The dataset contains 411k curated samples of 1295 unique commonly used Bengali graphemes. Additionally, the test set contains 900 uncommon Bengali graphemes for out of dictionary performance evaluation. The dataset is open-sourced as a part of a public Handwritten Grapheme Classification Challenge on Kaggle to benchmark vision algorithms for multi-target grapheme classification. The unique graphemes present in this dataset are selected based on commonality in the Google Bengali ASR corpus. From competition proceedings, we see that deep-learning methods can generalize to a large span of out of dictionary graphemes which are absent during training. Dataset and starter codes at www.kaggle.com/c/bengaliai-cv19.



### Medical Imaging and Computational Image Analysis in COVID-19 Diagnosis: A Review
- **Arxiv ID**: http://arxiv.org/abs/2010.02154v1
- **DOI**: 10.1016/j.compbiomed.2021.104605
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.02154v1)
- **Published**: 2020-10-01 06:38:06+00:00
- **Updated**: 2020-10-01 06:38:06+00:00
- **Authors**: Shahabedin Nabavi, Azar Ejmalian, Mohsen Ebrahimi Moghaddam, Ahmad Ali Abin, Alejandro F. Frangi, Mohammad Mohammadi, Hamidreza Saligheh Rad
- **Comment**: 29 pages, 4 tables
- **Journal**: Computers in Biology and Medicine, 2021, 104605,
- **Summary**: Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. The disease presents with symptoms such as shortness of breath, fever, dry cough, and chronic fatigue, amongst others. Sometimes the symptoms of the disease increase so much they lead to the death of the patients. The disease may be asymptomatic in some patients in the early stages, which can lead to increased transmission of the disease to others. Many studies have tried to use medical imaging for early diagnosis of COVID-19. This study attempts to review papers on automatic methods for medical image analysis and diagnosis of COVID-19. For this purpose, PubMed, Google Scholar, arXiv and medRxiv were searched to find related studies by the end of April 2020, and the essential points of the collected studies were summarised. The contribution of this study is four-fold: 1) to use as a tutorial of the field for both clinicians and technologists, 2) to comprehensively review the characteristics of COVID-19 as presented in medical images, 3) to examine automated artificial intelligence-based approaches for COVID-19 diagnosis based on the accuracy and the method used, 4) to express the research limitations in this field and the methods used to overcome them. COVID-19 reveals signs in medical images can be used for early diagnosis of the disease even in asymptomatic patients. Using automated machine learning-based methods can diagnose the disease with high accuracy from medical images and reduce time, cost and error of diagnostic procedure. It is recommended to collect bulk imaging data from patients in the shortest possible time to improve the performance of COVID-19 automated diagnostic methods.



### Quantum Annealing Approaches to the Phase-Unwrapping Problem in Synthetic-Aperture Radar Imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.00220v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.00220v1)
- **Published**: 2020-10-01 07:04:02+00:00
- **Updated**: 2020-10-01 07:04:02+00:00
- **Authors**: Khaled A. Helal Kelany, Nikitas Dimopoulos, Clemens P. J. Adolphs, Bardia Barabadi, Amirali Baniasadi
- **Comment**: None
- **Journal**: None
- **Summary**: The focus of this work is to explore the use of quantum annealing solvers for the problem of phase unwrapping of synthetic aperture radar (SAR) images. Although solutions to this problem exist based on network programming, these techniques do not scale well to larger-sized images. Our approach involves formulating the problem as a quadratic unconstrained binary optimization (QUBO) problem, which can be solved using a quantum annealer. Given that present embodiments of quantum annealers remain limited in the number of qubits they possess, we decompose the problem into a set of subproblems that can be solved individually. These individual solutions are close to optimal up to an integer constant, with one constant per sub-image. In a second phase, these integer constants are determined as a solution to yet another QUBO problem. We test our approach with a variety of software-based QUBO solvers and on a variety of images, both synthetic and real. Additionally, we experiment using D-Wave Systems's quantum annealer, the D-Wave 2000Q. The software-based solvers obtain high-quality solutions comparable to state-of-the-art phase-unwrapping solvers. We are currently working on optimally mapping the problem onto the restricted topology of the quantum annealer to improve the quality of the solution.



### Deep Group-wise Variational Diffeomorphic Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2010.00231v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00231v1)
- **Published**: 2020-10-01 07:37:28+00:00
- **Updated**: 2020-10-01 07:37:28+00:00
- **Authors**: Tycho F. A. van der Ouderaa, Ivana Išgum, Wouter B. Veldhuis, Bob D. de Vos
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are increasingly used for pair-wise image registration. We propose to extend current learning-based image registration to allow simultaneous registration of multiple images. To achieve this, we build upon the pair-wise variational and diffeomorphic VoxelMorph approach and present a general mathematical framework that enables both registration of multiple images to their geodesic average and registration in which any of the available images can be used as a fixed image. In addition, we provide a likelihood based on normalized mutual information, a well-known image similarity metric in registration, between multiple images, and a prior that allows for explicit control over the viscous fluid energy to effectively regularize deformations. We trained and evaluated our approach using intra-patient registration of breast MRI and Thoracic 4DCT exams acquired over multiple time points. Comparison with Elastix and VoxelMorph demonstrates competitive quantitative performance of the proposed method in terms of image similarity and reference landmark distances at significantly faster registration.



### MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset for Semantic Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2010.00243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00243v1)
- **Published**: 2020-10-01 08:03:47+00:00
- **Updated**: 2020-10-01 08:03:47+00:00
- **Authors**: Xiaoman Qi, PanPan Zhu, Yuebin Wang, Liqiang Zhang, Junhuan Peng, Mengfan Wu, Jialong Chen, Xudong Zhao, Ning Zang, P. Takis Mathiopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: To better understand scene images in the field of remote sensing, multi-label annotation of scene images is necessary. Moreover, to enhance the performance of deep learning models for dealing with semantic scene understanding tasks, it is vital to train them on large-scale annotated data. However, most existing datasets are annotated by a single label, which cannot describe the complex remote sensing images well because scene images might have multiple land cover classes. Few multi-label high spatial resolution remote sensing datasets have been developed to train deep learning models for multi-label based tasks, such as scene classification and image retrieval. To address this issue, in this paper, we construct a multi-label high spatial resolution remote sensing dataset named MLRSNet for semantic scene understanding with deep learning from the overhead perspective. It is composed of high-resolution optical satellite or aerial images. MLRSNet contains a total of 109,161 samples within 46 scene categories, and each image has at least one of 60 predefined labels. We have designed visual recognition tasks, including multi-label based image classification and image retrieval, in which a wide variety of deep learning approaches are evaluated with MLRSNet. The experimental results demonstrate that MLRSNet is a significant benchmark for future research, and it complements the current widely used datasets such as ImageNet, which fills gaps in multi-label image research. Furthermore, we will continue to expand the MLRSNet. MLRSNet and all related materials have been made publicly available at https://data.mendeley.com/datasets/7j9bv9vwsx/2 and https://github.com/cugbrs/MLRSNet.git.



### CariMe: Unpaired Caricature Generation with Multiple Exaggerations
- **Arxiv ID**: http://arxiv.org/abs/2010.00246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.00246v1)
- **Published**: 2020-10-01 08:14:32+00:00
- **Updated**: 2020-10-01 08:14:32+00:00
- **Authors**: Zheng Gu, Chuanqi Dong, Jing Huo, Wenbin Li, Yang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Caricature generation aims to translate real photos into caricatures with artistic styles and shape exaggerations while maintaining the identity of the subject. Different from the generic image-to-image translation, drawing a caricature automatically is a more challenging task due to the existence of various spacial deformations. Previous caricature generation methods are obsessed with predicting definite image warping from a given photo while ignoring the intrinsic representation and distribution for exaggerations in caricatures. This limits their ability on diverse exaggeration generation. In this paper, we generalize the caricature generation problem from instance-level warping prediction to distribution-level deformation modeling. Based on this assumption, we present the first exploration for unpaired CARIcature generation with Multiple Exaggerations (CariMe). Technically, we propose a Multi-exaggeration Warper network to learn the distribution-level mapping from photo to facial exaggerations. This makes it possible to generate diverse and reasonable exaggerations from randomly sampled warp codes given one input photo. To better represent the facial exaggeration and produce fine-grained warping, a deformation-field-based warping method is also proposed, which helps us to capture more detailed exaggerations than other point-based warping methods. Experiments and two perceptual studies prove the superiority of our method comparing with other state-of-the-art methods, showing the improvement of our work on caricature generation.



### RefVOS: A Closer Look at Referring Expressions for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.00263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00263v1)
- **Published**: 2020-10-01 09:10:53+00:00
- **Updated**: 2020-10-01 09:10:53+00:00
- **Authors**: Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, Xavier Giro-i-Nieto
- **Comment**: None
- **Journal**: None
- **Summary**: The task of video object segmentation with referring expressions (language-guided VOS) is to, given a linguistic phrase and a video, generate binary masks for the object to which the phrase refers. Our work argues that existing benchmarks used for this task are mainly composed of trivial cases, in which referents can be identified with simple phrases. Our analysis relies on a new categorization of the phrases in the DAVIS-2017 and Actor-Action datasets into trivial and non-trivial REs, with the non-trivial REs annotated with seven RE semantic categories. We leverage this data to analyze the results of RefVOS, a novel neural network that obtains competitive results for the task of language-guided image segmentation and state of the art results for language-guided VOS. Our study indicates that the major challenges for the task are related to understanding motion and static actions.



### Action Units Recognition by Pairwise Deep Architecture
- **Arxiv ID**: http://arxiv.org/abs/2010.00288v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00288v2)
- **Published**: 2020-10-01 10:34:41+00:00
- **Updated**: 2020-10-02 15:57:38+00:00
- **Authors**: Junya Saito, Ryosuke Kawamura, Akiyoshi Uchida, Sachihiro Youoku, Yuushi Toyoda, Takahisa Yamamoto, Xiaoyu Mi, Kentaro Murase
- **Comment**: We changed expression of text and data
- **Journal**: None
- **Summary**: In this paper, we propose a new automatic Action Units (AUs) recognition method used in a competition, Affective Behavior Analysis in-the-wild (ABAW). Our method tackles a problem of AUs label inconsistency among subjects by using pairwise deep architecture. While the baseline score is 0.31, our method achieved 0.67 in validation dataset of the competition.



### Cost-Sensitive Regularization for Diabetic Retinopathy Grading from Eye Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2010.00291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00291v1)
- **Published**: 2020-10-01 10:42:06+00:00
- **Updated**: 2020-10-01 10:42:06+00:00
- **Authors**: Adrian Galdran, José Dolz, Hadi Chakor, Hervé Lombaert, Ismail Ben Ayed
- **Comment**: This paper has been accepted for publication at MICCAI 2020
- **Journal**: None
- **Summary**: Assessing the degree of disease severity in biomedical images is a task similar to standard classification but constrained by an underlying structure in the label space. Such a structure reflects the monotonic relationship between different disease grades. In this paper, we propose a straightforward approach to enforce this constraint for the task of predicting Diabetic Retinopathy (DR) severity from eye fundus images based on the well-known notion of Cost-Sensitive classification. We expand standard classification losses with an extra term that acts as a regularizer, imposing greater penalties on predicted grades when they are farther away from the true grade associated to a particular image. Furthermore, we show how to adapt our method to the modelling of label noise in each of the sub-problems associated to DR grading, an approach we refer to as Atomic Sub-Task modeling. This yields models that can implicitly take into account the inherent noise present in DR grade annotations. Our experimental analysis on several public datasets reveals that, when a standard Convolutional Neural Network is trained using this simple strategy, improvements of 3-5\% of quadratic-weighted kappa scores can be achieved at a negligible computational cost. Code to reproduce our results is released at https://github.com/agaldran/cost_sensitive_loss_classification.



### Open-Set Hypothesis Transfer with Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2010.00292v1
- **DOI**: 10.1109/TIP.2021.3093393
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00292v1)
- **Published**: 2020-10-01 10:44:31+00:00
- **Updated**: 2020-10-01 10:44:31+00:00
- **Authors**: Zeyu Feng, Chang Xu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised open-set domain adaptation (UODA) is a realistic problem where unlabeled target data contain unknown classes. Prior methods rely on the coexistence of both source and target domain data to perform domain alignment, which greatly limits their applications when source domain data are restricted due to privacy concerns. This paper addresses the challenging hypothesis transfer setting for UODA, where data from source domain are no longer available during adaptation on target domain. We introduce a method that focuses on the semantic consistency under transformation of target data, which is rarely appreciated by previous domain adaptation methods. Specifically, our model first discovers confident predictions and performs classification with pseudo-labels. Then we enforce the model to output consistent and definite predictions on semantically similar inputs. As a result, unlabeled data can be classified into discriminative classes coincided with either source classes or unknown classes. Experimental results show that our model outperforms state-of-the-art methods on UODA benchmarks.



### Improving spatial domain based image formation through compressed sensing
- **Arxiv ID**: http://arxiv.org/abs/2010.00295v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00295v1)
- **Published**: 2020-10-01 10:56:20+00:00
- **Updated**: 2020-10-01 10:56:20+00:00
- **Authors**: Gene Stoltz, André Leon Nel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we improve image reconstruction in a single-pixel scanning system by selecting an detector optimal field of view. Image reconstruction is based on compressed sensing and image quality is compared to interpolated staring arrays. The image quality comparisons use a "dead leaves" data set, Bayesian estimation and the Peak-Signal-to-Noise Ratio (PSNR) measure.   Compressed sensing is explored as an interpolation algorithm and shows with high probability an improved performance compared to Lanczos interpolation. Furthermore, multi-level sampling in a single-pixel scanning system is simulated by dynamically altering the detector field of view. It was shown that multi-level sampling improves the distribution of the Peak-Signal-to-Noise Ratio.   We further explore the expected sampling level distributions and PSNR distributions for multi-level sampling. The PSNR distribution indicates that there is a small set of levels which will improve image quality over interpolated staring arrays. We further conclude that multi-level sampling will outperform single-level uniform random sampling on average.



### Can You Trust Your Pose? Confidence Estimation in Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2010.00347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00347v1)
- **Published**: 2020-10-01 12:25:48+00:00
- **Updated**: 2020-10-01 12:25:48+00:00
- **Authors**: Luca Ferranti, Xiaotian Li, Jani Boutellier, Juho Kannala
- **Comment**: To appear in ICPR 2020
- **Journal**: None
- **Summary**: Camera pose estimation in large-scale environments is still an open question and, despite recent promising results, it may still fail in some situations. The research so far has focused on improving subcomponents of estimation pipelines, to achieve more accurate poses. However, there is no guarantee for the result to be correct, even though the correctness of pose estimation is critically important in several visual localization applications,such as in autonomous navigation. In this paper we bring to attention a novel research question, pose confidence estimation,where we aim at quantifying how reliable the visually estimated pose is. We develop a novel confidence measure to fulfil this task and show that it can be flexibly applied to different datasets,indoor or outdoor, and for various visual localization pipelines.We also show that the proposed techniques can be used to accomplish a secondary goal: improving the accuracy of existing pose estimation pipelines. Finally, the proposed approach is computationally light-weight and adds only a negligible increase to the computational effort of pose estimation.



### Meta-Consolidation for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.00352v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.00352v1)
- **Published**: 2020-10-01 12:34:35+00:00
- **Updated**: 2020-10-01 12:34:35+00:00
- **Authors**: K J Joseph, Vineeth N Balasubramanian
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: The ability to continuously learn and adapt itself to new tasks, without losing grasp of already acquired knowledge is a hallmark of biological learning systems, which current deep learning systems fall short of. In this work, we present a novel methodology for continual learning called MERLIN: Meta-Consolidation for Continual Learning.   We assume that weights of a neural network $\boldsymbol \psi$, for solving task $\boldsymbol t$, come from a meta-distribution $p(\boldsymbol{\psi|t})$. This meta-distribution is learned and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once.   Our experiments with continual learning benchmarks of MNIST, CIFAR-10, CIFAR-100 and Mini-ImageNet datasets show consistent improvement over five baselines, including a recent state-of-the-art, corroborating the promise of MERLIN.



### Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2010.00361v2
- **DOI**: 10.1145/3394171.3413668
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00361v2)
- **Published**: 2020-10-01 12:46:38+00:00
- **Updated**: 2022-03-24 12:55:15+00:00
- **Authors**: Zipeng Xu, Fangxiang Feng, Xiaojie Wang, Yushu Yang, Huixing Jiang, Zhongyuan Wang
- **Comment**: Accepted at ACM International Conference on Multimedia (ACM MM 2020)
- **Journal**: None
- **Summary**: A goal-oriented visual dialogue involves multi-turn interactions between two agents, Questioner and Oracle. During which, the answer given by Oracle is of great significance, as it provides golden response to what Questioner concerns. Based on the answer, Questioner updates its belief on target visual content and further raises another question. Notably, different answers drive into different visual beliefs and future questions. However, existing methods always indiscriminately encode answers after much longer questions, resulting in a weak utilization of answers. In this paper, we propose an Answer-Driven Visual State Estimator (ADVSE) to impose the effects of different answers on visual states. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture the answer-driven effect on visual attention by sharpening question-related attention and adjusting it by answer-based logical operation at each turn. Then based on the focusing attention, we get the visual state estimation by Conditional Visual Information Fusion (CVIF), where overall information and difference information are fused conditioning on the question-answer state. We evaluate the proposed ADVSE to both question generator and guesser tasks on the large-scale GuessWhat?! dataset and achieve the state-of-the-art performances on both tasks. The qualitative results indicate that the ADVSE boosts the agent to generate highly efficient questions and obtains reliable visual attentions during the reasonable question generation and guess processes.



### DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.00400v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.00400v3)
- **Published**: 2020-10-01 13:37:58+00:00
- **Updated**: 2020-12-14 14:34:23+00:00
- **Authors**: Javier Hernandez-Ortega, Ruben Tolosana, Julian Fierrez, Aythami Morales
- **Comment**: None
- **Journal**: Proc. 35th AAAI Conference on Artificial Intelligence Workshops,
  2021
- **Summary**: This work introduces a novel DeepFake detection framework based on physiological measurement. In particular, we consider information related to the heart rate using remote photoplethysmography (rPPG). rPPG methods analyze video sequences looking for subtle color changes in the human skin, revealing the presence of human blood under the tissues. In this work we investigate to what extent rPPG is useful for the detection of DeepFake videos.   The proposed fake detector named DeepFakesON-Phys uses a Convolutional Attention Network (CAN), which extracts spatial and temporal information from video frames, analyzing and combining both sources to better detect fake videos. This detection approach has been experimentally evaluated using the latest public databases in the field: Celeb-DF and DFDC. The results achieved, above 98% AUC (Area Under the Curve) on both databases, outperform the state of the art and prove the success of fake detectors based on physiological measurement to detect the latest DeepFake videos.



### X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2010.00450v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2010.00450v1)
- **Published**: 2020-10-01 14:46:00+00:00
- **Updated**: 2020-10-01 14:46:00+00:00
- **Authors**: Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel
- **Comment**: 15 pages, 19 figures, accepted at SIGGRAPH Asia 2020, project
  webpage: https://xfields.mpi-inf.mpg.de/
- **Journal**: None
- **Summary**: We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the "basic tricks" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.



### From Handcrafted to Deep Features for Pedestrian Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2010.00456v2
- **DOI**: 10.1109/TPAMI.2021.3076733
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00456v2)
- **Published**: 2020-10-01 14:51:10+00:00
- **Updated**: 2021-05-12 03:59:38+00:00
- **Authors**: Jiale Cao, Yanwei Pang, Jin Xie, Fahad Shahbaz Khan, Ling Shao
- **Comment**: IEEE TPAMI, Projects: https://github.com/JialeCao001/PedSurvey
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Pedestrian detection is an important but challenging problem in computer vision, especially in human-centric tasks. Over the past decade, significant improvement has been witnessed with the help of handcrafted features and deep features. Here we present a comprehensive survey on recent advances in pedestrian detection. First, we provide a detailed review of single-spectral pedestrian detection that includes handcrafted features based methods and deep features based approaches. For handcrafted features based methods, we present an extensive review of approaches and find that handcrafted features with large freedom degrees in shape and space have better performance. In the case of deep features based approaches, we split them into pure CNN based methods and those employing both handcrafted and CNN based features. We give the statistical analysis and tendency of these methods, where feature enhanced, part-aware, and post-processing methods have attracted main attention. In addition to single-spectral pedestrian detection, we also review multi-spectral pedestrian detection, which provides more robust features for illumination variance. Furthermore, we introduce some related datasets and evaluation metrics, and compare some representative methods. We conclude this survey by emphasizing open problems that need to be addressed and highlighting various future directions. Researchers can track an up-to-date list at https://github.com/JialeCao001/PedSurvey.



### Bag of Tricks for Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2010.00467v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00467v3)
- **Published**: 2020-10-01 15:03:51+00:00
- **Updated**: 2021-03-31 09:34:21+00:00
- **Authors**: Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu
- **Comment**: ICLR 2021
- **Journal**: None
- **Summary**: Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7%, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.



### Physical Exercise Recommendation and Success Prediction Using Interconnected Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.00482v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IR, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00482v2)
- **Published**: 2020-10-01 15:22:59+00:00
- **Updated**: 2021-01-27 20:04:20+00:00
- **Authors**: Arash Mahyari, Peter Pirolli
- **Comment**: None
- **Journal**: None
- **Summary**: Unhealthy behaviors, e.g., physical inactivity and unhealthful food choice, are the primary healthcare cost drivers in developed countries. Pervasive computational, sensing, and communication technology provided by smartphones and smartwatches have made it possible to support individuals in their everyday lives to develop healthier lifestyles. In this paper, we propose an exercise recommendation system that also predicts individual success rates. The system, consisting of two inter-connected recurrent neural networks (RNNs), uses the history of workouts to recommend the next workout activity for each individual. The system then predicts the probability of successful completion of the predicted activity by the individual. The prediction accuracy of this interconnected-RNN model is assessed on previously published data from a four-week mobile health experiment and is shown to improve upon previous predictions from a computational cognitive model.



### Mini-DDSM: Mammography-based Automatic Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.00494v3
- **DOI**: 10.1145/3441369.3441370
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00494v3)
- **Published**: 2020-10-01 15:35:11+00:00
- **Updated**: 2020-11-24 10:13:41+00:00
- **Authors**: Charitha Dissanayake Lekamlage, Fabia Afzal, Erik Westerberg, Abbas Cheddad
- **Comment**: C.D. Lekamlage, F. Afzal, E. Westerberg and A. Cheddad, "Mini-DDSM:
  Mammography-based Automatic Age Estimation," in the 3rd International
  Conference on Digital Medicine and Image Processing (DMIP 2020), ACM, Kyoto,
  Japan, November 06-09, 2020
- **Journal**: None
- **Summary**: Age estimation has attracted attention for its various medical applications. There are many studies on human age estimation from biomedical images. However, there is no research done on mammograms for age estimation, as far as we know. The purpose of this study is to devise an AI-based model for estimating age from mammogram images. Due to lack of public mammography data sets that have the age attribute, we resort to using a web crawler to download thumbnail mammographic images and their age fields from the public data set; the Digital Database for Screening Mammography. The original images in this data set unfortunately can only be retrieved by a software which is broken. Subsequently, we extracted deep learning features from the collected data set, by which we built a model using Random Forests regressor to estimate the age automatically. The performance assessment was measured using the mean absolute error values. The average error value out of 10 tests on random selection of samples was around 8 years. In this paper, we show the merits of this approach to fill up missing age values. We ran logistic and linear regression models on another independent data set to further validate the advantage of our proposed work. This paper also introduces the free-access Mini-DDSM data set.



### Ray-based classification framework for high-dimensional data
- **Arxiv ID**: http://arxiv.org/abs/2010.00500v2
- **DOI**: None
- **Categories**: **cs.LG**, cond-mat.mes-hall, cs.CV, quant-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00500v2)
- **Published**: 2020-10-01 15:46:29+00:00
- **Updated**: 2022-02-26 15:52:07+00:00
- **Authors**: Justyna P. Zwolak, Sandesh S. Kalantre, Thomas McJunkin, Brian J. Weber, Jacob M. Taylor
- **Comment**: None
- **Journal**: Proceedings of the Machine Learning and the Physical Sciences
  Workshop at NeurIPS 2020, Vancouver, Canada
- **Summary**: While classification of arbitrary structures in high dimensions may require complete quantitative information, for simple geometrical structures, low-dimensional qualitative information about the boundaries defining the structures can suffice. Rather than using dense, multi-dimensional data, we propose a deep neural network (DNN) classification framework that utilizes a minimal collection of one-dimensional representations, called \emph{rays}, to construct the "fingerprint" of the structure(s) based on substantially reduced information. We empirically study this framework using a synthetic dataset of double and triple quantum dot devices and apply it to the classification problem of identifying the device state. We show that the performance of the ray-based classifier is already on par with traditional 2D images for low dimensional systems, while significantly cutting down the data acquisition cost.



### An Ultra Lightweight CNN for Low Resource Circuit Component Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.00505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00505v1)
- **Published**: 2020-10-01 15:54:08+00:00
- **Updated**: 2020-10-01 15:54:08+00:00
- **Authors**: Yingnan Ju, Yue Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an ultra lightweight system that can effectively recognize different circuit components in an image with very limited training data. Along with the system, we also release the data set we created for the task. A two-stage approach is employed by our system. Selective search was applied to find the location of each circuit component. Based on its result, we crop the original image into smaller pieces. The pieces are then fed to the Convolutional Neural Network (CNN) for classification to identify each circuit component. It is of engineering significance and works well in circuit component recognition in a low resource setting. The accuracy of our system reaches 93.4\%, outperforming the support vector machine (SVM) baseline (75.00%) and the existing state-of-the-art RetinaNet solutions (92.80%).



### Fast Few-Shot Classification by Few-Iteration Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.00511v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00511v3)
- **Published**: 2020-10-01 15:59:31+00:00
- **Updated**: 2022-03-20 19:22:34+00:00
- **Authors**: Ardhendu Shekhar Tripathi, Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: Accepted at ICRA 2021
- **Journal**: None
- **Summary**: Autonomous agents interacting with the real world need to learn new concepts efficiently and reliably. This requires learning in a low-data regime, which is a highly challenging problem. We address this task by introducing a fast optimization-based meta-learning method for few-shot classification. It consists of an embedding network, providing a general representation of the image, and a base learner module. The latter learns a linear classifier during the inference through an unrolled optimization procedure. We design an inner learning objective composed of (i) a robust classification loss on the support set and (ii) an entropy loss, allowing transductive learning from unlabeled query samples. By employing an efficient initialization module and a Steepest Descent based optimization algorithm, our base learner predicts a powerful classifier within only a few iterations. Further, our strategy enables important aspects of the base learner objective to be learned during meta-training. To the best of our knowledge, this work is the first to integrate both induction and transduction into the base learner in an optimization-based meta-learning framework. We perform a comprehensive experimental analysis, demonstrating the speed and effectiveness of our approach on four few-shot classification datasets. The Code is available at \href{https://github.com/4rdhendu/FIML}{\textcolor{blue}{https://github.com/4rdhendu/FIML}}.



### Referring Image Segmentation via Cross-Modal Progressive Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2010.00514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.00514v1)
- **Published**: 2020-10-01 16:02:30+00:00
- **Updated**: 2020-10-01 16:02:30+00:00
- **Authors**: Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li
- **Comment**: Accepted by CVPR 2020. Code is available at
  https://github.com/spyflying/CMPC-Refseg
- **Journal**: None
- **Summary**: Referring image segmentation aims at segmenting the foreground masks of the entities that can well match the description given in the natural language expression. Previous approaches tackle this problem using implicit feature interaction and fusion between visual and linguistic modalities, but usually fail to explore informative words of the expression to well align features from the two modalities for accurately identifying the referred entity. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a Text-Guided Feature Exchange (TGFE) module to effectively address the challenging task. Concretely, the CMPC module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the correct entity as well as suppress other irrelevant ones by multimodal graph reasoning. In addition to the CMPC module, we further leverage a simple yet effective TGFE module to integrate the reasoned multimodal features from different levels with the guidance of textual information. In this way, features from multi-levels could communicate with each other and be refined based on the textual context. We conduct extensive experiments on four popular referring segmentation benchmarks and achieve new state-of-the-art performances.



### Linguistic Structure Guided Context Modeling for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.00515v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.00515v3)
- **Published**: 2020-10-01 16:03:51+00:00
- **Updated**: 2020-10-05 08:49:43+00:00
- **Authors**: Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, Jizhong Han
- **Comment**: Accepted by ECCV 2020. Code is available at
  https://github.com/spyflying/LSCM-Refseg
- **Journal**: None
- **Summary**: Referring image segmentation aims to predict the foreground mask of the object referred by a natural language sentence. Multimodal context of the sentence is crucial to distinguish the referent from the background. Existing methods either insufficiently or redundantly model the multimodal context. To tackle this problem, we propose a "gather-propagate-distribute" scheme to model multimodal context by cross-modal interaction and implement this scheme as a novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which guides all the words to include valid multimodal context of the sentence while excluding disturbing ones through three steps over the multimodal feature, i.e., gathering, constrained propagation and distributing. Extensive experiments on four benchmarks demonstrate that our method outperforms all the previous state-of-the-arts.



### Neural encoding with visual attention
- **Arxiv ID**: http://arxiv.org/abs/2010.00516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2010.00516v1)
- **Published**: 2020-10-01 16:04:21+00:00
- **Updated**: 2020-10-01 16:04:21+00:00
- **Authors**: Meenakshi Khosla, Gia H. Ngo, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Visual perception is critically influenced by the focus of attention. Due to limited resources, it is well known that neural representations are biased in favor of attended locations. Using concurrent eye-tracking and functional Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human subjects watching movies, we first demonstrate that leveraging gaze information, in the form of attentional masking, can significantly improve brain response prediction accuracy in a neural encoding model. Next, we propose a novel approach to neural encoding by including a trainable soft-attention module. Using our new approach, we demonstrate that it is possible to learn visual attention policies by end-to-end learning merely on fMRI response data, and without relying on any eye-tracking. Interestingly, we find that attention locations estimated by the model on independent data agree well with the corresponding eye fixation patterns, despite no explicit supervision to do so. Together, these findings suggest that attention modules can be instrumental in neural encoding models of visual stimuli.



### Why Adversarial Interaction Creates Non-Homogeneous Patterns: A Pseudo-Reaction-Diffusion Model for Turing Instability
- **Arxiv ID**: http://arxiv.org/abs/2010.00521v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00521v2)
- **Published**: 2020-10-01 16:09:22+00:00
- **Updated**: 2020-12-08 10:29:39+00:00
- **Authors**: Litu Rout
- **Comment**: 35th AAAI Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: Long after Turing's seminal Reaction-Diffusion (RD) model, the elegance of his fundamental equations alleviated much of the skepticism surrounding pattern formation. Though Turing model is a simplification and an idealization, it is one of the best-known theoretical models to explain patterns as a reminiscent of those observed in nature. Over the years, concerted efforts have been made to align theoretical models to explain patterns in real systems. The apparent difficulty in identifying the specific dynamics of the RD system makes the problem particularly challenging. Interestingly, we observe Turing-like patterns in a system of neurons with adversarial interaction. In this study, we establish the involvement of Turing instability to create such patterns. By theoretical and empirical studies, we present a pseudo-reaction-diffusion model to explain the mechanism that may underlie these phenomena. While supervised learning attains homogeneous equilibrium, this paper suggests that the introduction of an adversary helps break this homogeneity to create non-homogeneous patterns at equilibrium. Further, we prove that randomly initialized gradient descent with over-parameterization can converge exponentially fast to an $\epsilon$-stationary point even under adversarial interaction. In addition, different from sole supervision, we show that the solutions obtained under adversarial interaction are not limited to a tiny subspace around initialization.



### Understanding the Role of Adversarial Regularization in Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.00522v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00522v1)
- **Published**: 2020-10-01 16:10:05+00:00
- **Updated**: 2020-10-01 16:10:05+00:00
- **Authors**: Litu Rout
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Despite numerous attempts sought to provide empirical evidence of adversarial regularization outperforming sole supervision, the theoretical understanding of such phenomena remains elusive. In this study, we aim to resolve whether adversarial regularization indeed performs better than sole supervision at a fundamental level. To bring this insight into fruition, we study vanishing gradient issue, asymptotic iteration complexity, gradient flow and provable convergence in the context of sole supervision and adversarial regularization. The key ingredient is a theoretical justification supported by empirical evidence of adversarial acceleration in gradient descent. In addition, motivated by a recently introduced unit-wise capacity based generalization bound, we analyze the generalization error in adversarial framework. Guided by our observation, we cast doubts on the ability of this measure to explain generalization. We therefore leave as open questions to explore new measures that can explain generalization behavior in adversarial learning. Furthermore, we observe an intriguing phenomenon in the neural embedded vector space while contrasting adversarial learning with sole supervision.



### A Multi-modal Machine Learning Approach and Toolkit to Automate Recognition of Early Stages of Dementia among British Sign Language Users
- **Arxiv ID**: http://arxiv.org/abs/2010.00536v1
- **DOI**: 10.1007/978-3-030-66096-3_20
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.00536v1)
- **Published**: 2020-10-01 16:35:48+00:00
- **Updated**: 2020-10-01 16:35:48+00:00
- **Authors**: Xing Liang, Anastassia Angelopoulou, Epaminondas Kapetanios, Bencie Woll, Reda Al-batat, Tyron Woolfe
- **Comment**: None
- **Journal**: ECCV 2020 Workshops. Lecture Notes in Computer Science, Vol 12536.
  Springer, Cham
- **Summary**: The ageing population trend is correlated with an increased prevalence of acquired cognitive impairments such as dementia. Although there is no cure for dementia, a timely diagnosis helps in obtaining necessary support and appropriate medication. Researchers are working urgently to develop effective technological tools that can help doctors undertake early identification of cognitive disorder. In particular, screening for dementia in ageing Deaf signers of British Sign Language (BSL) poses additional challenges as the diagnostic process is bound up with conditions such as quality and availability of interpreters, as well as appropriate questionnaires and cognitive tests. On the other hand, deep learning based approaches for image and video analysis and understanding are promising, particularly the adoption of Convolutional Neural Network (CNN), which require large amounts of training data. In this paper, however, we demonstrate novelty in the following way: a) a multi-modal machine learning based automatic recognition toolkit for early stages of dementia among BSL users in that features from several parts of the body contributing to the sign envelope, e.g., hand-arm movements and facial expressions, are combined, b) universality in that it is possible to apply our technique to users of any sign language, since it is language independent, c) given the trade-off between complexity and accuracy of machine learning (ML) prediction models as well as the limited amount of training and testing data being available, we show that our approach is not over-fitted and has the potential to scale up.



### Dynamic Facial Asset and Rig Generation from a Single Scan
- **Arxiv ID**: http://arxiv.org/abs/2010.00560v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00560v2)
- **Published**: 2020-10-01 17:25:25+00:00
- **Updated**: 2020-10-05 20:12:53+00:00
- **Authors**: Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin, Hao Li
- **Comment**: 18 pages, 25 figures, ACM SIGGRAPH Asia 2020
- **Journal**: None
- **Summary**: The creation of high-fidelity computer-generated (CG) characters used in film and gaming requires intensive manual labor and a comprehensive set of facial assets to be captured with complex hardware, resulting in high cost and long production cycles. In order to simplify and accelerate this digitization process, we propose a framework for the automatic generation of high-quality dynamic facial assets, including rigs which can be readily deployed for artists to polish. Our framework takes a single scan as input to generate a set of personalized blendshapes, dynamic and physically-based textures, as well as secondary facial components (e.g., teeth and eyeballs). Built upon a facial database consisting of pore-level details, with over $4,000$ scans of varying expressions and identities, we adopt a self-supervised neural network to learn personalized blendshapes from a set of template expressions. We also model the joint distribution between identities and expressions, enabling the inference of the full set of personalized blendshapes with dynamic appearances from a single neutral input scan. Our generated personalized face rig assets are seamlessly compatible with cutting-edge industry pipelines for facial animation and rendering. We demonstrate that our framework is robust and effective by inferring on a wide range of novel subjects, and illustrate compelling rendering results while animating faces with generated customized physically-based dynamic textures.



### ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention
- **Arxiv ID**: http://arxiv.org/abs/2010.00562v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00562v1)
- **Published**: 2020-10-01 17:28:47+00:00
- **Updated**: 2020-10-01 17:28:47+00:00
- **Authors**: Jose Manuel Gomez-Perez, Raul Ortega
- **Comment**: Accepted for publication as a long paper in EMNLP2020
- **Journal**: None
- **Summary**: Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.



### DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based Localization
- **Arxiv ID**: http://arxiv.org/abs/2010.00573v2
- **DOI**: 10.1109/TIP.2020.3043875
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00573v2)
- **Published**: 2020-10-01 17:44:25+00:00
- **Updated**: 2020-11-25 15:47:17+00:00
- **Authors**: Hanjiang Hu, Zhijian Qiao, Ming Cheng, Zhe Liu, Hesheng Wang
- **Comment**: Submitted to TIP
- **Journal**: None
- **Summary**: Long-Term visual localization under changing environments is a challenging problem in autonomous driving and mobile robotics due to season, illumination variance, etc. Image retrieval for localization is an efficient and effective solution to the problem. In this paper, we propose a novel multi-task architecture to fuse the geometric and semantic information into the multi-scale latent embedding representation for visual place recognition. To use the high-quality ground truths without any human effort, the effective multi-scale feature discriminator is proposed for adversarial training to achieve the domain adaptation from synthetic virtual KITTI dataset to real-world KITTI dataset. The proposed approach is validated on the Extended CMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial comparison experiments, where our performance outperforms state-of-the-art baselines for retrieval-based localization and large-scale place recognition under the challenging environment.



### TrueImage: A Machine Learning Algorithm to Improve the Quality of Telehealth Photos
- **Arxiv ID**: http://arxiv.org/abs/2010.02086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.02086v1)
- **Published**: 2020-10-01 17:47:57+00:00
- **Updated**: 2020-10-01 17:47:57+00:00
- **Authors**: Kailas Vodrahalli, Roxana Daneshjou, Roberto A Novoa, Albert Chiou, Justin M Ko, James Zou
- **Comment**: 12 pages, 5 figures, Preprint of an article published in Pacific
  Symposium on Biocomputing \c{opyright} 2020 World Scientific Publishing Co.,
  Singapore, http://psb.stanford.edu/
- **Journal**: None
- **Summary**: Telehealth is an increasingly critical component of the health care ecosystem, especially due to the COVID-19 pandemic. Rapid adoption of telehealth has exposed limitations in the existing infrastructure. In this paper, we study and highlight photo quality as a major challenge in the telehealth workflow. We focus on teledermatology, where photo quality is particularly important; the framework proposed here can be generalized to other health domains. For telemedicine, dermatologists request that patients submit images of their lesions for assessment. However, these images are often of insufficient quality to make a clinical diagnosis since patients do not have experience taking clinical photos. A clinician has to manually triage poor quality images and request new images to be submitted, leading to wasted time for both the clinician and the patient. We propose an automated image assessment machine learning pipeline, TrueImage, to detect poor quality dermatology photos and to guide patients in taking better photos. Our experiments indicate that TrueImage can reject 50% of the sub-par quality images, while retaining 80% of good quality images patients send in, despite heterogeneity and limitations in the training data. These promising results suggest that our solution is feasible and can improve the quality of teledermatology care.



### Utilizing Transfer Learning and a Customized Loss Function for Optic Disc Segmentation from Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/2010.00583v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00583v1)
- **Published**: 2020-10-01 17:55:15+00:00
- **Updated**: 2020-10-01 17:55:15+00:00
- **Authors**: Abdullah Sarhan, Ali Al-KhazÁly, Adam Gorner, Andrew Swift, Jon Rokne, Reda Alhajj, Andrew Crichton
- **Comment**: Accepted by ACCV 2020
- **Journal**: None
- **Summary**: Accurate segmentation of the optic disc from a retinal image is vital to extracting retinal features that may be highly correlated with retinal conditions such as glaucoma. In this paper, we propose a deep-learning based approach capable of segmenting the optic disc given a high-precision retinal fundus image. Our approach utilizes a UNET-based model with a VGG16 encoder trained on the ImageNet dataset. This study can be distinguished from other studies in the customization made for the VGG16 model, the diversity of the datasets adopted, the duration of disc segmentation, the loss function utilized, and the number of parameters required to train our model. Our approach was tested on seven publicly available datasets augmented by a dataset from a private clinic that was annotated by two Doctors of Optometry through a web portal built for this purpose. We achieved an accuracy of 99.78\% and a Dice coefficient of 94.73\% for a disc segmentation from a retinal image in 0.03 seconds. The results obtained from comprehensive experiments demonstrate the robustness of our approach to disc segmentation of retinal images obtained from different sources.



### StreamSoNG: A Soft Streaming Classification Approach
- **Arxiv ID**: http://arxiv.org/abs/2010.00635v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00635v2)
- **Published**: 2020-10-01 18:22:04+00:00
- **Updated**: 2021-07-13 16:52:18+00:00
- **Authors**: Wenlong Wu, James M. Keller, Jeffrey Dale, James C. Bezdek
- **Comment**: None
- **Journal**: None
- **Summary**: Examining most streaming clustering algorithms leads to the understanding that they are actually incremental classification models. They model existing and newly discovered structures via summary information that we call footprints. Incoming data is normally assigned a crisp label (into one of the structures) and that structure's footprint is incrementally updated. There is no reason that these assignments need to be crisp. In this paper, we propose a new streaming classification algorithm that uses Neural Gas prototypes as footprints and produces a possibilistic label vector (of typicalities) for each incoming vector. These typicalities are generated by a modified possibilistic k-nearest neighbor algorithm. The approach is tested on synthetic and real image datasets. We compare our approach to three other streaming classifiers based on the Adaptive Random Forest, Very Fast Decision Rules, and the DenStream algorithm with excellent results.



### Tabular GANs for uneven distribution
- **Arxiv ID**: http://arxiv.org/abs/2010.00638v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00638v1)
- **Published**: 2020-10-01 18:39:32+00:00
- **Updated**: 2020-10-01 18:39:32+00:00
- **Authors**: Insaf Ashrapov
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: GANs are well known for success in the realistic image generation. However, they can be applied in tabular data generation as well. We will review and examine some recent papers about tabular GANs in action. We will generate data to make train distribution bring closer to the test. Then compare model performance trained on the initial train dataset, with trained on the train with GAN generated data, also we train the model by sampling train by adversarial training. We show that using GAN might be an option in case of uneven data distribution between train and test data.



### Multiscale Detection of Cancerous Tissue in High Resolution Slide Scans
- **Arxiv ID**: http://arxiv.org/abs/2010.00641v1
- **DOI**: 10.1007/978-3-030-64559-5_11
- **Categories**: **cs.CV**, I.5.0; I.5.4; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.00641v1)
- **Published**: 2020-10-01 18:56:46+00:00
- **Updated**: 2020-10-01 18:56:46+00:00
- **Authors**: Qingchao Zhang, Coy D. Heldermon, Corey Toler-Franklin
- **Comment**: 14 pages, 7 figures, 2 tables
- **Journal**: Advances in Visual Computing. ISVC 2020. Lecture Notes in Computer
  Science
- **Summary**: We present an algorithm for multi-scale tumor (chimeric cell) detection in high resolution slide scans. The broad range of tumor sizes in our dataset pose a challenge for current Convolutional Neural Networks (CNN) which often fail when image features are very small (8 pixels). Our approach modifies the effective receptive field at different layers in a CNN so that objects with a broad range of varying scales can be detected in a single forward pass. We define rules for computing adaptive prior anchor boxes which we show are solvable under the equal proportion interval principle. Two mechanisms in our CNN architecture alleviate the effects of non-discriminative features prevalent in our data - a foveal detection algorithm that incorporates a cascade residual-inception module and a deconvolution module with additional context information. When integrated into a Single Shot MultiBox Detector (SSD), these additions permit more accurate detection of small-scale objects. The results permit efficient real-time analysis of medical images in pathology and related biomedical research fields.



### VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models
- **Arxiv ID**: http://arxiv.org/abs/2010.00654v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00654v3)
- **Published**: 2020-10-01 19:28:28+00:00
- **Updated**: 2021-11-04 23:49:01+00:00
- **Authors**: Zhisheng Xiao, Karsten Kreis, Jan Kautz, Arash Vahdat
- **Comment**: ICLR 2021 (spotlight)
- **Journal**: None
- **Summary**: Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. The source code is available at https://github.com/NVlabs/VAEBM



### Explaining Convolutional Neural Networks through Attribution-Based Input Sampling and Block-Wise Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2010.00672v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00672v2)
- **Published**: 2020-10-01 20:27:30+00:00
- **Updated**: 2020-12-24 21:33:05+00:00
- **Authors**: Sam Sattarzadeh, Mahesh Sudhakar, Anthony Lem, Shervin Mehryar, K. N. Plataniotis, Jongseong Jang, Hyunwoo Kim, Yeonjeong Jeong, Sangmin Lee, Kyunghoon Bae
- **Comment**: 9 pages, 9 figures, Accepted at the Thirty-Fifth AAAI Conference on
  Artificial Intelligence (AAAI-21)
- **Journal**: None
- **Summary**: As an emerging field in Machine Learning, Explainable AI (XAI) has been offering remarkable performance in interpreting the decisions made by Convolutional Neural Networks (CNNs). To achieve visual explanations for CNNs, methods based on class activation mapping and randomized input sampling have gained great popularity. However, the attribution methods based on these techniques provide lower resolution and blurry explanation maps that limit their explanation power. To circumvent this issue, visualization based on various layers is sought. In this work, we collect visualization maps from multiple layers of the model based on an attribution-based input sampling technique and aggregate them to reach a fine-grained and complete explanation. We also propose a layer selection strategy that applies to the whole family of CNN-based models, based on which our extraction framework is applied to visualize the last layers of each convolutional block of the model. Moreover, we perform an empirical analysis of the efficacy of derived lower-level information to enhance the represented attributions. Comprehensive experiments conducted on shallow and deep models trained on natural and industrial datasets, using both ground-truth and model-truth based evaluation metrics validate our proposed algorithm by meeting or outperforming the state-of-the-art methods in terms of explanation ability and visual quality, demonstrating that our method shows stability regardless of the size of objects or instances to be explained.



### Implicit Rank-Minimizing Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2010.00679v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00679v2)
- **Published**: 2020-10-01 20:48:52+00:00
- **Updated**: 2020-10-14 15:36:27+00:00
- **Authors**: Li Jing, Jure Zbontar, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.



### Automatic Deep Learning System for COVID-19 Infection Quantification in chest CT
- **Arxiv ID**: http://arxiv.org/abs/2010.01982v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01982v1)
- **Published**: 2020-10-01 21:05:59+00:00
- **Updated**: 2020-10-01 21:05:59+00:00
- **Authors**: Omar Ibrahim Alirr
- **Comment**: None
- **Journal**: None
- **Summary**: Coronavirus Disease spread globally and infected millions of people quickly, causing high pressure on the health-system facilities. PCR screening is the adopted diagnostic testing method for COVID-19 detection. However, PCR is criticized due its low sensitivity ratios, also, it is time-consuming and manual complicated process. CT imaging proved its ability to detect the disease even for asymptotic patients, which make it a trustworthy alternative for PCR. In addition, the appearance of COVID-19 infections in CT slices, offers high potential to support in disease evolution monitoring using automated infection segmentation methods. However, COVID-19 infection areas include high variations in term of size, shape, contrast and intensity homogeneity, which impose a big challenge on segmentation process. To address these challenges, this paper proposed an automatic deep learning system for COVID-19 infection areas segmentation. The system include different steps to enhance and improve infection areas appearance in the CT slices so they can be learned efficiently using the deep network. The system start prepare the region of interest by segmenting the lung organ, which then undergo edge enhancing diffusion filtering (EED) to improve the infection areas contrast and intensity homogeneity. The proposed FCN is implemented using U-net architecture with modified residual block with concatenation skip connection. The block improves the learning of gradient values by forwarding the infection area features through the network. To demonstrate the generalization and effectiveness of the proposed system, it is trained and tested using many 2D CT slices extracted from diverse datasets from different sources. The proposed system is evaluated using different measures and achieved dice overlapping score of 0.961 and 0.780 for lung and infection areas segmentation, respectively.



### Active Learning for Bayesian 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2010.00694v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.00694v2)
- **Published**: 2020-10-01 21:36:26+00:00
- **Updated**: 2021-02-21 04:56:35+00:00
- **Authors**: Razvan Caramalau, Binod Bhattarai, Tae-Kyun Kim
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: We propose a Bayesian approximation to a deep learning architecture for 3D hand pose estimation. Through this framework, we explore and analyse the two types of uncertainties that are influenced either by data or by the learning capability. Furthermore, we draw comparisons against the standard estimator over three popular benchmarks. The first contribution lies in outperforming the baseline while in the second part we address the active learning application. We also show that with a newly proposed acquisition function, our Bayesian 3D hand pose estimator obtains lowest errors with the least amount of data. The underlying code is publicly available at https://github.com/razvancaramalau/al_bhpe.



### Learned Dual-View Reflection Removal
- **Arxiv ID**: http://arxiv.org/abs/2010.00702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00702v1)
- **Published**: 2020-10-01 21:59:58+00:00
- **Updated**: 2020-10-01 21:59:58+00:00
- **Authors**: Simon Niklaus, Xuaner Cecilia Zhang, Jonathan T. Barron, Neal Wadhwa, Rahul Garg, Feng Liu, Tianfan Xue
- **Comment**: http://sniklaus.com/dualref
- **Journal**: None
- **Summary**: Traditional reflection removal algorithms either use a single image as input, which suffers from intrinsic ambiguities, or use multiple images from a moving camera, which is inconvenient for users. We instead propose a learning-based dereflection algorithm that uses stereo images as input. This is an effective trade-off between the two extremes: the parallax between two views provides cues to remove reflections, and two views are easy to capture due to the adoption of stereo cameras in smartphones. Our model consists of a learning-based reflection-invariant flow model for dual-view registration, and a learned synthesis model for combining aligned image pairs. Because no dataset for dual-view reflection removal exists, we render a synthetic dataset of dual-views with and without reflections for use in training. Our evaluation on an additional real-world dataset of stereo pairs shows that our algorithm outperforms existing single-image and multi-image dereflection approaches.



### BCNN: A Binary CNN with All Matrix Ops Quantized to 1 Bit Precision
- **Arxiv ID**: http://arxiv.org/abs/2010.00704v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00704v4)
- **Published**: 2020-10-01 22:04:13+00:00
- **Updated**: 2021-03-05 14:46:10+00:00
- **Authors**: Arthur J. Redfern, Lijun Zhu, Molly K. Newquist
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a CNN where all CNN style 2D convolution operations that lower to matrix matrix multiplication are fully binary. The network is derived from a common building block structure that is consistent with a constructive proof outline showing that binary neural networks are universal function approximators. 71.24% top 1 accuracy on the 2012 ImageNet validation set was achieved with a 2 step training procedure and implementation strategies optimized for binary operands are provided.



### Binary Neural Networks for Memory-Efficient and Effective Visual Place Recognition in Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/2010.00716v2
- **DOI**: 10.1109/TRO.2022.3148908
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00716v2)
- **Published**: 2020-10-01 22:59:34+00:00
- **Updated**: 2022-01-23 10:48:16+00:00
- **Authors**: Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier, Shoaib Ehsan
- **Comment**: None
- **Journal**: IEEE Transactions on Robotics, 2022
- **Summary**: Visual place recognition (VPR) is a robot's ability to determine whether a place was visited before using visual data. While conventional hand-crafted methods for VPR fail under extreme environmental appearance changes, those based on convolutional neural networks (CNNs) achieve state-of-the-art performance but result in heavy runtime processes and model sizes that demand a large amount of memory. Hence, CNN-based approaches are unsuitable for resource-constrained platforms, such as small robots and drones. In this paper, we take a multi-step approach of decreasing the precision of model parameters, combining it with network depth reduction and fewer neurons in the classifier stage to propose a new class of highly compact models that drastically reduces the memory requirements and computational effort while maintaining state-of-the-art VPR performance. To the best of our knowledge, this is the first attempt to propose binary neural networks for solving the visual place recognition problem effectively under changing conditions and with significantly reduced resource requirements. Our best-performing binary neural network, dubbed FloppyNet, achieves comparable VPR performance when considered against its full-precision and deeper counterparts while consuming 99% less memory and increasing the inference speed seven times.



### Deep Reinforcement Learning with Mixed Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2010.00717v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.00717v2)
- **Published**: 2020-10-01 23:02:59+00:00
- **Updated**: 2020-10-06 04:35:44+00:00
- **Authors**: Yanyu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has shown that map raw pixels from a single front-facing camera directly to steering commands are surprisingly powerful. This paper presents a convolutional neural network (CNN) to playing the CarRacing-v0 using imitation learning in OpenAI Gym. The dataset is generated by playing the game manually in Gym and used a data augmentation method to expand the dataset to 4 times larger than before. Also, we read the true speed, four ABS sensors, steering wheel position, and gyroscope for each image and designed a mixed model by combining the sensor input and image input. After training, this model can automatically detect the boundaries of road features and drive the robot like a human. By comparing with AlexNet and VGG16 using the average reward in CarRacing-v0, our model wins the maximum overall system performance.



### Using Unlabeled Data for Increasing Low-Shot Classification Accuracy of Relevant and Open-Set Irrelevant Images
- **Arxiv ID**: http://arxiv.org/abs/2010.00721v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00721v2)
- **Published**: 2020-10-01 23:11:07+00:00
- **Updated**: 2022-06-03 18:59:19+00:00
- **Authors**: Spiridon Kasapis, Geng Zhang, Jonathon Smereka, Nickolas Vlahopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: In search, exploration, and reconnaissance tasks performed with autonomous ground vehicles, an image classification capability is needed for specifically identifying targeted objects (relevant classes) and at the same time recognize when a candidate image does not belong to anyone of the relevant classes (irrelevant images). In this paper, we present an open-set low-shot classifier that uses, during its training, a modest number (less than 40) of labeled images for each relevant class, and unlabeled irrelevant images that are randomly selected at each epoch of the training process. The new classifier is capable of identifying images from the relevant classes, determining when a candidate image is irrelevant, and it can further recognize categories of irrelevant images that were not included in the training (unseen). The proposed low-shot classifier can be attached as a top layer to any pre-trained feature extractor when constructing a Convolutional Neural Network.




# Arxiv Papers in cs.CV on 2020-10-22
### Class-Conditional Defense GAN Against End-to-End Speech Attacks
- **Arxiv ID**: http://arxiv.org/abs/2010.11352v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CR, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2010.11352v2)
- **Published**: 2020-10-22 00:02:02+00:00
- **Updated**: 2021-02-20 02:51:55+00:00
- **Authors**: Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich
- **Comment**: 5 pages
- **Journal**: 46th IEEE International Conference on Acoustics, Speech, & Signal
  Processing (ICASSP), 2021
- **Summary**: In this paper we propose a novel defense approach against end-to-end adversarial attacks developed to fool advanced speech-to-text systems such as DeepSpeech and Lingvo. Unlike conventional defense approaches, the proposed approach does not directly employ low-level transformations such as autoencoding a given input signal aiming at removing potential adversarial perturbation. Instead of that, we find an optimal input vector for a class conditional generative adversarial network through minimizing the relative chordal distance adjustment between a given test input and the generator network. Then, we reconstruct the 1D signal from the synthesized spectrogram and the original phase information derived from the given input signal. Hence, this reconstruction does not add any extra noise to the signal and according to our experimental results, our defense-GAN considerably outperforms conventional defense algorithms both in terms of word error rate and sentence level recognition accuracy.



### Bandwidth-Adaptive Feature Sharing for Cooperative LIDAR Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.11353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11353v1)
- **Published**: 2020-10-22 00:12:58+00:00
- **Updated**: 2020-10-22 00:12:58+00:00
- **Authors**: Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser P. Fallah
- **Comment**: 8 pages, 4 figures, 2 table, 2020 IEEE 3rd Connected and Automated
  Vehicles Symposium: IEEE CAVS 2020
- **Journal**: None
- **Summary**: Situational awareness as a necessity in the connected and autonomous vehicles (CAV) domain is the subject of a significant number of researches in recent years. The driver's safety is directly dependent on the robustness, reliability, and scalability of such systems. Cooperative mechanisms have provided a solution to improve situational awareness by utilizing high speed wireless vehicular networks. These mechanisms mitigate problems such as occlusion and sensor range limitation. However, the network capacity is a factor determining the maximum amount of information being shared among cooperative entities. The notion of feature sharing, proposed in our previous work, aims to address these challenges by maintaining a balance between computation and communication load. In this work, we propose a mechanism to add flexibility in adapting to communication channel capacity and a novel decentralized shared data alignment method to further improve cooperative object detection performance. The performance of the proposed framework is verified through experiments on Volony dataset. The results confirm that our proposed framework outperforms our previous cooperative object detection method (FS-COD) in terms of average precision.



### QISTA-Net: DNN Architecture to Solve $\ell_q$-norm Minimization Problem and Image Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/2010.11363v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11363v1)
- **Published**: 2020-10-22 01:00:45+00:00
- **Updated**: 2020-10-22 01:00:45+00:00
- **Authors**: Gang-Xuan Lin, Shih-Wei Hu, Chun-Shien Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we reformulate the non-convex $\ell_q$-norm minimization problem with $q\in(0,1)$ into a 2-step problem, which consists of one convex and one non-convex subproblems, and propose a novel iterative algorithm called QISTA ($\ell_q$-ISTA) to solve the $\left(\ell_q\right)$-problem. By taking advantage of deep learning in accelerating optimization algorithms, together with the speedup strategy that using the momentum from all previous layers in the network, we propose a learning-based method, called QISTA-Net-s, to solve the sparse signal reconstruction problem. Extensive experimental comparisons demonstrate that the QISTA-Net-s yield better reconstruction qualities than state-of-the-art $\ell_1$-norm optimization (plus learning) algorithms even if the original sparse signal is noisy. On the other hand, based on the network architecture associated with QISTA, with considering the use of convolution layers, we proposed the QISTA-Net-n for solving the image CS problem, and the performance of the reconstruction still outperforms most of the state-of-the-art natural images reconstruction methods. QISTA-Net-n is designed in unfolding QISTA and adding the convolutional operator as the dictionary. This makes QISTA-Net-s interpretable. We provide complete experimental results that QISTA-Net-s and QISTA-Net-n contribute the better reconstruction performance than the competing.



### Learning Graph-Based Priors for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.11369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11369v1)
- **Published**: 2020-10-22 01:20:46+00:00
- **Updated**: 2020-10-22 01:20:46+00:00
- **Authors**: Colin Samplawski, Jannik Wolff, Tassilo Klein, Moin Nabi
- **Comment**: Presented at AAAI 2020 Workshop on Deep Learning on Graphs:
  Methodologies and Applications (DLGMA'20)
- **Journal**: None
- **Summary**: The task of zero-shot learning (ZSL) requires correctly predicting the label of samples from classes which were unseen at training time. This is achieved by leveraging side information about class labels, such as label attributes or word embeddings. Recently, attention has shifted to the more realistic task of generalized ZSL (GZSL) where test sets consist of seen and unseen samples. Recent approaches to GZSL have shown the value of generative models, which are used to generate samples from unseen classes. In this work, we incorporate an additional source of side information in the form of a relation graph over labels. We leverage this graph in order to learn a set of prior distributions, which encourage an aligned variational autoencoder (VAE) model to learn embeddings which respect the graph structure. Using this approach we are able to achieve improved performance on the CUB and SUN benchmarks over a strong baseline.



### Deep Learning for Distinguishing Normal versus Abnormal Chest Radiographs and Generalization to Unseen Diseases
- **Arxiv ID**: http://arxiv.org/abs/2010.11375v2
- **DOI**: 10.1038/s41598-021-93967-2
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11375v2)
- **Published**: 2020-10-22 01:52:51+00:00
- **Updated**: 2021-10-29 22:20:37+00:00
- **Authors**: Zaid Nabulsi, Andrew Sellergren, Shahar Jamshy, Charles Lau, Edward Santos, Atilla P. Kiraly, Wenxing Ye, Jie Yang, Rory Pilgrim, Sahar Kazemzadeh, Jin Yu, Sreenivasa Raju Kalidindi, Mozziyar Etemadi, Florencia Garcia-Vicente, David Melnick, Greg S. Corrado, Lily Peng, Krish Eswaran, Daniel Tse, Neeral Beladia, Yun Liu, Po-Hsuan Cameron Chen, Shravya Shetty
- **Comment**: None
- **Journal**: Nature Scientific Reports (2021)
- **Summary**: Chest radiography (CXR) is the most widely-used thoracic clinical imaging modality and is crucial for guiding the management of cardiothoracic conditions. The detection of specific CXR findings has been the main focus of several artificial intelligence (AI) systems. However, the wide range of possible CXR abnormalities makes it impractical to build specific systems to detect every possible condition. In this work, we developed and evaluated an AI system to classify CXRs as normal or abnormal. For development, we used a de-identified dataset of 248,445 patients from a multi-city hospital network in India. To assess generalizability, we evaluated our system using 6 international datasets from India, China, and the United States. Of these datasets, 4 focused on diseases that the AI was not trained to detect: 2 datasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our results suggest that the AI system generalizes to new patient populations and abnormalities. In a simulated workflow where the AI system prioritized abnormal cases, the turnaround time for abnormal cases reduced by 7-28%. These results represent an important step towards evaluating whether AI can be safely used to flag cases in a general setting where previously unseen abnormalities exist.



### Learning Occupancy Function from Point Clouds for Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2010.11378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11378v1)
- **Published**: 2020-10-22 02:07:29+00:00
- **Updated**: 2020-10-22 02:07:29+00:00
- **Authors**: Meng Jia, Matthew Kyan
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit function based surface reconstruction has been studied for a long time to recover 3D shapes from point clouds sampled from surfaces. Recently, Signed Distance Functions (SDFs) and Occupany Functions are adopted in learning-based shape reconstruction methods as implicit 3D shape representation. This paper proposes a novel method for learning occupancy functions from sparse point clouds and achieves better performance on challenging surface reconstruction tasks. Unlike the previous methods, which predict point occupancy with fully-connected multi-layer networks, we adapt the point cloud deep learning architecture, Point Convolution Neural Network (PCNN), to build our learning model. Specifically, we create a sampling operator and insert it into PCNN to continuously sample the feature space at the points where occupancy states need to be predicted. This method natively obtains point cloud data's geometric nature, and it's invariant to point permutation. Our occupancy function learning can be easily fit into procedures of point cloud up-sampling and surface reconstruction. Our experiments show state-of-the-art performance for reconstructing With ShapeNet dataset and demonstrate this method's well-generalization by testing it with McGill 3D dataset \cite{siddiqi2008retrieving}. Moreover, we find the learned occupancy function is relatively more rotation invariant than previous shape learning methods.



### DPD-InfoGAN: Differentially Private Distributed InfoGAN
- **Arxiv ID**: http://arxiv.org/abs/2010.11398v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11398v3)
- **Published**: 2020-10-22 03:07:01+00:00
- **Updated**: 2021-03-22 19:05:23+00:00
- **Authors**: Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal, Shlomo Dubnov
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are deep learning architectures capable of generating synthetic datasets. Despite producing high-quality synthetic images, the default GAN has no control over the kinds of images it generates. The Information Maximizing GAN (InfoGAN) is a variant of the default GAN that introduces feature-control variables that are automatically learned by the framework, hence providing greater control over the different kinds of images produced. Due to the high model complexity of InfoGAN, the generative distribution tends to be concentrated around the training data points. This is a critical problem as the models may inadvertently expose the sensitive and private information present in the dataset. To address this problem, we propose a differentially private version of InfoGAN (DP-InfoGAN). We also extend our framework to a distributed setting (DPD-InfoGAN) to allow clients to learn different attributes present in other clients' datasets in a privacy-preserving manner. In our experiments, we show that both DP-InfoGAN and DPD-InfoGAN can synthesize high-quality images with flexible control over image attributes while preserving privacy.



### Rethinking pooling in graph neural networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11418v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11418v1)
- **Published**: 2020-10-22 03:48:56+00:00
- **Updated**: 2020-10-22 03:48:56+00:00
- **Authors**: Diego Mesquita, Amauri H. Souza, Samuel Kaski
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Graph pooling is a central component of a myriad of graph neural network (GNN) architectures. As an inheritance from traditional CNNs, most approaches formulate graph pooling as a cluster assignment problem, extending the idea of local patches in regular grids to graphs. Despite the wide adherence to this design choice, no work has rigorously evaluated its influence on the success of GNNs. In this paper, we build upon representative GNNs and introduce variants that challenge the need for locality-preserving representations, either using randomization or clustering on the complement graph. Strikingly, our experiments demonstrate that using these variants does not result in any decrease in performance. To understand this phenomenon, we study the interplay between convolutional layers and the subsequent pooling ones. We show that the convolutions play a leading role in the learned representations. In contrast to the common belief, local pooling is not responsible for the success of GNNs on relevant and widely-used benchmarks.



### Learning Loss for Test-Time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.11422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11422v1)
- **Published**: 2020-10-22 03:56:34+00:00
- **Updated**: 2020-10-22 03:56:34+00:00
- **Authors**: Ildoo Kim, Younghoon Kim, Sungwoong Kim
- **Comment**: Accepted at NeurIPS 2020
- **Journal**: None
- **Summary**: Data augmentation has been actively studied for robust neural networks. Most of the recent data augmentation methods focus on augmenting datasets during the training phase. At the testing phase, simple transformations are still widely used for test-time augmentation. This paper proposes a novel instance-level test-time augmentation that efficiently selects suitable transformations for a test input. Our proposed method involves an auxiliary module to predict the loss of each possible transformation given the input. Then, the transformations having lower predicted losses are applied to the input. The network obtains the results by averaging the prediction results of augmented inputs. Experimental results on several image classification benchmarks show that the proposed instance-aware test-time augmentation improves the model's robustness against various corruptions.



### DeepCSR: A 3D Deep Learning Approach for Cortical Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2010.11423v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11423v1)
- **Published**: 2020-10-22 03:57:44+00:00
- **Updated**: 2020-10-22 03:57:44+00:00
- **Authors**: Rodrigo Santa Cruz, Leo Lebrat, Pierrick Bourgeat, Clinton Fookes, Jurgen Fripp, Olivier Salvado
- **Comment**: Accepted in 2021 IEEE Winter Conference on Applications of Computer
  Vision (WACV)
- **Journal**: None
- **Summary**: The study of neurodegenerative diseases relies on the reconstruction and analysis of the brain cortex from magnetic resonance imaging (MRI). Traditional frameworks for this task like FreeSurfer demand lengthy runtimes, while its accelerated variant FastSurfer still relies on a voxel-wise segmentation which is limited by its resolution to capture narrow continuous objects as cortical surfaces. Having these limitations in mind, we propose DeepCSR, a 3D deep learning framework for cortical surface reconstruction from MRI. Towards this end, we train a neural network model with hypercolumn features to predict implicit surface representations for points in a brain template space. After training, the cortical surface at a desired level of detail is obtained by evaluating surface representations at specific coordinates, and subsequently applying a topology correction algorithm and an isosurface extraction method. Thanks to the continuous nature of this approach and the efficacy of its hypercolumn features scheme, DeepCSR efficiently reconstructs cortical surfaces at high resolution capturing fine details in the cortical folding. Moreover, DeepCSR is as accurate, more precise, and faster than the widely used FreeSurfer toolbox and its deep learning powered variant FastSurfer on reconstructing cortical surfaces from MRI which should facilitate large-scale medical studies and new healthcare applications.



### Efficient Scale-Permuted Backbone with Learned Resource Distribution
- **Arxiv ID**: http://arxiv.org/abs/2010.11426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.11426v1)
- **Published**: 2020-10-22 03:59:51+00:00
- **Updated**: 2020-10-22 03:59:51+00:00
- **Authors**: Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Yin Cui, Mingxing Tan, Quoc Le, Xiaodan Song
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Recently, SpineNet has demonstrated promising results on object detection and image classification over ResNet model. However, it is unclear if the improvement adds up when combining scale-permuted backbone with advanced efficient operations and compound scaling. Furthermore, SpineNet is built with a uniform resource distribution over operations. While this strategy seems to be prevalent for scale-decreased models, it may not be an optimal design for scale-permuted models. In this work, we propose a simple technique to combine efficient operations and compound scaling with a previously learned scale-permuted architecture. We demonstrate the efficiency of scale-permuted model can be further improved by learning a resource distribution over the entire network. The resulting efficient scale-permuted models outperform state-of-the-art EfficientNet-based models on object detection and achieve competitive performance on image classification and semantic segmentation. Code and models will be open-sourced soon.



### Task-Adaptive Feature Transformer for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.11437v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.11437v1)
- **Published**: 2020-10-22 04:35:37+00:00
- **Updated**: 2020-10-22 04:35:37+00:00
- **Authors**: Jun Seo, Young-Hyun Park, Sung-Whan Yoon, Jaekyun Moon
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning allows machines to classify novel classes using only a few labeled samples. Recently, few-shot segmentation aiming at semantic segmentation on low sample data has also seen great interest. In this paper, we propose a learnable module for few-shot segmentation, the task-adaptive feature transformer (TAFT). TAFT linearly transforms task-specific high-level features to a set of task-agnostic features well-suited to the segmentation job. Using this task-conditioned feature transformation, the model is shown to effectively utilize the semantic information in novel classes to generate tight segmentation masks. The proposed TAFT module can be easily plugged into existing semantic segmentation algorithms to achieve few-shot segmentation capability with only a few added parameters. We combine TAFT with Deeplab V3+, a well-known segmentation architecture; experiments on the PASCAL-$5^i$ dataset confirm that this combination successfully adds few-shot learning capability to the segmentation algorithm, achieving the state-of-the-art few-shot segmentation performance in some key representative cases.



### GAN based Unsupervised Segmentation: Should We Match the Exact Number of Objects
- **Arxiv ID**: http://arxiv.org/abs/2010.11438v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11438v1)
- **Published**: 2020-10-22 04:36:41+00:00
- **Updated**: 2020-10-22 04:36:41+00:00
- **Authors**: Quan Liu, Isabella M. Gaeta, Bryan A. Millis, Matthew J. Tyska, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: The unsupervised segmentation is an increasingly popular topic in biomedical image analysis. The basic idea is to approach the supervised segmentation task as an unsupervised synthesis problem, where the intensity images can be transferred to the annotation domain using cycle-consistent adversarial learning. The previous studies have shown that the macro-level (global distribution level) matching on the number of the objects (e.g., cells, tissues, protrusions etc.) between two domains resulted in better segmentation performance. However, no prior studies have exploited whether the unsupervised segmentation performance would be further improved when matching the exact number of objects at micro-level (mini-batch level). In this paper, we propose a deep learning based unsupervised segmentation method for segmenting highly overlapped and dynamic sub-cellular microvilli. With this challenging task, both micro-level and macro-level matching strategies were evaluated. To match the number of objects at the micro-level, the novel fluorescence-based micro-level matching approach was presented. From the experimental results, the micro-level matching did not improve the segmentation performance, compared with the simpler macro-level matching.



### Fine-tuned Pre-trained Mask R-CNN Models for Surface Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.11464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11464v1)
- **Published**: 2020-10-22 06:09:59+00:00
- **Updated**: 2020-10-22 06:09:59+00:00
- **Authors**: Haruhiro Fujita, Masatoshi Itagaki, Kenta Ichikawa, Yew Kwang Hooi, Kazutaka Kawano, Ryo Yamamoto
- **Comment**: 12 page, 12 tables, 15 figures, to be published in one of
  professional journals
- **Journal**: None
- **Summary**: This study evaluates road surface object detection tasks using four Mask R-CNN models as a pre-study of surface deterioration detection of stone-made archaeological objects. The models were pre-trained and fine-tuned by COCO datasets and 15,188 segmented road surface annotation tags. The quality of the models were measured using Average Precisions and Average Recalls. Result indicates substantial number of counts of false negatives, i.e. left detection and unclassified detections. A modified confusion matrix model to avoid prioritizing IoU is tested and there are notable true positive increases in bounding box detection, but almost no changes in segmentation masks.



### Novel View Synthesis from only a 6-DoF Camera Pose by Two-stage Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11468v1)
- **Published**: 2020-10-22 06:23:40+00:00
- **Updated**: 2020-10-22 06:23:40+00:00
- **Authors**: Xiang Guo, Bo Li, Yuchao Dai, Tongxin Zhang, Hui Deng
- **Comment**: Accepted by International Conference on Pattern Recognition (ICPR
  2020)
- **Journal**: None
- **Summary**: Novel view synthesis is a challenging problem in computer vision and robotics. Different from the existing works, which need the reference images or 3D models of the scene to generate images under novel views, we propose a novel paradigm to this problem. That is, we synthesize the novel view from only a 6-DoF camera pose directly. Although this setting is the most straightforward way, there are few works addressing it. While, our experiments demonstrate that, with a concise CNN, we could get a meaningful parametric model that could reconstruct the correct scenery images only from the 6-DoF pose. To this end, we propose a two-stage learning strategy, which consists of two consecutive CNNs: GenNet and RefineNet. GenNet generates a coarse image from a camera pose. RefineNet is a generative adversarial network that refines the coarse image. In this way, we decouple the geometric relationship between mapping and texture detail rendering. Extensive experiments conducted on the public datasets prove the effectiveness of our method. We believe this paradigm is of high research and application value and could be an important direction in novel view synthesis.



### An explainable deep vision system for animal classification and detection in trail-camera images with automatic post-deployment retraining
- **Arxiv ID**: http://arxiv.org/abs/2010.11472v3
- **DOI**: 10.1016/j.knosys.2021.106815
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11472v3)
- **Published**: 2020-10-22 06:29:55+00:00
- **Updated**: 2020-12-08 02:17:00+00:00
- **Authors**: Golnaz Moallem, Don D. Pathirage, Joel Reznick, James Gallagher, Hamed Sari-Sarraf
- **Comment**: None
- **Journal**: Knowledge-Based Systems, Volume 216, 15 March 2021, 106815
- **Summary**: This paper introduces an automated vision system for animal detection in trail-camera images taken from a field under the administration of the Texas Parks and Wildlife Department. As traditional wildlife counting techniques are intrusive and labor intensive to conduct, trail-camera imaging is a comparatively non-intrusive method for capturing wildlife activity. However, given the large volume of images produced from trail-cameras, manual analysis of the images remains time-consuming and inefficient. We implemented a two-stage deep convolutional neural network pipeline to find animal-containing images in the first stage and then process these images to detect birds in the second stage. The animal classification system classifies animal images with overall 93% sensitivity and 96% specificity. The bird detection system achieves better than 93% sensitivity, 92% specificity, and 68% average Intersection-over-Union rate. The entire pipeline processes an image in less than 0.5 seconds as opposed to an average 30 seconds for a human labeler. We also addressed post-deployment issues related to data drift for the animal classification system as image features vary with seasonal changes. This system utilizes an automatic retraining algorithm to detect data drift and update the system. We introduce a novel technique for detecting drifted images and triggering the retraining procedure. Two statistical experiments are also presented to explain the prediction behavior of the animal classification system. These experiments investigate the cues that steers the system towards a particular decision. Statistical hypothesis testing demonstrates that the presence of an animal in the input image significantly contributes to the system's decisions.



### High resolution weakly supervised localization architectures for medical images
- **Arxiv ID**: http://arxiv.org/abs/2010.11475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11475v1)
- **Published**: 2020-10-22 06:42:00+00:00
- **Updated**: 2020-10-22 06:42:00+00:00
- **Authors**: Konpat Preechakul, Sira Sriswasdi, Boonserm Kijsirikul, Ekapol Chuangsuwanich
- **Comment**: submitted to ICASSP 2021
- **Journal**: None
- **Summary**: In medical imaging, Class-Activation Map (CAM) serves as the main explainability tool by pointing to the region of interest. Since the localization accuracy from CAM is constrained by the resolution of the model's feature map, one may expect that segmentation models, which generally have large feature maps, would produce more accurate CAMs. However, we have found that this is not the case due to task mismatch. While segmentation models are developed for datasets with pixel-level annotation, only image-level annotation is available in most medical imaging datasets. Our experiments suggest that Global Average Pooling (GAP) and Group Normalization are the main culprits that worsen the localization accuracy of CAM. To address this issue, we propose Pyramid Localization Network (PYLON), a model for high-accuracy weakly-supervised localization that achieved 0.62 average point localization accuracy on NIH's Chest X-Ray 14 dataset, compared to 0.45 for a traditional CAM model. Source code and extended results are available at https://github.com/cmb-chula/pylon.



### SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform
- **Arxiv ID**: http://arxiv.org/abs/2010.11488v1
- **DOI**: 10.1109/TVCG.2020.3032566
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11488v1)
- **Published**: 2020-10-22 07:15:23+00:00
- **Updated**: 2020-10-22 07:15:23+00:00
- **Authors**: Cheng Lin, Lingjie Liu, Changjian Li, Leif Kobbelt, Bin Wang, Shiqing Xin, Wenping Wang
- **Comment**: IEEE Transactions on Visualization and Computer Graphics (TVCG), to
  appear
- **Journal**: None
- **Summary**: Segmenting arbitrary 3D objects into constituent parts that are structurally meaningful is a fundamental problem encountered in a wide range of computer graphics applications. Existing methods for 3D shape segmentation suffer from complex geometry processing and heavy computation caused by using low-level features and fragmented segmentation results due to the lack of global consideration. We present an efficient method, called SEG-MAT, based on the medial axis transform (MAT) of the input shape. Specifically, with the rich geometrical and structural information encoded in the MAT, we are able to develop a simple and principled approach to effectively identify the various types of junctions between different parts of a 3D shape. Extensive evaluations and comparisons show that our method outperforms the state-of-the-art methods in terms of segmentation quality and is also one order of magnitude faster.



### 3D Meta-Registration: Learning to Learn Registration of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.11504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11504v1)
- **Published**: 2020-10-22 07:45:09+00:00
- **Updated**: 2020-10-22 07:45:09+00:00
- **Authors**: Lingjing Wang, Yu Hao, Xiang Li, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based point cloud registration models are often generalized from extensive training over a large volume of data to learn the ability to predict the desired geometric transformation to register 3D point clouds. In this paper, we propose a meta-learning based 3D registration model, named 3D Meta-Registration, that is capable of rapidly adapting and well generalizing to new 3D registration tasks for unseen 3D point clouds. Our 3D Meta-Registration gains a competitive advantage by training over a variety of 3D registration tasks, which leads to an optimized model for the best performance on the distribution of registration tasks including potentially unseen tasks. Specifically, the proposed 3D Meta-Registration model consists of two modules: 3D registration learner and 3D registration meta-learner. During the training, the 3D registration learner is trained to complete a specific registration task aiming to determine the desired geometric transformation that aligns the source point cloud with the target one. In the meantime, the 3D registration meta-learner is trained to provide the optimal parameters to update the 3D registration learner based on the learned task distribution. After training, the 3D registration meta-learner, which is learned with the optimized coverage of distribution of 3D registration tasks, is able to dynamically update 3D registration learners with desired parameters to rapidly adapt to new registration tasks. We tested our model on synthesized dataset ModelNet and FlyingThings3D, as well as real-world dataset KITTI. Experimental results demonstrate that 3D Meta-Registration achieves superior performance over other previous techniques (e.g. FlowNet3D).



### F-Siamese Tracker: A Frustum-based Double Siamese Network for 3D Single Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2010.11510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11510v1)
- **Published**: 2020-10-22 08:01:17+00:00
- **Updated**: 2020-10-22 08:01:17+00:00
- **Authors**: Hao Zou, Jinhao Cui, Xin Kong, Chujuan Zhang, Yong Liu, Feng Wen, Wanlong Li
- **Comment**: 7pages, 5 figure
- **Journal**: None
- **Summary**: This paper presents F-Siamese Tracker, a novel approach for single object tracking prominently characterized by more robustly integrating 2D and 3D information to reduce redundant search space. A main challenge in 3D single object tracking is how to reduce search space for generating appropriate 3D candidates. Instead of solely relying on 3D proposals, firstly, our method leverages the Siamese network applied on RGB images to produce 2D region proposals which are then extruded into 3D viewing frustums. Besides, we perform an online accuracy validation on the 3D frustum to generate refined point cloud searching space, which can be embedded directly into the existing 3D tracking backbone. For efficiency, our approach gains better performance with fewer candidates by reducing search space. In addition, benefited from introducing the online accuracy validation, for occasional cases with strong occlusions or very sparse points, our approach can still achieve high precision, even when the 2D Siamese tracker loses the target. This approach allows us to set a new state-of-the-art in 3D single object tracking by a significant margin on a sparse outdoor dataset (KITTI tracking). Moreover, experiments on 2D single object tracking show that our framework boosts 2D tracking performance as well.



### Malaria detection from RBC images using shallow Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11521v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11521v1)
- **Published**: 2020-10-22 08:32:10+00:00
- **Updated**: 2020-10-22 08:32:10+00:00
- **Authors**: Subrata Sarkar, Rati Sharma, Kushal Shah
- **Comment**: 8 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: The advent of Deep Learning models like VGG-16 and Resnet-50 has considerably revolutionized the field of image classification, and by using these Convolutional Neural Networks (CNN) architectures, one can get a high classification accuracy on a wide variety of image datasets. However, these Deep Learning models have a very high computational complexity and so incur a high computational cost of running these algorithms as well as make it hard to interpret the results. In this paper, we present a shallow CNN architecture which gives the same classification accuracy as the VGG-16 and Resnet-50 models for thin blood smear RBC slide images for detection of malaria, while decreasing the computational run time by an order of magnitude. This can offer a significant advantage for commercial deployment of these algorithms, especially in poorer countries in Africa and some parts of the Indian subcontinent, where the menace of malaria is quite severe.



### Non-convex Super-resolution of OCT images via sparse representation
- **Arxiv ID**: http://arxiv.org/abs/2010.12576v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2010.12576v1)
- **Published**: 2020-10-22 08:44:11+00:00
- **Updated**: 2020-10-22 08:44:11+00:00
- **Authors**: Gabriele Scrivanti, Luca Calatroni, Serena Morigi, Lindsay Nicholson, Alin Achim
- **Comment**: 4 pages, 2 figures, 1 table, 1 algorithm, submitted to ISBI2021
- **Journal**: None
- **Summary**: We propose a non-convex variational model for the super-resolution of Optical Coherence Tomography (OCT) images of the murine eye, by enforcing sparsity with respect to suitable dictionaries learnt from high-resolution OCT data. The statistical characteristics of OCT images motivate the use of {\alpha}-stable distributions for learning dictionaries, by considering the non-Gaussian case, {\alpha}=1. The sparsity-promoting cost function relies on a non-convex penalty - Cauchy-based or Minimax Concave Penalty (MCP) - which makes the problem particularly challenging. We propose an efficient algorithm for minimizing the function based on the forward-backward splitting strategy which guarantees at each iteration the existence and uniqueness of the proximal point. Comparisons with standard convex L1-based reconstructions show the better performance of non-convex models, especially in view of further OCT image analysis



### Convolutional Autoencoders for Human Motion Infilling
- **Arxiv ID**: http://arxiv.org/abs/2010.11531v1
- **DOI**: 10.1109/3DV50981.2020.00102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11531v1)
- **Published**: 2020-10-22 08:45:38+00:00
- **Updated**: 2020-10-22 08:45:38+00:00
- **Authors**: Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece, Remo Ziegler, Otmar Hilliges
- **Comment**: Accepted to 3DV 2020
- **Journal**: None
- **Summary**: In this paper we propose a convolutional autoencoder to address the problem of motion infilling for 3D human motion data. Given a start and end sequence, motion infilling aims to complete the missing gap in between, such that the filled in poses plausibly forecast the start sequence and naturally transition into the end sequence. To this end, we propose a single, end-to-end trainable convolutional autoencoder. We show that a single model can be used to create natural transitions between different types of activities. Furthermore, our method is not only able to fill in entire missing frames, but it can also be used to complete gaps where partial poses are available (e.g. from end effectors), or to clean up other forms of noise (e.g. Gaussian). Also, the model can fill in an arbitrary number of gaps that potentially vary in length. In addition, no further post-processing on the model's outputs is necessary such as smoothing or closing discontinuities at the end of the gap. At the heart of our approach lies the idea to cast motion infilling as an inpainting problem and to train a convolutional de-noising autoencoder on image-like representations of motion sequences. At training time, blocks of columns are removed from such images and we ask the model to fill in the gaps. We demonstrate the versatility of the approach via a number of complex motion sequences and report on thorough evaluations performed to better understand the capabilities and limitations of the proposed approach.



### Defense-guided Transferable Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2010.11535v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11535v2)
- **Published**: 2020-10-22 08:51:45+00:00
- **Updated**: 2020-11-04 01:34:31+00:00
- **Authors**: Zifei Zhang, Kai Qiao, Jian Chen, Ningning Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Though deep neural networks perform challenging tasks excellently, they are susceptible to adversarial examples, which mislead classifiers by applying human-imperceptible perturbations on clean inputs. Under the query-free black-box scenario, adversarial examples are hard to transfer to unknown models, and several methods have been proposed with the low transferability. To settle such issue, we design a max-min framework inspired by input transformations, which are benificial to both the adversarial attack and defense. Explicitly, we decrease loss values with inputs' affline transformations as a defense in the minimum procedure, and then increase loss values with the momentum iterative algorithm as an attack in the maximum procedure. To further promote transferability, we determine transformed values with the max-min theory. Extensive experiments on Imagenet demonstrate that our defense-guided transferable attacks achieve impressive increase on transferability. Experimentally, we show that our ASR of adversarial attack reaches to 58.38% on average, which outperforms the state-of-the-art method by 12.1% on the normally trained models and by 11.13% on the adversarially trained models. Additionally, we provide elucidative insights on the improvement of transferability, and our method is expected to be a benchmark for assessing the robustness of deep models.



### TLGAN: document Text Localization using Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/2010.11547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T10 (Primary), 68T07 (Secondary), I.5.1; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2010.11547v1)
- **Published**: 2020-10-22 09:19:13+00:00
- **Updated**: 2020-10-22 09:19:13+00:00
- **Authors**: Dongyoung Kim, Myungsung Kwak, Eunji Won, Sejung Shin, Jeongyeon Nam
- **Comment**: 17 pages, three figures, 4 tables, methods for IEEE ICDAR RRC SROIE
  task1 leader board
- **Journal**: None
- **Summary**: Text localization from the digital image is the first step for the optical character recognition task. Conventional image processing based text localization performs adequately for specific examples. Yet, a general text localization are only archived by recent deep-learning based modalities. Here we present document Text Localization Generative Adversarial Nets (TLGAN) which are deep neural networks to perform the text localization from digital image. TLGAN is an versatile and easy-train text localization model requiring a small amount of data. Training only ten labeled receipt images from Robust Reading Challenge on Scanned Receipts OCR and Information Extraction (SROIE), TLGAN achieved 99.83% precision and 99.64% recall for SROIE test data. Our TLGAN is a practical text localization solution requiring minimal effort for data labeling and model training and producing a state-of-art performance.



### Learning Dual Semantic Relations with Graph Attention for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2010.11550v1
- **DOI**: 10.1109/TCSVT.2020.3030656
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.11550v1)
- **Published**: 2020-10-22 09:21:32+00:00
- **Updated**: 2020-10-22 09:21:32+00:00
- **Authors**: Keyu Wen, Xiaodong Gu, Qingrong Cheng
- **Comment**: 14pages, 9 figures. Accepted at: IEEE Transactions on Circuits and
  Systems for Video Technology (Early Access Print) | |Codes Available at:
  https://github.com/kywen1119/DSRAN
- **Journal**: None
- **Summary**: Image-Text Matching is one major task in cross-modal information processing. The main challenge is to learn the unified visual and textual representations. Previous methods that perform well on this task primarily focus on not only the alignment between region features in images and the corresponding words in sentences, but also the alignment between relations of regions and relational words. However, the lack of joint learning of regional features and global features will cause the regional features to lose contact with the global context, leading to the mismatch with those non-object words which have global meanings in some sentences. In this work, in order to alleviate this issue, it is necessary to enhance the relations between regions and the relations between regional and global concepts to obtain a more accurate visual representation so as to be better correlated to the corresponding text. Thus, a novel multi-level semantic relations enhancement approach named Dual Semantic Relations Attention Network(DSRAN) is proposed which mainly consists of two modules, separate semantic relations module and the joint semantic relations module. DSRAN performs graph attention in both modules respectively for region-level relations enhancement and regional-global relations enhancement at the same time. With these two modules, different hierarchies of semantic relations are learned simultaneously, thus promoting the image-text matching process by providing more information for the final visual representation. Quantitative experimental results have been performed on MS-COCO and Flickr30K and our method outperforms previous approaches by a large margin due to the effectiveness of the dual semantic relations learning scheme. Codes are available at https://github.com/kywen1119/DSRAN.



### Fingerprint Orientation Estimation: Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/2010.11563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11563v1)
- **Published**: 2020-10-22 09:41:18+00:00
- **Updated**: 2020-10-22 09:41:18+00:00
- **Authors**: Amit Kumar Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: There is an exponential increase in portable electronic devices with biometric security mechanisms, in particular fingerprint biometric. A person has a limited number of fingerprints and it remains unchanged throughout his lifetime, once leaked to the adversary, it leaks for a lifetime. So, there is a need to secure the biometric template itself. In this survey paper, we review the different security models and fingerprint template protection techniques. The research challenges in different fingerprint template protection techniques are also highlighted in respective sections of the paper. This survey provides a comprehensive study of template protection techniques for fingerprint biometric systems and highlights the challenges and future opportunities.



### Face Hallucination via Split-Attention in Split-Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2010.11575v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11575v3)
- **Published**: 2020-10-22 10:09:31+00:00
- **Updated**: 2021-07-07 10:08:19+00:00
- **Authors**: Tao Lu, Yuanzhi Wang, Yanduo Zhang, Yu Wang, Wei Liu, Zhongyuan Wang, Junjun Jiang
- **Comment**: Accepted by ACM MM 2021
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) have been widely employed to promote the face hallucination due to the ability to predict high-frequency details from a large number of samples. However, most of them fail to take into account the overall facial profile and fine texture details simultaneously, resulting in reduced naturalness and fidelity of the reconstructed face, and further impairing the performance of downstream tasks (e.g., face detection, facial recognition). To tackle this issue, we propose a novel external-internal split attention group (ESAG), which encompasses two paths responsible for facial structure information and facial texture details, respectively. By fusing the features from these two paths, the consistency of facial structure and the fidelity of facial details are strengthened at the same time. Then, we propose a split-attention in split-attention network (SISN) to reconstruct photorealistic high-resolution facial images by cascading several ESAGs. Experimental results on face hallucination and face recognition unveil that the proposed method not only significantly improves the clarity of hallucinated faces, but also encourages the subsequent face recognition performance substantially. Codes have been released at https://github.com/mdswyz/SISN-Face-Hallucination.



### Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2010.11594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11594v1)
- **Published**: 2020-10-22 10:53:32+00:00
- **Updated**: 2020-10-22 10:53:32+00:00
- **Authors**: Yuanhao Zhai, Le Wang, Wei Tang, Qilin Zhang, Junsong Yuan, Gang Hua
- **Comment**: ECCV 2020 spotlight
- **Journal**: None
- **Summary**: Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods.



### On the Power of Deep but Naive Partial Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.11600v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.11600v2)
- **Published**: 2020-10-22 11:02:56+00:00
- **Updated**: 2021-02-08 07:32:23+00:00
- **Authors**: Junghoon Seo, Joon Suk Huh
- **Comment**: None
- **Journal**: 2021 IEEE ICASSP International Conference on Acoustics, Speech,
  and Signal Processing
- **Summary**: Partial label learning (PLL) is a class of weakly supervised learning where each training instance consists of a data and a set of candidate labels containing a unique ground truth label. To tackle this problem, a majority of current state-of-the-art methods employs either label disambiguation or averaging strategies. So far, PLL methods without such techniques have been considered impractical. In this paper, we challenge this view by revealing the hidden power of the oldest and naivest PLL method when it is instantiated with deep neural networks. Specifically, we show that, with deep neural networks, the naive model can achieve competitive performances against the other state-of-the-art methods, suggesting it as a strong baseline for PLL. We also address the question of how and why such a naive model works well with deep neural networks. Our empirical results indicate that deep neural networks trained on partially labeled examples generalize very well even in the over-parametrized regime and without label disambiguations or regularizations. We point out that existing learning theories on PLL are vacuous in the over-parametrized regime. Hence they cannot explain why the deep naive method works. We propose an alternative theory on how deep learning generalize in PLL problems.



### Self-Supervised Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2010.11619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2010.11619v1)
- **Published**: 2020-10-22 11:33:41+00:00
- **Updated**: 2020-10-22 11:33:41+00:00
- **Authors**: Florin-Alexandru Vasluianu, Andres Romero, Luc Van Gool, Radu Timofte
- **Comment**: 10 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Shadow removal is an important computer vision task aiming at the detection and successful removal of the shadow produced by an occluded light source and a photo-realistic restoration of the image contents. Decades of re-search produced a multitude of hand-crafted restoration techniques and, more recently, learned solutions from shad-owed and shadow-free training image pairs. In this work,we propose an unsupervised single image shadow removal solution via self-supervised learning by using a conditioned mask. In contrast to existing literature, we do not require paired shadowed and shadow-free images, instead we rely on self-supervision and jointly learn deep models to remove and add shadows to images. We validate our approach on the recently introduced ISTD and USR datasets. We largely improve quantitatively and qualitatively over the compared methods and set a new state-of-the-art performance in single image shadow removal.



### A ReLU Dense Layer to Improve the Performance of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.13572v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.13572v1)
- **Published**: 2020-10-22 11:56:01+00:00
- **Updated**: 2020-10-22 11:56:01+00:00
- **Authors**: Alireza M. Javid, Sandipan Das, Mikael Skoglund, Saikat Chatterjee
- **Comment**: Submitted to ICASSP 2021
- **Journal**: None
- **Summary**: We propose ReDense as a simple and low complexity way to improve the performance of trained neural networks. We use a combination of random weights and rectified linear unit (ReLU) activation function to add a ReLU dense (ReDense) layer to the trained neural network such that it can achieve a lower training loss. The lossless flow property (LFP) of ReLU is the key to achieve the lower training loss while keeping the generalization error small. ReDense does not suffer from vanishing gradient problem in the training due to having a shallow structure. We experimentally show that ReDense can improve the training and testing performance of various neural network architectures with different optimization loss and activation functions. Finally, we test ReDense on some of the state-of-the-art architectures and show the performance improvement on benchmark datasets.



### Learning to Sort Image Sequences via Accumulated Temporal Differences
- **Arxiv ID**: http://arxiv.org/abs/2010.11649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11649v1)
- **Published**: 2020-10-22 12:34:05+00:00
- **Updated**: 2020-10-22 12:34:05+00:00
- **Authors**: Gagan Kanojia, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Consider a set of n images of a scene with dynamic objects captured with a static or a handheld camera. Let the temporal order in which these images are captured be unknown. There can be n! possibilities for the temporal order in which these images could have been captured. In this work, we tackle the problem of temporally sequencing the unordered set of images of a dynamic scene captured with a hand-held camera. We propose a convolutional block which captures the spatial information through 2D convolution kernel and captures the temporal information by utilizing the differences present among the feature maps extracted from the input images. We evaluate the performance of the proposed approach on the dataset extracted from a standard action recognition dataset, UCF101. We show that the proposed approach outperforms the state-of-the-art methods by a significant margin. We show that the network generalizes well by evaluating it on a dataset extracted from the DAVIS dataset, a dataset meant for video object segmentation, when the same network was trained with a dataset extracted from UCF101, a dataset meant for action recognition.



### Restoring Negative Information in Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.11714v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.11714v2)
- **Published**: 2020-10-22 13:39:48+00:00
- **Updated**: 2020-10-26 03:33:08+00:00
- **Authors**: Yukuan Yang, Fangyun Wei, Miaojing Shi, Guoqi Li
- **Comment**: To appear in NeurIPS 2020
- **Journal**: None
- **Summary**: Few-shot learning has recently emerged as a new challenge in the deep learning field: unlike conventional methods that train the deep neural networks (DNNs) with a large number of labeled data, it asks for the generalization of DNNs on new classes with few annotated samples. Recent advances in few-shot learning mainly focus on image classification while in this paper we focus on object detection. The initial explorations in few-shot object detection tend to simulate a classification scenario by using the positive proposals in images with respect to certain object class while discarding the negative proposals of that class. Negatives, especially hard negatives, however, are essential to the embedding space learning in few-shot object detection. In this paper, we restore the negative information in few-shot object detection by introducing a new negative- and positive-representative based metric learning framework and a new inference scheme with negative and positive representatives. We build our work on a recent few-shot pipeline RepMet with several new modules to encode negative information for both training and testing. Extensive experiments on ImageNet-LOC and PASCAL VOC show our method substantially improves the state-of-the-art few-shot object detection solutions. Our code is available at https://github.com/yang-yk/NP-RepMet.



### Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.11757v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11757v4)
- **Published**: 2020-10-22 14:26:09+00:00
- **Updated**: 2021-03-29 14:33:42+00:00
- **Authors**: Chun-Fu Chen, Rameswar Panda, Kandan Ramakrishnan, Rogerio Feris, John Cohn, Aude Oliva, Quanfu Fan
- **Comment**: CVPR 2021 camera-ready version. Codes and models are available on
  https://github.com/IBM/action-recognition-pytorch
- **Journal**: None
- **Summary**: In recent years, a number of approaches based on 2D or 3D convolutional neural networks (CNN) have emerged for video action recognition, achieving state-of-the-art results on several large-scale benchmark datasets. In this paper, we carry out in-depth comparative analysis to better understand the differences between these approaches and the progress made by them. To this end, we develop an unified framework for both 2D-CNN and 3D-CNN action models, which enables us to remove bells and whistles and provides a common ground for fair comparison. We then conduct an effort towards a large-scale analysis involving over 300 action recognition models. Our comprehensive analysis reveals that a) a significant leap is made in efficiency for action recognition, but not in accuracy; b) 2D-CNN and 3D-CNN models behave similarly in terms of spatio-temporal representation abilities and transferability. Our codes are available at https://github.com/IBM/action-recognition-pytorch.



### FasterRCNN Monitoring of Road Damages: Competition and Deployment
- **Arxiv ID**: http://arxiv.org/abs/2010.11780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11780v1)
- **Published**: 2020-10-22 14:56:00+00:00
- **Updated**: 2020-10-22 14:56:00+00:00
- **Authors**: Hascoet Tristan, Yihao Zhang, Persch Andreas, Ryoichi Takashima, Tetsuya Takiguchi, Yasuo Ariki
- **Comment**: None
- **Journal**: None
- **Summary**: Maintaining aging infrastructure is a challenge currently faced by local and national administrators all around the world. An important prerequisite for efficient infrastructure maintenance is to continuously monitor (i.e., quantify the level of safety and reliability) the state of very large structures. Meanwhile, computer vision has made impressive strides in recent years, mainly due to successful applications of deep learning models. These novel progresses are allowing the automation of vision tasks, which were previously impossible to automate, offering promising possibilities to assist administrators in optimizing their infrastructure maintenance operations. In this context, the IEEE 2020 global Road Damage Detection (RDD) Challenge is giving an opportunity for deep learning and computer vision researchers to get involved and help accurately track pavement damages on road networks. This paper proposes two contributions to that topic: In a first part, we detail our solution to the RDD Challenge. In a second part, we present our efforts in deploying our model on a local road network, explaining the proposed methodology and encountered challenges.



### Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2010.11800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11800v1)
- **Published**: 2020-10-22 15:27:31+00:00
- **Updated**: 2020-10-22 15:27:31+00:00
- **Authors**: Zhengxia Zou
- **Comment**: project website: https://jiupinjia.github.io/skyar/
- **Journal**: None
- **Summary**: This paper proposes a vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles. Different from previous sky editing methods that either focus on static photos or require inertial measurement units integrated in smartphones on shooting videos, our method is purely vision-based, without any requirements on the capturing devices, and can be well applied to either online or offline processing scenarios. Our method runs in real-time and is free of user interactions. We decompose this artistic creation process into a couple of proxy tasks including sky matting, motion estimation, and image blending. Experiments are conducted on videos diversely captured in the wild by handheld smartphones and dash cameras, and show high fidelity and good generalization of our method in both visual quality and lighting/motion dynamics. Our code and animated results are available at \url{https://jiupinjia.github.io/skyar/}.



### Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free
- **Arxiv ID**: http://arxiv.org/abs/2010.11828v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11828v2)
- **Published**: 2020-10-22 16:06:34+00:00
- **Updated**: 2020-11-10 08:18:58+00:00
- **Authors**: Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, Zhangyang Wang
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Adversarial training and its many variants substantially improve deep network robustness, yet at the cost of compromising standard accuracy. Moreover, the training process is heavy and hence it becomes impractical to thoroughly explore the trade-off between accuracy and robustness. This paper asks this new question: how to quickly calibrate a trained model in-situ, to examine the achievable trade-offs between its standard and robust accuracies, without (re-)training it many times? Our proposed framework, Once-for-all Adversarial Training (OAT), is built on an innovative model-conditional training framework, with a controlling hyper-parameter as the input. The trained model could be adjusted among different standard and robust accuracies "for free" at testing time. As an important knob, we exploit dual batch normalization to separate standard and adversarial feature statistics, so that they can be learned in one model without degrading performance. We further extend OAT to a Once-for-all Adversarial Training and Slimming (OATS) framework, that allows for the joint trade-off among accuracy, robustness and runtime efficiency. Experiments show that, without any re-training nor ensembling, OAT/OATS achieve similar or even superior performance compared to dedicatedly trained models at various configurations. Our codes and pretrained models are available at: https://github.com/VITA-Group/Once-for-All-Adversarial-Training.



### Blind Video Temporal Consistency via Deep Video Prior
- **Arxiv ID**: http://arxiv.org/abs/2010.11838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11838v1)
- **Published**: 2020-10-22 16:19:20+00:00
- **Updated**: 2020-10-22 16:19:20+00:00
- **Authors**: Chenyang Lei, Yazhou Xing, Qifeng Chen
- **Comment**: NeurIPS 2020; github link: github.com/ChenyangLEI/deep-video-prior
- **Journal**: None
- **Summary**: Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with the Deep Video Prior. Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. Our source codes are publicly available at github.com/ChenyangLEI/deep-video-prior.



### Spatio-temporal Features for Generalized Detection of Deepfake Videos
- **Arxiv ID**: http://arxiv.org/abs/2010.11844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11844v1)
- **Published**: 2020-10-22 16:28:50+00:00
- **Updated**: 2020-10-22 16:28:50+00:00
- **Authors**: Ipek Ganiyusufoglu, L. Minh Ng, Nedko Savov, Sezer Karaoglu, Theo Gevers
- **Comment**: Submitted to Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: For deepfake detection, video-level detectors have not been explored as extensively as image-level detectors, which do not exploit temporal data. In this paper, we empirically show that existing approaches on image and sequence classifiers generalize poorly to new manipulation techniques. To this end, we propose spatio-temporal features, modeled by 3D CNNs, to extend the generalization capabilities to detect new sorts of deepfake videos. We show that spatial features learn distinct deepfake-method-specific attributes, while spatio-temporal features capture shared attributes between deepfake methods. We provide an in-depth analysis of how the sequential and spatio-temporal video encoders are utilizing temporal information using DFDC dataset arXiv:2006.07397. Thus, we unravel that our approach captures local spatio-temporal relations and inconsistencies in the deepfake videos while existing sequence encoders are indifferent to it. Through large scale experiments conducted on the FaceForensics++ arXiv:1901.08971 and Deeper Forensics arXiv:2001.03024 datasets, we show that our approach outperforms existing methods in terms of generalization capabilities.



### AEGIS: A real-time multimodal augmented reality computer vision based system to assist facial expression recognition for individuals with autism spectrum disorder
- **Arxiv ID**: http://arxiv.org/abs/2010.11884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2010.11884v1)
- **Published**: 2020-10-22 17:20:38+00:00
- **Updated**: 2020-10-22 17:20:38+00:00
- **Authors**: James Ren Hou Lee, Alexander Wong
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: The ability to interpret social cues comes naturally for most people, but for those living with Autism Spectrum Disorder (ASD), some experience a deficiency in this area. This paper presents the development of a multimodal augmented reality (AR) system which combines the use of computer vision and deep convolutional neural networks (CNN) in order to assist individuals with the detection and interpretation of facial expressions in social settings. The proposed system, which we call AEGIS (Augmented-reality Expression Guided Interpretation System), is an assistive technology deployable on a variety of user devices including tablets, smartphones, video conference systems, or smartglasses, showcasing its extreme flexibility and wide range of use cases, to allow integration into daily life with ease. Given a streaming video camera source, each real-world frame is passed into AEGIS, processed for facial bounding boxes, and then fed into our novel deep convolutional time windowed neural network (TimeConvNet). We leverage both spatial and temporal information in order to provide an accurate expression prediction, which is then converted into its corresponding visualization and drawn on top of the original video frame. The system runs in real-time, requires minimal set up and is simple to use. With the use of AEGIS, we can assist individuals living with ASD to learn to better identify expressions and thus improve their social experiences.



### GAZED- Gaze-guided Cinematic Editing of Wide-Angle Monocular Video Recordings
- **Arxiv ID**: http://arxiv.org/abs/2010.11886v1
- **DOI**: 10.1145/3313831.3376544
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.11886v1)
- **Published**: 2020-10-22 17:27:03+00:00
- **Updated**: 2020-10-22 17:27:03+00:00
- **Authors**: K L Bhanu Moorthy, Moneish Kumar, Ramanathan Subramaniam, Vineet Gandhi
- **Comment**: 10 pages
- **Journal**: In Proceedings of the 2020 CHI Conference on Human Factors in
  Computing Systems (CHI '20). Association for Computing Machinery, New York,
  NY, USA, 1-11
- **Summary**: We present GAZED- eye GAZe-guided EDiting for videos captured by a solitary, static, wide-angle and high-resolution camera. Eye-gaze has been effectively employed in computational applications as a cue to capture interesting scene content; we employ gaze as a proxy to select shots for inclusion in the edited video. Given the original video, scene content and user eye-gaze tracks are combined to generate an edited video comprising cinematically valid actor shots and shot transitions to generate an aesthetic and vivid representation of the original narrative. We model cinematic video editing as an energy minimization problem over shot selection, whose constraints capture cinematographic editing conventions. Gazed scene locations primarily determine the shots constituting the edited video. Effectiveness of GAZED against multiple competing methods is demonstrated via a psychophysical study involving 12 users and twelve performance videos.



### An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- **Arxiv ID**: http://arxiv.org/abs/2010.11929v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11929v2)
- **Published**: 2020-10-22 17:55:59+00:00
- **Updated**: 2021-06-03 13:08:56+00:00
- **Authors**: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
- **Comment**: Fine-tuning code and pre-trained models are available at
  https://github.com/google-research/vision_transformer. ICLR camera-ready
  version with 2 small modifications: 1) Added a discussion of CLS vs GAP
  classifier in the appendix, 2) Fixed an error in exaFLOPs computation in
  Figure 5 and Table 6 (relative performance of models is basically not
  affected)
- **Journal**: None
- **Summary**: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.



### Few-Shot Adaptation of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.11943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.11943v1)
- **Published**: 2020-10-22 17:59:29+00:00
- **Updated**: 2020-10-22 17:59:29+00:00
- **Authors**: Esther Robb, Wen-Sheng Chu, Abhishek Kumar, Jia-Bin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown remarkable performance in image synthesis tasks, but typically require a large number of training samples to achieve high-quality synthesis. This paper proposes a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images). FSGAN repurposes component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors. This provides a highly expressive parameter space for adaptation while constraining changes to the pretrained weights. We validate our method in a challenging few-shot setting of 5-100 images in the target domain. We show that our method has significant visual quality gains compared with existing GAN adaptation methods. We report qualitative and quantitative results showing the effectiveness of our method. We additionally highlight a problem for few-shot synthesis in the standard quantitative metric used by data-efficient image synthesis works. Code and additional results are available at http://e-271.github.io/few-shot-gan.



### Viability of Optical Coherence Tomography for Iris Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.10655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10655v1)
- **Published**: 2020-10-22 18:00:51+00:00
- **Updated**: 2020-10-22 18:00:51+00:00
- **Authors**: Renu Sharma, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose the use of Optical Coherence Tomography (OCT) imaging for the problem of iris presentation attack (PA) detection. We assess its viability by comparing its performance with respect to traditional iris imaging modalities, viz., near-infrared (NIR) and visible spectrum. OCT imaging provides a cross-sectional view of an eye, whereas traditional imaging provides 2D iris textural information. PA detection is performed using three state-of-the-art deep architectures (VGG19, ResNet50 and DenseNet121) to differentiate between bonafide and PA samples for each of the three imaging modalities. Experiments are performed on a dataset of 2,169 bonafide, 177 Van Dyke eyes and 360 cosmetic contact images acquired using all three imaging modalities under intra-attack (known PAs) and cross-attack (unknown PAs) scenarios. We observe promising results demonstrating OCT as a viable solution for iris presentation attack detection.



### Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2010.11971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.11971v1)
- **Published**: 2020-10-22 18:12:26+00:00
- **Updated**: 2020-10-22 18:12:26+00:00
- **Authors**: Yaochen Xie, Zhengyang Wang, Shuiwang Ji
- **Comment**: 15 pages, NeurIPS 2020
- **Journal**: None
- **Summary**: Self-supervised frameworks that learn denoising models with merely individual noisy images have shown strong capability and promising performance in various image denoising tasks. Existing self-supervised denoising frameworks are mostly built upon the same theoretical foundation, where the denoising models are required to be J-invariant. However, our analyses indicate that the current theory and the J-invariance may lead to denoising models with reduced performance. In this work, we introduce Noise2Same, a novel self-supervised denoising framework. In Noise2Same, a new self-supervised loss is proposed by deriving a self-supervised upper bound of the typical supervised loss. In particular, Noise2Same requires neither J-invariance nor extra information about the noise model and can be used in a wider range of denoising applications. We analyze our proposed Noise2Same both theoretically and experimentally. The experimental results show that our Noise2Same remarkably outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency. Our code is available at https://github.com/divelab/Noise2Same.



### MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences
- **Arxiv ID**: http://arxiv.org/abs/2010.11985v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.11985v2)
- **Published**: 2020-10-22 18:58:50+00:00
- **Updated**: 2021-04-28 18:44:01+00:00
- **Authors**: Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, Louis-Philippe Morency
- **Comment**: NAACL 2021
- **Journal**: None
- **Summary**: Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.



### Unsupervised deep learning for grading of age-related macular degeneration using retinal fundus images
- **Arxiv ID**: http://arxiv.org/abs/2010.11993v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2010.11993v1)
- **Published**: 2020-10-22 19:13:28+00:00
- **Updated**: 2020-10-22 19:13:28+00:00
- **Authors**: Baladitya Yellapragada, Sascha Hornhauer, Kiersten Snyder, Stella Yu, Glenn Yiu
- **Comment**: None
- **Journal**: None
- **Summary**: Many diseases are classified based on human-defined rubrics that are prone to bias. Supervised neural networks can automate the grading of retinal fundus images, but require labor-intensive annotations and are restricted to the specific trained task. Here, we employed an unsupervised network with Non-Parametric Instance Discrimination (NPID) to grade age-related macular degeneration (AMD) severity using fundus photographs from the Age-Related Eye Disease Study (AREDS). Our unsupervised algorithm demonstrated versatility across different AMD classification schemes without retraining, and achieved unbalanced accuracies comparable to supervised networks and human ophthalmologists in classifying advanced or referable AMD, or on the 4-step AMD severity scale. Exploring the networks behavior revealed disease-related fundus features that drove predictions and unveiled the susceptibility of more granular human-defined AMD severity schemes to misclassification by both ophthalmologists and neural networks. Importantly, unsupervised learning enabled unbiased, data-driven discovery of AMD features such as geographic atrophy, as well as other ocular phenotypes of the choroid, vitreous, and lens, such as visually-impairing cataracts, that were not pre-defined by human labels.



### CellCycleGAN: Spatiotemporal Microscopy Image Synthesis of Cell Populations using Statistical Shape Models and Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2010.12011v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/2010.12011v2)
- **Published**: 2020-10-22 20:02:41+00:00
- **Updated**: 2021-01-26 19:10:28+00:00
- **Authors**: Dennis Bhr, Dennis Eschweiler, Anuk Bhattacharyya, Daniel Moreno-Andrs, Wolfram Antonin, Johannes Stegmaier
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Automatic analysis of spatio-temporal microscopy images is inevitable for state-of-the-art research in the life sciences. Recent developments in deep learning provide powerful tools for automatic analyses of such image data, but heavily depend on the amount and quality of provided training data to perform well. To this end, we developed a new method for realistic generation of synthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei. The method combines spatiotemporal statistical shape models of different cell cycle stages with a conditional GAN to generate time series of cell populations and provides instance-level control of cell cycle stage and the fluorescence intensity of generated cells. We show the effect of the GAN conditioning and create a set of synthetic images that can be readily used for training and benchmarking of cell segmentation and tracking approaches.



### AutoPruning for Deep Neural Network with Dynamic Channel Masking
- **Arxiv ID**: http://arxiv.org/abs/2010.12021v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12021v2)
- **Published**: 2020-10-22 20:12:46+00:00
- **Updated**: 2020-11-02 04:06:51+00:00
- **Authors**: Baopu Li, Yanwen Fan, Zhihong Pan, Gang Zhang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Modern deep neural network models are large and computationally intensive. One typical solution to this issue is model pruning. However, most current pruning algorithms depend on hand crafted rules or domain expertise. To overcome this problem, we propose a learning based auto pruning algorithm for deep neural network, which is inspired by recent automatic machine learning(AutoML). A two objectives' problem that aims for the the weights and the best channels for each layer is first formulated. An alternative optimization approach is then proposed to derive the optimal channel numbers and weights simultaneously. In the process of pruning, we utilize a searchable hyperparameter, remaining ratio, to denote the number of channels in each convolution layer, and then a dynamic masking process is proposed to describe the corresponding channel evolution. To control the trade-off between the accuracy of a model and the pruning ratio of floating point operations, a novel loss function is further introduced. Preliminary experimental results on benchmark datasets demonstrate that our scheme achieves competitive results for neural network pruning.



### Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.12023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12023v1)
- **Published**: 2020-10-22 20:13:32+00:00
- **Updated**: 2020-10-22 20:13:32+00:00
- **Authors**: Zeyi Huang, Yang Zou, Vijayakumar Bhagavatula, Dong Huang
- **Comment**: Neural Information Processing Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO.



### Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.12035v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12035v2)
- **Published**: 2020-10-22 20:25:08+00:00
- **Updated**: 2020-11-18 01:09:01+00:00
- **Authors**: Lucas Tabelini, Rodrigo Berriel, Thiago M. Paixo, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Modern lane detection methods have achieved remarkable performances in complex real-world scenarios, but many have issues maintaining real-time efficiency, which is important for autonomous vehicles. In this work, we propose LaneATT: an anchor-based deep lane detection model, which, akin to other generic deep object detectors, uses the anchors for the feature pooling step. Since lanes follow a regular pattern and are highly correlated, we hypothesize that in some cases global information may be crucial to infer their positions, especially in conditions such as occlusion, missing lane markers, and others. Thus, this work proposes a novel anchor-based attention mechanism that aggregates global information. The model was evaluated extensively on three of the most widely used datasets in the literature. The results show that our method outperforms the current state-of-the-art methods showing both higher efficacy and efficiency. Moreover, an ablation study is performed along with a discussion on efficiency trade-off options that are useful in practice.



### Using Deep Image Priors to Generate Counterfactual Explanations
- **Arxiv ID**: http://arxiv.org/abs/2010.12046v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12046v1)
- **Published**: 2020-10-22 20:40:44+00:00
- **Updated**: 2020-10-22 20:40:44+00:00
- **Authors**: Vivek Narayanaswamy, Jayaraman J. Thiagarajan, Andreas Spanias
- **Comment**: None
- **Journal**: None
- **Summary**: Through the use of carefully tailored convolutional neural network architectures, a deep image prior (DIP) can be used to obtain pre-images from latent representation encodings. Though DIP inversion has been known to be superior to conventional regularized inversion strategies such as total variation, such an over-parameterized generator is able to effectively reconstruct even images that are not in the original data distribution. This limitation makes it challenging to utilize such priors for tasks such as counterfactual reasoning, wherein the goal is to generate small, interpretable changes to an image that systematically leads to changes in the model prediction. To this end, we propose a novel regularization strategy based on an auxiliary loss estimator jointly trained with the predictor, which efficiently guides the prior to recover natural pre-images. Our empirical studies with a real-world ISIC skin lesion detection problem clearly evidence the effectiveness of the proposed approach in synthesizing meaningful counterfactuals. In comparison, we find that the standard DIP inversion often proposes visually imperceptible perturbations to irrelevant parts of the image, thus providing no additional insights into the model behavior.



### Contrastive Learning with Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2010.12050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12050v1)
- **Published**: 2020-10-22 20:45:10+00:00
- **Updated**: 2020-10-22 20:45:10+00:00
- **Authors**: Chih-Hui Ho, Nuno Vasconcelos
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Contrastive learning (CL) is a popular technique for self-supervised learning (SSL) of visual representations. It uses pairs of augmentations of unlabeled training examples to define a classification task for pretext learning of a deep embedding. Despite extensive works in augmentation procedures, prior works do not address the selection of challenging negative pairs, as images within a sampled batch are treated independently. This paper addresses the problem, by introducing a new family of adversarial examples for constrastive learning and using these examples to define a new adversarial training algorithm for SSL, denoted as CLAE. When compared to standard CL, the use of adversarial examples creates more challenging positive pairs and adversarial training produces harder negative pairs by accounting for all images in a batch during the optimization. CLAE is compatible with many CL methods in the literature. Experiments show that it improves the performance of several existing CL baselines on multiple datasets.



### Zoom on the Keystrokes: Exploiting Video Calls for Keystroke Inference Attacks
- **Arxiv ID**: http://arxiv.org/abs/2010.12078v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2010.12078v1)
- **Published**: 2020-10-22 21:38:17+00:00
- **Updated**: 2020-10-22 21:38:17+00:00
- **Authors**: Mohd Sabra, Anindya Maiti, Murtuza Jadliwala
- **Comment**: None
- **Journal**: None
- **Summary**: Due to recent world events, video calls have become the new norm for both personal and professional remote communication. However, if a participant in a video call is not careful, he/she can reveal his/her private information to others in the call. In this paper, we design and evaluate an attack framework to infer one type of such private information from the video stream of a call -- keystrokes, i.e., text typed during the call. We evaluate our video-based keystroke inference framework using different experimental settings and parameters, including different webcams, video resolutions, keyboards, clothing, and backgrounds. Our relatively high keystroke inference accuracies under commonly occurring and realistic settings highlight the need for awareness and countermeasures against such attacks. Consequently, we also propose and evaluate effective mitigation techniques that can automatically protect users when they type during a video call.



### Language-Conditioned Imitation Learning for Robot Manipulation Tasks
- **Arxiv ID**: http://arxiv.org/abs/2010.12083v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12083v1)
- **Published**: 2020-10-22 21:49:08+00:00
- **Updated**: 2020-10-22 21:49:08+00:00
- **Authors**: Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, Heni Ben Amor
- **Comment**: Accepted to the 34th Conference on Neural Information Processing
  Systems (NeurIPS 2020), Vancouver, Canada as spotlight presentation
- **Journal**: None
- **Summary**: Imitation learning is a popular approach for teaching motor skills to robots. However, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., "go to the large green bowl"). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more fine-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods.



### Few-shot Image Recognition with Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2010.12084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12084v1)
- **Published**: 2020-10-22 21:57:27+00:00
- **Updated**: 2020-10-22 21:57:27+00:00
- **Authors**: Debasmit Das, J. H. Moon, C. S. George Lee
- **Comment**: International Symposium on Visual Computing (ISVC), 2020
- **Journal**: None
- **Summary**: In this paper, we extend the traditional few-shot learning (FSL) problem to the situation when the source-domain data is not accessible but only high-level information in the form of class prototypes is available. This limited information setup for the FSL problem deserves much attention due to its implication of privacy-preserving inaccessibility to the source-domain data but it has rarely been addressed before. Because of limited training data, we propose a non-parametric approach to this FSL problem by assuming that all the class prototypes are structurally arranged on a manifold. Accordingly, we estimate the novel-class prototype locations by projecting the few-shot samples onto the average of the subspaces on which the surrounding classes lie. During classification, we again exploit the structural arrangement of the categories by inducing a Markov chain on the graph constructed with the class prototypes. This manifold distance obtained using the Markov chain is expected to produce better results compared to a traditional nearest-neighbor-based Euclidean distance. To evaluate our proposed framework, we have tested it on two image datasets - the large-scale ImageNet and the small-scale but fine-grained CUB-200. We have also studied parameter sensitivity to better understand our framework.



### Towards falsifiable interpretability research
- **Arxiv ID**: http://arxiv.org/abs/2010.12016v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.12016v1)
- **Published**: 2020-10-22 22:03:41+00:00
- **Updated**: 2020-10-22 22:03:41+00:00
- **Authors**: Matthew L. Leavitt, Ari Morcos
- **Comment**: None
- **Journal**: None
- **Summary**: Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are "important" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.



### Flame Stability Analysis of Flame Spray Pyrolysis by Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2011.08673v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2011.08673v1)
- **Published**: 2020-10-22 22:52:13+00:00
- **Updated**: 2020-10-22 22:52:13+00:00
- **Authors**: Jessica Pan, Joseph A. Libera, Noah H. Paulson, Marius Stan
- **Comment**: 25 pages, 8 figures. International Journal of Advanced Manufacturing
  Technology 2020
- **Journal**: None
- **Summary**: Flame spray pyrolysis (FSP) is a process used to synthesize nanoparticles through the combustion of an atomized precursor solution; this process has applications in catalysts, battery materials, and pigments. Current limitations revolve around understanding how to consistently achieve a stable flame and the reliable production of nanoparticles. Machine learning and artificial intelligence algorithms that detect unstable flame conditions in real time may be a means of streamlining the synthesis process and improving FSP efficiency. In this study, the FSP flame stability is first quantified by analyzing the brightness of the flame's anchor point. This analysis is then used to label data for both unsupervised and supervised machine learning approaches. The unsupervised learning approach allows for autonomous labelling and classification of new data by representing data in a reduced dimensional space and identifying combinations of features that most effectively cluster it. The supervised learning approach, on the other hand, requires human labeling of training and test data, but is able to classify multiple objects of interest (such as the burner and pilot flames) within the video feed. The accuracy of each of these techniques is compared against the evaluations of human experts. Both the unsupervised and supervised approaches can track and classify FSP flame conditions in real time to alert users of unstable flame conditions. This research has the potential to autonomously track and manage flame spray pyrolysis as well as other flame technologies by monitoring and classifying the flame stability.



### Zero-Shot Learning from scratch (ZFS): leveraging local compositional representations
- **Arxiv ID**: http://arxiv.org/abs/2010.13320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13320v1)
- **Published**: 2020-10-22 23:11:18+00:00
- **Updated**: 2020-10-22 23:11:18+00:00
- **Authors**: Tristan Sylvain, Linda Petrini, R Devon Hjelm
- **Comment**: ICML 2019 Workshop on Understanding and Improving General-ization in
  Deep Learning, Long Beach, California, 2019 Spotlight presentation. arXiv
  admin note: text overlap with arXiv:1912.12179
- **Journal**: None
- **Summary**: Zero-shot classification is a generalization task where no instance from the target classes is seen during training. To allow for test-time transfer, each class is annotated with semantic information, commonly in the form of attributes or text descriptions. While classical zero-shot learning does not explicitly forbid using information from other datasets, the approaches that achieve the best absolute performance on image benchmarks rely on features extracted from encoders pretrained on Imagenet. This approach relies on hyper-optimized Imagenet-relevant parameters from the supervised classification setting, entangling important questions about the suitability of those parameters and how they were learned with more fundamental questions about representation learning and generalization. To remove these distractors, we propose a more challenging setting: Zero-Shot Learning from scratch (ZFS), which explicitly forbids the use of encoders fine-tuned on other datasets. Our analysis on this setting highlights the importance of local information, and compositional representations.



### GPS-Denied Navigation Using SAR Images and Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.12108v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12108v1)
- **Published**: 2020-10-22 23:25:43+00:00
- **Updated**: 2020-10-22 23:25:43+00:00
- **Authors**: Teresa White, Jesse Wheeler, Colton Lindstrom, Randall Christensen, Kevin R. Moon
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAV) often rely on GPS for navigation. GPS signals, however, are very low in power and easily jammed or otherwise disrupted. This paper presents a method for determining the navigation errors present at the beginning of a GPS-denied period utilizing data from a synthetic aperture radar (SAR) system. This is accomplished by comparing an online-generated SAR image with a reference image obtained a priori. The distortions relative to the reference image are learned and exploited with a convolutional neural network to recover the initial navigational errors, which can be used to recover the true flight trajectory throughout the synthetic aperture. The proposed neural network approach is able to learn to predict the initial errors on both simulated and real SAR image data.



### Tensor Reordering for CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/2010.12110v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12110v1)
- **Published**: 2020-10-22 23:45:34+00:00
- **Updated**: 2020-10-22 23:45:34+00:00
- **Authors**: Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot
- **Comment**: None
- **Journal**: None
- **Summary**: We show how parameter redundancy in Convolutional Neural Network (CNN) filters can be effectively reduced by pruning in spectral domain. Specifically, the representation extracted via Discrete Cosine Transform (DCT) is more conducive for pruning than the original space. By relying on a combination of weight tensor reshaping and reordering we achieve high levels of layer compression with just minor accuracy loss. Our approach is applied to compress pretrained CNNs and we show that minor additional fine-tuning allows our method to recover the original model performance after a significant parameter reduction. We validate our approach on ResNet-50 and MobileNet-V2 architectures for ImageNet classification task.




# Arxiv Papers in cs.CV on 2020-10-24
### Real-time Non-line-of-Sight imaging of dynamic scenes
- **Arxiv ID**: http://arxiv.org/abs/2010.12737v1
- **DOI**: 10.1038/s41467-021-26721-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12737v1)
- **Published**: 2020-10-24 01:40:06+00:00
- **Updated**: 2020-10-24 01:40:06+00:00
- **Authors**: Ji Hyun Nam, Eric Brandt, Sebastian Bauer, Xiaochun Liu, Eftychios Sifakis, Andreas Velten
- **Comment**: None
- **Journal**: Nature Communications 12, 6526 (2021)
- **Summary**: Non-Line-of-Sight (NLOS) imaging aims at recovering the 3D geometry of objects that are hidden from the direct line of sight. In the past, this method has suffered from the weak available multibounce signal limiting scene size, capture speed, and reconstruction quality. While algorithms capable of reconstructing scenes at several frames per second have been demonstrated, real-time NLOS video has only been demonstrated for retro-reflective objects where the NLOS signal strength is enhanced by 4 orders of magnitude or more. Furthermore, it has also been noted that the signal-to-noise ratio of reconstructions in NLOS methods drops quickly with distance and past reconstructions, therefore, have been limited to small scenes with depths of few meters. Actual models of noise and resolution in the scene have been simplistic, ignoring many of the complexities of the problem. We show that SPAD (Single-Photon Avalanche Diode) array detectors with a total of just 28 pixels combined with a specifically extended Phasor Field reconstruction algorithm can reconstruct live real-time videos of non-retro-reflective NLOS scenes. We provide an analysis of the Signal-to-Noise-Ratio (SNR) of our reconstructions and show that for our method it is possible to reconstruct the scene such that SNR, motion blur, angular resolution, and depth resolution are all independent of scene size suggesting that reconstruction of very large scenes may be possible. In the future, the light efficiency for NLOS imaging systems can be improved further by adding more pixels to the sensor array.



### Modular Networks for Compositional Instruction Following
- **Arxiv ID**: http://arxiv.org/abs/2010.12764v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12764v2)
- **Published**: 2020-10-24 03:48:45+00:00
- **Updated**: 2021-04-13 05:34:01+00:00
- **Authors**: Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, Trevor Darrell
- **Comment**: Published in NAACL-2021
- **Journal**: None
- **Summary**: Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.



### Improving the generalization of network based relative pose regression: dimension reduction as a regularizer
- **Arxiv ID**: http://arxiv.org/abs/2010.12796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.12796v1)
- **Published**: 2020-10-24 06:20:46+00:00
- **Updated**: 2020-10-24 06:20:46+00:00
- **Authors**: Xiaqing Ding, Yue Wang, Li Tang, Yanmei Jiao, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization occupies an important position in many areas such as Augmented Reality, robotics and 3D reconstruction. The state-of-the-art visual localization methods perform pose estimation using geometry based solver within the RANSAC framework. However, these methods require accurate pixel-level matching at high image resolution, which is hard to satisfy under significant changes from appearance, dynamics or perspective of view. End-to-end learning based regression networks provide a solution to circumvent the requirement for precise pixel-level correspondences, but demonstrate poor performance towards cross-scene generalization. In this paper, we explicitly add a learnable matching layer within the network to isolate the pose regression solver from the absolute image feature values, and apply dimension regularization on both the correlation feature channel and the image scale to further improve performance towards generalization and large viewpoint change. We implement this dimension regularization strategy within a two-layer pyramid based framework to regress the localization results from coarse to fine. In addition, the depth information is fused for absolute translational scale recovery. Through experiments on real world RGBD datasets we validate the effectiveness of our design in terms of improving both generalization performance and robustness towards viewpoint change, and also show the potential of regression based visual localization networks towards challenging occasions that are difficult for geometry based visual localization methods.



### REDE: End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination
- **Arxiv ID**: http://arxiv.org/abs/2010.12807v3
- **DOI**: 10.1109/LRA.2021.3062304
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12807v3)
- **Published**: 2020-10-24 06:45:39+00:00
- **Updated**: 2021-02-24 13:38:26+00:00
- **Authors**: Weitong Hua, Zhongxiang Zhou, Jun Wu, Huang Huang, Yue Wang, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Object 6D pose estimation is a fundamental task in many applications. Conventional methods solve the task by detecting and matching the keypoints, then estimating the pose. Recent efforts bringing deep learning into the problem mainly overcome the vulnerability of conventional methods to environmental variation due to the hand-crafted feature design. However, these methods cannot achieve end-to-end learning and good interpretability at the same time. In this paper, we propose REDE, a novel end-to-end object pose estimator using RGB-D data, which utilizes network for keypoint regression, and a differentiable geometric pose estimator for pose error back-propagation. Besides, to achieve better robustness when outlier keypoint prediction occurs, we further propose a differentiable outliers elimination method that regresses the candidate result and the confidence simultaneously. Via confidence weighted aggregation of multiple candidates, we can reduce the effect from the outliers in the final estimation. Finally, following the conventional method, we apply a learnable refinement process to further improve the estimation. The experimental results on three benchmark datasets show that REDE slightly outperforms the state-of-the-art approaches and is more robust to object occlusion.



### Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions
- **Arxiv ID**: http://arxiv.org/abs/2010.12831v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12831v2)
- **Published**: 2020-10-24 08:17:54+00:00
- **Updated**: 2021-04-11 23:54:25+00:00
- **Authors**: Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu Chang, Kai-Wei Chang
- **Comment**: NAACL 2021 Camera Ready
- **Journal**: None
- **Summary**: Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct ``mask-and-predict'' pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&L pre-training, while significantly reducing the amount of supervision needed for V&L models.



### Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions
- **Arxiv ID**: http://arxiv.org/abs/2010.12852v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.12852v2)
- **Published**: 2020-10-24 09:44:50+00:00
- **Updated**: 2021-06-17 09:44:12+00:00
- **Authors**: Radhika Dua, Sai Srinivas Kancheti, Vineeth N Balasubramanian
- **Comment**: MULA Workshop, CVPR 2021
- **Journal**: None
- **Summary**: Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer. We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test.



### Tissue characterization based on the analysis on i3DUS data for diagnosis support in neurosurgery
- **Arxiv ID**: http://arxiv.org/abs/2011.08129v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08129v1)
- **Published**: 2020-10-24 10:44:49+00:00
- **Updated**: 2020-10-24 10:44:49+00:00
- **Authors**: Mou-Cheng Xu
- **Comment**: MRes thesis
- **Journal**: None
- **Summary**: Brain shift makes the pre-operative MRI navigation highly inaccurate hence the intraoperative modalities are adopted in surgical theatre. Due to the excellent economic and portability merits, the Ultrasound imaging is used at our collaborating hospital, Charing Cross Hospital, Imperial College London, UK. However, it is found that intraoperative diagnosis on Ultrasound images is not straightforward and consistent, even for very experienced clinical experts. Hence, there is a demand to design a Computer-aided-diagnosis system to provide a robust second opinion to help the surgeons. The proposed CAD system based on "Mixed-Attention Res-U-net with asymmetric loss function" achieves the state-of-the-art results comparing to the ground truth by classification at pixel-level directly, it also outperforms all the current main stream pixel-level classification methods (e.g. U-net, FCN) in all the evaluation metrices.



### Persian Handwritten Digit, Character and Word Recognition Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.12880v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12880v2)
- **Published**: 2020-10-24 11:42:28+00:00
- **Updated**: 2020-11-14 06:20:53+00:00
- **Authors**: Mehdi Bonyani, Simindokht Jahangard, Morteza Daneshmand
- **Comment**: None
- **Journal**: None
- **Summary**: Digit, letter and word recognition for a particular script has various applications in todays commercial contexts. Nevertheless, only a limited number of relevant studies have dealt with Persian scripts. In this paper, deep neural networks are utilized through various DensNet architectures, as well as the Xception, are adopted, modified and further boosted through data augmentation and test time augmentation, in order to come up with an optical character recognition accounting for the particularities of the Persian language and the corresponding handwritings. Taking advantage of dividing the databases to training, validation and test sets, as well as k-fold cross validation, the comparison of the proposed method with various state-of-the-art alternatives is performed on the basis of the HODA and Sadri databases, which offer the most comprehensive collection of samples in terms of the various handwriting styles possessed by different human beings, as well as different forms each letter may take, which depend on its position within a word. On the HODA database, we achieve recognition rates of 99.72% and 89.99% for digits and characters, being 99.72%, 98.32% and 98.82% for digits, characters and words from the Sadri database, respectively.



### Discriminative feature generation for classification of imbalanced data
- **Arxiv ID**: http://arxiv.org/abs/2010.12888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12888v1)
- **Published**: 2020-10-24 12:19:05+00:00
- **Updated**: 2020-10-24 12:19:05+00:00
- **Authors**: Sungho Suh, Paul Lukowicz, Yong Oh Lee
- **Comment**: Submitted to Pattern Recognition, Elsevier
- **Journal**: None
- **Summary**: The data imbalance problem is a frequent bottleneck in the classification performance of neural networks. In this paper, we propose a novel supervised discriminative feature generation (DFG) method for a minority class dataset. DFG is based on the modified structure of a generative adversarial network consisting of four independent networks: generator, discriminator, feature extractor, and classifier. To augment the selected discriminative features of the minority class data by adopting an attention mechanism, the generator for the class-imbalanced target task is trained, and the feature extractor and classifier are regularized using the pre-trained features from a large source data. The experimental results show that the DFG generator enhances the augmentation of the label-preserved and diverse features, and the classification results are significantly improved on the target task. The feature generation model can contribute greatly to the development of data augmentation methods through discriminative feature generation and supervised attention methods.



### Classifying Eye-Tracking Data Using Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/2010.12913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12913v1)
- **Published**: 2020-10-24 15:18:07+00:00
- **Updated**: 2020-10-24 15:18:07+00:00
- **Authors**: Shafin Rahman, Sejuti Rahman, Omar Shahid, Md. Tahmeed Abdullah, Jubair Ahmed Sourov
- **Comment**: Accepted in: International Conference on Pattern Recognition (ICPR)
- **Journal**: None
- **Summary**: A plethora of research in the literature shows how human eye fixation pattern varies depending on different factors, including genetics, age, social functioning, cognitive functioning, and so on. Analysis of these variations in visual attention has already elicited two potential research avenues: 1) determining the physiological or psychological state of the subject and 2) predicting the tasks associated with the act of viewing from the recorded eye-fixation data. To this end, this paper proposes a visual saliency based novel feature extraction method for automatic and quantitative classification of eye-tracking data, which is applicable to both of the research directions. Instead of directly extracting features from the fixation data, this method employs several well-known computational models of visual attention to predict eye fixation locations as saliency maps. Comparing the saliency amplitudes, similarity and dissimilarity of saliency maps with the corresponding eye fixations maps gives an extra dimension of information which is effectively utilized to generate discriminative features to classify the eye-tracking data. Extensive experimentation using Saliency4ASD, Age Prediction, and Visual Perceptual Task dataset show that our saliency-based feature can achieve superior performance, outperforming the previous state-of-the-art methods by a considerable margin. Moreover, unlike the existing application-specific solutions, our method demonstrates performance improvement across three distinct problems from the real-life domain: Autism Spectrum Disorder screening, toddler age prediction, and human visual perceptual task classification, providing a general paradigm that utilizes the extra-information inherent in saliency maps for a more accurate classification.



### RUArt: A Novel Text-Centered Solution for Text-Based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2010.12917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.12917v1)
- **Published**: 2020-10-24 15:37:09+00:00
- **Updated**: 2020-10-24 15:37:09+00:00
- **Authors**: Zan-Xia Jin, Heran Wu, Chun Yang, Fang Zhou, Jingyan Qin, Lei Xiao, Xu-Cheng Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Text-based visual question answering (VQA) requires to read and understand text in an image to correctly answer a given question. However, most current methods simply add optical character recognition (OCR) tokens extracted from the image into the VQA model without considering contextual information of OCR tokens and mining the relationships between OCR tokens and scene objects. In this paper, we propose a novel text-centered method called RUArt (Reading, Understanding and Answering the Related Text) for text-based VQA. Taking an image and a question as input, RUArt first reads the image and obtains text and scene objects. Then, it understands the question, OCRed text and objects in the context of the scene, and further mines the relationships among them. Finally, it answers the related text for the given question through text semantic matching and reasoning. We evaluate our RUArt on two text-based VQA benchmarks (ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the reasons behind RUArt's effectiveness. Experimental results demonstrate that our method can effectively explore the contextual information of the text and mine the stable relationships between the text and objects.



### Non-local Meets Global: An Iterative Paradigm for Hyperspectral Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2010.12921v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12921v1)
- **Published**: 2020-10-24 15:53:56+00:00
- **Updated**: 2020-10-24 15:53:56+00:00
- **Authors**: Wei He, Quanming Yao, Chao Li, Naoto Yokoya, Qibin Zhao, Hongyan Zhang, Liangpei Zhang
- **Comment**: Accepted to TPAMI
- **Journal**: None
- **Summary**: Non-local low-rank tensor approximation has been developed as a state-of-the-art method for hyperspectral image (HSI) restoration, which includes the tasks of denoising, compressed HSI reconstruction and inpainting. Unfortunately, while its restoration performance benefits from more spectral bands, its runtime also substantially increases. In this paper, we claim that the HSI lies in a global spectral low-rank subspace, and the spectral subspaces of each full band patch group should lie in this global low-rank subspace. This motivates us to propose a unified paradigm combining the spatial and spectral properties for HSI restoration. The proposed paradigm enjoys performance superiority from the non-local spatial denoising and light computation complexity from the low-rank orthogonal basis exploration. An efficient alternating minimization algorithm with rank adaptation is developed. It is done by first solving a fidelity term-related problem for the update of a latent input image, and then learning a low-dimensional orthogonal basis and the related reduced image from the latent input image. Subsequently, non-local low-rank denoising is developed to refine the reduced image and orthogonal basis iteratively. Finally, the experiments on HSI denoising, compressed reconstruction, and inpainting tasks, with both simulated and real datasets, demonstrate its superiority with respect to state-of-the-art HSI restoration methods.



### LagNetViP: A Lagrangian Neural Network for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2010.12932v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12932v1)
- **Published**: 2020-10-24 16:50:14+00:00
- **Updated**: 2020-10-24 16:50:14+00:00
- **Authors**: Christine Allen-Blanchette, Sushant Veer, Anirudha Majumdar, Naomi Ehrich Leonard
- **Comment**: None
- **Journal**: None
- **Summary**: The dominant paradigms for video prediction rely on opaque transition models where neither the equations of motion nor the underlying physical quantities of the system are easily inferred. The equations of motion, as defined by Newton's second law, describe the time evolution of a physical system state and can therefore be applied toward the determination of future system states. In this paper, we introduce a video prediction model where the equations of motion are explicitly constructed from learned representations of the underlying physical quantities. To achieve this, we simultaneously learn a low-dimensional state representation and system Lagrangian. The kinetic and potential energy terms of the Lagrangian are distinctly modelled and the low-dimensional equations of motion are explicitly constructed using the Euler-Lagrange equations. We demonstrate the efficacy of this approach for video prediction on image sequences rendered in modified OpenAI gym Pendulum-v0 and Acrobot environments.



### Advancing Non-Contact Vital Sign Measurement using Synthetic Avatars
- **Arxiv ID**: http://arxiv.org/abs/2010.12949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.12949v1)
- **Published**: 2020-10-24 18:31:57+00:00
- **Updated**: 2020-10-24 18:31:57+00:00
- **Authors**: Daniel McDuff, Javier Hernandez, Erroll Wood, Xin Liu, Tadas Baltrusaitis
- **Comment**: None
- **Journal**: None
- **Summary**: Non-contact physiological measurement has the potential to provide low-cost, non-invasive health monitoring. However, machine vision approaches are often limited by the availability and diversity of annotated video datasets resulting in poor generalization to complex real-life conditions. To address these challenges, this work proposes the use of synthetic avatars that display facial blood flow changes and allow for systematic generation of samples under a wide variety of conditions. Our results show that training on both simulated and real video data can lead to performance gains under challenging conditions. We show state-of-the-art performance on three large benchmark datasets and improved robustness to skin type and motion.



### Automated triage of COVID-19 from various lung abnormalities using chest CT features
- **Arxiv ID**: http://arxiv.org/abs/2010.12967v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12967v1)
- **Published**: 2020-10-24 19:44:48+00:00
- **Updated**: 2020-10-24 19:44:48+00:00
- **Authors**: Dor Amran, Maayan Frid-Adar, Nimrod Sagie, Jannette Nassar, Asher Kabakovitch, Hayit Greenspan
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: The outbreak of COVID-19 has lead to a global effort to decelerate the pandemic spread. For this purpose chest computed-tomography (CT) based screening and diagnosis of COVID-19 suspected patients is utilized, either as a support or replacement to reverse transcription-polymerase chain reaction (RT-PCR) test. In this paper, we propose a fully automated AI based system that takes as input chest CT scans and triages COVID-19 cases. More specifically, we produce multiple descriptive features, including lung and infections statistics, texture, shape and location, to train a machine learning based classifier that distinguishes between COVID-19 and other lung abnormalities (including community acquired pneumonia). We evaluated our system on a dataset of 2191 CT cases and demonstrated a robust solution with 90.8% sensitivity at 85.4% specificity with 94.0% ROC-AUC. In addition, we present an elaborated feature analysis and ablation study to explore the importance of each feature.



### Improved Actor Relation Graph based Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.12968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.12968v2)
- **Published**: 2020-10-24 19:46:49+00:00
- **Updated**: 2020-12-29 16:56:54+00:00
- **Authors**: Zijian Kuang, Xinran Tie
- **Comment**: None
- **Journal**: None
- **Summary**: Video understanding is to recognize and classify different actions or activities appearing in the video. A lot of previous work, such as video captioning, has shown promising performance in producing general video understanding. However, it is still challenging to generate a fine-grained description of human actions and their interactions using state-of-the-art video captioning techniques. The detailed description of human actions and group activities is essential information, which can be used in real-time CCTV video surveillance, health care, sports video analysis, etc. This study proposes a video understanding method that mainly focused on group activity recognition by learning the pair-wise actor appearance similarity and actor positions. We propose to use Normalized cross-correlation (NCC) and the sum of absolute differences (SAD) to calculate the pair-wise appearance similarity and build the actor relationship graph to allow the graph convolution network to learn how to classify group activities. We also propose to use MobileNet as the backbone to extract features from each video frame. A visualization model is further introduced to visualize each input video frame with predicted bounding boxes on each human object and predict individual action and collective activity.



### Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.12970v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.12970v2)
- **Published**: 2020-10-24 19:59:28+00:00
- **Updated**: 2021-07-13 15:12:33+00:00
- **Authors**: Sreyas Mohan, Ramon Manzorro, Joshua L. Vincent, Binh Tang, Dev Yashpal Sheth, Eero P. Simoncelli, David S. Matteson, Peter A. Crozier, Carlos Fernandez-Granda
- **Comment**: The dataset and the code used to train and evaluate and our models
  are available at
  https://sreyas-mohan.github.io/electron-microscopy-denoising/
- **Journal**: None
- **Summary**: Denoising is a fundamental challenge in scientific imaging. Deep convolutional neural networks (CNNs) provide the current state of the art in denoising natural images, where they produce impressive results. However, their potential has barely been explored in the context of scientific imaging. Denoising CNNs are typically trained on real natural images artificially corrupted with simulated noise. In contrast, in scientific applications, noiseless ground-truth images are usually not available. To address this issue, we propose a simulation-based denoising (SBD) framework, in which CNNs are trained on simulated images. We test the framework on data obtained from transmission electron microscopy (TEM), an imaging technique with widespread applications in material science, biology, and medicine. SBD outperforms existing techniques by a wide margin on a simulated benchmark dataset, as well as on real data. Apart from the denoised images, SBD generates likelihood maps to visualize the agreement between the structure of the denoised image and the observed data. Our results reveal shortcomings of state-of-the-art denoising architectures, such as their small field-of-view: substantially increasing the field-of-view of the CNNs allows them to exploit non-local periodic patterns in the data, which is crucial at high noise levels. In addition, we analyze the generalization capability of SBD, demonstrating that the trained networks are robust to variations of imaging parameters and of the underlying signal structure. Finally, we release the first publicly available benchmark dataset of TEM images, containing 18,000 examples.



### Classification of Spot-welded Joints in Laser Thermography Data using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.12976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.12976v1)
- **Published**: 2020-10-24 20:38:12+00:00
- **Updated**: 2020-10-24 20:38:12+00:00
- **Authors**: Linh KÃ¤stner, Samim Ahmadi, Florian Jonietz, Mathias Ziegler, Peter Jung, Giuseppe Caire, Jens Lambrecht
- **Comment**: 9 pages,11 figures
- **Journal**: None
- **Summary**: Spot welding is a crucial process step in various industries. However, classification of spot welding quality is still a tedious process due to the complexity and sensitivity of the test material, which drain conventional approaches to its limits. In this paper, we propose an approach for quality inspection of spot weldings using images from laser thermography data.We propose data preparation approaches based on the underlying physics of spot welded joints, heated with pulsed laser thermography by analyzing the intensity over time and derive dedicated data filters to generate training datasets. Subsequently, we utilize convolutional neural networks to classify weld quality and compare the performance of different models against each other. We achieve competitive results in terms of classifying the different welding quality classes compared to traditional approaches, reaching an accuracy of more than 95 percent. Finally, we explore the effect of different augmentation methods.




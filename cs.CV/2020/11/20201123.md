# Arxiv Papers in cs.CV on 2020-11-23
### Imbalance Robust Softmax for Deep Embeeding Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.11155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11155v1)
- **Published**: 2020-11-23 00:43:07+00:00
- **Updated**: 2020-11-23 00:43:07+00:00
- **Authors**: Hao Zhu, Yang Yuan, Guosheng Hu, Xiang Wu, Neil Robertson
- **Comment**: has been accepted by ACCV 2020
- **Journal**: None
- **Summary**: Deep embedding learning is expected to learn a metric space in which features have smaller maximal intra-class distance than minimal inter-class distance. In recent years, one research focus is to solve the open-set problem by discriminative deep embedding learning in the field of face recognition (FR) and person re-identification (re-ID). Apart from open-set problem, we find that imbalanced training data is another main factor causing the performance degradation of FR and re-ID, and data imbalance widely exists in the real applications. However, very little research explores why and how data imbalance influences the performance of FR and re-ID with softmax or its variants. In this work, we deeply investigate data imbalance in the perspective of neural network optimisation and feature distribution about softmax. We find one main reason of performance degradation caused by data imbalance is that the weights (from the penultimate fully-connected layer) are far from their class centers in feature space. Based on this investigation, we propose a unified framework, Imbalance-Robust Softmax (IR-Softmax), which can simultaneously solve the open-set problem and reduce the influence of data imbalance. IR-Softmax can generalise to any softmax and its variants (which are discriminative for open-set problem) by directly setting the weights as their class centers, naturally solving the data imbalance problem. In this work, we explicitly re-formulate two discriminative softmax (A-Softmax and AM-Softmax) under the framework of IR-Softmax. We conduct extensive experiments on FR databases (LFW, MegaFace) and re-ID database (Market-1501, Duke), and IR-Softmax outperforms many state-of-the-art methods.



### Better Aggregation in Test-Time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11156v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11156v2)
- **Published**: 2020-11-23 00:46:00+00:00
- **Updated**: 2021-10-11 19:58:48+00:00
- **Authors**: Divya Shanmugam, Davis Blalock, Guha Balakrishnan, John Guttag
- **Comment**: None
- **Journal**: ICCV 2021
- **Summary**: Test-time augmentation -- the aggregation of predictions across transformed versions of a test input -- is a common practice in image classification. Traditionally, predictions are combined using a simple average. In this paper, we present 1) experimental analyses that shed light on cases in which the simple average is suboptimal and 2) a method to address these shortcomings. A key finding is that even when test-time augmentation produces a net improvement in accuracy, it can change many correct predictions into incorrect predictions. We delve into when and why test-time augmentation changes a prediction from being correct to incorrect and vice versa. Building on these insights, we present a learning-based method for aggregating test-time augmentations. Experiments across a diverse set of models, datasets, and augmentations show that our method delivers consistent improvements over existing approaches.



### Learnable Boundary Guided Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2011.11164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11164v2)
- **Published**: 2020-11-23 01:36:05+00:00
- **Updated**: 2021-08-16 04:40:26+00:00
- **Authors**: Jiequan Cui, Shu Liu, Liwei Wang, Jiaya Jia
- **Comment**: ICCV2021
- **Journal**: None
- **Summary**: Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, we reduce natural accuracy degradation. We use the model logits from one clean model to guide learning of another one robust model, taking into consideration that logits from the well trained clean model embed the most discriminative features of natural data, {\it e.g.}, generalizable classifier boundary. Our solution is to constrain logits from the robust model that takes adversarial examples as input and makes it similar to those from the clean model fed with corresponding natural data. It lets the robust model inherit the classifier boundary of the clean model. Moreover, we observe such boundary guidance can not only preserve high natural accuracy but also benefit model robustness, which gives new insights and facilitates progress for the adversarial community. Finally, extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet testify to the effectiveness of our method. We achieve new state-of-the-art robustness on CIFAR-100 without additional real or synthetic data with auto-attack benchmark \footnote{\url{https://github.com/fra31/auto-attack}}. Our code is available at \url{https://github.com/dvlab-research/LBGAT}.



### The Selectivity and Competition of the Mind's Eye in Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/2011.11167v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11167v2)
- **Published**: 2020-11-23 01:55:46+00:00
- **Updated**: 2021-03-23 15:43:27+00:00
- **Authors**: Edward Kim, Maryam Daniali, Jocelyn Rego, Garrett T. Kenyon
- **Comment**: 8 pages, under review
- **Journal**: None
- **Summary**: Research has shown that neurons within the brain are selective to certain stimuli. For example, the fusiform face area (FFA) region is known by neuroscientists to selectively activate when people see faces over non-face objects. However, the mechanisms by which the primary visual system directs information to the correct higher levels of the brain are currently unknown. In our work, we mimic several high-level neural mechanisms of perception by creating a novel computational model that incorporates lateral and top down feedback in the form of hierarchical competition. Not only do we show that these elements can help explain the information flow and selectivity of high level areas within the brain, we also demonstrate that these neural mechanisms provide the foundation of a novel classification framework that rivals traditional supervised learning in computer vision. Additionally, we present both quantitative and qualitative results that demonstrate that our generative framework is consistent with neurological themes and enables simple, yet robust category level classification.



### A Decade Survey of Content Based Image Retrieval using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.00641v2
- **DOI**: 10.1109/TCSVT.2021.3080920
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.00641v2)
- **Published**: 2020-11-23 02:12:30+00:00
- **Updated**: 2021-05-20 09:22:01+00:00
- **Authors**: Shiv Ram Dubey
- **Comment**: Published by IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: The content based image retrieval aims to find the similar images from a large scale dataset against a query image. Generally, the similarity between the representative features of the query image and dataset images is used to rank the images for retrieval. In early days, various hand designed feature descriptors have been investigated based on the visual cues such as color, texture, shape, etc. that represent the images. However, the deep learning has emerged as a dominating alternative of hand-designed feature engineering from a decade. It learns the features automatically from the data. This paper presents a comprehensive survey of deep learning based developments in the past decade for content based image retrieval. The categorization of existing state-of-the-art methods from different perspectives is also performed for greater understanding of the progress. The taxonomy used in this survey covers different supervision, different networks, different descriptor type and different retrieval type. A performance analysis is also performed using the state-of-the-art methods. The insights are also presented for the benefit of the researchers to observe the progress and to make the best choices. The survey presented in this paper will help in further research progress in image retrieval using deep learning.



### CoMatch: Semi-supervised Learning with Contrastive Graph Regularization
- **Arxiv ID**: http://arxiv.org/abs/2011.11183v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11183v2)
- **Published**: 2020-11-23 02:54:57+00:00
- **Updated**: 2021-03-03 01:58:15+00:00
- **Authors**: Junnan Li, Caiming Xiong, Steven Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning has been an effective paradigm for leveraging unlabeled data to reduce the reliance on labeled data. We propose CoMatch, a new semi-supervised learning method that unifies dominant approaches and addresses their limitations. CoMatch jointly learns two representations of the training data, their class probabilities and low-dimensional embeddings. The two representations interact with each other to jointly evolve. The embeddings impose a smoothness constraint on the class probabilities to improve the pseudo-labels, whereas the pseudo-labels regularize the structure of the embeddings through graph-based contrastive learning. CoMatch achieves state-of-the-art performance on multiple datasets. It achieves substantial accuracy improvements on the label-scarce CIFAR-10 and STL-10. On ImageNet with 1% labels, CoMatch achieves a top-1 accuracy of 66.0%, outperforming FixMatch by 12.6%. Furthermore, CoMatch achieves better representation learning performance on downstream tasks, outperforming both supervised learning and self-supervised learning. Code and pre-trained models are available at https://github.com/salesforce/CoMatch.



### Cancer image classification based on DenseNet model
- **Arxiv ID**: http://arxiv.org/abs/2011.11186v1
- **DOI**: 10.1088/1742-6596/1651/1/012143
- **Categories**: **cs.CV**, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.11186v1)
- **Published**: 2020-11-23 03:05:42+00:00
- **Updated**: 2020-11-23 03:05:42+00:00
- **Authors**: Ziliang Zhong, Muhang Zheng, Huafeng Mai, Jianan Zhao, Xinyi Liu
- **Comment**: None
- **Journal**: 2004-present Journal of Physics: Conference Series
- **Summary**: Computer-aided diagnosis establishes methods for robust assessment of medical image-based examination. Image processing introduced a promising strategy to facilitate disease classification and detection while diminishing unnecessary expenses. In this paper, we propose a novel metastatic cancer image classification model based on DenseNet Block, which can effectively identify metastatic cancer in small image patches taken from larger digital pathology scans. We evaluate the proposed approach to the slightly modified version of the PatchCamelyon (PCam) benchmark dataset. The dataset is the slightly modified version of the PatchCamelyon (PCam) benchmark dataset provided by Kaggle competition, which packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task. The experiments indicated that our model outperformed other classical methods like Resnet34, Vgg19. Moreover, we also conducted data augmentation experiment and study the relationship between Batches processed and loss value during the training and validation process.



### Attentional-GCNN: Adaptive Pedestrian Trajectory Prediction towards Generic Autonomous Vehicle Use Cases
- **Arxiv ID**: http://arxiv.org/abs/2011.11190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68T40
- **Links**: [PDF](http://arxiv.org/pdf/2011.11190v1)
- **Published**: 2020-11-23 03:13:26+00:00
- **Updated**: 2020-11-23 03:13:26+00:00
- **Authors**: Kunming Li, Stuart Eiffert, Mao Shan, Francisco Gomez-Donoso, Stewart Worrall, Eduardo Nebot
- **Comment**: 8 pages, 5 figures, submitted to ICRA 2021
- **Journal**: None
- **Summary**: Autonomous vehicle navigation in shared pedestrian environments requires the ability to predict future crowd motion both accurately and with minimal delay. Understanding the uncertainty of the prediction is also crucial. Most existing approaches however can only estimate uncertainty through repeated sampling of generative models. Additionally, most current predictive models are trained on datasets that assume complete observability of the crowd using an aerial view. These are generally not representative of real-world usage from a vehicle perspective, and can lead to the underestimation of uncertainty bounds when the on-board sensors are occluded. Inspired by prior work in motion prediction using spatio-temporal graphs, we propose a novel Graph Convolutional Neural Network (GCNN)-based approach, Attentional-GCNN, which aggregates information of implicit interaction between pedestrians in a crowd by assigning attention weight in edges of the graph. Our model can be trained to either output a probabilistic distribution or faster deterministic prediction, demonstrating applicability to autonomous vehicle use cases where either speed or accuracy with uncertainty bounds are required. To further improve the training of predictive models, we propose an automatically labelled pedestrian dataset collected from an intelligent vehicle platform representative of real-world use. Through experiments on a number of datasets, we show our proposed method achieves an improvement over the state of art by 10% Average Displacement Error (ADE) and 12% Final Displacement Error (FDE) with fast inference speeds.



### An off-the-grid approach to multi-compartment magnetic resonance fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/2011.11193v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2011.11193v1)
- **Published**: 2020-11-23 03:16:55+00:00
- **Updated**: 2020-11-23 03:16:55+00:00
- **Authors**: Mohammad Golbabaee, Clarice Poon
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel numerical approach to separate multiple tissue compartments in image voxels and to estimate quantitatively their nuclear magnetic resonance (NMR) properties and mixture fractions, given magnetic resonance fingerprinting (MRF) measurements. The number of tissues, their types or quantitative properties are not a-priori known, but the image is assumed to be composed of sparse compartments with linearly mixed Bloch magnetisation responses within voxels. Fine-grid discretisation of the multi-dimensional NMR properties creates large and highly coherent MRF dictionaries that can challenge scalability and precision of the numerical methods for (discrete) sparse approximation. To overcome these issues, we propose an off-the-grid approach equipped with an extended notion of the sparse group lasso regularisation for sparse approximation using continuous (non-discretised) Bloch response models. Further, the nonlinear and non-analytical Bloch responses are approximated by a neural network, enabling efficient back-propagation of the gradients through the proposed algorithm. Tested on simulated and in-vivo healthy brain MRF data, we demonstrate effectiveness of the proposed scheme compared to the baseline multicompartment MRF methods.



### V3H: View Variation and View Heredity for Incomplete Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/2011.11194v3
- **DOI**: 10.1109/TAI.2021.3052425
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.11194v3)
- **Published**: 2020-11-23 03:24:48+00:00
- **Updated**: 2021-04-30 08:34:39+00:00
- **Authors**: Xiang Fang, Yuchong Hu, Pan Zhou, Dapeng Oliver Wu
- **Comment**: Publisheded in IEEE Transactions on Artificial Intelligence
- **Journal**: IEEE Transactions on Artificial Intelligence 2020
- **Summary**: Real data often appear in the form of multiple incomplete views. Incomplete multi-view clustering is an effective method to integrate these incomplete views. Previous methods only learn the consistent information between different views and ignore the unique information of each view, which limits their clustering performance and generalizations. To overcome this limitation, we propose a novel View Variation and View Heredity approach (V3H). Inspired by the variation and the heredity in genetics, V3H first decomposes each subspace into a variation matrix for the corresponding view and a heredity matrix for all the views to represent the unique information and the consistent information respectively. Then, by aligning different views based on their cluster indicator matrices, V3H integrates the unique information from different views to improve the clustering performance. Finally, with the help of the adjustable low-rank representation based on the heredity matrix, V3H recovers the underlying true data structure to reduce the influence of the large incompleteness. More importantly, V3H presents possibly the first work to introduce genetics to clustering algorithms for learning simultaneously the consistent information and the unique information from incomplete multi-view data. Extensive experimental results on fifteen benchmark datasets validate its superiority over other state-of-the-arts.



### Complex-valued Iris Recognition Network
- **Arxiv ID**: http://arxiv.org/abs/2011.11198v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11198v4)
- **Published**: 2020-11-23 03:36:18+00:00
- **Updated**: 2022-02-16 04:17:35+00:00
- **Authors**: Kien Nguyen, Clinton Fookes, Sridha Sridharan, Arun Ross
- **Comment**: This paper has been accepted for publication in T-PAMI
- **Journal**: None
- **Summary**: In this work, we design a fully complex-valued neural network for the task of iris recognition. Unlike the problem of general object recognition, where real-valued neural networks can be used to extract pertinent features, iris recognition depends on the extraction of both phase and magnitude information from the input iris texture in order to better represent its biometric content. This necessitates the extraction and processing of phase information that cannot be effectively handled by a real-valued neural network. In this regard, we design a fully complex-valued neural network that can better capture the multi-scale, multi-resolution, and multi-orientation phase and amplitude features of the iris texture. We show a strong correspondence of the proposed complex-valued iris recognition network with Gabor wavelets that are used to generate the classical IrisCode; however, the proposed method enables a new capability of automatic complex-valued feature learning that is tailored for iris recognition. We conduct experiments on three benchmark datasets - ND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit of the proposed network for the task of iris recognition. We exploit visualization schemes to convey how the complex-valued network, when compared to standard real-valued networks, extracts fundamentally different features from the iris texture.



### Ranking Neural Checkpoints
- **Arxiv ID**: http://arxiv.org/abs/2011.11200v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11200v4)
- **Published**: 2020-11-23 04:05:46+00:00
- **Updated**: 2022-08-28 03:17:13+00:00
- **Authors**: Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang Wang, Boqing Gong
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: This paper is concerned with ranking many pre-trained deep neural networks (DNNs), called checkpoints, for the transfer learning to a downstream task. Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints from various sources. Which of them transfers the best to our downstream task of interest? Striving to answer this question thoroughly, we establish a neural checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking measures. These measures are generic, applying to the checkpoints of different output types without knowing how the checkpoints are pre-trained on which dataset. They also incur low computation cost, making them practically meaningful. Our results suggest that the linear separability of the features extracted by the checkpoints is a strong indicator of transferability. We also arrive at a new ranking measure, NLEEP, which gives rise to the best performance in the experiments.



### Modular Action Concept Grounding in Semantic Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.11201v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11201v4)
- **Published**: 2020-11-23 04:12:22+00:00
- **Updated**: 2022-04-26 13:31:26+00:00
- **Authors**: Wei Yu, Wenxin Chen, Songhenh Yin, Steve Easterbrook, Animesh Garg
- **Comment**: To appear in CVPR 2022
- **Journal**: None
- **Summary**: Recent works in video prediction have mainly focused on passive forecasting and low-level action-conditional prediction, which sidesteps the learning of interaction between agents and objects. We introduce the task of semantic action-conditional video prediction, which uses semantic action labels to describe those interactions and can be regarded as an inverse problem of action recognition. The challenge of this new task primarily lies in how to effectively inform the model of semantic action information. Inspired by the idea of Mixture of Experts, we embody each abstract label by a structured combination of various visual concept learners and propose a novel video prediction model, Modular Action Concept Network (MAC). Our method is evaluated on two newly designed synthetic datasets, CLEVR-Building-Blocks and Sapien-Kitchen, and one real-world dataset called Tower-Creation. Extensive experiments demonstrate that MAC can correctly condition on given instructions and generate corresponding future frames without need of bounding boxes. We further show that the trained model can make out-of-distribution generalization, be quickly adapted to new object categories and exploit its learnt features for object detection, showing the progression towards higher-level cognitive abilities. More visualizations can be found at http://www.pair.toronto.edu/mac/.



### Graph Attention Tracking
- **Arxiv ID**: http://arxiv.org/abs/2011.11204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11204v1)
- **Published**: 2020-11-23 04:26:45+00:00
- **Updated**: 2020-11-23 04:26:45+00:00
- **Authors**: Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua Wang, Liyan Zhang, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Siamese network based trackers formulate the visual tracking task as a similarity matching problem. Almost all popular Siamese trackers realize the similarity learning via convolutional feature cross-correlation between a target branch and a search branch. However, since the size of target feature region needs to be pre-fixed, these cross-correlation base methods suffer from either reserving much adverse background information or missing a great deal of foreground information. Moreover, the global matching between the target and search region also largely neglects the target structure and part-level information.   In this paper, to solve the above issues, we propose a simple target-aware Siamese graph attention network for general object tracking. We propose to establish part-to-part correspondence between the target and the search region with a complete bipartite graph, and apply the graph attention mechanism to propagate target information from the template feature to the search feature. Further, instead of using the pre-fixed region cropping for template-feature-area selection, we investigate a target-aware area selection mechanism to fit the size and aspect ratio variations of different objects. Experiments on challenging benchmarks including GOT-10k, UAV123, OTB-100 and LaSOT demonstrate that the proposed SiamGAT outperforms many state-of-the-art trackers and achieves leading performance. Code is available at: https://git.io/SiamGAT



### Structure-Aware Completion of Photogrammetric Meshes in Urban Road Environment
- **Arxiv ID**: http://arxiv.org/abs/2011.11210v3
- **DOI**: 10.1016/j.isprsjprs.2021.02.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11210v3)
- **Published**: 2020-11-23 05:04:28+00:00
- **Updated**: 2021-02-10 03:45:29+00:00
- **Authors**: Qing Zhu, Qisen Shang, Han Hu, Haojia Yu, Ruofei Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Photogrammetric mesh models obtained from aerial oblique images have been widely used for urban reconstruction. However, the photogrammetric meshes also suffer from severe texture problems, especially on the road areas due to occlusion. This paper proposes a structure-aware completion approach to improve the quality of meshes by removing undesired vehicles on the road seamlessly. Specifically, the discontinuous texture atlas is first integrated to a continuous screen space through rendering by the graphics pipeline; the rendering also records necessary mapping for deintegration to the original texture atlas after editing. Vehicle regions are masked by a standard object detection approach, e.g. Faster RCNN. Then, the masked regions are completed guided by the linear structures and regularities in the road region, which is implemented based on Patch Match. Finally, the completed rendered image is deintegrated to the original texture atlas and the triangles for the vehicles are also flattened for improved meshes. Experimental evaluations and analyses are conducted against three datasets, which are captured with different sensors and ground sample distances. The results reveal that the proposed method can quite realistic meshes after removing the vehicles. The structure-aware completion approach for road regions outperforms popular image completion methods and ablation study further confirms the effectiveness of the linear guidance. It should be noted that the proposed method is also capable to handle tiled mesh models for large-scale scenes. Dataset and code are available at vrlab.org.cn/~hanhu/projects/mesh.



### Adversarial Refinement Network for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.11221v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11221v2)
- **Published**: 2020-11-23 05:42:20+00:00
- **Updated**: 2020-11-24 02:16:10+00:00
- **Authors**: Xianjin Chao, Yanrui Bin, Wenqing Chu, Xuan Cao, Yanhao Ge, Chengjie Wang, Jilin Li, Feiyue Huang, Howard Leung
- **Comment**: Accepted by ACCV 2020(Oral)
- **Journal**: None
- **Summary**: Human motion prediction aims to predict future 3D skeletal sequences by giving a limited human motion as inputs. Two popular methods, recurrent neural networks and feed-forward deep networks, are able to predict rough motion trend, but motion details such as limb movement may be lost. To predict more accurate future human motion, we propose an Adversarial Refinement Network (ARNet) following a simple yet effective coarse-to-fine mechanism with novel adversarial error augmentation. Specifically, we take both the historical motion sequences and coarse prediction as input of our cascaded refinement network to predict refined human motion and strengthen the refinement network with adversarial error augmentation. During training, we deliberately introduce the error distribution by learning through the adversarial mechanism among different subjects. In testing, our cascaded refinement network alleviates the prediction error from the coarse predictor resulting in a finer prediction robustly. This adversarial error augmentation provides rich error cases as input to our refinement network, leading to better generalization performance on the testing dataset. We conduct extensive experiments on three standard benchmark datasets and show that our proposed ARNet outperforms other state-of-the-art methods, especially on challenging aperiodic actions in both short-term and long-term predictions.



### NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets
- **Arxiv ID**: http://arxiv.org/abs/2011.11232v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11232v5)
- **Published**: 2020-11-23 06:33:39+00:00
- **Updated**: 2022-04-19 06:04:38+00:00
- **Authors**: Gyeongsik Moon, Hongsuk Choi, Kyoung Mu Lee
- **Comment**: Published at CVPRW 2022
- **Journal**: None
- **Summary**: Most 3D human mesh regressors are fully supervised with 3D pseudo-GT human model parameters and weakly supervised with GT 2D/3D joint coordinates as the 3D pseudo-GTs bring great performance gain. The 3D pseudo-GTs are obtained by annotators, systems that iteratively fit 3D human model parameters to GT 2D/3D joint coordinates of training sets in the pre-processing stage of the regressors. The fitted 3D parameters at the last fitting iteration become the 3D pseudo-GTs, used to fully supervise the regressors. Optimization-based annotators, such as SMPLify-X, have been widely used to obtain the 3D pseudo-GTs. However, they often produce wrong 3D pseudo-GTs as they fit the 3D parameters to GT of each sample independently. To overcome the limitation, we present NeuralAnnot, a neural network-based annotator. The main idea of NeuralAnnot is to employ a neural network-based regressor and dedicate it for the annotation. Assuming no 3D pseudo-GTs are available, NeuralAnnot is weakly supervised with GT 2D/3D joint coordinates of training sets. The testing results on the same training sets become 3D pseudo-GTs, used to fully supervise the regressors. We show that 3D pseudo-GTs of NeuralAnnot are highly beneficial to train the regressors. We made our 3D pseudo-GTs publicly available.



### ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation
- **Arxiv ID**: http://arxiv.org/abs/2011.11233v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11233v2)
- **Published**: 2020-11-23 06:34:07+00:00
- **Updated**: 2023-08-03 01:44:49+00:00
- **Authors**: Xiaoxing Wang, Xiangxiang Chu, Yuda Fan, Zhexi Zhang, Bo Zhang, Xiaokang Yang, Junchi Yan
- **Comment**: ICCV2023
- **Journal**: None
- **Summary**: Albeit being a prevalent architecture searching approach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consistent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level optimization. We verify ROME extensively across 15 benchmarks to demonstrate its effectiveness and robustness.



### BiOpt: Bi-Level Optimization for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11245v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11245v1)
- **Published**: 2020-11-23 07:09:48+00:00
- **Updated**: 2020-11-23 07:09:48+00:00
- **Authors**: Jinlu Liu, Liang Song, Yongqiang Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation is a challenging task that aims to segment objects of new classes given scarce support images. In the inductive setting, existing prototype-based methods focus on extracting prototypes from the support images; however, they fail to utilize semantic information of the query images. In this paper, we propose Bi-level Optimization (BiOpt), which succeeds to compute class prototypes from the query images under inductive setting. The learning procedure of BiOpt is decomposed into two nested loops: inner and outer loop. On each task, the inner loop aims to learn optimized prototypes from the query images. An init step is conducted to fully exploit knowledge from both support and query features, so as to give reasonable initialized prototypes into the inner loop. The outer loop aims to learn a discriminative embedding space across different tasks. Extensive experiments on two benchmarks verify the superiority of our proposed BiOpt algorithm. In particular, we consistently achieve the state-of-the-art performance on 5-shot PASCAL-$5^i$ and 1-shot COCO-$20^i$.



### Application of Facial Recognition using Convolutional Neural Networks for Entry Access Control
- **Arxiv ID**: http://arxiv.org/abs/2011.11257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11257v1)
- **Published**: 2020-11-23 07:55:24+00:00
- **Updated**: 2020-11-23 07:55:24+00:00
- **Authors**: Lars Lien Ankile, Morgan Feet Heggland, Kjartan Krange
- **Comment**: 10 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: The purpose of this paper is to design a solution to the problem of facial recognition by use of convolutional neural networks, with the intention of applying the solution in a camera-based home-entry access control system. More specifically, the paper focuses on solving the supervised classification problem of taking images of people as input and classifying the person in the image as one of the authors or not. Two approaches are proposed: (1) building and training a neural network called WoodNet from scratch and (2) leveraging transfer learning by utilizing a network pre-trained on the ImageNet database and adapting it to this project's data and classes. In order to train the models to recognize the authors, a dataset containing more than 150 000 images has been created, balanced over the authors and others. Image extraction from videos and image augmentation techniques were instrumental for dataset creation. The results are two models classifying the individuals in the dataset with high accuracy, achieving over 99% accuracy on held-out test data. The pre-trained model fitted significantly faster than WoodNet, and seems to generalize better. However, these results come with a few caveats. Because of the way the dataset was compiled, as well as the high accuracy, one has reason to believe the models over-fitted to the data to some degree. An added consequence of the data compilation method is that the test dataset may not be sufficiently different from the training data, limiting its ability to validate generalization of the models. However, utilizing the models in a web-cam based system, classifying faces in real-time, shows promising results and indicates that the models generalized fairly well for at least some of the classes (see the accompanying video).



### 3D Registration for Self-Occluded Objects in Context
- **Arxiv ID**: http://arxiv.org/abs/2011.11260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11260v1)
- **Published**: 2020-11-23 08:05:28+00:00
- **Updated**: 2020-11-23 08:05:28+00:00
- **Authors**: Zheng Dang, Fei Wang, Mathieu Salzmann
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: While much progress has been made on the task of 3D point cloud registration, there still exists no learning-based method able to estimate the 6D pose of an object observed by a 2.5D sensor in a scene. The challenges of this scenario include the fact that most measurements are outliers depicting the object's surrounding context, and the mismatch between the complete 3D object model and its self-occluded observations.   We introduce the first deep learning framework capable of effectively handling this scenario. Our method consists of an instance segmentation module followed by a pose estimation one. It allows us to perform 3D registration in a one-shot manner, without requiring an expensive iterative procedure. We further develop an on-the-fly rendering-based training strategy that is both time- and memory-efficient. Our experiments evidence the superiority of our approach over the state-of-the-art traditional and learning-based 3D registration methods.



### Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.11261v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11261v2)
- **Published**: 2020-11-23 08:05:39+00:00
- **Updated**: 2021-08-31 20:46:37+00:00
- **Authors**: Zehua Zhang, David Crandall
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel technique for self-supervised video representation learning by: (a) decoupling the learning objective into two contrastive subtasks respectively emphasizing spatial and temporal features, and (b) performing it hierarchically to encourage multi-scale understanding. Motivated by their effectiveness in supervised learning, we first introduce spatial-temporal feature learning decoupling and hierarchical learning to the context of unsupervised video learning. We show by experiments that augmentations can be manipulated as regularization to guide the network to learn desired semantics in contrastive learning, and we propose a way for the model to separately capture spatial and temporal features at multiple scales. We also introduce an approach to overcome the problem of divergent levels of instance invariance at different hierarchies by modeling the invariance as loss weights for objective re-weighting. Experiments on downstream action recognition benchmarks on UCF101 and HMDB51 show that our proposed Hierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial improvements over directly learning spatial-temporal features as a whole and achieves competitive performance when compared with other state-of-the-art unsupervised methods. Code will be made available.



### MEG: Multi-Evidence GNN for Multimodal Semantic Forensics
- **Arxiv ID**: http://arxiv.org/abs/2011.11286v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11286v1)
- **Published**: 2020-11-23 09:01:28+00:00
- **Updated**: 2020-11-23 09:01:28+00:00
- **Authors**: Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan
- **Comment**: To be published at ICPR 2020
- **Journal**: None
- **Summary**: Fake news often involves semantic manipulations across modalities such as image, text, location etc and requires the development of multimodal semantic forensics for its detection. Recent research has centered the problem around images, calling it image repurposing -- where a digitally unmanipulated image is semantically misrepresented by means of its accompanying multimodal metadata such as captions, location, etc. The image and metadata together comprise a multimedia package. The problem setup requires algorithms to perform multimodal semantic forensics to authenticate a query multimedia package using a reference dataset of potentially related packages as evidences. Existing methods are limited to using a single evidence (retrieved package), which ignores potential performance improvement from the use of multiple evidences. In this work, we introduce a novel graph neural network based model for multimodal semantic forensics, which effectively utilizes multiple retrieved packages as evidences and is scalable with the number of evidences. We compare the scalability and performance of our model against existing methods. Experimental results show that the proposed model outperforms existing state-of-the-art algorithms with an error reduction of up to 25%.



### Industrial object, machine part and defect recognition towards fully automated industrial monitoring employing deep learning. The case of multilevel VGG19
- **Arxiv ID**: http://arxiv.org/abs/2011.11305v1
- **DOI**: 10.1007/s12652-021-03688-7
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11305v1)
- **Published**: 2020-11-23 10:05:50+00:00
- **Updated**: 2020-11-23 10:05:50+00:00
- **Authors**: Ioannis D. Apostolopoulos, Mpesiana Tzani
- **Comment**: 17 pages, 10 figures
- **Journal**: Journal of Ambient Intelligence and Humanized Computing, 2022
- **Summary**: Modern industry requires modern solutions for monitoring the automatic production of goods. Smart monitoring of the functionality of the mechanical parts of technology systems or machines is mandatory for a fully automatic production process. Although Deep Learning has been advancing, allowing for real-time object detection and other tasks, little has been investigated about the effectiveness of specially designed Convolutional Neural Networks for defect detection and industrial object recognition. In the particular study, we employed six publically available industrial-related datasets containing defect materials and industrial tools or engine parts, aiming to develop a specialized model for pattern recognition. Motivated by the recent success of the Virtual Geometry Group (VGG) network, we propose a modified version of it, called Multipath VGG19, which allows for more local and global feature extraction, while the extra features are fused via concatenation. The experiments verified the effectiveness of MVGG19 over the traditional VGG19. Specifically, top classification performance was achieved in five of the six image datasets, while the average classification improvement was 6.95%.



### Legacy Photo Editing with Learned Noise Prior
- **Arxiv ID**: http://arxiv.org/abs/2011.11309v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11309v2)
- **Published**: 2020-11-23 10:18:01+00:00
- **Updated**: 2020-11-24 08:27:30+00:00
- **Authors**: Zhao Yuzhi, Po Lai-Man, Wang Xuehui, Liu Kangcheng, Zhang Yujia, Yu Wing-Yin, Xian Pengfei, Xiong Jingjing
- **Comment**: accepted by IEEE WACV 2021, 2nd round submission
- **Journal**: None
- **Summary**: There are quite a number of photographs captured under undesirable conditions in the last century. Thus, they are often noisy, regionally incomplete, and grayscale formatted. Conventional approaches mainly focus on one point so that those restoration results are not perceptually sharp or clean enough. To solve these problems, we propose a noise prior learner NEGAN to simulate the noise distribution of real legacy photos using unpaired images. It mainly focuses on matching high-frequency parts of noisy images through discrete wavelet transform (DWT) since they include most of noise statistics. We also create a large legacy photo dataset for learning noise prior. Using learned noise prior, we can easily build valid training pairs by degrading clean images. Then, we propose an IEGAN framework performing image editing including joint denoising, inpainting and colorization based on the estimated noise prior. We evaluate the proposed system and compare it with state-of-the-art image enhancement methods. The experimental results demonstrate that it achieves the best perceptual quality. https://github.com/zhaoyuzhi/Legacy-Photo-Editing-with-Learned-Noise-Prior for the codes and the proposed LP dataset.



### Uncovering the Bias in Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/2011.11311v2
- **DOI**: 10.20378/irb-50304
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.11311v2)
- **Published**: 2020-11-23 10:20:10+00:00
- **Updated**: 2021-11-16 09:34:55+00:00
- **Authors**: Jessica Deuschel, Bettina Finzel, Ines Rieger
- **Comment**: Accepted at the Kolloquium Forschende Frauen 2020 - published in
  "Gender in Gesellschaft 4.0: Beitr\"age Bamberger
  Nachwuchswissenschaftlerinnen(2021)"
- **Journal**: Kolloquium Forschende Frauen 2020 - Gender in Gesellschaft 4.0:
  Beitraege Bamberger Nachwuchswissenschaftlerinnen
- **Summary**: Over the past decades the machine and deep learning community has celebrated great achievements in challenging tasks such as image classification. The deep architecture of artificial neural networks together with the plenitude of available data makes it possible to describe highly complex relations. Yet, it is still impossible to fully capture what the deep learning model has learned and to verify that it operates fairly and without creating bias, especially in critical tasks, for instance those arising in the medical field. One example for such a task is the detection of distinct facial expressions, called Action Units, in facial images. Considering this specific task, our research aims to provide transparency regarding bias, specifically in relation to gender and skin color. We train a neural network for Action Unit classification and analyze its performance quantitatively based on its accuracy and qualitatively based on heatmaps. A structured review of our results indicates that we are able to detect bias. Even though we cannot conclude from our results that lower classification performance emerged solely from gender and skin color bias, these biases must be addressed, which is why we end by giving suggestions on how the detected bias can be avoided.



### Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary Raster Data
- **Arxiv ID**: http://arxiv.org/abs/2011.11314v2
- **DOI**: 10.1109/TGRS.2021.3068532
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11314v2)
- **Published**: 2020-11-23 10:28:10+00:00
- **Updated**: 2021-05-25 13:25:48+00:00
- **Authors**: Gerald Baier, Antonin Deschemps, Michael Schmitt, Naoto Yokoya
- **Comment**: None
- **Journal**: None
- **Summary**: We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fr\'echet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth and we published the newly created high-resolution dataset at https://ieee-dataport.org/open-access/geonrw.



### Characterization of Industrial Smoke Plumes from Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2011.11344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11344v1)
- **Published**: 2020-11-23 11:54:32+00:00
- **Updated**: 2020-11-23 11:54:32+00:00
- **Authors**: Michael Mommert, Mario Sigel, Marcel Neuhausler, Linus Scheibenreif, Damian Borth
- **Comment**: To be presented at the "Tackling Climate Change with Machine
  Learning" workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: The major driver of global warming has been identified as the anthropogenic release of greenhouse gas (GHG) emissions from industrial activities. The quantitative monitoring of these emissions is mandatory to fully understand their effect on the Earth's climate and to enforce emission regulations on a large scale. In this work, we investigate the possibility to detect and quantify industrial smoke plumes from globally and freely available multi-band image data from ESA's Sentinel-2 satellites. Using a modified ResNet-50, we can detect smoke plumes of different sizes with an accuracy of 94.3%. The model correctly ignores natural clouds and focuses on those imaging channels that are related to the spectral absorption from aerosols and water vapor, enabling the localization of smoke. We exploit this localization ability and train a U-Net segmentation model on a labeled sub-sample of our data, resulting in an Intersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the detection of any smoke plume of 94.0%; on average, our model can reproduce the area covered by smoke in an image to within 5.6%. The performance of our model is mostly limited by occasional confusion with surface objects, the inability to identify semi-transparent smoke, and human limitations to properly identify smoke based on RGB-only images. Nevertheless, our results enable us to reliably detect and qualitatively estimate the level of smoke activity in order to monitor activity in industrial plants across the globe. Our data set and code base are publicly available.



### A Learning-based Optimization Algorithm:Image Registration Optimizer Network
- **Arxiv ID**: http://arxiv.org/abs/2011.11365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11365v1)
- **Published**: 2020-11-23 12:44:52+00:00
- **Updated**: 2020-11-23 12:44:52+00:00
- **Authors**: Jia Wang, Ping Wang, Biao Li, Yinghui Gao, Siyi Zhao
- **Comment**: 6pages
- **Journal**: None
- **Summary**: Remote sensing image registration is valuable for image-based navigation system despite posing many challenges. As the search space of registration is usually non-convex, the optimization algorithm, which aims to search the best transformation parameters, is a challenging step. Conventional optimization algorithms can hardly reconcile the contradiction of simultaneous rapid convergence and the global optimization. In this paper, a novel learning-based optimization algorithm named Image Registration Optimizer Network (IRON) is proposed, which can predict the global optimum after single iteration. The IRON is trained by a 3D tensor (9x9x9), which consists of similar metric values. The elements of the 3D tensor correspond to the 9x9x9 neighbors of the initial parameters in the search space. Then, the tensor's label is a vector that points to the global optimal parameters from the initial parameters. Because of the special architecture, the IRON could predict the global optimum directly for any initialization. The experimental results demonstrate that the proposed algorithm performs better than other classical optimization algorithms as it has higher accuracy, lower root of mean square error (RMSE), and more efficiency. Our IRON codes are available for further study.https://www.github.com/jaxwangkd04/IRON



### SCGAN: Saliency Map-guided Colorization with Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2011.11377v1
- **DOI**: 10.1109/TCSVT.2020.3037688
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.11377v1)
- **Published**: 2020-11-23 13:06:54+00:00
- **Updated**: 2020-11-23 13:06:54+00:00
- **Authors**: Yuzhi Zhao, Lai-Man Po, Kwok-Wai Cheung, Wing-Yin Yu, Yasar Abbas Ur Rehman
- **Comment**: accepted by IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Given a grayscale photograph, the colorization system estimates a visually plausible colorful image. Conventional methods often use semantics to colorize grayscale images. However, in these methods, only classification semantic information is embedded, resulting in semantic confusion and color bleeding in the final colorized image. To address these issues, we propose a fully automatic Saliency Map-guided Colorization with Generative Adversarial Network (SCGAN) framework. It jointly predicts the colorization and saliency map to minimize semantic confusion and color bleeding in the colorized image. Since the global features from pre-trained VGG-16-Gray network are embedded to the colorization encoder, the proposed SCGAN can be trained with much less data than state-of-the-art methods to achieve perceptually reasonable colorization. In addition, we propose a novel saliency map-based guidance method. Branches of the colorization decoder are used to predict the saliency map as a proxy target. Moreover, two hierarchical discriminators are utilized for the generated colorization and saliency map, respectively, in order to strengthen visual perception performance. The proposed system is evaluated on ImageNet validation set. Experimental results show that SCGAN can generate more reasonable colorized images than state-of-the-art techniques.



### Deep Learning for Automatic Quality Grading of Mangoes: Methods and Insights
- **Arxiv ID**: http://arxiv.org/abs/2011.11378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11378v1)
- **Published**: 2020-11-23 13:09:47+00:00
- **Updated**: 2020-11-23 13:09:47+00:00
- **Authors**: Shih-Lun Wu, Hsiao-Yen Tung, Yu-Lun Hsu
- **Comment**: Accepted to ICMLA 2020; 8 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: The quality grading of mangoes is a crucial task for mango growers as it vastly affects their profit. However, until today, this process still relies on laborious efforts of humans, who are prone to fatigue and errors. To remedy this, the paper approaches the grading task with various convolutional neural networks (CNN), a tried-and-tested deep learning technology in computer vision. The models involved include Mask R-CNN (for background removal), the numerous past winners of the ImageNet challenge, namely AlexNet, VGGs, and ResNets; and, a family of self-defined convolutional autoencoder-classifiers (ConvAE-Clfs) inspired by the claimed benefit of multi-task learning in classification tasks. Transfer learning is also adopted in this work via utilizing the ImageNet pretrained weights. Besides elaborating on the preprocessing techniques, training details, and the resulting performance, we go one step further to provide explainable insights into the model's working with the help of saliency maps and principal component analysis (PCA). These insights provide a succinct, meaningful glimpse into the intricate deep learning black box, fostering trust, and can also be presented to humans in real-world use cases for reviewing the grading results.



### Automated Quality Assessment of Hand Washing Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.11383v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11383v2)
- **Published**: 2020-11-23 13:22:53+00:00
- **Updated**: 2020-12-01 16:05:27+00:00
- **Authors**: Maksims Ivanovs, Roberts Kadikis, Martins Lulla, Aleksejs Rutkovskis, Atis Elsts
- **Comment**: None
- **Journal**: None
- **Summary**: Washing hands is one of the most important ways to prevent infectious diseases, including COVID-19. Unfortunately, medical staff does not always follow the World Health Organization (WHO) hand washing guidelines in their everyday work. To this end, we present neural networks for automatically recognizing the different washing movements defined by the WHO. We train the neural network on a part of a large (2000+ videos) real-world labeled dataset with the different washing movements. The preliminary results show that using pre-trained neural network models such as MobileNetV2 and Xception for the task, it is possible to achieve >64 % accuracy in recognizing the different washing movements. We also describe the collection and the structure of the above open-access dataset created as part of this work. Finally, we describe how the neural network can be used to construct a mobile phone application for automatic quality control and real-time feedback for medical professionals.



### PLOP: Learning without Forgetting for Continual Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11390v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11390v3)
- **Published**: 2020-11-23 13:35:03+00:00
- **Updated**: 2021-03-11 09:43:29+00:00
- **Authors**: Arthur Douillard, Yifu Chen, Arnaud Dapogny, Matthieu Cord
- **Comment**: Accepted at CVPR 2021, code:
  https://github.com/arthurdouillard/CVPR2021_PLOP
- **Journal**: None
- **Summary**: Deep learning approaches are nowadays ubiquitously used to tackle computer vision tasks such as semantic segmentation, requiring large datasets and substantial computational power. Continual learning for semantic segmentation (CSS) is an emerging trend that consists in updating an old model by sequentially adding new classes. However, continual learning methods are usually prone to catastrophic forgetting. This issue is further aggravated in CSS where, at each step, old classes from previous iterations are collapsed into the background. In this paper, we propose Local POD, a multi-scale pooling distillation scheme that preserves long- and short-range spatial relationships at feature level. Furthermore, we design an entropy-based pseudo-labelling of the background w.r.t. classes predicted by the old model to deal with background shift and avoid catastrophic forgetting of the old classes. Our approach, called PLOP, significantly outperforms state-of-the-art methods in existing CSS scenarios, as well as in newly proposed challenging benchmarks.



### Multi-task Learning for Human Settlement Extent Regression and Local Climate Zone Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.11452v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11452v1)
- **Published**: 2020-11-23 14:54:13+00:00
- **Updated**: 2020-11-23 14:54:13+00:00
- **Authors**: Chunping Qiu, Lukas Liebel, Lloyd H. Hughes, Michael Schmitt, Marco Krner, Xiao Xiang Zhu
- **Comment**: This work has been accepted by IEEE GRSL for publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: Human Settlement Extent (HSE) and Local Climate Zone (LCZ) maps are both essential sources, e.g., for sustainable urban development and Urban Heat Island (UHI) studies. Remote sensing (RS)- and deep learning (DL)-based classification approaches play a significant role by providing the potential for global mapping. However, most of the efforts only focus on one of the two schemes, usually on a specific scale. This leads to unnecessary redundancies, since the learned features could be leveraged for both of these related tasks. In this letter, the concept of multi-task learning (MTL) is introduced to HSE regression and LCZ classification for the first time. We propose a MTL framework and develop an end-to-end Convolutional Neural Network (CNN), which consists of a backbone network for shared feature learning, attention modules for task-specific feature learning, and a weighting strategy for balancing the two tasks. We additionally propose to exploit HSE predictions as a prior for LCZ classification to enhance the accuracy. The MTL approach was extensively tested with Sentinel-2 data of 13 cities across the world. The results demonstrate that the framework is able to provide a competitive solution for both tasks.



### Automatic Detection and Classification of Tick-borne Skin Lesions using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.11459v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11459v1)
- **Published**: 2020-11-23 15:16:14+00:00
- **Updated**: 2020-11-23 15:16:14+00:00
- **Authors**: Lauren Michelle Pfeifer, Matias Valdenegro-Toro
- **Comment**: 2 pages, 8 figures, with appendix
- **Journal**: None
- **Summary**: Around the globe, ticks are the culprit of transmitting a variety of bacterial, viral and parasitic diseases. The incidence of tick-borne diseases has drastically increased within the last decade, with annual cases of Lyme disease soaring to an estimated 300,000 in the United States alone. As a result, more efforts in improving lesion identification approaches and diagnostics for tick-borne illnesses is critical. The objective for this study is to build upon the approach used by Burlina et al. by using a variety of convolutional neural network models to detect tick-borne skin lesions. We expanded the data inputs by acquiring images from Google in seven different languages to test if this would diversify training data and improve the accuracy of skin lesion detection. The final dataset included nearly 6,080 images and was trained on a combination of architectures (ResNet 34, ResNet 50, VGG 19, and Dense Net 121). We obtained an accuracy of 80.72% with our model trained on the DenseNet 121 architecture.



### Unsupervised Difficulty Estimation with Action Scores
- **Arxiv ID**: http://arxiv.org/abs/2011.11461v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11461v1)
- **Published**: 2020-11-23 15:18:44+00:00
- **Updated**: 2020-11-23 15:18:44+00:00
- **Authors**: Octavio Arriaga, Matias Valdenegro-Toro
- **Comment**: 2 pages, 6 figures, with appendix
- **Journal**: None
- **Summary**: Evaluating difficulty and biases in machine learning models has become of extreme importance as current models are now being applied in real-world situations. In this paper we present a simple method for calculating a difficulty score based on the accumulation of losses for each sample during training. We call this the action score. Our proposed method does not require any modification of the model neither any external supervision, as it can be implemented as callback that gathers information from the training process. We test and analyze our approach in two different settings: image classification, and object detection, and we show that in both settings the action score can provide insights about model and dataset biases.



### TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks
- **Arxiv ID**: http://arxiv.org/abs/2011.11479v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11479v3)
- **Published**: 2020-11-23 15:40:15+00:00
- **Updated**: 2021-08-17 17:47:31+00:00
- **Authors**: Humam Alwassel, Silvio Giancola, Bernard Ghanem
- **Comment**: Accepted to ICCV 2021 workshops proceedings
- **Journal**: None
- **Summary**: Due to the large memory footprint of untrimmed videos, current state-of-the-art video localization methods operate atop precomputed video clip features. These features are extracted from video encoders typically trained for trimmed action classification tasks, making such features not necessarily suitable for temporal localization. In this work, we propose a novel supervised pretraining paradigm for clip features that not only trains to classify activities but also considers background clips and global video information to improve temporal sensitivity. Extensive experiments show that using features trained with our novel pretraining strategy significantly improves the performance of recent state-of-the-art methods on three tasks: Temporal Action Localization, Action Proposal Generation, and Dense Video Captioning. We also show that our pretraining approach is effective across three encoder architectures and two pretraining datasets. We believe video feature encoding is an important building block for localization algorithms, and extracting temporally-sensitive features should be of paramount importance in building more accurate models. The code and pretrained models are available on our project website.



### HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features
- **Arxiv ID**: http://arxiv.org/abs/2011.11498v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11498v3)
- **Published**: 2020-11-23 15:59:41+00:00
- **Updated**: 2021-09-09 10:31:59+00:00
- **Authors**: Cheng Sun, Min Sun, Hwann-Tzong Chen
- **Comment**: Code at https://github.com/sunset1995/HoHoNet. Video at
  https://www.youtube.com/watch?v=xXtRaRKmMpA
- **Journal**: None
- **Summary**: We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution $512 \times 1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin.



### Re-identification = Retrieval + Verification: Back to Essence and Forward with a New Metric
- **Arxiv ID**: http://arxiv.org/abs/2011.11506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11506v1)
- **Published**: 2020-11-23 16:11:19+00:00
- **Updated**: 2020-11-23 16:11:19+00:00
- **Authors**: Zheng Wang, Xin Yuan, Toshihiko Yamasaki, Yutian Lin, Xin Xu, Wenjun Zeng
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Re-identification (re-ID) is currently investigated as a closed-world image retrieval task, and evaluated by retrieval based metrics. The algorithms return ranking lists to users, but cannot tell which images are the true target. In essence, current re-ID overemphasizes the importance of retrieval but underemphasizes that of verification, \textit{i.e.}, all returned images are considered as the target. On the other hand, re-ID should also include the scenario that the query identity does not appear in the gallery. To this end, we go back to the essence of re-ID, \textit{i.e.}, a combination of retrieval and verification in an open-set setting, and put forward a new metric, namely, Genuine Open-set re-ID Metric (GOM).   GOM explicitly balances the effect of performing retrieval and verification into a single unified metric. It can also be decomposed into a family of sub-metrics, enabling a clear analysis of re-ID performance. We evaluate the effectiveness of GOM on the re-ID benchmarks, showing its ability to capture important aspects of re-ID performance that have not been taken into account by established metrics so far. Furthermore, we show GOM scores excellent in aligning with human visual evaluation of re-ID performance. Related codes are available at https://github.com/YuanXinCherry/Person-reID-Evaluation



### Elastic Interaction of Particles for Robotic Tactile Simulation
- **Arxiv ID**: http://arxiv.org/abs/2011.11528v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11528v1)
- **Published**: 2020-11-23 16:37:00+00:00
- **Updated**: 2020-11-23 16:37:00+00:00
- **Authors**: Yikai Wang, Wenbing Huang, Bin Fang, Fuchun Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Tactile sensing plays an important role in robotic perception and manipulation. To overcome the real-world limitations of data collection, simulating tactile response in virtual environment comes as a desire direction of robotic research. Most existing works model the tactile sensor as a rigid multi-body, which is incapable of reflecting the elastic property of the tactile sensor as well as characterizing the fine-grained physical interaction between two objects. In this paper, we propose Elastic Interaction of Particles (EIP), a novel framework for tactile emulation. At its core, EIP models the tactile sensor as a group of coordinated particles, and the elastic theory is applied to regulate the deformation of particles during the contact process. The implementation of EIP is conducted from scratch, without resorting to any existing physics engine. Experiments to verify the effectiveness of our method have been carried out on two applications: robotic perception with tactile data and 3D geometric reconstruction by tactile-visual fusion. It is possible to open up a new vein for robotic tactile simulation, and contribute to various downstream robotic tasks.



### Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.11534v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11534v4)
- **Published**: 2020-11-23 16:48:35+00:00
- **Updated**: 2022-04-19 05:59:10+00:00
- **Authors**: Gyeongsik Moon, Hongsuk Choi, Kyoung Mu Lee
- **Comment**: Published at CVPRW 2022
- **Journal**: None
- **Summary**: Whole-body 3D human mesh estimation aims to reconstruct the 3D human body, hands, and face simultaneously. Although several methods have been proposed, accurate prediction of 3D hands, which consist of 3D wrist and fingers, still remains challenging due to two reasons. First, the human kinematic chain has not been carefully considered when predicting the 3D wrists. Second, previous works utilize body features for the 3D fingers, where the body feature barely contains finger information. To resolve the limitations, we present Hand4Whole, which has two strong points over previous works. First, we design Pose2Pose, a module that utilizes joint features for 3D joint rotations. Using Pose2Pose, Hand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints largely contribute to 3D wrist rotations in the human kinematic chain. Second, Hand4Whole discards the body feature when predicting 3D finger rotations. Our Hand4Whole is trained in an end-to-end manner and produces much better 3D hand results than previous whole-body 3D human mesh estimation methods. The codes are available here at https://github.com/mks0601/Hand4Whole_RELEASE.



### Planar 3D Transfer Learning for End to End Unimodal MRI Unbalanced Data Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11557v1)
- **Published**: 2020-11-23 17:11:50+00:00
- **Updated**: 2020-11-23 17:11:50+00:00
- **Authors**: Martin Kolarik, Radim Burget, Carlos M. Travieso-Gonzalez, Jan Kocica
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach of 2D to 3D transfer learning based on mapping pre-trained 2D convolutional neural network weights into planar 3D kernels. The method is validated by the proposed planar 3D res-u-net network with encoder transferred from the 2D VGG-16, which is applied for a single-stage unbalanced 3D image data segmentation. In particular, we evaluate the method on the MICCAI 2016 MS lesion segmentation challenge dataset utilizing solely fluid-attenuated inversion recovery (FLAIR) sequence without brain extraction for training and inference to simulate real medical praxis. The planar 3D res-u-net network performed the best both in sensitivity and Dice score amongst end to end methods processing raw MRI scans and achieved comparable Dice score to a state-of-the-art unimodal not end to end approach. Complete source code was released under the open-source license, and this paper complies with the Machine learning reproducibility checklist. By implementing practical transfer learning for 3D data representation, we could segment heavily unbalanced data without selective sampling and achieved more reliable results using less training data in a single modality. From a medical perspective, the unimodal approach gives an advantage in real praxis as it does not require co-registration nor additional scanning time during an examination. Although modern medical imaging methods capture high-resolution 3D anatomy scans suitable for computer-aided detection system processing, deployment of automatic systems for interpretation of radiology imaging is still rather theoretical in many medical areas. Our work aims to bridge the gap by offering a solution for partial research questions.



### Comparing Normalization Methods for Limited Batch Size Segmentation Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.11559v1
- **DOI**: 10.1109/TSP49548.2020.9163397
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11559v1)
- **Published**: 2020-11-23 17:13:24+00:00
- **Updated**: 2020-11-23 17:13:24+00:00
- **Authors**: Martin Kolarik, Radim Burget, Kamil Riha
- **Comment**: None
- **Journal**: 2020 43rd International Conference on Telecommunications and
  Signal Processing (TSP)
- **Summary**: The widespread use of Batch Normalization has enabled training deeper neural networks with more stable and faster results. However, the Batch Normalization works best using large batch size during training and as the state-of-the-art segmentation convolutional neural network architectures are very memory demanding, large batch size is often impossible to achieve on current hardware. We evaluate the alternative normalization methods proposed to solve this issue on a problem of binary spine segmentation from 3D CT scan. Our results show the effectiveness of Instance Normalization in the limited batch size neural network training environment. Out of all the compared methods the Instance Normalization achieved the highest result with Dice coefficient = 0.96 which is comparable to our previous results achieved by deeper network with longer training time. We also show that the Instance Normalization implementation used in this experiment is computational time efficient when compared to the network without any normalization method.



### A Closed-Form Solution to Local Non-Rigid Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/2011.11567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11567v2)
- **Published**: 2020-11-23 17:26:19+00:00
- **Updated**: 2021-07-13 12:59:01+00:00
- **Authors**: Shaifali Parashar, Yuxuan Long, Mathieu Salzmann, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: A recent trend in Non-Rigid Structure-from-Motion (NRSfM) is to express local, differential constraints between pairs of images, from which the surface normal at any point can be obtained by solving a system of polynomial equations. The systems of equations derived in previous work, however, are of high degree, having up to five real solutions, thus requiring a computationally expensive strategy to select a unique solution. Furthermore, they suffer from degeneracies that make the resulting estimates unreliable, without any mechanism to identify this situation.   In this paper, we show that, under widely applicable assumptions, we can derive a new system of equation in terms of the surface normals whose two solutions can be obtained in closed-form and can easily be disambiguated locally. Our formalism further allows us to assess how reliable the estimated local normals are and, hence, to discard them if they are not. Our experiments show that our reconstructions, obtained from two or more views, are significantly more accurate than those of state-of-the-art methods, while also being faster.



### RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2011.11572v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11572v5)
- **Published**: 2020-11-23 17:33:53+00:00
- **Updated**: 2021-04-16 09:01:08+00:00
- **Authors**: Saeid Asgari Taghanaki, Jieliang Luo, Ran Zhang, Ye Wang, Pradeep Kumar Jayaraman, Krishna Murthy Jatavallabhula
- **Comment**: Published at the Robust and Reliable Machine Learning in the Real
  World Workshop, ICLR 2021
- **Journal**: None
- **Summary**: The 3D deep learning community has seen significant strides in pointcloud processing over the last few years. However, the datasets on which deep models have been trained have largely remained the same. Most datasets comprise clean, clutter-free pointclouds canonicalized for pose. Models trained on these datasets fail in uninterpretible and unintuitive ways when presented with data that contains transformations "unseen" at train time. While data augmentation enables models to be robust to "previously seen" input transformations, 1) we show that this does not work for unseen transformations during inference, and 2) data augmentation makes it difficult to analyze a model's inherent robustness to transformations. To this end, we create a publicly available dataset for robustness analysis of point cloud classification models (independent of data augmentation) to input transformations, called RobustPointSet. Our experiments indicate that despite all the progress in the point cloud classification, there is no single architecture that consistently performs better -- several fail drastically -- when evaluated on transformed test sets. We also find that robustness to unseen transformations cannot be brought about merely by extensive data augmentation. RobustPointSet can be accessed through https://github.com/AutodeskAILab/RobustPointSet.



### Scattering Transform Based Image Clustering using Projection onto Orthogonal Complement
- **Arxiv ID**: http://arxiv.org/abs/2011.11586v2
- **DOI**: 10.1145/3463944.3469098
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11586v2)
- **Published**: 2020-11-23 17:59:03+00:00
- **Updated**: 2020-11-24 19:16:39+00:00
- **Authors**: Angel Villar-Corrales, Veniamin I. Morgenshtern
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years, large improvements in image clustering have been driven by the recent advances in deep learning. However, due to the architectural complexity of deep neural networks, there is no mathematical theory that explains the success of deep clustering techniques. In this work we introduce Projected-Scattering Spectral Clustering (PSSC), a state-of-the-art, stable, and fast algorithm for image clustering, which is also mathematically interpretable. PSSC includes a novel method to exploit the geometric structure of the scattering transform of small images. This method is inspired by the observation that, in the scattering transform domain, the subspaces formed by the eigenvectors corresponding to the few largest eigenvalues of the data matrices of individual classes are nearly shared among different classes. Therefore, projecting out those shared subspaces reduces the intra-class variability, substantially increasing the clustering performance. We call this method Projection onto Orthogonal Complement (POC). Our experiments demonstrate that PSSC obtains the best results among all shallow clustering algorithms. Moreover, it achieves comparable clustering performance to that of recent state-of-the-art clustering techniques, while reducing the execution time by more than one order of magnitude. In the spirit of reproducible research, we publish a high quality code repository along with the paper.



### Abiotic Stress Prediction from RGB-T Images of Banana Plantlets
- **Arxiv ID**: http://arxiv.org/abs/2011.11597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11597v1)
- **Published**: 2020-11-23 18:15:33+00:00
- **Updated**: 2020-11-23 18:15:33+00:00
- **Authors**: Sagi Levanon, Oshry Markovich, Itamar Gozlan, Ortal Bakhshian, Alon Zvirin, Yaron Honen, Ron Kimmel
- **Comment**: Accepted paper at ECCV 2020 Workshop on Computer Vision Problems in
  Plant Phenotyping
- **Journal**: None
- **Summary**: Prediction of stress conditions is important for monitoring plant growth stages, disease detection, and assessment of crop yields. Multi-modal data, acquired from a variety of sensors, offers diverse perspectives and is expected to benefit the prediction process. We present several methods and strategies for abiotic stress prediction in banana plantlets, on a dataset acquired during a two and a half weeks period, of plantlets subject to four separate water and fertilizer treatments. The dataset consists of RGB and thermal images, taken once daily of each plant. Results are encouraging, in the sense that neural networks exhibit high prediction rates (over $90\%$ amongst four classes), in cases where there are hardly any noticeable features distinguishing the treatments, much higher than field experts can supply.



### Yet it moves: Learning from Generic Motions to Generate IMU data from YouTube videos
- **Arxiv ID**: http://arxiv.org/abs/2011.11600v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11600v1)
- **Published**: 2020-11-23 18:16:46+00:00
- **Updated**: 2020-11-23 18:16:46+00:00
- **Authors**: Vitor Fortes Rey, Kamalveer Kaur Garewal, Paul Lukowicz
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition (HAR) using wearable sensors has benefited much less from recent advances in Machine Learning than fields such as computer vision and natural language processing. This is to a large extent due to the lack of large scale repositories of labeled training data. In our research we aim to facilitate the use of online videos, which exists in ample quantity for most activities and are much easier to label than sensor data, to simulate labeled wearable motion sensor data. In previous work we already demonstrate some preliminary results in this direction focusing on very simple, activity specific simulation models and a single sensor modality (acceleration norm)\cite{10.1145/3341162.3345590}. In this paper we show how we can train a regression model on generic motions for both accelerometer and gyro signals and then apply it to videos of the target activities to generate synthetic IMU data (acceleration and gyro norms) that can be used to train and/or improve HAR models. We demonstrate that systems trained on simulated data generated by our regression model can come to within around 10% of the mean F1 score of a system trained on real sensor data. Furthermore we show that by either including a small amount of real sensor data for model calibration or simply leveraging the fact that (in general) we can easily generate much more simulated data from video than we can collect in terms of real sensor data the advantage of real sensor data can be eventually equalized.



### High Fidelity Interactive Video Segmentation Using Tensor Decomposition Boundary Loss Convolutional Tessellations and Context Aware Skip Connections
- **Arxiv ID**: http://arxiv.org/abs/2011.11602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11602v1)
- **Published**: 2020-11-23 18:21:42+00:00
- **Updated**: 2020-11-23 18:21:42+00:00
- **Authors**: Anthony D. Rhodes, Manan Goel
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a high fidelity deep learning algorithm (HyperSeg) for interactive video segmentation tasks using a convolutional network with context-aware skip connections, and compressed, hypercolumn image features combined with a convolutional tessellation procedure. In order to maintain high output fidelity, our model crucially processes and renders all image features in high resolution, without utilizing downsampling or pooling procedures. We maintain this consistent, high grade fidelity efficiently in our model chiefly through two means: (1) We use a statistically-principled tensor decomposition procedure to modulate the number of hypercolumn features and (2) We render these features in their native resolution using a convolutional tessellation technique. For improved pixel level segmentation results, we introduce a boundary loss function; for improved temporal coherence in video data, we include temporal image information in our model. Through experiments, we demonstrate the improved accuracy of our model against baseline models for interactive segmentation tasks using high resolution video data. We also introduce a benchmark video segmentation dataset, the VFX Segmentation Dataset, which contains over 27,046 high resolution video frames, including greenscreen and various composited scenes with corresponding, hand crafted, pixel level segmentations. Our work presents an extension to improvement to state of the art segmentation fidelity with high resolution data and can be used across a broad range of application domains, including VFX pipelines and medical imaging disciplines.



### Interpretable Visual Reasoning via Induced Symbolic Space
- **Arxiv ID**: http://arxiv.org/abs/2011.11603v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11603v2)
- **Published**: 2020-11-23 18:21:49+00:00
- **Updated**: 2021-08-24 13:55:14+00:00
- **Authors**: Zhonghao Wang, Kai Wang, Mo Yu, Jinjun Xiong, Wen-mei Hwu, Mark Hasegawa-Johnson, Humphrey Shi
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We study the problem of concept induction in visual reasoning, i.e., identifying concepts and their hierarchical relationships from question-answer pairs associated with images; and achieve an interpretable model via working on the induced symbolic concept space. To this end, we first design a new framework named object-centric compositional attention model (OCCAM) to perform the visual reasoning task with object-level visual features. Then, we come up with a method to induce concepts of objects and relations using clues from the attention patterns between objects' visual features and question words. Finally, we achieve a higher level of interpretability by imposing OCCAM on the objects represented in the induced symbolic concept space. Our model design makes this an easy adaption via first predicting the concepts of objects and relations and then projecting the predicted concepts back to the visual feature space so the compositional reasoning module can process normally. Experiments on the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of the art without human-annotated functional programs; 2) our induced concepts are both accurate and sufficient as OCCAM achieves an on-par performance on objects represented either in visual features or in the induced symbolic concept space.



### Transfer Learning for Oral Cancer Detection using Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2011.11610v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11610v2)
- **Published**: 2020-11-23 18:35:59+00:00
- **Updated**: 2021-04-09 18:41:26+00:00
- **Authors**: Rutwik Palaskar, Renu Vyas, Vilas Khedekar, Sangeeta Palaskar, Pranjal Sahu
- **Comment**: None
- **Journal**: None
- **Summary**: Oral cancer has more than 83% survival rate if detected in its early stages, however, only 29% of cases are currently detected early. Deep learning techniques can detect patterns of oral cancer cells and can aid in its early detection. In this work, we present the first results of neural networks for oral cancer detection using microscopic images. We compare numerous state-of-the-art models via transfer learning approach and collect and release an augmented dataset of high-quality microscopic images of oral cancer. We present a comprehensive study of different models and report their performance on this type of data. Overall, we obtain a 10-15% absolute improvement with transfer learning methods compared to a simple Convolutional Neural Network baseline. Ablation studies show the added benefit of data augmentation techniques with finetuning for this task.



### Cycle-consistent Generative Adversarial Networks for Neural Style Transfer using data from Chang'E-4
- **Arxiv ID**: http://arxiv.org/abs/2011.11627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11627v1)
- **Published**: 2020-11-23 18:57:27+00:00
- **Updated**: 2020-11-23 18:57:27+00:00
- **Authors**: J. de Curt, R. Duvall
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have had tremendous applications in Computer Vision. Yet, in the context of space science and planetary exploration the door is open for major advances. We introduce tools to handle planetary data from the mission Chang'E-4 and present a framework for Neural Style Transfer using Cycle-consistency from rendered images. The experiments are conducted in the context of the Iris Lunar Rover, a nano-rover that will be deployed in lunar terrain in 2021 as the flagship of Carnegie Mellon, being the first unmanned rover of America to be on the Moon.



### Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11630v1)
- **Published**: 2020-11-23 18:59:08+00:00
- **Updated**: 2020-11-23 18:59:08+00:00
- **Authors**: Hala Lamdouar, Charig Yang, Weidi Xie, Andrew Zisserman
- **Comment**: ACCV 2020
- **Journal**: None
- **Summary**: The objective of this paper is to design a computational architecture that discovers camouflaged objects in videos, specifically by exploiting motion information to perform object segmentation. We make the following three contributions: (i) We propose a novel architecture that consists of two essential components for breaking camouflage, namely, a differentiable registration module to align consecutive frames based on the background, which effectively emphasises the object boundary in the difference image, and a motion segmentation module with memory that discovers the moving objects, while maintaining the object permanence even when motion is absent at some point. (ii) We collect the first large-scale Moving Camouflaged Animals (MoCA) video dataset, which consists of over 140 clips across a diverse range of animals (67 categories). (iii) We demonstrate the effectiveness of the proposed model on MoCA, and achieve competitive performance on the unsupervised segmentation protocol on DAVIS2016 by only relying on motion.



### Low-Resolution Face Recognition In Resource-Constrained Environments
- **Arxiv ID**: http://arxiv.org/abs/2011.11674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11674v1)
- **Published**: 2020-11-23 19:14:02+00:00
- **Updated**: 2020-11-23 19:14:02+00:00
- **Authors**: Mozhdeh Rouhsedaghat, Yifan Wang, Shuowen Hu, Suya You, C. -C. Jay Kuo
- **Comment**: 11 pages, 5 figures, under consideration at Pattern Recognition
  Letters
- **Journal**: None
- **Summary**: A non-parametric low-resolution face recognition model for resource-constrained environments with limited networking and computing is proposed in this work. Such environments often demand a small model capable of being effectively trained on a small number of labeled data samples, with low training complexity, and low-resolution input images. To address these challenges, we adopt an emerging explainable machine learning methodology called successive subspace learning (SSL).SSL offers an explainable non-parametric model that flexibly trades the model size for verification performance. Its training complexity is significantly lower since its model is trained in a one-pass feedforward manner without backpropagation. Furthermore, active learning can be conveniently incorporated to reduce the labeling cost. The effectiveness of the proposed model is demonstrated by experiments on the LFW and the CMU Multi-PIE datasets.



### Scaling Wide Residual Networks for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11675v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11675v2)
- **Published**: 2020-11-23 19:14:11+00:00
- **Updated**: 2021-02-08 04:07:27+00:00
- **Authors**: Liang-Chieh Chen, Huiyu Wang, Siyuan Qiao
- **Comment**: Update experimental results
- **Journal**: None
- **Summary**: The Wide Residual Networks (Wide-ResNets), a shallow but wide model variant of the Residual Networks (ResNets) by stacking a small number of residual blocks with large channel sizes, have demonstrated outstanding performance on multiple dense prediction tasks. However, since proposed, the Wide-ResNet architecture has barely evolved over the years. In this work, we revisit its architecture design for the recent challenging panoptic segmentation task, which aims to unify semantic segmentation and instance segmentation. A baseline model is obtained by incorporating the simple and effective Squeeze-and-Excitation and Switchable Atrous Convolution to the Wide-ResNets. Its network capacity is further scaled up or down by adjusting the width (i.e., channel size) and depth (i.e., number of layers), resulting in a family of SWideRNets (short for Scaling Wide Residual Networks). We demonstrate that such a simple scaling scheme, coupled with grid search, identifies several SWideRNets that significantly advance state-of-the-art performance on panoptic segmentation datasets in both the fast model regime and strong model regime.



### Sequential Topological Representations for Predictive Models of Deformable Objects
- **Arxiv ID**: http://arxiv.org/abs/2011.11693v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11693v2)
- **Published**: 2020-11-23 19:45:15+00:00
- **Updated**: 2021-05-10 20:31:42+00:00
- **Authors**: Rika Antonova, Anastasiia Varava, Peiyang Shi, J. Frederico Carvalho, Danica Kragic
- **Comment**: To appear in PMLR (Proceedings of Machine Learning Research) as part
  of L4DC (Learning for Dynamics and Control) conference proceedings
- **Journal**: None
- **Summary**: Deformable objects present a formidable challenge for robotic manipulation due to the lack of canonical low-dimensional representations and the difficulty of capturing, predicting, and controlling such objects. We construct compact topological representations to capture the state of highly deformable objects that are topologically nontrivial. We develop an approach that tracks the evolution of this topological state through time. Under several mild assumptions, we prove that the topology of the scene and its evolution can be recovered from point clouds representing the scene. Our further contribution is a method to learn predictive models that take a sequence of past point cloud observations as input and predict a sequence of topological states, conditioned on target/future control actions. Our experiments with highly deformable objects in simulation show that the proposed multistep predictive models yield more precise results than those obtained from computational topology libraries. These models can leverage patterns inferred across various objects and offer fast multistep predictions suitable for real-time applications.



### Explainable-by-design Semi-Supervised Representation Learning for COVID-19 Diagnosis from CT Imaging
- **Arxiv ID**: http://arxiv.org/abs/2011.11719v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11719v3)
- **Published**: 2020-11-23 20:51:22+00:00
- **Updated**: 2021-09-02 10:10:08+00:00
- **Authors**: Abel Daz Berenguer, Hichem Sahli, Boris Joukovsky, Maryna Kvasnytsia, Ine Dirks, Mitchel Alioscha-Perez, Nikos Deligiannis, Panagiotis Gonidakis, Sebastin Amador Snchez, Redona Brahimetaj, Evgenia Papavasileiou, Jonathan Cheung-Wai Chana, Fei Li, Shangzhen Song, Yixin Yang, Sofie Tilborghs, Siri Willems, Tom Eelbode, Jeroen Bertels, Dirk Vandermeulen, Frederik Maes, Paul Suetens, Lucas Fidon, Tom Vercauteren, David Robben, Arne Brys, Dirk Smeets, Bart Ilsen, Nico Buls, Nina Watt, Johan de Mey, Annemiek Snoeckx, Paul M. Parizel, Julien Guiot, Louis Deprez, Paul Meunier, Stefaan Gryspeerdt, Kristof De Smet, Bart Jansen, Jef Vandemeulebroucke
- **Comment**: None
- **Journal**: None
- **Summary**: Our motivating application is a real-world problem: COVID-19 classification from CT imaging, for which we present an explainable Deep Learning approach based on a semi-supervised classification pipeline that employs variational autoencoders to extract efficient feature embedding. We have optimized the architecture of two different networks for CT images: (i) a novel conditional variational autoencoder (CVAE) with a specific architecture that integrates the class labels inside the encoder layers and uses side information with shared attention layers for the encoder, which make the most of the contextual clues for representation learning, and (ii) a downstream convolutional neural network for supervised classification using the encoder structure of the CVAE. With the explainable classification results, the proposed diagnosis system is very effective for COVID-19 classification. Based on the promising results obtained qualitatively and quantitatively, we envisage a wide deployment of our developed technique in large-scale clinical studies.Code is available at https://git.etrovub.be/AVSP/ct-based-covid-19-diagnostic-tool.git.



### Siamese Tracking with Lingual Object Constraints
- **Arxiv ID**: http://arxiv.org/abs/2011.11721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11721v1)
- **Published**: 2020-11-23 20:55:08+00:00
- **Updated**: 2020-11-23 20:55:08+00:00
- **Authors**: Maximilian Filtenborg, Efstratios Gavves, Deepak Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Classically, visual object tracking involves following a target object throughout a given video, and it provides us the motion trajectory of the object. However, for many practical applications, this output is often insufficient since additional semantic information is required to act on the video material. Example applications of this are surveillance and target-specific video summarization, where the target needs to be monitored with respect to certain predefined constraints, e.g., 'when standing near a yellow car'. This paper explores, tracking visual objects subjected to additional lingual constraints. Differently from Li et al., we impose additional lingual constraints upon tracking, which enables new applications of tracking. Whereas in their work the goal is to improve and extend upon tracking itself. To perform benchmarks and experiments, we contribute two datasets: c-MOT16 and c-LaSOT, curated through appending additional constraints to the frames of the original LaSOT and MOT16 datasets. We also experiment with two deep models SiamCT-DFG and SiamCT-CA, obtained through extending a recent state-of-the-art Siamese tracking method and adding modules inspired from the fields of natural language processing and visual question answering. Through experimental results, we show that the proposed model SiamCT-CA can significantly outperform its counterparts. Furthermore, our method enables the selective compression of videos, based on the validity of the constraint.



### From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion
- **Arxiv ID**: http://arxiv.org/abs/2011.11722v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11722v1)
- **Published**: 2020-11-23 20:55:54+00:00
- **Updated**: 2020-11-23 20:55:54+00:00
- **Authors**: Deepali Jain, Atil Iscen, Ken Caluwaerts
- **Comment**: None
- **Journal**: 4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA
- **Summary**: Legged robots navigating crowded scenes and complex terrains in the real world are required to execute dynamic leg movements while processing visual input for obstacle avoidance and path planning. We show that a quadruped robot can acquire both of these skills by means of hierarchical reinforcement learning (HRL). By virtue of their hierarchical structure, our policies learn to implicitly break down this joint problem by concurrently learning High Level (HL) and Low Level (LL) neural network policies. These two levels are connected by a low dimensional hidden layer, which we call latent command. HL receives a first-person camera view, whereas LL receives the latent command from HL and the robot's on-board sensors to control its actuators. We train policies to walk in two different environments: a curved cliff and a maze. We show that hierarchical policies can concurrently learn to locomote and navigate in these environments, and show they are more efficient than non-hierarchical neural network policies. This architecture also allows for knowledge reuse across tasks. LL networks trained on one task can be transferred to a new task in a new environment. Finally HL, which processes camera images, can be evaluated at much lower and varying frequencies compared to LL, thus reducing computation times and bandwidth requirements.



### Rotation-Only Bundle Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2011.11724v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11724v2)
- **Published**: 2020-11-23 20:57:11+00:00
- **Updated**: 2021-03-27 23:23:05+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We propose a novel method for estimating the global rotations of the cameras independently of their positions and the scene structure. When two calibrated cameras observe five or more of the same points, their relative rotation can be recovered independently of the translation. We extend this idea to multiple views, thereby decoupling the rotation estimation from the translation and structure estimation. Our approach provides several benefits such as complete immunity to inaccurate translations and structure, and the accuracy improvement when used with rotation averaging. We perform extensive evaluations on both synthetic and real datasets, demonstrating consistent and significant gains in accuracy when used with the state-of-the-art rotation averaging method.



### End-to-End Framework for Efficient Deep Learning Using Metasurfaces Optics
- **Arxiv ID**: http://arxiv.org/abs/2011.11728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.11728v2)
- **Published**: 2020-11-23 21:06:04+00:00
- **Updated**: 2021-05-21 13:22:00+00:00
- **Authors**: Carlos Mauricio Villegas Burgos, Tianqi Yang, Nick Vamivakas, Yuhao Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning using Convolutional Neural Networks (CNNs) has been shown to significantly out-performed many conventional vision algorithms. Despite efforts to increase the CNN efficiency both algorithmically and with specialized hardware, deep learning remains difficult to deploy in resource-constrained environments. In this paper, we propose an end-to-end framework to explore optically compute the CNNs in free-space, much like a computational camera. Compared to existing free-space optics-based approaches which are limited to processing single-channel (i.e., grayscale) inputs, we propose the first general approach, based on nanoscale meta-surface optics, that can process RGB data directly from the natural scenes. Our system achieves up to an order of magnitude energy saving, simplifies the sensor design, all the while sacrificing little network accuracy.



### RISE-SLAM: A Resource-aware Inverse Schmidt Estimator for SLAM
- **Arxiv ID**: http://arxiv.org/abs/2011.11730v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11730v1)
- **Published**: 2020-11-23 21:10:32+00:00
- **Updated**: 2020-11-23 21:10:32+00:00
- **Authors**: Tong Ke, Kejian J. Wu, Stergios I. Roumeliotis
- **Comment**: IROS 2019
- **Journal**: None
- **Summary**: In this paper, we present the RISE-SLAM algorithm for performing visual-inertial simultaneous localization and mapping (SLAM), while improving estimation consistency. Specifically, in order to achieve real-time operation, existing approaches often assume previously-estimated states to be perfectly known, which leads to inconsistent estimates. Instead, based on the idea of the Schmidt-Kalman filter, which has processing cost linear in the size of the state vector but quadratic memory requirements, we derive a new consistent approximate method in the information domain, which has linear memory requirements and adjustable (constant to linear) processing cost. In particular, this method, the resource-aware inverse Schmidt estimator (RISE), allows trading estimation accuracy for computational efficiency. Furthermore, and in order to better address the requirements of a SLAM system during an exploration vs. a relocalization phase, we employ different configurations of RISE (in terms of the number and order of states updated) to maximize accuracy while preserving efficiency. Lastly, we evaluate the proposed RISE-SLAM algorithm on publicly-available datasets and demonstrate its superiority, both in terms of accuracy and efficiency, as compared to alternative visual-inertial SLAM systems.



### HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms
- **Arxiv ID**: http://arxiv.org/abs/2011.11731v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11731v2)
- **Published**: 2020-11-23 21:14:19+00:00
- **Updated**: 2021-03-27 02:23:03+00:00
- **Authors**: Mahmoud Afifi, Marcus A. Brubaker, Michael S. Brown
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: While generative adversarial networks (GANs) can successfully produce high-quality images, they can be challenging to control. Simplifying GAN-based image generation is critical for their adoption in graphic design and artistic work. This goal has led to significant interest in methods that can intuitively control the appearance of images generated by GANs. In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images' colors. We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN architecture to control the colors of GAN-generated images specified by a target color histogram feature. We then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network to keep the original image's content while changing the colors based on the given target histogram. We show that this histogram-based approach offers a better way to control GAN-generated and real images' colors while producing more compelling results compared to existing alternative strategies.



### Detecting hidden signs of diabetes in external eye photographs
- **Arxiv ID**: http://arxiv.org/abs/2011.11732v1
- **DOI**: 10.1038/s41551-022-00867-5
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11732v1)
- **Published**: 2020-11-23 21:14:34+00:00
- **Updated**: 2020-11-23 21:14:34+00:00
- **Authors**: Boris Babenko, Akinori Mitani, Ilana Traynis, Naho Kitade, Preeti Singh, April Maa, Jorge Cuadros, Greg S. Corrado, Lily Peng, Dale R. Webster, Avinash Varadarajan, Naama Hammel, Yun Liu
- **Comment**: None
- **Journal**: Nature Biomedical Engineering 2022
- **Summary**: Diabetes-related retinal conditions can be detected by examining the posterior of the eye. By contrast, examining the anterior of the eye can reveal conditions affecting the front of the eye, such as changes to the eyelids, cornea, or crystalline lens. In this work, we studied whether external photographs of the front of the eye can reveal insights into both diabetic retinal diseases and blood glucose control. We developed a deep learning system (DLS) using external eye photographs of 145,832 patients with diabetes from 301 diabetic retinopathy (DR) screening sites in one US state, and evaluated the DLS on three validation sets containing images from 198 sites in 18 other US states. In validation set A (n=27,415 patients, all undilated), the DLS detected poor blood glucose control (HbA1c > 9%) with an area under receiver operating characteristic curve (AUC) of 70.2; moderate-or-worse DR with an AUC of 75.3; diabetic macular edema with an AUC of 78.0; and vision-threatening DR with an AUC of 79.4. For all 4 prediction tasks, the DLS's AUC was higher (p<0.001) than using available self-reported baseline characteristics (age, sex, race/ethnicity, years with diabetes). In terms of positive predictive value, the predicted top 5% of patients had a 67% chance of having HbA1c > 9%, and a 20% chance of having vision threatening diabetic retinopathy. The results generalized to dilated pupils (validation set B, 5,058 patients) and to a different screening service (validation set C, 10,402 patients). Our results indicate that external eye photographs contain information useful for healthcare providers managing patients with diabetes, and may help prioritize patients for in-person screening. Further work is needed to validate these findings on different devices and patient populations (those without diabetes) to evaluate its utility for remote diagnosis and management.



### Learnable Gabor modulated complex-valued networks for orientation robustness
- **Arxiv ID**: http://arxiv.org/abs/2011.11734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11734v2)
- **Published**: 2020-11-23 21:22:27+00:00
- **Updated**: 2021-10-05 18:01:10+00:00
- **Authors**: Felix Richards, Adeline Paiement, Xianghua Xie, Elisabeth Sola, Pierre-Alain Duc
- **Comment**: Submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Robustness to transformation is desirable in many computer vision tasks, given that input data often exhibits pose variance. While translation invariance and equivariance is a documented phenomenon of CNNs, sensitivity to other transformations is typically encouraged through data augmentation. We investigate the modulation of complex valued convolutional weights with learned Gabor filters to enable orientation robustness. The resulting network can generate orientation dependent features free of interpolation with a single set of learnable rotation-governing parameters. By choosing to either retain or pool orientation channels, the choice of equivariance versus invariance can be directly controlled. Moreover, we introduce rotational weight-tying through a proposed cyclic Gabor convolution, further enabling generalisation over rotations. We combine these innovations into Learnable Gabor Convolutional Networks (LGCNs), that are parameter-efficient and offer increased model complexity. We demonstrate their rotation invariance and equivariance on MNIST, BSD and a dataset of simulated and real astronomical images of Galactic cirri.



### Large Scale Multimodal Classification Using an Ensemble of Transformer Models and Co-Attention
- **Arxiv ID**: http://arxiv.org/abs/2011.11735v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11735v1)
- **Published**: 2020-11-23 21:22:54+00:00
- **Updated**: 2020-11-23 21:22:54+00:00
- **Authors**: Varnith Chordia, Vijay Kumar BG
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and efficient product classification is significant for E-commerce applications, as it enables various downstream tasks such as recommendation, retrieval, and pricing. Items often contain textual and visual information, and utilizing both modalities usually outperforms classification utilizing either mode alone. In this paper we describe our methodology and results for the SIGIR eCom Rakuten Data Challenge. We employ a dual attention technique to model image-text relationships using pretrained language and image embeddings. While dual attention has been widely used for Visual Question Answering(VQA) tasks, ours is the first attempt to apply the concept for multimodal classification.



### Accurate and Rapid Diagnosis of COVID-19 Pneumonia with Batch Effect Removal of Chest CT-Scans and Interpretable Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2011.11736v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11736v2)
- **Published**: 2020-11-23 21:23:55+00:00
- **Updated**: 2021-01-08 07:08:00+00:00
- **Authors**: Rassa Ghavami Modegh, Mehrab Hamidi, Saeed Masoudian, Amir Mohseni, Hamzeh Lotfalinezhad, Mohammad Ali Kazemi, Behnaz Moradi, Mahyar Ghafoori, Omid Motamedi, Omid Pournik, Kiara Rezaei-Kalantari, Amirreza Manteghinezhad, Shaghayegh Haghjooy Javanmard, Fateme Abdoli Nezhad, Ahmad Enhesari, Mohammad Saeed Kheyrkhah, Razieh Eghtesadi, Javid Azadbakht, Akbar Aliasgharzadeh, Mohammad Reza Sharif, Ali Khaleghi, Abbas Foroutan, Hossein Ghanaati, Hamed Dashti, Hamid R. Rabiee
- **Comment**: 27 pages, 4 figures. Some minor changes have been applied to the
  text, some fomulae are added to help the descriptions become more clear, two
  names and two names are corrected (The full version of the names are
  included)
- **Journal**: None
- **Summary**: COVID-19 is a virus with high transmission rate that demands rapid identification of the infected patients to reduce the spread of the disease. The current gold-standard test, Reverse-Transcription Polymerase Chain Reaction (RT-PCR), has a high rate of false negatives. Diagnosing from CT-scan images as a more accurate alternative has the challenge of distinguishing COVID-19 from other pneumonia diseases. Artificial intelligence can help radiologists and physicians to accelerate the process of diagnosis, increase its accuracy, and measure the severity of the disease. We designed a new interpretable deep neural network to distinguish healthy people, patients with COVID-19, and patients with other pneumonia diseases from axial lung CT-scan images. Our model also detects the infected areas and calculates the percentage of the infected lung volume. We first preprocessed the images to eliminate the batch effects of different devices, and then adopted a weakly supervised method to train the model without having any tags for the infected parts. We trained and evaluated the model on a large dataset of 3359 samples from 6 different medical centers. The model reached sensitivities of 97.75% and 98.15%, and specificities of 87% and 81.03% in separating healthy people from the diseased and COVID-19 from other diseases, respectively. It also demonstrated similar performance for 1435 samples from 6 different medical centers which proves its generalizability. The performance of the model on a large diverse dataset, its generalizability, and interpretability makes it suitable to be used as a reliable diagnostic system.



### Federated Semi-Supervised Learning for COVID Region Segmentation in Chest CT using Multi-National Data from China, Italy, Japan
- **Arxiv ID**: http://arxiv.org/abs/2011.11750v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11750v1)
- **Published**: 2020-11-23 21:51:26+00:00
- **Updated**: 2020-11-23 21:51:26+00:00
- **Authors**: Dong Yang, Ziyue Xu, Wenqi Li, Andriy Myronenko, Holger R. Roth, Stephanie Harmon, Sheng Xu, Baris Turkbey, Evrim Turkbey, Xiaosong Wang, Wentao Zhu, Gianpaolo Carrafiello, Francesca Patella, Maurizio Cariati, Hirofumi Obinata, Hitoshi Mori, Kaku Tamura, Peng An, Bradford J. Wood, Daguang Xu
- **Comment**: Accepted with minor revision to Medical Image Analysis
- **Journal**: None
- **Summary**: The recent outbreak of COVID-19 has led to urgent needs for reliable diagnosis and management of SARS-CoV-2 infection. As a complimentary tool, chest CT has been shown to be able to reveal visual patterns characteristic for COVID-19, which has definite value at several stages during the disease course. To facilitate CT analysis, recent efforts have focused on computer-aided characterization and diagnosis, which has shown promising results. However, domain shift of data across clinical data centers poses a serious challenge when deploying learning-based models. In this work, we attempt to find a solution for this challenge via federated and semi-supervised learning. A multi-national database consisting of 1704 scans from three countries is adopted to study the performance gap, when training a model with one dataset and applying it to another. Expert radiologists manually delineated 945 scans for COVID-19 findings. In handling the variability in both the data and annotations, a novel federated semi-supervised learning technique is proposed to fully utilize all available data (with or without annotations). Federated learning avoids the need for sensitive data-sharing, which makes it favorable for institutions and nations with strict regulatory policy on data privacy. Moreover, semi-supervision potentially reduces the annotation burden under a distributed setting. The proposed framework is shown to be effective compared to fully supervised scenarios with conventional data sharing instead of model weight sharing.



### Boosting Contrastive Self-Supervised Learning with False Negative Cancellation
- **Arxiv ID**: http://arxiv.org/abs/2011.11765v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11765v2)
- **Published**: 2020-11-23 22:17:21+00:00
- **Updated**: 2022-01-02 10:45:31+00:00
- **Authors**: Tri Huynh, Simon Kornblith, Matthew R. Walter, Michael Maire, Maryam Khademi
- **Comment**: Code is available at https://github.com/google-research/fnc
- **Journal**: None
- **Summary**: Self-supervised representation learning has made significant leaps fueled by progress in contrastive learning, which seeks to learn transformations that embed positive input pairs nearby, while pushing negative pairs far apart. While positive pairs can be generated reliably (e.g., as different views of the same image), it is difficult to accurately establish negative pairs, defined as samples from different images regardless of their semantic content or visual features. A fundamental problem in contrastive learning is mitigating the effects of false negatives. Contrasting false negatives induces two critical issues in representation learning: discarding semantic information and slow convergence. In this paper, we propose novel approaches to identify false negatives, as well as two strategies to mitigate their effect, i.e. false negative elimination and attraction, while systematically performing rigorous evaluations to study this problem in detail. Our method exhibits consistent improvements over existing contrastive learning-based methods. Without labels, we identify false negatives with 40% accuracy among 1000 semantic classes on ImageNet, and achieve 5.8% absolute improvement in top-1 accuracy over the previous state-of-the-art when finetuning with 1% labels. Our code is available at https://github.com/google-research/fnc.



### Automatic Recognition of the Supraspinatus Tendinopathy from Ultrasound Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.11777v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11777v1)
- **Published**: 2020-11-23 22:41:41+00:00
- **Updated**: 2020-11-23 22:41:41+00:00
- **Authors**: Mostafa Jahanifar, Neda Zamani Tajeddin, Meisam Hasani, Babak Shekarchi, Kamran Azema
- **Comment**: None
- **Journal**: None
- **Summary**: Tendon injuries like tendinopathies, full and partial thickness tears are prevalent, and the supraspinatus tendon (SST) is the most vulnerable ones in the rotator cuff. Early diagnosis of SST tendinopathies is of high importance and hard to achieve using ultrasound imaging. In this paper, an automatic tendinopathy recognition framework based on convolutional neural networks has been proposed to assist the diagnosis. This framework has two essential parts of tendon segmentation and classification. Tendon segmentation is done through a novel network, NASUNet, which follows an encoder-decoder architecture paradigm and utilizes a multi-scale Enlarging cell. Moreover, a general classification pipeline has been proposed for tendinopathy recognition, which supports different base models as the feature extractor engine. Two feature maps comprising positional information of the tendon region have been introduced as the network input to make the classification network spatial-aware. To evaluate the tendinopathy recognition system, a data set consisting of 100 SST ultrasound images have been acquired, in which tendinopathy cases are double-verified by magnetic resonance imaging. In both segmentation and classification tasks, lack of training data has been compensated by incorporating knowledge transferring, transfer learning, and data augmentation techniques. In cross-validation experiments, the proposed tendinopathy recognition model achieves 91% accuracy, 86.67% sensitivity, and 92.86% specificity, showing state-of-the-art performance against other models.



### KeepAugment: A Simple Information-Preserving Data Augmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.11778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11778v1)
- **Published**: 2020-11-23 22:43:04+00:00
- **Updated**: 2020-11-23 22:43:04+00:00
- **Authors**: Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, Qiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation (DA) is an essential technique for training state-of-the-art deep learning systems. In this paper, we empirically show data augmentation might introduce noisy augmented examples and consequently hurt the performance on unaugmented data during inference. To alleviate this issue, we propose a simple yet highly effective approach, dubbed \emph{KeepAugment}, to increase augmented images fidelity. The idea is first to use the saliency map to detect important regions on the original images and then preserve these informative regions during augmentation. This information-preserving strategy allows us to generate more faithful training examples. Empirically, we demonstrate our method significantly improves on a number of prior art data augmentation schemes, e.g. AutoAugment, Cutout, random erasing, achieving promising results on image classification, semi-supervised image classification, multi-view multi-camera tracking and object detection.



### AlphaMatch: Improving Consistency for Semi-supervised Learning with Alpha-divergence
- **Arxiv ID**: http://arxiv.org/abs/2011.11779v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11779v1)
- **Published**: 2020-11-23 22:43:45+00:00
- **Updated**: 2020-11-23 22:43:45+00:00
- **Authors**: Chengyue Gong, Dilin Wang, Qiang Liu
- **Comment**: preprint
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) is a key approach toward more data-efficient machine learning by jointly leverage both labeled and unlabeled data. We propose AlphaMatch, an efficient SSL method that leverages data augmentations, by efficiently enforcing the label consistency between the data points and the augmented data derived from them. Our key technical contribution lies on: 1) using alpha-divergence to prioritize the regularization on data with high confidence, achieving a similar effect as FixMatch but in a more flexible fashion, and 2) proposing an optimization-based, EM-like algorithm to enforce the consistency, which enjoys better convergence than iterative regularization procedures used in recent SSL methods such as FixMatch, UDA, and MixMatch. AlphaMatch is simple and easy to implement, and consistently outperforms prior arts on standard benchmarks, e.g. CIFAR-10, SVHN, CIFAR-100, STL-10. Specifically, we achieve 91.3% test accuracy on CIFAR-10 with just 4 labelled data per class, substantially improving over the previously best 88.7% accuracy achieved by FixMatch.



### Robust image stitching with multiple registrations
- **Arxiv ID**: http://arxiv.org/abs/2011.11784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11784v1)
- **Published**: 2020-11-23 23:08:39+00:00
- **Updated**: 2020-11-23 23:08:39+00:00
- **Authors**: Charles Herrmann, Chen Wang, Richard Strong Bowen, Emil Keyder, Michael Krainin, Ce Liu, Ramin Zabih
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Panorama creation is one of the most widely deployed techniques in computer vision. In addition to industry applications such as Google Street View, it is also used by millions of consumers in smartphones and other cameras. Traditionally, the problem is decomposed into three phases: registration, which picks a single transformation of each source image to align it to the other inputs, seam finding, which selects a source image for each pixel in the final result, and blending, which fixes minor visual artifacts. Here, we observe that the use of a single registration often leads to errors, especially in scenes with significant depth variation or object motion. We propose instead the use of multiple registrations, permitting regions of the image at different depths to be captured with greater accuracy. MRF inference techniques naturally extend to seam finding over multiple registrations, and we show here that their energy functions can be readily modified with new terms that discourage duplication and tearing, common problems that are exacerbated by the use of multiple registrations. Our techniques are closely related to layer-based stereo, and move image stitching closer to explicit scene modeling. Experimental evidence demonstrates that our techniques often generate significantly better panoramas when there is substantial motion or parallax.



### Prior to Segment: Foreground Cues for Weakly Annotated Classes in Partially Supervised Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11787v2)
- **Published**: 2020-11-23 23:15:06+00:00
- **Updated**: 2021-04-10 22:29:26+00:00
- **Authors**: David Biertimpel, Sindi Shkodrani, Anil S. Baslamisli, Nra Baka
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation methods require large datasets with expensive and thus limited instance-level mask labels. Partially supervised instance segmentation aims to improve mask prediction with limited mask labels by utilizing the more abundant weak box labels. In this work, we show that a class agnostic mask head, commonly used in partially supervised instance segmentation, has difficulties learning a general concept of foreground for the weakly annotated classes using box supervision only. To resolve this problem we introduce an object mask prior (OMP) that provides the mask head with the general concept of foreground implicitly learned by the box classification head under the supervision of all classes. This helps the class agnostic mask head to focus on the primary object in a region of interest (RoI) and improves generalization to the weakly annotated classes. We test our approach on the COCO dataset using different splits of strongly and weakly supervised classes. Our approach significantly improves over the Mask R-CNN baseline and obtains competitive performance with the state-of-the-art, while offering a much simpler architecture.



### Object-centered image stitching
- **Arxiv ID**: http://arxiv.org/abs/2011.11789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11789v1)
- **Published**: 2020-11-23 23:20:09+00:00
- **Updated**: 2020-11-23 23:20:09+00:00
- **Authors**: Charles Herrmann, Chen Wang, Richard Strong Bowen, Emil Keyder, Ramin Zabih
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Image stitching is typically decomposed into three phases: registration, which aligns the source images with a common target image; seam finding, which determines for each target pixel the source image it should come from; and blending, which smooths transitions over the seams. As described in [1], the seam finding phase attempts to place seams between pixels where the transition between source images is not noticeable. Here, we observe that the most problematic failures of this approach occur when objects are cropped, omitted, or duplicated. We therefore take an object-centered approach to the problem, leveraging recent advances in object detection [2,3,4]. We penalize candidate solutions with this class of error by modifying the energy function used in the seam finding stage. This produces substantially more realistic stitching results on challenging imagery. In addition, these methods can be used to determine when there is non-recoverable occlusion in the input data, and also suggest a simple evaluation metric that can be used to evaluate the output of stitching algorithms.




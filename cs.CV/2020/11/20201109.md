# Arxiv Papers in cs.CV on 2020-11-09
### Distance-Based Anomaly Detection for Industrial Surfaces Using Triplet Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.04121v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04121v2)
- **Published**: 2020-11-09 00:35:21+00:00
- **Updated**: 2020-11-10 04:20:49+00:00
- **Authors**: Tareq Tayeh, Sulaiman Aburakhia, Ryan Myers, Abdallah Shami
- **Comment**: 6 pages, 8 figures, 2020 IEEE 11th Annual Information Technology,
  Electronics and Mobile Communication Conference (Best Paper Award in the
  category of Image Processing and Artificial Intelligence)
- **Journal**: None
- **Summary**: Surface anomaly detection plays an important quality control role in many manufacturing industries to reduce scrap production. Machine-based visual inspections have been utilized in recent years to conduct this task instead of human experts. In particular, deep learning Convolutional Neural Networks (CNNs) have been at the forefront of these image processing-based solutions due to their predictive accuracy and efficiency. Training a CNN on a classification objective requires a sufficiently large amount of defective data, which is often not available. In this paper, we address that challenge by training the CNN on surface texture patches with a distance-based anomaly detection objective instead. A deep residual-based triplet network model is utilized, and defective training samples are synthesized exclusively from non-defective samples via random erasing techniques to directly learn a similarity metric between the same-class samples and out-of-class samples. Evaluation results demonstrate the approach's strength in detecting different types of anomalies, such as bent, broken, or cracked surfaces, for known surfaces that are part of the training data and unseen novel surfaces.



### Localising In Complex Scenes Using Balanced Adversarial Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2011.04122v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04122v1)
- **Published**: 2020-11-09 00:40:50+00:00
- **Updated**: 2020-11-09 00:40:50+00:00
- **Authors**: Gil Avraham, Yan Zuo, Tom Drummond
- **Comment**: Accepted at 3DV 2020
- **Journal**: None
- **Summary**: Domain adaptation and generative modelling have collectively mitigated the expensive nature of data collection and labelling by leveraging the rich abundance of accurate, labelled data in simulation environments. In this work, we study the performance gap that exists between representations optimised for localisation on simulation environments and the application of such representations in a real-world setting. Our method exploits the shared geometric similarities between simulation and real-world environments whilst maintaining invariance towards visual discrepancies. This is achieved by optimising a representation extractor to project both simulated and real representations into a shared representation space. Our method uses a symmetrical adversarial approach which encourages the representation extractor to conceal the domain that features are extracted from and simultaneously preserves robust attributes between source and target domains that are beneficial for localisation. We evaluate our method by adapting representations optimised for indoor Habitat simulated environments (Matterport3D and Replica) to a real-world indoor environment (Active Vision Dataset), showing that it compares favourably against fully-supervised approaches.



### Deep Learning based Monocular Depth Prediction: Datasets, Methods and Applications
- **Arxiv ID**: http://arxiv.org/abs/2011.04123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04123v1)
- **Published**: 2020-11-09 01:03:13+00:00
- **Updated**: 2020-11-09 01:03:13+00:00
- **Authors**: Qing Li, Jiasong Zhu, Jun Liu, Rui Cao, Qingquan Li, Sen Jia, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating depth from RGB images can facilitate many computer vision tasks, such as indoor localization, height estimation, and simultaneous localization and mapping (SLAM). Recently, monocular depth estimation has obtained great progress owing to the rapid development of deep learning techniques. They surpass traditional machine learning-based methods by a large margin in terms of accuracy and speed. Despite the rapid progress in this topic, there are lacking of a comprehensive review, which is needed to summarize the current progress and provide the future directions. In this survey, we first introduce the datasets for depth estimation, and then give a comprehensive introduction of the methods from three perspectives: supervised learning-based methods, unsupervised learning-based methods, and sparse samples guidance-based methods. In addition, downstream applications that benefit from the progress have also been illustrated. Finally, we point out the future directions and conclude the paper.



### Fine Perceptive GANs for Brain MR Image Super-Resolution in Wavelet Domain
- **Arxiv ID**: http://arxiv.org/abs/2011.04145v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04145v1)
- **Published**: 2020-11-09 02:09:44+00:00
- **Updated**: 2020-11-09 02:09:44+00:00
- **Authors**: Senrong You, Yong Liu, Baiying Lei, Shuqiang Wang
- **Comment**: 9 pages, 11figures
- **Journal**: None
- **Summary**: Magnetic resonance imaging plays an important role in computer-aided diagnosis and brain exploration. However, limited by hardware, scanning time and cost, it's challenging to acquire high-resolution (HR) magnetic resonance (MR) image clinically. In this paper, fine perceptive generative adversarial networks (FP-GANs) is proposed to produce HR MR images from low-resolution counterparts. It can cope with the detail insensitive problem of the existing super-resolution model in a divide-and-conquer manner. Specifically, FP-GANs firstly divides an MR image into low-frequency global approximation and high-frequency anatomical texture in wavelet domain. Then each sub-band generative adversarial network (sub-band GAN) conquers the super-resolution procedure of each single sub-band image. Meanwhile, sub-band attention is deployed to tune focus between global and texture information. It can focus on sub-band images instead of feature maps to further enhance the anatomical reconstruction ability of FP-GANs. In addition, inverse discrete wavelet transformation (IDWT) is integrated into model for taking the reconstruction of whole image into account. Experiments on MultiRes_7T dataset demonstrate that FP-GANs outperforms the competing methods quantitatively and qualitatively.



### Multi-modal, multi-task, multi-attention (M3) deep learning detection of reticular pseudodrusen: towards automated and accessible classification of age-related macular degeneration
- **Arxiv ID**: http://arxiv.org/abs/2011.05142v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05142v2)
- **Published**: 2020-11-09 03:26:38+00:00
- **Updated**: 2020-11-11 13:26:39+00:00
- **Authors**: Qingyu Chen, Tiarnan D. L. Keenan, Alexis Allot, Yifan Peng, Elvira Agrón, Amitha Domalpally, Caroline C. W. Klaver, Daniel T. Luttikhuizen, Marcus H. Colyer, Catherine A. Cukras, Henry E. Wiley, M. Teresa Magone, Chantal Cousineau-Krieger, Wai T. Wong, Yingying Zhu, Emily Y. Chew, Zhiyong Lu
- **Comment**: 5 figures and 4 tables, To appear in Journal of the American Medical
  Informatics Association
- **Journal**: None
- **Summary**: Objective Reticular pseudodrusen (RPD), a key feature of age-related macular degeneration (AMD), are poorly detected by human experts on standard color fundus photography (CFP) and typically require advanced imaging modalities such as fundus autofluorescence (FAF). The objective was to develop and evaluate the performance of a novel 'M3' deep learning framework on RPD detection. Materials and Methods A deep learning framework M3 was developed to detect RPD presence accurately using CFP alone, FAF alone, or both, employing >8000 CFP-FAF image pairs obtained prospectively (Age-Related Eye Disease Study 2). The M3 framework includes multi-modal (detection from single or multiple image modalities), multi-task (training different tasks simultaneously to improve generalizability), and multi-attention (improving ensembled feature representation) operation. Performance on RPD detection was compared with state-of-the-art deep learning models and 13 ophthalmologists; performance on detection of two other AMD features (geographic atrophy and pigmentary abnormalities) was also evaluated. Results For RPD detection, M3 achieved area under receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP alone, FAF alone, and both, respectively. M3 performance on CFP was very substantially superior to human retinal specialists (median F1-score 0.644 versus 0.350). External validation (on Rotterdam Study, Netherlands) demonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also accurately detected geographic atrophy and pigmentary abnormalities (AUROC 0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion This study demonstrates the successful development, robust evaluation, and external validation of a novel deep learning framework that enables accessible, accurate, and automated AMD diagnosis and prognosis.



### Geometric Structure Aided Visual Inertial Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.04173v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04173v1)
- **Published**: 2020-11-09 03:48:39+00:00
- **Updated**: 2020-11-09 03:48:39+00:00
- **Authors**: Huaiyang Huang, Haoyang Ye, Jianhao Jiao, Yuxiang Sun, Ming Liu
- **Comment**: submitted to ICRA2021
- **Journal**: None
- **Summary**: Visual Localization is an essential component in autonomous navigation. Existing approaches are either based on the visual structure from SLAM/SfM or the geometric structure from dense mapping. To take the advantages of both, in this work, we present a complete visual inertial localization system based on a hybrid map representation to reduce the computational cost and increase the positioning accuracy. Specially, we propose two modules for data association and batch optimization, respectively. To this end, we develop an efficient data association module to associate map components with local features, which takes only $2$ms to generate temporal landmarks. For batch optimization, instead of using visual factors, we develop a module to estimate a pose prior from the instant localization results to constrain poses. The experimental results on the EuRoC MAV dataset demonstrate a competitive performance compared to the state of the arts. Specially, our system achieves an average position error in 1.7 cm with 100% recall. The timings show that the proposed modules reduce the computational cost by 20-30%. We will make our implementation open source at http://github.com/hyhuang1995/gmmloc.



### Two-Stream Appearance Transfer Network for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.04181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04181v1)
- **Published**: 2020-11-09 04:21:02+00:00
- **Updated**: 2020-11-09 04:21:02+00:00
- **Authors**: Chengkang Shen, Peiyan Wang, Wei Tang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Pose guided person image generation means to generate a photo-realistic person image conditioned on an input person image and a desired pose. This task requires spatial manipulation of the source image according to the target pose. However, the generative adversarial networks (GANs) widely used for image generation and translation rely on spatially local and translation equivariant operators, i.e., convolution, pooling and unpooling, which cannot handle large image deformation. This paper introduces a novel two-stream appearance transfer network (2s-ATN) to address this challenge. It is a multi-stage architecture consisting of a source stream and a target stream. Each stage features an appearance transfer module and several two-stream feature fusion modules. The former finds the dense correspondence between the two-stream feature maps and then transfers the appearance information from the source stream to the target stream. The latter exchange local information between the two streams and supplement the non-local appearance transfer. Both quantitative and qualitative results indicate the proposed 2s-ATN can effectively handle large spatial deformation and occlusion while retaining the appearance details. It outperforms prior states of the art on two widely used benchmarks.



### Detecting Outliers with Foreign Patch Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2011.04197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04197v2)
- **Published**: 2020-11-09 05:26:38+00:00
- **Updated**: 2022-04-13 13:04:07+00:00
- **Authors**: Jeremy Tan, Benjamin Hou, James Batten, Huaqi Qiu, Bernhard Kainz
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org
- **Journal**: None
- **Summary**: In medical imaging, outliers can contain hypo/hyper-intensities, minor deformations, or completely altered anatomy. To detect these irregularities it is helpful to learn the features present in both normal and abnormal images. However this is difficult because of the wide range of possible abnormalities and also the number of ways that normal anatomy can vary naturally. As such, we leverage the natural variations in normal anatomy to create a range of synthetic abnormalities. Specifically, the same patch region is extracted from two independent samples and replaced with an interpolation between both patches. The interpolation factor, patch size, and patch location are randomly sampled from uniform distributions. A wide residual encoder decoder is trained to give a pixel-wise prediction of the patch and its interpolation factor. This encourages the network to learn what features to expect normally and to identify where foreign patterns have been introduced. The estimate of the interpolation factor lends itself nicely to the derivation of an outlier score. Meanwhile the pixel-wise output allows for pixel- and subject- level predictions using the same model.



### PAMS: Quantized Super-Resolution via Parameterized Max Scale
- **Arxiv ID**: http://arxiv.org/abs/2011.04212v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04212v1)
- **Published**: 2020-11-09 06:16:05+00:00
- **Updated**: 2020-11-09 06:16:05+00:00
- **Authors**: Huixia Li, Chenqian Yan, Shaohui Lin, Xiawu Zheng, Yuchao Li, Baochang Zhang, Fan Yang, Rongrong Ji
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have shown dominant performance in the task of super-resolution (SR). However, their heavy memory cost and computation overhead significantly restrict their practical deployments on resource-limited devices, which mainly arise from the floating-point storage and operations between weights and activations. Although previous endeavors mainly resort to fixed-point operations, quantizing both weights and activations with fixed coding lengths may cause significant performance drop, especially on low bits. Specifically, most state-of-the-art SR models without batch normalization have a large dynamic quantization range, which also serves as another cause of performance drop. To address these two issues, we propose a new quantization scheme termed PArameterized Max Scale (PAMS), which applies the trainable truncated parameter to explore the upper bound of the quantization range adaptively. Finally, a structured knowledge transfer (SKT) loss is introduced to fine-tune the quantized network. Extensive experiments demonstrate that the proposed PAMS scheme can well compress and accelerate the existing SR models such as EDSR and RDN. Notably, 8-bit PAMS-EDSR improves PSNR on Set5 benchmark from 32.095dB to 32.124dB with 2.42$\times$ compression ratio, which achieves a new state-of-the-art.



### An improved helmet detection method for YOLOv3 on an unbalanced dataset
- **Arxiv ID**: http://arxiv.org/abs/2011.04214v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04214v2)
- **Published**: 2020-11-09 06:17:30+00:00
- **Updated**: 2020-12-01 02:39:21+00:00
- **Authors**: Rui Geng, Yixuan Ma, Wanhong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The YOLOv3 target detection algorithm is widely used in industry due to its high speed and high accuracy, but it has some limitations, such as the accuracy degradation of unbalanced datasets. The YOLOv3 target detection algorithm is based on a Gaussian fuzzy data augmentation approach to pre-process the data set and improve the YOLOv3 target detection algorithm. Through the efficient pre-processing, the confidence level of YOLOv3 is generally improved by 0.01-0.02 without changing the recognition speed of YOLOv3, and the processed images also perform better in image localization due to effective feature fusion, which is more in line with the requirement of recognition speed and accuracy in production.



### EPSR: Edge Profile Super resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.05308v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05308v3)
- **Published**: 2020-11-09 06:58:07+00:00
- **Updated**: 2021-05-12 08:49:25+00:00
- **Authors**: Jiun Lee, Jaekwang Kim, Inyong Yun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Edge Profile Super Resolution(EPSR) method to preserve structure information and to restore texture. We make EPSR by stacking modified Fractal Residual Network(mFRN) structures hierarchically and repeatedly. mFRN is made up of lots of Residual Edge Profile Blocks(REPBs) consisting of three different modules such as Residual Efficient Channel Attention Block(RECAB) module, Edge Profile(EP) module, and Context Network(CN) module. RECAB produces more informative features with high frequency components. From the feature, EP module produce structure informed features by generating edge profile itself. Finally, CN module captures details by exploiting high frequency information such as texture and structure with proper sharpness. As repeating the procedure in mFRN structure, our EPSR could extract high-fidelity features and thus it prevents texture loss and preserves structure with appropriate sharpness. Experimental results present that our EPSR achieves competitive performance against state-of-the-art methods in PSNR and SSIM evaluation metrics as well as visual results.



### End-to-end Lane Shape Prediction with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2011.04233v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.04233v2)
- **Published**: 2020-11-09 07:42:55+00:00
- **Updated**: 2020-11-28 10:55:44+00:00
- **Authors**: Ruijin Liu, Zejian Yuan, Tie Liu, Zhiliang Xiong
- **Comment**: 9 pages, 7 figures, accepted by WACV 2021
- **Journal**: None
- **Summary**: Lane detection, the process of identifying lane markings as approximated curves, is widely used for lane departure warning and adaptive cruise control in autonomous vehicles. The popular pipeline that solves it in two steps -- feature extraction plus post-processing, while useful, is too inefficient and flawed in learning the global context and lanes' long and thin structures. To tackle these issues, we propose an end-to-end method that directly outputs parameters of a lane shape model, using a network built with a transformer to learn richer structures and context. The lane shape model is formulated based on road structures and camera pose, providing physical interpretation for parameters of network output. The transformer models non-local interactions with a self-attention mechanism to capture slender structures and global context. The proposed method is validated on the TuSimple benchmark and shows state-of-the-art accuracy with the most lightweight model size and fastest speed. Additionally, our method shows excellent adaptability to a challenging self-collected lane detection dataset, showing its powerful deployment potential in real applications. Codes are available at https://github.com/liuruijin17/LSTR.



### Dual ResGCN for Balanced Scene GraphGeneration
- **Arxiv ID**: http://arxiv.org/abs/2011.04234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04234v1)
- **Published**: 2020-11-09 07:44:17+00:00
- **Updated**: 2020-11-09 07:44:17+00:00
- **Authors**: Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yanbo Fan, Fumin Shen, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Visual scene graph generation is a challenging task. Previous works have achieved great progress, but most of them do not explicitly consider the class imbalance issue in scene graph generation. Models learned without considering the class imbalance tend to predict the majority classes, which leads to a good performance on trivial frequent predicates, but poor performance on informative infrequent predicates. However, predicates of minority classes often carry more semantic and precise information~(\textit{e.g.}, \emph{`on'} v.s \emph{`parked on'}). % which leads to a good score of recall, but a poor score of mean recall. To alleviate the influence of the class imbalance, we propose a novel model, dubbed \textit{dual ResGCN}, which consists of an object residual graph convolutional network and a relation residual graph convolutional network. The two networks are complementary to each other. The former captures object-level context information, \textit{i.e.,} the connections among objects. We propose a novel ResGCN that enhances object features in a cross attention manner. Besides, we stack multiple contextual coefficients to alleviate the imbalance issue and enrich the prediction diversity. The latter is carefully designed to explicitly capture relation-level context information \textit{i.e.,} the connections among relations. We propose to incorporate the prior about the co-occurrence of relation pairs into the graph to further help alleviate the class imbalance issue. Extensive evaluations of three tasks are performed on the large-scale database VG to demonstrate the superiority of the proposed method.



### Real-time object detection method based on improved YOLOv4-tiny
- **Arxiv ID**: http://arxiv.org/abs/2011.04244v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.04244v2)
- **Published**: 2020-11-09 08:26:28+00:00
- **Updated**: 2020-12-02 09:19:32+00:00
- **Authors**: Zicong Jiang, Liquan Zhao, Shuaiyang Li, Yanfei Jia
- **Comment**: 14pages,7figures,2tables
- **Journal**: Journal of Network Intelligence, Volume 7, Number 1, February 2022
- **Summary**: The "You only look once v4"(YOLOv4) is one type of object detection methods in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on YOLOv4-tiny. It firstly uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved YOLOv4-tiny. Simulation results show that the proposed method has faster object detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of average precision as the YOLOv4-tiny. It is more suitable for real-time object detection.



### Improved Soccer Action Spotting using both Audio and Video Streams
- **Arxiv ID**: http://arxiv.org/abs/2011.04258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04258v1)
- **Published**: 2020-11-09 09:12:44+00:00
- **Updated**: 2020-11-09 09:12:44+00:00
- **Authors**: Bastien Vanderplaetse, Stéphane Dupont
- **Comment**: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops, 2020, pp. 896-897
- **Journal**: None
- **Summary**: In this paper, we propose a study on multi-modal (audio and video) action spotting and classification in soccer videos. Action spotting and classification are the tasks that consist in finding the temporal anchors of events in a video and determine which event they are. This is an important application of general activity understanding. Here, we propose an experimental study on combining audio and video information at different stages of deep neural network architectures. We used the SoccerNet benchmark dataset, which contains annotated events for 500 soccer game videos from the Big Five European leagues. Through this work, we evaluated several ways to integrate audio stream into video-only-based architectures. We observed an average absolute improvement of the mean Average Precision (mAP) metric of $7.43\%$ for the action classification task and of $4.19\%$ for the action spotting task.



### Robust Visual Tracking via Statistical Positive Sample Generation and Gradient Aware Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.04260v1
- **DOI**: 10.1145/3338533.3366556
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04260v1)
- **Published**: 2020-11-09 09:14:58+00:00
- **Updated**: 2020-11-09 09:14:58+00:00
- **Authors**: Lijian Lin, Haosheng Chen, Yanjie Liang, Yan Yan, Hanzi Wang
- **Comment**: 6 pages
- **Journal**: ACM MM Asia2019
- **Summary**: In recent years, Convolutional Neural Network (CNN) based trackers have achieved state-of-the-art performance on multiple benchmark datasets. Most of these trackers train a binary classifier to distinguish the target from its background. However, they suffer from two limitations. Firstly, these trackers cannot effectively handle significant appearance variations due to the limited number of positive samples. Secondly, there exists a significant imbalance of gradient contributions between easy and hard samples, where the easy samples usually dominate the computation of gradient. In this paper, we propose a robust tracking method via Statistical Positive sample generation and Gradient Aware learning (SPGA) to address the above two limitations. To enrich the diversity of positive samples, we present an effective and efficient statistical positive sample generation algorithm to generate positive samples in the feature space. Furthermore, to handle the issue of imbalance between easy and hard samples, we propose a gradient sensitive loss to harmonize the gradient contributions between easy and hard samples. Extensive experiments on three challenging benchmark datasets including OTB50, OTB100 and VOT2016 demonstrate that the proposed SPGA performs favorably against several state-of-the-art trackers.



### Unified Quality Assessment of In-the-Wild Videos with Mixed Datasets Training
- **Arxiv ID**: http://arxiv.org/abs/2011.04263v2
- **DOI**: 10.1007/s11263-020-01408-w
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04263v2)
- **Published**: 2020-11-09 09:22:57+00:00
- **Updated**: 2020-11-15 09:13:58+00:00
- **Authors**: Dingquan Li, Tingting Jiang, Ming Jiang
- **Comment**: 20 pages, 12 figures, 7 tables, accepted by IJCV. This is the version
  provided to IJCV office
- **Journal**: None
- **Summary**: Video quality assessment (VQA) is an important problem in computer vision. The videos in computer vision applications are usually captured in the wild. We focus on automatically assessing the quality of in-the-wild videos, which is a challenging problem due to the absence of reference videos, the complexity of distortions, and the diversity of video contents. Moreover, the video contents and distortions among existing datasets are quite different, which leads to poor performance of data-driven methods in the cross-dataset evaluation setting. To improve the performance of quality assessment models, we borrow intuitions from human perception, specifically, content dependency and temporal-memory effects of human visual system. To face the cross-dataset evaluation challenge, we explore a mixed datasets training strategy for training a single VQA model with multiple datasets. The proposed unified framework explicitly includes three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment, to jointly predict relative quality, perceptual quality, and subjective quality. Experiments are conducted on four publicly available datasets for VQA in the wild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental results verify the effectiveness of the mixed datasets training strategy and prove the superior performance of the unified model in comparison with the state-of-the-art models. For reproducible research, we make the PyTorch implementation of our method available at https://github.com/lidq92/MDTVSFA.



### CapWAP: Captioning with a Purpose
- **Arxiv ID**: http://arxiv.org/abs/2011.04264v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04264v1)
- **Published**: 2020-11-09 09:23:55+00:00
- **Updated**: 2020-11-09 09:23:55+00:00
- **Authors**: Adam Fisch, Kenton Lee, Ming-Wei Chang, Jonathan H. Clark, Regina Barzilay
- **Comment**: EMNLP 2020
- **Journal**: None
- **Summary**: The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with a Purpose (CapWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs---a natural expression of information need---from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CapWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.



### A Broad Dataset is All You Need for One-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.04267v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04267v2)
- **Published**: 2020-11-09 09:31:17+00:00
- **Updated**: 2022-10-29 14:58:30+00:00
- **Authors**: Claudio Michaelis, Matthias Bethge, Alexander S. Ecker
- **Comment**: None
- **Journal**: None
- **Summary**: Is it possible to detect arbitrary objects from a single example? A central problem of all existing attempts at one-shot object detection is the generalization gap: Object categories used during training are detected much more reliably than novel ones. We here show that this generalization gap can be nearly closed by increasing the number of object categories used during training. Doing so allows us to improve generalization from seen to unseen classes from 45% to 89% and improve the state-of-the-art on COCO by 5.4 %AP50 (from 22.0 to 27.5). We verify that the effect is caused by the number of categories and not the number of training samples, and that it holds for different models, backbones and datasets. This result suggests that the key to strong few-shot detection models may not lie in sophisticated metric learning approaches, but instead simply in scaling the number of categories. We hope that our findings will help to better understand the challenges of few-shot learning and encourage future data annotation efforts to focus on wider datasets with a broader set of categories rather than gathering more samples per category.



### Sketch-Inspector: a Deep Mixture Model for High-Quality Sketch Generation of Cats
- **Arxiv ID**: http://arxiv.org/abs/2011.04280v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2011.04280v1)
- **Published**: 2020-11-09 09:53:03+00:00
- **Updated**: 2020-11-09 09:53:03+00:00
- **Authors**: Yunkui Pang, Zhiqing Pan, Ruiyang Sun, Shuchong Wang
- **Comment**: 12 pages, 7 figures, ISVC 2020 accepted
- **Journal**: None
- **Summary**: With the involvement of artificial intelligence (AI), sketches can be automatically generated under certain topics. Even though breakthroughs have been made in previous studies in this area, a relatively high proportion of the generated figures are too abstract to recognize, which illustrates that AIs fail to learn the general pattern of the target object when drawing. This paper posits that supervising the process of stroke generation can lead to a more accurate sketch interpretation. Based on that, a sketch generating system with an assistant convolutional neural network (CNN) predictor to suggest the shape of the next stroke is presented in this paper. In addition, a CNN-based discriminator is introduced to judge the recognizability of the end product. Since the base-line model is ineffective at generating multi-class sketches, we restrict the model to produce one category. Because the image of a cat is easy to identify, we consider cat sketches selected from the QuickDraw data set. This paper compares the proposed model with the original Sketch-RNN on 75K human-drawn cat sketches. The result indicates that our model produces sketches with higher quality than human's sketches.



### Learning the Best Pooling Strategy for Visual Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/2011.04305v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04305v5)
- **Published**: 2020-11-09 10:22:35+00:00
- **Updated**: 2021-07-06 14:22:26+00:00
- **Authors**: Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, Changhu Wang
- **Comment**: CVPR 2021 camera-ready (oral). The new version fixes a few typos and
  updates citations
- **Journal**: None
- **Summary**: Visual Semantic Embedding (VSE) is a dominant approach for vision-language retrieval, which aims at learning a deep embedding space such that visual data are embedded close to their semantic text labels or descriptions. Recent VSE models use complex methods to better contextualize and aggregate multi-modal features into holistic embeddings. However, we discover that surprisingly simple (but carefully selected) global pooling functions (e.g., max pooling) outperform those complex models, across different feature extractors. Despite its simplicity and effectiveness, seeking the best pooling function for different data modality and feature extractor is costly and tedious, especially when the size of features varies (e.g., text, video). Therefore, we propose a Generalized Pooling Operator (GPO), which learns to automatically adapt itself to the best pooling strategy for different features, requiring no manual tuning while staying effective and efficient. We extend the VSE model using this proposed GPO and denote it as VSE$\infty$.   Without bells and whistles, VSE$\infty$ outperforms previous VSE methods significantly on image-text retrieval benchmarks across popular feature extractors. With a simple adaptation, variants of VSE$\infty$ further demonstrate its strength by achieving the new state of the art on two video-text retrieval datasets. Comprehensive experiments and visualizations confirm that GPO always discovers the best pooling strategy and can be a plug-and-play feature aggregation module for standard VSE models. Code and pre-trained models are available at https://vse-infty.github.io.



### EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach
- **Arxiv ID**: http://arxiv.org/abs/2011.04307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04307v2)
- **Published**: 2020-11-09 10:23:55+00:00
- **Updated**: 2020-11-18 10:47:33+00:00
- **Authors**: Yannick Bukschat, Marcus Vetter
- **Comment**: fixed typo
- **Journal**: None
- **Summary**: In this paper we introduce EfficientPose, a new approach for 6D object pose estimation. Our method is highly accurate, efficient and scalable over a wide range of computational resources. Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves a new state-of-the-art accuracy of 97.35% in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at https://github.com/ybkscht/EfficientPose.



### Patch-based field-of-view matching in multi-modal images for electroporation-based ablations
- **Arxiv ID**: http://arxiv.org/abs/2011.11759v1
- **DOI**: 10.1016/j.compmedimag.2020.101750
- **Categories**: **eess.IV**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2011.11759v1)
- **Published**: 2020-11-09 11:27:45+00:00
- **Updated**: 2020-11-09 11:27:45+00:00
- **Authors**: Luc Lafitte, Rémi Giraud, Cornel Zachiu, Mario Ries, Olivier Sutter, Antoine Petit, Olivier Seror, Clair Poignard, Baudouin Denis de Senneville
- **Comment**: 22 pages, 9 figures
- **Journal**: Computerized Medical Imaging and Graphics (2020)
- **Summary**: Various multi-modal imaging sensors are currently involved at different steps of an interventional therapeutic work-flow. Cone beam computed tomography (CBCT), computed tomography (CT) or Magnetic Resonance (MR) images thereby provides complementary functional and/or structural information of the targeted region and organs at risk. Merging this information relies on a correct spatial alignment of the observed anatomy between the acquired images. This can be achieved by the means of multi-modal deformable image registration (DIR), demonstrated to be capable of estimating dense and elastic deformations between images acquired by multiple imaging devices. However, due to the typically different field-of-view (FOV) sampled across the various imaging modalities, such algorithms may severely fail in finding a satisfactory solution.   In the current study we propose a new fast method to align the FOV in multi-modal 3D medical images. To this end, a patch-based approach is introduced and combined with a state-of-the-art multi-modal image similarity metric in order to cope with multi-modal medical images. The occurrence of estimated patch shifts is computed for each spatial direction and the shift value with maximum occurrence is selected and used to adjust the image field-of-view.   We show that a regional registration approach using voxel patches provides a good structural compromise between the voxel-wise and "global shifts" approaches. The method was thereby beneficial for CT to CBCT and MRI to CBCT registration tasks, especially when highly different image FOVs are involved. Besides, the benefit of the method for CT to CBCT and MRI to CBCT image registration is analyzed, including the impact of artifacts generated by percutaneous needle insertions. Additionally, the computational needs are demonstrated to be compatible with clinical constraints in the practical case of on-line procedures.



### MAGNeto: An Efficient Deep Learning Method for the Extractive Tags Summarization Problem
- **Arxiv ID**: http://arxiv.org/abs/2011.04349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.04349v1)
- **Published**: 2020-11-09 11:34:21+00:00
- **Updated**: 2020-11-09 11:34:21+00:00
- **Authors**: Hieu Trong Phung, Anh Tuan Vu, Tung Dinh Nguyen, Lam Thanh Do, Giang Nam Ngo, Trung Thanh Tran, Ngoc C. Lê
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study a new image annotation task named Extractive Tags Summarization (ETS). The goal is to extract important tags from the context lying in an image and its corresponding tags. We adjust some state-of-the-art deep learning models to utilize both visual and textual information. Our proposed solution consists of different widely used blocks like convolutional and self-attention layers, together with a novel idea of combining auxiliary loss functions and the gating mechanism to glue and elevate these fundamental components and form a unified architecture. Besides, we introduce a loss function that aims to reduce the imbalance of the training data and a simple but effective data augmentation technique dedicated to alleviates the effect of outliers on the final results. Last but not least, we explore an unsupervised pre-training strategy to further boost the performance of the model by making use of the abundant amount of available unlabeled data. Our model shows the good results as 90% $F_\text{1}$ score on the public NUS-WIDE benchmark, and 50% $F_\text{1}$ score on a noisy large-scale real-world private dataset. Source code for reproducing the experiments is publicly available at: https://github.com/pixta-dev/labteam



### An Empirical Study of Visual Features for DNN based Audio-Visual Speech Enhancement in Multi-talker Environments
- **Arxiv ID**: http://arxiv.org/abs/2011.04359v1
- **DOI**: 10.1109/ICASSP39728.2021.9414000
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04359v1)
- **Published**: 2020-11-09 11:48:14+00:00
- **Updated**: 2020-11-09 11:48:14+00:00
- **Authors**: Shrishti Saha Shetu, Soumitro Chakrabarty, Emanuël A. P. Habets
- **Comment**: None
- **Journal**: None
- **Summary**: Audio-visual speech enhancement (AVSE) methods use both audio and visual features for the task of speech enhancement and the use of visual features has been shown to be particularly effective in multi-speaker scenarios. In the majority of deep neural network (DNN) based AVSE methods, the audio and visual data are first processed separately using different sub-networks, and then the learned features are fused to utilize the information from both modalities. There have been various studies on suitable audio input features and network architectures, however, to the best of our knowledge, there is no published study that has investigated which visual features are best suited for this specific task. In this work, we perform an empirical study of the most commonly used visual features for DNN based AVSE, the pre-processing requirements for each of these features, and investigate their influence on the performance. Our study shows that despite the overall better performance of embedding-based features, their computationally intensive pre-processing make their use difficult in low resource systems. For such systems, optical flow or raw pixels-based features might be better suited.



### EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes
- **Arxiv ID**: http://arxiv.org/abs/2011.04389v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04389v2)
- **Published**: 2020-11-09 12:44:29+00:00
- **Updated**: 2020-11-10 20:11:31+00:00
- **Authors**: Hoang-An Le, Thomas Mensink, Partha Das, Sezer Karaoglu, Theo Gevers
- **Comment**: Accepted for publishing at WACV 2021
- **Journal**: None
- **Summary**: Multimodal large-scale datasets for outdoor scenes are mostly designed for urban driving problems. The scenes are highly structured and semantically different from scenarios seen in nature-centered scenes such as gardens or parks. To promote machine learning methods for nature-oriented applications, such as agriculture and gardening, we propose the multimodal synthetic dataset for Enclosed garDEN scenes (EDEN). The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Experimental results on the state-of-the-art methods for semantic segmentation and monocular depth prediction, two important tasks in computer vision, show positive impact of pre-training deep networks on our dataset for unstructured natural scenes. The dataset and related materials will be available at https://lhoangan.github.io/eden.



### SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments
- **Arxiv ID**: http://arxiv.org/abs/2011.04408v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04408v7)
- **Published**: 2020-11-09 13:24:45+00:00
- **Updated**: 2023-07-17 23:30:43+00:00
- **Authors**: Hanjiang Hu, Baoquan Yang, Zhijian Qiao, Shiqi Liu, Jiacheng Zhu, Zuxin Liu, Wenhao Ding, Ding Zhao, Hesheng Wang
- **Comment**: Accepted by IROS 2023, 23 pages, 13 figures, 10 tables
- **Journal**: None
- **Summary**: Different environments pose a great challenge to the outdoor robust visual perception for long-term autonomous driving, and the generalization of learning-based algorithms on different environments is still an open problem. Although monocular depth prediction has been well studied recently, few works focus on the robustness of learning-based depth prediction across different environments, e.g. changing illumination and seasons, owing to the lack of such a multi-environment real-world dataset and benchmark. To this end, the first cross-season monocular depth prediction dataset and benchmark, SeasonDepth, is introduced to benchmark the depth estimation performance under different environments. We investigate several state-of-the-art representative open-source supervised and self-supervised depth prediction methods using newly-formulated metrics. Through extensive experimental evaluation on the proposed dataset and cross-dataset evaluation with current autonomous driving datasets, the performance and robustness against the influence of multiple environments are analyzed qualitatively and quantitatively. We show that long-term monocular depth prediction is still challenging and believe our work can boost further research on the long-term robustness and generalization for outdoor visual perception. The dataset is available on https://seasondepth.github.io, and the benchmark toolkit is available on https://github.com/ SeasonDepth/SeasonDepth.



### FACEGAN: Facial Attribute Controllable rEenactment GAN
- **Arxiv ID**: http://arxiv.org/abs/2011.04439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04439v1)
- **Published**: 2020-11-09 14:04:15+00:00
- **Updated**: 2020-11-09 14:04:15+00:00
- **Authors**: Soumya Tripathy, Juho Kannala, Esa Rahtu
- **Comment**: Accepted to WACV-2021
- **Journal**: None
- **Summary**: The face reenactment is a popular facial animation method where the person's identity is taken from the source image and the facial motion from the driving image. Recent works have demonstrated high quality results by combining the facial landmark based motion representations with the generative adversarial networks. These models perform best if the source and driving images depict the same person or if the facial structures are otherwise very similar. However, if the identity differs, the driving facial structures leak to the output distorting the reenactment result. We propose a novel Facial Attribute Controllable rEenactment GAN (FACEGAN), which transfers the facial motion from the driving face via the Action Unit (AU) representation. Unlike facial landmarks, the AUs are independent of the facial structure preventing the identity leak. Moreover, AUs provide a human interpretable way to control the reenactment. FACEGAN processes background and face regions separately for optimized output quality. The extensive quantitative and qualitative comparisons show a clear improvement over the state-of-the-art in a single source reenactment task. The results are best illustrated in the reenactment video provided in the supplementary material. The source code will be made available upon publication of the paper.



### TTVOS: Lightweight Video Object Segmentation with Adaptive Template Attention Module and Temporal Consistency Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.04445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04445v3)
- **Published**: 2020-11-09 14:09:54+00:00
- **Updated**: 2021-04-04 10:02:52+00:00
- **Authors**: Hyojin Park, Ganesh Venkatesh, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation (semi-VOS) is widely used in many applications. This task is tracking class-agnostic objects from a given target mask. For doing this, various approaches have been developed based on online-learning, memory networks, and optical flow. These methods show high accuracy but are hard to be utilized in real-world applications due to slow inference time and tremendous complexity. To resolve this problem, template matching methods are devised for fast processing speed but sacrificing lots of performance in previous models. We introduce a novel semi-VOS model based on a template matching method and a temporal consistency loss to reduce the performance gap from heavy models while expediting inference time a lot. Our template matching method consists of short-term and long-term matching. The short-term matching enhances target object localization, while long-term matching improves fine details and handles object shape-changing through the newly proposed adaptive template attention module. However, the long-term matching causes error-propagation due to the inflow of the past estimated results when updating the template. To mitigate this problem, we also propose a temporal consistency loss for better temporal coherence between neighboring frames by adopting the concept of a transition matrix. Our model obtains 79.5% J&F score at the speed of 73.8 FPS on the DAVIS16 benchmark. The code is available in https://github.com/HYOJINPARK/TTVOS.



### Neural Architecture Search with an Efficient Multiobjective Evolutionary Framework
- **Arxiv ID**: http://arxiv.org/abs/2011.04463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.04463v1)
- **Published**: 2020-11-09 14:41:10+00:00
- **Updated**: 2020-11-09 14:41:10+00:00
- **Authors**: Maria Baldeon Calisto, Susana Lai-Yuen
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning methods have become very successful at solving many complex tasks such as image classification and segmentation, speech recognition and machine translation. Nevertheless, manually designing a neural network for a specific problem is very difficult and time-consuming due to the massive hyperparameter search space, long training times, and lack of technical guidelines for the hyperparameter selection. Moreover, most networks are highly complex, task specific and over-parametrized. Recently, multiobjective neural architecture search (NAS) methods have been proposed to automate the design of accurate and efficient architectures. However, they only optimize either the macro- or micro-structure of the architecture requiring the unset hyperparameters to be manually defined, and do not use the information produced during the optimization process to increase the efficiency of the search. In this work, we propose EMONAS, an Efficient MultiObjective Neural Architecture Search framework for the automatic design of neural architectures while optimizing the network's accuracy and size. EMONAS is composed of a search space that considers both the macro- and micro-structure of the architecture, and a surrogate-assisted multiobjective evolutionary based algorithm that efficiently searches for the best hyperparameters using a Random Forest surrogate and guiding selection probabilities. EMONAS is evaluated on the task of 3D cardiac segmentation from the MICCAI ACDC challenge, which is crucial for disease diagnosis, risk evaluation, and therapy decision. The architecture found with EMONAS is ranked within the top 10 submissions of the challenge in all evaluation metrics, performing better or comparable to other approaches while reducing the search time by more than 50% and having considerably fewer number of parameters.



### A Poisson multi-Bernoulli mixture filter for coexisting point and extended targets
- **Arxiv ID**: http://arxiv.org/abs/2011.04464v2
- **DOI**: 10.1109/TSP.2021.3072006
- **Categories**: **stat.ME**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2011.04464v2)
- **Published**: 2020-11-09 14:41:40+00:00
- **Updated**: 2021-05-18 06:28:22+00:00
- **Authors**: Ángel F. García-Fernández, Jason L. Williams, Lennart Svensson, Yuxuan Xia
- **Comment**: Matlab files can be found at
  https://github.com/Agarciafernandez/Coexisting-point-extended-target-PMBM-filter
  and
  https://github.com/yuhsuansia/Coexisting-point-extended-target-PMBM-filter. A
  relevant multi-object tracking course can be found at
  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw
- **Journal**: in IEEE Transactions on Signal Processing, vol. 69, pp. 2600-2610,
  2021
- **Summary**: This paper proposes a Poisson multi-Bernoulli mixture (PMBM) filter for coexisting point and extended targets, i.e., for scenarios where there may be simultaneous point and extended targets. The PMBM filter provides a recursion to compute the multi-target filtering posterior based on probabilistic information on data associations, and single-target predictions and updates. In this paper, we first derive the PMBM filter update for a generalised measurement model, which can include measurements originated from point and extended targets. Second, we propose a single-target space that accommodates both point and extended targets and derive the filtering recursion that propagates Gaussian densities for point targets and gamma Gaussian inverse Wishart densities for extended targets. As a computationally efficient approximation of the PMBM filter, we also develop a Poisson multi-Bernoulli (PMB) filter for coexisting point and extended targets. The resulting filters are analysed via numerical simulations.



### DynaVSR: Dynamic Adaptive Blind Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.04482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04482v1)
- **Published**: 2020-11-09 15:07:32+00:00
- **Updated**: 2020-11-09 15:07:32+00:00
- **Authors**: Suyoung Lee, Myungsub Choi, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Most conventional supervised super-resolution (SR) algorithms assume that low-resolution (LR) data is obtained by downscaling high-resolution (HR) data with a fixed known kernel, but such an assumption often does not hold in real scenarios. Some recent blind SR algorithms have been proposed to estimate different downscaling kernels for each input LR image. However, they suffer from heavy computational overhead, making them infeasible for direct application to videos. In this work, we present DynaVSR, a novel meta-learning-based framework for real-world video SR that enables efficient downscaling model estimation and adaptation to the current input. Specifically, we train a multi-frame downscaling module with various types of synthetic blur kernels, which is seamlessly combined with a video SR network for input-aware adaptation. Experimental results show that DynaVSR consistently improves the performance of the state-of-the-art video SR models by a large margin, with an order of magnitude faster inference time compared to the existing blind SR approaches.



### A Fast Hybrid Cascade Network for Voxel-based 3D Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.04522v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04522v3)
- **Published**: 2020-11-09 15:58:33+00:00
- **Updated**: 2023-04-28 02:30:13+00:00
- **Authors**: Ji Luo, Hui Cao, Jie Wang, Siyu Zhang, Shen Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Voxel-based 3D object classification has been thoroughly studied in recent years. Most previous methods convert the classic 2D convolution into a 3D form that will be further applied to objects with binary voxel representation for classification. However, the binary voxel representation is not very effective for 3D convolution in many cases. In this paper, we propose a hybrid cascade architecture for voxel-based 3D object classification. It consists of three stages composed of fully connected and convolutional layers, dealing with easy, moderate, and hard 3D models respectively. Both accuracy and speed can be balanced in our proposed method. By giving each voxel a signed distance value, an obvious gain regarding the accuracy can be observed. Besides, the mean inference time can be speeded up hugely compared with the state-of-the-art point cloud and voxel based methods.



### MinkLoc3D: Point Cloud Based Large-Scale Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.04530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04530v1)
- **Published**: 2020-11-09 16:11:52+00:00
- **Updated**: 2020-11-09 16:11:52+00:00
- **Authors**: Jacek Komorowski
- **Comment**: Winter Conference on Applications of Computer Vision (WACV) 2021.
  Project web site: https://github.com/jac99/MinkLoc3D
- **Journal**: None
- **Summary**: The paper presents a learning-based method for computing a discriminative 3D point cloud descriptor for place recognition purposes. Existing methods, such as PointNetVLAD, are based on unordered point cloud representation. They use PointNet as the first processing step to extract local features, which are later aggregated into a global descriptor. The PointNet architecture is not well suited to capture local geometric structures. Thus, state-of-the-art methods enhance vanilla PointNet architecture by adding different mechanism to capture local contextual information, such as graph convolutional networks or using hand-crafted features. We present an alternative approach, dubbed MinkLoc3D, to compute a discriminative 3D point cloud descriptor, based on a sparse voxelized point cloud representation and sparse 3D convolutions. The proposed method has a simple and efficient architecture. Evaluation on standard benchmarks proves that MinkLoc3D outperforms current state-of-the-art. Our code is publicly available on the project website: https://github.com/jac99/MinkLoc3D



### Learning to Localize in New Environments from Synthetic Training Data
- **Arxiv ID**: http://arxiv.org/abs/2011.04539v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04539v2)
- **Published**: 2020-11-09 16:19:35+00:00
- **Updated**: 2021-06-21 08:34:34+00:00
- **Authors**: Dominik Winkelbauer, Maximilian Denninger, Rudolph Triebel
- **Comment**: 7 pages, 3 figures; in Proceedings of the IEEE International
  Conference on Robotics and Automation (ICRA), 2021
- **Journal**: None
- **Summary**: Most existing approaches for visual localization either need a detailed 3D model of the environment or, in the case of learning-based methods, must be retrained for each new scene. This can either be very expensive or simply impossible for large, unknown environments, for example in search-and-rescue scenarios. Although there are learning-based approaches that operate scene-agnostically, the generalization capability of these methods is still outperformed by classical approaches. In this paper, we present an approach that can generalize to new scenes by applying specific changes to the model architecture, including an extended regression part, the use of hierarchical correlation layers, and the exploitation of scale and uncertainty information. Our approach outperforms the 5-point algorithm using SIFT features on equally big images and additionally surpasses all previous learning-based approaches that were trained on different data. It is also superior to most of the approaches that were specifically trained on the respective scenes. We also evaluate our approach in a scenario where only very few reference images are available, showing that under such more realistic conditions our learning-based approach considerably exceeds both existing learning-based and classical methods.



### Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts
- **Arxiv ID**: http://arxiv.org/abs/2011.04554v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04554v1)
- **Published**: 2020-11-09 16:53:54+00:00
- **Updated**: 2020-11-09 16:53:54+00:00
- **Authors**: Ece Takmaz, Mario Giulianelli, Sandro Pezzelle, Arabella Sinclair, Raquel Fernández
- **Comment**: In Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2020)
- **Journal**: None
- **Summary**: Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.



### Masked Face Image Classification with Sparse Representation based on Majority Voting Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2011.04556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04556v1)
- **Published**: 2020-11-09 16:55:14+00:00
- **Updated**: 2020-11-09 16:55:14+00:00
- **Authors**: Han Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse approximation is the problem to find the sparsest linear combination for a signal from a redundant dictionary, which is widely applied in signal processing and compressed sensing. In this project, I manage to implement the Orthogonal Matching Pursuit (OMP) algorithm and Sparse Representation-based Classification (SRC) algorithm, then use them to finish the task of masked image classification with majority voting. Here the experiment was token on the AR data-set, and the result shows the superiority of OMP algorithm combined with SRC algorithm over masked face image classification with an accuracy of 98.4%.



### MPRNet: Multi-Path Residual Network for Lightweight Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.04566v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04566v1)
- **Published**: 2020-11-09 17:11:15+00:00
- **Updated**: 2020-11-09 17:11:15+00:00
- **Authors**: Armin Mehri, Parichehr B. Ardakani, Angel D. Sappa
- **Comment**: 10 pages, 5 figures, conference, accepted by WACV2021
- **Journal**: None
- **Summary**: Lightweight super resolution networks have extremely importance for real-world applications. In recent years several SR deep learning approaches with outstanding achievement have been introduced by sacrificing memory and computational cost. To overcome this problem, a novel lightweight super resolution network is proposed, which improves the SOTA performance in lightweight SR and performs roughly similar to computationally expensive networks. Multi-Path Residual Network designs with a set of Residual concatenation Blocks stacked with Adaptive Residual Blocks: ($i$) to adaptively extract informative features and learn more expressive spatial context information; ($ii$) to better leverage multi-level representations before up-sampling stage; and ($iii$) to allow an efficient information and gradient flow within the network. The proposed architecture also contains a new attention mechanism, Two-Fold Attention Module, to maximize the representation ability of the model. Extensive experiments show the superiority of our model against other SOTA SR approaches.



### Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.05317v1
- **DOI**: 10.3390/s21020455
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05317v1)
- **Published**: 2020-11-09 17:37:31+00:00
- **Updated**: 2020-11-09 17:37:31+00:00
- **Authors**: Hammam Alshazly, Christoph Linse, Erhardt Barth, Thomas Martinetz
- **Comment**: None
- **Journal**: Sensors - 2021
- **Summary**: This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopt advanced deep network architectures and propose a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conduct extensive sets of experiments on two CT image datasets, namely the SARS-CoV-2 CT-scan and the COVID19-CT. The obtained results show superior performances for our models compared with previous studies, where our best models achieve average accuracy, precision, sensitivity, specificity and F1 score of 99.4%, 99.6%, 99.8%, 99.6% and 99.4% on the SARS-CoV-2 dataset; and 92.9%, 91.3%, 93.7%, 92.2% and 92.5% on the COVID19-CT dataset, respectively. Furthermore, we apply two visualization techniques to provide visual explanations for the models' predictions. The visualizations show well-separated clusters for CT images of COVID-19 from other lung diseases, and accurate localizations of the COVID-19 associated regions.



### Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze
- **Arxiv ID**: http://arxiv.org/abs/2011.04592v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04592v1)
- **Published**: 2020-11-09 17:45:32+00:00
- **Updated**: 2020-11-09 17:45:32+00:00
- **Authors**: Ece Takmaz, Sandro Pezzelle, Lisa Beinborn, Raquel Fernández
- **Comment**: In Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2020)
- **Journal**: None
- **Summary**: When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled $\textit{sequentially}$. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural${-}$particularly when gaze is encoded with a dedicated recurrent component.



### Fast Fourier Intrinsic Network
- **Arxiv ID**: http://arxiv.org/abs/2011.04612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.04612v1)
- **Published**: 2020-11-09 18:14:39+00:00
- **Updated**: 2020-11-09 18:14:39+00:00
- **Authors**: Yanlin Qian, Miaojing Shi, Joni-Kristian Kämäräinen, Jiri Matas
- **Comment**: WACV 2021 - camera ready
- **Journal**: None
- **Summary**: We address the problem of decomposing an image into albedo and shading. We propose the Fast Fourier Intrinsic Network, FFI-Net in short, that operates in the spectral domain, splitting the input into several spectral bands. Weights in FFI-Net are optimized in the spectral domain, allowing faster convergence to a lower error. FFI-Net is lightweight and does not need auxiliary networks for training. The network is trained end-to-end with a novel spectral loss which measures the global distance between the network prediction and corresponding ground truth. FFI-Net achieves state-of-the-art performance on MPI-Sintel, MIT Intrinsic, and IIW datasets.



### Find it if You Can: End-to-End Adversarial Erasing for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.04626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04626v1)
- **Published**: 2020-11-09 18:35:35+00:00
- **Updated**: 2020-11-09 18:35:35+00:00
- **Authors**: Erik Stammes, Tom F. H. Runia, Michael Hofmann, Mohsen Ghafoorian
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Semantic segmentation is a task that traditionally requires a large dataset of pixel-level ground truth labels, which is time-consuming and expensive to obtain. Recent advancements in the weakly-supervised setting show that reasonable performance can be obtained by using only image-level labels. Classification is often used as a proxy task to train a deep neural network from which attention maps are extracted. However, the classification task needs only the minimum evidence to make predictions, hence it focuses on the most discriminative object regions. To overcome this problem, we propose a novel formulation of adversarial erasing of the attention maps. In contrast to previous adversarial erasing methods, we optimize two networks with opposing loss functions, which eliminates the requirement of certain suboptimal strategies; for instance, having multiple training steps that complicate the training process or a weight sharing policy between networks operating on different distributions that might be suboptimal for performance. The proposed solution does not require saliency masks, instead it uses a regularization loss to prevent the attention maps from spreading to less discriminative object regions. Our experiments on the Pascal VOC dataset demonstrate that our adversarial approach increases segmentation performance by 2.1 mIoU compared to our baseline and by 1.0 mIoU compared to previous adversarial erasing approaches.



### Ontology-driven Event Type Classification in Images
- **Arxiv ID**: http://arxiv.org/abs/2011.04714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04714v1)
- **Published**: 2020-11-09 19:43:55+00:00
- **Updated**: 2020-11-09 19:43:55+00:00
- **Authors**: Eric Müller-Budack, Matthias Springstein, Sherzod Hakimov, Kevin Mrutzek, Ralph Ewerth
- **Comment**: Accepted for publication in: IEEE Winter Conference on Applications
  of Computer Vision (WACV) 2021
- **Journal**: None
- **Summary**: Event classification can add valuable information for semantic search and the increasingly important topic of fact validation in news. So far, only few approaches address image classification for newsworthy event types such as natural disasters, sports events, or elections. Previous work distinguishes only between a limited number of event types and relies on rather small datasets for training. In this paper, we present a novel ontology-driven approach for the classification of event types in images. We leverage a large number of real-world news events to pursue two objectives: First, we create an ontology based on Wikidata comprising the majority of event types. Second, we introduce a novel large-scale dataset that was acquired through Web crawling. Several baselines are proposed including an ontology-driven learning approach that aims to exploit structured information of a knowledge graph to learn relevant event relations using deep neural networks. Experimental results on existing as well as novel benchmark datasets demonstrate the superiority of the proposed ontology-driven approach.



### Predicting the Future is like Completing a Painting!
- **Arxiv ID**: http://arxiv.org/abs/2011.04750v1
- **DOI**: 10.1109/ACCESS.2021.3101718
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04750v1)
- **Published**: 2020-11-09 20:48:06+00:00
- **Updated**: 2020-11-09 20:48:06+00:00
- **Authors**: Nadir Maaroufi, Mehdi Najib, Mohamed Bakhouya
- **Comment**: 25 pages, 12 figures
- **Journal**: IEEE Access ( Volume: 9), 2021, 119918 - 119938
- **Summary**: This article is an introductory work towards a larger research framework relative to Scientific Prediction. It is a mixed between science and philosophy of science, therefore we can talk about Experimental Philosophy of Science. As a first result, we introduce a new forecasting method based on image completion, named Forecasting Method by Image Inpainting (FM2I). In fact, time series forecasting is transformed into fully images- and signal-based processing procedures. After transforming a time series data into its corresponding image, the problem of data forecasting becomes essentially a problem of image inpainting problem, i.e., completing missing data in the image. An extensive experimental evaluation is conducted using a large dataset proposed by the well-known M3-competition. Results show that FM2I represents an efficient and robust tool for time series forecasting. It has achieved prominent results in terms of accuracy and outperforms the best M3 forecasting methods.



### Learning to Infer Semantic Parameters for 3D Shape Editing
- **Arxiv ID**: http://arxiv.org/abs/2011.04755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04755v1)
- **Published**: 2020-11-09 20:58:49+00:00
- **Updated**: 2020-11-09 20:58:49+00:00
- **Authors**: Fangyin Wei, Elena Sizikova, Avneesh Sud, Szymon Rusinkiewicz, Thomas Funkhouser
- **Comment**: 22 pages and 19 figures including supplementary material; to be
  published in the proceedings of 3DV 2020
- **Journal**: None
- **Summary**: Many applications in 3D shape design and augmentation require the ability to make specific edits to an object's semantic parameters (e.g., the pose of a person's arm or the length of an airplane's wing) while preserving as much existing details as possible. We propose to learn a deep network that infers the semantic parameters of an input shape and then allows the user to manipulate those parameters. The network is trained jointly on shapes from an auxiliary synthetic template and unlabeled realistic models, ensuring robustness to shape variability while relieving the need to label realistic exemplars. At testing time, edits within the parameter space drive deformations to be applied to the original shape, which provides semantically-meaningful manipulation while preserving the details. This is in contrast to prior methods that either use autoencoders with a limited latent-space dimensionality, failing to preserve arbitrary detail, or drive deformations with purely-geometric controls, such as cages, losing the ability to update local part regions. Experiments with datasets of chairs, airplanes, and human bodies demonstrate that our method produces more natural edits than prior work.



### MUSE: Textual Attributes Guided Portrait Painting Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.04761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.04761v2)
- **Published**: 2020-11-09 21:05:21+00:00
- **Updated**: 2021-09-20 01:47:31+00:00
- **Authors**: Xiaodan Hu, Pengfei Yu, Kevin Knight, Heng Ji, Bo Li, Honghui Shi
- **Comment**: Accepted by AIART 2021
- **Journal**: None
- **Summary**: We propose a novel approach, MUSE, to illustrate textual attributes visually via portrait generation. MUSE takes a set of attributes written in text, in addition to facial features extracted from a photo of the subject as input. We propose 11 attribute types to represent inspirations from a subject's profile, emotion, story, and environment. We propose a novel stacked neural network architecture by extending an image-to-image generative model to accept textual attributes. Experiments show that our approach significantly outperforms several state-of-the-art methods without using textual attributes, with Inception Score score increased by 6% and Fr\'echet Inception Distance (FID) score decreased by 11%, respectively. We also propose a new attribute reconstruction metric to evaluate whether the generated portraits preserve the subject's attributes. Experiments show that our approach can accurately illustrate 78% textual attributes, which also help MUSE capture the subject in a more creative and expressive way.



### Predicting Landsat Reflectance with Deep Generative Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.04762v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04762v1)
- **Published**: 2020-11-09 21:06:04+00:00
- **Updated**: 2020-11-09 21:06:04+00:00
- **Authors**: Shahine Bouabid, Maxim Chernetskiy, Maxime Rischard, Jevgenij Gamper
- **Comment**: None
- **Journal**: None
- **Summary**: Public satellite missions are commonly bound to a trade-off between spatial and temporal resolution as no single sensor provides fine-grained acquisitions with frequent coverage. This hinders their potential to assist vegetation monitoring or humanitarian actions, which require detecting rapid and detailed terrestrial surface changes. In this work, we probe the potential of deep generative models to produce high-resolution optical imagery by fusing products with different spatial and temporal characteristics. We introduce a dataset of co-registered Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat surface reflectance time series and demonstrate the ability of our generative model to blend coarse daily reflectance information into low-paced finer acquisitions. We benchmark our proposed model against state-of-the-art reflectance fusion algorithms.



### Deep Reinforcement Learning for Navigation in AAA Video Games
- **Arxiv ID**: http://arxiv.org/abs/2011.04764v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04764v2)
- **Published**: 2020-11-09 21:07:56+00:00
- **Updated**: 2020-11-17 19:09:57+00:00
- **Authors**: Eloi Alonso, Maxim Peter, David Goumard, Joshua Romoff
- **Comment**: Accepted to the NeurIPS 2020 Challenges of Real-World RL workshop
- **Journal**: None
- **Summary**: In video games, non-player characters (NPCs) are used to enhance the players' experience in a variety of ways, e.g., as enemies, allies, or innocent bystanders. A crucial component of NPCs is navigation, which allows them to move from one point to another on the map. The most popular approach for NPC navigation in the video game industry is to use a navigation mesh (NavMesh), which is a graph representation of the map, with nodes and edges indicating traversable areas. Unfortunately, complex navigation abilities that extend the character's capacity for movement, e.g., grappling hooks, jetpacks, teleportation, or double-jumps, increases the complexity of the NavMesh, making it intractable in many practical scenarios. Game designers are thus constrained to only add abilities that can be handled by a NavMesh if they want to have NPC navigation. As an alternative, we propose to use Deep Reinforcement Learning (Deep RL) to learn how to navigate 3D maps using any navigation ability. We test our approach on complex 3D environments in the Unity game engine that are notably an order of magnitude larger than maps typically used in the Deep RL literature. One of these maps is directly modeled after a Ubisoft AAA game. We find that our approach performs surprisingly well, achieving at least $90\%$ success rate on all tested scenarios. A video of our results is available at https://youtu.be/WFIf9Wwlq8M.



### Learnings from Frontier Development Lab and SpaceML -- AI Accelerators for NASA and ESA
- **Arxiv ID**: http://arxiv.org/abs/2011.04776v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04776v1)
- **Published**: 2020-11-09 21:23:03+00:00
- **Updated**: 2020-11-09 21:23:03+00:00
- **Authors**: Siddha Ganju, Anirudh Koul, Alexander Lavin, Josh Veitch-Michaelis, Meher Kasam, James Parr
- **Comment**: None
- **Journal**: None
- **Summary**: Research with AI and ML technologies lives in a variety of settings with often asynchronous goals and timelines: academic labs and government organizations pursue open-ended research focusing on discoveries with long-term value, while research in industry is driven by commercial pursuits and hence focuses on short-term timelines and return on investment. The journey from research to product is often tacit or ad hoc, resulting in technology transition failures, further exacerbated when research and development is interorganizational and interdisciplinary. Even more, much of the ability to produce results remains locked in the private repositories and know-how of the individual researcher, slowing the impact on future research by others and contributing to the ML community's challenges in reproducibility. With research organizations focused on an exploding array of fields, opportunities for the handover and maturation of interdisciplinary research reduce. With these tensions, we see an emerging need to measure the correctness, impact, and relevance of research during its development to enable better collaboration, improved reproducibility, faster progress, and more trusted outcomes. We perform a case study of the Frontier Development Lab (FDL), an AI accelerator under a public-private partnership from NASA and ESA. FDL research follows principled practices that are grounded in responsible development, conduct, and dissemination of AI research, enabling FDL to churn successful interdisciplinary and interorganizational research projects, measured through NASA's Technology Readiness Levels. We also take a look at the SpaceML Open Source Research Program, which helps accelerate and transition FDL's research to deployable projects with wide spread adoption amongst citizen scientists.



### After All, Only The Last Neuron Matters: Comparing Multi-modal Fusion Functions for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.04779v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.04779v1)
- **Published**: 2020-11-09 21:27:32+00:00
- **Updated**: 2020-11-09 21:27:32+00:00
- **Authors**: Mohamed Karim Belaid
- **Comment**: None
- **Journal**: None
- **Summary**: From object segmentation to word vector representations, Scene Graph Generation (SGG) became a complex task built upon numerous research results. In this paper, we focus on the last module of this model: the fusion function. The role of this latter is to combine three hidden states. We perform an ablation test in order to compare different implementations. First, we reproduce the state-of-the-art results using SUM, and GATE functions. Then we expand the original solution by adding more model-agnostic functions: an adapted version of DIST and a mixture between MFB and GATE. On the basis of the state-of-the-art configuration, DIST performed the best Recall @ K, which makes it now part of the state-of-the-art.




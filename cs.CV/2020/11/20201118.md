# Arxiv Papers in cs.CV on 2020-11-18
### AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling
- **Arxiv ID**: http://arxiv.org/abs/2011.09011v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09011v2)
- **Published**: 2020-11-18 00:15:23+00:00
- **Updated**: 2021-04-13 19:17:16+00:00
- **Authors**: Dilin Wang, Meng Li, Chengyue Gong, Vikas Chandra
- **Comment**: 2021 Conference on Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efficient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efficiency and accuracy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the final searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose AttentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose algorithms to efficiently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large number of networks across a wide range of FLOPs. Our discovered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outperforms SOTA models, including BigNAS and Once-for-All networks. We also achieve ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models are available at https://github.com/facebookresearch/AttentiveNAS.



### A Novel Memory-Efficient Deep Learning Training Framework via Error-Bounded Lossy Compression
- **Arxiv ID**: http://arxiv.org/abs/2011.09017v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09017v1)
- **Published**: 2020-11-18 00:47:21+00:00
- **Updated**: 2020-11-18 00:47:21+00:00
- **Authors**: Sian Jin, Guanpeng Li, Shuaiwen Leon Song, Dingwen Tao
- **Comment**: 11 pages, 11 figures, 1 table, accepted by PPoPP '21 as a poster
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are becoming increasingly deeper, wider, and non-linear due to the growing demands on prediction accuracy and analysis quality. When training a DNN model, the intermediate activation data must be saved in the memory during forward propagation and then restored for backward propagation. However, state-of-the-art accelerators such as GPUs are only equipped with very limited memory capacities due to hardware design constraints, which significantly limits the maximum batch size and hence performance speedup when training large-scale DNNs.   In this paper, we propose a novel memory-driven high performance DNN training framework that leverages error-bounded lossy compression to significantly reduce the memory requirement for training in order to allow training larger networks. Different from the state-of-the-art solutions that adopt image-based lossy compressors such as JPEG to compress the activation data, our framework purposely designs error-bounded lossy compression with a strict error-controlling mechanism. Specifically, we provide theoretical analysis on the compression error propagation from the altered activation data to the gradients, and then empirically investigate the impact of altered gradients over the entire training process. Based on these analyses, we then propose an improved lossy compressor and an adaptive scheme to dynamically configure the lossy compression error-bound and adjust the training batch size to further utilize the saved memory space for additional speedup. We evaluate our design against state-of-the-art solutions with four popular DNNs and the ImageNet dataset. Results demonstrate that our proposed framework can significantly reduce the training memory consumption by up to 13.5x and 1.8x over the baseline training and state-of-the-art framework with compression, respectively, with little or no accuracy loss.



### ADCPNet: Adaptive Disparity Candidates Prediction Network for Efficient Real-Time Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2011.09023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09023v1)
- **Published**: 2020-11-18 01:18:52+00:00
- **Updated**: 2020-11-18 01:18:52+00:00
- **Authors**: He Dai, Xuchong Zhang, Yongli Zhao, Hongbin Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient real-time disparity estimation is critical for the application of stereo vision systems in various areas. Recently, stereo network based on coarse-to-fine method has largely relieved the memory constraints and speed limitations of large-scale network models. Nevertheless, all of the previous coarse-to-fine designs employ constant offsets and three or more stages to progressively refine the coarse disparity map, still resulting in unsatisfactory computation accuracy and inference time when deployed on mobile devices. This paper claims that the coarse matching errors can be corrected efficiently with fewer stages as long as more accurate disparity candidates can be provided. Therefore, we propose a dynamic offset prediction module to meet different correction requirements of diverse objects and design an efficient two-stage framework. Besides, we propose a disparity-independent convolution to further improve the performance since it is more consistent with the local statistical characteristics of the compact cost volume. The evaluation results on multiple datasets and platforms clearly demonstrate that, the proposed network outperforms the state-of-the-art lightweight models especially for mobile devices in terms of accuracy and speed. Code will be made available.



### Your "Flamingo" is My "Bird": Fine-Grained, or Not
- **Arxiv ID**: http://arxiv.org/abs/2011.09040v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09040v3)
- **Published**: 2020-11-18 02:24:54+00:00
- **Updated**: 2021-03-28 06:17:51+00:00
- **Authors**: Dongliang Chang, Kaiyue Pang, Yixiao Zheng, Zhanyu Ma, Yi-Zhe Song, Jun Guo
- **Comment**: Accepted as an oral of CVPR2021. Code:
  https://github.com/PRIS-CV/Fine-Grained-or-Not
- **Journal**: None
- **Summary**: Whether what you see in Figure 1 is a "flamingo" or a "bird", is the question we ask in this paper. While fine-grained visual classification (FGVC) strives to arrive at the former, for the majority of us non-experts just "bird" would probably suffice. The real question is therefore -- how can we tailor for different fine-grained definitions under divergent levels of expertise. For that, we re-envisage the traditional setting of FGVC, from single-label classification, to that of top-down traversal of a pre-defined coarse-to-fine label hierarchy -- so that our answer becomes "bird"-->"Phoenicopteriformes"-->"Phoenicopteridae"-->"flamingo". To approach this new problem, we first conduct a comprehensive human study where we confirm that most participants prefer multi-granularity labels, regardless whether they consider themselves experts. We then discover the key intuition that: coarse-level label prediction exacerbates fine-grained feature learning, yet fine-level feature betters the learning of coarse-level classifier. This discovery enables us to design a very simple albeit surprisingly effective solution to our new problem, where we (i) leverage level-specific classification heads to disentangle coarse-level features with fine-grained ones, and (ii) allow finer-grained features to participate in coarser-grained label predictions, which in turn helps with better disentanglement. Experiments show that our method achieves superior performance in the new FGVC setting, and performs better than state-of-the-art on traditional single-label FGVC problem as well. Thanks to its simplicity, our method can be easily implemented on top of any existing FGVC frameworks and is parameter-free.



### SoftSeg: Advantages of soft versus binary training for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.09041v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09041v1)
- **Published**: 2020-11-18 02:25:09+00:00
- **Updated**: 2020-11-18 02:25:09+00:00
- **Authors**: Charley Gros, Andreanne Lemay, Julien Cohen-Adad
- **Comment**: None
- **Journal**: None
- **Summary**: Most image segmentation algorithms are trained on binary masks formulated as a classification task per pixel. However, in applications such as medical imaging, this "black-and-white" approach is too constraining because the contrast between two tissues is often ill-defined, i.e., the voxels located on objects' edges contain a mixture of tissues. Consequently, assigning a single "hard" label can result in a detrimental approximation. Instead, a soft prediction containing non-binary values would overcome that limitation. We introduce SoftSeg, a deep learning training approach that takes advantage of soft ground truth labels, and is not bound to binary predictions. SoftSeg aims at solving a regression instead of a classification problem. This is achieved by using (i) no binarization after preprocessing and data augmentation, (ii) a normalized ReLU final activation layer (instead of sigmoid), and (iii) a regression loss function (instead of the traditional Dice loss). We assess the impact of these three features on three open-source MRI segmentation datasets from the spinal cord gray matter, the multiple sclerosis brain lesion, and the multimodal brain tumor segmentation challenges. Across multiple cross-validation iterations, SoftSeg outperformed the conventional approach, leading to an increase in Dice score of 2.0% on the gray matter dataset (p=0.001), 3.3% for the MS lesions, and 6.5% for the brain tumors. SoftSeg produces consistent soft predictions at tissues' interfaces and shows an increased sensitivity for small objects. The richness of soft labels could represent the inter-expert variability, the partial volume effect, and complement the model uncertainty estimation. The developed training pipeline can easily be incorporated into most of the existing deep learning architectures. It is already implemented in the freely-available deep learning toolbox ivadomed (https://ivadomed.org).



### A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus
- **Arxiv ID**: http://arxiv.org/abs/2011.09046v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.09046v2)
- **Published**: 2020-11-18 02:42:36+00:00
- **Updated**: 2020-11-24 04:11:13+00:00
- **Authors**: Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, Fei Sha
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying a short segment in a long video that semantically matches a text query is a challenging task that has important application potentials in language-based video search, browsing, and navigation. Typical retrieval systems respond to a query with either a whole video or a pre-defined video segment, but it is challenging to localize undefined segments in untrimmed and unsegmented videos where exhaustively searching over all possible segments is intractable. The outstanding challenge is that the representation of a video must account for different levels of granularity in the temporal domain. To tackle this problem, we propose the HierArchical Multi-Modal EncodeR (HAMMER) that encodes a video at both the coarse-grained clip level and the fine-grained frame level to extract information at different scales based on multiple subtasks, namely, video retrieval, segment temporal localization, and masked language modeling. We conduct extensive experiments to evaluate our model on moment localization in video corpus on ActivityNet Captions and TVR datasets. Our approach outperforms the previous methods as well as strong baselines, establishing new state-of-the-art for this task.



### Visual Time Series Forecasting: An Image-driven Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.09052v3
- **DOI**: 10.1145/3490354.3494387
- **Categories**: **cs.CV**, cs.LG, econ.EM
- **Links**: [PDF](http://arxiv.org/pdf/2011.09052v3)
- **Published**: 2020-11-18 02:51:37+00:00
- **Updated**: 2021-11-19 19:38:46+00:00
- **Authors**: Srijan Sood, Zhen Zeng, Naftali Cohen, Tucker Balch, Manuela Veloso
- **Comment**: This work appeared in the 2nd ACM International Conference on AI in
  Finance (ICAIF-2021)
- **Journal**: None
- **Summary**: Time series forecasting is essential for agents to make decisions. Traditional approaches rely on statistical methods to forecast given past numeric values. In practice, end-users often rely on visualizations such as charts and plots to reason about their forecasts. Inspired by practitioners, we re-imagine the topic by creating a novel framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we leverage advances in deep learning to extend the field of time series forecasting to a visual setting. We capture input data as an image and train a model to produce the subsequent image. This approach results in predicting distributions as opposed to pointwise values. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, when using image-based evaluation metrics, we find the proposed visual forecasting method to outperform various numerical baselines, including ARIMA and a numerical variation of our method. We demonstrate the benefits of incorporating vision-based approaches in forecasting tasks -- both for the quality of the forecasts produced, as well as the metrics that can be used to evaluate them.



### Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2011.09055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09055v2)
- **Published**: 2020-11-18 02:57:47+00:00
- **Updated**: 2020-11-23 04:50:43+00:00
- **Authors**: Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, Shenghua Gao
- **Comment**: Under review of IEEE Transactions on Pattern Analysis and Machine
  Intelligence. arXiv admin note: text overlap with arXiv:1909.12224
- **Journal**: None
- **Summary**: We tackle human image synthesis, including human motion imitation, appearance transfer, and novel view synthesis, within a unified framework. It means that the model, once being trained, can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints to estimate the human body structure. However, they only express the position information with no abilities to characterize the personalized shape of the person and model the limb rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape. It can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block (AttLWB) that propagates the source information in both image and feature spaces to the synthesized reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method can support a more flexible warping from multiple sources. To further improve the generalization ability of the unseen source images, a one/few-shot adversarial learning is applied. In detail, it firstly trains a model in an extensive training set. Then, it finetunes the model by one/few-shot unseen image(s) in a self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024) results. Also, we build a new dataset, namely iPER dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our methods in terms of preserving face identity, shape consistency, and clothes details. All codes and dataset are available on https://impersonator.org/work/impersonator-plus-plus.html.



### Layer-Wise Data-Free CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/2011.09058v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09058v3)
- **Published**: 2020-11-18 03:00:05+00:00
- **Updated**: 2022-05-19 21:28:08+00:00
- **Authors**: Maxwell Horton, Yanzi Jin, Ali Farhadi, Mohammad Rastegari
- **Comment**: None
- **Journal**: None
- **Summary**: We present a computationally efficient method for compressing a trained neural network without using real data. We break the problem of data-free network compression into independent layer-wise compressions. We show how to efficiently generate layer-wise training data using only a pretrained network. We use this data to perform independent layer-wise compressions on the pretrained network. We also show how to precondition the network to improve the accuracy of our layer-wise compression method. We present results for layer-wise compression using quantization and pruning. When quantizing, we compress with higher accuracy than related works while using orders of magnitude less compute. When compressing MobileNetV2 and evaluating on ImageNet, our method outperforms existing methods for quantization at all bit-widths, achieving a $+0.34\%$ improvement in $8$-bit quantization, and a stronger improvement at lower bit-widths (up to a $+28.50\%$ improvement at $5$ bits). When pruning, we outperform baselines of a similar compute envelope, achieving $1.5$ times the sparsity rate at the same accuracy. We also show how to combine our efficient method with high-compute generative methods to improve upon their results.



### Towards Online Monitoring and Data-driven Control: A Study of Segmentation Algorithms for Laser Powder Bed Fusion Processes
- **Arxiv ID**: http://arxiv.org/abs/2011.09065v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09065v2)
- **Published**: 2020-11-18 03:30:16+00:00
- **Updated**: 2021-06-10 19:49:10+00:00
- **Authors**: Alexander Nettekoven, Scott Fish, Joseph Beaman, Ufuk Topcu
- **Comment**: None
- **Journal**: None
- **Summary**: An increasing number of laser powder bed fusion machines use off-axis infrared cameras to improve online monitoring and data-driven control capabilities. However, there is still a severe lack of algorithmic solutions to properly process the infrared images from these cameras that has led to several key limitations: a lack of online monitoring capabilities for the laser tracks, insufficient pre-processing of the infrared images for data-driven methods, and large memory requirements for storing the infrared images. To address these limitations, we study over 30 segmentation algorithms that segment each infrared image into a foreground and background. By evaluating each algorithm based on its segmentation accuracy, computational speed, and spatter detection characteristics, we identify promising algorithmic solutions. The identified algorithms can be readily applied to the laser powder bed fusion machines to address each of the above limitations and thus, significantly improve process control.



### Shaping Deep Feature Space towards Gaussian Mixture for Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.09066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09066v1)
- **Published**: 2020-11-18 03:32:27+00:00
- **Updated**: 2020-11-18 03:32:27+00:00
- **Authors**: Weitao Wan, Jiansheng Chen, Cheng Yu, Tong Wu, Yuanyi Zhong, Ming-Hsuan Yang
- **Comment**: Under review of TPAMI
- **Journal**: None
- **Summary**: The softmax cross-entropy loss function has been widely used to train deep models for various tasks. In this work, we propose a Gaussian mixture (GM) loss function for deep neural networks for visual classification. Unlike the softmax cross-entropy loss, our method explicitly shapes the deep feature space towards a Gaussian Mixture distribution. With a classification margin and a likelihood regularization, the GM loss facilitates both high classification performance and accurate modeling of the feature distribution. The GM loss can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on the discrepancy between feature distributions of the inputs and the training set. Furthermore, theoretical analysis shows that a symmetric feature space can be achieved by using the GM loss, which enables the models to perform robustly against adversarial attacks. The proposed model can be implemented easily and efficiently without using extra trainable parameters. Extensive evaluations demonstrate that the proposed method performs favorably not only on image classification but also on robust detection of adversarial examples generated by strong attacks under different threat models.



### Deep Positional and Relational Feature Learning for Rotation-Invariant Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2011.09080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09080v1)
- **Published**: 2020-11-18 04:16:51+00:00
- **Updated**: 2020-11-18 04:16:51+00:00
- **Authors**: Ruixuan Yu, Xin Wei, Federico Tombari, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a rotation-invariant deep network for point clouds analysis. Point-based deep networks are commonly designed to recognize roughly aligned 3D shapes based on point coordinates, but suffer from performance drops with shape rotations. Some geometric features, e.g., distances and angles of points as inputs of network, are rotation-invariant but lose positional information of points. In this work, we propose a novel deep network for point clouds by incorporating positional information of points as inputs while yielding rotation-invariance. The network is hierarchical and relies on two modules: a positional feature embedding block and a relational feature embedding block. Both modules and the whole network are proven to be rotation-invariant when processing point clouds as input. Experiments show state-of-the-art classification and segmentation performances on benchmark datasets, and ablation studies demonstrate effectiveness of the network design.



### MUST-GAN: Multi-level Statistics Transfer for Self-driven Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.09084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09084v3)
- **Published**: 2020-11-18 04:38:48+00:00
- **Updated**: 2021-04-09 02:22:46+00:00
- **Authors**: Tianxiang Ma, Bo Peng, Wei Wang, Jing Dong
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Pose-guided person image generation usually involves using paired source-target images to supervise the training, which significantly increases the data preparation effort and limits the application of the models. To deal with this problem, we propose a novel multi-level statistics transfer model, which disentangles and transfers multi-level appearance features from person images and merges them with pose features to reconstruct the source person images themselves. So that the source images can be used as supervision for self-driven person image generation. Specifically, our model extracts multi-level features from the appearance encoder and learns the optimal appearance representation through attention mechanism and attributes statistics. Then we transfer them to a pose-guided generator for re-fusion of appearance and pose. Our approach allows for flexible manipulation of person appearance and pose properties to perform pose transfer and clothes style transfer tasks. Experimental results on the DeepFashion dataset demonstrate our method's superiority compared with state-of-the-art supervised and unsupervised methods. In addition, our approach also performs well in the wild.



### UP-DETR: Unsupervised Pre-training for Object Detection with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2011.09094v3
- **DOI**: 10.1109/TPAMI.2022.3216514
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09094v3)
- **Published**: 2020-11-18 05:16:11+00:00
- **Updated**: 2023-07-24 11:28:46+00:00
- **Authors**: Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen
- **Comment**: Accepted by TPAMI 2022 and CVPR 2021
- **Journal**: IEEE.Trans.Pattern.Anal.Mach.Intell. Oct. 21 (2002)
- **Summary**: DEtection TRansformer (DETR) for object detection reaches competitive performance compared with Faster R-CNN via a transformer encoder-decoder architecture. However, trained with scratch transformers, DETR needs large-scale training data and an extreme long training schedule even on COCO dataset. Inspired by the great success of pre-training transformers in natural language processing, we propose a novel pretext task named random query patch detection in Unsupervised Pre-training DETR (UP-DETR). Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the input image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we find that freezing the CNN backbone is the prerequisite for the success of pre-training transformers. (2) To perform multi-query localization, we develop UP-DETR with multi-query patch detection with attention mask. Besides, UP-DETR also provides a unified perspective for fine-tuning object detection and one-shot detection tasks. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr.



### Viewpoint-aware Progressive Clustering for Unsupervised Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2011.09099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09099v1)
- **Published**: 2020-11-18 05:40:14+00:00
- **Updated**: 2020-11-18 05:40:14+00:00
- **Authors**: Aihua Zheng, Xia Sun, Chenglong Li, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (Re-ID) is an active task due to its importance in large-scale intelligent monitoring in smart cities. Despite the rapid progress in recent years, most existing methods handle vehicle Re-ID task in a supervised manner, which is both time and labor-consuming and limits their application to real-life scenarios. Recently, unsupervised person Re-ID methods achieve impressive performance by exploring domain adaption or clustering-based techniques. However, one cannot directly generalize these methods to vehicle Re-ID since vehicle images present huge appearance variations in different viewpoints. To handle this problem, we propose a novel viewpoint-aware clustering algorithm for unsupervised vehicle Re-ID. In particular, we first divide the entire feature space into different subspaces according to the predicted viewpoints and then perform a progressive clustering to mine the accurate relationship among samples. Comprehensive experiments against the state-of-the-art methods on two multi-viewpoint benchmark datasets VeRi and VeRi-Wild validate the promising performance of the proposed method in both with and without domain adaption scenarios while handling unsupervised vehicle Re-ID.



### Masked Linear Regression for Learning Local Receptive Fields for Facial Expression Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2011.09104v1
- **DOI**: 10.1007/s11263-019-01256-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09104v1)
- **Published**: 2020-11-18 06:04:24+00:00
- **Updated**: 2020-11-18 06:04:24+00:00
- **Authors**: Nazar Khan, Arbish Akram, Arif Mahmood, Sania Ashraf, Kashif Murtaza
- **Comment**: IJCV Journal
- **Journal**: International Journal of Computer Vision, vol 128, no 5, 2020
- **Summary**: Compared to facial expression recognition, expression synthesis requires a very high-dimensional mapping. This problem exacerbates with increasing image sizes and limits existing expression synthesis approaches to relatively small images. We observe that facial expressions often constitute sparsely distributed and locally correlated changes from one expression to another. By exploiting this observation, the number of parameters in an expression synthesis model can be significantly reduced. Therefore, we propose a constrained version of ridge regression that exploits the local and sparse structure of facial expressions. We consider this model as masked regression for learning local receptive fields. In contrast to the existing approaches, our proposed model can be efficiently trained on larger image sizes. Experiments using three publicly available datasets demonstrate that our model is significantly better than $\ell_0, \ell_1$ and $\ell_2$-regression, SVD based approaches, and kernelized regression in terms of mean-squared-error, visual quality as well as computational and spatial complexities. The reduction in the number of parameters allows our method to generalize better even after training on smaller datasets. The proposed algorithm is also compared with state-of-the-art GANs including Pix2Pix, CycleGAN, StarGAN and GANimation. These GANs produce photo-realistic results as long as the testing and the training distributions are similar. In contrast, our results demonstrate significant generalization of the proposed algorithm over out-of-dataset human photographs, pencil sketches and even animal faces.



### Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2011.09113v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09113v1)
- **Published**: 2020-11-18 06:33:20+00:00
- **Updated**: 2020-11-18 06:33:20+00:00
- **Authors**: Gaurav Kumar Nayak, Konda Reddy Mopuri, Anirban Chakraborty
- **Comment**: Accepted in WACV 2021
- **Journal**: None
- **Summary**: Knowledge Distillation is an effective method to transfer the learning across deep neural networks. Typically, the dataset originally used for training the Teacher model is chosen as the "Transfer Set" to conduct the knowledge transfer to the Student. However, this original training data may not always be freely available due to privacy or sensitivity concerns. In such scenarios, existing approaches either iteratively compose a synthetic set representative of the original training dataset, one sample at a time or learn a generative model to compose such a transfer set. However, both these approaches involve complex optimization (GAN training or several backpropagation steps to synthesize one sample) and are often computationally expensive. In this paper, as a simple alternative, we investigate the effectiveness of "arbitrary transfer sets" such as random noise, publicly available synthetic, and natural datasets, all of which are completely unrelated to the original training dataset in terms of their visual or semantic contents. Through extensive experiments on multiple benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover and validate surprising effectiveness of using arbitrary data to conduct knowledge distillation when this dataset is "target-class balanced". We believe that this important observation can potentially lead to designing baselines for the data-free knowledge distillation task.



### Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with Airlight and Scattering Coefficient Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.09114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09114v2)
- **Published**: 2020-11-18 06:33:47+00:00
- **Updated**: 2020-12-02 05:21:21+00:00
- **Authors**: Yuki Fujimura, Motoharu Sonogashira, Masaaki Iiyama
- **Comment**: 14 pages, extended version of our ACCV2020 paper
- **Journal**: None
- **Summary**: We propose a learning-based multi-view stereo (MVS) method in scattering media, such as fog or smoke, with a novel cost volume, called the dehazing cost volume. Images captured in scattering media are degraded due to light scattering and attenuation caused by suspended particles. This degradation depends on scene depth; thus, it is difficult for traditional MVS methods to evaluate photometric consistency because the depth is unknown before three-dimensional (3D) reconstruction. The dehazing cost volume can solve this chicken-and-egg problem of depth estimation and image restoration by computing the scattering effect using swept planes in the cost volume. We also propose a method of estimating scattering parameters, such as airlight, and a scattering coefficient, which are required for our dehazing cost volume. The output depth of a network with our dehazing cost volume can be regarded as a function of these parameters; thus, they are geometrically optimized with a sparse 3D point cloud obtained at a structure-from-motion step. Experimental results on synthesized hazy images indicate the effectiveness of our dehazing cost volume against the ordinary cost volume regarding scattering media. We also demonstrated the applicability of our dehazing cost volume to real foggy scenes.



### More Informed Random Sample Consensus
- **Arxiv ID**: http://arxiv.org/abs/2011.09116v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09116v1)
- **Published**: 2020-11-18 06:43:50+00:00
- **Updated**: 2020-11-18 06:43:50+00:00
- **Authors**: Guoxiang Zhang, YangQuan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Random sample consensus (RANSAC) is a robust model-fitting algorithm. It is widely used in many fields including image-stitching and point cloud registration. In RANSAC, data is uniformly sampled for hypothesis generation. However, this uniform sampling strategy does not fully utilize all the information on many problems. In this paper, we propose a method that samples data with a L\'{e}vy distribution together with a data sorting algorithm. In the hypothesis sampling step of the proposed method, data is sorted with a sorting algorithm we proposed, which sorts data based on the likelihood of a data point being in the inlier set. Then, hypotheses are sampled from the sorted data with L\'{e}vy distribution. The proposed method is evaluated on both simulation and real-world public datasets. Our method shows better results compared with the uniform baseline method.



### Adversarial Profiles: Detecting Out-Distribution & Adversarial Samples in Pre-trained CNNs
- **Arxiv ID**: http://arxiv.org/abs/2011.09123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09123v1)
- **Published**: 2020-11-18 07:10:13+00:00
- **Updated**: 2020-11-18 07:10:13+00:00
- **Authors**: Arezoo Rajabi, Rakesh B. Bobba
- **Comment**: Accepted on DSN Workshop on Dependable and Secure Machine Learning
  2019
- **Journal**: DSN Workshop on Dependable and Secure Machine Learning (DSML 2019)
- **Summary**: Despite high accuracy of Convolutional Neural Networks (CNNs), they are vulnerable to adversarial and out-distribution examples. There are many proposed methods that tend to detect or make CNNs robust against these fooling examples. However, most such methods need access to a wide range of fooling examples to retrain the network or to tune detection parameters. Here, we propose a method to detect adversarial and out-distribution examples against a pre-trained CNN without needing to retrain the CNN or needing access to a wide variety of fooling examples. To this end, we create adversarial profiles for each class using only one adversarial attack generation technique. We then wrap a detector around the pre-trained CNN that applies the created adversarial profile to each input and uses the output to decide whether or not the input is legitimate. Our initial evaluation of this approach using MNIST dataset show that adversarial profile based detection is effective in detecting at least 92 of out-distribution examples and 59% of adversarial examples.



### 3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics
- **Arxiv ID**: http://arxiv.org/abs/2011.09127v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09127v2)
- **Published**: 2020-11-18 07:14:55+00:00
- **Updated**: 2021-05-14 02:39:44+00:00
- **Authors**: Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming Wang Cao Li, Zengqi Xun, Chengyue Sun, Rongfei Jia, Binqiang Zhao, Hao Zhang
- **Comment**: Project page:
  https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset
- **Journal**: None
- **Summary**: We introduce 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics), a new, large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility. From layout semantics down to texture details of individual objects, our dataset is freely available to the academic community and beyond. Currently, 3D-FRONT contains 18,968 rooms diversely furnished by 3D objects, far surpassing all publicly available scene datasets. In addition, the 13,151 furniture objects all come with high-quality textures. While the floorplans and layout designs are directly sourced from professional creations, the interior designs in terms of furniture styles, color, and textures have been carefully curated based on a recommender system we develop to attain consistent styles as expert designs. Furthermore, we release Trescope, a light-weight rendering tool, to support benchmark rendering of 2D images and annotations from 3D-FRONT. We demonstrate two applications, interior scene synthesis and texture synthesis, that are especially tailored to the strengths of our new dataset. The project page is at: https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset.



### Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2011.09141v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2011.09141v3)
- **Published**: 2020-11-18 07:39:13+00:00
- **Updated**: 2021-04-12 20:40:12+00:00
- **Authors**: Christoph B. Rist, David Emmerichs, Markus Enzweiler, Dariu M. Gavrila
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic scene completion is the task of jointly estimating 3D geometry and semantics of objects and surfaces within a given extent. This is a particularly challenging task on real-world data that is sparse and occluded. We propose a scene segmentation network based on local Deep Implicit Functions as a novel learning-based method for scene completion. Unlike previous work on scene completion, our method produces a continuous scene representation that is not based on voxelization. We encode raw point clouds into a latent space locally and at multiple spatial resolutions. A global scene completion function is subsequently assembled from the localized function patches. We show that this continuous representation is suitable to encode geometric and semantic properties of extensive outdoor scenes without the need for spatial discretization (thus avoiding the trade-off between level of scene detail and the scene extent that can be covered).   We train and evaluate our method on semantically annotated LiDAR scans from the Semantic KITTI dataset. Our experiments verify that our method generates a powerful representation that can be decoded into a dense 3D description of a given scene. The performance of our method surpasses the state of the art on the Semantic KITTI Scene Completion Benchmark in terms of geometric completion intersection-over-union (IoU).



### DeepNAG: Deep Non-Adversarial Gesture Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.09149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09149v1)
- **Published**: 2020-11-18 08:00:12+00:00
- **Updated**: 2020-11-18 08:00:12+00:00
- **Authors**: Mehran Maghoumi, Eugene M. Taranta II, Joseph J. LaViola Jr
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Synthetic data generation to improve classification performance (data augmentation) is a well-studied problem. Recently, generative adversarial networks (GAN) have shown superior image data augmentation performance, but their suitability in gesture synthesis has received inadequate attention. Further, GANs prohibitively require simultaneous generator and discriminator network training. We tackle both issues in this work. We first discuss a novel, device-agnostic GAN model for gesture synthesis called DeepGAN. Thereafter, we formulate DeepNAG by introducing a new differentiable loss function based on dynamic time warping and the average Hausdorff distance, which allows us to train DeepGAN's generator without requiring a discriminator. Through evaluations, we compare the utility of DeepGAN and DeepNAG against two alternative techniques for training five recognizers using data augmentation over six datasets. We further investigate the perceived quality of synthesized samples via an Amazon Mechanical Turk user study based on the HYPE benchmark. We find that DeepNAG outperforms DeepGAN in accuracy, training time (up to 17x faster), and realism, thereby opening the door to a new line of research in generator network design and training for gesture synthesis. Our source code is available at https://www.deepnag.com.



### RSINet: Rotation-Scale Invariant Network for Online Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2011.09153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09153v1)
- **Published**: 2020-11-18 08:19:14+00:00
- **Updated**: 2020-11-18 08:19:14+00:00
- **Authors**: Yang Fang, Geun-Sik Jo, Chang-Hee Lee
- **Comment**: 8 pages, 5 figures, the paper has been accepted by international
  conference on pattern recognition 2020
- **Journal**: None
- **Summary**: Most Siamese network-based trackers perform the tracking process without model update, and cannot learn targetspecific variation adaptively. Moreover, Siamese-based trackers infer the new state of tracked objects by generating axis-aligned bounding boxes, which contain extra background noise, and are unable to accurately estimate the rotation and scale transformation of moving objects, thus potentially reducing tracking performance. In this paper, we propose a novel Rotation-Scale Invariant Network (RSINet) to address the above problem. Our RSINet tracker consists of a target-distractor discrimination branch and a rotation-scale estimation branch, the rotation and scale knowledge can be explicitly learned by a multi-task learning method in an end-to-end manner. In addtion, the tracking model is adaptively optimized and updated under spatio-temporal energy control, which ensures model stability and reliability, as well as high tracking efficiency. Comprehensive experiments on OTB-100, VOT2018, and LaSOT benchmarks demonstrate that our proposed RSINet tracker yields new state-of-the-art performance compared with recent trackers, while running at real-time speed about 45 FPS.



### Dense Contrastive Learning for Self-Supervised Visual Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2011.09157v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09157v2)
- **Published**: 2020-11-18 08:42:32+00:00
- **Updated**: 2021-04-04 11:41:26+00:00
- **Authors**: Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li
- **Comment**: 11 pages. Accepted to IEEE/CVF Conf. Comp. Vision Pattern Recognition
  (CVPR) 2021; Oral paper
- **Journal**: None
- **Summary**: To date, most existing self-supervised learning methods are designed and optimized for image classification. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To fill this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning, which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images. Compared to the baseline method MoCo-v2, our method introduces negligible computation overhead (only <1% slower), but demonstrates consistently superior performance when transferring to downstream dense prediction tasks including object detection, semantic segmentation and instance segmentation; and outperforms the state-of-the-art methods by a large margin. Specifically, over the strong MoCo-v2 baseline, our method achieves significant improvements of 2.0% AP on PASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO instance segmentation, 3.0% mIoU on PASCAL VOC semantic segmentation and 1.8% mIoU on Cityscapes semantic segmentation. Code is available at: https://git.io/AdelaiDet



### Privileged Knowledge Distillation for Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.09158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09158v2)
- **Published**: 2020-11-18 08:52:15+00:00
- **Updated**: 2020-12-03 12:52:54+00:00
- **Authors**: Peisen Zhao, Lingxi Xie, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Online Action Detection (OAD) in videos is proposed as a per-frame labeling task to address the real-time prediction tasks that can only obtain the previous and current video frames. This paper presents a novel learning-with-privileged based framework for online action detection where the future frames only observable at the training stages are considered as a form of privileged information. Knowledge distillation is employed to transfer the privileged information from the offline teacher to the online student. We note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in input data rather than the network architecture. We propose Privileged Knowledge Distillation (PKD) which (i) schedules a curriculum learning procedure and (ii) inserts auxiliary nodes to the student model, both for shrinking the information gap and improving learning performance. Compared to other OAD methods that explicitly predict future frames, our approach avoids learning unpredictable unnecessary yet inconsistent visual contents and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.



### Positive-Congruent Training: Towards Regression-Free Model Updates
- **Arxiv ID**: http://arxiv.org/abs/2011.09161v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09161v3)
- **Published**: 2020-11-18 09:00:44+00:00
- **Updated**: 2021-05-17 20:10:51+00:00
- **Authors**: Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng Wang, Wei Xia, Stefano Soatto
- **Comment**: Accepted to CVPR 2021 (oral)
- **Journal**: None
- **Summary**: Reducing inconsistencies in the behavior of different versions of an AI system can be as important in practice as reducing its overall error. In image classification, sample-wise inconsistencies appear as "negative flips": A new model incorrectly predicts the output for a test sample that was correctly classified by the old (reference) model. Positive-congruent (PC) training aims at reducing error rate while at the same time reducing negative flips, thus maximizing congruency with the reference model only on positive predictions, unlike model distillation. We propose a simple approach for PC training, Focal Distillation, which enforces congruence with the reference model by giving more weights to samples that were correctly classified. We also found that, if the reference model itself can be chosen as an ensemble of multiple deep neural networks, negative flips can be further reduced without affecting the new model's accuracy.



### TJU-DHD: A Diverse High-Resolution Dataset for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.09170v1
- **DOI**: 10.1109/TIP.2020.3034487
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09170v1)
- **Published**: 2020-11-18 09:32:24+00:00
- **Updated**: 2020-11-18 09:32:24+00:00
- **Authors**: Yanwei Pang, Jiale Cao, Yazhao Li, Jin Xie, Hanqing Sun, Jinfeng Gong
- **Comment**: object detection and pedestrian detection. website:
  https://github.com/tjubiit/TJU-DHD
- **Journal**: IEEE Transactions on Image Processing, 2020
- **Summary**: Vehicles, pedestrians, and riders are the most important and interesting objects for the perception modules of self-driving vehicles and video surveillance. However, the state-of-the-art performance of detecting such important objects (esp. small objects) is far from satisfying the demand of practical systems. Large-scale, rich-diversity, and high-resolution datasets play an important role in developing better object detection methods to satisfy the demand. Existing public large-scale datasets such as MS COCO collected from websites do not focus on the specific scenarios. Moreover, the popular datasets (e.g., KITTI and Citypersons) collected from the specific scenarios are limited in the number of images and instances, the resolution, and the diversity. To attempt to solve the problem, we build a diverse high-resolution dataset (called TJU-DHD). The dataset contains 115,354 high-resolution images (52% images have a resolution of 1624$\times$1200 pixels and 48% images have a resolution of at least 2,560$\times$1,440 pixels) and 709,330 labeled objects in total with a large variance in scale and appearance. Meanwhile, the dataset has a rich diversity in season variance, illumination variance, and weather variance. In addition, a new diverse pedestrian dataset is further built. With the four different detectors (i.e., the one-stage RetinaNet, anchor-free FCOS, two-stage FPN, and Cascade R-CNN), experiments about object detection and pedestrian detection are conducted. We hope that the newly built dataset can help promote the research on object detection and pedestrian detection in these two scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD.



### CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2011.09190v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09190v2)
- **Published**: 2020-11-18 10:24:38+00:00
- **Updated**: 2020-11-26 20:17:53+00:00
- **Authors**: Di Ma, Fan Zhang, David R. Bull
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new Generative Adversarial Network for Compressed Video quality Enhancement (CVEGAN). The CVEGAN generator benefits from the use of a novel Mul2Res block (with multiple levels of residual learning branches), an enhanced residual non-local block (ERNB) and an enhanced convolutional block attention module (ECBAM). The ERNB has also been employed in the discriminator to improve the representational capability. The training strategy has also been re-designed specifically for video compression applications, to employ a relativistic sphere GAN (ReSphereGAN) training methodology together with new perceptual loss functions. The proposed network has been fully evaluated in the context of two typical video compression enhancement tools: post-processing (PP) and spatial resolution adaptation (SRA). CVEGAN has been fully integrated into the MPEG HEVC video coding test model (HM16.20) and experimental results demonstrate significant coding gains (up to 28% for PP and 38% for SRA compared to the anchor) over existing state-of-the-art architectures for both coding tools across multiple datasets.



### Res-GCNN: A Lightweight Residual Graph Convolutional Neural Networks for Human Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2011.09214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09214v1)
- **Published**: 2020-11-18 11:18:16+00:00
- **Updated**: 2020-11-18 11:18:16+00:00
- **Authors**: Yanwu Ge, Mingliang Song
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Autonomous driving vehicles (ADVs) hold great hopes to solve traffic congestion problems and reduce the number of traffic accidents. Accurate trajectories prediction of other traffic agents around ADVs is of key importance to achieve safe and efficient driving. Pedestrians, particularly, are more challenging to forecast due to their complex social in-teractions and randomly moving patterns. We propose a Residual Graph Convolutional Neural Network (Res-GCNN), which models the interactive behaviors of pedes-trians by using the adjacent matrix of the constructed graph for the current scene. Though the proposed Res-GCNN is quite lightweight with only about 6.4 kilo parameters which outperforms all other methods in terms of parameters size, our experimental results show an improvement over the state of art by 13.3% on the Final Displacement Error (FDE) which reaches 0.65 meter. As for the Average Dis-placement Error (ADE), we achieve a suboptimal result (the value is 0.37 meter), which is also very competitive. The Res-GCNN is evaluated in the platform with an NVIDIA GeForce RTX1080Ti GPU, and its mean inference time of the whole dataset is only about 2.2 microseconds. Compared with other methods, the proposed method shows strong potential for onboard application accounting for forecasting accuracy and time efficiency. The code will be made publicly available on GitHub.



### CGAP2: Context and gap aware predictive pose framework for early detection of gestures
- **Arxiv ID**: http://arxiv.org/abs/2011.09216v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09216v1)
- **Published**: 2020-11-18 11:21:04+00:00
- **Updated**: 2020-11-18 11:21:04+00:00
- **Authors**: Nishant Bhattacharya, Suresh Sundaram
- **Comment**: None
- **Journal**: None
- **Summary**: With a growing interest in autonomous vehicles' operation, there is an equally increasing need for efficient anticipatory gesture recognition systems for human-vehicle interaction. Existing gesture-recognition algorithms have been primarily restricted to historical data. In this paper, we propose a novel context and gap aware pose prediction framework(CGAP2), which predicts future pose data for anticipatory recognition of gestures in an online fashion. CGAP2 implements an encoder-decoder architecture paired with a pose prediction module to anticipate future frames followed by a shallow classifier. CGAP2 pose prediction module uses 3D convolutional layers and depends on the number of pose frames supplied, the time difference between each pose frame, and the number of predicted pose frames. The performance of CGAP2 is evaluated on the Human3.6M dataset with the MPJPE metric. For pose prediction of 15 frames in advance, an error of 79.0mm is achieved. The pose prediction module consists of only 26M parameters and can run at 50 FPS on the NVidia RTX Titan. Furthermore, the ablation study indicates supplying higher context information to the pose prediction module can be detrimental for anticipatory recognition. CGAP2 has a 1-second time advantage compared to other gesture recognition systems, which can be crucial for autonomous vehicles.



### FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2011.09230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09230v2)
- **Published**: 2020-11-18 11:58:19+00:00
- **Updated**: 2021-03-25 07:22:12+00:00
- **Authors**: Jaemin Na, Heechul Jung, Hyung Jin Chang, Wonjun Hwang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) methods for learning domain invariant representations have achieved remarkable progress. However, most of the studies were based on direct adaptation from the source domain to the target domain and have suffered from large domain discrepancies. In this paper, we propose a UDA method that effectively handles such large domain discrepancies. We introduce a fixed ratio-based mixup to augment multiple intermediate domains between the source and target domain. From the augmented-domains, we train the source-dominant model and the target-dominant model that have complementary characteristics. Using our confidence-based learning methodologies, e.g., bidirectional matching with high-confidence predictions and self-penalization using low-confidence predictions, the models can learn from each other or from its own results. Through our proposed methods, the models gradually transfer domain knowledge from the source to the target domain. Extensive experiments demonstrate the superiority of our proposed method on three public benchmarks: Office-31, Office-Home, and VisDA-2017.



### A Multi-class Approach -- Building a Visual Classifier based on Textual Descriptions using Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.09236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.09236v1)
- **Published**: 2020-11-18 12:06:55+00:00
- **Updated**: 2020-11-18 12:06:55+00:00
- **Authors**: Preeti Jagdish Sajjan, Frank G. Glavin
- **Comment**: AICS 2020: Irish Conference on Artificial Intelligence and Cognitive
  Science
- **Journal**: None
- **Summary**: Machine Learning (ML) techniques for image classification routinely require many labelled images for training the model and while testing, we ought to use images belonging to the same domain as those used for training. In this paper, we overcome the two main hurdles of ML, i.e. scarcity of data and constrained prediction of the classification model. We do this by introducing a visual classifier which uses a concept of transfer learning, namely Zero-Shot Learning (ZSL), and standard Natural Language Processing techniques. We train a classifier by mapping labelled images to their textual description instead of training it for specific classes. Transfer learning involves transferring knowledge across domains that are similar. ZSL intelligently applies the knowledge learned while training for future recognition tasks. ZSL differentiates classes as two types: seen and unseen classes. Seen classes are the classes upon which we have trained our model and unseen classes are the classes upon which we test our model. The examples from unseen classes have not been encountered in the training phase. Earlier research in this domain focused on developing a binary classifier but, in this paper, we present a multi-class classifier with a Zero-Shot Learning approach.



### Deep learning models for gastric signet ring cell carcinoma classification in whole slide images
- **Arxiv ID**: http://arxiv.org/abs/2011.09247v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09247v1)
- **Published**: 2020-11-18 12:39:51+00:00
- **Updated**: 2020-11-18 12:39:51+00:00
- **Authors**: Fahdi Kanavati, Shin Ichihara, Michael Rambeau, Osamu Iizuka, Koji Arihiro, Masayuki Tsuneki
- **Comment**: None
- **Journal**: None
- **Summary**: Signet ring cell carcinoma (SRCC) of the stomach is a rare type of cancer with a slowly rising incidence. It tends to be more difficult to detect by pathologists mainly due to its cellular morphology and diffuse invasion manner, and it has poor prognosis when detected at an advanced stage. Computational pathology tools that can assist pathologists in detecting SRCC would be of a massive benefit. In this paper, we trained deep learning models using transfer learning, fully-supervised learning, and weakly-supervised learning to predict SRCC in Whole Slide Images (WSIs) using a training set of 1,765 WSIs. We evaluated the models on four different test sets of about 500 images each. The best model achieved a Receiver Operator Curve (ROC) area under the curve (AUC) of at least 0.99 on all four test sets, setting a top baseline performance for SRCC WSI classification.



### Inspecting state of the art performance and NLP metrics in image-based medical report generation
- **Arxiv ID**: http://arxiv.org/abs/2011.09257v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, I.2.7; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2011.09257v3)
- **Published**: 2020-11-18 13:09:12+00:00
- **Updated**: 2022-01-15 06:05:51+00:00
- **Authors**: Pablo Pino, Denis Parra, Pablo Messina, Cecilia Besa, Sergio Uribe
- **Comment**: 3 pages, 1 figure, 1 table. Accepted in LatinX in AI workshop at
  NeurIPS 2020. (v3 updated ack)
- **Journal**: None
- **Summary**: Several deep learning architectures have been proposed over the last years to deal with the problem of generating a written report given an imaging exam as input. Most works evaluate the generated reports using standard Natural Language Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant progress. In this article, we contrast this progress by comparing state of the art (SOTA) models against weak baselines. We show that simple and even naive approaches yield near SOTA performance on most traditional NLP metrics. We conclude that evaluation methods in this task should be further studied towards correctly measuring clinical accuracy, ideally involving physicians to contribute to this end.



### Continuous Emotion Recognition with Spatiotemporal Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.09280v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09280v2)
- **Published**: 2020-11-18 13:42:05+00:00
- **Updated**: 2021-01-15 14:49:00+00:00
- **Authors**: Thomas Teixeira, Eric Granger, Alessandro Lameiras Koerich
- **Comment**: 33 pages
- **Journal**: None
- **Summary**: Facial expressions are one of the most powerful ways for depicting specific patterns in human behavior and describing human emotional state. Despite the impressive advances of affective computing over the last decade, automatic video-based systems for facial expression recognition still cannot handle properly variations in facial expression among individuals as well as cross-cultural and demographic aspects. Nevertheless, recognizing facial expressions is a difficult task even for humans. In this paper, we investigate the suitability of state-of-the-art deep learning architectures based on convolutional neural networks (CNNs) for continuous emotion recognition using long video sequences captured in-the-wild. This study focuses on deep learning models that allow encoding spatiotemporal relations in videos considering a complex and multi-dimensional emotion space, where values of valence and arousal must be predicted. We have developed and evaluated convolutional recurrent neural networks combining 2D-CNNs and long short term-memory units, and inflated 3D-CNN models, which are built by inflating the weights of a pre-trained 2D-CNN model during fine-tuning, using application-specific videos. Experimental results on the challenging SEWA-DB dataset have shown that these architectures can effectively be fine-tuned to encode the spatiotemporal information from successive raw pixel images and achieve state-of-the-art results on such a dataset.



### Stretchable Cells Help DARTS Search Better
- **Arxiv ID**: http://arxiv.org/abs/2011.09300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09300v2)
- **Published**: 2020-11-18 14:15:51+00:00
- **Updated**: 2022-04-18 13:43:38+00:00
- **Authors**: Tao Huang, Shan You, Yibo Yang, Zhuozhuo Tu, Fei Wang, Chen Qian, Changshui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable neural architecture search (DARTS) has gained much success in discovering flexible and diverse cell types. To reduce the evaluation gap, the supernet is expected to have identical layers with the target network. However, even for this consistent search, the searched cells often suffer from poor performance, especially for the supernet with fewer layers, as current DARTS methods are prone to wide and shallow cells, and this topology collapse induces sub-optimal searched cells. In this paper, we alleviate this issue by endowing the cells with explicit stretchability, so the search can be directly implemented on our stretchable cells for both operation type and topology simultaneously. Concretely, we introduce a set of topological variables and a combinatorial probabilistic distribution to explicitly model the target topology. With more diverse and complex topologies, our method adapts well for various layer numbers. Extensive experiments on CIFAR-10 and ImageNet show that our stretchable cells obtain better performance with fewer layers and parameters. For example, our method can improve DARTS by 0.28\% accuracy on CIFAR-10 dataset with 45\% parameters reduced or 2.9\% with similar FLOPs on ImageNet dataset.



### TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to Inverse Imaging Problems
- **Arxiv ID**: http://arxiv.org/abs/2012.05703v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.05703v3)
- **Published**: 2020-11-18 14:19:30+00:00
- **Updated**: 2021-09-18 04:20:19+00:00
- **Authors**: Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Hua Huang, Carola-Bibiane Schnlieb
- **Comment**: The Journal version (47 pages) of arXiv:2002.09611 (ICML'20 Award
  Paper); Code is released at https://github.com/Vandermode/TFPnP
- **Journal**: None
- **Summary**: Plug-and-Play (PnP) is a non-convex optimization framework that combines proximal algorithms, for example, the alternating direction method of multipliers (ADMM), with advanced denoising priors. Over the past few years, great empirical success has been obtained by PnP algorithms, especially for the ones that integrate deep learning-based denoisers. However, a key challenge of PnP approaches is the need for manual parameter tweaking as it is essential to obtain high-quality results across the high discrepancy in imaging conditions and varying scene content. In this work, we present a class of tuning-free PnP proximal algorithms that can determine parameters such as denoising strength, termination time, and other optimization-specific parameters automatically. A core part of our approach is a policy network for automated parameter search which can be effectively learned via a mixture of model-free and model-based deep reinforcement learning strategies. We demonstrate, through rigorous numerical and visual experiments, that the learned policy can customize parameters to different settings, and is often more efficient and effective than existing handcrafted criteria. Moreover, we discuss several practical considerations of PnP denoisers, which together with our learned policy yield state-of-the-art results. This advanced performance is prevalent on both linear and nonlinear exemplar inverse imaging problems, and in particular shows promising results on compressed sensing MRI, sparse-view CT, single-photon imaging, and phase retrieval.



### End-to-End Object Detection with Adaptive Clustering Transformer
- **Arxiv ID**: http://arxiv.org/abs/2011.09315v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09315v2)
- **Published**: 2020-11-18 14:36:37+00:00
- **Updated**: 2021-10-18 07:15:55+00:00
- **Authors**: Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li, Xiaogang Wang, Hongsheng Li, Hao Dong
- **Comment**: BMVC 2021 Oral
- **Journal**: None
- **Summary**: End-to-end Object Detection with Transformer (DETR)proposes to perform object detection with Transformer and achieve comparable performance with two-stage object detection like Faster-RCNN. However, DETR needs huge computational resources for training and inference due to the high-resolution spatial input. In this paper, a novel variant of transformer named Adaptive Clustering Transformer(ACT) has been proposed to reduce the computation cost for high-resolution input. ACT cluster the query features adaptively using Locality Sensitive Hashing (LSH) and ap-proximate the query-key interaction using the prototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside self-attention into O(NK) where K is the number of prototypes in each layer. ACT can be a drop-in module replacing the original self-attention module without any training. ACT achieves a good balance between accuracy and computation cost (FLOPs). The code is available as supplementary for the ease of experiment replication and verification. Code is released at \url{https://github.com/gaopengcuhk/SMCA-DETR/}



### Online Exemplar Fine-Tuning for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2011.09330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09330v1)
- **Published**: 2020-11-18 15:13:16+00:00
- **Updated**: 2020-11-18 15:13:16+00:00
- **Authors**: Taewon Kang, Soohyun Kim, Sunwoo Kim, Seungryong Kim
- **Comment**: 10 pages, 13 figures
- **Journal**: None
- **Summary**: Existing techniques to solve exemplar-based image-to-image translation within deep convolutional neural networks (CNNs) generally require a training phase to optimize the network parameters on domain-specific and task-specific benchmarks, thus having limited applicability and generalization ability. In this paper, we propose a novel framework, for the first time, to solve exemplar-based translation through an online optimization given an input image pair, called online exemplar fine-tuning (OEFT), in which we fine-tune the off-the-shelf and general-purpose networks to the input image pair themselves. We design two sub-networks, namely correspondence fine-tuning and multiple GAN inversion, and optimize these network parameters and latent codes, starting from the pre-trained ones, with well-defined loss functions. Our framework does not require the off-line training phase, which has been the main challenge of existing methods, but the pre-trained networks to enable optimization in online. Experimental results prove that our framework is effective in having a generalization power to unseen image pairs and clearly even outperforms the state-of-the-arts needing the intensive training phase.



### Self-Gradient Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.09364v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09364v2)
- **Published**: 2020-11-18 16:04:05+00:00
- **Updated**: 2020-11-19 04:16:05+00:00
- **Authors**: Hossein Aboutalebi, Mohammad Javad Shafiee Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: The incredible effectiveness of adversarial attacks on fooling deep neural networks poses a tremendous hurdle in the widespread adoption of deep learning in safety and security-critical domains. While adversarial defense mechanisms have been proposed since the discovery of the adversarial vulnerability issue of deep neural networks, there is a long path to fully understand and address this issue. In this study, we hypothesize that part of the reason for the incredible effectiveness of adversarial attacks is their ability to implicitly tap into and exploit the gradient flow of a deep neural network. This innate ability to exploit gradient flow makes defending against such attacks quite challenging. Motivated by this hypothesis we argue that if a deep neural network architecture can explicitly tap into its own gradient flow during the training, it can boost its defense capability significantly. Inspired by this fact, we introduce the concept of self-gradient networks, a novel deep neural network architecture designed to be more robust against adversarial perturbations. Gradient flow information is leveraged within self-gradient networks to achieve greater perturbation stability beyond what can be achieved in the standard training process. We conduct a theoretical analysis to gain better insights into the behaviour of the proposed self-gradient networks to illustrate the efficacy of leverage this additional gradient flow information. The proposed self-gradient network architecture enables much more efficient and effective adversarial training, leading to faster convergence towards an adversarially robust solution by at least 10X. Experimental results demonstrate the effectiveness of self-gradient networks when compared with state-of-the-art adversarial learning strategies, with 10% improvement on the CIFAR10 dataset under PGD and CW adversarial perturbations.



### Attentional Separation-and-Aggregation Network for Self-supervised Depth-Pose Learning in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2011.09369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.09369v1)
- **Published**: 2020-11-18 16:07:30+00:00
- **Updated**: 2020-11-18 16:07:30+00:00
- **Authors**: Feng Gao, Jincheng Yu, Hao Shen, Yu Wang, Huazhong Yang
- **Comment**: accepted by CoRL2020
- **Journal**: None
- **Summary**: Learning depth and ego-motion from unlabeled videos via self-supervision from epipolar projection can improve the robustness and accuracy of the 3D perception and localization of vision-based robots. However, the rigid projection computed by ego-motion cannot represent all scene points, such as points on moving objects, leading to false guidance in these regions. To address this problem, we propose an Attentional Separation-and-Aggregation Network (ASANet), which can learn to distinguish and extract the scene's static and dynamic characteristics via the attention mechanism. We further propose a novel MotionNet with an ASANet as the encoder, followed by two separate decoders, to estimate the camera's ego-motion and the scene's dynamic motion field. Then, we introduce an auto-selecting approach to detect the moving objects for dynamic-aware learning automatically. Empirical experiments demonstrate that our method can achieve the state-of-the-art performance on the KITTI benchmark.



### Diverse Plausible Shape Completions from Ambiguous Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2011.09390v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09390v1)
- **Published**: 2020-11-18 16:42:51+00:00
- **Updated**: 2020-11-18 16:42:51+00:00
- **Authors**: Brad Saund, Dmitry Berenson
- **Comment**: None
- **Journal**: None
- **Summary**: We propose PSSNet, a network architecture for generating diverse plausible 3D reconstructions from a single 2.5D depth image. Existing methods tend to produce only small variations on a single shape, even when multiple shapes are consistent with an observation. To obtain diversity we alter a Variational Auto Encoder by providing a learned shape bounding box feature as side information during training. Since these features are known during training, we are able to add a supervised loss to the encoder and noiseless values to the decoder. To evaluate, we sample a set of completions from a network, construct a set of plausible shape matches for each test observation, and compare using our plausible diversity metric defined over sets of shapes. We perform experiments using Shapenet mugs and partially-occluded YCB objects and find that our method performs comparably in datasets with little ambiguity, and outperforms existing methods when many shapes plausibly fit an observed depth image. We demonstrate one use for PSSNet on a physical robot when grasping objects in occlusion and clutter.



### Self-Supervised Physics-Guided Deep Learning Reconstruction For High-Resolution 3D LGE CMR
- **Arxiv ID**: http://arxiv.org/abs/2011.09414v1
- **DOI**: 10.1109/ISBI48211.2021.9434054
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2011.09414v1)
- **Published**: 2020-11-18 17:22:21+00:00
- **Updated**: 2020-11-18 17:22:21+00:00
- **Authors**: Burhaneddin Yaman, Chetan Shenoy, Zilin Deng, Steen Moeller, Hossam El-Rewaidy, Reza Nezafat, Mehmet Akakaya
- **Comment**: None
- **Journal**: Proceedings of IEEE ISBI, 2021
- **Summary**: Late gadolinium enhancement (LGE) cardiac MRI (CMR) is the clinical standard for diagnosis of myocardial scar. 3D isotropic LGE CMR provides improved coverage and resolution compared to 2D imaging. However, image acceleration is required due to long scan times and contrast washout. Physics-guided deep learning (PG-DL) approaches have recently emerged as an improved accelerated MRI strategy. Training of PG-DL methods is typically performed in supervised manner requiring fully-sampled data as reference, which is challenging in 3D LGE CMR. Recently, a self-supervised learning approach was proposed to enable training PG-DL techniques without fully-sampled data. In this work, we extend this self-supervised learning approach to 3D imaging, while tackling challenges related to small training database sizes of 3D volumes. Results and a reader study on prospectively accelerated 3D LGE show that the proposed approach at 6-fold acceleration outperforms the clinically utilized compressed sensing approach at 3-fold acceleration.



### Convolutional Autoencoder for Blind Hyperspectral Image Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2011.09420v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09420v1)
- **Published**: 2020-11-18 17:41:31+00:00
- **Updated**: 2020-11-18 17:41:31+00:00
- **Authors**: Yasiru Ranasinghe, Sanjaya Herath, Kavinga Weerasooriya, Mevan Ekanayake, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath
- **Comment**: 7 pages, 4 figures, conference
- **Journal**: None
- **Summary**: In the remote sensing context spectral unmixing is a technique to decompose a mixed pixel into two fundamental representatives: endmembers and abundances. In this paper, a novel architecture is proposed to perform blind unmixing on hyperspectral images. The proposed architecture consists of convolutional layers followed by an autoencoder. The encoder transforms the feature space produced through convolutional layers to a latent space representation. Then, from these latent characteristics the decoder reconstructs the roll-out image of the monochrome image which is at the input of the architecture; and each single-band image is fed sequentially. Experimental results on real hyperspectral data concludes that the proposed algorithm outperforms existing unmixing methods at abundance estimation and generates competitive results for endmember extraction with RMSE and SAD as the metrics, respectively.



### Fast Motion Understanding with Spatiotemporal Neural Networks and Dynamic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2011.09427v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.09427v1)
- **Published**: 2020-11-18 17:55:07+00:00
- **Updated**: 2020-11-18 17:55:07+00:00
- **Authors**: Anthony Bisulco, Fernando Cladera Ojeda, Volkan Isler, Daniel D. Lee
- **Comment**: None
- **Journal**: International Conference on Robotics and Automation (ICRA) 2021
- **Summary**: This paper presents a Dynamic Vision Sensor (DVS) based system for reasoning about high speed motion. As a representative scenario, we consider the case of a robot at rest reacting to a small, fast approaching object at speeds higher than 15m/s. Since conventional image sensors at typical frame rates observe such an object for only a few frames, estimating the underlying motion presents a considerable challenge for standard computer vision systems and algorithms. In this paper we present a method motivated by how animals such as insects solve this problem with their relatively simple vision systems.   Our solution takes the event stream from a DVS and first encodes the temporal events with a set of causal exponential filters across multiple time scales. We couple these filters with a Convolutional Neural Network (CNN) to efficiently extract relevant spatiotemporal features. The combined network learns to output both the expected time to collision of the object, as well as the predicted collision point on a discretized polar grid. These critical estimates are computed with minimal delay by the network in order to react appropriately to the incoming object. We highlight the results of our system to a toy dart moving at 23.4m/s with a 24.73{\deg} error in ${\theta}$, 18.4mm average discretized radius prediction error, and 25.03% median time to collision prediction error.



### GenderRobustness: Robustness of Gender Detection in Facial Recognition Systems with variation in Image Properties
- **Arxiv ID**: http://arxiv.org/abs/2011.10472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10472v2)
- **Published**: 2020-11-18 18:13:23+00:00
- **Updated**: 2020-11-26 22:18:15+00:00
- **Authors**: Sharadha Srinivasan, Madan Musuvathi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent times, there have been increasing accusations on artificial intelligence systems and algorithms of computer vision of possessing implicit biases. Even though these conversations are more prevalent now and systems are improving by performing extensive testing and broadening their horizon, biases still do exist. One such class of systems where bias is said to exist is facial recognition systems, where bias has been observed on the basis of gender, ethnicity, skin tone and other facial attributes. This is even more disturbing, given the fact that these systems are used in practically every sector of the industries today. From as critical as criminal identification to as simple as getting your attendance registered, these systems have gained a huge market, especially in recent years. That in itself is a good enough reason for developers of these systems to ensure that the bias is kept to a bare minimum or ideally non-existent, to avoid major issues like favoring a particular gender, race, or class of people or rather making a class of people susceptible to false accusations due to inability of these systems to correctly recognize those people.



### FROST: Faster and more Robust One-shot Semi-supervised Training
- **Arxiv ID**: http://arxiv.org/abs/2011.09471v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.09471v4)
- **Published**: 2020-11-18 18:56:03+00:00
- **Updated**: 2020-12-04 14:04:18+00:00
- **Authors**: Helena E. Liu, Leslie N. Smith
- **Comment**: Withdrawn because the results reported were due to an error in our
  code
- **Journal**: None
- **Summary**: Recent advances in one-shot semi-supervised learning have lowered the barrier for deep learning of new applications. However, the state-of-the-art for semi-supervised learning is slow to train and the performance is sensitive to the choices of the labeled data and hyper-parameter values. In this paper, we present a one-shot semi-supervised learning method that trains up to an order of magnitude faster and is more robust than state-of-the-art methods. Specifically, we show that by combining semi-supervised learning with a one-stage, single network version of self-training, our FROST methodology trains faster and is more robust to choices for the labeled samples and changes in hyper-parameters. Our experiments demonstrate FROST's capability to perform well when the composition of the unlabeled data is unknown; that is when the unlabeled data contain unequal numbers of each class and can contain out-of-distribution examples that don't belong to any of the training classes. High performance, speed of training, and insensitivity to hyper-parameters make FROST the most practical method for one-shot semi-supervised training. Our code is available at https://github.com/HelenaELiu/FROST.



### Adversarial collision attacks on image hashing functions
- **Arxiv ID**: http://arxiv.org/abs/2011.09473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09473v1)
- **Published**: 2020-11-18 18:59:02+00:00
- **Updated**: 2020-11-18 18:59:02+00:00
- **Authors**: Brian Dolhansky, Cristian Canton Ferrer
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing images with a perceptual algorithm is a common approach to solving duplicate image detection problems. However, perceptual image hashing algorithms are differentiable, and are thus vulnerable to gradient-based adversarial attacks. We demonstrate that not only is it possible to modify an image to produce an unrelated hash, but an exact image hash collision between a source and target image can be produced via minuscule adversarial perturbations. In a white box setting, these collisions can be replicated across nearly every image pair and hash type (including both deep and non-learned hashes). Furthermore, by attacking points other than the output of a hashing function, an attacker can avoid having to know the details of a particular algorithm, resulting in collisions that transfer across different hash sizes or model architectures. Using these techniques, an adversary can poison the image lookup table of a duplicate image detection service, resulting in undefined or unwanted behavior. Finally, we offer several potential mitigations to gradient-based image hash attacks.



### Extracting and Learning Fine-Grained Labels from Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2011.09517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.09517v1)
- **Published**: 2020-11-18 19:56:08+00:00
- **Updated**: 2020-11-18 19:56:08+00:00
- **Authors**: Tanveer Syeda-Mahmood, Ph. D, K. C. L Wong, Ph. D, Joy T. Wu, M. D., M. P. H, Ashutosh Jadhav, Ph. D, Orest Boyko, M. D. Ph. D
- **Comment**: This paper won the Homer R. Warner Award at AMIA 2020 awarded to a
  paper that best describes approaches to improving computerized information
  acquisition, knowledge data acquisition and management, and experimental
  results documenting the value of these approaches. The paper shows a
  combination of textual and visual processing to automatically recognize
  complex findings in chest X-rays
- **Journal**: None
- **Summary**: Chest radiographs are the most common diagnostic exam in emergency rooms and intensive care units today. Recently, a number of researchers have begun working on large chest X-ray datasets to develop deep learning models for recognition of a handful of coarse finding classes such as opacities, masses and nodules. In this paper, we focus on extracting and learning fine-grained labels for chest X-ray images. Specifically we develop a new method of extracting fine-grained labels from radiology reports by combining vocabulary-driven concept extraction with phrasal grouping in dependency parse trees for association of modifiers with findings. A total of 457 fine-grained labels depicting the largest spectrum of findings to date were selected and sufficiently large datasets acquired to train a new deep learning model designed for fine-grained classification. We show results that indicate a highly accurate label extraction process and a reliable learning of fine-grained labels. The resulting network, to our knowledge, is the first to recognize fine-grained descriptions of findings in images covering over nine modifiers including laterality, location, severity, size and appearance.



### TRAT: Tracking by Attention Using Spatio-Temporal Features
- **Arxiv ID**: http://arxiv.org/abs/2011.09524v1
- **DOI**: 10.1016/j.neucom.2022.04.043
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09524v1)
- **Published**: 2020-11-18 20:11:12+00:00
- **Updated**: 2020-11-18 20:11:12+00:00
- **Authors**: Hasan Saribas, Hakan Cevikalp, Okan Kpkl, Bedirhan Uzun
- **Comment**: None
- **Journal**: None
- **Summary**: Robust object tracking requires knowledge of tracked objects' appearance, motion and their evolution over time. Although motion provides distinctive and complementary information especially for fast moving objects, most of the recent tracking architectures primarily focus on the objects' appearance information. In this paper, we propose a two-stream deep neural network tracker that uses both spatial and temporal features. Our architecture is developed over ATOM tracker and contains two backbones: (i) 2D-CNN network to capture appearance features and (ii) 3D-CNN network to capture motion features. The features returned by the two networks are then fused with attention based Feature Aggregation Module (FAM). Since the whole architecture is unified, it can be trained end-to-end. The experimental results show that the proposed tracker TRAT (TRacking by ATtention) achieves state-of-the-art performance on most of the benchmarks and it significantly outperforms the baseline ATOM tracker.



### Contextual Fusion For Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2011.09526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09526v1)
- **Published**: 2020-11-18 20:13:23+00:00
- **Updated**: 2020-11-18 20:13:23+00:00
- **Authors**: Aiswarya Akumalla, Seth Haney, Maksim Bazhenov
- **Comment**: None
- **Journal**: None
- **Summary**: Mammalian brains handle complex reasoning tasks in a gestalt manner by integrating information from regions of the brain that are specialised to individual sensory modalities. This allows for improved robustness and better generalisation ability. In contrast, deep neural networks are usually designed to process one particular information stream and susceptible to various types of adversarial perturbations. While many methods exist for detecting and defending against adversarial attacks, they do not generalise across a range of attacks and negatively affect performance on clean, unperturbed data. We developed a fusion model using a combination of background and foreground features extracted in parallel from Places-CNN and Imagenet-CNN. We tested the benefits of the fusion approach on preserving adversarial robustness for human perceivable (e.g., Gaussian blur) and network perceivable (e.g., gradient-based) attacks for CIFAR-10 and MS COCO data sets. For gradient based attacks, our results show that fusion allows for significant improvements in classification without decreasing performance on unperturbed data and without need to perform adversarial retraining. Our fused model revealed improvements for Gaussian blur type perturbations as well. The increase in performance from fusion approach depended on the variability of the image contexts; larger increases were seen for classes of images with larger differences in their contexts. We also demonstrate the effect of regularization to bias the classifier decision in the presence of a known adversary. We propose that this biologically inspired approach to integrate information across multiple modalities provides a new way to improve adversarial robustness that can be complementary to current state of the art approaches.



### Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language
- **Arxiv ID**: http://arxiv.org/abs/2011.09530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09530v1)
- **Published**: 2020-11-18 20:21:19+00:00
- **Updated**: 2020-11-18 20:21:19+00:00
- **Authors**: Hassan Akbari, Hamid Palangi, Jianwei Yang, Sudha Rao, Asli Celikyilmaz, Roland Fernandez, Paul Smolensky, Jianfeng Gao, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Neuro-symbolic representations have proved effective in learning structure information in vision and language. In this paper, we propose a new model architecture for learning multi-modal neuro-symbolic representations for video captioning. Our approach uses a dictionary learning-based method of learning relations between videos and their paired text descriptions. We refer to these relations as relative roles and leverage them to make each token role-aware using attention. This results in a more structured and interpretable architecture that incorporates modality-specific inductive biases for the captioning task. Intuitively, the model is able to learn spatial, temporal, and cross-modal relations in a given pair of video and text. The disentanglement achieved by our proposal gives the model more capacity to capture multi-modal structures which result in captions with higher quality for videos. Our experiments on two established video captioning datasets verifies the effectiveness of the proposed approach based on automatic metrics. We further conduct a human evaluation to measure the grounding and relevance of the generated captions and observe consistent improvement for the proposed model. The codes and trained models can be found at https://github.com/hassanhub/R3Transformer



### StressNet: Detecting Stress in Thermal Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.09540v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP, H.1; I.2; I.3; I.4; I.5; J.3; J.4
- **Links**: [PDF](http://arxiv.org/pdf/2011.09540v2)
- **Published**: 2020-11-18 20:47:23+00:00
- **Updated**: 2020-11-23 18:38:23+00:00
- **Authors**: Satish Kumar, A S M Iftekhar, Michael Goebel, Tom Bullock, Mary H. MacLean, Michael B. Miller, Tyler Santander, Barry Giesbrecht, Scott T. Grafton, B. S. Manjunath
- **Comment**: 11 pages, 10 figues, 2 tables, Conference WACV2021
- **Journal**: None
- **Summary**: Precise measurement of physiological signals is critical for the effective monitoring of human vital signs. Recent developments in computer vision have demonstrated that signals such as pulse rate and respiration rate can be extracted from digital video of humans, increasing the possibility of contact-less monitoring. This paper presents a novel approach to obtaining physiological signals and classifying stress states from thermal video. The proposed network--"StressNet"--features a hybrid emission representation model that models the direct emission and absorption of heat by the skin and underlying blood vessels. This results in an information-rich feature representation of the face, which is used by spatio-temporal network for reconstructing the ISTI ( Initial Systolic Time Interval: a measure of change in cardiac sympathetic activity that is considered to be a quantitative index of stress in humans ). The reconstructed ISTI signal is fed into a stress-detection model to detect and classify the individual's stress state ( i.e. stress or no stress ). A detailed evaluation demonstrates that StressNet achieves estimated the ISTI signal with 95% accuracy and detect stress with average precision of 0.842. The source code is available on Github.



### Visual Diver Face Recognition for Underwater Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/2011.09556v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09556v1)
- **Published**: 2020-11-18 21:57:09+00:00
- **Updated**: 2020-11-18 21:57:09+00:00
- **Authors**: Jungseok Hong, Sadman Sakib Enan, Christopher Morse, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a deep-learned facial recognition method for underwater robots to identify scuba divers. Specifically, the proposed method is able to recognize divers underwater with faces heavily obscured by scuba masks and breathing apparatus. Our contribution in this research is towards robust facial identification of individuals under significant occlusion of facial features and image degradation from underwater optical distortions. With the ability to correctly recognize divers, autonomous underwater vehicles (AUV) will be able to engage in collaborative tasks with the correct person in human-robot teams and ensure that instructions are accepted from only those authorized to command the robots. We demonstrate that our proposed framework is able to learn discriminative features from real-world diver faces through different data augmentation and generation techniques. Experimental evaluations show that this framework achieves a 3-fold increase in prediction accuracy compared to the state-of-the-art (SOTA) algorithms and is well-suited for embedded inference on robotic platforms.



### Robustified Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2011.09563v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09563v2)
- **Published**: 2020-11-18 22:21:54+00:00
- **Updated**: 2021-03-24 21:18:25+00:00
- **Authors**: Jiajin Zhang, Hanqing Chao, Pingkun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) is widely used to transfer knowledge from a labeled source domain to an unlabeled target domain with different data distribution. While extensive studies attested that deep learning models are vulnerable to adversarial attacks, the adversarial robustness of models in domain adaptation application has largely been overlooked. This paper points out that the inevitable domain distribution deviation in UDA is a critical barrier to model robustness on the target domain. To address the problem, we propose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA) framework for training robust UDA models. With the introduced contrastive robust training and source anchored adversarial contrastive losses, our proposed CURDA framework can effectively robustify UDA models by simultaneously minimizing the data distribution deviation and the distance between target domain clean-adversarial pairs without creating classification confusion. Experiments on several public benchmarks show that CURDA can significantly improve model robustness in the target domain with only minor cost of accuracy on the clean samples.



### An Efficient and Scalable Deep Learning Approach for Road Damage Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.09577v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09577v3)
- **Published**: 2020-11-18 23:05:41+00:00
- **Updated**: 2020-12-17 17:58:08+00:00
- **Authors**: Sadra Naddaf-Sh, M-Mahdi Naddaf-Sh, Amir R. Kashani, Hassan Zargarzadeh
- **Comment**: removed redundant postscripts
- **Journal**: None
- **Summary**: Pavement condition evaluation is essential to time the preventative or rehabilitative actions and control distress propagation. Failing to conduct timely evaluations can lead to severe structural and financial loss of the infrastructure and complete reconstructions. Automated computer-aided surveying measures can provide a database of road damage patterns and their locations. This database can be utilized for timely road repairs to gain the minimum cost of maintenance and the asphalt's maximum durability. This paper introduces a deep learning-based surveying scheme to analyze the image-based distress data in real-time. A database consisting of a diverse population of crack distress types such as longitudinal, transverse, and alligator cracks, photographed using mobile-device is used. Then, a family of efficient and scalable models that are tuned for pavement crack detection is trained, and various augmentation policies are explored. Proposed models, resulted in F1-scores, ranging from 52% to 56%, and average inference time from 178-10 images per second. Finally, the performance of the object detectors are examined, and error analysis is reported against various images. The source code is available at https://github.com/mahdi65/roadDamageDetection2020.



### Patient-independent Epileptic Seizure Prediction using Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2011.09581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09581v1)
- **Published**: 2020-11-18 23:13:48+00:00
- **Updated**: 2020-11-18 23:13:48+00:00
- **Authors**: Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: Epilepsy is one of the most prevalent neurological diseases among humans and can lead to severe brain injuries, strokes, and brain tumors. Early detection of seizures can help to mitigate injuries, and can be used to aid the treatment of patients with epilepsy. The purpose of a seizure prediction system is to successfully identify the pre-ictal brain stage, which occurs before a seizure event. Patient-independent seizure prediction models are designed to offer accurate performance across multiple subjects within a dataset, and have been identified as a real-world solution to the seizure prediction problem. However, little attention has been given for designing such models to adapt to the high inter-subject variability in EEG data. Methods: We propose two patient-independent deep learning architectures with different learning strategies that can learn a global function utilizing data from multiple subjects. Results: Proposed models achieve state-of-the-art performance for seizure prediction on the CHB-MIT-EEG dataset, demonstrating 88.81% and 91.54% accuracy respectively. Conclusions: The Siamese model trained on the proposed learning strategy is able to learn patterns related to patient variations in data while predicting seizures. Significance: Our models show superior performance for patient-independent seizure prediction, and the same architecture can be used as a patient-specific classifier after model adaptation. We are the first study that employs model interpretation to understand classifier behavior for the task for seizure prediction, and we also show that the MFCC feature map utilized by our models contains predictive biomarkers related to interictal and pre-ictal brain states.



### ACRONYM: A Large-Scale Grasp Dataset Based on Simulation
- **Arxiv ID**: http://arxiv.org/abs/2011.09584v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09584v1)
- **Published**: 2020-11-18 23:24:00+00:00
- **Updated**: 2020-11-18 23:24:00+00:00
- **Authors**: Clemens Eppner, Arsalan Mousavian, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce ACRONYM, a dataset for robot grasp planning based on physics simulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872 objects from 262 different categories, each labeled with the grasp result obtained from a physics simulator. We show the value of this large and diverse dataset by using it to train two state-of-the-art learning-based grasp planning algorithms. Grasp performance improves significantly when compared to the original smaller dataset. Data and tools can be accessed at https://sites.google.com/nvidia.com/graspdataset.




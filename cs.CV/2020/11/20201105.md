# Arxiv Papers in cs.CV on 2020-11-05
### Universal Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2011.02594v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.02594v1)
- **Published**: 2020-11-05 00:20:38+00:00
- **Updated**: 2020-11-05 00:20:38+00:00
- **Authors**: Yueming Yin, Zhen Yang, Haifeng Hu, Xiaofu Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation enables intelligent models to transfer knowledge from a labeled source domain to a similar but unlabeled target domain. Recent study reveals that knowledge can be transferred from one source domain to another unknown target domain, called Universal Domain Adaptation (UDA). However, in the real-world application, there are often more than one source domain to be exploited for domain adaptation. In this paper, we formally propose a more general domain adaptation setting, universal multi-source domain adaptation (UMDA), where the label sets of multiple source domains can be different and the label set of target domain is completely unknown. The main challenges in UMDA are to identify the common label set between each source domain and target domain, and to keep the model scalable as the number of source domains increases. To address these challenges, we propose a universal multi-source adaptation network (UMAN) to solve the domain adaptation problem without increasing the complexity of the model in various UMDA settings. In UMAN, we estimate the reliability of each known class in the common label set via the prediction margin, which helps adversarial training to better align the distributions of multiple source domains and target domain in the common label set. Moreover, the theoretical guarantee for UMAN is also provided. Massive experimental results show that existing UDA and multi-source DA (MDA) methods cannot be directly applied to UMDA and the proposed UMAN achieves the state-of-the-art performance in various UMDA settings.



### Transforming Facial Weight of Real Images by Editing Latent Space of StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2011.02606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02606v1)
- **Published**: 2020-11-05 01:45:18+00:00
- **Updated**: 2020-11-05 01:45:18+00:00
- **Authors**: V N S Rama Krishna Pinnimty, Matt Zhao, Palakorn Achananuparp, Ee-Peng Lim
- **Comment**: None
- **Journal**: None
- **Summary**: We present an invert-and-edit framework to automatically transform facial weight of an input face image to look thinner or heavier by leveraging semantic facial attributes encoded in the latent space of Generative Adversarial Networks (GANs). Using a pre-trained StyleGAN as the underlying generator, we first employ an optimization-based embedding method to invert the input image into the StyleGAN latent space. Then, we identify the facial-weight attribute direction in the latent space via supervised learning and edit the inverted latent code by moving it positively or negatively along the extracted feature axis. Our framework is empirically shown to produce high-quality and realistic facial-weight transformations without requiring training GANs with a large amount of labeled face images from scratch. Ultimately, our framework can be utilized as part of an intervention to motivate individuals to make healthier food choices by visualizing the future impacts of their behavior on appearance.



### Learning a Decentralized Multi-arm Motion Planner
- **Arxiv ID**: http://arxiv.org/abs/2011.02608v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2011.02608v1)
- **Published**: 2020-11-05 01:47:23+00:00
- **Updated**: 2020-11-05 01:47:23+00:00
- **Authors**: Huy Ha, Jingxi Xu, Shuran Song
- **Comment**: CoRL 2020
- **Journal**: None
- **Summary**: We present a closed-loop multi-arm motion planner that is scalable and flexible with team size. Traditional multi-arm robot systems have relied on centralized motion planners, whose runtimes often scale exponentially with team size, and thus, fail to handle dynamic environments with open-loop control. In this paper, we tackle this problem with multi-agent reinforcement learning, where a decentralized policy is trained to control one robot arm in the multi-arm system to reach its target end-effector pose given observations of its workspace state and target end-effector pose. The policy is trained using Soft Actor-Critic with expert demonstrations from a sampling-based motion planning algorithm (i.e., BiRRT). By leveraging classical planning algorithms, we can improve the learning efficiency of the reinforcement learning algorithm while retaining the fast inference time of neural networks. The resulting policy scales sub-linearly and can be deployed on multi-arm systems with variable team sizes. Thanks to the closed-loop and decentralized formulation, our approach generalizes to 5-10 multi-arm systems and dynamic moving targets (>90% success rate for a 10-arm system), despite being trained on only 1-4 arm planning tasks with static targets. Code and data links can be found at https://multiarm.cs.columbia.edu.



### Lets Play Music: Audio-driven Performance Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.02631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02631v1)
- **Published**: 2020-11-05 03:13:46+00:00
- **Updated**: 2020-11-05 03:13:46+00:00
- **Authors**: Hao Zhu, Yi Li, Feixia Zhu, Aihua Zheng, Ran He
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: We propose a new task named Audio-driven Per-formance Video Generation (APVG), which aims to synthesizethe video of a person playing a certain instrument guided bya given music audio clip. It is a challenging task to gener-ate the high-dimensional temporal consistent videos from low-dimensional audio modality. In this paper, we propose a multi-staged framework to achieve this new task to generate realisticand synchronized performance video from given music. Firstly,we provide both global appearance and local spatial informationby generating the coarse videos and keypoints of body and handsfrom a given music respectively. Then, we propose to transformthe generated keypoints to heatmap via a differentiable spacetransformer, since the heatmap offers more spatial informationbut is harder to generate directly from audio. Finally, wepropose a Structured Temporal UNet (STU) to extract bothintra-frame structured information and inter-frame temporalconsistency. They are obtained via graph-based structure module,and CNN-GRU based high-level temporal module respectively forfinal video generation. Comprehensive experiments validate theeffectiveness of our proposed framework.



### GPR-based Model Reconstruction System for Underground Utilities Using GPRNet
- **Arxiv ID**: http://arxiv.org/abs/2011.02635v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02635v3)
- **Published**: 2020-11-05 03:26:01+00:00
- **Updated**: 2021-05-18 16:06:31+00:00
- **Authors**: Jinglun Feng, Liang Yang, Ejup Hoxha, Diar Sanakov, Stanislav Sotnikov, Jizhong Xiao
- **Comment**: Accepted by ICRA 2021
- **Journal**: None
- **Summary**: Ground Penetrating Radar (GPR) is one of the most important non-destructive evaluation (NDE) instruments to detect and locate underground objects (i.e., rebars, utility pipes). Many previous researches focus on GPR image-based feature detection only, and none can process sparse GPR measurements to successfully reconstruct a very fine and detailed 3D model of underground objects for better visualization. To address this problem, this paper presents a novel robotic system to collect GPR data, localize the underground utilities, and reconstruct the underground objects' dense point cloud model. This system is composed of three modules: 1) visual-inertial-based GPR data collection module, which tags the GPR measurements with positioning information provided by an omnidirectional robot; 2) a deep neural network (DNN) migration module to interpret the raw GPR B-scan image into a cross-section of object model; 3) a DNN-based 3D reconstruction module, i.e., GPRNet, to generate underground utility model with the fine 3D point cloud. In this paper, both the quantitative and qualitative experiment results verify our method that can generate a dense and complete point cloud model of pipe-shaped utilities based on a sparse input, i.e., GPR raw data incompleteness and various noise. The experiment results on synthetic data and field test data further support the effectiveness of our approach.



### Towards Disentangling Latent Space for Unsupervised Semantic Face Editing
- **Arxiv ID**: http://arxiv.org/abs/2011.02638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02638v2)
- **Published**: 2020-11-05 03:29:24+00:00
- **Updated**: 2021-07-19 01:21:52+00:00
- **Authors**: Kanglin Liu, Gaofeng Cao, Fei Zhou, Bozhi Liu, Jiang Duan, Guoping Qiu
- **Comment**: 11pages, 8 figures
- **Journal**: None
- **Summary**: Facial attributes in StyleGAN generated images are entangled in the latent space which makes it very difficult to independently control a specific attribute without affecting the others. Supervised attribute editing requires annotated training data which is difficult to obtain and limits the editable attributes to those with labels. Therefore, unsupervised attribute editing in an disentangled latent space is key to performing neat and versatile semantic face editing. In this paper, we present a new technique termed Structure-Texture Independent Architecture with Weight Decomposition and Orthogonal Regularization (STIA-WO) to disentangle the latent space for unsupervised semantic face editing. By applying STIA-WO to GAN, we have developed a StyleGAN termed STGAN-WO which performs weight decomposition through utilizing the style vector to construct a fully controllable weight matrix to regulate image synthesis, and employs orthogonal regularization to ensure each entry of the style vector only controls one independent feature matrix. To further disentangle the facial attributes, STGAN-WO introduces a structure-texture independent architecture which utilizes two independently and identically distributed (i.i.d.) latent vectors to control the synthesis of the texture and structure components in a disentangled way. Unsupervised semantic editing is achieved by moving the latent code in the coarse layers along its orthogonal directions to change texture related attributes or changing the latent code in the fine layers to manipulate structure related ones. We present experimental results which show that our new STGAN-WO can achieve better attribute editing than state of the art methods.



### Learning to Respond with Your Favorite Stickers: A Framework of Unifying Multi-Modality and User Preference in Multi-Turn Dialog
- **Arxiv ID**: http://arxiv.org/abs/2011.03322v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.03322v1)
- **Published**: 2020-11-05 03:31:17+00:00
- **Updated**: 2020-11-05 03:31:17+00:00
- **Authors**: Shen Gao, Xiuying Chen, Li Liu, Dongyan Zhao, Rui Yan
- **Comment**: Accepted by TOIS. arXiv admin note: substantial text overlap with
  arXiv:2003.04679
- **Journal**: None
- **Summary**: Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching the stickers image with previous utterances. However, existing methods usually focus on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers. Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context and sticker using history of user. Two main challenges are confronted in this task. One is to model the sticker preference of user based on the previous sticker selection history. Another challenge is to jointly fuse the user preference and the matching between dialog context and candidate sticker into final prediction making. To tackle these challenges, we propose a \emph{Preference Enhanced Sticker Response Selector} (PESRS) model. Specifically, PESRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker and each utterance. Then, we model the user preference by using the recently selected stickers as input, and use a key-value memory network to store the preference representation. PESRS then learns the short-term and long-term dependency between all interaction results by a fusion network, and dynamically fuse the user preference representation into the final sticker selection prediction. Extensive experiments conducted on a large-scale real-world dialog dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics. Experiments also verify the effectiveness of each component of PESRS.



### Utilizing Every Image Object for Semi-supervised Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/2011.02655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02655v1)
- **Published**: 2020-11-05 04:25:25+00:00
- **Updated**: 2020-11-05 04:25:25+00:00
- **Authors**: Haidong Zhu, Arka Sadhu, Zhaoheng Zheng, Ram Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: Phrase grounding models localize an object in the image given a referring expression. The annotated language queries available during training are limited, which also limits the variations of language combinations that a model can see during training. In this paper, we study the case applying objects without labeled queries for training the semi-supervised phrase grounding. We propose to use learned location and subject embedding predictors (LSEP) to generate the corresponding language embeddings for objects lacking annotated queries in the training set. With the assistance of the detector, we also apply LSEP to train a grounding model on images without any annotation. We evaluate our method based on MAttNet on three public datasets: RefCOCO, RefCOCO+, and RefCOCOg. We show that our predictors allow the grounding system to learn from the objects without labeled queries and improve accuracy by 34.9\% relatively with the detection results.



### Compositional Scalable Object SLAM
- **Arxiv ID**: http://arxiv.org/abs/2011.02658v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02658v1)
- **Published**: 2020-11-05 04:46:25+00:00
- **Updated**: 2020-11-05 04:46:25+00:00
- **Authors**: Akash Sharma, Wei Dong, Michael Kaess
- **Comment**: Submitted to the 2021 IEEE International Conference on Robotics and
  Automation (ICRA) 7 pages, 7 figures
- **Journal**: None
- **Summary**: We present a fast, scalable, and accurate Simultaneous Localization and Mapping (SLAM) system that represents indoor scenes as a graph of objects. Leveraging the observation that artificial environments are structured and occupied by recognizable objects, we show that a compositional scalable object mapping formulation is amenable to a robust SLAM solution for drift-free large scale indoor reconstruction. To achieve this, we propose a novel semantically assisted data association strategy that obtains unambiguous persistent object landmarks, and a 2.5D compositional rendering method that enables reliable frame-to-model RGB-D tracking. Consequently, we deliver an optimized online implementation that can run at near frame rate with a single graphics card, and provide a comprehensive evaluation against state of the art baselines. An open source implementation will be provided at https://placeholder.



### Deep Active Learning with Augmentation-based Consistency Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.02666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02666v1)
- **Published**: 2020-11-05 05:22:58+00:00
- **Updated**: 2020-11-05 05:22:58+00:00
- **Authors**: SeulGi Hong, Heonjin Ha, Junmo Kim, Min-Kook Choi
- **Comment**: None
- **Journal**: None
- **Summary**: In active learning, the focus is mainly on the selection strategy of unlabeled data for enhancing the generalization capability of the next learning cycle. For this, various uncertainty measurement methods have been proposed. On the other hand, with the advent of data augmentation metrics as the regularizer on general deep learning, we notice that there can be a mutual influence between the method of unlabeled data selection and the data augmentation-based regularization techniques in active learning scenarios. Through various experiments, we confirmed that consistency-based regularization from analytical learning theory could affect the generalization capability of the classifier in combination with the existing uncertainty measurement method. By this fact, we propose a methodology to improve generalization ability, by applying data augmentation-based techniques to an active learning scenario. For the data augmentation-based regularization loss, we redefined cutout (co) and cutmix (cm) strategies as quantitative metrics and applied at both model training and unlabeled data selection steps. We have shown that the augmentation-based regularizer can lead to improved performance on the training step of active learning, while that same approach can be effectively combined with the uncertainty measurement metrics proposed so far. We used datasets such as FashionMNIST, CIFAR10, CIFAR100, and STL10 to verify the performance of the proposed active learning technique for multiple image classification tasks. Our experiments show consistent performance gains for each dataset and budget scenario.



### AOT: Appearance Optimal Transport Based Identity Swapping for Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.02674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02674v1)
- **Published**: 2020-11-05 06:17:04+00:00
- **Updated**: 2020-11-05 06:17:04+00:00
- **Authors**: Hao Zhu, Chaoyou Fu, Qianyi Wu, Wayne Wu, Chen Qian, Ran He
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Recent studies have shown that the performance of forgery detection can be improved with diverse and challenging Deepfakes datasets. However, due to the lack of Deepfakes datasets with large variance in appearance, which can be hardly produced by recent identity swapping methods, the detection algorithm may fail in this situation. In this work, we provide a new identity swapping algorithm with large differences in appearance for face forgery detection. The appearance gaps mainly arise from the large discrepancies in illuminations and skin colors that widely exist in real-world scenarios. However, due to the difficulties of modeling the complex appearance mapping, it is challenging to transfer fine-grained appearances adaptively while preserving identity traits. This paper formulates appearance mapping as an optimal transport problem and proposes an Appearance Optimal Transport model (AOT) to formulate it in both latent and pixel space. Specifically, a relighting generator is designed to simulate the optimal transport plan. It is solved via minimizing the Wasserstein distance of the learned features in the latent space, enabling better performance and less computation than conventional optimization. To further refine the solution of the optimal transport plan, we develop a segmentation game to minimize the Wasserstein distance in the pixel space. A discriminator is introduced to distinguish the fake parts from a mix of real and fake image patches. Extensive experiments reveal that the superiority of our method when compared with state-of-the-art methods and the ability of our generated data to improve the performance of face forgery detection.



### Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for Perturbation Difficulty
- **Arxiv ID**: http://arxiv.org/abs/2011.02675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02675v2)
- **Published**: 2020-11-05 06:21:24+00:00
- **Updated**: 2020-11-07 02:57:50+00:00
- **Authors**: Camilo Pestana, Wei Liu, David Glance, Ajmal Mian
- **Comment**: Paper Accepted at WACV 2021
- **Journal**: None
- **Summary**: Dataset bias is a problem in adversarial machine learning, especially in the evaluation of defenses. An adversarial attack or defense algorithm may show better results on the reported dataset than can be replicated on other datasets. Even when two algorithms are compared, their relative performance can vary depending on the dataset. Deep learning offers state-of-the-art solutions for image recognition, but deep models are vulnerable even to small perturbations. Research in this area focuses primarily on adversarial attacks and defense algorithms. In this paper, we report for the first time, a class of robust images that are both resilient to attacks and that recover better than random images under adversarial attacks using simple defense techniques. Thus, a test dataset with a high proportion of robust images gives a misleading impression about the performance of an adversarial attack or defense. We propose three metrics to determine the proportion of robust images in a dataset and provide scoring to determine the dataset bias. We also provide an ImageNet-R dataset of 15000+ robust images to facilitate further research on this intriguing phenomenon of image strength under attack. Our dataset, combined with the proposed metrics, is valuable for unbiased benchmarking of adversarial attack and defense algorithms.



### A Multi-resolution Model for Histopathology Image Classification and Localization with Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.02679v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02679v1)
- **Published**: 2020-11-05 06:42:39+00:00
- **Updated**: 2020-11-05 06:42:39+00:00
- **Authors**: Jiayun Li, Wenyuan Li, Anthony Sisk, Huihui Ye, W. Dean Wallace, William Speier, Corey W. Arnold
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Histopathological images provide rich information for disease diagnosis. Large numbers of histopathological images have been digitized into high resolution whole slide images, opening opportunities in developing computational image analysis tools to reduce pathologists' workload and potentially improve inter- and intra- observer agreement. Most previous work on whole slide image analysis has focused on classification or segmentation of small pre-selected regions-of-interest, which requires fine-grained annotation and is non-trivial to extend for large-scale whole slide analysis. In this paper, we proposed a multi-resolution multiple instance learning model that leverages saliency maps to detect suspicious regions for fine-grained grade prediction. Instead of relying on expensive region- or pixel-level annotations, our model can be trained end-to-end with only slide-level labels. The model is developed on a large-scale prostate biopsy dataset containing 20,229 slides from 830 patients. The model achieved 92.7% accuracy, 81.8% Cohen's Kappa for benign, low grade (i.e. Grade group 1) and high grade (i.e. Grade group >= 2) prediction, an area under the receiver operating characteristic curve (AUROC) of 98.2% and an average precision (AP) of 97.4% for differentiating malignant and benign slides. The model obtained an AUROC of 99.4% and an AP of 99.8% for cancer detection on an external dataset.



### Center-wise Local Image Mixture For Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.02697v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02697v3)
- **Published**: 2020-11-05 08:20:31+00:00
- **Updated**: 2021-10-18 02:15:36+00:00
- **Authors**: Hao Li, Xiaopeng Zhang, Hongkai Xiong
- **Comment**: Accepted by BMVC2021
- **Journal**: None
- **Summary**: Contrastive learning based on instance discrimination trains model to discriminate different transformations of the anchor sample from other samples, which does not consider the semantic similarity among samples. This paper proposes a new kind of contrastive learning method, named CLIM, which uses positives from other samples in the dataset. This is achieved by searching local similar samples of the anchor, and selecting samples that are closer to the corresponding cluster center, which we denote as center-wise local image selection. The selected samples are instantiated via an data mixture strategy, which performs as a smoothing regularization. As a result, CLIM encourages both local similarity and global aggregation in a robust way, which we find is beneficial for feature representation. Besides, we introduce \emph{multi-resolution} augmentation, which enables the representation to be scale invariant. We reach 75.5% top-1 accuracy with linear evaluation over ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels.



### DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.02709v3
- **DOI**: 10.1109/IJCNN52387.2021.9533527
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02709v3)
- **Published**: 2020-11-05 08:57:15+00:00
- **Updated**: 2020-11-21 23:59:25+00:00
- **Authors**: Zhenxing Zhang, Lambert Schomaker
- **Comment**: None
- **Journal**: 2021 International Joint Conference on Neural Networks (IJCNN)
  Proceedings
- **Summary**: Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. Visualization of the attention maps shows that the channel-aware attention module is able to localize the discriminative regions, while the pixel-aware attention module has the ability to capture the globally visual contents for the generation of an image.



### Few-Shot Object Detection in Real Life: Case Study on Auto-Harvest
- **Arxiv ID**: http://arxiv.org/abs/2011.02719v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02719v1)
- **Published**: 2020-11-05 09:24:33+00:00
- **Updated**: 2020-11-05 09:24:33+00:00
- **Authors**: Kevin Riou, Jingwen Zhu, Suiyi Ling, Mathis Piquet, Vincent Truffault, Patrick Le Callet
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Confinement during COVID-19 has caused serious effects on agriculture all over the world. As one of the efficient solutions, mechanical harvest/auto-harvest that is based on object detection and robotic harvester becomes an urgent need. Within the auto-harvest system, robust few-shot object detection model is one of the bottlenecks, since the system is required to deal with new vegetable/fruit categories and the collection of large-scale annotated datasets for all the novel categories is expensive. There are many few-shot object detection models that were developed by the community. Yet whether they could be employed directly for real life agricultural applications is still questionable, as there is a context-gap between the commonly used training datasets and the images collected in real life agricultural scenarios. To this end, in this study, we present a novel cucumber dataset and propose two data augmentation strategies that help to bridge the context-gap. Experimental results show that 1) the state-of-the-art few-shot object detection model performs poorly on the novel `cucumber' category; and 2) the proposed augmentation strategies outperform the commonly used ones.



### An analysis of the transfer learning of convolutional neural networks for artistic images
- **Arxiv ID**: http://arxiv.org/abs/2011.02727v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02727v2)
- **Published**: 2020-11-05 09:45:32+00:00
- **Updated**: 2020-11-24 13:18:23+00:00
- **Authors**: Nicolas Gonthier, Yann Gousseau, Sa√Ød Ladjal
- **Comment**: Accepted at Workshop on Fine Art Pattern Extraction and Recognition
  (FAPER), ICPR, 2020
- **Journal**: None
- **Summary**: Transfer learning from huge natural image datasets, fine-tuning of deep neural networks and the use of the corresponding pre-trained networks have become de facto the core of art analysis applications. Nevertheless, the effects of transfer learning are still poorly understood. In this paper, we first use techniques for visualizing the network internal representations in order to provide clues to the understanding of what the network has learned on artistic images. Then, we provide a quantitative analysis of the changes introduced by the learning process thanks to metrics in both the feature and parameter spaces, as well as metrics computed on the set of maximal activation images. These analyses are performed on several variations of the transfer learning procedure. In particular, we observed that the network could specialize some pre-trained filters to the new image modality and also that higher layers tend to concentrate classes. Finally, we have shown that a double fine-tuning involving a medium-size artistic dataset can improve the classification on smaller datasets, even when the task changes.



### Goal-driven Long-Term Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.02751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02751v2)
- **Published**: 2020-11-05 10:47:33+00:00
- **Updated**: 2020-11-06 04:11:11+00:00
- **Authors**: Hung Tran, Vuong Le, Truyen Tran
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: The prediction of humans' short-term trajectories has advanced significantly with the use of powerful sequential modeling and rich environment feature extraction. However, long-term prediction is still a major challenge for the current methods as the errors could accumulate along the way. Indeed, consistent and stable prediction far to the end of a trajectory inherently requires deeper analysis into the overall structure of that trajectory, which is related to the pedestrian's intention on the destination of the journey. In this work, we propose to model a hypothetical process that determines pedestrians' goals and the impact of such process on long-term future trajectories. We design Goal-driven Trajectory Prediction model - a dual-channel neural network that realizes such intuition. The two channels of the network take their dedicated roles and collaborate to generate future trajectories. Different than conventional goal-conditioned, planning-based methods, the model architecture is designed to generalize the patterns and work across different scenes with arbitrary geometrical and semantic structures. The model is shown to outperform the state-of-the-art in various settings, especially in large prediction horizons. This result is another evidence for the effectiveness of adaptive structured representation of visual and geometrical features in human behavior analysis.



### Robust Unsupervised Video Anomaly Detection by Multi-Path Frame Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.02763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02763v2)
- **Published**: 2020-11-05 11:34:12+00:00
- **Updated**: 2021-05-27 05:53:57+00:00
- **Authors**: Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, Qi Qi
- **Comment**: Paper accepted by IEEE Transactions on Neural Networks and Learning
  Systems (TNNLS). Article DOI: 10.1109/TNNLS.2021.3083152
- **Journal**: None
- **Summary**: Video anomaly detection is commonly used in many applications such as security surveillance and is very challenging.A majority of recent video anomaly detection approaches utilize deep reconstruction models, but their performance is often suboptimal because of insufficient reconstruction error differences between normal and abnormal video frames in practice. Meanwhile, frame prediction-based anomaly detection methods have shown promising performance. In this paper, we propose a novel and robust unsupervised video anomaly detection method by frame prediction with proper design which is more in line with the characteristics of surveillance videos. The proposed method is equipped with a multi-path ConvGRU-based frame prediction network that can better handle semantically informative objects and areas of different scales and capture spatial-temporal dependencies in normal videos. A noise tolerance loss is introduced during training to mitigate the interference caused by background noise. Extensive experiments have been conducted on the CUHK Avenue, ShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that our proposed method outperforms existing state-of-the-art approaches. Remarkably, our proposed method obtains the frame-level AUROC score of 88.3% on the CUHK Avenue dataset.



### Fast Object Detection with Latticed Multi-Scale Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.02780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.02780v1)
- **Published**: 2020-11-05 12:16:30+00:00
- **Updated**: 2020-11-05 12:16:30+00:00
- **Authors**: Yue Shi, Bo Jiang, Zhengping Che, Jian Tang
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Scale variance is one of the crucial challenges in multi-scale object detection. Early approaches address this problem by exploiting the image and feature pyramid, which raises suboptimal results with computation burden and constrains from inherent network structures. Pioneering works also propose multi-scale (i.e., multi-level and multi-branch) feature fusions to remedy the issue and have achieved encouraging progress. However, existing fusions still have certain limitations such as feature scale inconsistency, ignorance of level-wise semantic transformation, and coarse granularity. In this work, we present a novel module, the Fluff block, to alleviate drawbacks of current multi-scale fusion methods and facilitate multi-scale object detection. Specifically, Fluff leverages both multi-level and multi-branch schemes with dilated convolutions to have rapid, effective and finer-grained feature fusions. Furthermore, we integrate Fluff to SSD as FluffNet, a powerful real-time single-stage detector for multi-scale object detection. Empirical results on MS COCO and PASCAL VOC have demonstrated that FluffNet obtains remarkable efficiency with state-of-the-art accuracy. Additionally, we indicate the great generality of the Fluff block by showing how to embed it to other widely-used detectors as well.



### Deep Metric Learning with Spherical Embedding
- **Arxiv ID**: http://arxiv.org/abs/2011.02785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02785v1)
- **Published**: 2020-11-05 12:32:12+00:00
- **Updated**: 2020-11-05 12:32:12+00:00
- **Authors**: Dingyi Zhang, Yingming Li, Zhongfei Zhang
- **Comment**: To appear in NeurIPS 2020. Code is available at
  https://github.com/Dyfine/SphericalEmbedding
- **Journal**: None
- **Summary**: Deep metric learning has attracted much attention in recent years, due to seamlessly combining the distance metric learning and deep neural network. Many endeavors are devoted to design different pair-based angular loss functions, which decouple the magnitude and direction information for embedding vectors and ensure the training and testing measure consistency. However, these traditional angular losses cannot guarantee that all the sample embeddings are on the surface of the same hypersphere during the training stage, which would result in unstable gradient in batch optimization and may influence the quick convergence of the embedding learning. In this paper, we first investigate the effect of the embedding norm for deep metric learning with angular distance, and then propose a spherical embedding constraint (SEC) to regularize the distribution of the norms. SEC adaptively adjusts the embeddings to fall on the same hypersphere and performs more balanced direction update. Extensive experiments on deep metric learning, face recognition, and contrastive self-supervised learning show that the SEC-based angular space learning strategy significantly improves the performance of the state-of-the-art.



### Intriguing Properties of Contrastive Losses
- **Arxiv ID**: http://arxiv.org/abs/2011.02803v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.02803v3)
- **Published**: 2020-11-05 13:19:48+00:00
- **Updated**: 2021-10-23 18:25:17+00:00
- **Authors**: Ting Chen, Calvin Luo, Lala Li
- **Comment**: NeurIPS 2021. Code and visualization at
  https://contrastive-learning.github.io/intriguing
- **Journal**: None
- **Summary**: We study three intriguing properties of contrastive learning. First, we generalize the standard contrastive loss to a broader family of losses, and we find that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head. Second, we study if instance-based contrastive learning (with a global image representation) can learn well on images with multiple objects present. We find that meaningful hierarchical local features can be learned despite the fact that these objectives operate on global instance-level features. Finally, we study the phenomenon of feature suppression among competing features shared across augmented views, such as "color distribution" vs "object class". We construct datasets with explicit and controllable competing features, and show that, for contrastive learning, a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features over others, and could suffer from learning saturation for scenarios where existing augmentations cannot fully address the feature suppression. This poses open challenges to existing contrastive learning techniques.



### UAV-AdNet: Unsupervised Anomaly Detection using Deep Neural Networks for Aerial Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2011.02853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.02853v1)
- **Published**: 2020-11-05 14:26:29+00:00
- **Updated**: 2020-11-05 14:26:29+00:00
- **Authors**: Ilker Bozcan, Erdal Kayacan
- **Comment**: 7 pages, 4 figures, 2 tables, accepted to the 2020 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2020)
- **Journal**: None
- **Summary**: Anomaly detection is a key goal of autonomous surveillance systems that should be able to alert unusual observations. In this paper, we propose a holistic anomaly detection system using deep neural networks for surveillance of critical infrastructures (e.g., airports, harbors, warehouses) using an unmanned aerial vehicle (UAV). First, we present a heuristic method for the explicit representation of spatial layouts of objects in bird-view images. Then, we propose a deep neural network architecture for unsupervised anomaly detection (UAV-AdNet), which is trained on environment representations and GPS labels of bird-view images jointly. Unlike studies in the literature, we combine GPS and image data to predict abnormal observations. We evaluate our model against several baselines on our aerial surveillance dataset and show that it performs better in scene reconstruction and several anomaly detection tasks. The codes, trained models, dataset, and video will be available at https://bozcani.github.io/uavadnet.



### A Tree-structure Convolutional Neural Network for Temporal Features Exaction on Sensor-based Multi-resident Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.03042v1
- **DOI**: 10.1007/978-981-15-7670-6_43
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03042v1)
- **Published**: 2020-11-05 14:31:00+00:00
- **Updated**: 2020-11-05 14:31:00+00:00
- **Authors**: Jingjing Cao, Fukang Guo, Xin Lai, Qiang Zhou, Jinshan Dai
- **Comment**: 12 pages, 4 figures
- **Journal**: International Conference on Neural Computing for Advanced
  Applications NCAA 2020: Neural Computing for Advanced Applications pp 513-525
- **Summary**: With the propagation of sensor devices applied in smart home, activity recognition has ignited huge interest and most existing works assume that there is only one habitant. While in reality, there are generally multiple residents at home, which brings greater challenge to recognize activities. In addition, many conventional approaches rely on manual time series data segmentation ignoring the inherent characteristics of events and their heuristic hand-crafted feature generation algorithms are difficult to exploit distinctive features to accurately classify different activities. To address these issues, we propose an end-to-end Tree-Structure Convolutional neural network based framework for Multi-Resident Activity Recognition (TSC-MRAR). First, we treat each sample as an event and obtain the current event embedding through the previous sensor readings in the sliding window without splitting the time series data. Then, in order to automatically generate the temporal features, a tree-structure network is designed to derive the temporal dependence of nearby readings. The extracted features are fed into the fully connected layer, which can jointly learn the residents labels and the activity labels simultaneously. Finally, experiments on CASAS datasets demonstrate the high performance in multi-resident activity recognition of our model compared to state-of-the-art techniques.



### This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.02863v2
- **DOI**: 10.1007/978-3-030-93736-2_34
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02863v2)
- **Published**: 2020-11-05 14:43:07+00:00
- **Updated**: 2021-03-31 07:13:23+00:00
- **Authors**: Meike Nauta, Annemarie Jutte, Jesper Provoost, Christin Seifert
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Image recognition with prototypes is considered an interpretable alternative for black box deep learning models. Classification depends on the extent to which a test image "looks like" a prototype. However, perceptual similarity for humans can be different from the similarity learned by the classification model. Hence, only visualising prototypes can be insufficient for a user to understand what a prototype exactly represents, and why the model considers a prototype and an image to be similar. We address this ambiguity and argue that prototypes should be explained. We improve interpretability by automatically enhancing visual prototypes with textual quantitative information about visual characteristics deemed important by the classification model. Specifically, our method clarifies the meaning of a prototype by quantifying the influence of colour hue, shape, texture, contrast and saturation and can generate both global and local explanations. Because of the generality of our approach, it can improve the interpretability of any similarity-based method for prototypical image recognition. In our experiments, we apply our method to the existing Prototypical Part Network (ProtoPNet). Our analysis confirms that the global explanations are generalisable, and often correspond to the visually perceptible properties of a prototype. Our explanations are especially relevant for prototypes which might have been interpreted incorrectly otherwise. By explaining such 'misleading' prototypes, we improve the interpretability and simulatability of a prototype-based classification model. We also use our method to check whether visually similar prototypes have similar explanations, and are able to discover redundancy. Code is available at https://github.com/M-Nauta/Explaining_Prototypes .



### Improving Robotic Grasping on Monocular Images Via Multi-Task Learning and Positional Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.02888v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02888v1)
- **Published**: 2020-11-05 14:58:30+00:00
- **Updated**: 2020-11-05 14:58:30+00:00
- **Authors**: William Prew, Toby Breckon, Magnus Bordewich, Ulrik Beierholm
- **Comment**: 8 pages, 6 figures, Accepted at the International Conference on
  Pattern Recognition 2020 (ICPR)
- **Journal**: None
- **Summary**: In this paper, we introduce two methods of improving real-time object grasping performance from monocular colour images in an end-to-end CNN architecture. The first is the addition of an auxiliary task during model training (multi-task learning). Our multi-task CNN model improves grasping performance from a baseline average of 72.04% to 78.14% on the large Jacquard grasping dataset when performing a supplementary depth reconstruction task. The second is introducing a positional loss function that emphasises loss per pixel for secondary parameters (gripper angle and width) only on points of an object where a successful grasp can take place. This increases performance from a baseline average of 72.04% to 78.92% as well as reducing the number of training epochs required. These methods can be also performed in tandem resulting in a further performance increase to 79.12% while maintaining sufficient inference speed to afford real-time grasp processing.



### Hyperrealistic Image Inpainting with Hypergraphs
- **Arxiv ID**: http://arxiv.org/abs/2011.02904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02904v1)
- **Published**: 2020-11-05 15:28:57+00:00
- **Updated**: 2020-11-05 15:28:57+00:00
- **Authors**: Gourav Wadhwa, Abhinav Dhall, Subrahmanyam Murala, Usman Tariq
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer Vision
  (WACV), 2021
- **Journal**: None
- **Summary**: Image inpainting is a non-trivial task in computer vision due to multiple possibilities for filling the missing data, which may be dependent on the global information of the image. Most of the existing approaches use the attention mechanism to learn the global context of the image. This attention mechanism produces semantically plausible but blurry results because of incapability to capture the global context. In this paper, we introduce hypergraph convolution on spatial features to learn the complex relationship among the data. We introduce a trainable mechanism to connect nodes using hyperedges for hypergraph convolution. To the best of our knowledge, hypergraph convolution have never been used on spatial features for any image-to-image tasks in computer vision. Further, we introduce gated convolution in the discriminator to enforce local consistency in the predicted image. The experiments on Places2, CelebA-HQ, Paris Street View, and Facades datasets, show that our approach achieves state-of-the-art results.



### Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2011.02910v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02910v4)
- **Published**: 2020-11-05 15:35:46+00:00
- **Updated**: 2021-08-25 18:35:45+00:00
- **Authors**: Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X. Creighton, Russell H. Taylor, Mathias Unberath
- **Comment**: Our code is available at
  https://github.com/mli0603/stereo-transformer
- **Journal**: ICCV 2021 Oral
- **Summary**: Stereo depth estimation relies on optimal correspondence matching between pixels on epipolar lines in the left and right images to infer depth. In this work, we revisit the problem from a sequence-to-sequence correspondence perspective to replace cost volume construction with dense pixel matching using position information and attention. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and provides confidence estimates, and 3) imposes uniqueness constraints during the matching process. We report promising results on both synthetic and real-world datasets and demonstrate that STTR generalizes across different domains, even without fine-tuning.



### Imagining Grounded Conceptual Representations from Perceptual Information in Situated Guessing Games
- **Arxiv ID**: http://arxiv.org/abs/2011.02917v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02917v1)
- **Published**: 2020-11-05 15:42:29+00:00
- **Updated**: 2020-11-05 15:42:29+00:00
- **Authors**: Alessandro Suglia, Antonio Vergari, Ioannis Konstas, Yonatan Bisk, Emanuele Bastianelli, Andrea Vanzo, Oliver Lemon
- **Comment**: Accepted to the International Conference on Computational Linguistics
  (COLING) 2020
- **Journal**: None
- **Summary**: In visual guessing games, a Guesser has to identify a target object in a scene by asking questions to an Oracle. An effective strategy for the players is to learn conceptual representations of objects that are both discriminative and expressive enough to ask questions and guess correctly. However, as shown by Suglia et al. (2020), existing models fail to learn truly multi-modal representations, relying instead on gold category labels for objects in the scene both at training and inference time. This provides an unnatural performance advantage when categories at inference time match those at training time, and it causes models to fail in more realistic "zero-shot" scenarios where out-of-domain object categories are involved. To overcome this issue, we introduce a novel "imagination" module based on Regularized Auto-Encoders, that learns context-aware and category-aware latent embeddings without relying on category labels at inference time. Our imagination module outperforms state-of-the-art competitors by 8.26% gameplay accuracy in the CompGuessWhat?! zero-shot scenario (Suglia et al., 2020), and it improves the Oracle and Guesser accuracy by 2.08% and 12.86% in the GuessWhat?! benchmark, when no gold categories are available at inference time. The imagination module also boosts reasoning about object properties and attributes.



### Street to Cloud: Improving Flood Maps With Crowdsourcing and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.08010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08010v1)
- **Published**: 2020-11-05 16:36:58+00:00
- **Updated**: 2020-11-05 16:36:58+00:00
- **Authors**: Veda Sunkara, Matthew Purri, Bertrand Le Saux, Jennifer Adams
- **Comment**: 5 pages, 2 figures, Tackling Climate Change with Machine Learning
  workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: To address the mounting destruction caused by floods in climate-vulnerable regions, we propose Street to Cloud, a machine learning pipeline for incorporating crowdsourced ground truth data into the segmentation of satellite imagery of floods. We propose this approach as a solution to the labor-intensive task of generating high-quality, hand-labeled training data, and demonstrate successes and failures of different plausible crowdsourcing approaches in our model. Street to Cloud leverages community reporting and machine learning to generate novel, near-real time insights into the extent of floods to be used for emergency response.



### Conflicting Bundles: Adapting Architectures Towards the Improved Training of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.02956v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02956v1)
- **Published**: 2020-11-05 16:41:04+00:00
- **Updated**: 2020-11-05 16:41:04+00:00
- **Authors**: David Peer, Sebastian Stabinger, Antonio Rodriguez-Sanchez
- **Comment**: Accepted at WACV2021
- **Journal**: None
- **Summary**: Designing neural network architectures is a challenging task and knowing which specific layers of a model must be adapted to improve the performance is almost a mystery. In this paper, we introduce a novel theory and metric to identify layers that decrease the test accuracy of the trained models, this identification is done as early as at the beginning of training. In the worst-case, such a layer could lead to a network that can not be trained at all. More precisely, we identified those layers that worsen the performance because they produce conflicting training bundles as we show in our novel theoretical analysis, complemented by our extensive empirical studies. Based on these findings, a novel algorithm is introduced to remove performance decreasing layers automatically. Architectures found by this algorithm achieve a competitive accuracy when compared against the state-of-the-art architectures. While keeping such high accuracy, our approach drastically reduces memory consumption and inference time for different computer vision tasks.



### Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA
- **Arxiv ID**: http://arxiv.org/abs/2011.03006v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03006v2)
- **Published**: 2020-11-05 17:59:14+00:00
- **Updated**: 2021-10-08 19:18:27+00:00
- **Authors**: Adnan Siraj Rakin, Yukui Luo, Xiaolin Xu, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: The wide deployment of Deep Neural Networks (DNN) in high-performance cloud computing platforms brought to light multi-tenant cloud field-programmable gate arrays (FPGA) as a popular choice of accelerator to boost performance due to its hardware reprogramming flexibility. Such a multi-tenant FPGA setup for DNN acceleration potentially exposes DNN interference tasks under severe threat from malicious users. This work, to the best of our knowledge, is the first to explore DNN model vulnerabilities in multi-tenant FPGAs. We propose a novel adversarial attack framework: Deep-Dup, in which the adversarial tenant can inject adversarial faults to the DNN model in the victim tenant of FPGA. Specifically, she can aggressively overload the shared power distribution system of FPGA with malicious power-plundering circuits, achieving adversarial weight duplication (AWD) hardware attack that duplicates certain DNN weight packages during data transmission between off-chip memory and on-chip buffer, to hijack the DNN function of the victim tenant. Further, to identify the most vulnerable DNN weight packages for a given malicious objective, we propose a generic vulnerable weight package searching algorithm, called Progressive Differential Evolution Search (P-DES), which is, for the first time, adaptive to both deep learning white-box and black-box attack models. The proposed Deep-Dup is experimentally validated in a developed multi-tenant FPGA prototype, for two popular deep learning applications, i.e., Object Detection and Image Classification. Successful attacks are demonstrated in six popular DNN architectures (e.g., YOLOv2, ResNet-50, MobileNet, etc.)



### Intra-Domain Task-Adaptive Transfer Learning to Determine Acute Ischemic Stroke Onset Time
- **Arxiv ID**: http://arxiv.org/abs/2011.03350v1
- **DOI**: 10.1016/j.compmedimag.2021.101926
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03350v1)
- **Published**: 2020-11-05 18:28:54+00:00
- **Updated**: 2020-11-05 18:28:54+00:00
- **Authors**: Haoyue Zhang, Jennifer S Polson, Kambiz Nael, Noriko Salamon, Bryan Yoo, Suzie El-Saden, Fabien Scalzo, William Speier, Corey W Arnold
- **Comment**: None
- **Journal**: Computerized Medical Imaging and Graphics Volume 90, June 2021,
  101926
- **Summary**: Treatment of acute ischemic strokes (AIS) is largely contingent upon the time since stroke onset (TSS). However, TSS may not be readily available in up to 25% of patients with unwitnessed AIS. Current clinical guidelines for patients with unknown TSS recommend the use of MRI to determine eligibility for thrombolysis, but radiology assessments have high inter-reader variability. In this work, we present deep learning models that leverage MRI diffusion series to classify TSS based on clinically validated thresholds. We propose an intra-domain task-adaptive transfer learning method, which involves training a model on an easier clinical task (stroke detection) and then refining the model with different binary thresholds of TSS. We apply this approach to both 2D and 3D CNN architectures with our top model achieving an ROC-AUC value of 0.74, with a sensitivity of 0.70 and a specificity of 0.81 for classifying TSS < 4.5 hours. Our pretrained models achieve better classification metrics than the models trained from scratch, and these metrics exceed those of previously published models applied to our dataset. Furthermore, our pipeline accommodates a more inclusive patient cohort than previous work, as we did not exclude imaging studies based on clinical, demographic, or image processing criteria. When applied to this broad spectrum of patients, our deep learning model achieves an overall accuracy of 75.78% when classifying TSS < 4.5 hours, carrying potential therapeutic implications for patients with unknown TSS.



### CompressAI: a PyTorch library and evaluation platform for end-to-end compression research
- **Arxiv ID**: http://arxiv.org/abs/2011.03029v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03029v1)
- **Published**: 2020-11-05 18:40:50+00:00
- **Updated**: 2020-11-05 18:40:50+00:00
- **Authors**: Jean B√©gaint, Fabien Racap√©, Simon Feltman, Akshay Pushparaja
- **Comment**: 19 pages, 11 figures
- **Journal**: None
- **Summary**: This paper presents CompressAI, a platform that provides custom operations, layers, models and tools to research, develop and evaluate end-to-end image and video compression codecs. In particular, CompressAI includes pre-trained models and evaluation tools to compare learned methods with traditional codecs. Multiple models from the state-of-the-art on learned end-to-end compression have thus been reimplemented in PyTorch and trained from scratch. We also report objective comparison results using PSNR and MS-SSIM metrics vs. bit-rate, using the Kodak image dataset as test set. Although this framework currently implements models for still-picture compression, it is intended to be soon extended to the video compression domain.



### MorphEyes: Variable Baseline Stereo For Quadrotor Navigation
- **Arxiv ID**: http://arxiv.org/abs/2011.03077v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03077v1)
- **Published**: 2020-11-05 20:04:35+00:00
- **Updated**: 2020-11-05 20:04:35+00:00
- **Authors**: Nitin J. Sanket, Chahat Deep Singh, Varun Asthana, Cornelia Ferm√ºller, Yiannis Aloimonos
- **Comment**: 7 pages, 10 figures, 1 table. Under review in ICRA 2021
- **Journal**: None
- **Summary**: Morphable design and depth-based visual control are two upcoming trends leading to advancements in the field of quadrotor autonomy. Stereo-cameras have struck the perfect balance of weight and accuracy of depth estimation but suffer from the problem of depth range being limited and dictated by the baseline chosen at design time. In this paper, we present a framework for quadrotor navigation based on a stereo camera system whose baseline can be adapted on-the-fly. We present a method to calibrate the system at a small number of discrete baselines and interpolate the parameters for the entire baseline range. We present an extensive theoretical analysis of calibration and synchronization errors. We showcase three different applications of such a system for quadrotor navigation: (a) flying through a forest, (b) flying through an unknown shaped/location static/dynamic gap, and (c) accurate 3D pose detection of an independently moving object. We show that our variable baseline system is more accurate and robust in all three scenarios. To our knowledge, this is the first work that applies the concept of morphable design to achieve a variable baseline stereo vision system on a quadrotor.



### Towards Keypoint Guided Self-Supervised Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.03091v1
- **DOI**: 10.5220/0009190005830589
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03091v1)
- **Published**: 2020-11-05 20:45:03+00:00
- **Updated**: 2020-11-05 20:45:03+00:00
- **Authors**: Kristijan Bartol, David Bojanic, Tomislav Petkovic, Tomislav Pribanic, Yago Diez Donoso
- **Comment**: None
- **Journal**: 15th International Joint Conference on Computer Vision, Imaging
  and Computer Graphics Theory and Applications, 2019
- **Summary**: This paper proposes to use keypoints as a self-supervision clue for learning depth map estimation from a collection of input images. As ground truth depth from real images is difficult to obtain, there are many unsupervised and self-supervised approaches to depth estimation that have been proposed. Most of these unsupervised approaches use depth map and ego-motion estimations to reproject the pixels from the current image into the adjacent image from the image collection. Depth and ego-motion estimations are evaluated based on pixel intensity differences between the correspondent original and reprojected pixels. Instead of reprojecting the individual pixels, we propose to first select image keypoints in both images and then reproject and compare the correspondent keypoints of the two images. The keypoints should describe the distinctive image features well. By learning a deep model with and without the keypoint extraction technique, we show that using the keypoints improve the depth estimation learning. We also propose some future directions for keypoint-guided learning of structure-from-motion problems.



### End-to-end Deep Learning Methods for Automated Damage Detection in Extreme Events at Various Scales
- **Arxiv ID**: http://arxiv.org/abs/2011.03098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03098v1)
- **Published**: 2020-11-05 21:21:19+00:00
- **Updated**: 2020-11-05 21:21:19+00:00
- **Authors**: Yongsheng Bai, Halil Sezen, Alper Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: Robust Mask R-CNN (Mask Regional Convolu-tional Neural Network) methods are proposed and tested for automatic detection of cracks on structures or their components that may be damaged during extreme events, such as earth-quakes. We curated a new dataset with 2,021 labeled images for training and validation and aimed to find end-to-end deep neural networks for crack detection in the field. With data augmentation and parameters fine-tuning, Path Aggregation Network (PANet) with spatial attention mechanisms and High-resolution Network (HRNet) are introduced into Mask R-CNNs. The tests on three public datasets with low- or high-resolution images demonstrate that the proposed methods can achieve a big improvement over alternative networks, so the proposed method may be sufficient for crack detection for a variety of scales in real applications.



### Identifying and interpreting tuning dimensions in deep networks
- **Arxiv ID**: http://arxiv.org/abs/2011.03043v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2011.03043v2)
- **Published**: 2020-11-05 21:26:03+00:00
- **Updated**: 2020-12-08 00:01:04+00:00
- **Authors**: Nolan S. Dey, J. Eric Taylor, Bryan P. Tripp, Alexander Wong, Graham W. Taylor
- **Comment**: 15 pages, 12 figures, Camera-ready for Shared Visual Representations
  in Human & Machine Intelligence NeurIPS Workshop 2020
- **Journal**: None
- **Summary**: In neuroscience, a tuning dimension is a stimulus attribute that accounts for much of the activation variance of a group of neurons. These are commonly used to decipher the responses of such groups. While researchers have attempted to manually identify an analogue to these tuning dimensions in deep neural networks, we are unaware of an automatic way to discover them. This work contributes an unsupervised framework for identifying and interpreting "tuning dimensions" in deep networks. Our method correctly identifies the tuning dimensions of a synthetic Gabor filter bank and tuning dimensions of the first two layers of InceptionV1 trained on ImageNet.



### Smart Time-Multiplexing of Quads Solves the Multicamera Interference Problem
- **Arxiv ID**: http://arxiv.org/abs/2011.03102v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03102v1)
- **Published**: 2020-11-05 21:29:21+00:00
- **Updated**: 2020-11-05 21:29:21+00:00
- **Authors**: Tomislav Pribanic, Tomislav Petkovic, David Bojanic, Kristijan Bartol
- **Comment**: None
- **Journal**: None
- **Summary**: Time-of-flight (ToF) cameras are becoming increasingly popular for 3D imaging. Their optimal usage has been studied from the several aspects. One of the open research problems is the possibility of a multicamera interference problem when two or more ToF cameras are operating simultaneously. In this work we present an efficient method to synchronize multiple operating ToF cameras. Our method is based on the time-division multiplexing, but unlike traditional time multiplexing, it does not decrease the effective camera frame rate. Additionally, for unsynchronized cameras, we provide a robust method to extract from their corresponding video streams, frames which are not subject to multicamera interference problem. We demonstrate our approach through a series of experiments and with a different level of support available for triggering, ranging from a hardware triggering to purely random software triggering.



### Can Human Sex Be Learned Using Only 2D Body Keypoint Estimations?
- **Arxiv ID**: http://arxiv.org/abs/2011.03104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03104v2)
- **Published**: 2020-11-05 21:30:51+00:00
- **Updated**: 2022-04-20 07:05:43+00:00
- **Authors**: Kristijan Bartol, Tomislav Pribanic, David Bojanic, Tomislav Petkovic
- **Comment**: There was an error in the implementation of the base experiment (#1),
  i.e., the data preparation step. More specifically, the labels used both for
  training and evaluation were wrong, thus making the conclusions invalid
- **Journal**: None
- **Summary**: In this paper, we analyze human male and female sex recognition problem and present a fully automated classification system using only 2D keypoints. The keypoints represent human joints. A keypoint set consists of 15 joints and the keypoint estimations are obtained using an OpenPose 2D keypoint detector. We learn a deep learning model to distinguish males and females using the keypoints as input and binary labels as output. We use two public datasets in the experimental section - 3DPeople and PETA. On PETA dataset, we report a 77% accuracy. We provide model performance details on both PETA and 3DPeople. To measure the effect of noisy 2D keypoint detections on the performance, we run separate experiments on 3DPeople ground truth and noisy keypoint data. Finally, we extract a set of factors that affect the classification accuracy and propose future work. The advantage of the approach is that the input is small and the architecture is simple, which enables us to run many experiments and keep the real-time performance in inference. The source code, with the experiments and data preparation scripts, are available on GitHub (https://github.com/kristijanbartol/human-sex-classifier).



### IMU-Assisted Learning of Single-View Rolling Shutter Correction
- **Arxiv ID**: http://arxiv.org/abs/2011.03106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03106v2)
- **Published**: 2020-11-05 21:33:25+00:00
- **Updated**: 2021-09-14 14:56:02+00:00
- **Authors**: Jiawei Mo, Md Jahidul Islam, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: Rolling shutter distortion is highly undesirable for photography and computer vision algorithms (e.g., visual SLAM) because pixels can be potentially captured at different times and poses. In this paper, we propose a deep neural network to predict depth and row-wise pose from a single image for rolling shutter correction. Our contribution in this work is to incorporate inertial measurement unit (IMU) data into the pose refinement process, which, compared to the state-of-the-art, greatly enhances the pose prediction. The improved accuracy and robustness make it possible for numerous vision algorithms to use imagery captured by rolling shutter cameras and produce highly accurate results. We also extend a dataset to have real rolling shutter images, IMU data, depth maps, camera poses, and corresponding global shutter images for rolling shutter correction training. We demonstrate the efficacy of the proposed method by evaluating the performance of Direct Sparse Odometry (DSO) algorithm on rolling shutter imagery corrected using the proposed approach. Results show marked improvements of the DSO algorithm over using uncorrected imagery, validating the proposed approach.



### Uncertainty-Aware Vehicle Orientation Estimation for Joint Detection-Prediction Models
- **Arxiv ID**: http://arxiv.org/abs/2011.03114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03114v1)
- **Published**: 2020-11-05 21:59:44+00:00
- **Updated**: 2020-11-05 21:59:44+00:00
- **Authors**: Henggang Cui, Fang-Chieh Chou, Jake Charland, Carlos Vallespi-Gonzalez, Nemanja Djuric
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a critical component of a self-driving system, tasked with inferring the current states of the surrounding traffic actors. While there exist a number of studies on the problem of inferring the position and shape of vehicle actors, understanding actors' orientation remains a challenge for existing state-of-the-art detectors. Orientation is an important property for downstream modules of an autonomous system, particularly relevant for motion prediction of stationary or reversing actors where current approaches struggle. We focus on this task and present a method that extends the existing models that perform joint object detection and motion prediction, allowing us to more accurately infer vehicle orientations. In addition, the approach is able to quantify prediction uncertainty, outputting the probability that the inferred orientation is flipped, which allows for improved motion prediction and safer autonomous operations. Empirical results show the benefits of the approach, obtaining state-of-the-art performance on the open-sourced nuScenes data set.



### Ellipse Loss for Scene-Compliant Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.03139v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03139v2)
- **Published**: 2020-11-05 23:33:56+00:00
- **Updated**: 2021-03-25 21:32:55+00:00
- **Authors**: Henggang Cui, Hoda Shajari, Sai Yalamanchi, Nemanja Djuric
- **Comment**: Henggang Cui and Hoda Shajari contributed equally to this work.
  Accepted for publication at IEEE International Conference on Robotics and
  Automation (ICRA) 2021
- **Journal**: None
- **Summary**: Motion prediction is a critical part of self-driving technology, responsible for inferring future behavior of traffic actors in autonomous vehicle's surroundings. In order to ensure safe and efficient operations, prediction models need to output accurate trajectories that obey the map constraints. In this paper, we address this task and propose a novel ellipse loss that allows the models to better reason about scene compliance and predict more realistic trajectories. Ellipse loss penalizes off-road predictions directly in a supervised manner, by projecting the output trajectories into the top-down map frame using a differentiable trajectory rasterizer module. Moreover, it takes into account actor dimensions and orientation, providing more direct training signals to the model. We applied ellipse loss to a recently proposed state-of-the-art joint detection-prediction model to showcase its benefits. Evaluation on large-scale autonomous driving data strongly indicates that the method allows for more accurate and more realistic trajectory predictions.




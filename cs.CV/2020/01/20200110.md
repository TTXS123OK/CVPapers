# Arxiv Papers in cs.CV on 2020-01-10
### Image Inpainting by Multiscale Spline Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2001.03270v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03270v1)
- **Published**: 2020-01-10 01:15:14+00:00
- **Updated**: 2020-01-10 01:15:14+00:00
- **Authors**: Ghazale Ghorbanzade, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi
- **Comment**: six pages and five figures
- **Journal**: None
- **Summary**: Recovering the missing regions of an image is a task that is called image inpainting. Depending on the shape of missing areas, different methods are presented in the literature. One of the challenges of this problem is extracting features that lead to better results. Experimental results show that both global and local features are useful for this purpose. In this paper, we propose a multi-scale image inpainting method that utilizes both local and global features. The first step of this method is to determine how many scales we need to use, which depends on the width of the lines in the map of the missing region. Then we apply adaptive image inpainting to the damaged areas of the image, and the lost pixels are predicted. Each scale is inpainted and the result is resized to the original size. Then a voting process produces the final result. The proposed method is tested on damaged images with scratches and creases. The metric that we use to evaluate our approach is PSNR. On average, we achieved 1.2 dB improvement over some existing inpainting approaches.



### Efficient Memory Management for Deep Neural Net Inference
- **Arxiv ID**: http://arxiv.org/abs/2001.03288v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03288v3)
- **Published**: 2020-01-10 02:45:41+00:00
- **Updated**: 2020-02-16 02:32:54+00:00
- **Authors**: Yury Pisarchyk, Juhyun Lee
- **Comment**: 6 pages, 6 figures, MLSys 2020 Workshop on Resource-Constrained
  Machine Learning (ReCoML 2020)
- **Journal**: None
- **Summary**: While deep neural net inference was considered a task for servers only, latest advances in technology allow the task of inference to be moved to mobile and embedded devices, desired for various reasons ranging from latency to privacy. These devices are not only limited by their compute power and battery, but also by their inferior physical memory and cache, and thus, an efficient memory manager becomes a crucial component for deep neural net inference at the edge. We explore various strategies to smartly share memory buffers among intermediate tensors in deep neural nets. Employing these can result in up to 11% smaller memory footprint than the state of the art.



### Diagnosing Colorectal Polyps in the Wild with Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.03305v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.03305v1)
- **Published**: 2020-01-10 04:55:01+00:00
- **Updated**: 2020-01-10 04:55:01+00:00
- **Authors**: Rodney LaLonde, Pujan Kandel, Concetto Spampinato, Michael B. Wallace, Ulas Bagci
- **Comment**: Accepted for publication at ISBI 2020 (IEEE International Symposium
  on Biomedical Imaging). Code is publicly available at
  https://github.com/lalonderodney/D-Caps
- **Journal**: None
- **Summary**: Colorectal cancer, largely arising from precursor lesions called polyps, remains one of the leading causes of cancer-related death worldwide. Current clinical standards require the resection and histopathological analysis of polyps due to test accuracy and sensitivity of optical biopsy methods falling substantially below recommended levels. In this study, we design a novel capsule network architecture (D-Caps) to improve the viability of optical biopsy of colorectal polyps. Our proposed method introduces several technical novelties including a novel capsule architecture with a capsule-average pooling (CAP) method to improve efficiency in large-scale image classification. We demonstrate improved results over the previous state-of-the-art convolutional neural network (CNN) approach by as much as 43%. This work provides an important benchmark on the new Mayo Polyp dataset, a significantly more challenging and larger dataset than previous polyp studies, with results stratified across all available categories, imaging devices and modalities, and focus modes to promote future direction into AI-driven colorectal cancer screening systems. Code is publicly available at https://github.com/lalonderodney/D-Caps .



### Convolutional Neural Networks based Focal Loss for Class Imbalance Problem: A Case Study of Canine Red Blood Cells Morphology Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.03329v1
- **DOI**: 10.1007/s12652-020-01773-x
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03329v1)
- **Published**: 2020-01-10 07:31:57+00:00
- **Updated**: 2020-01-10 07:31:57+00:00
- **Authors**: Kitsuchart Pasupa, Supawit Vatathanavaro, Suchat Tungjitnob
- **Comment**: None
- **Journal**: Journal of Ambient Intelligence and Humanized Computing (2020),
  https://link.springer.com/article/10.1007/s12652-020-01773-x
- **Summary**: Morphologies of red blood cells are normally interpreted by a pathologist. It is time-consuming and laborious. Furthermore, a misclassified red blood cell morphology will lead to false disease diagnosis and improper treatment. Thus, a decent pathologist must truly be an expert in classifying red blood cell morphology. In the past decade, many approaches have been proposed for classifying human red blood cell morphology. However, those approaches have not addressed the class imbalance problem in classification. A class imbalance problem---a problem where the numbers of samples in classes are very different---is one of the problems that can lead to a biased model towards the majority class. Due to the rarity of every type of abnormal blood cell morphology, the data from the collection process are usually imbalanced. In this study, we aimed to solve this problem specifically for classification of dog red blood cell morphology by using a Convolutional Neural Network (CNN)---a well-known deep learning technique---in conjunction with a focal loss function, adept at handling class imbalance problem. The proposed technique was conducted on a well-designed framework: two different CNNs were used to verify the effectiveness of the focal loss function and the optimal hyper-parameters were determined by 5-fold cross-validation. The experimental results show that both CNNs models augmented with the focal loss function achieved higher $F_{1}$-scores, compared to the models augmented with a conventional cross-entropy loss function that does not address class imbalance problem. In other words, the focal loss function truly enabled the CNNs models to be less biased towards the majority class than the cross-entropy did in the classification task of imbalanced dog red blood cell data.



### Visual Question Answering on 360° Images
- **Arxiv ID**: http://arxiv.org/abs/2001.03339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03339v1)
- **Published**: 2020-01-10 08:18:21+00:00
- **Updated**: 2020-01-10 08:18:21+00:00
- **Authors**: Shih-Han Chou, Wei-Lun Chao, Wei-Sheng Lai, Min Sun, Ming-Hsuan Yang
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: In this work, we introduce VQA 360, a novel task of visual question answering on 360 images. Unlike a normal field-of-view image, a 360 image captures the entire visual content around the optical center of a camera, demanding more sophisticated spatial understanding and reasoning. To address this problem, we collect the first VQA 360 dataset, containing around 17,000 real-world image-question-answer triplets for a variety of question types. We then study two different VQA models on VQA 360, including one conventional model that takes an equirectangular image (with intrinsic distortion) as input and one dedicated model that first projects a 360 image onto cubemaps and subsequently aggregates the information from multiple spatial resolutions. We demonstrate that the cubemap-based model with multi-level fusion and attention diffusion performs favorably against other variants and the equirectangular-based models. Nevertheless, the gap between the humans' and machines' performance reveals the need for more advanced VQA 360 algorithms. We, therefore, expect our dataset and studies to serve as the benchmark for future development in this challenging task. Dataset, code, and pre-trained models are available online.



### Temporally Folded Convolutional Neural Networks for Sequence Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2001.03340v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.03340v1)
- **Published**: 2020-01-10 08:18:39+00:00
- **Updated**: 2020-01-10 08:18:39+00:00
- **Authors**: Matthias Weissenbacher
- **Comment**: 8 pages, 4 figures, submitted to IJCAI 2020 Proceedings
- **Journal**: None
- **Summary**: In this work we propose a novel approach to utilize convolutional neural networks for time series forecasting. The time direction of the sequential data with spatial dimensions $D=1,2$ is considered democratically as the input of a spatiotemporal $(D+1)$-dimensional convolutional neural network. Latter then reduces the data stream from $D +1 \to D$ dimensions followed by an incriminator cell which uses this information to forecast the subsequent time step. We empirically compare this strategy to convolutional LSTM's and LSTM's on their performance on the sequential MNIST and the JSB chorals dataset, respectively. We conclude that temporally folded convolutional neural networks (TFC's) may outperform the conventional recurrent strategies.



### RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2001.03343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03343v1)
- **Published**: 2020-01-10 08:29:20+00:00
- **Updated**: 2020-01-10 08:29:20+00:00
- **Authors**: Peixuan Li, Huaici Zhao, Pengfei Liu, Feidao Cao
- **Comment**: 11 pages, 4 figures and 7 tables
- **Journal**: None
- **Summary**: In this work, we propose an efficient and accurate monocular 3D detection framework in single shot. Most successful 3D detectors take the projection constraint from the 3D bounding box to the 2D box as an important component. Four edges of a 2D box provide only four constraints and the performance deteriorates dramatically with the small error of the 2D detector. Different from these approaches, our method predicts the nine perspective keypoints of a 3D bounding box in image space, and then utilize the geometric relationship of 3D and 2D perspectives to recover the dimension, location, and orientation in 3D space. In this method, the properties of the object can be predicted stably even when the estimation of keypoints is very noisy, which enables us to obtain fast detection speed with a small architecture. Training our method only uses the 3D properties of the object without the need for external networks or supervision data. Our method is the first real-time system for monocular image 3D detection while achieves state-of-the-art performance on the KITTI benchmark. Code will be released at https://github.com/Banconxuan/RTM3D.



### NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization
- **Arxiv ID**: http://arxiv.org/abs/2001.03360v4
- **DOI**: 10.1109/TPAMI.2020.3013269
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03360v4)
- **Published**: 2020-01-10 09:26:04+00:00
- **Updated**: 2020-08-04 00:56:49+00:00
- **Authors**: Qi Wang, Junyu Gao, Wei Lin, Xuelong Li
- **Comment**: Accepted by T-PAMI
- **Journal**: None
- **Summary**: In the last decade, crowd counting and localization attract much attention of researchers due to its wide-spread applications, including crowd monitoring, public safety, space design, etc. Many Convolutional Neural Networks (CNN) are designed for tackling this task. However, currently released datasets are so small-scale that they can not meet the needs of the supervised CNN-based algorithms. To remedy this problem, we construct a large-scale congested crowd counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0~20,033). Besides, a benchmark website is developed for impartially evaluating the different methods, which allows researchers to submit the results of the test set. Based on the proposed dataset, we further describe the data characteristics, evaluate the performance of some mainstream state-of-the-art (SOTA) methods, and analyze the new problems that arise on the new data. What's more, the benchmark is deployed at \url{https://www.crowdbenchmark.com/}, and the dataset/code/models/results are available at \url{https://gjy3035.github.io/NWPU-Crowd-Sample-Code/}.



### microbatchGAN: Stimulating Diversity with Multi-Adversarial Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2001.03376v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.03376v1)
- **Published**: 2020-01-10 10:31:27+00:00
- **Updated**: 2020-01-10 10:31:27+00:00
- **Authors**: Gonçalo Mordido, Haojin Yang, Christoph Meinel
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: We propose to tackle the mode collapse problem in generative adversarial networks (GANs) by using multiple discriminators and assigning a different portion of each minibatch, called microbatch, to each discriminator. We gradually change each discriminator's task from distinguishing between real and fake samples to discriminating samples coming from inside or outside its assigned microbatch by using a diversity parameter $\alpha$. The generator is then forced to promote variety in each minibatch to make the microbatch discrimination harder to achieve by each discriminator. Thus, all models in our framework benefit from having variety in the generated set to reduce their respective losses. We show evidence that our solution promotes sample diversity since early training stages on multiple datasets.



### SeismiQB -- a novel framework for deep learning with seismic data
- **Arxiv ID**: http://arxiv.org/abs/2001.06416v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.06416v1)
- **Published**: 2020-01-10 10:45:56+00:00
- **Updated**: 2020-01-10 10:45:56+00:00
- **Authors**: Alexander Koryagin, Roman Khudorozhkov, Sergey Tsimfer, Darima Mylzenova
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Deep Neural Networks were successfully adopted in numerous domains to solve various image-related tasks, ranging from simple classification to fine borders annotation. Naturally, many researches proposed to use it to solve geological problems. Unfortunately, many of the seismic processing tools were developed years before the era of machine learning, including the most popular SEG-Y data format for storing seismic cubes. Its slow loading speed heavily hampers experimentation speed, which is essential for getting acceptable results. Worse yet, there is no widely-used format for storing surfaces inside the volume (for example, seismic horizons). To address these problems, we've developed an open-sourced Python framework with emphasis on working with neural networks, that provides convenient tools for (i) fast loading seismic cubes in multiple data formats and converting between them, (ii) generating crops of desired shape and augmenting them with various transformations, and (iii) pairing cube data with labeled horizons or other types of geobodies.



### Seismic horizon detection with neural networks
- **Arxiv ID**: http://arxiv.org/abs/2001.03390v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03390v1)
- **Published**: 2020-01-10 11:30:50+00:00
- **Updated**: 2020-01-10 11:30:50+00:00
- **Authors**: Alexander Koryagin, Darima Mylzenova, Roman Khudorozhkov, Sergey Tsimfer
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last few years, Convolutional Neural Networks (CNNs) were successfully adopted in numerous domains to solve various image-related tasks, ranging from simple classification to fine borders annotation. Tracking seismic horizons is no different, and there are a lot of papers proposing the usage of such models to avoid time-consuming hand-picking. Unfortunately, most of them are (i) either trained on synthetic data, which can't fully represent the complexity of subterranean structures, (ii) trained and tested on the same cube, or (iii) lack reproducibility and precise descriptions of the model-building process. With all that in mind, the main contribution of this paper is an open-sourced research of applying binary segmentation approach to the task of horizon detection on multiple real seismic cubes with a focus on inter-cube generalization of the predictive model.



### DSGN: Deep Stereo Geometry Network for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.03398v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03398v3)
- **Published**: 2020-01-10 11:44:37+00:00
- **Updated**: 2020-04-08 03:30:28+00:00
- **Authors**: Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia
- **Comment**: Accepted by CVPR 2020 (Camera Ready)
- **Journal**: None
- **Summary**: Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors because there is a large performance gap between image-based and LiDAR-based methods. It is caused by the way to form representation for the prediction in 3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN), significantly reduces this gap by detecting 3D objects on a differentiable volumetric representation -- 3D geometric volume, which effectively encodes 3D geometric structure for 3D regular space. With this representation, we learn depth information and semantic cues simultaneously. For the first time, we provide a simple and effective one-stage stereo-based 3D detection pipeline that jointly estimates the depth and detects 3D objects in an end-to-end learning manner. Our approach outperforms previous stereo-based 3D detectors (about 10 higher in terms of AP) and even achieves comparable performance with several LiDAR-based methods on the KITTI 3D object detection leaderboard. Our code is publicly available at https://github.com/chenyilun95/DSGN.



### Improving Image Autoencoder Embeddings with Perceptual Loss
- **Arxiv ID**: http://arxiv.org/abs/2001.03444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03444v2)
- **Published**: 2020-01-10 13:48:09+00:00
- **Updated**: 2020-04-03 09:39:35+00:00
- **Authors**: Gustav Grund Pihlgren, Fredrik Sandin, Marcus Liwicki
- **Comment**: Accepted at IJCNN/WCCI 2020
- **Journal**: None
- **Summary**: Autoencoders are commonly trained using element-wise loss. However, element-wise loss disregards high-level structures in the image which can lead to embeddings that disregard them as well. A recent improvement to autoencoders that helps alleviate this problem is the use of perceptual loss. This work investigates perceptual loss from the perspective of encoder embeddings themselves. Autoencoders are trained to embed images from three different computer vision datasets using perceptual loss based on a pretrained model as well as pixel-wise loss. A host of different predictors are trained to perform object positioning and classification on the datasets given the embedded images as input. The two kinds of losses are evaluated by comparing how the predictors performed with embeddings from the differently trained autoencoders. The results show that, in the image domain, the embeddings generated by autoencoders trained with perceptual loss enable more accurate predictions than those trained with element-wise loss. Furthermore, the results show that, on the task of object positioning of a small-scale feature, perceptual loss can improve the results by a factor 10. The experimental setup is available online: https://github.com/guspih/Perceptual-Autoencoders



### A Differentiable Recurrent Surface for Asynchronous Event-Based Data
- **Arxiv ID**: http://arxiv.org/abs/2001.03455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03455v2)
- **Published**: 2020-01-10 14:09:40+00:00
- **Updated**: 2020-07-31 08:56:34+00:00
- **Authors**: Marco Cannici, Marco Ciccone, Andrea Romanoni, Matteo Matteucci
- **Comment**: 23 pages, 6 figures
- **Journal**: None
- **Summary**: Dynamic Vision Sensors (DVSs) asynchronously stream events in correspondence of pixels subject to brightness changes. Differently from classic vision devices, they produce a sparse representation of the scene. Therefore, to apply standard computer vision algorithms, events need to be integrated into a frame or event-surface. This is usually attained through hand-crafted grids that reconstruct the frame using ad-hoc heuristics. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn end-to-end task-dependent event-surfaces. Compared to existing reconstruction approaches, our learned event-surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.



### Compressive sensing based privacy for fall detection
- **Arxiv ID**: http://arxiv.org/abs/2001.03463v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03463v1)
- **Published**: 2020-01-10 14:26:03+00:00
- **Updated**: 2020-01-10 14:26:03+00:00
- **Authors**: Ronak Gupta, Prashant Anand, Santanu Chaudhury, Brejesh Lall, Sanjay Singh
- **Comment**: accepted in NCVPRIPG 2019
- **Journal**: None
- **Summary**: Fall detection holds immense importance in the field of healthcare, where timely detection allows for instant medical assistance. In this context, we propose a 3D ConvNet architecture which consists of 3D Inception modules for fall detection. The proposed architecture is a custom version of Inflated 3D (I3D) architecture, that takes compressed measurements of video sequence as spatio-temporal input, obtained from compressive sensing framework, rather than video sequence as input, as in the case of I3D convolutional neural network. This is adopted since privacy raises a huge concern for patients being monitored through these RGB cameras. The proposed framework for fall detection is flexible enough with respect to a wide variety of measurement matrices. Ten action classes randomly selected from Kinetics-400 with no fall examples, are employed to train our 3D ConvNet post compressive sensing with different types of sensing matrices on the original video clips. Our results show that 3D ConvNet performance remains unchanged with different sensing matrices. Also, the performance obtained with Kinetics pre-trained 3D ConvNet on compressively sensed fall videos from benchmark datasets is better than the state-of-the-art techniques.



### SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2001.03483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03483v1)
- **Published**: 2020-01-10 14:44:23+00:00
- **Updated**: 2020-01-10 14:44:23+00:00
- **Authors**: Steve Dias Da Cruz, Oliver Wasenmüller, Hans-Peter Beise, Thomas Stifter, Didier Stricker
- **Comment**: This paper is accepted at IEEE Winter Conference on Applications of
  Computer Vision (WACV), 2020. Supplementary material is available under
  https://sviro.kl.dfki.de/downloads/papers/wacv_supplementary.pdf
- **Journal**: None
- **Summary**: We release SVIRO, a synthetic dataset for sceneries in the passenger compartment of ten different vehicles, in order to analyze machine learning-based approaches for their generalization capacities and reliability when trained on a limited number of variations (e.g. identical backgrounds and textures, few instances per class). This is in contrast to the intrinsically high variability of common benchmark datasets, which focus on improving the state-of-the-art of general tasks. Our dataset contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification. The advantage of our use-case is twofold: The proximity to a realistic application to benchmark new approaches under novel circumstances while reducing the complexity to a more tractable environment, such that applications and theoretical questions can be tested on a more challenging dataset as toy problems. The data and evaluation server are available under https://sviro.kl.dfki.de.



### Deformable Groupwise Image Registration using Low-Rank and Sparse Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2001.03509v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2001.03509v1)
- **Published**: 2020-01-10 15:25:36+00:00
- **Updated**: 2020-01-10 15:25:36+00:00
- **Authors**: Roland Haase, Stefan Heldmann, Jan Lellmann
- **Comment**: 22 pages, 14 figures
- **Journal**: None
- **Summary**: Low-rank and sparse decompositions and robust PCA (RPCA) are highly successful techniques in image processing and have recently found use in groupwise image registration. In this paper, we investigate the drawbacks of the most common RPCA-dissimi\-larity metric in image registration and derive an improved version. In particular, this new metric models low-rank requirements through explicit constraints instead of penalties and thus avoids the pitfalls of the established metric. Equipped with total variation regularization, we present a theoretically justified multilevel scheme based on first-order primal-dual optimization to solve the resulting non-parametric registration problem. As confirmed by numerical experiments, our metric especially lends itself to data involving recurring changes in object appearance and potential sparse perturbations. We numerically compare its peformance to a number of related approaches.



### Pruning Convolutional Neural Networks with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2001.03554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2001.03554v1)
- **Published**: 2020-01-10 16:44:41+00:00
- **Updated**: 2020-01-10 16:44:41+00:00
- **Authors**: Mathilde Caron, Ari Morcos, Piotr Bojanowski, Julien Mairal, Armand Joulin
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks trained without supervision come close to matching performance with supervised pre-training, but sometimes at the cost of an even higher number of parameters. Extracting subnetworks from these large unsupervised convnets with preserved performance is of particular interest to make them less computationally intensive. Typical pruning methods operate during training on a task while trying to maintain the performance of the pruned network on the same task. However, in self-supervised feature learning, the training objective is agnostic on the representation transferability to downstream tasks. Thus, preserving performance for this objective does not ensure that the pruned subnetwork remains effective for solving downstream tasks. In this work, we investigate the use of standard pruning methods, developed primarily for supervised learning, for networks trained without labels (i.e. on self-supervised tasks). We show that pruned masks obtained with or without labels reach comparable performance when re-trained on labels, suggesting that pruning operates similarly for self-supervised and supervised learning. Interestingly, we also find that pruning preserves the transfer performance of self-supervised subnetwork representations.



### Video Coding for Machines: A Paradigm of Collaborative Compression and Intelligent Analytics
- **Arxiv ID**: http://arxiv.org/abs/2001.03569v2
- **DOI**: 10.1109/TIP.2020.3016485
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03569v2)
- **Published**: 2020-01-10 17:24:13+00:00
- **Updated**: 2020-01-13 16:03:58+00:00
- **Authors**: Ling-Yu Duan, Jiaying Liu, Wenhan Yang, Tiejun Huang, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Video coding, which targets to compress and reconstruct the whole frame, and feature compression, which only preserves and transmits the most critical information, stand at two ends of the scale. That is, one is with compactness and efficiency to serve for machine vision, and the other is with full fidelity, bowing to human perception. The recent endeavors in imminent trends of video compression, e.g. deep learning based coding tools and end-to-end image/video coding, and MPEG-7 compact feature descriptor standards, i.e. Compact Descriptors for Visual Search and Compact Descriptors for Video Analysis, promote the sustainable and fast development in their own directions, respectively. In this paper, thanks to booming AI technology, e.g. prediction and generation models, we carry out exploration in the new area, Video Coding for Machines (VCM), arising from the emerging MPEG standardization efforts1. Towards collaborative compression and intelligent analytics, VCM attempts to bridge the gap between feature coding for machine vision and video coding for human vision. Aligning with the rising Analyze then Compress instance Digital Retina, the definition, formulation, and paradigm of VCM are given first. Meanwhile, we systematically review state-of-the-art techniques in video compression and feature compression from the unique perspective of MPEG standardization, which provides the academic and industrial evidence to realize the collaborative compression of video and feature streams in a broad range of AI applications. Finally, we come up with potential VCM solutions, and the preliminary results have demonstrated the performance and efficiency gains. Further direction is discussed as well.



### Subjective Annotation for a Frame Interpolation Benchmark using Artefact Amplification
- **Arxiv ID**: http://arxiv.org/abs/2001.06409v2
- **DOI**: 10.1007/s41233-020-00037-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06409v2)
- **Published**: 2020-01-10 18:20:37+00:00
- **Updated**: 2020-04-28 19:06:04+00:00
- **Authors**: Hui Men, Vlad Hosu, Hanhe Lin, Andrés Bruhn, Dietmar Saupe
- **Comment**: arXiv admin note: text overlap with arXiv:1901.05362
- **Journal**: None
- **Summary**: Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing the predicted flow fields with the ground truth or indirectly by using the predicted flow fields for frame interpolation and then comparing the interpolated frames with the actual frames. In the latter case, objective quality measures such as the mean squared error are typically employed. However, it is well known that for image quality assessment, the actual quality experienced by the user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality assessment crowdscouring study for the interpolated frames provided by one of the optical flow benchmarks, the Middlebury benchmark. We collected forced-choice paired comparisons between interpolated images and corresponding ground truth. To increase the sensitivity of observers when judging minute difference in paired comparisons we introduced a new method to the field of full-reference quality assessment, called artefact amplification. From the crowdsourcing data, we reconstructed absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This re-ranking not only shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks, the results also provide the ground truth for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA. By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed slightly better than the currently best FR-IQA approach from the literature.



### In Defense of Grid Features for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2001.03615v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03615v2)
- **Published**: 2020-01-10 18:59:13+00:00
- **Updated**: 2020-04-02 19:36:27+00:00
- **Authors**: Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, Xinlei Chen
- **Comment**: None
- **Journal**: CVPR, 2020
- **Summary**: Popularized as 'bottom-up' attention, bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering (VQA). However, it is not clear whether the advantages of regions (e.g. better localization) are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well - running more than an order of magnitude faster with the same accuracy (e.g. if pre-trained in a similar fashion). Through extensive experiments, we verify that this observation holds true across different VQA models (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71), datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available.



### Can Giraffes Become Birds? An Evaluation of Image-to-image Translation for Data Generation
- **Arxiv ID**: http://arxiv.org/abs/2001.03637v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03637v2)
- **Published**: 2020-01-10 19:29:11+00:00
- **Updated**: 2020-05-31 03:25:39+00:00
- **Authors**: Daniel V. Ruiz, Gabriel Salomon, Eduardo Todt
- **Comment**: Accepted for presentation at the Computer on the Beach (COTB'20) 2020
- **Journal**: None
- **Summary**: There is an increasing interest in image-to-image translation with applications ranging from generating maps from satellite images to creating entire clothes' images from only contours. In the present work, we investigate image-to-image translation using Generative Adversarial Networks (GANs) for generating new data, taking as a case study the morphing of giraffes images into bird images. Morphing a giraffe into a bird is a challenging task, as they have different scales, textures, and morphology. An unsupervised cross-domain translator entitled InstaGAN was trained on giraffes and birds, along with their respective masks, to learn translation between both domains. A dataset of synthetic bird images was generated using translation from originally giraffe images while preserving the original spatial arrangement and background. It is important to stress that the generated birds do not exist, being only the result of a latent representation learned by InstaGAN. Two subsets of common literature datasets were used for training the GAN and generating the translated images: COCO and Caltech-UCSD Birds 200-2011. To evaluate the realness and quality of the generated images and masks, qualitative and quantitative analyses were made. For the quantitative analysis, a pre-trained Mask R-CNN was used for the detection and segmentation of birds on Pascal VOC, Caltech-UCSD Birds 200-2011, and our new dataset entitled FakeSet. The generated dataset achieved detection and segmentation results close to the real datasets, suggesting that the generated images are realistic enough to be detected and segmented by a state-of-the-art deep neural network.



### Network of Steel: Neural Font Style Transfer from Heavy Metal to Corporate Logos
- **Arxiv ID**: http://arxiv.org/abs/2001.03659v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.03659v1)
- **Published**: 2020-01-10 20:41:15+00:00
- **Updated**: 2020-01-10 20:41:15+00:00
- **Authors**: Aram Ter-Sarkisov
- **Comment**: Accepted for oral presentation at ICPRAM 2020
- **Journal**: None
- **Summary**: We introduce a method for transferring style from the logos of heavy metal bands onto corporate logos using a VGG16 network. We establish the contribution of different layers and loss coefficients to the learning of style, minimization of artefacts and maintenance of readability of corporate logos. We find layers and loss coefficients that produce a good tradeoff between heavy metal style and corporate logo readability. This is the first step both towards sparse font style transfer and corporate logo decoration using generative networks. Heavy metal and corporate logos are very different artistically, in the way they emphasize emotions and readability, therefore training a model to fuse the two is an interesting problem.



### Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for Language Grounding Tasks in Street View
- **Arxiv ID**: http://arxiv.org/abs/2001.03671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.03671v1)
- **Published**: 2020-01-10 21:35:28+00:00
- **Updated**: 2020-01-10 21:35:28+00:00
- **Authors**: Harsh Mehta, Yoav Artzi, Jason Baldridge, Eugene Ie, Piotr Mirowski
- **Comment**: None
- **Journal**: None
- **Summary**: The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both of the Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in Chen et al. (2019) and show that the panoramas we have added to StreetLearn fully support both Touchdown tasks and can be used effectively for further research and comparison.



### Learning Topometric Semantic Maps from Occupancy Grids
- **Arxiv ID**: http://arxiv.org/abs/2001.03676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.03676v1)
- **Published**: 2020-01-10 22:06:10+00:00
- **Updated**: 2020-01-10 22:06:10+00:00
- **Authors**: Markus Hiller, Chen Qiu, Florian Particke, Christian Hofmann, Jörn Thielecke
- **Comment**: Presented at the 2019 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Today's mobile robots are expected to operate in complex environments they share with humans. To allow intuitive human-robot collaboration, robots require a human-like understanding of their surroundings in terms of semantically classified instances. In this paper, we propose a new approach for deriving such instance-based semantic maps purely from occupancy grids. We employ a combination of deep learning techniques to detect, segment and extract door hypotheses from a random-sized map. The extraction is followed by a post-processing chain to further increase the accuracy of our approach, as well as place categorization for the three classes room, door and corridor. All detected and classified entities are described as instances specified in a common coordinate system, while a topological map is derived to capture their spatial links. To train our two neural networks used for detection and map segmentation, we contribute a simulator that automatically creates and annotates the required training data. We further provide insight into which features are learned to detect doorways, and how the simulated training data can be augmented to train networks for the direct application on real-world grid maps. We evaluate our approach on several publicly available real-world data sets. Even though the used networks are solely trained on simulated data, our approach demonstrates high robustness and effectiveness in various real-world indoor environments.



### Understanding Graph Isomorphism Network for rs-fMRI Functional Connectivity Analysis
- **Arxiv ID**: http://arxiv.org/abs/2001.03690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.03690v2)
- **Published**: 2020-01-10 23:40:09+00:00
- **Updated**: 2020-05-25 02:53:52+00:00
- **Authors**: Byung-Hoon Kim, Jong Chul Ye
- **Comment**: This paper is accepted for Frontiers in Neuroscience
- **Journal**: None
- **Summary**: Graph neural networks (GNN) rely on graph operations that include neural network training for various graph related tasks. Recently, several attempts have been made to apply the GNNs to functional magnetic resonance image (fMRI) data. Despite recent progresses, a common limitation is its difficulty to explain the classification results in a neuroscientifically explainable way. Here, we develop a framework for analyzing the fMRI data using the Graph Isomorphism Network (GIN), which was recently proposed as a powerful GNN for graph classification. One of the important contributions of this paper is the observation that the GIN is a dual representation of convolutional neural network (CNN) in the graph space where the shift operation is defined using the adjacency matrix. This understanding enables us to exploit CNN-based saliency map techniques for the GNN, which we tailor to the proposed GIN with one-hot encoding, to visualize the important regions of the brain. We validate our proposed framework using large-scale resting-state fMRI (rs-fMRI) data for classifying the sex of the subject based on the graph structure of the brain. The experiment was consistent with our expectation such that the obtained saliency map show high correspondence with previous neuroimaging evidences related to sex differences.




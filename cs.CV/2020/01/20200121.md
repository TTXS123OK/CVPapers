# Arxiv Papers in cs.CV on 2020-01-21
### Neural Style Difference Transfer and Its Application to Font Generation
- **Arxiv ID**: http://arxiv.org/abs/2001.07321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07321v1)
- **Published**: 2020-01-21 03:32:44+00:00
- **Updated**: 2020-01-21 03:32:44+00:00
- **Authors**: Gantugs Atarsaikhan, Brian Kenji Iwana, Seiichi Uchida
- **Comment**: Submitted to DAS2020
- **Journal**: None
- **Summary**: Designing fonts requires a great deal of time and effort. It requires professional skills, such as sketching, vectorizing, and image editing. Additionally, each letter has to be designed individually. In this paper, we will introduce a method to create fonts automatically. In our proposed method, the difference of font styles between two different fonts is found and transferred to another font using neural style transfer. Neural style transfer is a method of stylizing the contents of an image with the styles of another image. We proposed a novel neural style difference and content difference loss for the neural style transfer. With these losses, new fonts can be generated by adding or removing font styles from a font. We provided experimental results with various combinations of input fonts and discussed limitations and future development for the proposed method.



### Breast lesion segmentation in ultrasound images with limited annotated data
- **Arxiv ID**: http://arxiv.org/abs/2001.07322v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.07322v1)
- **Published**: 2020-01-21 03:34:42+00:00
- **Updated**: 2020-01-21 03:34:42+00:00
- **Authors**: Bahareh Behboodi, Mina Amiri, Rupert Brooks, Hassan Rivaz
- **Comment**: Accepted to ISBI 2020
- **Journal**: None
- **Summary**: Ultrasound (US) is one of the most commonly used imaging modalities in both diagnosis and surgical interventions due to its low-cost, safety, and non-invasive characteristic. US image segmentation is currently a unique challenge because of the presence of speckle noise. As manual segmentation requires considerable efforts and time, the development of automatic segmentation algorithms has attracted researchers attention. Although recent methodologies based on convolutional neural networks have shown promising performances, their success relies on the availability of a large number of training data, which is prohibitively difficult for many applications. Therefore, in this study we propose the use of simulated US images and natural images as auxiliary datasets in order to pre-train our segmentation network, and then to fine-tune with limited in vivo data. We show that with as little as 19 in vivo images, fine-tuning the pre-trained network improves the dice score by 21% compared to training from scratch. We also demonstrate that if the same number of natural and simulation US images is available, pre-training on simulation data is preferable.



### Face Verification via learning the kernel matrix
- **Arxiv ID**: http://arxiv.org/abs/2001.07323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07323v1)
- **Published**: 2020-01-21 03:39:09+00:00
- **Updated**: 2020-01-21 03:39:09+00:00
- **Authors**: Ning Yuan, Xiao-Jun Wu, He-Feng Yin
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The kernel function is introduced to solve the nonlinear pattern recognition problem. The advantage of a kernel method often depends critically on a proper choice of the kernel function. A promising approach is to learn the kernel from data automatically. Over the past few years, some methods which have been proposed to learn the kernel have some limitations: learning the parameters of some prespecified kernel function and so on. In this paper, the nonlinear face verification via learning the kernel matrix is proposed. A new criterion is used in the new algorithm to avoid inverting the possibly singular within-class which is a computational problem. The experimental results obtained on the facial database XM2VTS using the Lausanne protocol show that the verification performance of the new method is superior to that of the primary method Client Specific Kernel Discriminant Analysis (CSKDA). The method CSKDA needs to choose a proper kernel function through many experiments, while the new method could learn the kernel from data automatically which could save a lot of time and have the robust performance.



### Transfer Learning using Neural Ordinary Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2001.07342v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07342v1)
- **Published**: 2020-01-21 04:59:08+00:00
- **Updated**: 2020-01-21 04:59:08+00:00
- **Authors**: Rajath S, Sumukh Aithal K, Natarajan Subramanyam
- **Comment**: None
- **Journal**: None
- **Summary**: A concept of using Neural Ordinary Differential Equations(NODE) for Transfer Learning has been introduced. In this paper we use the EfficientNets to explore transfer learning on CIFAR-10 dataset. We use NODE for fine-tuning our model. Using NODE for fine tuning provides more stability during training and validation.These continuous depth blocks can also have a trade off between numerical precision and speed .Using Neural ODEs for transfer learning has resulted in much stable convergence of the loss function.



### VMRFANet:View-Specific Multi-Receptive Field Attention Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2001.07354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07354v1)
- **Published**: 2020-01-21 06:31:18+00:00
- **Updated**: 2020-01-21 06:31:18+00:00
- **Authors**: Honglong Cai, Yuedong Fang, Zhiguan Wang, Tingchun Yeh, Jinxing Cheng
- **Comment**: Accepted by ICAART2020
- **Journal**: None
- **Summary**: Person re-identification (re-ID) aims to retrieve the same person across different cameras. In practice, it still remains a challenging task due to background clutter, variations on body poses and view conditions, inaccurate bounding box detection, etc. To tackle these issues, in this paper, we propose a novel multi-receptive field attention (MRFA) module that utilizes filters of various sizes to help network focusing on informative pixels. Besides, we present a view-specific mechanism that guides attention module to handle the variation of view conditions. Moreover, we introduce a Gaussian horizontal random cropping/padding method which further improves the robustness of our proposed network. Comprehensive experiments demonstrate the effectiveness of each component. Our method achieves 95.5% / 88.1% in rank-1 / mAP on Market-1501, 88.9% / 80.0% on DukeMTMC-reID, 81.1% / 78.8% on CUHK03 labeled dataset and 78.9% / 75.3% on CUHK03 detected dataset, outperforming current state-of-the-art methods.



### From Planes to Corners: Multi-Purpose Primitive Detection in Unorganized 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2001.07360v2
- **DOI**: 10.1109/LRA.2020.2969936
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.07360v2)
- **Published**: 2020-01-21 06:51:47+00:00
- **Updated**: 2020-04-24 15:32:37+00:00
- **Authors**: Christiane Sommer, Yumin Sun, Leonidas Guibas, Daniel Cremers, Tolga Birdal
- **Comment**: Accepted to IEEE Robotics and Automation Letters 2020 | Video:
  https://youtu.be/nHWJrA6RcB0 | Code:
  https://github.com/c-sommer/orthogonal-planes
- **Journal**: IEEE Robotics and Automation Letters 5(2) 2020, 1764-1771
- **Summary**: We propose a new method for segmentation-free joint estimation of orthogonal planes, their intersection lines, relationship graph and corners lying at the intersection of three orthogonal planes. Such unified scene exploration under orthogonality allows for multitudes of applications such as semantic plane detection or local and global scan alignment, which in turn can aid robot localization or grasping tasks. Our two-stage pipeline involves a rough yet joint estimation of orthogonal planes followed by a subsequent joint refinement of plane parameters respecting their orthogonality relations. We form a graph of these primitives, paving the way to the extraction of further reliable features: lines and corners. Our experiments demonstrate the validity of our approach in numerous scenarios from wall detection to 6D tracking, both on synthetic and real data.



### An End-to-end Deep Learning Approach for Landmark Detection and Matching in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2001.07434v1
- **DOI**: 10.1117/12.2549302
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07434v1)
- **Published**: 2020-01-21 10:35:48+00:00
- **Updated**: 2020-01-21 10:35:48+00:00
- **Authors**: Monika Grewal, Timo M. Deist, Jan Wiersma, Peter A. N. Bosman, Tanja Alderliesten
- **Comment**: SPIE Medical Imaging Conference - 2020
- **Journal**: None
- **Summary**: Anatomical landmark correspondences in medical images can provide additional guidance information for the alignment of two images, which, in turn, is crucial for many medical applications. However, manual landmark annotation is labor-intensive. Therefore, we propose an end-to-end deep learning approach to automatically detect landmark correspondences in pairs of two-dimensional (2D) images. Our approach consists of a Siamese neural network, which is trained to identify salient locations in images as landmarks and predict matching probabilities for landmark pairs from two different images. We trained our approach on 2D transverse slices from 168 lower abdominal Computed Tomography (CT) scans. We tested the approach on 22,206 pairs of 2D slices with varying levels of intensity, affine, and elastic transformations. The proposed approach finds an average of 639, 466, and 370 landmark matches per image pair for intensity, affine, and elastic transformations, respectively, with spatial matching errors of at most 1 mm. Further, more than 99% of the landmark pairs are within a spatial matching error of 2 mm, 4 mm, and 8 mm for image pairs with intensity, affine, and elastic transformations, respectively. To investigate the utility of our developed approach in a clinical setting, we also tested our approach on pairs of transverse slices selected from follow-up CT scans of three patients. Visual inspection of the results revealed landmark matches in both bony anatomical regions as well as in soft tissues lacking prominent intensity gradients.



### Evaluating Weakly Supervised Object Localization Methods Right
- **Arxiv ID**: http://arxiv.org/abs/2001.07437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07437v2)
- **Published**: 2020-01-21 10:50:06+00:00
- **Updated**: 2020-04-01 05:35:21+00:00
- **Authors**: Junsuk Choe, Seong Joon Oh, Seungho Lee, Sanghyuk Chun, Zeynep Akata, Hyunjung Shim
- **Comment**: CVPR 2020 camera-ready. First two authors contributed equally. Code:
  https://github.com/clovaai/wsolevaluation
- **Journal**: None
- **Summary**: Weakly-supervised object localization (WSOL) has gained popularity over the last years for its promise to train localization models with only image-level labels. Since the seminal WSOL work of class activation mapping (CAM), the field has focused on how to expand the attention regions to cover objects more broadly and localize them better. However, these strategies rely on full localization supervision to validate hyperparameters and for model selection, which is in principle prohibited under the WSOL setup. In this paper, we argue that WSOL task is ill-posed with only image-level labels, and propose a new evaluation protocol where full supervision is limited to only a small held-out set not overlapping with the test set. We observe that, under our protocol, the five most recent WSOL methods have not made a major improvement over the CAM baseline. Moreover, we report that existing WSOL methods have not reached the few-shot learning baseline, where the full-supervision at validation time is used for model training instead. Based on our findings, we discuss some future directions for WSOL.



### Learning Diverse Features with Part-Level Resolution for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2001.07442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07442v1)
- **Published**: 2020-01-21 11:01:56+00:00
- **Updated**: 2020-01-21 11:01:56+00:00
- **Authors**: Ben Xie, Xiaofu Wu, Suofei Zhang, Shiliang Zhao, Ming Li
- **Comment**: 8 pages, 5 figures, submitted to IEEE TCSVT
- **Journal**: None
- **Summary**: Learning diverse features is key to the success of person re-identification. Various part-based methods have been extensively proposed for learning local representations, which, however, are still inferior to the best-performing methods for person re-identification. This paper proposes to construct a strong lightweight network architecture, termed PLR-OSNet, based on the idea of Part-Level feature Resolution over the Omni-Scale Network (OSNet) for achieving feature diversity. The proposed PLR-OSNet has two branches, one branch for global feature representation and the other branch for local feature representation. The local branch employs a uniform partition strategy for part-level feature resolution but produces only a single identity-prediction loss, which is in sharp contrast to the existing part-based methods. Empirical evidence demonstrates that the proposed PLR-OSNet achieves state-of-the-art performance on popular person Re-ID datasets, including Market1501, DukeMTMC-reID and CUHK03, despite its small model size.



### Detecting Face2Face Facial Reenactment in Videos
- **Arxiv ID**: http://arxiv.org/abs/2001.07444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07444v1)
- **Published**: 2020-01-21 11:03:50+00:00
- **Updated**: 2020-01-21 11:03:50+00:00
- **Authors**: Prabhat Kumar, Mayank Vatsa, Richa Singh
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Visual content has become the primary source of information, as evident in the billions of images and videos, shared and uploaded on the Internet every single day. This has led to an increase in alterations in images and videos to make them more informative and eye-catching for the viewers worldwide. Some of these alterations are simple, like copy-move, and are easily detectable, while other sophisticated alterations like reenactment based DeepFakes are hard to detect. Reenactment alterations allow the source to change the target expressions and create photo-realistic images and videos. While technology can be potentially used for several applications, the malicious usage of automatic reenactment has a very large social implication. It is therefore important to develop detection techniques to distinguish real images and videos with the altered ones. This research proposes a learning-based algorithm for detecting reenactment based alterations. The proposed algorithm uses a multi-stream network that learns regional artifacts and provides a robust performance at various compression levels. We also propose a loss function for the balanced learning of the streams for the proposed network. The performance is evaluated on the publicly available FaceForensics dataset. The results show state-of-the-art classification accuracy of 99.96%, 99.10%, and 91.20% for no, easy, and hard compression factors, respectively.



### P$^2$-GAN: Efficient Style Transfer Using Single Style Image
- **Arxiv ID**: http://arxiv.org/abs/2001.07466v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07466v2)
- **Published**: 2020-01-21 12:08:08+00:00
- **Updated**: 2020-01-30 16:37:22+00:00
- **Authors**: Zhentan Zheng, Jianyi Liu
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Style transfer is a useful image synthesis technique that can re-render given image into another artistic style while preserving its content information. Generative Adversarial Network (GAN) is a widely adopted framework toward this task for its better representation ability on local style patterns than the traditional Gram-matrix based methods. However, most previous methods rely on sufficient amount of pre-collected style images to train the model. In this paper, a novel Patch Permutation GAN (P$^2$-GAN) network that can efficiently learn the stroke style from a single style image is proposed. We use patch permutation to generate multiple training samples from the given style image. A patch discriminator that can simultaneously process patch-wise images and natural images seamlessly is designed. We also propose a local texture descriptor based criterion to quantitatively evaluate the style transfer quality. Experimental results showed that our method can produce finer quality re-renderings from single style image with improved computational efficiency compared with many state-of-the-arts methods.



### Instance Segmentation of Visible and Occluded Regions for Finding and Picking Target from a Pile of Objects
- **Arxiv ID**: http://arxiv.org/abs/2001.07475v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07475v1)
- **Published**: 2020-01-21 12:28:37+00:00
- **Updated**: 2020-01-21 12:28:37+00:00
- **Authors**: Kentaro Wada, Shingo Kitagawa, Kei Okada, Masayuki Inaba
- **Comment**: 8 pages, 11 figures, IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2018
- **Journal**: None
- **Summary**: We present a robotic system for picking a target from a pile of objects that is capable of finding and grasping the target object by removing obstacles in the appropriate order. The fundamental idea is to segment instances with both visible and occluded masks, which we call `instance occlusion segmentation'. To achieve this, we extend an existing instance segmentation model with a novel `relook' architecture, in which the model explicitly learns the inter-instance relationship. Also, by using image synthesis, we make the system capable of handling new objects without human annotations. The experimental results show the effectiveness of the relook architecture when compared with a conventional model and of the image synthesis when compared to a human-annotated dataset. We also demonstrate the capability of our system to achieve picking a target in a cluttered environment with a real robot.



### Joint Learning of Instance and Semantic Segmentation for Robotic Pick-and-Place with Heavy Occlusions in Clutter
- **Arxiv ID**: http://arxiv.org/abs/2001.07481v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07481v1)
- **Published**: 2020-01-21 12:37:08+00:00
- **Updated**: 2020-01-21 12:37:08+00:00
- **Authors**: Kentaro Wada, Kei Okada, Masayuki Inaba
- **Comment**: 7 pages, 13 figures, IEEE International Conference on Robotics and
  Automation (ICRA) 2019
- **Journal**: None
- **Summary**: We present joint learning of instance and semantic segmentation for visible and occluded region masks. Sharing the feature extractor with instance occlusion segmentation, we introduce semantic occlusion segmentation into the instance segmentation model. This joint learning fuses the instance- and image-level reasoning of the mask prediction on the different segmentation tasks, which was missing in the previous work of learning instance segmentation only (instance-only). In the experiments, we evaluated the proposed joint learning comparing the instance-only learning on the test dataset. We also applied the joint learning model to 2 different types of robotic pick-and-place tasks (random and target picking) and evaluated its effectiveness to achieve real-world robotic tasks.



### Multimodal Deep Unfolding for Guided Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2001.07575v1
- **DOI**: 10.1109/TIP.2020.3014729
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07575v1)
- **Published**: 2020-01-21 14:41:53+00:00
- **Updated**: 2020-01-21 14:41:53+00:00
- **Authors**: Iman Marivani, Evaggelia Tsiligianni, Bruno Cornelis, Nikos Deligiannis
- **Comment**: None
- **Journal**: None
- **Summary**: The reconstruction of a high resolution image given a low resolution observation is an ill-posed inverse problem in imaging. Deep learning methods rely on training data to learn an end-to-end mapping from a low-resolution input to a high-resolution output. Unlike existing deep multimodal models that do not incorporate domain knowledge about the problem, we propose a multimodal deep learning design that incorporates sparse priors and allows the effective integration of information from another image modality into the network architecture. Our solution relies on a novel deep unfolding operator, performing steps similar to an iterative algorithm for convolutional sparse coding with side information; therefore, the proposed neural network is interpretable by design. The deep unfolding architecture is used as a core component of a multimodal framework for guided image super-resolution. An alternative multimodal design is investigated by employing residual learning to improve the training efficiency. The presented multimodal approach is applied to super-resolution of near-infrared and multi-spectral images as well as depth upsampling using RGB images as side information. Experimental results show that our model outperforms state-of-the-art methods.



### Geometric Proxies for Live RGB-D Stream Enhancement and Consolidation
- **Arxiv ID**: http://arxiv.org/abs/2001.07577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07577v1)
- **Published**: 2020-01-21 14:42:35+00:00
- **Updated**: 2020-01-21 14:42:35+00:00
- **Authors**: Adrien Kaiser, Jos√© Alonso Ybanez Zepeda, Tamy Boubekeur
- **Comment**: extension of our ECCV 2018 paper at
  http://openaccess.thecvf.com/content_ECCV_2018/html/Adrien_Kaiser_Proxy_Clouds_for_ECCV_2018_paper.html
- **Journal**: None
- **Summary**: We propose a geometric superstructure for unified real-time processing of RGB-D data. Modern RGB-D sensors are widely used for indoor 3D capture, with applications ranging from modeling to robotics, through augmented reality. Nevertheless, their use is limited by their low resolution, with frames often corrupted with noise, missing data and temporal inconsistencies. Our approach consists in generating and updating through time a single set of compact local statistics parameterized over detected geometric proxies, which are fed from raw RGB-D data. Our proxies provide several processing primitives, which improve the quality of the RGB-D stream on the fly or lighten further operations. Experimental results confirm that our lightweight analysis framework copes well with embedded execution as well as moderate memory and computational capabilities compared to state-of-the-art methods. Processing RGB-D data with our proxies allows noise and temporal flickering removal, hole filling and resampling. As a substitute of the observed scene, our proxies can additionally be applied to compression and scene reconstruction. We present experiments performed with our framework in indoor scenes of different natures within a recent open RGB-D dataset.



### Motif Difference Field: A Simple and Effective Image Representation of Time Series for Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.07582v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.07582v1)
- **Published**: 2020-01-21 14:48:43+00:00
- **Updated**: 2020-01-21 14:48:43+00:00
- **Authors**: Yadong Zhang, Xin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Time series motifs play an important role in the time series analysis. The motif-based time series clustering is used for the discovery of higher-order patterns or structures in time series data. Inspired by the convolutional neural network (CNN) classifier based on the image representations of time series, motif difference field (MDF) is proposed. Compared to other image representations of time series, MDF is simple and easy to construct. With the Fully Convolution Network (FCN) as the classifier, MDF demonstrates the superior performance on the UCR time series dataset in benchmark with other time series classification methods. It is interesting to find that the triadic time series motifs give the best result in the test. Due to the motif clustering reflected in MDF, the significant motifs are detected with the help of the Gradient-weighted Class Activation Mapping (Grad-CAM). The areas in MDF with high weight in Grad-CAM have a high contribution from the significant motifs with the desired ordinal patterns associated with the signature patterns in time series. However, the signature patterns cannot be identified with the neural network classifiers directly based on the time series.



### PatchPerPix for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.07626v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07626v4)
- **Published**: 2020-01-21 16:06:51+00:00
- **Updated**: 2022-12-08 17:46:30+00:00
- **Authors**: Peter Hirsch, Lisa Mais, Dagmar Kainmueller
- **Comment**: ECCV2020, code: https://github.com/Kainmueller-Lab/PatchPerPix
- **Journal**: None
- **Summary**: We present a novel method for proposal free instance segmentation that can handle sophisticated object shapes which span large parts of an image and form dense object clusters with crossovers. Our method is based on predicting dense local shape descriptors, which we assemble to form instances. All instances are assembled simultaneously in one go. To our knowledge, our method is the first non-iterative method that yields instances that are composed of learnt shape patches. We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy data of cell nuclei. We show furthermore that our method also applies to 3d light microscopy data of Drosophila neurons, which exhibit extreme cases of complex shape clusters



### batchboost: regularization for stabilizing training with resistance to underfitting & overfitting
- **Arxiv ID**: http://arxiv.org/abs/2001.07627v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.07627v1)
- **Published**: 2020-01-21 16:07:27+00:00
- **Updated**: 2020-01-21 16:07:27+00:00
- **Authors**: Maciej A. Czyzewski
- **Comment**: 6 pages; 5 figures
- **Journal**: None
- **Summary**: Overfitting & underfitting and stable training are an important challenges in machine learning. Current approaches for these issues are mixup, SamplePairing and BC learning. In our work, we state the hypothesis that mixing many images together can be more effective than just two. Batchboost pipeline has three stages: (a) pairing: method of selecting two samples. (b) mixing: how to create a new one from two samples. (c) feeding: combining mixed samples with new ones from dataset into batch (with ratio $\gamma$). Note that sample that appears in our batch propagates with subsequent iterations with less and less importance until the end of training. Pairing stage calculates the error per sample, sorts the samples and pairs with strategy: hardest with easiest one, than mixing stage merges two samples using mixup, $x_1 + (1-\lambda)x_2$. Finally, feeding stage combines new samples with mixed by ratio 1:1. Batchboost has 0.5-3% better accuracy than the current state-of-the-art mixup regularization on CIFAR-10 & Fashion-MNIST. Our method is slightly better than SamplePairing technique on small datasets (up to 5%). Batchboost provides stable training on not tuned parameters (like weight decay), thus its a good method to test performance of different architectures. Source code is at: https://github.com/maciejczyzewski/batchboost



### HRFA: High-Resolution Feature-based Attack
- **Arxiv ID**: http://arxiv.org/abs/2001.07631v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.07631v2)
- **Published**: 2020-01-21 16:21:20+00:00
- **Updated**: 2020-10-22 13:08:04+00:00
- **Authors**: Zhixing Ye, Sizhe Chen, Peidong Zhang, Chengjin Sun, Xiaolin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks have long been developed for revealing the vulnerability of Deep Neural Networks (DNNs) by adding imperceptible perturbations to the input. Most methods generate perturbations like normal noise, which is not interpretable and without semantic meaning. In this paper, we propose High-Resolution Feature-based Attack (HRFA), yielding authentic adversarial examples with up to $1024 \times 1024$ resolution. HRFA exerts attack by modifying the latent feature representation of the image, i.e., the gradients back propagate not only through the victim DNN, but also through the generative model that maps the feature space to the image space. In this way, HRFA generates adversarial examples that are in high-resolution, realistic, noise-free, and hence is able to evade several denoising-based defenses. In the experiment, the effectiveness of HRFA is validated by attacking the object classification and face verification tasks with BigGAN and StyleGAN, respectively. The advantages of HRFA are verified from the high quality, high authenticity, and high attack success rate faced with defenses.



### SAUNet: Shape Attentive U-Net for Interpretable Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.07645v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07645v3)
- **Published**: 2020-01-21 16:48:54+00:00
- **Updated**: 2020-03-16 17:59:21+00:00
- **Authors**: Jesse Sun, Fatemeh Darbehani, Mark Zaidi, Bo Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is a difficult but important task for many clinical operations such as cardiac bi-ventricular volume estimation. More recently, there has been a shift to utilizing deep learning and fully convolutional neural networks (CNNs) to perform image segmentation that has yielded state-of-the-art results in many public benchmark datasets. Despite the progress of deep learning in medical image segmentation, standard CNNs are still not fully adopted in clinical settings as they lack robustness and interpretability. Shapes are generally more meaningful features than solely textures of images, which are features regular CNNs learn, causing a lack of robustness. Likewise, previous works surrounding model interpretability have been focused on post hoc gradient-based saliency methods. However, gradient-based saliency methods typically require additional computations post hoc and have been shown to be unreliable for interpretability. Thus, we present a new architecture called Shape Attentive U-Net (SAUNet) which focuses on model interpretability and robustness. The proposed architecture attempts to address these limitations by the use of a secondary shape stream that captures rich shape-dependent information in parallel with the regular texture stream. Furthermore, we suggest multi-resolution saliency maps can be learned using our dual-attention decoder module which allows for multi-level interpretability and mitigates the need for additional computations post hoc. Our method also achieves state-of-the-art results on the two large public cardiac MRI image segmentation datasets of SUN09 and AC17.



### FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence
- **Arxiv ID**: http://arxiv.org/abs/2001.07685v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.07685v2)
- **Published**: 2020-01-21 18:32:27+00:00
- **Updated**: 2020-11-25 17:22:06+00:00
- **Authors**: Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel
- **Comment**: Published at NeurIPS 2020 as a conference paper
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.



### TEASER: Fast and Certifiable Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2001.07715v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, math.OC, 68T40, 74Pxx, 46N10, 65D19, I.2.9; G.1.6; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2001.07715v2)
- **Published**: 2020-01-21 18:56:00+00:00
- **Updated**: 2020-10-17 20:03:04+00:00
- **Authors**: Heng Yang, Jingnan Shi, Luca Carlone
- **Comment**: Accepted to IEEE Transactions on Robotics (T-RO). Code:
  https://github.com/MIT-SPARK/TEASER-plusplus. 20 pages main text, 24 pages
  appendix
- **Journal**: IEEE Transactions on Robotics (T-RO), 2020
- **Summary**: We propose the first fast and certifiable algorithm for the registration of two sets of 3D points in the presence of large amounts of outlier correspondences. We first reformulate the registration problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem is still non-convex and combinatorial in nature, we show that (i) TLS scale and (component-wise) translation estimation can be solved in polynomial time via adaptive voting, (ii) TLS rotation estimation can be relaxed to a semidefinite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates, and (iii) the graph-theoretic framework allows drastic pruning of outliers by finding the maximum clique. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While solving large SDP relaxations is typically slow, we develop a second fast and certifiable algorithm, named TEASER++, that uses graduated non-convexity to solve the rotation subproblem and leverages Douglas-Rachford Splitting to efficiently certify global optimality.   For both algorithms, we provide theoretical bounds on the estimation errors, which are the first of their kind for robust registration problems. Moreover, we test their performance on standard, object detection, and the 3DMatch benchmarks, and show that (i) both algorithms dominate the state of the art and are robust to more than 99% outliers, (ii) TEASER++ can run in milliseconds, and (iii) TEASER++ is so robust it can also solve problems without correspondences, where it largely outperforms ICP and it is more accurate than Go-ICP while being orders of magnitude faster.



### EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions
- **Arxiv ID**: http://arxiv.org/abs/2001.07739v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07739v3)
- **Published**: 2020-01-21 19:09:08+00:00
- **Updated**: 2020-03-09 16:14:31+00:00
- **Authors**: Joy O. Egede, Siyang Song, Temitayo A. Olugbade, Chongyang Wang, Amanda Williams, Hongying Meng, Min Aung, Nicholas D. Lane, Michel Valstar, Nadia Bianchi-Berthouze
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The EmoPain 2020 Challenge is the first international competition aimed at creating a uniform platform for the comparison of machine learning and multimedia processing methods of automatic chronic pain assessment from human expressive behaviour, and also the identification of pain-related behaviours. The objective of the challenge is to promote research in the development of assistive technologies that help improve the quality of life for people with chronic pain via real-time monitoring and feedback to help manage their condition and remain physically active. The challenge also aims to encourage the use of the relatively underutilised, albeit vital bodily expression signals for automatic pain and pain-related emotion recognition. This paper presents a description of the challenge, competition guidelines, bench-marking dataset, and the baseline systems' architecture and performance on the three sub-tasks: pain estimation from facial expressions, pain recognition from multimodal movement, and protective movement behaviour detection.



### Block-wise Scrambled Image Recognition Using Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/2001.07761v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07761v1)
- **Published**: 2020-01-21 20:22:10+00:00
- **Updated**: 2020-01-21 20:22:10+00:00
- **Authors**: Koki Madono, Masayuki Tanaka, Masaki Onishi, Tetsuji Ogawa
- **Comment**: 6 pages Artificial Intelligence of Things(AAAI-2020 WS)
- **Journal**: None
- **Summary**: In this study, a perceptually hidden object-recognition method is investigated to generate secure images recognizable by humans but not machines. Hence, both the perceptual information hiding and the corresponding object recognition methods should be developed. Block-wise image scrambling is introduced to hide perceptual information from a third party. In addition, an adaptation network is proposed to recognize those scrambled images. Experimental comparisons conducted using CIFAR datasets demonstrated that the proposed adaptation network performed well in incorporating simple perceptual information hiding into DNN-based image classification.



### Adaptive Loss Function for Super Resolution Neural Networks Using Convex Optimization Techniques
- **Arxiv ID**: http://arxiv.org/abs/2001.07766v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07766v1)
- **Published**: 2020-01-21 20:31:10+00:00
- **Updated**: 2020-01-21 20:31:10+00:00
- **Authors**: Seyed Mehdi Ayyoubzadeh, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Single Image Super-Resolution (SISR) task refers to learn a mapping from low-resolution images to the corresponding high-resolution ones. This task is known to be extremely difficult since it is an ill-posed problem. Recently, Convolutional Neural Networks (CNNs) have achieved state of the art performance on SISR. However, the images produced by CNNs do not contain fine details of the images. Generative Adversarial Networks (GANs) aim to solve this issue and recover sharp details. Nevertheless, GANs are notoriously difficult to train. Besides that, they generate artifacts in the high-resolution images. In this paper, we have proposed a method in which CNNs try to align images in different spaces rather than only the pixel space. Such a space is designed using convex optimization techniques. CNNs are encouraged to learn high-frequency components of the images as well as low-frequency components. We have shown that the proposed method can recover fine details of the images and it is stable in the training process.



### Lesion Harvester: Iteratively Mining Unlabeled Lesions and Hard-Negative Examples at Scale
- **Arxiv ID**: http://arxiv.org/abs/2001.07776v3
- **DOI**: 10.1109/TMI.2020.3022034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07776v3)
- **Published**: 2020-01-21 21:09:49+00:00
- **Updated**: 2020-11-23 17:50:20+00:00
- **Authors**: Jinzheng Cai, Adam P. Harrison, Youjing Zheng, Ke Yan, Yuankai Huo, Jing Xiao, Lin Yang, Le Lu
- **Comment**: 13 pages, 13 figures, to appear in IEEE Transactions on Medical
  Imaging
- **Journal**: IEEE Transactions on Medical Imaging, 2020
- **Summary**: Acquiring large-scale medical image data, necessary for training machine learning algorithms, is frequently intractable, due to prohibitive expert-driven annotation costs. Recent datasets extracted from hospital archives, e.g., DeepLesion, have begun to address this problem. However, these are often incompletely or noisily labeled, e.g., DeepLesion leaves over 50% of its lesions unlabeled. Thus, effective methods to harvest missing annotations are critical for continued progress in medical image analysis. This is the goal of our work, where we develop a powerful system to harvest missing lesions from the DeepLesion dataset at high precision. Accepting the need for some degree of expert labor to achieve high fidelity, we exploit a small fully-labeled subset of medical image volumes and use it to intelligently mine annotations from the remainder. To do this, we chain together a highly sensitive lesion proposal generator and a very selective lesion proposal classifier. While our framework is generic, we optimize our performance by proposing a 3D contextual lesion proposal generator and by using a multi-view multi-scale lesion proposal classifier. These produce harvested and hard-negative proposals, which we then re-use to finetune our proposal generator by using a novel hard negative suppression loss, continuing this process until no extra lesions are found. Extensive experimental analysis demonstrates that our method can harvest an additional 9,805 lesions while keeping precision above 90%. To demonstrate the benefits of our approach, we show that lesion detectors trained on our harvested lesions can significantly outperform the same variants only trained on the original annotations, with boost of average precision of 7% to 10%. We open source our annotations at https://github.com/JimmyCai91/DeepLesionAnnotation.



### Depth Completion Using a View-constrained Deep Prior
- **Arxiv ID**: http://arxiv.org/abs/2001.07791v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07791v3)
- **Published**: 2020-01-21 21:56:01+00:00
- **Updated**: 2020-12-01 22:07:55+00:00
- **Authors**: Pallabi Ghosh, Vibhav Vineet, Larry S. Davis, Abhinav Shrivastava, Sudipta Sinha, Neel Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown that the structure of convolutional neural networks (CNNs) induces a strong prior that favors natural images. This prior, known as a deep image prior (DIP), is an effective regularizer in inverse problems such as image denoising and inpainting. We extend the concept of the DIP to depth images. Given color images and noisy and incomplete target depth maps, we optimize a randomly-initialized CNN model to reconstruct a depth map restored by virtue of using the CNN network structure as a prior combined with a view-constrained photo-consistency loss. This loss is computed using images from a geometrically calibrated camera from nearby viewpoints. We apply this deep depth prior for inpainting and refining incomplete and noisy depth maps within both binocular and multi-view stereo pipelines. Our quantitative and qualitative evaluation shows that our refined depth maps are more accurate and complete, and after fusion, produces dense 3D models of higher quality.



### GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems
- **Arxiv ID**: http://arxiv.org/abs/2001.07792v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07792v3)
- **Published**: 2020-01-21 21:58:45+00:00
- **Updated**: 2020-06-23 20:13:52+00:00
- **Authors**: Yanmao Man, Ming Li, Ryan Gerdes
- **Comment**: Accepted by USENIX RAID 2020. Source code is available at
  https://github.com/Harry1993/GhostImage
- **Journal**: None
- **Summary**: In vision-based object classification systems imaging sensors perceive the environment and machine learning is then used to detect and classify objects for decision-making purposes; e.g., to maneuver an automated vehicle around an obstacle or to raise an alarm to indicate the presence of an intruder in surveillance settings. In this work we demonstrate how the perception domain can be remotely and unobtrusively exploited to enable an attacker to create spurious objects or alter an existing object. An automated system relying on a detection/classification framework subject to our attack could be made to undertake actions with catastrophic results due to attacker-induced misperception.   We focus on camera-based systems and show that it is possible to remotely project adversarial patterns into camera systems by exploiting two common effects in optical imaging systems, viz., lens flare/ghost effects and auto-exposure control. To improve the robustness of the attack to channel effects, we generate optimal patterns by integrating adversarial machine learning techniques with a trained end-to-end channel model. We experimentally demonstrate our attacks using a low-cost projector, on three different image datasets, in indoor and outdoor environments, and with three different cameras. Experimental results show that, depending on the projector-camera distance, attack success rates can reach as high as 100% and under targeted conditions.



### Weakly Supervised Temporal Action Localization Using Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.07793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07793v1)
- **Published**: 2020-01-21 22:01:17+00:00
- **Updated**: 2020-01-21 22:01:17+00:00
- **Authors**: Ashraful Islam, Richard J. Radke
- **Comment**: accepted to WACV 2020
- **Journal**: None
- **Summary**: Temporal action localization is an important step towards video understanding. Most current action localization methods depend on untrimmed videos with full temporal annotations of action instances. However, it is expensive and time-consuming to annotate both action labels and temporal boundaries of videos. To this end, we propose a weakly supervised temporal action localization method that only requires video-level action instances as supervision during training. We propose a classification module to generate action labels for each segment in the video, and a deep metric learning module to learn the similarity between different action instances. We jointly optimize a balanced binary cross-entropy loss and a metric loss using a standard backpropagation algorithm. Extensive experiments demonstrate the effectiveness of both of these components in temporal localization. We evaluate our algorithm on two challenging untrimmed video datasets: THUMOS14 and ActivityNet1.2. Our approach improves the current state-of-the-art result for THUMOS14 by 6.5% mAP at IoU threshold 0.5, and achieves competitive performance for ActivityNet1.2.



### Scientific Image Tampering Detection Based On Noise Inconsistencies: A Method And Datasets
- **Arxiv ID**: http://arxiv.org/abs/2001.07799v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07799v2)
- **Published**: 2020-01-21 22:29:56+00:00
- **Updated**: 2020-03-04 20:46:46+00:00
- **Authors**: Ziyue Xiang, Daniel E. Acuna
- **Comment**: None
- **Journal**: None
- **Summary**: Scientific image tampering is a problem that affects not only authors but also the general perception of the research community. Although previous researchers have developed methods to identify tampering in natural images, these methods may not thrive under the scientific setting as scientific images have different statistics, format, quality, and intentions. Therefore, we propose a scientific-image specific tampering detection method based on noise inconsistencies, which is capable of learning and generalizing to different fields of science. We train and test our method on a new dataset of manipulated western blot and microscopy imagery, which aims at emulating problematic images in science. The test results show that our method can detect various types of image manipulation in different scenarios robustly, and it outperforms existing general-purpose image tampering detection schemes. We discuss applications beyond these two types of images and suggest next steps for making detection of problematic images a systematic step in peer review and science in general.



### Depth-Based Selective Blurring in Stereo Images Using Accelerated Framework
- **Arxiv ID**: http://arxiv.org/abs/2001.07809v1
- **DOI**: 10.1007/s13319-014-0014-7
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07809v1)
- **Published**: 2020-01-21 23:26:39+00:00
- **Updated**: 2020-01-21 23:26:39+00:00
- **Authors**: Subhayan Mukherjee, Ram Mohana Reddy Guddeti
- **Comment**: arXiv admin note: text overlap with arXiv:2001.06967
- **Journal**: 3D Research (Springer) 5, Article number: 14 (2014)
- **Summary**: We propose a hybrid method for stereo disparity estimation by combining block and region-based stereo matching approaches. It generates dense depth maps from disparity measurements of only 18 % image pixels (left or right). The methodology involves segmenting pixel lightness values using fast K-Means implementation, refining segment boundaries using morphological filtering and connected components analysis; then determining boundaries' disparities using sum of absolute differences (SAD) cost function. Complete disparity maps are reconstructed from boundaries' disparities. We consider an application of our method for depth-based selective blurring of non-interest regions of stereo images, using Gaussian blur to de-focus users' non-interest regions. Experiments on Middlebury dataset demonstrate that our method outperforms traditional disparity estimation approaches using SAD and normalized cross correlation by up to 33.6 % and some recent methods by up to 6.1 %. Further, our method is highly parallelizable using CPU and GPU framework based on Java Thread Pool and APARAPI with speed-up of 5.8 for 250 stereo video frames (4,096 x 2,304).




# Arxiv Papers in cs.CV on 2020-01-07
### HybridPose: 6D Object Pose Estimation under Hybrid Representations
- **Arxiv ID**: http://arxiv.org/abs/2001.01869v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.01869v4)
- **Published**: 2020-01-07 03:08:27+00:00
- **Updated**: 2020-10-16 04:34:15+00:00
- **Authors**: Chen Song, Jiaru Song, Qixing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce HybridPose, a novel 6D object pose estimation approach. HybridPose utilizes a hybrid intermediate representation to express different geometric information in the input image, including keypoints, edge vectors, and symmetry correspondences. Compared to a unitary representation, our hybrid representation allows pose regression to exploit more and diverse features when one type of predicted representation is inaccurate (e.g., because of occlusion). Different intermediate representations used by HybridPose can all be predicted by the same simple neural network, and outliers in predicted intermediate representations are filtered by a robust regression module. Compared to state-of-the-art pose estimation approaches, HybridPose is comparable in running time and accuracy. For example, on Occlusion Linemod dataset, our method achieves a prediction speed of 30 fps with a mean ADD(-S) accuracy of 47.5%, representing a state-of-the-art performance. The implementation of HybridPose is available at https://github.com/chensong1995/HybridPose.



### MW-GAN: Multi-Warping GAN for Caricature Generation with Multi-Style Geometric Exaggeration
- **Arxiv ID**: http://arxiv.org/abs/2001.01870v2
- **DOI**: 10.1109/TIP.2021.3118984
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01870v2)
- **Published**: 2020-01-07 03:08:30+00:00
- **Updated**: 2021-12-19 08:25:32+00:00
- **Authors**: Haodi Hou, Jing Huo, Jing Wu, Yu-Kun Lai, Yang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Given an input face photo, the goal of caricature generation is to produce stylized, exaggerated caricatures that share the same identity as the photo. It requires simultaneous style transfer and shape exaggeration with rich diversity, and meanwhile preserving the identity of the input. To address this challenging problem, we propose a novel framework called Multi-Warping GAN (MW-GAN), including a style network and a geometric network that are designed to conduct style transfer and geometric exaggeration respectively. We bridge the gap between the style and landmarks of an image with corresponding latent code spaces by a dual way design, so as to generate caricatures with arbitrary styles and geometric exaggeration, which can be specified either through random sampling of latent code or from a given caricature sample. Besides, we apply identity preserving loss to both image space and landmark space, leading to a great improvement in quality of generated caricatures. Experiments show that caricatures generated by MW-GAN have better quality than existing methods.



### From Open Set to Closed Set: Supervised Spatial Divide-and-Conquer for Object Counting
- **Arxiv ID**: http://arxiv.org/abs/2001.01886v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.01886v2)
- **Published**: 2020-01-07 04:36:53+00:00
- **Updated**: 2020-05-31 08:59:29+00:00
- **Authors**: Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Chunhua Shen, Zhiguo Cao
- **Comment**: Extended version of arXiv:1908.06473
- **Journal**: None
- **Summary**: Visual counting, a task that aims to estimate the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0, inf) in theory. However, collected data and labeled instances are limited in reality, which means that only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are prone to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting has an interesting and exclusive property---spatially decomposable. A dense region can always be divided until sub-region counts are within the previously observed closed set. We therefore introduce the idea of spatial divide-and-conquer (S-DC) that transforms open-set counting into a closed-set problem. This idea is implemented by a novel Supervised Spatial Divide-and-Conquer Network (SS-DCNet). Thus, SS-DCNet can only learn from a closed set but generalize well to open-set scenarios via S-DC. SS-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. We provide theoretical analyses as well as a controlled experiment on toy data, demonstrating why closed-set modeling makes sense. Extensive experiments show that SS-DCNet achieves the state-of-the-art performance. Code and models are available at: https://tinyurl.com/SS-DCNet.



### Fast and robust multiplane single molecule localization microscopy using deep neural network
- **Arxiv ID**: http://arxiv.org/abs/2001.01893v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01893v1)
- **Published**: 2020-01-07 05:12:14+00:00
- **Updated**: 2020-01-07 05:12:14+00:00
- **Authors**: Toshimitsu Aritake, Hideitsu Hino, Shigeyuki Namiki, Daisuke Asanuma, Kenzo Hirose, Noboru Murata
- **Comment**: None
- **Journal**: None
- **Summary**: Single molecule localization microscopy is widely used in biological research for measuring the nanostructures of samples smaller than the diffraction limit. This study uses multifocal plane microscopy and addresses the 3D single molecule localization problem, where lateral and axial locations of molecules are estimated. However, when we multifocal plane microscopy is used, the estimation accuracy of 3D localization is easily deteriorated by the small lateral drifts of camera positions. We formulate a 3D molecule localization problem along with the estimation of the lateral drifts as a compressed sensing problem, A deep neural network was applied to accurately and efficiently solve this problem. The proposed method is robust to the lateral drifts and achieves an accuracy of 20 nm laterally and 50 nm axially without an explicit drift correction.



### Automated Pavement Crack Segmentation Using U-Net-based Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2001.01912v4
- **DOI**: 10.1109/ACCESS.2020.3003638
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.01912v4)
- **Published**: 2020-01-07 07:18:13+00:00
- **Updated**: 2020-06-30 14:43:42+00:00
- **Authors**: Stephen L. H. Lau, Edwin K. P. Chong, Xu Yang, Xin Wang
- **Comment**: Accepted for publication in IEEE Access
- **Journal**: None
- **Summary**: Automated pavement crack image segmentation is challenging because of inherent irregular patterns, lighting conditions, and noise in images. Conventional approaches require a substantial amount of feature engineering to differentiate crack regions from non-affected regions. In this paper, we propose a deep learning technique based on a convolutional neural network to perform segmentation tasks on pavement crack images. Our approach requires minimal feature engineering compared to other machine learning techniques. We propose a U-Net-based network architecture in which we replace the encoder with a pretrained ResNet-34 neural network. We use a "one-cycle" training schedule based on cyclical learning rates to speed up the convergence. Our method achieves an F1 score of 96% on the CFD dataset and 73% on the Crack500 dataset, outperforming other algorithms tested on these datasets. We perform ablation studies on various techniques that helped us get marginal performance boosts, i.e., the addition of spatial and channel squeeze and excitation (SCSE) modules, training with gradually increasing image sizes, and training various neural network layers with different learning rates.



### A water-obstacle separation and refinement network for unmanned surface vehicles
- **Arxiv ID**: http://arxiv.org/abs/2001.01921v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.01921v1)
- **Published**: 2020-01-07 07:47:52+00:00
- **Updated**: 2020-01-07 07:47:52+00:00
- **Authors**: Borja Bovcon, Matej Kristan
- **Comment**: 6 pages + references, 6 figures, submitted to ICRA2020. MODD2 and
  MaSTr1325 datasets are available at
  http://box.vicos.si/borja/viamaro/index.html
- **Journal**: None
- **Summary**: Obstacle detection by semantic segmentation shows a great promise for autonomous navigation in unmanned surface vehicles (USV). However, existing methods suffer from poor estimation of the water edge in the presence of visual ambiguities, poor detection of small obstacles and high false-positive rate on water reflections and wakes. We propose a new deep encoder-decoder architecture, a water-obstacle separation and refinement network (WaSR), to address these issues. Detection and water edge accuracy are improved by a novel decoder that gradually fuses inertial information from IMU with the visual features from the encoder. In addition, a novel loss function is designed to increase the separation between water and obstacle features early on in the network. Subsequently, the capacity of the remaining layers in the decoder is better utilised, leading to a significant reduction in false positives and increased true positives. Experimental results show that WaSR outperforms the current state-of-the-art by a large margin, yielding a 14% increase in F-measure over the second-best method.



### Detection of Diabetic Anomalies in Retinal Images using Morphological Cascading Decision Tree
- **Arxiv ID**: http://arxiv.org/abs/2001.01953v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01953v1)
- **Published**: 2020-01-07 10:20:11+00:00
- **Updated**: 2020-01-07 10:20:11+00:00
- **Authors**: Faisal Ghaffar, Sarwar Khan, Bunyarit Uyyanonvara, Chanjira Sinthanayothin, Hirohiko Kaneko
- **Comment**: None
- **Journal**: None
- **Summary**: This research aims to develop an efficient system for screening of diabetic retinopathy. Diabetic retinopathy is the major cause of blindness. Severity of diabetic retinopathy is recognized by some features, such as blood vessel area, exudates, haemorrhages and microaneurysms. To grade the disease the screening system must efficiently detect these features. In this paper we are proposing a simple and fast method for detection of diabetic retinopathy. We do pre-processing of grey-scale image and find all labelled connected components (blobs) in an image regardless of whether it is haemorrhages, exudates, vessels, optic disc or anything else. Then we apply some constraints such as compactness, area of blob, intensity and contrast for screening of candidate connectedcomponent responsible for diabetic retinopathy. We obtain our final results by doing some post processing. The results are compared with ground truths. Performance is measured by finding the recall (sensitivity). We took 10 images of dimension 500 * 752. The mean recall is 90.03%.



### Delineating Bone Surfaces in B-Mode Images Constrained by Physics of Ultrasound Propagation
- **Arxiv ID**: http://arxiv.org/abs/2001.02001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02001v1)
- **Published**: 2020-01-07 12:34:42+00:00
- **Updated**: 2020-01-07 12:34:42+00:00
- **Authors**: Firat Ozdemir, Christine Tanner, Orcun Goksel
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Bone surface delineation in ultrasound is of interest due to its potential in diagnosis, surgical planning, and post-operative follow-up in orthopedics, as well as the potential of using bones as anatomical landmarks in surgical navigation. We herein propose a method to encode the physics of ultrasound propagation into a factor graph formulation for the purpose of bone surface delineation. In this graph structure, unary node potentials encode the local likelihood for being a soft tissue or acoustic-shadow (behind bone surface) region, both learned through image descriptors. Pair-wise edge potentials encode ultrasound propagation constraints of bone surfaces given their large acoustic-impedance difference. We evaluate the proposed method in comparison with four earlier approaches, on in-vivo ultrasound images collected from dorsal and volar views of the forearm. The proposed method achieves an average root-mean-square error and symmetric Hausdorff distance of 0.28mm and 1.78mm, respectively. It detects 99.9% of the annotated bone surfaces with a mean scanline error (distance to annotations) of 0.39mm.



### Deep Reinforcement Learning for Active Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2001.02024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02024v2)
- **Published**: 2020-01-07 13:35:41+00:00
- **Updated**: 2020-12-16 10:23:42+00:00
- **Authors**: Erik GÃ¤rtner, Aleksis Pirinen, Cristian Sminchisescu
- **Comment**: Accepted to The Thirty-Fourth AAAI Conference on Artificial
  Intelligence (AAAI-20). Submission updated to include supplementary material
- **Journal**: None
- **Summary**: Most 3d human pose estimation methods assume that input -- be it images of a scene collected from one or several viewpoints, or from a video -- is given. Consequently, they focus on estimates leveraging prior knowledge and measurement by fusing information spatially and/or temporally, whenever available. In this paper we address the problem of an active observer with freedom to move and explore the scene spatially -- in `time-freeze' mode -- and/or temporally, by selecting informative viewpoints that improve its estimation accuracy. Towards this end, we introduce Pose-DRL, a fully trainable deep reinforcement learning-based active pose estimation architecture which learns to select appropriate views, in space and time, to feed an underlying monocular pose estimator. We evaluate our model using single- and multi-target estimators with strong result in both settings. Our system further learns automatic stopping conditions in time and transition functions to the next temporal processing step in videos. In extensive experiments with the Panoptic multi-view setup, and for complex scenes containing multiple people, we show that our model learns to select viewpoints that yield significantly more accurate pose estimates compared to strong multi-view baselines.



### Multimodal Semantic Transfer from Text to Image. Fine-Grained Image Classification by Distributional Semantics
- **Arxiv ID**: http://arxiv.org/abs/2001.02372v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.02372v1)
- **Published**: 2020-01-07 14:26:06+00:00
- **Updated**: 2020-01-07 14:26:06+00:00
- **Authors**: Simon Donig, Maria Christoforaki, Bernhard Bermeitinger, Siegfried Handschuh
- **Comment**: 19 pages, second half in German as published in DHd2020
- **Journal**: None
- **Summary**: In the last years, image classification processes like neural networks in the area of art-history and Heritage Informatics have experienced a broad distribution (Lang and Ommer 2018). These methods face several challenges, including the handling of comparatively small amounts of data as well as high-dimensional data in the Digital Humanities. Here, a Convolutional Neural Network (CNN) is used that output is not, as usual, a series of flat text labels but a series of semantically loaded vectors. These vectors result from a Distributional Semantic Model (DSM) which is generated from an in-domain text corpus.   -----   In den letzten Jahren hat die Verwendung von Bildklassifizierungsverfahren wie neuronalen Netzwerken auch im Bereich der historischen Bildwissenschaften und der Heritage Informatics weite Verbreitung gefunden (Lang und Ommer 2018). Diese Verfahren stehen dabei vor einer Reihe von Herausforderungen, darunter dem Umgangmit den vergleichsweise kleinen Datenmengen sowie zugleich hochdimensionalen Da-tenr\"aumen in den digitalen Geisteswissenschaften. Meist bilden diese Methoden dieKlassifizierung auf einen vergleichsweise flachen Raum ab. Dieser flache Zugang verliert im Bem\"uhen um ontologische Eindeutigkeit eine Reihe von relevanten Dimensionen, darunter taxonomische, mereologische und assoziative Beziehungen zwischenden Klassen beziehungsweise dem nicht formalisierten Kontext. Dabei wird ein Convolutional Neural Network (CNN) genutzt, dessen Ausgabe im Trainingsprozess, anders als herk\"ommlich, nicht auf einer Serie flacher Textlabel beruht, sondern auf einer Serie von Vektoren. Diese Vektoren resultieren aus einem Distributional Semantic Model (DSM), welches aus einem Dom\"ane-Textkorpus generiert wird.



### AD-VO: Scale-Resilient Visual Odometry Using Attentive Disparity Map
- **Arxiv ID**: http://arxiv.org/abs/2001.02090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02090v1)
- **Published**: 2020-01-07 15:01:57+00:00
- **Updated**: 2020-01-07 15:01:57+00:00
- **Authors**: Joosung Lee, Sangwon Hwang, Kyungjae Lee, Woo Jin Kim, Junhyeop Lee, Tae-young Chung, Sangyoun Lee
- **Comment**: 5 pages, 5 figures, 2018.02 papers
- **Journal**: None
- **Summary**: Visual odometry is an essential key for a localization module in SLAM systems. However, previous methods require tuning the system to adapt environment changes. In this paper, we propose a learning-based approach for frame-to-frame monocular visual odometry estimation. The proposed network is only learned by disparity maps for not only covering the environment changes but also solving the scale problem. Furthermore, attention block and skip-ordering scheme are introduced to achieve robust performance in various driving environment. Our network is compared with the conventional methods which use common domain such as color or optical flow. Experimental results confirm that the proposed network shows better performance than other approaches with higher and more stable results.



### State Transition Modeling of the Smoking Behavior using LSTM Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.02101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.02101v1)
- **Published**: 2020-01-07 15:06:28+00:00
- **Updated**: 2020-01-07 15:06:28+00:00
- **Authors**: Chrisogonas O. Odhiambo, Casey A. Cole, Alaleh Torkjazi, Homayoun Valafar
- **Comment**: 8 pages, CSCI 2019
- **Journal**: None
- **Summary**: The use of sensors has pervaded everyday life in several applications including human activity monitoring, healthcare, and social networks. In this study, we focus on the use of smartwatch sensors to recognize smoking activity. More specifically, we have reformulated the previous work in detection of smoking to include in-context recognition of smoking. Our presented reformulation of the smoking gesture as a state-transition model that consists of the mini-gestures hand-to-lip, hand-on-lip, and hand-off-lip, has demonstrated improvement in detection rates nearing 100% using conventional neural networks. In addition, we have begun the utilization of Long-Short-Term Memory (LSTM) neural networks to allow for in-context detection of gestures with accuracy nearing 97%.



### General 3D Room Layout from a Single View by Render-and-Compare
- **Arxiv ID**: http://arxiv.org/abs/2001.02149v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02149v2)
- **Published**: 2020-01-07 16:14:00+00:00
- **Updated**: 2020-07-21 15:41:56+00:00
- **Authors**: Sinisa Stekovic, Shreyas Hampali, Mahdi Rad, Sayan Deb Sarkar, Friedrich Fraundorfer, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method to reconstruct the 3D layout of a room (walls, floors, ceilings) from a single perspective view in challenging conditions, by contrast with previous single-view methods restricted to cuboid-shaped layouts. This input view can consist of a color image only, but considering a depth map results in a more accurate reconstruction. Our approach is formalized as solving a constrained discrete optimization problem to find the set of 3D polygons that constitute the layout. In order to deal with occlusions between components of the layout, which is a problem ignored by previous works, we introduce an analysis-by-synthesis method to iteratively refine the 3D layout estimate. As no dataset was available to evaluate our method quantitatively, we created one together with several appropriate metrics. Our dataset consists of 293 images from ScanNet, which we annotated with precise 3D layouts. It offers three times more samples than the popular NYUv2 303 benchmark, and a much larger variety of layouts.



### Inferring Convolutional Neural Networks' accuracies from their architectural characterizations
- **Arxiv ID**: http://arxiv.org/abs/2001.02160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2001.02160v2)
- **Published**: 2020-01-07 16:41:58+00:00
- **Updated**: 2020-01-10 03:52:48+00:00
- **Authors**: Duc Hoang, Jesse Hamer, Gabriel N. Perdue, Steven R. Young, Jonathan Miller, Anushree Ghosh
- **Comment**: 6 pages, 5 figures, 5 tables, to appear in proceedings of the 18th
  International Conference on Machine Learning and Applications - ICMLA 2019
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have shown strong promise for analyzing scientific data from many domains including particle imaging detectors. However, the challenge of choosing the appropriate network architecture (depth, kernel shapes, activation functions, etc.) for specific applications and different data sets is still poorly understood. In this paper, we study the relationships between a CNN's architecture and its performance by proposing a systematic language that is useful for comparison between different CNN's architectures before training time. We characterize CNN's architecture by different attributes, and demonstrate that the attributes can be predictive of the networks' performance in two specific computer vision-based physics problems -- event vertex finding and hadron multiplicity classification in the MINERvA experiment at Fermi National Accelerator Laboratory. In doing so, we extract several architectural attributes from optimized networks' architecture for the physics problems, which are outputs of a model selection algorithm called Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL). We use machine learning models to predict whether a network can perform better than a certain threshold accuracy before training. The models perform 16-20% better than random guessing. Additionally, we found an coefficient of determination of 0.966 for an Ordinary Least Squares model in a regression on accuracy over a large population of networks.



### Robust Facial Landmark Detection via Aggregation on Geometrically Manipulated Faces
- **Arxiv ID**: http://arxiv.org/abs/2001.03113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03113v1)
- **Published**: 2020-01-07 16:43:09+00:00
- **Updated**: 2020-01-07 16:43:09+00:00
- **Authors**: Seyed Mehdi Iranmanesh, Ali Dabouei, Sobhan Soleymani, Hadi Kazemi, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a practical approach to the problem of facial landmark detection. The proposed method can deal with large shape and appearance variations under the rich shape deformation. To handle the shape variations we equip our method with the aggregation of manipulated face images. The proposed framework generates different manipulated faces using only one given face image. The approach utilizes the fact that small but carefully crafted geometric manipulation in the input domain can fool deep face recognition models. We propose three different approaches to generate manipulated faces in which two of them perform the manipulations via adversarial attacks and the other one uses known transformations. Aggregating the manipulated faces provides a more robust landmark detection approach which is able to capture more important deformations and variations of the face shapes. Our approach is demonstrated its superiority compared to the state-of-the-art method on benchmark datasets AFLW, 300-W, and COFW.



### Trained Trajectory based Automated Parking System using Visual SLAM on Surround View Cameras
- **Arxiv ID**: http://arxiv.org/abs/2001.02161v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.02161v3)
- **Published**: 2020-01-07 16:43:27+00:00
- **Updated**: 2021-05-19 20:34:33+00:00
- **Authors**: Nivedita Tripathi, Senthil Yogamani
- **Comment**: Accepted for presentation at CVPR 2021 Workshop on Women in Computer
  Vision
- **Journal**: None
- **Summary**: Automated Parking is becoming a standard feature in modern vehicles. Existing parking systems build a local map to be able to plan for maneuvering towards a detected slot. Next generation parking systems have an use case where they build a persistent map of the environment where the car is frequently parked, say for example, home parking or office parking. The pre-built map helps in re-localizing the vehicle better when its trying to park the next time. This is achieved by augmenting the parking system with a Visual SLAM pipeline and the feature is called trained trajectory parking in the automotive industry. In this paper, we discuss the use cases, design and implementation of a trained trajectory automated parking system. The proposed system is deployed on commercial vehicles and the consumer application is illustrated in \url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the application and the details of vision algorithms are kept at high level.



### An Exploration of Embodied Visual Exploration
- **Arxiv ID**: http://arxiv.org/abs/2001.02192v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2001.02192v2)
- **Published**: 2020-01-07 17:40:32+00:00
- **Updated**: 2020-08-21 02:58:53+00:00
- **Authors**: Santhosh K. Ramakrishnan, Dinesh Jayaraman, Kristen Grauman
- **Comment**: 30 main + 21 appendix pages, 23 figures
- **Journal**: None
- **Summary**: Embodied computer vision considers perception for robots in novel, unstructured environments. Of particular importance is the embodied visual exploration problem: how might a robot equipped with a camera scope out a new environment? Despite the progress thus far, many basic questions pertinent to this problem remain unanswered: (i) What does it mean for an agent to explore its environment well? (ii) Which methods work well, and under which assumptions and environmental settings? (iii) Where do current approaches fall short, and where might future work seek to improve? Seeking answers to these questions, we first present a taxonomy for existing visual exploration algorithms and create a standard framework for benchmarking them. We then perform a thorough empirical study of the four state-of-the-art paradigms using the proposed framework with two photorealistic simulated 3D environments, a state-of-the-art exploration architecture, and diverse evaluation metrics. Our experimental results offer insights and suggest new performance metrics and baselines for future work in visual exploration. Code, models and data are publicly available: https://github.com/facebookresearch/exploring_exploration



### Dynamic Task Weighting Methods for Multi-task Networks in Autonomous Driving Systems
- **Arxiv ID**: http://arxiv.org/abs/2001.02223v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.02223v2)
- **Published**: 2020-01-07 18:54:21+00:00
- **Updated**: 2020-06-27 20:01:15+00:00
- **Authors**: Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2020
- **Journal**: None
- **Summary**: Deep multi-task networks are of particular interest for autonomous driving systems. They can potentially strike an excellent trade-off between predictive performance, hardware constraints and efficient use of information from multiple types of annotations and modalities. However, training such models is non-trivial and requires balancing learning over all tasks as their respective losses display different scales, ranges and dynamics across training. Multiple task weighting methods that adjust the losses in an adaptive way have been proposed recently on different datasets and combinations of tasks, making it difficult to compare them. In this work, we review and systematically evaluate nine task weighting strategies on common grounds on three automotive datasets (KITTI, Cityscapes and WoodScape). We then propose a novel method combining evolutionary meta-learning and task-based selective backpropagation, for computing task weights leading to reliable network training. Our method outperforms state-of-the-art methods by a significant margin on a two-task application.



### Visual-Semantic Graph Attention Networks for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.02302v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02302v6)
- **Published**: 2020-01-07 22:22:46+00:00
- **Updated**: 2021-03-06 05:42:22+00:00
- **Authors**: Zhijun Liang, Juan Rojas, Junfa Liu, Yisheng Guan
- **Comment**: 7 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: In scene understanding, robotics benefit from not only detecting individual scene instances but also from learning their possible interactions. Human-Object Interaction (HOI) Detection infers the action predicate on a <human, predicate, object> triplet. Contextual information has been found critical in inferring interactions. However, most works only use local features from single human-object pair for inference. Few works have studied the disambiguating contribution of subsidiary relations made available via graph networks. Similarly, few have learned to effectively leverage visual cues along with the intrinsic semantic regularities contained in HOIs. We contribute a dual-graph attention network that effectively aggregates contextual visual, spatial, and semantic information dynamically from primary human-object relations as well as subsidiary relations through attention mechanisms for strong disambiguating power. We achieve comparable results on two benchmarks: V-COCO and HICO-DET. Code is available at \url{https://github.com/birlrobotics/vs-gats}.



### Aggressive Perception-Aware Navigation using Deep Optical Flow Dynamics and PixelMPC
- **Arxiv ID**: http://arxiv.org/abs/2001.02307v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2001.02307v1)
- **Published**: 2020-01-07 22:33:12+00:00
- **Updated**: 2020-01-07 22:33:12+00:00
- **Authors**: Keuntaek Lee, Jason Gibson, Evangelos A. Theodorou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, vision-based control has gained traction by leveraging the power of machine learning. In this work, we couple a model predictive control (MPC) framework to a visual pipeline. We introduce deep optical flow (DOF) dynamics, which is a combination of optical flow and robot dynamics. Using the DOF dynamics, MPC explicitly incorporates the predicted movement of relevant pixels into the planned trajectory of a robot. Our implementation of DOF is memory-efficient, data-efficient, and computationally cheap so that it can be computed in real-time for use in an MPC framework. The suggested Pixel Model Predictive Control (PixelMPC) algorithm controls the robot to accomplish a high-speed racing task while maintaining visibility of the important features (gates). This improves the reliability of vision-based estimators for localization and can eventually lead to safe autonomous flight. The proposed algorithm is tested in a photorealistic simulation with a high-speed drone racing task.



### Bridging Knowledge Graphs to Generate Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2001.02314v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02314v4)
- **Published**: 2020-01-07 23:35:52+00:00
- **Updated**: 2020-07-18 10:59:53+00:00
- **Authors**: Alireza Zareian, Svebor Karaman, Shih-Fu Chang
- **Comment**: To be presented at ECCV 2020
- **Journal**: None
- **Summary**: Scene graphs are powerful representations that parse images into their abstract semantic elements, i.e., objects and their interactions, which facilitates visual comprehension and explainable reasoning. On the other hand, commonsense knowledge graphs are rich repositories that encode how the world is structured, and how general concepts interact. In this paper, we present a unified formulation of these two constructs, where a scene graph is seen as an image-conditioned instantiation of a commonsense knowledge graph. Based on this new perspective, we re-formulate scene graph generation as the inference of a bridge between the scene and commonsense graphs, where each entity or predicate instance in the scene graph has to be linked to its corresponding entity or predicate class in the commonsense graph. To this end, we propose a novel graph-based neural network that iteratively propagates information between the two graphs, as well as within each of them, while gradually refining their bridge in each iteration. Our Graph Bridging Network, GB-Net, successively infers edges and nodes, allowing to simultaneously exploit and refine the rich, heterogeneous structure of the interconnected scene and commonsense graphs. Through extensive experimentation, we showcase the superior accuracy of GB-Net compared to the most recent methods, resulting in a new state of the art. We publicly release the source code of our method.




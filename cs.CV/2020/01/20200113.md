# Arxiv Papers in cs.CV on 2020-01-13
### Modeling of Pruning Techniques for Deep Neural Networks Simplification
- **Arxiv ID**: http://arxiv.org/abs/2001.04062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04062v1)
- **Published**: 2020-01-13 04:51:59+00:00
- **Updated**: 2020-01-13 04:51:59+00:00
- **Authors**: Morteza Mousa Pasandi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi
- **Comment**: six pages, eight figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) suffer from different issues, such as computational complexity and the number of parameters. In recent years pruning techniques are employed to reduce the number of operations and model size in CNNs. Different pruning methods are proposed, which are based on pruning the connections, channels, and filters. Various techniques and tricks accompany pruning methods, and there is not a unifying framework to model all the pruning methods. In this paper pruning methods are investigated, and a general model which is contained the majority of pruning techniques is proposed. The advantages and disadvantages of the pruning methods can be identified, and all of them can be summarized under this model. The final goal of this model is to provide a general approach for all of the pruning methods with different structures and applications.



### Boosting Occluded Image Classification via Subspace Decomposition Based Estimation of Deep Features
- **Arxiv ID**: http://arxiv.org/abs/2001.04066v1
- **DOI**: 10.1109/TCYB.2019.2931067
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04066v1)
- **Published**: 2020-01-13 05:36:27+00:00
- **Updated**: 2020-01-13 05:36:27+00:00
- **Authors**: Feng Cen, Guanghui Wang
- **Comment**: None
- **Journal**: IEEE Transactions on Cybernetics, 2019
- **Summary**: Classification of partially occluded images is a highly challenging computer vision problem even for the cutting edge deep learning technologies. To achieve a robust image classification for occluded images, this paper proposes a novel scheme using subspace decomposition based estimation (SDBE). The proposed SDBE-based classification scheme first employs a base convolutional neural network to extract the deep feature vector (DFV) and then utilizes the SDBE to compute the DFV of the original occlusion-free image for classification. The SDBE is performed by projecting the DFV of the occluded image onto the linear span of a class dictionary (CD) along the linear span of an occlusion error dictionary (OED). The CD and OED are constructed respectively by concatenating the DFVs of a training set and the occlusion error vectors of an extra set of image pairs. Two implementations of the SDBE are studied in this paper: the $l_1$-norm and the squared $l_2$-norm regularized least-squares estimates. By employing the ResNet-152, pre-trained on the ILSVRC2012 training set, as the base network, the proposed SBDE-based classification scheme is extensively evaluated on the Caltech-101 and ILSVRC2012 datasets. Extensive experimental results demonstrate that the proposed SDBE-based scheme dramatically boosts the classification accuracy for occluded images, and achieves around $22.25\%$ increase in classification accuracy under $20\%$ occlusion on the ILSVRC2012 dataset.



### Natural Image Matting via Guided Contextual Attention
- **Arxiv ID**: http://arxiv.org/abs/2001.04069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04069v1)
- **Published**: 2020-01-13 05:59:17+00:00
- **Updated**: 2020-01-13 05:59:17+00:00
- **Authors**: Yaoyi Li, Hongtao Lu
- **Comment**: AAAI-20
- **Journal**: None
- **Summary**: Over the last few years, deep learning based approaches have achieved outstanding improvements in natural image matting. Many of these methods can generate visually plausible alpha estimations, but typically yield blurry structures or textures in the semitransparent area. This is due to the local ambiguity of transparent objects. One possible solution is to leverage the far-surrounding information to estimate the local opacity. Traditional affinity-based methods often suffer from the high computational complexity, which are not suitable for high resolution alpha estimation. Inspired by affinity-based method and the successes of contextual attention in inpainting, we develop a novel end-to-end approach for natural image matting with a guided contextual attention module, which is specifically designed for image matting. Guided contextual attention module directly propagates high-level opacity information globally based on the learned low-level affinity. The proposed method can mimic information flow of affinity-based methods and utilize rich features learned by deep neural networks simultaneously. Experiment results on Composition-1k testing set and alphamatting.com benchmark dataset demonstrate that our method outperforms state-of-the-art approaches in natural image matting. Code and models are available at https://github.com/Yaoyi-Li/GCA-Matting.



### Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2001.04074v3
- **DOI**: 10.1016/j.knosys.2020.106062
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04074v3)
- **Published**: 2020-01-13 06:07:27+00:00
- **Updated**: 2020-05-29 07:23:43+00:00
- **Authors**: Farhana Sultana, Abu Sufian, Paramartha Dutta
- **Comment**: 38 pages, 29 figures, 8 tables
- **Journal**: journal = "Knowledge-Based Systems", volume = "201-202", pages =
  "106062", year = "2020"
- **Summary**: From the autonomous car driving to medical diagnosis, the requirement of the task of image segmentation is everywhere. Segmentation of an image is one of the indispensable tasks in computer vision. This task is comparatively complicated than other vision tasks as it needs low-level spatial information. Basically, image segmentation can be of two types: semantic segmentation and instance segmentation. The combined version of these two basic tasks is known as panoptic segmentation. In the recent era, the success of deep convolutional neural networks (CNN) has influenced the field of segmentation greatly and gave us various successful models to date. In this survey, we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN. We have also specified comparative architectural details of some state-of-the-art models and discuss their training details to present a lucid understanding of hyper-parameter tuning of those models. We have also drawn a comparison among the performance of those models on different datasets. Lastly, we have given a glimpse of some state-of-the-art panoptic segmentation models.



### Residual Attention Net for Superior Cross-Domain Time Sequence Modeling
- **Arxiv ID**: http://arxiv.org/abs/2001.04077v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04077v1)
- **Published**: 2020-01-13 06:14:04+00:00
- **Updated**: 2020-01-13 06:14:04+00:00
- **Authors**: Seth H. Huang, Xu Lingjie, Jiang Congwei
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel architecture, residual attention net (RAN), which merges a sequence architecture, universal transformer, and a computer vision architecture, residual net, with a high-way architecture for cross-domain sequence modeling. The architecture aims at addressing the long dependency issue often faced by recurrent-neural-net-based structures. This paper serves as a proof-of-concept for a new architecture, with RAN aiming at providing the model a higher level understanding of sequence patterns. To our best knowledge, we are the first to propose such an architecture. Out of the standard 85 UCR data sets, we have achieved 35 state-of-the-art results with 10 results matching current state-of-the-art results without further model fine-tuning. The results indicate that such architecture is promising in complex, long-sequence modeling and may have vast, cross-domain applications.



### GridMask Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.04086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04086v2)
- **Published**: 2020-01-13 07:27:05+00:00
- **Updated**: 2020-01-14 03:47:39+00:00
- **Authors**: Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel data augmentation method `GridMask' in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.



### Deep learning achieves perfect anomaly detection on 108,308 retinal images including unlearned diseases
- **Arxiv ID**: http://arxiv.org/abs/2001.05859v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05859v5)
- **Published**: 2020-01-13 08:32:47+00:00
- **Updated**: 2020-07-19 13:38:07+00:00
- **Authors**: Ayaka Suzuki, Yoshiro Suzuki
- **Comment**: None
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) scanning is useful in detecting various retinal diseases. However, there are not enough ophthalmologists who can diagnose retinal OCT images in much of the world. To provide OCT screening inexpensively and extensively, an automated diagnosis system is indispensable. Although many machine learning techniques have been presented for assisting ophthalmologists in diagnosing retinal OCT images, there is no technique that can diagnose independently without relying on an ophthalmologist, i.e., there is no technique that does not overlook any anomaly, including unlearned diseases. As long as there is a risk of overlooking a disease with a technique, ophthalmologists must double-check even those images that the technique classifies as normal. Here, we show that our deep-learning-based binary classifier (normal or abnormal) achieved a perfect classification on 108,308 two-dimensional retinal OCT images, i.e., true positive rate = 1.000000 and true negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000. Although the test set included three types of diseases, two of these were not used for training. However, all test images were correctly classified. Furthermore, we demonstrated that our scheme was able to cope with differences in patient race. No conventional approach has achieved the above performances. Our work has a sufficient possibility of raising automated diagnosis techniques for retinal OCT images from "assistant for ophthalmologists" to "independent diagnosis system without ophthalmologists".



### Predicting population neural activity in the Algonauts challenge using end-to-end trained Siamese networks and group convolutions
- **Arxiv ID**: http://arxiv.org/abs/2001.05841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05841v1)
- **Published**: 2020-01-13 08:36:10+00:00
- **Updated**: 2020-01-13 08:36:10+00:00
- **Authors**: Georgin Jacob, Harish Katti
- **Comment**: None
- **Journal**: None
- **Summary**: The Algonauts challenge is about predicting the object representations in the form of Representational Dissimilarity Matrices (RDMS) derived from visual brain regions. We used a customized deep learning model using the concept of Siamese networks and group convolutions to predict neural distances corresponding to a pair of images. Training data was best explained by distances computed over the last layer.



### A Bayesian Filter for Multi-view 3D Multi-object Tracking with Occlusion Handling
- **Arxiv ID**: http://arxiv.org/abs/2001.04118v4
- **DOI**: 10.1109/TPAMI.2020.3034435
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2001.04118v4)
- **Published**: 2020-01-13 09:34:07+00:00
- **Updated**: 2020-10-27 10:06:49+00:00
- **Authors**: Jonah Ong, Ba Tuong Vo, Ba Ngu Vo, Du Yong Kim, Sven Nordholm
- **Comment**: 18 pages, 11 figures, TPAMI
- **Journal**: None
- **Summary**: This paper proposes an online multi-camera multi-object tracker that only requires monocular detector training, independent of the multi-camera configurations, allowing seamless extension/deletion of cameras without retraining effort. The proposed algorithm has a linear complexity in the total number of detections across the cameras, and hence scales gracefully with the number of cameras. It operates in the 3D world frame, and provides 3D trajectory estimates of the objects. The key innovation is a high fidelity yet tractable 3D occlusion model, amenable to optimal Bayesian multi-view multi-object filtering, which seamlessly integrates, into a single Bayesian recursion, the sub-tasks of track management, state estimation, clutter rejection, and occlusion/misdetection handling. The proposed algorithm is evaluated on the latest WILDTRACKS dataset, and demonstrated to work in very crowded scenes on a new dataset.



### Memorizing Comprehensively to Learn Adaptively: Unsupervised Cross-Domain Person Re-ID with Multi-level Memory
- **Arxiv ID**: http://arxiv.org/abs/2001.04123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04123v1)
- **Published**: 2020-01-13 09:48:03+00:00
- **Updated**: 2020-01-13 09:48:03+00:00
- **Authors**: Xinyu Zhang, Dong Gong, Jiewei Cao, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised cross-domain person re-identification (Re-ID) aims to adapt the information from the labelled source domain to an unlabelled target domain. Due to the lack of supervision in the target domain, it is crucial to identify the underlying similarity-and-dissimilarity relationships among the unlabelled samples in the target domain. In order to use the whole data relationships efficiently in mini-batch training, we apply a series of memory modules to maintain an up-to-date representation of the entire dataset. Unlike the simple exemplar memory in previous works, we propose a novel multi-level memory network (MMN) to discover multi-level complementary information in the target domain, relying on three memory modules, i.e., part-level memory, instance-level memory, and domain-level memory. The proposed memory modules store multi-level representations of the target domain, which capture both the fine-grained differences between images and the global structure for the holistic target domain. The three memory modules complement each other and systematically integrate multi-level supervision from bottom to up. Experiments on three datasets demonstrate that the multi-level memory modules cooperatively boost the unsupervised cross-domain Re-ID task, and the proposed MMN achieves competitive results.



### Incremental Unsupervised Domain-Adversarial Training of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.04129v1
- **DOI**: 10.1109/TNNLS.2020.3025954
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.04129v1)
- **Published**: 2020-01-13 09:54:35+00:00
- **Updated**: 2020-01-13 09:54:35+00:00
- **Authors**: Antonio-Javier Gallego, Jorge Calvo-Zaragoza, Robert B. Fisher
- **Comment**: 26 pages, 7 figures
- **Journal**: IEEE Trans. Neural Networks Learn. Syst. 32(11): 4864-4878 (2021)
- **Summary**: In the context of supervised statistical learning, it is typically assumed that the training set comes from the same distribution that draws the test samples. When this is not the case, the behavior of the learned model is unpredictable and becomes dependent upon the degree of similarity between the distribution of the training set and the distribution of the test set. One of the research topics that investigates this scenario is referred to as domain adaptation. Deep neural networks brought dramatic advances in pattern recognition and that is why there have been many attempts to provide good domain adaptation algorithms for these models. Here we take a different avenue and approach the problem from an incremental point of view, where the model is adapted to the new domain iteratively. We make use of an existing unsupervised domain-adaptation algorithm to identify the target samples on which there is greater confidence about their true label. The output of the model is analyzed in different ways to determine the candidate samples. The selected set is then added to the source training set by considering the labels provided by the network as ground truth, and the process is repeated until all target samples are labelled. Our results report a clear improvement with respect to the non-incremental case in several datasets, also outperforming other state-of-the-art domain adaptation algorithms.



### Towards Interpretable and Robust Hand Detection via Pixel-wise Prediction
- **Arxiv ID**: http://arxiv.org/abs/2001.04163v1
- **DOI**: 10.1016/j.patcog.2020.107202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04163v1)
- **Published**: 2020-01-13 11:18:09+00:00
- **Updated**: 2020-01-13 11:18:09+00:00
- **Authors**: Dan Liu, Libo Zhang, Tiejian Luo, Lili Tao, Yanjun Wu
- **Comment**: Accepted to Pattern Recognition
- **Journal**: None
- **Summary**: The lack of interpretability of existing CNN-based hand detection methods makes it difficult to understand the rationale behind their predictions. In this paper, we propose a novel neural network model, which introduces interpretability into hand detection for the first time. The main improvements include: (1) Detect hands at pixel level to explain what pixels are the basis for its decision and improve transparency of the model. (2) The explainable Highlight Feature Fusion block highlights distinctive features among multiple layers and learns discriminative ones to gain robust performance. (3) We introduce a transparent representation, the rotation map, to learn rotation features instead of complex and non-transparent rotation and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10 hours in our experiments. Experimental results on the VIVA and Oxford hand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed.



### Separating Content from Style Using Adversarial Learning for Recognizing Text in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2001.04189v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04189v3)
- **Published**: 2020-01-13 12:41:42+00:00
- **Updated**: 2020-12-12 08:11:22+00:00
- **Authors**: Canjie Luo, Qingxiang Lin, Yuliang Liu, Lianwen Jin, Chunhua Shen
- **Comment**: Accepted to appear in International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: We propose to improve text recognition from a new perspective by separating the text content from complex backgrounds. As vanilla GANs are not sufficiently robust to generate sequence-like characters in natural images, we propose an adversarial learning framework for the generation and recognition of multiple characters in an image. The proposed framework consists of an attention-based recognizer and a generative adversarial architecture. Furthermore, to tackle the issue of lacking paired training samples, we design an interactive joint training scheme, which shares attention masks from the recognizer to the discriminator, and enables the discriminator to extract the features of each character for further adversarial training. Benefiting from the character-level adversarial training, our framework requires only unpaired simple data for style supervision. Each target style sample containing only one randomly chosen character can be simply synthesized online during the training. This is significant as the training does not require costly paired samples or character-level annotations. Thus, only the input images and corresponding text labels are needed. In addition to the style normalization of the backgrounds, we refine character patterns to ease the recognition task. A feedback mechanism is proposed to bridge the gap between the discriminator and the recognizer. Therefore, the discriminator can guide the generator according to the confusion of the recognizer, so that the generated patterns are clearer for recognition. Experiments on various benchmarks, including both regular and irregular text, demonstrate that our method significantly reduces the difficulty of recognition. Our framework can be integrated into recent recognition methods to achieve new state-of-the-art recognition accuracy.



### Deep Learning for Person Re-identification: A Survey and Outlook
- **Arxiv ID**: http://arxiv.org/abs/2001.04193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04193v2)
- **Published**: 2020-01-13 12:49:22+00:00
- **Updated**: 2021-01-06 03:17:49+00:00
- **Authors**: Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, Steven C. H. Hoi
- **Comment**: 20 pages, 8 figures. Accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for FOUR different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.



### Handwritten Character Recognition Using Unique Feature Extraction Technique
- **Arxiv ID**: http://arxiv.org/abs/2001.04208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04208v1)
- **Published**: 2020-01-13 13:06:06+00:00
- **Updated**: 2020-01-13 13:06:06+00:00
- **Authors**: Sai Abhishikth Ayyadevara, P N V Sai Ram Teja, Bharath K P, Rajesh Kumar M
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most arduous and captivating domains under image processing is handwritten character recognition. In this paper we have proposed a feature extraction technique which is a combination of unique features of geometric, zone-based hybrid, gradient features extraction approaches and three different neural networks namely the Multilayer Perceptron network using Backpropagation algorithm (MLP BP), the Multilayer Perceptron network using Levenberg-Marquardt algorithm (MLP LM) and the Convolutional neural network (CNN) which have been implemented along with the Minimum Distance Classifier (MDC). The procedures lead to the conclusion that the proposed feature extraction algorithm is more accurate than its individual counterparts and also that Convolutional Neural Network is the most efficient neural network of the three in consideration.



### Radial Based Analysis of GRNN in Non-Textured Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2001.04215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04215v1)
- **Published**: 2020-01-13 13:12:15+00:00
- **Updated**: 2020-01-13 13:12:15+00:00
- **Authors**: Karthik R, Anvita Dwivedi, Haripriya M, Bharath K P, Rajesh Kumar M
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting algorithms are used to restore some damaged or missing information region of an image based on the surrounding information. The method proposed in this paper applies the radial based analysis of image inpainting on GRNN. The damaged areas are first isolated from rest of the areas and then arranged by their size and then inpainted using GRNN. The training of the neural network is done using different radii to achieve a better outcome. A comparative analysis is done for different regression-based algorithms. The overall results are compared with the results achieved by the other algorithms as LS-SVM with reference to the PSNR value.



### Adversarial Loss for Semantic Segmentation of Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2001.04269v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04269v2)
- **Published**: 2020-01-13 14:22:54+00:00
- **Updated**: 2020-01-18 10:21:37+00:00
- **Authors**: Clint Sebastian, Raffaele Imbriaco, Egor Bondarev, Peter H. N. de With
- **Comment**: IEEE Symposium on Information Theory and Signal Processing in the
  Benelux (May 2019)
- **Journal**: None
- **Summary**: Automatic building extraction from aerial imagery has several applications in urban planning, disaster management, and change detection. In recent years, several works have adopted deep convolutional neural networks (CNNs) for building extraction, since they produce rich features that are invariant against lighting conditions, shadows, etc. Although several advances have been made, building extraction from aerial imagery still presents multiple challenges. Most of the deep learning segmentation methods optimize the per-pixel loss with respect to the ground truth without knowledge of the context. This often leads to imperfect outputs that may lead to missing or unrefined regions. In this work, we propose a novel loss function combining both adversarial and cross-entropy losses that learn to understand both local and global contexts for semantic segmentation. The newly proposed loss function deployed on the DeepLab v3+ network obtains state-of-the-art results on the Massachusetts buildings dataset. The loss function improves the structure and refines the edges of buildings without requiring any of the commonly used post-processing methods, such as Conditional Random Fields. We also perform ablation studies to understand the impact of the adversarial loss. Finally, the proposed method achieves a relaxed F1 score of 95.59% on the Massachusetts buildings dataset compared to the previous best F1 of 94.88%.



### Deep Image Translation with an Affinity-Based Change Prior for Unsupervised Multimodal Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.04271v2
- **DOI**: 10.1109/TGRS.2021.3056196
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.04271v2)
- **Published**: 2020-01-13 14:23:24+00:00
- **Updated**: 2021-03-08 13:57:53+00:00
- **Authors**: Luigi Tommaso Luppino, Michael Kampffmeyer, Filippo Maria Bianchi, Gabriele Moser, Sebastiano Bruno Serpico, Robert Jenssen, Stian Normann Anfinsen
- **Comment**: None
- **Journal**: None
- **Summary**: Image translation with convolutional neural networks has recently been used as an approach to multimodal change detection. Existing approaches train the networks by exploiting supervised information of the change areas, which, however, is not always available. A main challenge in the unsupervised problem setting is to avoid that change pixels affect the learning of the translation function. We propose two new network architectures trained with loss functions weighted by priors that reduce the impact of change pixels on the learning objective. The change prior is derived in an unsupervised fashion from relational pixel information captured by domain-specific affinity matrices. Specifically, we use the vertex degrees associated with an absolute affinity difference matrix and demonstrate their utility in combination with cycle consistency and adversarial training. The proposed neural networks are compared with state-of-the-art algorithms. Experiments conducted on three real datasets show the effectiveness of our methodology.



### High-Fidelity Synthesis with Disentangled Representation
- **Arxiv ID**: http://arxiv.org/abs/2001.04296v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04296v1)
- **Published**: 2020-01-13 14:39:40+00:00
- **Updated**: 2020-01-13 14:39:40+00:00
- **Authors**: Wonkwang Lee, Donggyun Kim, Seunghoon Hong, Honglak Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.



### Visually Guided Self Supervised Learning of Speech Representations
- **Arxiv ID**: http://arxiv.org/abs/2001.04316v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2001.04316v2)
- **Published**: 2020-01-13 14:53:22+00:00
- **Updated**: 2020-02-20 12:51:50+00:00
- **Authors**: Abhinav Shukla, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Maja Pantic
- **Comment**: Accepted at ICASSP 2020 v2: Updated to the ICASSP 2020 camera ready
  version
- **Journal**: None
- **Summary**: Self supervised representation learning has recently attracted a lot of research interest for both the audio and visual modalities. However, most works typically focus on a particular modality or feature alone and there has been very limited work that studies the interaction between the two modalities for learning self supervised representations. We propose a framework for learning audio representations guided by the visual modality in the context of audiovisual speech. We employ a generative audio-to-video training scheme in which we animate a still image corresponding to a given audio clip and optimize the generated video to be as close as possible to the real video of the speech segment. Through this process, the audio encoder network learns useful speech representations that we evaluate on emotion recognition and speech recognition. We achieve state of the art results for emotion recognition and competitive results for speech recognition. This demonstrates the potential of visual supervision for learning audio representations as a novel way for self-supervised learning which has not been explored in the past. The proposed unsupervised audio features can leverage a virtually unlimited amount of training data of unlabelled audiovisual speech and have a large number of potentially promising applications.



### Hierarchical Modeling of Multidimensional Data in Regularly Decomposed Spaces: Synthesis and Perspective
- **Arxiv ID**: http://arxiv.org/abs/2001.04322v1
- **DOI**: None
- **Categories**: **cs.CV**, E.1; I.4; I.5; I.6
- **Links**: [PDF](http://arxiv.org/pdf/2001.04322v1)
- **Published**: 2020-01-13 14:59:07+00:00
- **Updated**: 2020-01-13 14:59:07+00:00
- **Authors**: Olivier Guye
- **Comment**: 60 pages, 9 figures, research report
- **Journal**: None
- **Summary**: This fourth and last tome is focusing on describing the envisioned works for a project that has been presented in the preceding tome. It is about a new approach dedicated to the coding of still and moving pictures, trying to bridge the MPEG-4 and MPEG-7 standard bodies. The aim of this project is to define the principles of self-descriptive video coding. In order to establish them, the document is composed in five chapters that describe the various envisioned techniques for developing such a new approach in visual coding: - image segmentation, - computation of visual descriptors, - computation of perceptual groupings, - building of visual dictionaries, - picture and video coding. Based on the techniques of multiresolution computing, it is proposed to develop an image segmentation made from piecewise regular components, to compute attributes on the frame and the rendering of so produced shapes, independently to the geometric transforms that can occur in the image plane, and to gather them into perceptual groupings so as to be able in performing recognition of partially hidden patterns. Due to vector quantization of shapes frame and rendering, it will appear that simple shapes may be compared to a visual alphabet and that complex shapes then become words written using this alphabet and be recorded into a dictionary. With the help of a nearest neighbour scanning applied on the picture shapes, the self-descriptive coding will then generate a sentence made from words written using the simple shape alphabet.



### A Novel Inspection System For Variable Data Printing Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.04325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04325v1)
- **Published**: 2020-01-13 15:07:13+00:00
- **Updated**: 2020-01-13 15:07:13+00:00
- **Authors**: Oren Haik, Oded Perry, Eli Chen, Peter Klammer
- **Comment**: Accepted for publication in: Winter Applications of Computer Vision
  (WACV) 2020
- **Journal**: None
- **Summary**: We present a novel approach for inspecting variable data prints (VDP) with an ultra-low false alarm rate (0.005%) and potential applicability to other real-world problems. The system is based on a comparison between two images: a reference image and an image captured by low-cost scanners. The comparison task is challenging as low-cost imaging systems create artifacts that may erroneously be classified as true (genuine) defects. To address this challenge we introduce two new fusion methods, for change detection applications, which are both fast and efficient. The first is an early fusion method that combines the two input images into a single pseudo-color image. The second, called Change-Detection Single Shot Detector (CD-SSD) leverages the SSD by fusing features in the middle of the network. We demonstrate the effectiveness of the proposed deep learning-based approach with a large dataset from real-world printing scenarios. Finally, we evaluate our models on a different domain of aerial imagery change detection (AICD). Our best method clearly outperforms the state-of-the-art baseline on this dataset.



### Classifying All Interacting Pairs in a Single Shot
- **Arxiv ID**: http://arxiv.org/abs/2001.04360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04360v1)
- **Published**: 2020-01-13 15:51:45+00:00
- **Updated**: 2020-01-13 15:51:45+00:00
- **Authors**: Sanaa Chafik, Astrid Orcesi, Romaric Audigier, Bertrand Luvison
- **Comment**: WACV 2020 (to appear)
- **Journal**: None
- **Summary**: In this paper, we introduce a novel human interaction detection approach, based on CALIPSO (Classifying ALl Interacting Pairs in a Single shOt), a classifier of human-object interactions. This new single-shot interaction classifier estimates interactions simultaneously for all human-object pairs, regardless of their number and class. State-of-the-art approaches adopt a multi-shot strategy based on a pairwise estimate of interactions for a set of human-object candidate pairs, which leads to a complexity depending, at least, on the number of interactions or, at most, on the number of candidate pairs. In contrast, the proposed method estimates the interactions on the whole image. Indeed, it simultaneously estimates all interactions between all human subjects and object targets by performing a single forward pass throughout the image. Consequently, it leads to a constant complexity and computation time independent of the number of subjects, objects or interactions in the image. In detail, interaction classification is achieved on a dense grid of anchors thanks to a joint multi-task network that learns three complementary tasks simultaneously: (i) prediction of the types of interaction, (ii) estimation of the presence of a target and (iii) learning of an embedding which maps interacting subject and target to a same representation, by using a metric learning strategy. In addition, we introduce an object-centric passive-voice verb estimation which significantly improves results. Evaluations on the two well-known Human-Object Interaction image datasets, V-COCO and HICO-DET, demonstrate the competitiveness of the proposed method (2nd place) compared to the state-of-the-art while having constant computation time regardless of the number of objects and interactions in the image.



### RoutedFusion: Learning Real-time Depth Map Fusion
- **Arxiv ID**: http://arxiv.org/abs/2001.04388v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04388v2)
- **Published**: 2020-01-13 16:46:41+00:00
- **Updated**: 2020-04-03 09:15:29+00:00
- **Authors**: Silvan Weder, Johannes L. Sch√∂nberger, Marc Pollefeys, Martin R. Oswald
- **Comment**: 11 pages, 8 figures, accepted to CVPR 2020
- **Journal**: None
- **Summary**: The efficient fusion of depth maps is a key part of most state-of-the-art 3D reconstruction methods. Besides requiring high accuracy, these depth fusion methods need to be scalable and real-time capable. To this end, we present a novel real-time capable machine learning-based method for depth map fusion. Similar to the seminal depth map fusion approach by Curless and Levoy, we only update a local group of voxels to ensure real-time capability. Instead of a simple linear fusion of depth information, we propose a neural network that predicts non-linear updates to better account for typical fusion errors. Our network is composed of a 2D depth routing network and a 3D depth fusion network which efficiently handle sensor-specific noise and outliers. This is especially useful for surface edges and thin objects for which the original approach suffers from thickening artifacts. Our method outperforms the traditional fusion approach and related learned approaches on both synthetic and real data. We demonstrate the performance of our method in reconstructing fine geometric details from noise and outlier contaminated data on various scenes.



### Towards Automated Swimming Analytics Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.04433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04433v1)
- **Published**: 2020-01-13 18:06:53+00:00
- **Updated**: 2020-01-13 18:06:53+00:00
- **Authors**: Timothy Woinoski, Alon Harell, Ivan V. Bajic
- **Comment**: None
- **Journal**: None
- **Summary**: Methods for creating a system to automate the collection of swimming analytics on a pool-wide scale are considered in this paper. There has not been much work on swimmer tracking or the creation of a swimmer database for machine learning purposes. Consequently, methods for collecting swimmer data from videos of swim competitions are explored and analyzed. The result is a guide to the creation of a comprehensive collection of swimming data suitable for training swimmer detection and tracking systems. With this database in place, systems can then be created to automate the collection of swimming analytics.



### AttentionAnatomy: A unified framework for whole-body organs at risk segmentation using multiple partially annotated datasets
- **Arxiv ID**: http://arxiv.org/abs/2001.04446v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2001.04446v1)
- **Published**: 2020-01-13 18:31:34+00:00
- **Updated**: 2020-01-13 18:31:34+00:00
- **Authors**: Shanlin Sun, Yang Liu, Narisu Bai, Hao Tang, Xuming Chen, Qian Huang, Yong Liu, Xiaohui Xie
- **Comment**: accepted by ISBI 2020 (4 pages, 2 figures)
- **Journal**: None
- **Summary**: Organs-at-risk (OAR) delineation in computed tomography (CT) is an important step in Radiation Therapy (RT) planning. Recently, deep learning based methods for OAR delineation have been proposed and applied in clinical practice for separate regions of the human body (head and neck, thorax, and abdomen). However, there are few researches regarding the end-to-end whole-body OARs delineation because the existing datasets are mostly partially or incompletely annotated for such task. In this paper, our proposed end-to-end convolutional neural network model, called \textbf{AttentionAnatomy}, can be jointly trained with three partially annotated datasets, segmenting OARs from whole body. Our main contributions are: 1) an attention module implicitly guided by body region label to modulate the segmentation branch output; 2) a prediction re-calibration operation, exploiting prior information of the input images, to handle partial-annotation(HPA) problem; 3) a new hybrid loss function combining batch Dice loss and spatially balanced focal loss to alleviate the organ size imbalance problem. Experimental results of our proposed framework presented significant improvements in both S{\o}rensen-Dice coefficient (DSC) and 95\% Hausdorff distance compared to the baseline model.



### Unsupervised Audiovisual Synthesis via Exemplar Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2001.04463v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2001.04463v3)
- **Published**: 2020-01-13 18:56:45+00:00
- **Updated**: 2021-07-03 05:24:44+00:00
- **Authors**: Kangle Deng, Aayush Bansal, Deva Ramanan
- **Comment**: ICLR 2021; Project page -- https://www.cs.cmu.edu/~exemplar-ae/
- **Journal**: None
- **Summary**: We present an unsupervised approach that converts the input speech of any individual into audiovisual streams of potentially-infinitely many output speakers. Our approach builds on simple autoencoders that project out-of-sample data onto the distribution of the training set. We use Exemplar Autoencoders to learn the voice, stylistic prosody, and visual appearance of a specific target exemplar speech. In contrast to existing methods, the proposed approach can be easily extended to an arbitrarily large number of speakers and styles using only 3 minutes of target audio-video data, without requiring {\em any} training data for the input speaker. To do so, we learn audiovisual bottleneck representations that capture the structured linguistic content of speech. We outperform prior approaches on both audio and video synthesis, and provide extensive qualitative analysis on our project page -- https://www.cs.cmu.edu/~exemplar-ae/.



### Identifying Table Structure in Documents using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2001.05853v1)
- **Published**: 2020-01-13 20:42:40+00:00
- **Updated**: 2020-01-13 20:42:40+00:00
- **Authors**: Nataliya Le Vine, Claus Horn, Matthew Zeigenfuse, Mark Rowan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.01947
- **Journal**: None
- **Summary**: In many industries, as well as in academic research, information is primarily transmitted in the form of unstructured documents (this article, for example). Hierarchically-related data is rendered as tables, and extracting information from tables in such documents presents a significant challenge. Many existing methods take a bottom-up approach, first integrating lines into cells, then cells into rows or columns, and finally inferring a structure from the resulting 2-D layout. But such approaches neglect the available prior information relating to table structure, namely that the table is merely an arbitrary representation of a latent logical structure. We propose a top-down approach, first using a conditional generative adversarial network to map a table image into a standardised `skeleton' table form denoting approximate row and column borders without table content, then deriving latent table structure using xy-cut projection and Genetic Algorithm optimisation. The approach is easily adaptable to different table configurations and requires small data set sizes for training.



### Rethinking Curriculum Learning with Incremental Labels and Adaptive Compensation
- **Arxiv ID**: http://arxiv.org/abs/2001.04529v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2001.04529v3)
- **Published**: 2020-01-13 21:00:46+00:00
- **Updated**: 2020-08-13 16:00:02+00:00
- **Authors**: Madan Ravi Ganesh, Jason J. Corso
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Like humans, deep networks have been shown to learn better when samples are organized and introduced in a meaningful order or curriculum. Conventional curriculum learning schemes introduce samples in their order of difficulty. This forces models to begin learning from a subset of the available data while adding the external overhead of evaluating the difficulty of samples. In this work, we propose Learning with Incremental Labels and Adaptive Compensation (LILAC), a two-phase method that incrementally increases the number of unique output labels rather than the difficulty of samples while consistently using the entire dataset throughout training. In the first phase, Incremental Label Introduction, we partition data into mutually exclusive subsets, one that contains a subset of the ground-truth labels and another that contains the remaining data attached to a pseudo-label. Throughout the training process, we recursively reveal unseen ground-truth labels in fixed increments until all the labels are known to the model. In the second phase, Adaptive Compensation, we optimize the loss function using altered target vectors of previously misclassified samples. The target vectors of such samples are modified to a smoother distribution to help models learn better. On evaluating across three standard image benchmarks, CIFAR-10, CIFAR-100, and STL-10, we show that LILAC outperforms all comparable baselines. Further, we detail the importance of pacing the introduction of new labels to a model as well as the impact of using a smooth target vector.



### Deep convolutional neural networks for multi-planar lung nodule detection: improvement in small nodule identification
- **Arxiv ID**: http://arxiv.org/abs/2001.04537v3
- **DOI**: 10.1002/mp.14648
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04537v3)
- **Published**: 2020-01-13 21:37:33+00:00
- **Updated**: 2020-12-09 20:40:29+00:00
- **Authors**: Sunyi Zheng, Ludo J. Cornelissen, Xiaonan Cui, Xueping Jing, Raymond N. J. Veldhuis, Matthijs Oudkerk, Peter M. A. van Ooijen
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: In clinical practice, small lung nodules can be easily overlooked by radiologists. The paper aims to provide an efficient and accurate detection system for small lung nodules while keeping good performance for large nodules. Methods: We propose a multi-planar detection system using convolutional neural networks. The 2-D convolutional neural network model, U-net++, was trained by axial, coronal, and sagittal slices for the candidate detection task. All possible nodule candidates from the three different planes are combined. For false positive reduction, we apply 3-D multi-scale dense convolutional neural networks to efficiently remove false positive candidates. We use the public LIDC-IDRI dataset which includes 888 CT scans with 1186 nodules annotated by four radiologists. Results: After ten-fold cross-validation, our proposed system achieves a sensitivity of 94.2% with 1.0 false positive/scan and a sensitivity of 96.0% with 2.0 false positives/scan. Although it is difficult to detect small nodules (i.e. < 6 mm), our designed CAD system reaches a sensitivity of 93.4% (95.0%) of these small nodules at an overall false positive rate of 1.0 (2.0) false positives/scan. At the nodule candidate detection stage, results show that a multi-planar method is capable to detect more nodules compared to using a single plane. Conclusion: Our approach achieves good performance not only for small nodules, but also for large lesions on this dataset. This demonstrates the effectiveness and efficiency of our developed CAD system for lung nodule detection. Significance: The proposed system could provide support for radiologists on early detection of lung cancer.



### Visual Storytelling via Predicting Anchor Word Embeddings in the Stories
- **Arxiv ID**: http://arxiv.org/abs/2001.04541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04541v1)
- **Published**: 2020-01-13 21:47:12+00:00
- **Updated**: 2020-01-13 21:47:12+00:00
- **Authors**: Bowen Zhang, Hexiang Hu, Fei Sha
- **Comment**: Accepted by ICCV19 CLVL workshop: 3rd Workshop on Closing the Loop
  Between Vision and Language
- **Journal**: None
- **Summary**: We propose a learning model for the task of visual storytelling. The main idea is to predict anchor word embeddings from the images and use the embeddings and the image features jointly to generate narrative sentences. We use the embeddings of randomly sampled nouns from the groundtruth stories as the target anchor word embeddings to learn the predictor. To narrate a sequence of images, we use the predicted anchor word embeddings and the image features as the joint input to a seq2seq model. As opposed to state-of-the-art methods, the proposed model is simple in design, easy to optimize, and attains the best results in most automatic evaluation metrics. In human evaluation, the method also outperforms competing methods.



### Learning Transformation-Aware Embeddings for Image Forensics
- **Arxiv ID**: http://arxiv.org/abs/2001.04547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04547v1)
- **Published**: 2020-01-13 22:01:24+00:00
- **Updated**: 2020-01-13 22:01:24+00:00
- **Authors**: Aparna Bharati, Daniel Moreira, Patrick Flynn, Anderson Rocha, Kevin Bowyer, Walter Scheirer
- **Comment**: Supplemental material for this paper is available at
  https://drive.google.com/file/d/1covDhaTN24zkmyQf1XCTZHNrUZdZqGyo/view?usp=sharing
- **Journal**: None
- **Summary**: A dramatic rise in the flow of manipulated image content on the Internet has led to an aggressive response from the media forensics research community. New efforts have incorporated increased usage of techniques from computer vision and machine learning to detect and profile the space of image manipulations. This paper addresses Image Provenance Analysis, which aims at discovering relationships among different manipulated image versions that share content. One of the main sub-problems for provenance analysis that has not yet been addressed directly is the edit ordering of images that share full content or are near-duplicates. The existing large networks that generate image descriptors for tasks such as object recognition may not encode the subtle differences between these image covariates. This paper introduces a novel deep learning-based approach to provide a plausible ordering to images that have been generated from a single image through transformations. Our approach learns transformation-aware descriptors using weak supervision via composited transformations and a rank-based quadruplet loss. To establish the efficacy of the proposed approach, comparisons with state-of-the-art handcrafted and deep learning-based descriptors, and image matching approaches are made. Further experimentation validates the proposed approach in the context of image provenance analysis.



### Deep Learning Stereo Vision at the edge
- **Arxiv ID**: http://arxiv.org/abs/2001.04552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04552v1)
- **Published**: 2020-01-13 22:30:41+00:00
- **Updated**: 2020-01-13 22:30:41+00:00
- **Authors**: Luca Puglia, Cormac Brick
- **Comment**: None
- **Journal**: None
- **Summary**: We present an overview of the methodology used to build a new stereo vision solution that is suitable for System on Chip. This new solution was developed to bring computer vision capability to embedded devices that live in a power constrained environment. The solution is constructured as a hybrid between classical Stereo Vision techniques and deep learning approaches. The stereoscopic module is composed of two separate modules: one that accelerates the neural network we trained and one that accelerates the front-end part. The system is completely passive and does not require any structured light to obtain very compelling accuracy. With respect to the previous Stereo Vision solutions offered by the industries we offer a major improvement is robustness to noise. This is mainly possible due to the deep learning part of the chosen architecture. We submitted our result to Middlebury dataset challenge. It currently ranks as the best System on Chip solution. The system has been developed for low latency applications which require better than real time performance on high definition videos.



### Boosting Deep Face Recognition via Disentangling Appearance and Geometry
- **Arxiv ID**: http://arxiv.org/abs/2001.04559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04559v1)
- **Published**: 2020-01-13 23:19:58+00:00
- **Updated**: 2020-01-13 23:19:58+00:00
- **Authors**: Ali Dabouei, Fariborz Taherkhani, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.



### 180-degree Outpainting from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2001.04568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2001.04568v1)
- **Published**: 2020-01-13 23:50:34+00:00
- **Updated**: 2020-01-13 23:50:34+00:00
- **Authors**: Zhenqiang Ying, Alan Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Presenting context images to a viewer's peripheral vision is one of the most effective techniques to enhance immersive visual experiences. However, most images only present a narrow view, since the field-of-view (FoV) of standard cameras is small. To overcome this limitation, we propose a deep learning approach that learns to predict a 180{\deg} panoramic image from a narrow-view image. Specifically, we design a foveated framework that applies different strategies on near-periphery and mid-periphery regions. Two networks are trained separately, and then are employed jointly to sequentially perform narrow-to-90{\deg} generation and 90{\deg}-to-180{\deg} generation. The generated outputs are then fused with their aligned inputs to produce expanded equirectangular images for viewing. Our experimental results show that single-view-to-panoramic image generation using deep learning is both feasible and promising.




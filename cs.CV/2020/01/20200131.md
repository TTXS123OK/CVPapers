# Arxiv Papers in cs.CV on 2020-01-31
### ParkingSticker: A Real-World Object Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/2001.11639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11639v2)
- **Published**: 2020-01-31 03:01:20+00:00
- **Updated**: 2020-02-12 15:07:29+00:00
- **Authors**: Caroline Potts, Ethem F. Can, Aysu Ezen-Can, Xiangqian Hu
- **Comment**: 8 pages, 8 figures; Updated authors
- **Journal**: None
- **Summary**: We present a new and challenging object detection dataset, ParkingSticker, which mimics the type of data available in industry problems more closely than popular existing datasets like PASCAL VOC. ParkingSticker contains 1,871 images that come from a security camera's video footage. The objective is to identify parking stickers on cars approaching a gate that the security camera faces. Bounding boxes are drawn around parking stickers in the images. The parking stickers are much smaller on average than the objects in other popular object detection datasets; this makes ParkingSticker a challenging test for object detection methods. This dataset also very realistically represents the data available in many industry problems where a customer presents a few video frames and asks for a solution to a very difficult problem. Performance of various object detection pipelines using a YOLOv2 architecture are presented and indicate that identifying the parking stickers in ParkingSticker is challenging yet feasible. We believe that this dataset will challenge researchers to solve a real-world problem with real-world constraints such as non-ideal camera positioning and small object-size-to-image-size ratios.



### Convolutional Hierarchical Attention Network for Query-Focused Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2002.03740v3
- **DOI**: 10.1109/TIP.2020.2985868
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03740v3)
- **Published**: 2020-01-31 04:30:14+00:00
- **Updated**: 2020-02-15 03:26:30+00:00
- **Authors**: Shuwen Xiao, Zhou Zhao, Zijian Zhang, Xiaohui Yan, Min Yang
- **Comment**: Accepted by AAAI 2020 Conference
- **Journal**: None
- **Summary**: Previous approaches for video summarization mainly concentrate on finding the most diverse and representative visual contents as video summary without considering the user's preference. This paper addresses the task of query-focused video summarization, which takes user's query and a long video as inputs and aims to generate a query-focused video summary. In this paper, we consider the task as a problem of computing similarity between video shots and query. To this end, we propose a method, named Convolutional Hierarchical Attention Network (CHAN), which consists of two parts: feature encoding network and query-relevance computing module. In the encoding network, we employ a convolutional network with local self-attention mechanism and query-aware global attention mechanism to learns visual information of each shot. The encoded features will be sent to query-relevance computing module to generate queryfocused video summary. Extensive experiments on the benchmark dataset demonstrate the competitive performance and show the effectiveness of our approach.



### Modality Compensation Network: Cross-Modal Adaptation for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.11657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11657v1)
- **Published**: 2020-01-31 04:51:55+00:00
- **Updated**: 2020-01-31 04:51:55+00:00
- **Authors**: Sijie Song, Jiaying Liu, Yanghao Li, Zongming Guo
- **Comment**: Accepted by IEEE Trans. on Image Processing, 2020. Project page:
  http://39.96.165.147/Projects/MCN_tip2020_ssj/MCN_tip_2020_ssj.html
- **Journal**: None
- **Summary**: With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.



### Symmetrical Synthesis for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.11658v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2001.11658v3)
- **Published**: 2020-01-31 04:56:47+00:00
- **Updated**: 2020-04-23 06:17:18+00:00
- **Authors**: Geonmo Gu, Byungsoo Ko
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Deep metric learning aims to learn embeddings that contain semantic similarity information among data points. To learn better embeddings, methods to generate synthetic hard samples have been proposed. Existing methods of synthetic hard sample generation are adopting autoencoders or generative adversarial networks, but this leads to more hyper-parameters, harder optimization, and slower training speed. In this paper, we address these problems by proposing a novel method of synthetic hard sample generation called symmetrical synthesis. Given two original feature points from the same class, the proposed method firstly generates synthetic points with each other as an axis of symmetry. Secondly, it performs hard negative pair mining within the original and synthetic points to select a more informative negative pair for computing the metric learning loss. Our proposed method is hyper-parameter free and plug-and-play for existing metric learning losses without network modification. We demonstrate the superiority of our proposed method over existing methods for a variety of loss functions on clustering and image retrieval tasks. Our implementations is publicly available.



### Augmenting Visual Question Answering with Semantic Frame Information in a Multitask Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2001.11673v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11673v1)
- **Published**: 2020-01-31 06:31:39+00:00
- **Updated**: 2020-01-31 06:31:39+00:00
- **Authors**: Mehrdad Alizadeh, Barbara Di Eugenio
- **Comment**: 14th IEEE International Conference on SEMANTIC COMPUTING, 8 Pages,
  February 2020, San Diego CA USA
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) concerns providing answers to Natural Language questions about images. Several deep neural network approaches have been proposed to model the task in an end-to-end fashion. Whereas the task is grounded in visual processing, if the question focuses on events described by verbs, the language understanding component becomes crucial. Our hypothesis is that models should be aware of verb semantics, as expressed via semantic role labels, argument types, and/or frame elements. Unfortunately, no VQA dataset exists that includes verb semantic information. Our first contribution is a new VQA dataset (imSituVQA) that we built by taking advantage of the imSitu annotations. The imSitu dataset consists of images manually labeled with semantic frame elements, mostly taken from FrameNet. Second, we propose a multitask CNN-LSTM VQA model that learns to classify the answers as well as the semantic frame elements. Our experiments show that semantic frame element classification helps the VQA system avoid inconsistent responses and improves performance.



### Robot Navigation in Unseen Spaces using an Abstract Map
- **Arxiv ID**: http://arxiv.org/abs/2001.11684v2
- **DOI**: 10.1109/TCDS.2020.2993855
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11684v2)
- **Published**: 2020-01-31 07:40:44+00:00
- **Updated**: 2020-05-15 05:31:34+00:00
- **Authors**: Ben Talbot, Feras Dayoub, Peter Corke, Gordon Wyeth
- **Comment**: 15 pages, published in IEEE Transactions on Cognitive and
  Developmental Systems (http://doi.org/10.1109/TCDS.2020.2993855), see
  https://btalb.github.io/abstract_map/ for access to software
- **Journal**: None
- **Summary**: Human navigation in built environments depends on symbolic spatial information which has unrealised potential to enhance robot navigation capabilities. Information sources such as labels, signs, maps, planners, spoken directions, and navigational gestures communicate a wealth of spatial information to the navigators of built environments; a wealth of information that robots typically ignore. We present a robot navigation system that uses the same symbolic spatial information employed by humans to purposefully navigate in unseen built environments with a level of performance comparable to humans. The navigation system uses a novel data structure called the abstract map to imagine malleable spatial models for unseen spaces from spatial symbols. Sensorimotor perceptions from a robot are then employed to provide purposeful navigation to symbolic goal locations in the unseen environment. We show how a dynamic system can be used to create malleable spatial models for the abstract map, and provide an open source implementation to encourage future work in the area of symbolic navigation. Symbolic navigation performance of humans and a robot is evaluated in a real-world built environment. The paper concludes with a qualitative analysis of human navigation strategies, providing further insights into how the symbolic navigation capabilities of robots in unseen built environments can be improved in the future.



### C-DLinkNet: considering multi-level semantic features for human parsing
- **Arxiv ID**: http://arxiv.org/abs/2001.11690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11690v2)
- **Published**: 2020-01-31 07:47:45+00:00
- **Updated**: 2020-04-05 11:12:12+00:00
- **Authors**: Yu Lu, Muyan Feng, Ming Wu, Chuang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Human parsing is an essential branch of semantic segmentation, which is a fine-grained semantic segmentation task to identify the constituent parts of human. The challenge of human parsing is to extract effective semantic features to resolve deformation and multi-scale variations. In this work, we proposed an end-to-end model called C-DLinkNet based on LinkNet, which contains a new module named Smooth Module to combine the multi-level features in Decoder part. C-DLinkNet is capable of producing competitive parsing performance compared with the state-of-the-art methods with smaller input sizes and no additional information, i.e., achiving mIoU=53.05 on the validation set of LIP dataset.



### Inter-slice image augmentation based on frame interpolation for boosting medical image segmentation accuracy
- **Arxiv ID**: http://arxiv.org/abs/2001.11698v1
- **DOI**: 10.3233/FAIA200314
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11698v1)
- **Published**: 2020-01-31 08:12:46+00:00
- **Updated**: 2020-01-31 08:12:46+00:00
- **Authors**: Zhaotao Wu, Jia Wei, Wenguang Yuan, Jiabing Wang, Tolga Tasdizen
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the idea of inter-slice image augmentation whereby the numbers of the medical images and the corresponding segmentation labels are increased between two consecutive images in order to boost medical image segmentation accuracy. Unlike conventional data augmentation methods in medical imaging, which only increase the number of training samples directly by adding new virtual samples using simple parameterized transformations such as rotation, flipping, scaling, etc., we aim to augment data based on the relationship between two consecutive images, which increases not only the number but also the information of training samples. For this purpose, we propose a frame-interpolation-based data augmentation method to generate intermediate medical images and the corresponding segmentation labels between two consecutive images. We train and test a supervised U-Net liver segmentation network on SLIVER07 and CHAOS2019, respectively, with the augmented training samples, and obtain segmentation scores exhibiting significant improvement compared to the conventional augmentation methods.



### Generalized Visual Information Analysis via Tensorial Algebra
- **Arxiv ID**: http://arxiv.org/abs/2001.11708v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.AC, math.RA
- **Links**: [PDF](http://arxiv.org/pdf/2001.11708v2)
- **Published**: 2020-01-31 08:47:35+00:00
- **Updated**: 2021-01-17 10:58:56+00:00
- **Authors**: Liang Liao, Stephen John Maybank
- **Comment**: 42 pages, 17 figures
- **Journal**: None
- **Summary**: Higher order data is modeled using matrices whose entries are numerical arrays of a fixed size. These arrays, called t-scalars, form a commutative ring under the convolution product. Matrices with elements in the ring of t-scalars are referred to as t-matrices. The t-matrices can be scaled, added and multiplied in the usual way. There are t-matrix generalizations of positive matrices, orthogonal matrices and Hermitian symmetric matrices. With the t-matrix model, it is possible to generalize many well-known matrix algorithms. In particular, the t-matrices are used to generalize the SVD (Singular Value Decomposition), HOSVD (High Order SVD), PCA (Principal Component Analysis), 2DPCA (Two Dimensional PCA) and GCA (Grassmannian Component Analysis). The generalized t-matrix algorithms, namely TSVD, THOSVD,TPCA, T2DPCA and TGCA, are applied to low-rank approximation, reconstruction,and supervised classification of images. Experiments show that the t-matrix algorithms compare favorably with standard matrix algorithms.



### Automated quantification of myocardial tissue characteristics from native T1 mapping using neural networks with Bayesian inference for uncertainty-based quality-control
- **Arxiv ID**: http://arxiv.org/abs/2001.11711v1
- **DOI**: 10.1186/s12968-020-00650-y
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2001.11711v1)
- **Published**: 2020-01-31 08:51:33+00:00
- **Updated**: 2020-01-31 08:51:33+00:00
- **Authors**: Esther Puyol Anton, Bram Ruijsink, Christian F. Baumgartner, Matthew Sinclair, Ender Konukoglu, Reza Razavi, Andrew P. King
- **Comment**: None
- **Journal**: None
- **Summary**: Tissue characterisation with CMR parametric mapping has the potential to detect and quantify both focal and diffuse alterations in myocardial structure not assessable by late gadolinium enhancement. Native T1 mapping in particular has shown promise as a useful biomarker to support diagnostic, therapeutic and prognostic decision-making in ischaemic and non-ischaemic cardiomyopathies. Convolutional neural networks with Bayesian inference are a category of artificial neural networks which model the uncertainty of the network output. This study presents an automated framework for tissue characterisation from native ShMOLLI T1 mapping at 1.5T using a Probabilistic Hierarchical Segmentation (PHiSeg) network. In addition, we use the uncertainty information provided by the PHiSeg network in a novel automated quality control (QC) step to identify uncertain T1 values. The PHiSeg network and QC were validated against manual analysis on a cohort of the UK Biobank containing healthy subjects and chronic cardiomyopathy patients. We used the proposed method to obtain reference T1 ranges for the left ventricular myocardium in healthy subjects as well as common clinical cardiac conditions. T1 values computed from automatic and manual segmentations were highly correlated (r=0.97). Bland-Altman analysis showed good agreement between the automated and manual measurements. The average Dice metric was 0.84 for the left ventricular myocardium. The sensitivity of detection of erroneous outputs was 91%. Finally, T1 values were automatically derived from 14,683 CMR exams from the UK Biobank. The proposed pipeline allows for automatic analysis of myocardial native T1 mapping and includes a QC process to detect potentially erroneous results. T1 reference values were presented for healthy subjects and common clinical cardiac conditions from the largest cohort to date using T1-mapping images.



### A Generative Adversarial Network for AI-Aided Chair Design
- **Arxiv ID**: http://arxiv.org/abs/2001.11715v1
- **DOI**: 10.1109/MIPR.2019.00098
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11715v1)
- **Published**: 2020-01-31 08:57:32+00:00
- **Updated**: 2020-01-31 08:57:32+00:00
- **Authors**: Zhibo Liu, Feng Gao, Yizhou Wang
- **Comment**: 6 pages, 5 figures, accepted at MIPR2019
- **Journal**: None
- **Summary**: We present a method for improving human design of chairs. The goal of the method is generating enormous chair candidates in order to facilitate human designer by creating sketches and 3d models accordingly based on the generated chair design. It consists of an image synthesis module, which learns the underlying distribution of training dataset, a super-resolution module, which improve quality of generated image and human involvements. Finally, we manually pick one of the generated candidates to create a real life chair for illustration.



### AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2001.11737v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.11737v2)
- **Published**: 2020-01-31 09:45:12+00:00
- **Updated**: 2020-02-03 07:04:25+00:00
- **Authors**: Ilker Bozcan, Erdal Kayacan
- **Comment**: 7 pages, 8 figures, 3 tables; for the associated dataset, see
  http://bozcani.github.io/auairdataset accepted to ICRA 2020
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images. As a result of this, several aerial datasets have been introduced, including visual data with object annotations. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors). In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for traffic-related object category) from recorded RGB videos. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics. The dataset is available at https://bozcani.github.io/auairdataset.



### Localizing Interpretable Multi-scale informative Patches Derived from Media Classification Task
- **Arxiv ID**: http://arxiv.org/abs/2002.03737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03737v2)
- **Published**: 2020-01-31 10:04:18+00:00
- **Updated**: 2020-04-17 08:14:52+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Xiaolong Hu, Hui Zhu, Yongjun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) always depend on wider receptive field (RF) and more complex non-linearity to achieve state-of-the-art performance, while suffering the increased difficult to interpret how relevant patches contribute the final prediction. In this paper, we construct an interpretable AnchorNet equipped with our carefully designed RFs and linearly spatial aggregation to provide patch-wise interpretability of the input media meanwhile localizing multi-scale informative patches only supervised on media-level labels without any extra bounding box annotations. Visualization of localized informative image and text patches show the superior multi-scale localization capability of AnchorNet. We further use localized patches for downstream classification tasks across widely applied networks. Experimental results demonstrate that replacing the original inputs with their patches for classification can get a clear inference acceleration with only tiny performance degradation, which proves that localized patches can indeed retain the most semantics and evidences of the original inputs.



### A memory of motion for visual predictive control tasks
- **Arxiv ID**: http://arxiv.org/abs/2001.11759v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11759v3)
- **Published**: 2020-01-31 10:45:04+00:00
- **Updated**: 2020-05-07 09:51:18+00:00
- **Authors**: Antonio Paolillo, Teguh Santoso Lembono, Sylvain Calinon
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: This paper addresses the problem of efficiently achieving visual predictive control tasks. To this end, a memory of motion, containing a set of trajectories built off-line, is used for leveraging precomputation and dealing with difficult visual tasks. Standard regression techniques, such as k-nearest neighbors and Gaussian process regression, are used to query the memory and provide on-line a warm-start and a way point to the control optimization process. The proposed technique allows the control scheme to achieve high performance and, at the same time, keep the computational time limited. Simulation and experimental results, carried out with a 7-axis manipulator, show the effectiveness of the approach.



### Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN
- **Arxiv ID**: http://arxiv.org/abs/2001.11761v3
- **DOI**: 10.1109/IJCNN48605.2020.9206960
- **Categories**: **cs.CV**, cs.HC, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2001.11761v3)
- **Published**: 2020-01-31 10:46:59+00:00
- **Updated**: 2020-12-07 20:15:46+00:00
- **Authors**: Milad Mozafari, Leila Reddy, Rufin VanRullen
- **Comment**: Accepted to IEEE IJCNN2020
- **Journal**: None
- **Summary**: Decoding and reconstructing images from brain imaging data is a research area of high interest. Recent progress in deep generative neural networks has introduced new opportunities to tackle this problem. Here, we employ a recently proposed large-scale bi-directional generative adversarial network, called BigBiGAN, to decode and reconstruct natural scenes from fMRI patterns. BigBiGAN converts images into a 120-dimensional latent space which encodes class and attribute information together, and can also reconstruct images based on their latent vectors. We computed a linear mapping between fMRI data, acquired over images from 150 different categories of ImageNet, and their corresponding BigBiGAN latent vectors. Then, we applied this mapping to the fMRI activity patterns obtained from 50 new test images from 50 unseen categories in order to retrieve their latent vectors, and reconstruct the corresponding images. Pairwise image decoding from the predicted latent vectors was highly accurate (84%). Moreover, qualitative and quantitative assessments revealed that the resulting image reconstructions were visually plausible, successfully captured many attributes of the original images, and had high perceptual similarity with the original content. This method establishes a new state-of-the-art for fMRI-based natural image reconstruction, and can be flexibly updated to take into account any future improvements in generative models of natural scene images.



### Unsupervised deep clustering for predictive texture pattern discovery in medical images
- **Arxiv ID**: http://arxiv.org/abs/2002.03721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03721v1)
- **Published**: 2020-01-31 10:57:59+00:00
- **Updated**: 2020-01-31 10:57:59+00:00
- **Authors**: Matthias Perkonigg, Daniel Sobotka, Ahmed Ba-Ssalamah, Georg Langs
- **Comment**: Medical Imaging meets NeurIPS 2019
- **Journal**: None
- **Summary**: Predictive marker patterns in imaging data are a means to quantify disease and progression, but their identification is challenging, if the underlying biology is poorly understood. Here, we present a method to identify predictive texture patterns in medical images in an unsupervised way. Based on deep clustering networks, we simultaneously encode and cluster medical image patches in a low-dimensional latent space. The resulting clusters serve as features for disease staging, linking them to the underlying disease. We evaluate the method on 70 T1-weighted magnetic resonance images of patients with different stages of liver steatosis. The deep clustering approach is able to find predictive clusters with a stable ranking, differentiating between low and high steatosis with an F1-Score of 0.78.



### Automatic lung segmentation in routine imaging is primarily a data diversity problem, not a methodology problem
- **Arxiv ID**: http://arxiv.org/abs/2001.11767v2
- **DOI**: 10.1186/s41747-020-00173-2
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, stat.ML, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2001.11767v2)
- **Published**: 2020-01-31 11:01:35+00:00
- **Updated**: 2020-08-20 15:02:26+00:00
- **Authors**: Johannes Hofmanninger, Florian Prayer, Jeanny Pan, Sebastian Rohrich, Helmut Prosch, Georg Langs
- **Comment**: 10 pages, 5 figures, 5 tables
- **Journal**: Eur Radiol Exp 4, 50 (2020)
- **Summary**: Automated segmentation of anatomical structures is a crucial step in image analysis. For lung segmentation in computed tomography, a variety of approaches exist, involving sophisticated pipelines trained and validated on different datasets. However, the clinical applicability of these approaches across diseases remains limited. We compared four generic deep learning approaches trained on various datasets and two readily available lung segmentation algorithms. We performed evaluation on routine imaging data with more than six different disease patterns and three published data sets. Using different deep learning approaches, mean Dice similarity coefficients (DSCs) on test datasets varied not over 0.02. When trained on a diverse routine dataset (n = 36) a standard approach (U-net) yields a higher DSC (0.97 $\pm$ 0.05) compared to training on public datasets such as Lung Tissue Research Consortium (0.94 $\pm$ 0.13, p = 0.024) or Anatomy 3 (0.92 $\pm$ 0.15, p = 0.001). Trained on routine data (n = 231) covering multiple diseases, U-net compared to reference methods yields a DSC of 0.98 $\pm$ 0.03 versus 0.94 $\pm$ 0.12 (p = 0.024).



### Universal Semantic Segmentation for Fisheye Urban Driving Images
- **Arxiv ID**: http://arxiv.org/abs/2002.03736v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03736v2)
- **Published**: 2020-01-31 11:19:00+00:00
- **Updated**: 2020-08-24 13:02:09+00:00
- **Authors**: Yaozu Ye, Kailun Yang, Kaite Xiang, Juan Wang, Kaiwei Wang
- **Comment**: SMC2020 recieved
- **Journal**: None
- **Summary**: Semantic segmentation is a critical method in the field of autonomous driving. When performing semantic image segmentation, a wider field of view (FoV) helps to obtain more information about the surrounding environment, making automatic driving safer and more reliable, which could be offered by fisheye cameras. However, large public fisheye datasets are not available, and the fisheye images captured by the fisheye camera with large FoV comes with large distortion, so commonly-used semantic segmentation model cannot be directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation method is proposed to transform rectilinear image to fisheye image in a more comprehensive way. In the training process, rectilinear images are transformed into fisheye images in seven DoF, which simulates the fisheye images taken by cameras of different positions, orientations and focal lengths. The result shows that training with the seven-DoF augmentation can improve the model's accuracy and robustness against different distorted fisheye data. This seven-DoF augmentation provides a universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. Also, we provide specific parameter settings of the augmentation for autonomous driving. At last, we tested our universal semantic segmentation model on real fisheye images and obtained satisfactory results. The code and configurations are released at https://github.com/Yaozhuwa/FisheyeSeg.



### iCap: Interactive Image Captioning with Predictive Text
- **Arxiv ID**: http://arxiv.org/abs/2001.11782v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11782v3)
- **Published**: 2020-01-31 11:33:12+00:00
- **Updated**: 2020-02-22 04:15:55+00:00
- **Authors**: Zhengxiong Jia, Xirong Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we study a brand new topic of interactive image captioning with human in the loop. Different from automated image captioning where a given test image is the sole input in the inference stage, we have access to both the test image and a sequence of (incomplete) user-input sentences in the interactive scenario. We formulate the problem as Visually Conditioned Sentence Completion (VCSC). For VCSC, we propose asynchronous bidirectional decoding for image caption completion (ABD-Cap). With ABD-Cap as the core module, we build iCap, a web-based interactive image captioning system capable of predicting new text with respect to live input from a user. A number of experiments covering both automated evaluations and real user studies show the viability of our proposals.



### Noise2Inverse: Self-supervised deep convolutional denoising for tomography
- **Arxiv ID**: http://arxiv.org/abs/2001.11801v3
- **DOI**: 10.1109/TCI.2020.3019647
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11801v3)
- **Published**: 2020-01-31 12:50:24+00:00
- **Updated**: 2020-09-15 08:27:07+00:00
- **Authors**: Allard A. Hendriksen, Daniel M. Pelt, K. Joost Batenburg
- **Comment**: This paper appears in: IEEE Transactions on Computational Imaging On
  page(s): 1320-1335 Print ISSN: 2333-9403 Online ISSN: 2333-9403 Digital
  Object Identifier: 10.1109/TCI.2020.3019647
- **Journal**: None
- **Summary**: Recovering a high-quality image from noisy indirect measurements is an important problem with many applications. For such inverse problems, supervised deep convolutional neural network (CNN)-based denoising methods have shown strong results, but the success of these supervised methods critically depends on the availability of a high-quality training dataset of similar measurements. For image denoising, methods are available that enable training without a separate training dataset by assuming that the noise in two different pixels is uncorrelated. However, this assumption does not hold for inverse problems, resulting in artifacts in the denoised images produced by existing methods. Here, we propose Noise2Inverse, a deep CNN-based denoising method for linear image reconstruction algorithms that does not require any additional clean or noisy data. Training a CNN-based denoiser is enabled by exploiting the noise model to compute multiple statistically independent reconstructions. We develop a theoretical framework which shows that such training indeed obtains a denoising CNN, assuming the measured noise is element-wise independent and zero-mean. On simulated CT datasets, Noise2Inverse demonstrates an improvement in peak signal-to-noise ratio and structural similarity index compared to state-of-the-art image denoising methods and conventional reconstruction methods, such as Total-Variation Minimization. We also demonstrate that the method is able to significantly reduce noise in challenging real-world experimental datasets.



### Lossless Attention in Convolutional Networks for Facial Expression Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2001.11869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11869v1)
- **Published**: 2020-01-31 14:38:35+00:00
- **Updated**: 2020-01-31 14:38:35+00:00
- **Authors**: Chuang Wang, Ruimin Hu, Min Hu, Jiang Liu, Ting Ren, Shan He, Ming Jiang, Jing Miao
- **Comment**: 5 pages,4 figures
- **Journal**: None
- **Summary**: Unlike the constraint frontal face condition, faces in the wild have various unconstrained interference factors, such as complex illumination, changing perspective and various occlusions. Facial expressions recognition (FER) in the wild is a challenging task and existing methods can't perform well. However, for occluded faces (containing occlusion caused by other objects and self-occlusion caused by head posture changes), the attention mechanism has the ability to focus on the non-occluded regions automatically. In this paper, we propose a Lossless Attention Model (LLAM) for convolutional neural networks (CNN) to extract attention-aware features from faces. Our module avoids decay information in the process of generating attention maps by using the information of the previous layer and not reducing the dimensionality. Sequentially, we adaptively refine the feature responses by fusing the attention map with the feature map. We participate in the seven basic expression classification sub-challenges of FG-2020 Affective Behavior Analysis in-the-wild Challenge. And we validate our method on the Aff-Wild2 datasets released by the Challenge. The total accuracy (Accuracy) and the unweighted mean (F1) of our method on the validation set are 0.49 and 0.38 respectively, and the final result is 0.42 (0.67 F1-Score + 0.33 Accuracy).



### Predicting Goal-directed Attention Control Using Inverse-Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.11921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11921v1)
- **Published**: 2020-01-31 15:53:52+00:00
- **Updated**: 2020-01-31 15:53:52+00:00
- **Authors**: Gregory J. Zelinsky, Yupei Chen, Seoyoung Ahn, Hossein Adeli, Zhibo Yang, Lihan Huang, Dimitrios Samaras, Minh Hoai
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding how goal states control behavior is a question ripe for interrogation by new methods from machine learning. These methods require large and labeled datasets to train models. To annotate a large-scale image dataset with observed search fixations, we collected 16,184 fixations from people searching for either microwaves or clocks in a dataset of 4,366 images (MS-COCO). We then used this behaviorally-annotated dataset and the machine learning method of Inverse-Reinforcement Learning (IRL) to learn target-specific reward functions and policies for these two target goals. Finally, we used these learned policies to predict the fixations of 60 new behavioral searchers (clock = 30, microwave = 30) in a disjoint test dataset of kitchen scenes depicting both a microwave and a clock (thus controlling for differences in low-level image contrast). We found that the IRL model predicted behavioral search efficiency and fixation-density maps using multiple metrics. Moreover, reward maps from the IRL model revealed target-specific patterns that suggest, not just attention guidance by target features, but also guidance by scene context (e.g., fixations along walls in the search of clocks). Using machine learning and the psychologically-meaningful principle of reward, it is possible to learn the visual features used in goal-directed attention control.



### A Heteroscedastic Uncertainty Model for Decoupling Sources of MRI Image Quality
- **Arxiv ID**: http://arxiv.org/abs/2001.11927v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11927v1)
- **Published**: 2020-01-31 16:04:41+00:00
- **Updated**: 2020-01-31 16:04:41+00:00
- **Authors**: Richard Shaw, Carole H. Sudre, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Quality control (QC) of medical images is essential to ensure that downstream analyses such as segmentation can be performed successfully. Currently, QC is predominantly performed visually at significant time and operator cost. We aim to automate the process by formulating a probabilistic network that estimates uncertainty through a heteroscedastic noise model, hence providing a proxy measure of task-specific image quality that is learnt directly from the data. By augmenting the training data with different types of simulated k-space artefacts, we propose a novel cascading CNN architecture based on a student-teacher framework to decouple sources of uncertainty related to different k-space augmentations in an entirely self-supervised manner. This enables us to predict separate uncertainty quantities for the different types of data degradation. While the uncertainty measures reflect the presence and severity of image artefacts, the network also provides the segmentation predictions given the quality of the data. We show models trained with simulated artefacts provide informative measures of uncertainty on real-world images and we validate our uncertainty predictions on problematic images identified by human-raters.



### A framework for large-scale mapping of human settlement extent from Sentinel-2 images via fully convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2001.11935v1
- **DOI**: 10.1016/j.isprsjprs.2020.01.028
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11935v1)
- **Published**: 2020-01-31 16:23:34+00:00
- **Updated**: 2020-01-31 16:23:34+00:00
- **Authors**: C. Qiu, M. Schmitt, C. Geiss, T. K. Chen, X. X. Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Human settlement extent (HSE) information is a valuable indicator of world-wide urbanization as well as the resulting human pressure on the natural environment. Therefore, mapping HSE is critical for various environmental issues at local, regional, and even global scales. This paper presents a deep-learning-based framework to automatically map HSE from multi-spectral Sentinel-2 data using regionally available geo-products as training labels. A straightforward, simple, yet effective fully convolutional network-based architecture, Sen2HSE, is implemented as an example for semantic segmentation within the framework. The framework is validated against both manually labelled checking points distributed evenly over the test areas, and the OpenStreetMap building layer. The HSE mapping results were extensively compared to several baseline products in order to thoroughly evaluate the effectiveness of the proposed HSE mapping framework. The HSE mapping power is consistently demonstrated over 10 representative areas across the world. We also present one regional-scale and one country-wide HSE mapping example from our framework to show the potential for upscaling. The results of this study contribute to the generalization of the applicability of CNN-based approaches for large-scale urban mapping to cases where no up-to-date and accurate ground truth is available, as well as the subsequent monitor of global urbanization.



### Age-Conditioned Synthesis of Pediatric Computed Tomography with Auxiliary Classifier Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00011v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00011v1)
- **Published**: 2020-01-31 16:52:10+00:00
- **Updated**: 2020-01-31 16:52:10+00:00
- **Authors**: Chi Nok Enoch Kan, Najibakram Maheenaboobacker, Dong Hye Ye
- **Comment**: Accepted for publication at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2020
- **Journal**: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI
  2020)
- **Summary**: Deep learning is a popular and powerful tool in computed tomography (CT) image processing such as organ segmentation, but its requirement of large training datasets remains a challenge. Even though there is a large anatomical variability for children during their growth, the training datasets for pediatric CT scans are especially hard to obtain due to risks of radiation to children. In this paper, we propose a method to conditionally synthesize realistic pediatric CT images using a new auxiliary classifier generative adversarial network (ACGAN) architecture by taking age information into account. The proposed network generated age-conditioned high-resolution CT images to enrich pediatric training datasets.



### Continuous Emotion Recognition via Deep Convolutional Autoencoder and Support Vector Regressor
- **Arxiv ID**: http://arxiv.org/abs/2001.11976v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11976v1)
- **Published**: 2020-01-31 17:47:16+00:00
- **Updated**: 2020-01-31 17:47:16+00:00
- **Authors**: Sevegni Odilon Clement Allognon, Alessandro L. Koerich, Alceu de S. Britto Jr
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic facial expression recognition is an important research area in the emotion recognition and computer vision. Applications can be found in several domains such as medical treatment, driver fatigue surveillance, sociable robotics, and several other human-computer interaction systems. Therefore, it is crucial that the machine should be able to recognize the emotional state of the user with high accuracy. In recent years, deep neural networks have been used with great success in recognizing emotions. In this paper, we present a new model for continuous emotion recognition based on facial expression recognition by using an unsupervised learning approach based on transfer learning and autoencoders. The proposed approach also includes preprocessing and post-processing techniques which contribute favorably to improving the performance of predicting the concordance correlation coefficient for arousal and valence dimensions. Experimental results for predicting spontaneous and natural emotions on the RECOLA 2016 dataset have shown that the proposed approach based on visual information can achieve CCCs of 0.516 and 0.264 for valence and arousal, respectively.



### Learning Deep Analysis Dictionaries for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2001.12010v2
- **DOI**: 10.1109/TSP.2020.3036902
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.12010v2)
- **Published**: 2020-01-31 18:59:35+00:00
- **Updated**: 2020-11-10 06:37:37+00:00
- **Authors**: Jun-Jie Huang, Pier Luigi Dragotti
- **Comment**: Accepted by IEEE Transactions on Signal Processing
- **Journal**: None
- **Summary**: Inspired by the recent success of deep neural networks and the recent efforts to develop multi-layer dictionary models, we propose a Deep Analysis dictionary Model (DeepAM) which is optimized to address a specific regression task known as single image super-resolution. Contrary to other multi-layer dictionary models, our architecture contains L layers of analysis dictionary and soft-thresholding operators to gradually extract high-level features and a layer of synthesis dictionary which is designed to optimize the regression task at hand. In our approach, each analysis dictionary is partitioned into two sub-dictionaries: an Information Preserving Analysis Dictionary (IPAD) and a Clustering Analysis Dictionary (CAD). The IPAD together with the corresponding soft-thresholds is designed to pass the key information from the previous layer to the next layer, while the CAD together with the corresponding soft-thresholding operator is designed to produce a sparse feature representation of its input data that facilitates discrimination of key features. DeepAM uses both supervised and unsupervised setup. Simulation results show that the proposed deep analysis dictionary model achieves better performance compared to a deep neural network that has the same structure and is optimized using back-propagation when training datasets are small.



### Learning Deep Analysis Dictionaries -- Part II: Convolutional Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/2002.00022v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00022v1)
- **Published**: 2020-01-31 19:02:10+00:00
- **Updated**: 2020-01-31 19:02:10+00:00
- **Authors**: Jun-Jie Huang, Pier Luigi Dragotti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a Deep Convolutional Analysis Dictionary Model (DeepCAM) by learning convolutional dictionaries instead of unstructured dictionaries as in the case of deep analysis dictionary model introduced in the companion paper. Convolutional dictionaries are more suitable for processing high-dimensional signals like for example images and have only a small number of free parameters. By exploiting the properties of a convolutional dictionary, we present an efficient convolutional analysis dictionary learning approach. A L-layer DeepCAM consists of L layers of convolutional analysis dictionary and element-wise soft-thresholding pairs and a single layer of convolutional synthesis dictionary. Similar to DeepAM, each convolutional analysis dictionary is composed of a convolutional Information Preserving Analysis Dictionary (IPAD) and a convolutional Clustering Analysis Dictionary (CAD). The IPAD and the CAD are learned using variations of the proposed learning algorithm. We demonstrate that DeepCAM is an effective multilayer convolutional model and, on single image super-resolution, achieves performance comparable with other methods while also showing good generalization capabilities.



### Analysis of Gender Inequality In Face Recognition Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2002.00065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00065v1)
- **Published**: 2020-01-31 21:32:53+00:00
- **Updated**: 2020-01-31 21:32:53+00:00
- **Authors**: VÃ­tor Albiero, Krishnapriya K. S., Kushal Vangara, Kai Zhang, Michael C. King, Kevin W. Bowyer
- **Comment**: Paper will appear at The 2nd Workshop on Demographic Variation in the
  Performance of Biometric Systems at WACV 2020
- **Journal**: None
- **Summary**: We present a comprehensive analysis of how and why face recognition accuracy differs between men and women. We show that accuracy is lower for women due to the combination of (1) the impostor distribution for women having a skew toward higher similarity scores, and (2) the genuine distribution for women having a skew toward lower similarity scores. We show that this phenomenon of the impostor and genuine distributions for women shifting closer towards each other is general across datasets of African-American, Caucasian, and Asian faces. We show that the distribution of facial expressions may differ between male/female, but that the accuracy difference persists for image subsets rated confidently as neutral expression. The accuracy difference also persists for image subsets rated as close to zero pitch angle. Even when removing images with forehead partially occluded by hair/hat, the same impostor/genuine accuracy difference persists. We show that the female genuine distribution improves when only female images without facial cosmetics are used, but that the female impostor distribution also degrades at the same time. Lastly, we show that the accuracy difference persists even if a state-of-the-art deep learning method is trained from scratch using training data explicitly balanced between male and female images and subjects.



### Hybrid Graph Neural Networks for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2002.00092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.00092v1)
- **Published**: 2020-01-31 23:06:03+00:00
- **Updated**: 2020-01-31 23:06:03+00:00
- **Authors**: Ao Luo, Fan Yang, Xin Li, Dong Nie, Zhicheng Jiao, Shangchen Zhou, Hong Cheng
- **Comment**: To appear in AAAI 2020
- **Journal**: None
- **Summary**: Crowd counting is an important yet challenging task due to the large scale and density variation. Recent investigations have shown that distilling rich relations among multi-scale features and exploiting useful information from the auxiliary task, i.e., localization, are vital for this task. Nevertheless, how to comprehensively leverage these relations within a unified network architecture is still a challenging problem. In this paper, we present a novel network structure called Hybrid Graph Neural Network (HyGnn) which targets to relieve the problem by interweaving the multi-scale features for crowd density as well as its auxiliary task (localization) together and performing joint reasoning over a graph. Specifically, HyGnn integrates a hybrid graph to jointly represent the task-specific feature maps of different scales as nodes, and two types of relations as edges:(i) multi-scale relations for capturing the feature dependencies across scales and (ii) mutual beneficial relations building bridges for the cooperation between counting and localization. Thus, through message passing, HyGnn can distill rich relations between the nodes to obtain more powerful representations, leading to robust and accurate results. Our HyGnn performs significantly well on four challenging datasets: ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF_QNRF, outperforming the state-of-the-art approaches by a large margin.



### Post-Training Piecewise Linear Quantization for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.00104v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.00104v2)
- **Published**: 2020-01-31 23:47:00+00:00
- **Updated**: 2020-03-18 18:49:40+00:00
- **Authors**: Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, Joseph Hassoun
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization plays an important role in the energy-efficient deployment of deep neural networks on resource-limited devices. Post-training quantization is highly desirable since it does not require retraining or access to the full training dataset. The well-established uniform scheme for post-training quantization achieves satisfactory results by converting neural networks from full-precision to 8-bit fixed-point integers. However, it suffers from significant performance degradation when quantizing to lower bit-widths. In this paper, we propose a piecewise linear quantization (PWLQ) scheme to enable accurate approximation for tensor values that have bell-shaped distributions with long tails. Our approach breaks the entire quantization range into non-overlapping regions for each tensor, with each region being assigned an equal number of quantization levels. Optimal breakpoints that divide the entire range are found by minimizing the quantization error. Compared to state-of-the-art post-training quantization methods, experimental results show that our proposed method achieves superior performance on image classification, semantic segmentation, and object detection with minor overhead.




# Arxiv Papers in cs.CV on 2020-03-25
### Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach
- **Arxiv ID**: http://arxiv.org/abs/2003.11163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11163v2)
- **Published**: 2020-03-25 00:26:54+00:00
- **Updated**: 2020-04-10 05:48:51+00:00
- **Authors**: Zhe Zhang, Chunyu Wang, Wenhu Qin, Wenjun Zeng
- **Comment**: Accepted by CVPR 2020. Code is released at
  https://github.com/CHUNYUWANG/imu-human-pose-pytorch
- **Journal**: None
- **Summary**: We propose to estimate 3D human pose from multi-view images and a few IMUs attached at person's limbs. It operates by firstly detecting 2D poses from the two signals, and then lifting them to the 3D space. We present a geometric approach to reinforce the visual features of each pair of joints based on the IMUs. This notably improves 2D pose estimation accuracy especially when one joint is occluded. We call this approach Orientation Regularized Network (ORN). Then we lift the multi-view 2D poses to the 3D space by an Orientation Regularized Pictorial Structure Model (ORPSM) which jointly minimizes the projection error between the 3D and 2D poses, along with the discrepancy between the 3D pose and IMU orientations. The simple two-step approach reduces the error of the state-of-the-art by a large margin on a public dataset. Our code will be released at https://github.com/CHUNYUWANG/imu-human-pose-pytorch.



### GraphChallenge.org Sparse Deep Neural Network Performance
- **Arxiv ID**: http://arxiv.org/abs/2004.01181v2
- **DOI**: 10.1109/HPEC43674.2020.9286253
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.01181v2)
- **Published**: 2020-03-25 00:29:12+00:00
- **Updated**: 2020-04-06 02:38:52+00:00
- **Authors**: Jeremy Kepner, Simon Alford, Vijay Gadepally, Michael Jones, Lauren Milechin, Albert Reuther, Ryan Robinett, Sid Samsi
- **Comment**: 7 pages, 7 figures, 80 references, to be submitted to IEEE HPEC 2020.
  This work reports new updated results on prior work reported in
  arXiv:1909.05631. arXiv admin note: substantial text overlap with
  arXiv:1807.03165, arXiv:1708.02937. arXiv admin note: text overlap with
  arXiv:2003.09269
- **Journal**: None
- **Summary**: The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to developing new solutions for analyzing graphs and sparse data. Sparse AI analytics present unique scalability difficulties. The Sparse Deep Neural Network (DNN) Challenge draws upon prior challenges from machine learning, high performance computing, and visual analytics to create a challenge that is reflective of emerging sparse AI systems. The sparse DNN challenge is based on a mathematically well-defined DNN inference computation and can be implemented in any programming environment. In 2019 several sparse DNN challenge submissions were received from a wide range of authors and organizations. This paper presents a performance analysis of the best performers of these submissions. These submissions show that their state-of-the-art sparse DNN execution time, $T_{\rm DNN}$, is a strong function of the number of DNN operations performed, $N_{\rm op}$. The sparse DNN challenge provides a clear picture of current sparse DNN systems and underscores the need for new innovations to achieve high performance on very large sparse DNNs.



### Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/2003.11172v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0; I.4.8; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.11172v1)
- **Published**: 2020-03-25 01:13:04+00:00
- **Updated**: 2020-03-25 01:13:04+00:00
- **Authors**: Yiwen Hua, Puneet Kohli, Pritish Uplavikar, Anand Ravi, Saravana Gunaseelan, Jason Orozco, Edward Li
- **Comment**: Main paper: 17 pages, 7 figures, 3 tables. Supplementary: 11 pages, 7
  figures, 4 tables. See http://github.com/leiainc/holopix50k for downloading
  the dataset
- **Journal**: None
- **Summary**: With the mass-market adoption of dual-camera mobile phones, leveraging stereo information in computer vision has become increasingly important. Current state-of-the-art methods utilize learning-based algorithms, where the amount and quality of training samples heavily influence results. Existing stereo image datasets are limited either in size or subject variety. Hence, algorithms trained on such datasets do not generalize well to scenarios encountered in mobile photography. We present Holopix50k, a novel in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix mobile social platform. In this work, we describe our data collection process and statistically compare our dataset to other popular stereo datasets. We experimentally show that using our dataset significantly improves results for tasks such as stereo super-resolution and self-supervised monocular depth estimation. Finally, we showcase practical applications of our dataset to motivate novel works and use cases. The Holopix50k dataset is available at http://github.com/leiainc/holopix50k



### Patch-based Non-Local Bayesian Networks for Blind Confocal Microscopy Denoising
- **Arxiv ID**: http://arxiv.org/abs/2003.11177v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11177v2)
- **Published**: 2020-03-25 01:49:58+00:00
- **Updated**: 2020-05-26 23:36:22+00:00
- **Authors**: Saeed Izadi, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Confocal microscopy is essential for histopathologic cell visualization and quantification. Despite its significant role in biology, fluorescence confocal microscopy suffers from the presence of inherent noise during image acquisition. Non-local patch-wise Bayesian mean filtering (NLB) was until recently the state-of-the-art denoising approach. However, classic denoising methods have been outperformed by neural networks in recent years. In this work, we propose to exploit the strengths of NLB in the framework of Bayesian deep learning. We do so by designing a convolutional neural network and training it to learn parameters of a Gaussian model approximating the prior on noise-free patches given their nearest, similar yet non-local, neighbors. We then apply Bayesian reasoning to leverage the prior and information from the noisy patch in the process of approximating the noise-free patch. Specifically, we use the closed-form analytic \textit{maximum a posteriori} (MAP) estimate in the NLB algorithm to obtain the noise-free patch that maximizes the posterior distribution. The performance of our proposed method is evaluated on confocal microscopy images with real noise Poisson-Gaussian noise. Our experiments reveal the superiority of our approach against state-of-the-art unsupervised denoising techniques.



### Aerial Imagery based LIDAR Localization for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2003.11192v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11192v1)
- **Published**: 2020-03-25 02:52:05+00:00
- **Updated**: 2020-03-25 02:52:05+00:00
- **Authors**: Ankit Vora, Siddharth Agarwal, Gaurav Pandey, James McBride
- **Comment**: 6 pages, 7 figures, Submitted to International Conference on
  Intelligent Robots and Systems (IROS-2020), For the video, see
  https://www.youtube.com/watch?v=vcY74Z9bOLk
- **Journal**: None
- **Summary**: This paper presents a localization technique using aerial imagery maps and LIDAR based ground reflectivity for autonomous vehicles in urban environments. Traditional localization techniques using LIDAR reflectivity rely on high definition reflectivity maps generated from a mapping vehicle. The cost and effort required to maintain such prior maps are generally very high because it requires a fleet of expensive mapping vehicles. In this work we propose a localization technique where the vehicle localizes using aerial/satellite imagery, eradicating the need to develop and maintain complex high-definition maps. The proposed technique has been tested on a real world dataset collected from a test track in Ann Arbor, Michigan. This research concludes that aerial imagery based maps provides real-time localization performance similar to state-of-the-art LIDAR based maps for autonomous vehicles in urban environments at reduced costs.



### Prior-enlightened and Motion-robust Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2003.11209v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11209v2)
- **Published**: 2020-03-25 04:16:56+00:00
- **Updated**: 2020-03-26 02:30:40+00:00
- **Authors**: Ya Zhou, Jianfeng Xu, Kazuyuki Tasaka, Zhibo Chen, Weiping Li
- **Comment**: 26 pages, 13 figures, and 7 tables
- **Journal**: None
- **Summary**: Various blur distortions in video will cause negative impact on both human viewing and video-based applications, which makes motion-robust deblurring methods urgently needed. Most existing works have strong dataset dependency and limited generalization ability in handling challenging scenarios, like blur in low contrast or severe motion areas, and non-uniform blur. Therefore, we propose a PRiOr-enlightened and MOTION-robust video deblurring model (PROMOTION) suitable for challenging blurs. On the one hand, we use 3D group convolution to efficiently encode heterogeneous prior information, explicitly enhancing the scenes' perception while mitigating the output's artifacts. On the other hand, we design the priors representing blur distribution, to better handle non-uniform blur in spatio-temporal domain. Besides the classical camera shake caused global blurry, we also prove the generalization for the downstream task suffering from local blur. Extensive experiments demonstrate we can achieve the state-of-the-art performance on well-known REDS and GoPro datasets, and bring machine task gain.



### Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2003.11211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11211v1)
- **Published**: 2020-03-25 04:23:18+00:00
- **Updated**: 2020-03-25 04:23:18+00:00
- **Authors**: Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, Satoshi Iizuka
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: We propose an efficient pipeline for large-scale landmark image retrieval that addresses the diversity of the dataset through two-stage discriminative re-ranking. Our approach is based on embedding the images in a feature-space using a convolutional neural network trained with a cosine softmax loss. Due to the variance of the images, which include extreme viewpoint changes such as having to retrieve images of the exterior of a landmark from images of the interior, this is very challenging for approaches based exclusively on visual similarity. Our proposed re-ranking approach improves the results in two steps: in the sort-step, $k$-nearest neighbor search with soft-voting to sort the retrieved results based on their label similarity to the query images, and in the insert-step, we add additional samples from the dataset that were not retrieved by image-similarity. This approach allows overcoming the low visual diversity in retrieved images. In-depth experimental results show that the proposed approach significantly outperforms existing approaches on the challenging Google Landmarks Datasets. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google Landmark Recognition 2019 challenge on Kaggle. Our code is publicly available here: \url{https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution}



### A New Multiple Max-pooling Integration Module and Cross Multiscale Deconvolution Network Based on Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.11213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11213v1)
- **Published**: 2020-03-25 04:27:01+00:00
- **Updated**: 2020-03-25 04:27:01+00:00
- **Authors**: Hongfeng You, Shengwei Tian, Long Yu, Xiang Ma, Yan Xing, Ning Xin
- **Comment**: None
- **Journal**: None
- **Summary**: To better retain the deep features of an image and solve the sparsity problem of the end-to-end segmentation model, we propose a new deep convolutional network model for medical image pixel segmentation, called MC-Net. The core of this network model consists of four parts, namely, an encoder network, a multiple max-pooling integration module, a cross multiscale deconvolution decoder network and a pixel-level classification layer. In the network structure of the encoder, we use multiscale convolution instead of the traditional single-channel convolution. The multiple max-pooling integration module first integrates the output features of each submodule of the encoder network and reduces the number of parameters by convolution using a kernel size of 1. At the same time, each max-pooling layer (the pooling size of each layer is different) is spliced after each convolution to achieve the translation invariance of the feature maps of each submodule. We use the output feature maps from the multiple max-pooling integration module as the input of the decoder network; the multiscale convolution of each submodule in the decoder network is cross-fused with the feature maps generated by the corresponding multiscale convolution in the encoder network. Using the above feature map processing methods solves the sparsity problem after the max-pooling layer-generating matrix and enhances the robustness of the classification. We compare our proposed model with the well-known Fully Convolutional Networks for Semantic Segmentation (FCNs), DecovNet, PSPNet, U-net, SgeNet and other state-of-the-art segmentation networks such as HyperDenseNet, MS-Dual, Espnetv2, Denseaspp using one binary Kaggle 2018 data science bowl dataset and two multiclass dataset and obtain encouraging experimental results.



### ASFD: Automatic and Scalable Face Detector
- **Arxiv ID**: http://arxiv.org/abs/2003.11228v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11228v3)
- **Published**: 2020-03-25 06:00:47+00:00
- **Updated**: 2020-03-31 16:09:40+00:00
- **Authors**: Bin Zhang, Jian Li, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Yili Xia, Wenjiang Pei, Rongrong Ji
- **Comment**: Ranked No.1 on WIDER Face
  (http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html)
- **Journal**: None
- **Summary**: In this paper, we propose a novel Automatic and Scalable Face Detector (ASFD), which is based on a combination of neural architecture search techniques as well as a new loss design. First, we propose an automatic feature enhance module named Auto-FEM by improved differential architecture search, which allows efficient multi-scale feature fusion and context enhancement. Second, we use Distance-based Regression and Margin-based Classification (DRMC) multi-task loss to predict accurate bounding boxes and learn highly discriminative deep features. Third, we adopt compound scaling methods and uniformly scale the backbone, feature modules, and head networks to develop a family of ASFD, which are consistently more efficient than the state-of-the-art face detectors. Extensive experiments conducted on popular benchmarks, e.g. WIDER FACE and FDDB, demonstrate that our ASFD-D6 outperforms the prior strong competitors, and our lightweight ASFD-D0 runs at more than 120 FPS with Mobilenet for VGA-resolution images.



### GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet
- **Arxiv ID**: http://arxiv.org/abs/2003.11236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.11236v1)
- **Published**: 2020-03-25 06:54:10+00:00
- **Updated**: 2020-03-25 06:54:10+00:00
- **Authors**: Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, Changshui Zhang
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: Training a supernet matters for one-shot neural architecture search (NAS) methods since it serves as a basic performance estimator for different architectures (paths). Current methods mainly hold the assumption that a supernet should give a reasonable ranking over all paths. They thus treat all paths equally, and spare much effort to train paths. However, it is harsh for a single supernet to evaluate accurately on such a huge-scale search space (e.g., $7^{21}$). In this paper, instead of covering all paths, we ease the burden of supernet by encouraging it to focus more on evaluation of those potentially-good ones, which are identified using a surrogate portion of validation data. Concretely, during training, we propose a multi-path sampling strategy with rejection, and greedily filter the weak paths. The training efficiency is thus boosted since the training space has been greedily shrunk from all paths to those potentially-good ones. Moreover, we further adopt an exploration and exploitation policy by introducing an empirical candidate path pool. Our proposed method GreedyNAS is easy-to-follow, and experimental results on ImageNet dataset indicate that it can achieve better Top-1 accuracy under same search space and FLOPs or latency level, but with only $\sim$60\% of supernet training cost. By searching on a larger space, our GreedyNAS can also obtain new state-of-the-art architectures.



### What Deep CNNs Benefit from Global Covariance Pooling: An Optimization Perspective
- **Arxiv ID**: http://arxiv.org/abs/2003.11241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11241v1)
- **Published**: 2020-03-25 07:00:45+00:00
- **Updated**: 2020-03-25 07:00:45+00:00
- **Authors**: Qilong Wang, Li Zhang, Banggu Wu, Dongwei Ren, Peihua Li, Wangmeng Zuo, Qinghua Hu
- **Comment**: Accepted to CVPR 2020; Project Page:
  https://github.com/ZhangLi-CS/GCP_Optimization
- **Journal**: None
- **Summary**: Recent works have demonstrated that global covariance pooling (GCP) has the ability to improve performance of deep convolutional neural networks (CNNs) on visual classification task. Despite considerable advance, the reasons on effectiveness of GCP on deep CNNs have not been well studied. In this paper, we make an attempt to understand what deep CNNs benefit from GCP in a viewpoint of optimization. Specifically, we explore the effect of GCP on deep CNNs in terms of the Lipschitzness of optimization loss and the predictiveness of gradients, and show that GCP can make the optimization landscape more smooth and the gradients more predictive. Furthermore, we discuss the connection between GCP and second-order optimization for deep CNNs. More importantly, above findings can account for several merits of covariance pooling for training deep CNNs that have not been recognized previously or fully explored, including significant acceleration of network convergence (i.e., the networks trained with GCP can support rapid decay of learning rates, achieving favorable performance while significantly reducing number of training epochs), stronger robustness to distorted examples generated by image corruptions and perturbations, and good generalization ability to different vision tasks, e.g., object detection and instance segmentation. We conduct extensive experiments using various deep CNN models on diversified tasks, and the results provide strong support to our findings.



### Safety-Aware Hardening of 3D Object Detection Neural Network Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.11242v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.11242v3)
- **Published**: 2020-03-25 07:06:11+00:00
- **Updated**: 2020-04-01 09:46:22+00:00
- **Authors**: Chih-Hong Cheng
- **Comment**: This version is similar to v1 with an added statement: "The
  evaluation using KITTI dataset in this paper is for knowledge dissemination
  and scientific publication and is not for commercial use"
- **Journal**: None
- **Summary**: We study how state-of-the-art neural networks for 3D object detection using a single-stage pipeline can be made safety aware. We start with the safety specification (reflecting the capability of other components) that partitions the 3D input space by criticality, where the critical area employs a separate criterion on robustness under perturbation, quality of bounding boxes, and the tolerance over false negatives demonstrated on the training set. In the architecture design, we consider symbolic error propagation to allow feature-level perturbation. Subsequently, we introduce a specialized loss function reflecting (1) the safety specification, (2) the use of single-stage detection architecture, and finally, (3) the characterization of robustness under perturbation. We also replace the commonly seen non-max-suppression post-processing algorithm by a safety-aware non-max-inclusion algorithm, in order to maintain the safety claim created by the neural network. The concept is detailed by extending the state-of-the-art PIXOR detector which creates object bounding boxes in bird's eye view with inputs from point clouds.



### VaB-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11249v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.11249v2)
- **Published**: 2020-03-25 07:34:06+00:00
- **Updated**: 2020-12-03 12:18:11+00:00
- **Authors**: Jongwon Choi, Kwang Moo Yi, Jihoon Kim, Jinho Choo, Byoungjip Kim, Jin-Yeop Chang, Youngjune Gwon, Hyung Jin Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Active Learning for discriminative models has largely been studied with the focus on individual samples, with less emphasis on how classes are distributed or which classes are hard to deal with. In this work, we show that this is harmful. We propose a method based on the Bayes' rule, that can naturally incorporate class imbalance into the Active Learning framework. We derive that three terms should be considered together when estimating the probability of a classifier making a mistake for a given sample; i) probability of mislabelling a class, ii) likelihood of the data given a predicted class, and iii) the prior probability on the abundance of a predicted class. Implementing these terms requires a generative model and an intractable likelihood estimation. Therefore, we train a Variational Auto Encoder (VAE) for this purpose. To further tie the VAE with the classifier and facilitate VAE training, we use the classifiers' deep feature representations as input to the VAE. By considering all three probabilities, among them especially the data imbalance, we can substantially improve the potential of existing methods under limited data budget. We show that our method can be applied to classification tasks on multiple different datasets -- including one that is a real-world dataset with heavy data imbalance -- significantly outperforming the state of the art.



### Multiscale Sparsifying Transform Learning for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2003.11265v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11265v5)
- **Published**: 2020-03-25 08:13:16+00:00
- **Updated**: 2021-07-25 18:16:20+00:00
- **Authors**: Ashkan Abbasi, Amirhassan Monadjemi, Leyuan Fang, Hossein Rabbani, Neda Noormohammadi, Yi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The data-driven sparse methods such as synthesis dictionary learning (e.g., K-SVD) and sparsifying transform learning have been proven effective in image denoising. However, they are intrinsically single-scale which can lead to suboptimal results. We propose two methods developed based on wavelet subbands mixing to efficiently combine the merits of both single and multiscale methods. We show that an efficient multiscale method can be devised without the need for denoising detail subbands which substantially reduces the runtime. The proposed methods are initially derived within the framework of sparsifying transform learning denoising, and then, they are generalized to propose our multiscale extensions for the well-known K-SVD and SAIST image denoising methods. We analyze and assess the studied methods thoroughly and compare them with the well-known and state-of-the-art methods. The experiments show that our methods are able to offer good trade-offs between performance and complexity.



### Content Adaptive and Error Propagation Aware Deep Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2003.11282v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11282v1)
- **Published**: 2020-03-25 09:04:24+00:00
- **Updated**: 2020-03-25 09:04:24+00:00
- **Authors**: Guo Lu, Chunlei Cai, Xiaoyun Zhang, Li Chen, Wanli Ouyang, Dong Xu, Zhiyong Gao
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Recently, learning based video compression methods attract increasing attention. However, the previous works suffer from error propagation due to the accumulation of reconstructed error in inter predictive coding. Meanwhile, the previous learning based video codecs are also not adaptive to different video contents. To address these two problems, we propose a content adaptive and error propagation aware video compression system. Specifically, our method employs a joint training strategy by considering the compression performance of multiple consecutive frames instead of a single frame. Based on the learned long-term temporal information, our approach effectively alleviates error propagation in reconstructed frames. More importantly, instead of using the hand-crafted coding modes in the traditional compression systems, we design an online encoder updating scheme in our system. The proposed approach updates the parameters for encoder according to the rate-distortion criterion but keeps the decoder unchanged in the inference stage. Therefore, the encoder is adaptive to different video contents and achieves better compression performance by reducing the domain gap between the training and testing datasets. Our method is simple yet effective and outperforms the state-of-the-art learning based video codecs on benchmark datasets without increasing the model size or decreasing the decoding speed.



### SCATTER: Selective Context Attentional Scene Text Recognizer
- **Arxiv ID**: http://arxiv.org/abs/2003.11288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11288v1)
- **Published**: 2020-03-25 09:20:28+00:00
- **Updated**: 2020-03-25 09:20:28+00:00
- **Authors**: Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, R. Manmatha
- **Comment**: In CVPR 2020
- **Journal**: None
- **Summary**: Scene Text Recognition (STR), the task of recognizing text against complex image backgrounds, is an active area of research. Current state-of-the-art (SOTA) methods still struggle to recognize text written in arbitrary shapes. In this paper, we introduce a novel architecture for STR, named Selective Context ATtentional Text Recognizer (SCATTER). SCATTER utilizes a stacked block architecture with intermediate supervision during training, that paves the way to successfully train a deep BiLSTM encoder, thus improving the encoding of contextual dependencies. Decoding is done using a two-step 1D attention mechanism. The first attention step re-weights visual features from a CNN backbone together with contextual features computed by a BiLSTM layer. The second attention step, similar to previous papers, treats the features as a sequence and attends to the intra-sequence relationships. Experiments show that the proposed approach surpasses SOTA performance on irregular text recognition benchmarks by 3.7\% on average.



### A Unified Object Motion and Affinity Model for Online Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.11291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11291v2)
- **Published**: 2020-03-25 09:36:43+00:00
- **Updated**: 2020-04-03 03:08:41+00:00
- **Authors**: Junbo Yin, Wenguan Wang, Qinghao Meng, Ruigang Yang, Jianbing Shen
- **Comment**: Accepted to CVPR 2020. Code: https://github.com/yinjunbo/UMA-MOT
- **Journal**: None
- **Summary**: Current popular online multi-object tracking (MOT) solutions apply single object trackers (SOTs) to capture object motions, while often requiring an extra affinity network to associate objects, especially for the occluded ones. This brings extra computational overhead due to repetitive feature extraction for SOT and affinity computation. Meanwhile, the model size of the sophisticated affinity network is usually non-trivial. In this paper, we propose a novel MOT framework that unifies object motion and affinity model into a single network, named UMA, in order to learn a compact feature that is discriminative for both object motion and affinity measure. In particular, UMA integrates single object tracking and metric learning into a unified triplet network by means of multi-task learning. Such design brings advantages of improved computation efficiency, low memory requirement and simplified training procedure. In addition, we equip our model with a task-specific attention module, which is used to boost task-aware feature learning. The proposed UMA can be easily trained end-to-end, and is elegant - requiring only one training stage. Experimental results show that it achieves promising performance on several MOT Challenge benchmarks.



### Cylindrical Convolutional Networks for Joint Object Detection and Viewpoint Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.11303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11303v1)
- **Published**: 2020-03-25 10:24:58+00:00
- **Updated**: 2020-03-25 10:24:58+00:00
- **Authors**: Sunghun Joung, Seungryong Kim, Hanjae Kim, Minsu Kim, Ig-Jae Kim, Junghyun Cho, Kwanghoon Sohn
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Existing techniques to encode spatial invariance within deep convolutional neural networks only model 2D transformation fields. This does not account for the fact that objects in a 2D space are a projection of 3D ones, and thus they have limited ability to severe object viewpoint changes. To overcome this limitation, we introduce a learnable module, cylindrical convolutional networks (CCNs), that exploit cylindrical representation of a convolutional kernel defined in the 3D space. CCNs extract a view-specific feature through a view-specific convolutional kernel to predict object category scores at each viewpoint. With the view-specific feature, we simultaneously determine objective category and viewpoints using the proposed sinusoidal soft-argmax module. Our experiments demonstrate the effectiveness of the cylindrical convolutional networks on joint object detection and viewpoint estimation.



### DCDLearn: Multi-order Deep Cross-distance Learning for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.11315v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11315v2)
- **Published**: 2020-03-25 10:46:54+00:00
- **Updated**: 2020-03-28 04:34:22+00:00
- **Authors**: Rixing Zhu, Jianwu Fang, Hongke Xu, Hongkai Yu, Jianru Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (Re-ID) has become a popular research topic owing to its practicability in intelligent transportation systems. Vehicle Re-ID suffers the numerous challenges caused by drastic variation in illumination, occlusions, background, resolutions, viewing angles, and so on. To address it, this paper formulates a multi-order deep cross-distance learning (\textbf{DCDLearn}) model for vehicle re-identification, where an efficient one-view CycleGAN model is developed to alleviate exhaustive and enumerative cross-camera matching problem in previous works and smooth the domain discrepancy of cross cameras. Specially, we treat the transferred images and the reconstructed images generated by one-view CycleGAN as multi-order augmented data for deep cross-distance learning, where the cross distances of multi-order image set with distinct identities are learned by optimizing an objective function with multi-order augmented triplet loss and center loss to achieve the camera-invariance and identity-consistency. Extensive experiments on three vehicle Re-ID datasets demonstrate that the proposed method achieves significant improvement over the state-of-the-arts, especially for the small scale dataset.



### SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.11337v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11337v2)
- **Published**: 2020-03-25 11:35:16+00:00
- **Updated**: 2020-05-19 03:40:08+00:00
- **Authors**: Zhuoping Yu, Zhong Gao, Hansheng Chen, Yuyao Huang
- **Comment**: None
- **Journal**: None
- **Summary**: For vehicles equipped with the automatic parking system, the accuracy and speed of the parking slot detection are crucial. But the high accuracy is obtained at the price of low speed or expensive computation equipment, which are sensitive for many car manufacturers. In this paper, we proposed a detector using CNN(convolutional neural networks) for faster speed and smaller model size while keeps accuracy. To achieve the optimal balance, we developed a strategy to select the best receptive fields and prune the redundant channels automatically after each training epoch. The proposed model is capable of jointly detecting corners and line features of parking slots while running efficiently in real time on average processors. The model has a frame rate of about 30 FPS on a 2.3 GHz CPU core, yielding parking slot corner localization error of 1.51$\pm$2.14 cm (std. err.) and slot detection accuracy of 98\%, generally satisfying the requirements in both speed and accuracy on on-board mobile terminals.



### Data Uncertainty Learning in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.11339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11339v1)
- **Published**: 2020-03-25 11:40:38+00:00
- **Updated**: 2020-03-25 11:40:38+00:00
- **Authors**: Jie Chang, Zhonghao Lan, Changmao Cheng, Yichen Wei
- **Comment**: Accepted as poster by CVPR2020
- **Journal**: None
- **Summary**: Modeling data uncertainty is important for noisy images, but seldom explored for face recognition. The pioneer work, PFE, considers uncertainty by modeling each face image embedding as a Gaussian distribution. It is quite effective. However, it uses fixed feature (mean of the Gaussian) from an existing model. It only estimates the variance and relies on an ad-hoc and costly metric. Thus, it is not easy to use. It is unclear how uncertainty affects feature learning.   This work applies data uncertainty learning to face recognition, such that the feature (mean) and uncertainty (variance) are learnt simultaneously, for the first time. Two learning methods are proposed. They are easy to use and outperform existing deterministic methods as well as PFE on challenging unconstrained scenarios. We also provide insightful analysis on how incorporating uncertainty estimation helps reducing the adverse effects of noisy samples and affects the feature learning.



### Circumventing Outliers of AutoAugment with Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2003.11342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11342v1)
- **Published**: 2020-03-25 11:51:41+00:00
- **Updated**: 2020-03-25 11:51:41+00:00
- **Authors**: Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, Qi Tian
- **Comment**: 18 pages, 4 figures
- **Journal**: None
- **Summary**: AutoAugment has been a powerful algorithm that improves the accuracy of many vision tasks, yet it is sensitive to the operator space as well as hyper-parameters, and an improper setting may degenerate network optimization. This paper delves deep into the working mechanism, and reveals that AutoAugment may remove part of discriminative information from the training image and so insisting on the ground-truth label is no longer the best option. To relieve the inaccuracy of supervision, we make use of knowledge distillation that refers to the output of a teacher model to guide network training. Experiments are performed in standard image classification benchmarks, and demonstrate the effectiveness of our approach in suppressing noise of data augmentation and stabilizing training. Upon the cooperation of knowledge distillation and AutoAugment, we claim the new state-of-the-art on ImageNet classification with a top-1 accuracy of 85.8%.



### Fisheye Distortion Rectification from Deep Straight Lines
- **Arxiv ID**: http://arxiv.org/abs/2003.11386v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2003.11386v1)
- **Published**: 2020-03-25 13:20:00+00:00
- **Updated**: 2020-03-25 13:20:00+00:00
- **Authors**: Zhu-Cun Xue, Nan Xue, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel line-aware rectification network (LaRecNet) to address the problem of fisheye distortion rectification based on the classical observation that straight lines in 3D space should be still straight in image planes. Specifically, the proposed LaRecNet contains three sequential modules to (1) learn the distorted straight lines from fisheye images; (2) estimate the distortion parameters from the learned heatmaps and the image appearance; and (3) rectify the input images via a proposed differentiable rectification layer. To better train and evaluate the proposed model, we create a synthetic line-rich fisheye (SLF) dataset that contains the distortion parameters and well-annotated distorted straight lines of fisheye images. The proposed method enables us to simultaneously calibrate the geometric distortion parameters and rectify fisheye images. Extensive experiments demonstrate that our model achieves state-of-the-art performance in terms of both geometric accuracy and image quality on several evaluation metrics. In particular, the images rectified by LaRecNet achieve an average reprojection error of 0.33 pixels on the SLF dataset and produce the highest peak signal-to-noise ratio (PSNR) and structure similarity index (SSIM) compared with the groundtruth.



### End-to-End Entity Classification on Multimodal Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/2003.12383v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.12383v1)
- **Published**: 2020-03-25 14:57:52+00:00
- **Updated**: 2020-03-25 14:57:52+00:00
- **Authors**: W. X. Wilcke, P. Bloem, V. de Boer, R. H. van t Veer, F. A. H. van Harmelen
- **Comment**: Submitted to the 17th International Conference on Principles of
  Knowledge Representation and Reasoning (2020)
- **Journal**: None
- **Summary**: End-to-end multimodal learning on knowledge graphs has been left largely unaddressed. Instead, most end-to-end models such as message passing networks learn solely from the relational information encoded in graphs' structure: raw values, or literals, are either omitted completely or are stripped from their values and treated as regular nodes. In either case we lose potentially relevant information which could have otherwise been exploited by our learning methods. To avoid this, we must treat literals and non-literals as separate cases. We must also address each modality separately and accordingly: numbers, texts, images, geometries, et cetera. We propose a multimodal message passing network which not only learns end-to-end from the structure of graphs, but also from their possibly divers set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including images and geometries, which are projected into a joint representation space together with their relational information. We demonstrate our model on a node classification task, and evaluate the effect that each modality has on the overall performance. Our result supports our hypothesis that including information from multiple modalities can help our models obtain a better overall performance.



### Commentaries on "Learning Sensorimotor Control with Neuromorphic Sensors: Toward Hyperdimensional Active Perception" [Science Robotics Vol. 4 Issue 30 (2019) 1-10
- **Arxiv ID**: http://arxiv.org/abs/2003.11458v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11458v1)
- **Published**: 2020-03-25 15:53:58+00:00
- **Updated**: 2020-03-25 15:53:58+00:00
- **Authors**: Denis Kleyko, Ross W. Gayler, Evgeny Osipov
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: This correspondence comments on the findings reported in a recent Science Robotics article by Mitrokhin et al. [1]. The main goal of this commentary is to expand on some of the issues touched on in that article. Our experience is that hyperdimensional computing is very different from other approaches to computation and that it can take considerable exposure to its concepts before attaining practically useful understanding. Therefore, in order to provide an overview of the area to the first time reader of [1], the commentary includes a brief historic overview as well as connects the findings of the article to a larger body of literature existing in the area.



### PiP: Planning-informed Trajectory Prediction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2003.11476v2
- **DOI**: 10.1007/978-3-030-58589-1_36
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.11476v2)
- **Published**: 2020-03-25 16:09:54+00:00
- **Updated**: 2021-01-18 06:14:56+00:00
- **Authors**: Haoran Song, Wenchao Ding, Yuxuan Chen, Shaojie Shen, Michael Yu Wang, Qifeng Chen
- **Comment**: European Conference on Computer Vision (ECCV) 2020; Project page at
  http://haoran-song.github.io/planning-informed-prediction
- **Journal**: None
- **Summary**: It is critical to predict the motion of surrounding vehicles for self-driving planning, especially in a socially compliant and flexible way. However, future prediction is challenging due to the interaction and uncertainty in driving behaviors. We propose planning-informed trajectory prediction (PiP) to tackle the prediction problem in the multi-agent setting. Our approach is differentiated from the traditional manner of prediction, which is only based on historical information and decoupled with planning. By informing the prediction process with the planning of ego vehicle, our method achieves the state-of-the-art performance of multi-agent forecasting on highway datasets. Moreover, our approach enables a novel pipeline which couples the prediction and planning, by conditioning PiP on multiple candidate trajectories of the ego vehicle, which is highly beneficial for autonomous driving in interactive scenarios.



### Not all domains are equally complex: Adaptive Multi-Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.11504v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.11504v1)
- **Published**: 2020-03-25 17:16:00+00:00
- **Updated**: 2020-03-25 17:16:00+00:00
- **Authors**: Ali Senhaji, Jenni Raitoharju, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches are highly specialized and require training separate models for different tasks. Multi-domain learning looks at ways to learn a multitude of different tasks, each coming from a different domain, at once. The most common approach in multi-domain learning is to form a domain agnostic model, the parameters of which are shared among all domains, and learn a small number of extra domain-specific parameters for each individual new domain. However, different domains come with different levels of difficulty; parameterizing the models of all domains using an augmented version of the domain agnostic model leads to unnecessarily inefficient solutions, especially for easy to solve tasks. We propose an adaptive parameterization approach to deep neural networks for multi-domain learning. The proposed approach performs on par with the original approach while reducing by far the number of parameters, leading to efficient multi-domain learning solutions.



### Visual-Inertial Telepresence for Aerial Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.11509v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11509v2)
- **Published**: 2020-03-25 17:26:03+00:00
- **Updated**: 2020-06-20 17:41:40+00:00
- **Authors**: Jongseok Lee, Ribin Balachandran, Yuri S. Sarkisov, Marco De Stefano, Andre Coelho, Kashmira Shinde, Min Jun Kim, Rudolph Triebel, Konstantin Kondak
- **Comment**: Accepted to International Conference on Robotics and Automation
  (ICRA) 2020, IEEE copyright, 8 pages, 10 figures
- **Journal**: None
- **Summary**: This paper presents a novel telepresence system for enhancing aerial manipulation capabilities. It involves not only a haptic device, but also a virtual reality that provides a 3D visual feedback to a remotely-located teleoperator in real-time. We achieve this by utilizing onboard visual and inertial sensors, an object tracking algorithm and a pre-generated object database. As the virtual reality has to closely match the real remote scene, we propose an extension of a marker tracking algorithm with visual-inertial odometry. Both indoor and outdoor experiments show benefits of our proposed system in achieving advanced aerial manipulation tasks, namely grasping, placing, force exertion and peg-in-hole insertion.



### Improved Techniques for Training Single-Image GANs
- **Arxiv ID**: http://arxiv.org/abs/2003.11512v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11512v2)
- **Published**: 2020-03-25 17:33:25+00:00
- **Updated**: 2020-11-17 10:55:13+00:00
- **Authors**: Tobias Hinz, Matthew Fisher, Oliver Wang, Stefan Wermter
- **Comment**: WACV 2021. Code and supplementary material available at
  https://github.com/tohinz/ConSinGAN
- **Journal**: None
- **Summary**: Recently there has been an interest in the potential of learning generative models from a single image, as opposed to from a large dataset. This task is of practical significance, as it means that generative models can be used in domains where collecting a large dataset is not feasible. However, training a model capable of generating realistic images from only a single sample is a difficult problem. In this work, we conduct a number of experiments to understand the challenges of training these methods and propose some best practices that we found allowed us to generate improved results over previous work in this space. One key piece is that unlike prior single image generation methods, we concurrently train several stages in a sequential multi-stage manner, allowing us to learn models with fewer stages of increasing image resolution. Compared to a recent state of the art baseline, our model is up to six times faster to train, has fewer parameters, and can better capture the global structure of images.



### Training Binary Neural Networks with Real-to-Binary Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2003.11535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11535v1)
- **Published**: 2020-03-25 17:54:38+00:00
- **Updated**: 2020-03-25 17:54:38+00:00
- **Authors**: Brais Martinez, Jing Yang, Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: This paper shows how to train binary networks to within a few percent points ($\sim 3-5 \%$) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed advances and carefully adjusting the optimization procedure. Secondly, we show that by attempting to minimize the discrepancy between the output of the binary and the corresponding real-valued convolution, additional significant accuracy gains can be obtained. We materialize this idea in two complementary ways: (1) with a loss function, during training, by matching the spatial attention maps computed at the output of the binary and real-valued convolutions, and (2) in a data-driven manner, by using the real-valued activations, available during inference prior to the binarization process, for re-scaling the activations right after the binary convolution. Finally, we show that, when putting all of our improvements together, the proposed model beats the current state of the art by more than 5% top-1 accuracy on ImageNet and reduces the gap to its real-valued counterpart to less than 3% and 5% top-1 accuracy on CIFAR-100 and ImageNet respectively when using a ResNet-18 architecture. Code available at https://github.com/brais-martinez/real2binary.



### HP2IFS: Head Pose estimation exploiting Partitioned Iterated Function Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.11536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11536v1)
- **Published**: 2020-03-25 17:56:45+00:00
- **Updated**: 2020-03-25 17:56:45+00:00
- **Authors**: Carmen Bisogni, Michele Nappi, Chiara Pero, Stefano Ricciardi
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the actual head orientation from 2D images, with regard to its three degrees of freedom, is a well known problem that is highly significant for a large number of applications involving head pose knowledge. Consequently, this topic has been tackled by a plethora of methods and algorithms the most part of which exploits neural networks. Machine learning methods, indeed, achieve accurate head rotation values yet require an adequate training stage and, to that aim, a relevant number of positive and negative examples. In this paper we take a different approach to this topic by using fractal coding theory and particularly Partitioned Iterated Function Systems to extract the fractal code from the input head image and to compare this representation to the fractal code of a reference model through Hamming distance. According to experiments conducted on both the BIWI and the AFLW2000 databases, the proposed PIFS based head pose estimation method provides accurate yaw/pitch/roll angular values, with a performance approaching that of state of the art of machine-learning based algorithms and exceeding most of non-training based approaches.



### Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?
- **Arxiv ID**: http://arxiv.org/abs/2003.11539v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.11539v2)
- **Published**: 2020-03-25 17:58:42+00:00
- **Updated**: 2020-06-17 08:11:10+00:00
- **Authors**: Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, Phillip Isola
- **Comment**: First two authors contributed equally. Project Page:
  https://people.csail.mit.edu/yuewang/projects/rfs/ Code:
  http://github.com/WangYueFt/rfs/
- **Journal**: None
- **Summary**: The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/WangYueFt/rfs/.



### Learning What to Learn for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.11540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11540v2)
- **Published**: 2020-03-25 17:58:43+00:00
- **Updated**: 2020-05-01 16:10:19+00:00
- **Authors**: Goutam Bhat, Felix Jremo Lawin, Martin Danelljan, Andreas Robinson, Michael Felsberg, Luc Van Gool, Radu Timofte
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Video object segmentation (VOS) is a highly challenging problem, since the target object is only defined during inference with a given first-frame reference mask. The problem of how to capture and utilize this limited target information remains a fundamental research question. We address this by introducing an end-to-end trainable VOS architecture that integrates a differentiable few-shot learning module. This internal learner is designed to predict a powerful parametric model of the target by minimizing a segmentation error in the first frame. We further go beyond standard few-shot learning techniques by learning what the few-shot learner should learn. This allows us to achieve a rich internal representation of the target in the current frame, significantly increasing the segmentation accuracy of our approach. We perform extensive experiments on multiple benchmarks. Our approach sets a new state-of-the-art on the large-scale YouTube-VOS 2018 dataset by achieving an overall score of 81.5, corresponding to a 2.6% relative improvement over the previous best result.



### Interval Neural Networks: Uncertainty Scores
- **Arxiv ID**: http://arxiv.org/abs/2003.11566v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML, I.5.1; I.4.5; J.3; I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2003.11566v1)
- **Published**: 2020-03-25 18:03:51+00:00
- **Updated**: 2020-03-25 18:03:51+00:00
- **Authors**: Luis Oala, Cosmas Hei, Jan Macdonald, Maximilian Mrz, Wojciech Samek, Gitta Kutyniok
- **Comment**: LO and CH contributed equally
- **Journal**: None
- **Summary**: We propose a fast, non-Bayesian method for producing uncertainty scores in the output of pre-trained deep neural networks (DNNs) using a data-driven interval propagating network. This interval neural network (INN) has interval valued parameters and propagates its input using interval arithmetic. The INN produces sensible lower and upper bounds encompassing the ground truth. We provide theoretical justification for the validity of these bounds. Furthermore, its asymmetric uncertainty scores offer additional, directional information beyond what Gaussian-based, symmetric variance estimation can provide. We find that noise in the data is adequately captured by the intervals produced with our method. In numerical experiments on an image reconstruction task, we demonstrate the practical utility of INNs as a proxy for the prediction error in comparison to two state-of-the-art uncertainty quantification methods. In summary, INNs produce fast, theoretically justified uncertainty scores for DNNs that are easy to interpret, come with added information and pose as improved error proxies - features that may prove useful in advancing the usability of DNNs especially in sensitive applications such as health care.



### Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2003.11571v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11571v2)
- **Published**: 2020-03-25 18:16:05+00:00
- **Updated**: 2021-03-26 19:57:02+00:00
- **Authors**: Wei Sun, Tianfu Wu
- **Comment**: 16 pages (w/o ref), 15 figures)
- **Journal**: None
- **Summary**: With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable inputs. This paper focuses on a recent emerged task, layout-to-image, to learn generative models that are capable of synthesizing photo-realistic images from spatial layout (i.e., object bounding boxes configured in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors). This paper first proposes an intuitive paradigm for the task, layout-to-mask-to-image, to learn to unfold object masks of given bounding boxes in an input layout to bridge the gap between the input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks for the proposed layout-to-mask-to-image with style control at both image and mask levels. Object masks are learned from the input layout and iteratively refined along stages in the generator network. Style control at the image level is the same as in vanilla GANs, while style control at the object mask level is realized by a proposed novel feature normalization scheme, Instance-Sensitive and Layout-Aware Normalization. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained.



### PyMatting: A Python Library for Alpha Matting
- **Arxiv ID**: http://arxiv.org/abs/2003.12382v1
- **DOI**: 10.21105/joss.02481
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.12382v1)
- **Published**: 2020-03-25 18:46:23+00:00
- **Updated**: 2020-03-25 18:46:23+00:00
- **Authors**: Thomas Germer, Tobias Uelwer, Stefan Conrad, Stefan Harmeling
- **Comment**: None
- **Journal**: Journal of Open Source Software (2020), 5(54), 2481
- **Summary**: An important step of many image editing tasks is to extract specific objects from an image in order to place them in a scene of a movie or compose them onto another background. Alpha matting describes the problem of separating the objects in the foreground from the background of an image given only a rough sketch. We introduce the PyMatting package for Python which implements various approaches to solve the alpha matting problem. Our toolbox is also able to extract the foreground of an image given the alpha matte. The implementation aims to be computationally efficient and easy to use. The source code of PyMatting is available under an open-source license at https://github.com/pymatting/pymatting.



### Exploring Long Tail Visual Relationship Recognition with Large Vocabulary
- **Arxiv ID**: http://arxiv.org/abs/2004.00436v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.10; I.5.0; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2004.00436v7)
- **Published**: 2020-03-25 19:03:29+00:00
- **Updated**: 2021-09-25 04:13:23+00:00
- **Authors**: Sherif Abdelkarim, Aniket Agarwal, Panos Achlioptas, Jun Chen, Jiaji Huang, Boyang Li, Kenneth Church, Mohamed Elhoseiny
- **Comment**: None
- **Journal**: None
- **Summary**: Several approaches have been proposed in recent literature to alleviate the long-tail problem, mainly in object classification tasks. In this paper, we make the first large-scale study concerning the task of Long-Tail Visual Relationship Recognition (LTVRR). LTVRR aims at improving the learning of structured visual relationships that come from the long-tail (e.g., "rabbit grazing on grass"). In this setup, the subject, relation, and object classes each follow a long-tail distribution. To begin our study and make a future benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets. We use these benchmarks to study the performance of several state-of-the-art long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top of existing models and despite being simple, our results show that they can remarkably improve the performance, especially on tail classes. Benchmarks, code, and models have been made available at: https://github.com/Vision-CAIR/LTVRR.



### Learning Multi-Scale Photo Exposure Correction
- **Arxiv ID**: http://arxiv.org/abs/2003.11596v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.11596v3)
- **Published**: 2020-03-25 19:33:51+00:00
- **Updated**: 2021-03-30 05:19:09+00:00
- **Authors**: Mahmoud Afifi, Konstantinos G. Derpanis, Bjrn Ommer, Michael S. Brown
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Capturing photographs with wrong exposures remains a major source of errors in camera-based imaging. Exposure problems are categorized as either: (i) overexposed, where the camera exposure was too long, resulting in bright and washed-out image regions, or (ii) underexposed, where the exposure was too short, resulting in dark regions. Both under- and overexposure greatly reduce the contrast and visual appeal of an image. Prior work mainly focuses on underexposed images or general image enhancement. In contrast, our proposed method targets both over- and underexposure errors in photographs. We formulate the exposure correction problem as two main sub-problems: (i) color enhancement and (ii) detail enhancement. Accordingly, we propose a coarse-to-fine deep neural network (DNN) model, trainable in an end-to-end manner, that addresses each sub-problem separately. A key aspect of our solution is a new dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image. Our method achieves results on par with existing state-of-the-art methods on underexposed images and yields significant improvements for images suffering from overexposure errors.



### COVID-19 Image Data Collection
- **Arxiv ID**: http://arxiv.org/abs/2003.11597v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.11597v1)
- **Published**: 2020-03-25 19:37:25+00:00
- **Updated**: 2020-03-25 19:37:25+00:00
- **Authors**: Joseph Paul Cohen, Paul Morrison, Lan Dao
- **Comment**: Dataset available here:
  https://github.com/ieee8023/covid-chestxray-dataset
- **Journal**: None
- **Summary**: This paper describes the initial COVID-19 open image data collection. It was created by assembling medical images from websites and publications and currently contains 123 frontal view X-rays.



### Covid-19: Automatic detection from X-Ray images utilizing Transfer Learning with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.11617v1
- **DOI**: 10.1007/s13246-020-00865-4
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2003.11617v1)
- **Published**: 2020-03-25 20:34:30+00:00
- **Updated**: 2020-03-25 20:34:30+00:00
- **Authors**: Ioannis D. Apostolopoulos, Tzani Bessiana
- **Comment**: None
- **Journal**: Physical and Engineering Sciences in Medicine 43:635-40;2020
- **Summary**: In this study, a dataset of X-Ray images from patients with common pneumonia, Covid-19, and normal incidents was utilized for the automatic detection of the Coronavirus. The aim of the study is to evaluate the performance of state-of-the-art Convolutional Neural Network architectures proposed over recent years for medical image classification. Specifically, the procedure called transfer learning was adopted. With transfer learning, the detection of various abnormalities in small medical image datasets is an achievable target, often yielding remarkable results. The dataset utilized in this experiment is a collection of 1427 X-Ray images. 224 images with confirmed Covid-19, 700 images with confirmed common pneumonia, and 504 images of normal conditions are included. The data was collected from the available X-Ray images on public medical repositories. With transfer learning, an overall accuracy of 97.82% in the detection of Covid-19 is achieved.



### VIOLIN: A Large-Scale Dataset for Video-and-Language Inference
- **Arxiv ID**: http://arxiv.org/abs/2003.11618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2003.11618v1)
- **Published**: 2020-03-25 20:39:05+00:00
- **Updated**: 2020-03-25 20:39:05+00:00
- **Authors**: Jingzhou Liu, Wenhu Chen, Yu Cheng, Zhe Gan, Licheng Yu, Yiming Yang, Jingjing Liu
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: We introduce a new task, Video-and-Language Inference, for joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. A new large-scale dataset, named Violin (VIdeO-and-Language INference), is introduced for this task, which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels. In order to address our new multimodal inference task, a model is required to possess sophisticated reasoning skills, from surface-level grounding (e.g., identifying objects and characters in the video) to in-depth commonsense reasoning (e.g., inferring causal relations of events in the video). We present a detailed analysis of the dataset and an extensive evaluation over many strong baselines, providing valuable insights on the challenges of this new task.



### Deep Grouping Model for Unified Perceptual Parsing
- **Arxiv ID**: http://arxiv.org/abs/2003.11647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11647v1)
- **Published**: 2020-03-25 21:16:09+00:00
- **Updated**: 2020-03-25 21:16:09+00:00
- **Authors**: Zhiheng Li, Wenxuan Bao, Jiayang Zheng, Chenliang Xu
- **Comment**: Accepted by CVPR 2020
- **Journal**: CVPR 2020
- **Summary**: The perceptual-based grouping process produces a hierarchical and compositional image representation that helps both human and machine vision systems recognize heterogeneous visual concepts. Examples can be found in the classical hierarchical superpixel segmentation or image parsing works. However, the grouping process is largely overlooked in modern CNN-based image segmentation networks due to many challenges, including the inherent incompatibility between the grid-shaped CNN feature map and the irregular-shaped perceptual grouping hierarchy. Overcoming these challenges, we propose a deep grouping model (DGM) that tightly marries the two types of representations and defines a bottom-up and a top-down process for feature exchanging. When evaluating the model on the recent Broden+ dataset for the unified perceptual parsing task, it achieves state-of-the-art results while having a small computational overhead compared to other contextual-based segmentation models. Furthermore, the DGM has better interpretability compared with modern CNN methods.



### iTAML: An Incremental Task-Agnostic Meta-learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2003.11652v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.11652v1)
- **Published**: 2020-03-25 21:42:48+00:00
- **Updated**: 2020-03-25 21:42:48+00:00
- **Authors**: Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous meta-learning techniques, our approach is task-agnostic. When presented with a continuum of data, our model automatically identifies the task and quickly adapts to it with just a single update. We perform extensive experiments on five datasets in a class-incremental setting, leading to significant improvements over the state of the art methods (e.g., a 21.3% boost on CIFAR100 with 10 incremental tasks). Specifically, on large-scale datasets that generally prove difficult cases for incremental learning, our approach delivers absolute gains as high as 19.1% and 7.4% on ImageNet and MS-Celeb datasets, respectively.



### DeepStrip: High Resolution Boundary Refinement
- **Arxiv ID**: http://arxiv.org/abs/2003.11670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.11670v1)
- **Published**: 2020-03-25 22:44:48+00:00
- **Updated**: 2020-03-25 22:44:48+00:00
- **Authors**: Peng Zhou, Brian Price, Scott Cohen, Gregg Wilensky, Larry S. Davis
- **Comment**: None
- **Journal**: CVPR 2020
- **Summary**: In this paper, we target refining the boundaries in high resolution images given low resolution masks. For memory and computation efficiency, we propose to convert the regions of interest into strip images and compute a boundary prediction in the strip domain. To detect the target boundary, we present a framework with two prediction layers. First, all potential boundaries are predicted as an initial prediction and then a selection layer is used to pick the target boundary and smooth the result. To encourage accurate prediction, a loss which measures the boundary distance in the strip domain is introduced. In addition, we enforce a matching consistency and C0 continuity regularization to the network to reduce false alarms. Extensive experiments on both public and a newly created high resolution dataset strongly validate our approach.




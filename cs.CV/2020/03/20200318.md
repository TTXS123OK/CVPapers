# Arxiv Papers in cs.CV on 2020-03-18
### Watching the World Go By: Representation Learning from Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.07990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.07990v2)
- **Published**: 2020-03-18 00:07:21+00:00
- **Updated**: 2020-05-07 17:23:14+00:00
- **Authors**: Daniel Gordon, Kiana Ehsani, Dieter Fox, Ali Farhadi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at https://www.github.com/danielgordon10/vince



### Graph Attention Network based Pruning for Reconstructing 3D Liver Vessel Morphology from Contrasted CT Images
- **Arxiv ID**: http://arxiv.org/abs/2003.07999v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07999v1)
- **Published**: 2020-03-18 01:03:33+00:00
- **Updated**: 2020-03-18 01:03:33+00:00
- **Authors**: Donghao Zhang, Siqi Liu, Shikha Chaganti, Eli Gibson, Zhoubing Xu, Sasa Grbic, Weidong Cai, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: With the injection of contrast material into blood vessels, multi-phase contrasted CT images can enhance the visibility of vessel networks in the human body. Reconstructing the 3D geometric morphology of liver vessels from the contrasted CT images can enable multiple liver preoperative surgical planning applications. Automatic reconstruction of liver vessel morphology remains a challenging problem due to the morphological complexity of liver vessels and the inconsistent vessel intensities among different multi-phase contrasted CT images. On the other side, high integrity is required for the 3D reconstruction to avoid decision making biases. In this paper, we propose a framework for liver vessel morphology reconstruction using both a fully convolutional neural network and a graph attention network. A fully convolutional neural network is first trained to produce the liver vessel centerline heatmap. An over-reconstructed liver vessel graph model is then traced based on the heatmap using an image processing based algorithm. We use a graph attention network to prune the false-positive branches by predicting the presence probability of each segmented branch in the initial reconstruction using the aggregated CNN features. We evaluated the proposed framework on an in-house dataset consisting of 418 multi-phase abdomen CT images with contrast. The proposed graph network pruning improves the overall reconstruction F1 score by 6.4% over the baseline. It also outperformed the other state-of-the-art curvilinear structure reconstruction algorithms.



### AMIL: Adversarial Multi Instance Learning for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2003.08002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08002v1)
- **Published**: 2020-03-18 01:22:16+00:00
- **Updated**: 2020-03-18 01:22:16+00:00
- **Authors**: Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation has an important impact on a wide range of applications from human-computer interface to surveillance and content-based video retrieval. For human pose estimation, joint obstructions and overlapping upon human bodies result in departed pose estimation. To address these problems, by integrating priors of the structure of human bodies, we present a novel structure-aware network to discreetly consider such priors during the training of the network. Typically, learning such constraints is a challenging task. Instead, we propose generative adversarial networks as our learning model in which we design two residual multiple instance learning (MIL) models with the identical architecture, one is used as the generator and the other one is used as the discriminator. The discriminator task is to distinguish the actual poses from the fake ones. If the pose generator generates the results that the discriminator is not able to distinguish from the real ones, the model has successfully learnt the priors. In the proposed model, the discriminator differentiates the ground-truth heatmaps from the generated ones, and later the adversarial loss back-propagates to the generator. Such procedure assists the generator to learn reasonable body configurations and is proved to be advantageous to improve the pose estimation accuracy. Meanwhile, we propose a novel function for MIL. It is an adjustable structure for both instance selection and modeling to appropriately pass the information between instances in a single bag. In the proposed residual MIL neural network, the pooling action adequately updates the instance contribution to its bag. The proposed adversarial residual multi-instance neural network that is based on pooling has been validated on two datasets for the human pose estimation task and successfully outperforms the other state-of-arts models.



### ScanSSD: Scanning Single Shot Detector for Mathematical Formulas in PDF Document Images
- **Arxiv ID**: http://arxiv.org/abs/2003.08005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08005v1)
- **Published**: 2020-03-18 01:32:44+00:00
- **Updated**: 2020-03-18 01:32:44+00:00
- **Authors**: Parag Mali, Puneeth Kukkadapu, Mahshad Mahdavi, Richard Zanibbi
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: We introduce the Scanning Single Shot Detector (ScanSSD) for locating math formulas offset from text and embedded in textlines. ScanSSD uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results. For our experiments we use the TFD-ICDAR2019v2 dataset, a modification of the GTDB scanned math article collection. ScanSSD detects characters in formulas with high accuracy, obtaining a 0.926 f-score, and detects formulas with high recall overall. Detection errors are largely minor, such as splitting formulas at large whitespace gaps (e.g., for variable constraints) and merging formulas on adjacent textlines. Formula detection f-scores of 0.796 (IOU $\geq0.5$) and 0.733 (IOU $\ge 0.75$) are obtained. Our data, evaluation tools, and code are publicly available.



### A Dynamic Reduction Network for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.08013v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2003.08013v1)
- **Published**: 2020-03-18 02:10:12+00:00
- **Updated**: 2020-03-18 02:10:12+00:00
- **Authors**: Lindsey Gray, Thomas Klijnsma, Shamik Ghosh
- **Comment**: 4 pages, 2 figures, to be updated
- **Journal**: None
- **Summary**: Classifying whole images is a classic problem in machine learning, and graph neural networks are a powerful methodology to learn highly irregular geometries. It is often the case that certain parts of a point cloud are more important than others when determining overall classification. On graph structures this started by pooling information at the end of convolutional filters, and has evolved to a variety of staged pooling techniques on static graphs. In this paper, a dynamic graph formulation of pooling is introduced that removes the need for predetermined graph structure. It achieves this by dynamically learning the most important relationships between data via an intermediate clustering. The network architecture yields interesting results considering representation size and efficiency. It also adapts easily to a large number of tasks from image classification to energy regression in high energy particle physics.



### Applying r-spatiogram in object tracking for occlusion handling
- **Arxiv ID**: http://arxiv.org/abs/2003.08021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08021v1)
- **Published**: 2020-03-18 02:42:51+00:00
- **Updated**: 2020-03-18 02:42:51+00:00
- **Authors**: Niloufar Salehi Dastjerdi, M. Omair Ahmad
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking is one of the most important problems in computer vision. The aim of video tracking is to extract the trajectories of a target or object of interest, i.e. accurately locate a moving target in a video sequence and discriminate target from non-targets in the feature space of the sequence. So, feature descriptors can have significant effects on such discrimination. In this paper, we use the basic idea of many trackers which consists of three main components of the reference model, i.e., object modeling, object detection and localization, and model updating. However, there are major improvements in our system. Our forth component, occlusion handling, utilizes the r-spatiogram to detect the best target candidate. While spatiogram contains some moments upon the coordinates of the pixels, r-spatiogram computes region-based compactness on the distribution of the given feature in the image that captures richer features to represent the objects. The proposed research develops an efficient and robust way to keep tracking the object throughout video sequences in the presence of significant appearance variations and severe occlusions. The proposed method is evaluated on the Princeton RGBD tracking dataset considering sequences with different challenges and the obtained results demonstrate the effectiveness of the proposed method.



### Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario
- **Arxiv ID**: http://arxiv.org/abs/2003.08024v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08024v3)
- **Published**: 2020-03-18 03:04:03+00:00
- **Updated**: 2022-06-16 12:53:28+00:00
- **Authors**: Yu Tian, Kunbo Zhang, Leyuan Wang, Zhenan Sun
- **Comment**: 14pages,8figures
- **Journal**: None
- **Summary**: Face anti-spoofing is the key to preventing security breaches in biometric recognition applications. Existing software-based and hardware-based face liveness detection methods are effective in constrained environments or designated datasets only. Deep learning method using RGB and infrared images demands a large amount of training data for new attacks. In this paper, we present a face anti-spoofing method in a real-world scenario by automatic learning the physical characteristics in polarization images of a real face compared to a deceptive attack. A computational framework is developed to extract and classify the unique face features using convolutional neural networks and SVM together. Our real-time polarized face anti-spoofing (PAAS) detection method uses a on-chip integrated polarization imaging sensor with optimized processing algorithms. Extensive experiments demonstrate the advantages of the PAAS technique to counter diverse face spoofing attacks (print, replay, mask) in uncontrolled indoor and outdoor conditions by learning polarized face images of 33 people. A four-directional polarized face image dataset is released to inspire future applications within biometric anti-spoofing field.



### MUTATT: Visual-Textual Mutual Guidance for Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2003.08027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08027v2)
- **Published**: 2020-03-18 03:14:58+00:00
- **Updated**: 2020-03-20 05:01:15+00:00
- **Authors**: Shuai Wang, Fan Lyu, Wei Feng, Song Wang
- **Comment**: 6 pages, Accepted by ICME-2020
- **Journal**: None
- **Summary**: Referring expression comprehension (REC) aims to localize a text-related region in a given image by a referring expression in natural language. Existing methods focus on how to build convincing visual and language representations independently, which may significantly isolate visual and language information. In this paper, we argue that for REC the referring expression and the target region are semantically correlated and subject, location and relationship consistency exist between vision and language.On top of this, we propose a novel approach called MutAtt to construct mutual guidance between vision and language, which treat vision and language equally thus yield compact information matching. Specifically, for each module of subject, location and relationship, MutAtt builds two kinds of attention-based mutual guidance strategies. One strategy is to generate vision-guided language embedding for the sake of matching relevant visual feature. The other reversely generates language-guided visual feature to match relevant language embedding. This mutual guidance strategy can effectively guarantees the vision-language consistency in three modules. Experiments on three popular REC datasets demonstrate that the proposed approach outperforms the current state-of-the-art methods.



### Object-Based Image Coding: A Learning-Driven Revisit
- **Arxiv ID**: http://arxiv.org/abs/2003.08033v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08033v1)
- **Published**: 2020-03-18 04:00:17+00:00
- **Updated**: 2020-03-18 04:00:17+00:00
- **Authors**: Qi Xia, Haojie Liu, Zhan Ma
- **Comment**: ICME2020
- **Journal**: None
- **Summary**: The Object-Based Image Coding (OBIC) that was extensively studied about two decades ago, promised a vast application perspective for both ultra-low bitrate communication and high-level semantical content understanding, but it had rarely been used due to the inefficient compact representation of object with arbitrary shape. A fundamental issue behind is how to efficiently process the arbitrary-shaped objects at a fine granularity (e.g., feature element or pixel wise). To attack this, we have proposed to apply the element-wise masking and compression by devising an object segmentation network for image layer decomposition, and parallel convolution-based neural image compression networks to process masked foreground objects and background scene separately. All components are optimized in an end-to-end learning framework to intelligently weigh their (e.g., object and background) contributions for visually pleasant reconstruction. We have conducted comprehensive experiments to evaluate the performance on PASCAL VOC dataset at a very low bitrate scenario (e.g., $\lesssim$0.1 bits per pixel - bpp) which have demonstrated noticeable subjective quality improvement compared with JPEG2K, HEVC-based BPG and another learned image compression method. All relevant materials are made publicly accessible at https://njuvision.github.io/Neural-Object-Coding/.



### Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.08040v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08040v3)
- **Published**: 2020-03-18 04:43:25+00:00
- **Updated**: 2020-06-09 17:56:27+00:00
- **Authors**: Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas S. Huang, Humphrey Shi
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We consider the problem of unsupervised domain adaptation for semantic segmentation by easing the domain shift between the source domain (synthetic data) and the target domain (real data) in this work. State-of-the-art approaches prove that performing semantic-level alignment is helpful in tackling the domain shift issue. Based on the observation that stuff categories usually share similar appearances across images of different domains while things (i.e. object instances) have much larger differences, we propose to improve the semantic-level alignment with different strategies for stuff regions and for things: 1) for the stuff categories, we generate feature representation for each class and conduct the alignment operation from the target domain to the source domain; 2) for the thing categories, we generate feature representation for each individual instance and encourage the instance in the target domain to align with the most similar one in the source domain. In this way, the individual differences within thing categories will also be considered to alleviate over-alignment. In addition to our proposed method, we further reveal the reason why the current adversarial loss is often unstable in minimizing the distribution discrepancy and show that our method can help ease this issue by minimizing the most similar stuff and instance features between the source and the target domains. We conduct extensive experiments in two unsupervised domain adaptation tasks, i.e. GTA5 to Cityscapes and SYNTHIA to Cityscapes, and achieve the new state-of-the-art segmentation accuracy.



### STH: Spatio-Temporal Hybrid Convolution for Efficient Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.08042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08042v1)
- **Published**: 2020-03-18 04:46:30+00:00
- **Updated**: 2020-03-18 04:46:30+00:00
- **Authors**: Xu Li, Jingwen Wang, Lin Ma, Kaihao Zhang, Fengzong Lian, Zhanhui Kang, Jinjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Effective and Efficient spatio-temporal modeling is essential for action recognition. Existing methods suffer from the trade-off between model performance and model complexity. In this paper, we present a novel Spatio-Temporal Hybrid Convolution Network (denoted as "STH") which simultaneously encodes spatial and temporal video information with a small parameter cost. Different from existing works that sequentially or parallelly extract spatial and temporal information with different convolutional layers, we divide the input channels into multiple groups and interleave the spatial and temporal operations in one convolutional layer, which deeply incorporates spatial and temporal clues. Such a design enables efficient spatio-temporal modeling and maintains a small model scale. STH-Conv is a general building block, which can be plugged into existing 2D CNN architectures such as ResNet and MobileNet by replacing the conventional 2D-Conv blocks (2D convolutions). STH network achieves competitive or even better performance than its competitors on benchmark datasets such as Something-Something (V1 & V2), Jester, and HMDB-51. Moreover, STH enjoys performance superiority over 3D CNNs while maintaining an even smaller parameter cost than 2D CNNs.



### Capsule GAN Using Capsule Network for Generator Architecture
- **Arxiv ID**: http://arxiv.org/abs/2003.08047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2003.08047v1)
- **Published**: 2020-03-18 05:14:51+00:00
- **Updated**: 2020-03-18 05:14:51+00:00
- **Authors**: Kanako Marusaki, Hiroshi Watanabe
- **Comment**: 7 pages and 8 figures
- **Journal**: None
- **Summary**: This paper presents Capsule GAN, a Generative adversarial network using Capsule Network not only in the discriminator but also in the generator. Recently, Generative adversarial networks (GANs) has been intensively studied. However, generating images by GANs is difficult. Therefore, GANs sometimes generate poor quality images. These GANs use convolutional neural networks (CNNs). However, CNNs have the defect that the relational information between features of the image may be lost. Capsule Network, proposed by Hinton in 2017, overcomes the defect of CNNs. Capsule GAN reported previously uses Capsule Network in the discriminator. However, instead of using Capsule Network, Capsule GAN reported in previous studies uses CNNs in generator architecture like DCGAN. This paper introduces two approaches to use Capsule Network in the generator. One is to use DigitCaps layer from the discriminator as the input to the generator. DigitCaps layer is the output layer of Capsule Network. It has the features of the input images of the discriminator. The other is to use the reverse operation of recognition process in Capsule Network in the generator. We compare Capsule GAN proposed in this paper with conventional GAN using CNN and Capsule GAN which uses Capsule Network in the discriminator only. The datasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN outperforms the GAN using CNN and the GAN using Capsule Network in the discriminator only. The architecture of Capsule GAN proposed in this paper is a basic architecture using Capsule Network. Therefore, we can apply the existing improvement techniques for GANs to Capsule GAN.



### Estimation of Orofacial Kinematics in Parkinson's Disease: Comparison of 2D and 3D Markerless Systems for Motion Tracking
- **Arxiv ID**: http://arxiv.org/abs/2003.08048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08048v1)
- **Published**: 2020-03-18 05:18:27+00:00
- **Updated**: 2020-03-18 05:18:27+00:00
- **Authors**: Diego L. Guarin, Aidan Dempster, Andrea Bandini, Yana Yunusova, Babak Taati
- **Comment**: 4 pages, 1 table
- **Journal**: None
- **Summary**: Orofacial deficits are common in people with Parkinson's disease (PD) and their evolution might represent an important biomarker of disease progression. We are developing an automated system for assessment of orofacial function in PD that can be used in-home or in-clinic and can provide useful and objective clinical information that informs disease management. Our current approach relies on color and depth cameras for the estimation of 3D facial movements. However, depth cameras are not commonly available, might be expensive, and require specialized software for control and data processing. The objective of this paper was to evaluate if depth cameras are needed to differentiate between healthy controls and PD patients based on features extracted from orofacial kinematics. Results indicate that 2D features, extracted from color cameras only, are as informative as 3D features, extracted from color and depth cameras, differentiating healthy controls from PD patients. These results pave the way for the development of a universal system for automatic and objective assessment of orofacial function in PD.



### Can AI decrypt fashion jargon for you?
- **Arxiv ID**: http://arxiv.org/abs/2003.08052v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.HC, I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2003.08052v1)
- **Published**: 2020-03-18 05:32:04+00:00
- **Updated**: 2020-03-18 05:32:04+00:00
- **Authors**: Yuan Shen, Shanduojiao Jiang, Muhammad Rizky Wellyanto, Ranjitha Kumar
- **Comment**: 5 pages, 6 figures, Accepted at workshop paper for AI4HCI at CHI2020
- **Journal**: None
- **Summary**: When people talk about fashion, they care about the underlying meaning of fashion concepts,e.g., style.For example, people ask questions like what features make this dress smart.However, the product descriptions in today fashion websites are full of domain specific and low level words. It is not clear to people how exactly those low level descriptions can contribute to a style or any high level fashion concept. In this paper, we proposed a data driven solution to address this concept understanding issues by leveraging a large number of existing product data on fashion sites. We first collected and categorized 1546 fashion keywords into 5 different fashion categories. Then, we collected a new fashion product dataset with 853,056 products in total. Finally, we trained a deep learning model that can explicitly predict and explain high level fashion concepts in a product image with its low level and domain specific fashion features.



### OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.08056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.08056v1)
- **Published**: 2020-03-18 05:52:10+00:00
- **Updated**: 2020-03-18 05:52:10+00:00
- **Authors**: Changhee Won, Hochang Seok, Zhaopeng Cui, Marc Pollefeys, Jongwoo Lim
- **Comment**: accepted by ICRA 2020
- **Journal**: None
- **Summary**: In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360 degrees coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.



### Deep Spatial Gradient and Temporal Depth Learning for Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2003.08061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08061v1)
- **Published**: 2020-03-18 06:11:20+00:00
- **Updated**: 2020-03-18 06:11:20+00:00
- **Authors**: Zezheng Wang, Zitong Yu, Chenxu Zhao, Xiangyu Zhu, Yunxiao Qin, Qiusheng Zhou, Feng Zhou, Zhen Lei
- **Comment**: Accepted by CVPR2020 (oral)
- **Journal**: None
- **Summary**: Face anti-spoofing is critical to the security of face recognition systems. Depth supervised learning has been proven as one of the most effective methods for face anti-spoofing. Despite the great success, most previous works still formulate the problem as a single-frame multi-task one by simply augmenting the loss with depth, while neglecting the detailed fine-grained information and the interplay between facial depths and moving patterns. In contrast, we design a new approach to detect presentation attacks from multiple frames based on two insights: 1) detailed discriminative clues (e.g., spatial gradient magnitude) between living and spoofing face may be discarded through stacked vanilla convolutions, and 2) the dynamics of 3D moving faces provide important clues in detecting the spoofing faces. The proposed method is able to capture discriminative details via Residual Spatial Gradient Block (RSGB) and encode spatio-temporal information from Spatio-Temporal Propagation Module (STPM) efficiently. Moreover, a novel Contrastive Depth Loss is presented for more accurate depth supervision. To assess the efficacy of our method, we also collect a Double-modal Anti-spoofing Dataset (DMAD) which provides actual depth for each sample. The experiments demonstrate that the proposed approach achieves state-of-the-art results on five benchmark datasets including OULU-NPU, SiW, CASIA-MFSD, Replay-Attack, and the new DMAD. Codes will be available at https://github.com/clks-wzz/FAS-SGTD.



### Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2003.08069v3
- **DOI**: 10.1109/TPAMI.2020.3024900
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08069v3)
- **Published**: 2020-03-18 07:10:44+00:00
- **Updated**: 2021-05-07 07:39:23+00:00
- **Authors**: Changxing Ding, Kan Wang, Pengfei Wang, Dacheng Tao
- **Comment**: Accepted Version to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Part-level representations are important for robust person re-identification (ReID), but in practice feature quality suffers due to the body part misalignment problem. In this paper, we present a robust, compact, and easy-to-use method called the Multi-task Part-aware Network (MPN), which is designed to extract semantically aligned part-level features from pedestrian images. MPN solves the body part misalignment problem via multi-task learning (MTL) in the training stage. More specifically, it builds one main task (MT) and one auxiliary task (AT) for each body part on the top of the same backbone model. The ATs are equipped with a coarse prior of the body part locations for training images. ATs then transfer the concept of the body parts to the MTs via optimizing the MT parameters to identify part-relevant channels from the backbone model. Concept transfer is accomplished by means of two novel alignment strategies: namely, parameter space alignment via hard parameter sharing and feature space alignment in a class-wise manner. With the aid of the learned high-quality parameters, MTs can independently extract semantically aligned part-level features from relevant channels in the testing stage. MPN has three key advantages: 1) it does not need to conduct body part detection in the inference stage; 2) its model is very compact and efficient for both training and testing; 3) in the training stage, it requires only coarse priors of body part locations, which are easy to obtain. Systematic experiments on four large-scale ReID databases demonstrate that MPN consistently outperforms state-of-the-art approaches by significant margins. Code is available at https://github.com/WangKan0128/MPN.



### Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2003.08073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08073v1)
- **Published**: 2020-03-18 07:21:09+00:00
- **Updated**: 2020-03-18 07:21:09+00:00
- **Authors**: Moab Arar, Yiftach Ginger, Dov Danon, Ilya Leizerson, Amit Bermano, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Many applications, such as autonomous driving, heavily rely on multi-modal data where spatial alignment between the modalities is required. Most multi-modal registration methods struggle computing the spatial correspondence between the images using prevalent cross-modality similarity measures. In this work, we bypass the difficulties of developing cross-modality similarity measures, by training an image-to-image translation network on the two input modalities. This learned translation allows training the registration network using simple and reliable mono-modality metrics. We perform multi-modal registration using two networks - a spatial transformation network and a translation network. We show that by encouraging our translation network to be geometry preserving, we manage to train an accurate spatial transformation network. Compared to state-of-the-art multi-modal methods our presented method is unsupervised, requiring no pairs of aligned modalities for training, and can be adapted to any pair of modalities. We evaluate our method quantitatively and qualitatively on commercial datasets, showing that it performs well on several modalities and achieves accurate alignment.



### OpenGAN: Open Set Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.08074v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08074v1)
- **Published**: 2020-03-18 07:24:37+00:00
- **Updated**: 2020-03-18 07:24:37+00:00
- **Authors**: Luke Ditria, Benjamin J. Meyer, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Many existing conditional Generative Adversarial Networks (cGANs) are limited to conditioning on pre-defined and fixed class-level semantic labels or attributes. We propose an open set GAN architecture (OpenGAN) that is conditioned per-input sample with a feature embedding drawn from a metric space. Using a state-of-the-art metric learning model that encodes both class-level and fine-grained semantic information, we are able to generate samples that are semantically similar to a given source image. The semantic information extracted by the metric learning model transfers to out-of-distribution novel classes, allowing the generative model to produce samples that are outside of the training distribution. We show that our proposed method is able to generate 256$\times$256 resolution images from novel classes that are of similar visual quality to those from the training classes. In lieu of a source image, we demonstrate that random sampling of the metric space also results in high-quality samples. We show that interpolation in the feature space and latent space results in semantically and visually plausible transformations in the image space. Finally, the usefulness of the generated samples to the downstream task of data augmentation is demonstrated. We show that classifier performance can be significantly improved by augmenting the training data with OpenGAN samples on classes that are outside of the GAN training distribution.



### Scene Text Recognition via Transformer
- **Arxiv ID**: http://arxiv.org/abs/2003.08077v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08077v4)
- **Published**: 2020-03-18 07:38:02+00:00
- **Updated**: 2020-04-29 02:56:28+00:00
- **Authors**: Xinjie Feng, Hongxun Yao, Yuankai Qi, Jun Zhang, Shengping Zhang
- **Comment**: We found that there are some errors in the experiment code, and we
  are correcting the result temporarily, so we temporarily withdraw this paper
- **Journal**: None
- **Summary**: Scene text recognition with arbitrary shape is very challenging due to large variations in text shapes, fonts, colors, backgrounds, etc. Most state-of-the-art algorithms rectify the input image into the normalized image, then treat the recognition as a sequence prediction task. The bottleneck of such methods is the rectification, which will cause errors due to distortion perspective. In this paper, we find that the rectification is completely unnecessary. What all we need is the spatial attention. We therefore propose a simple but extremely effective scene text recognition method based on transformer [50]. Different from previous transformer based models [56,34], which just use the decoder of the transformer to decode the convolutional attention, the proposed method use a convolutional feature maps as word embedding input into transformer. In such a way, our method is able to make full use of the powerful attention mechanism of the transformer. Extensive experimental results show that the proposed method significantly outperforms state-of-the-art methods by a very large margin on both regular and irregular text datasets. On one of the most challenging CUTE dataset whose state-of-the-art prediction accuracy is 89.6%, our method achieves 99.3%, which is a pretty surprising result. We will release our source code and believe that our method will be a new benchmark of scene text recognition with arbitrary shapes.



### Federated Visual Classification with Real-World Data Distribution
- **Arxiv ID**: http://arxiv.org/abs/2003.08082v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08082v3)
- **Published**: 2020-03-18 07:55:49+00:00
- **Updated**: 2020-07-17 14:25:27+00:00
- **Authors**: Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning enables visual models to be trained on-device, bringing advantages for user privacy (data need never leave the device), but challenges in terms of data diversity and quality. Whilst typical models in the datacenter are trained using data that are independent and identically distributed (IID), data at source are typically far from IID. Furthermore, differing quantities of data are typically available at each device (imbalance). In this work, we characterize the effect these real-world data distributions have on distributed learning, using as a benchmark the standard Federated Averaging (FedAvg) algorithm. To do so, we introduce two new large-scale datasets for species and landmark classification, with realistic per-user data splits that simulate real-world edge learning scenarios. We also develop two new algorithms (FedVC, FedIR) that intelligently resample and reweight over the client pool, bringing large improvements in accuracy and stability in training. The datasets are made available online.



### MagicEyes: A Large Scale Eye Gaze Estimation Dataset for Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/2003.08806v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08806v1)
- **Published**: 2020-03-18 08:23:57+00:00
- **Updated**: 2020-03-18 08:23:57+00:00
- **Authors**: Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle Zimmermann, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1908.09060
- **Journal**: None
- **Summary**: With the emergence of Virtual and Mixed Reality (XR) devices, eye tracking has received significant attention in the computer vision community. Eye gaze estimation is a crucial component in XR -- enabling energy efficient rendering, multi-focal displays, and effective interaction with content. In head-mounted XR devices, the eyes are imaged off-axis to avoid blocking the field of view. This leads to increased challenges in inferring eye related quantities and simultaneously provides an opportunity to develop accurate and robust learning based approaches. To this end, we present MagicEyes, the first large scale eye dataset collected using real MR devices with comprehensive ground truth labeling. MagicEyes includes $587$ subjects with $80,000$ images of human-labeled ground truth and over $800,000$ images with gaze target labels. We evaluate several state-of-the-art methods on MagicEyes and also propose a new multi-task EyeNet model designed for detecting the cornea, glints and pupil along with eye segmentation in a single forward pass.



### Transformer Networks for Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2003.08111v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08111v3)
- **Published**: 2020-03-18 09:17:49+00:00
- **Updated**: 2020-10-21 15:26:14+00:00
- **Authors**: Francesco Giuliari, Irtiza Hasan, Marco Cristani, Fabio Galasso
- **Comment**: To appear in International Conference on Pattern Recognition (ICPR)
  2020
- **Journal**: None
- **Summary**: Most recent successes on forecasting the people motion are based on LSTM models and all most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene. We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks. Our proposed Transformers predict the trajectories of the individual people in the scene. These are "simple" model because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model without bells and whistles yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH + UCY. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data. Code is available at https://github.com/FGiuliari/Trajectory-Transformer.



### Rotate-and-Render: Unsupervised Photorealistic Face Rotation from Single-View Images
- **Arxiv ID**: http://arxiv.org/abs/2003.08124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08124v1)
- **Published**: 2020-03-18 09:54:46+00:00
- **Updated**: 2020-03-18 09:54:46+00:00
- **Authors**: Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, Xiaogang Wang
- **Comment**: To appear in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2020. Code and models are available at:
  https://github.com/Hangz-nju-cuhk/Rotate-and-Render
- **Journal**: None
- **Summary**: Though face rotation has achieved rapid progress in recent years, the lack of high-quality paired training data remains a great hurdle for existing methods. The current generative models heavily rely on datasets with multi-view images of the same person. Thus, their generated results are restricted by the scale and domain of the data source. To overcome these challenges, we propose a novel unsupervised framework that can synthesize photo-realistic rotated faces using only single-view image collections in the wild. Our key insight is that rotating faces in the 3D space back and forth, and re-rendering them to the 2D plane can serve as a strong self-supervision. We leverage the recent advances in 3D face modeling and high-resolution GAN to constitute our building blocks. Since the 3D rotation-and-render on faces can be applied to arbitrary angles without losing details, our approach is extremely suitable for in-the-wild scenarios (i.e. no paired data are available), where existing methods fall short. Extensive experiments demonstrate that our approach has superior synthesis quality as well as identity preservation over the state-of-the-art methods, across a wide range of poses and domains. Furthermore, we validate that our rotate-and-render framework naturally can act as an effective data augmentation engine for boosting modern face recognition systems even on strong baseline models.



### A Driver Fatigue Recognition Algorithm Based on Spatio-Temporal Feature Sequence
- **Arxiv ID**: http://arxiv.org/abs/2003.08134v1
- **DOI**: 10.1109/CISP-BMEI48845.2019.8965990
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08134v1)
- **Published**: 2020-03-18 10:25:27+00:00
- **Updated**: 2020-03-18 10:25:27+00:00
- **Authors**: Chen Zhang, Xiaobo Lu, Zhiliang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Researches show that fatigue driving is one of the important causes of road traffic accidents, so it is of great significance to study the driver fatigue recognition algorithm to improve road traffic safety. In recent years, with the development of deep learning, the field of pattern recognition has made great development. This paper designs a real-time fatigue state recognition algorithm based on spatio-temporal feature sequence, which can be mainly applied to the scene of fatigue driving recognition. The algorithm is divided into three task networks: face detection network, facial landmark detection and head pose estimation network, fatigue recognition network. Experiments show that the algorithm has the advantages of small volume, high speed and high accuracy.



### The State of Lifelong Learning in Service Robots: Current Bottlenecks in Object Perception and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2003.08151v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08151v3)
- **Published**: 2020-03-18 11:00:55+00:00
- **Updated**: 2021-05-06 18:58:38+00:00
- **Authors**: S. Hamidreza Kasaei, Jorik Melsen, Floris van Beers, Christiaan Steenkist, Klemen Voncina
- **Comment**: None
- **Journal**: None
- **Summary**: Service robots are appearing more and more in our daily life. The development of service robots combines multiple fields of research, from object perception to object manipulation. The state-of-the-art continues to improve to make a proper coupling between object perception and manipulation. This coupling is necessary for service robots not only to perform various tasks in a reasonable amount of time but also to continually adapt to new environments and safely interact with non-expert human users. Nowadays, robots are able to recognize various objects, and quickly plan a collision-free trajectory to grasp a target object in predefined settings. Besides, in most of the cases, there is a reliance on large amounts of training data. Therefore, the knowledge of such robots is fixed after the training phase, and any changes in the environment require complicated, time-consuming, and expensive robot re-programming by human experts. Therefore, these approaches are still too rigid for real-life applications in unstructured environments, where a significant portion of the environment is unknown and cannot be directly sensed or controlled. In such environments, no matter how extensive the training data used for batch learning, a robot will always face new objects. Therefore, apart from batch learning, the robot should be able to continually learn about new object categories and grasp affordances from very few training examples on-site. Moreover, apart from robot self-learning, non-expert users could interactively guide the process of experience acquisition by teaching new concepts, or by correcting insufficient or erroneous concepts. In this way, the robot will constantly learn how to help humans in everyday tasks by gaining more and more experiences without the need for re-programming.



### SwapText: Image Based Texts Transfer in Scenes
- **Arxiv ID**: http://arxiv.org/abs/2003.08152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08152v1)
- **Published**: 2020-03-18 11:02:17+00:00
- **Updated**: 2020-03-18 11:02:17+00:00
- **Authors**: Qiangpeng Yang, Hongsheng Jin, Jun Huang, Wei Lin
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Swapping text in scene images while preserving original fonts, colors, sizes and background textures is a challenging task due to the complex interplay between different factors. In this work, we present SwapText, a three-stage framework to transfer texts across scene images. First, a novel text swapping network is proposed to replace text labels only in the foreground image. Second, a background completion network is learned to reconstruct background images. Finally, the generated foreground image and background image are used to generate the word image by the fusion network. Using the proposing framework, we can manipulate the texts of the input images even with severe geometric distortion. Qualitative and quantitative results are presented on several scene text datasets, including regular and irregular text datasets. We conducted extensive experiments to prove the usefulness of our method such as image based text translation, text image synthesis, etc.



### 3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels
- **Arxiv ID**: http://arxiv.org/abs/2003.08162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08162v1)
- **Published**: 2020-03-18 11:35:11+00:00
- **Updated**: 2020-03-18 11:35:11+00:00
- **Authors**: Qi Zhang, Antoni B. Chan
- **Comment**: 8 pages, 5 figures, AAAI Conference on Artificial Intelligence, AAAI,
  New York, Feb 2020
- **Journal**: None
- **Summary**: Crowd counting has been studied for decades and a lot of works have achieved good performance, especially the DNNs-based density map estimation methods. Most existing crowd counting works focus on single-view counting, while few works have studied multi-view counting for large and wide scenes, where multiple cameras are used. Recently, an end-to-end multi-view crowd counting method called multi-view multi-scale (MVMS) has been proposed, which fuses multiple camera views using a CNN to predict a 2D scene-level density map on the ground-plane. Unlike MVMS, we propose to solve the multi-view crowd counting task through 3D feature fusion with 3D scene-level density maps, instead of the 2D ground-plane ones. Compared to 2D fusion, the 3D fusion extracts more information of the people along z-dimension (height), which helps to solve the scale variations across multiple views. The 3D density maps still preserve the 2D density maps property that the sum is the count, while also providing 3D information about the crowd density. We also explore the projection consistency among the 3D prediction and the ground-truth in the 2D views to further enhance the counting performance. The proposed method is tested on 3 multi-view counting datasets and achieves better or comparable counting performance to the state-of-the-art.



### Neuroevolution of Self-Interpretable Agents
- **Arxiv ID**: http://arxiv.org/abs/2003.08165v2
- **DOI**: 10.1145/3377930.3389847
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08165v2)
- **Published**: 2020-03-18 11:40:35+00:00
- **Updated**: 2020-04-02 09:00:39+00:00
- **Authors**: Yujin Tang, Duong Nguyen, David Ha
- **Comment**: To appear at the Genetic and Evolutionary Computation Conference
  (GECCO 2020) as a full paper
- **Journal**: None
- **Summary**: Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail. Videos of our results and source code available at https://attentionagent.github.io/



### High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.08177v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08177v4)
- **Published**: 2020-03-18 12:18:35+00:00
- **Updated**: 2020-04-02 03:40:39+00:00
- **Authors**: Guan'an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang, Shuliang Wang, Gang Yu, Erjin Zhou, Jian Sun
- **Comment**: accepted by CVPR'20
- **Journal**: None
- **Summary**: Occluded person re-identification (ReID) aims to match occluded person images to holistic ones across dis-joint cameras. In this paper, we propose a novel framework by learning high-order relation and topology information for discriminative features and robust alignment. At first, we use a CNN backbone and a key-points estimation model to extract semantic local features. Even so, occluded images still suffer from occlusion and outliers. Then, we view the local features of an image as nodes of a graph and propose an adaptive direction graph convolutional (ADGC)layer to pass relation information between nodes. The proposed ADGC layer can automatically suppress the message-passing of meaningless features by dynamically learning di-rection and degree of linkage. When aligning two groups of local features from two images, we view it as a graph matching problem and propose a cross-graph embedded-alignment (CGEA) layer to jointly learn and embed topology information to local features, and straightly predict similarity score. The proposed CGEA layer not only take full use of alignment learned by graph matching but also re-place sensitive one-to-one matching with a robust soft one. Finally, extensive experiments on occluded, partial, and holistic ReID tasks show the effectiveness of our proposed method. Specifically, our framework significantly outperforms state-of-the-art by6.5%mAP scores on Occluded-Duke dataset.



### J$\hat{\text{A}}$A-Net: Joint Facial Action Unit Detection and Face Alignment via Adaptive Attention
- **Arxiv ID**: http://arxiv.org/abs/2003.08834v3
- **DOI**: 10.1007/s11263-020-01378-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08834v3)
- **Published**: 2020-03-18 12:50:19+00:00
- **Updated**: 2020-09-24 03:08:58+00:00
- **Authors**: Zhiwen Shao, Zhilei Liu, Jianfei Cai, Lizhuang Ma
- **Comment**: This paper is the extended version of arXiv:1803.05588, and is
  accepted by International Journal of Computer Vision
- **Journal**: None
- **Summary**: Facial action unit (AU) detection and face alignment are two highly correlated tasks, since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. However, most existing AU detection works handle the two tasks independently by treating face alignment as a preprocessing, and often use landmarks to predefine a fixed region or attention for each AU. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared feature is learned firstly, and high-level feature of face alignment is fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment feature and global feature for AU detection. Extensive experiments demonstrate that our framework (i) significantly outperforms the state-of-the-art AU detection methods on the challenging BP4D, DISFA, GFT and BP4D+ benchmarks, (ii) can adaptively capture the irregular region of each AU, (iii) achieves competitive performance for face alignment, and (iv) also works well under partial occlusions and non-frontal poses. The code for our method is available at https://github.com/ZhiwenShao/PyTorch-JAANet.



### Task-Adaptive Clustering for Semi-Supervised Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.08221v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08221v1)
- **Published**: 2020-03-18 13:50:19+00:00
- **Updated**: 2020-03-18 13:50:19+00:00
- **Authors**: Jun Seo, Sung Whan Yoon, Jaekyun Moon
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Few-shot learning aims to handle previously unseen tasks using only a small amount of new training data. In preparing (or meta-training) a few-shot learner, however, massive labeled data are necessary. In the real world, unfortunately, labeled data are expensive and/or scarce. In this work, we propose a few-shot learner that can work well under the semi-supervised setting where a large portion of training data is unlabeled. Our method employs explicit task-conditioning in which unlabeled sample clustering for the current task takes place in a new projection space different from the embedding feature space. The conditioned clustering space is linearly constructed so as to quickly close the gap between the class centroids for the current task and the independent per-class reference vectors meta-trained across tasks. In a more general setting, our method introduces a concept of controlling the degree of task-conditioning for meta-learning: the amount of task-conditioning varies with the number of repetitive updates for the clustering space. Extensive simulation results based on the miniImageNet and tieredImageNet datasets show state-of-the-art semi-supervised few-shot classification performance of the proposed method. Simulation results also indicate that the proposed task-adaptive clustering shows graceful degradation with a growing number of distractor samples, i.e., unlabeled sample images coming from outside the candidate classes.



### Detection of Pitt-Hopkins Syndrome based on morphological facial features
- **Arxiv ID**: http://arxiv.org/abs/2003.08229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08229v2)
- **Published**: 2020-03-18 14:01:16+00:00
- **Updated**: 2020-03-19 08:43:41+00:00
- **Authors**: Elena D'Amato, Constantino Carlos Reyes-Aldasoro, Maria Felicia Faienza, Marcella Zollino
- **Comment**: Submitted to MIUA 2020
- **Journal**: None
- **Summary**: This work describes an automatic methodology to discriminate between individuals with the genetic disorder Pitt-Hopkins syndrome (PTHS), and healthy individuals. As input data, the methodology accepts unconstrained frontal facial photographs, from which faces are located with Histograms of Oriented Gradients features descriptors. Pre-processing steps of the methodology consist of colour normalisation, scaling down, rotation, and cropping in order to produce a series of images of faces with consistent dimensions. Sixty eight facial landmarks are automatically located on each face through a cascade of regression functions learnt via gradient boosting to estimate the shape from an initial approximation. The intensities of a sparse set of pixels indexed relative to this initial estimate are used to determine the landmarks. A set of carefully selected geometric features, for example, relative width of the mouth, or angle of the nose, are extracted from the landmarks. The features are used to investigate the statistical differences between the two populations of PTHS and healthy controls. The methodology was tested on 71 individuals with PTHS and 55 healthy controls. Two geometric features related to the nose and mouth showed statistical difference between the two populations.



### Rethinking Object Detection in Retail Stores
- **Arxiv ID**: http://arxiv.org/abs/2003.08230v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08230v3)
- **Published**: 2020-03-18 14:01:54+00:00
- **Updated**: 2020-12-05 05:14:15+00:00
- **Authors**: Yuanqiang Cai, Longyin Wen, Libo Zhang, Dawei Du, Weiqiang Wang
- **Comment**: Information Error
- **Journal**: None
- **Summary**: The convention standard for object detection uses a bounding box to represent each individual object instance. However, it is not practical in the industry-relevant applications in the context of warehouses due to severe occlusions among groups of instances of the same categories. In this paper, we propose a new task, ie, simultaneously object localization and counting, abbreviated as Locount, which requires algorithms to localize groups of objects of interest with the number of instances. However, there does not exist a dataset or benchmark designed for such a task. To this end, we collect a large-scale object localization and counting dataset with rich annotations in retail stores, which consists of 50,394 images with more than 1.9 million object instances in 140 categories. Together with this dataset, we provide a new evaluation protocol and divide the training and testing subsets to fairly evaluate the performance of algorithms for Locount, developing a new benchmark for the Locount task. Moreover, we present a cascaded localization and counting network as a strong baseline, which gradually classifies and regresses the bounding boxes of objects with the predicted numbers of instances enclosed in the bounding boxes, trained in an end-to-end manner. Extensive experiments are conducted on the proposed dataset to demonstrate its significance and the analysis discussions on failure cases are provided to indicate future directions. Dataset is available at https://isrc.iscas.ac.cn/gitlab/research/locount-dataset.



### Eisen: a python package for solid deep learning
- **Arxiv ID**: http://arxiv.org/abs/2004.02747v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02747v1)
- **Published**: 2020-03-18 14:07:54+00:00
- **Updated**: 2020-03-18 14:07:54+00:00
- **Authors**: Frank Mancolo
- **Comment**: None
- **Journal**: None
- **Summary**: Eisen is an open source python package making the implementation of deep learning methods easy. It is specifically tailored to medical image analysis and computer vision tasks, but its flexibility allows extension to any application. Eisen is based on PyTorch and it follows the same architecture of other packages belonging to the PyTorch ecosystem. This simplifies its use and allows it to be compatible with modules provided by other packages. Eisen implements multiple dataset loading methods, I/O for various data formats, data manipulation and transformation, full implementation of training, validation and test loops, implementation of losses and network architectures, automatic export of training artifacts, summaries and logs, visual experiment building, command line interface and more. Furthermore, it is open to user contributions by the community. Documentation, examples and code can be downloaded from http://eisen.ai.



### CAFENet: Class-Agnostic Few-Shot Edge Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2003.08235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08235v1)
- **Published**: 2020-03-18 14:18:59+00:00
- **Updated**: 2020-03-18 14:18:59+00:00
- **Authors**: Young-Hyun Park, Jun Seo, Jaekyun Moon
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: We tackle a novel few-shot learning challenge, which we call few-shot semantic edge detection, aiming to localize crisp boundaries of novel categories using only a few labeled samples. We also present a Class-Agnostic Few-shot Edge detection Network (CAFENet) based on meta-learning strategy. CAFENet employs a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. We also propose a new regularization method based on multi-split matching. In meta-training, the metric-learning problem with high-dimensional vectors are divided into small subproblems with low-dimensional sub-vectors. Since there is no existing dataset for few-shot semantic edge detection, we construct two new datasets, FSE-1000 and SBD-$5^i$, and evaluate the performance of the proposed CAFENet on them. Extensive simulation results confirm the performance merits of the techniques adopted in CAFENet.



### Fixing the train-test resolution discrepancy: FixEfficientNet
- **Arxiv ID**: http://arxiv.org/abs/2003.08237v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08237v5)
- **Published**: 2020-03-18 14:22:58+00:00
- **Updated**: 2020-11-18 09:56:31+00:00
- **Authors**: Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv Jgou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides an extensive analysis of the performance of the EfficientNet image classifiers with several recent training procedures, in particular one that corrects the discrepancy between train and test images. The resulting network, called FixEfficientNet, significantly outperforms the initial architecture with the same number of parameters.   For instance, our FixEfficientNet-B0 trained without additional training data achieves 79.3% top-1 accuracy on ImageNet with 5.3M parameters. This is a +0.5% absolute improvement over the Noisy student EfficientNet-B0 trained with 300M unlabeled images. An EfficientNet-L2 pre-trained with weak supervision on 300M unlabeled images and further optimized with FixRes achieves 88.5% top-1 accuracy (top-5: 98.7%), which establishes the new state of the art for ImageNet with a single crop.   These improvements are thoroughly evaluated with cleaner protocols than the one usually employed for Imagenet, and particular we show that our improvement remains in the experimental setting of ImageNet-v2, that is less prone to overfitting, and with ImageNet Real Labels. In both cases we also establish the new state of the art.



### LRC-Net: Learning Discriminative Features on Point Clouds by Encoding Local Region Contexts
- **Arxiv ID**: http://arxiv.org/abs/2003.08240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08240v2)
- **Published**: 2020-03-18 14:34:08+00:00
- **Updated**: 2020-03-21 05:48:44+00:00
- **Authors**: Xinhai Liu, Zhizhong Han, Fangzhou Hong, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To be published at GMP2020
- **Journal**: None
- **Summary**: Learning discriminative feature directly on point clouds is still challenging in the understanding of 3D shapes. Recent methods usually partition point clouds into local region sets, and then extract the local region features with fixed-size CNN or MLP, and finally aggregate all individual local features into a global feature using simple max pooling. However, due to the irregularity and sparsity in sampled point clouds, it is hard to encode the fine-grained geometry of local regions and their spatial relationships when only using the fixed-size filters and individual local feature integration, which limit the ability to learn discriminative features. To address this issue, we present a novel Local-Region-Context Network (LRC-Net), to learn discriminative features on point clouds by encoding the fine-grained contexts inside and among local regions simultaneously. LRC-Net consists of two main modules. The first module, named intra-region context encoding, is designed for capturing the geometric correlation inside each local region by novel variable-size convolution filter. The second module, named inter-region context encoding, is proposed for integrating the spatial relationships among local regions based on spatial similarity measures. Experimental results show that LRC-Net is competitive with state-of-the-art methods in shape classification and shape segmentation applications.



### Deep Open Space Segmentation using Automotive Radar
- **Arxiv ID**: http://arxiv.org/abs/2004.03449v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03449v1)
- **Published**: 2020-03-18 14:49:29+00:00
- **Updated**: 2020-03-18 14:49:29+00:00
- **Authors**: Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Fahed Al Hassanat, Elnaz Jahani Heravi, Robert Laganiere, Julien Rebut, Waqas Malik
- **Comment**: IEEE MTT-S International Conference on Microwaves for Intelligent
  Mobility (ICMIM 2020)
- **Journal**: None
- **Summary**: In this work, we propose the use of radar with advanced deep segmentation models to identify open space in parking scenarios. A publically available dataset of radar observations called SCORP was collected. Deep models are evaluated with various radar input representations. Our proposed approach achieves low memory usage and real-time processing speeds, and is thus very well suited for embedded deployment.



### Cross-domain Self-supervised Learning for Domain Adaptation with Few Source Labels
- **Arxiv ID**: http://arxiv.org/abs/2003.08264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08264v1)
- **Published**: 2020-03-18 15:11:07+00:00
- **Updated**: 2020-03-18 15:11:07+00:00
- **Authors**: Donghyun Kim, Kuniaki Saito, Tae-Hyun Oh, Bryan A. Plummer, Stan Sclaroff, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Existing unsupervised domain adaptation methods aim to transfer knowledge from a label-rich source domain to an unlabeled target domain. However, obtaining labels for some source domains may be very expensive, making complete labeling as used in prior work impractical. In this work, we investigate a new domain adaptation scenario with sparsely labeled source data, where only a few examples in the source domain have been labeled, while the target domain is unlabeled. We show that when labeled source examples are limited, existing methods often fail to learn discriminative features applicable for both source and target domains. We propose a novel Cross-Domain Self-supervised (CDS) learning approach for domain adaptation, which learns features that are not only domain-invariant but also class-discriminative. Our self-supervised learning method captures apparent visual similarity with in-domain self-supervision in a domain adaptive manner and performs cross-domain feature matching with across-domain self-supervision. In extensive experiments with three standard benchmark datasets, our method significantly boosts performance of target accuracy in the new target domain with few source labels and is even helpful on classical domain adaptation scenarios.



### An Artificial Intelligence-Based System to Assess Nutrient Intake for Hospitalised Patients
- **Arxiv ID**: http://arxiv.org/abs/2003.08273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08273v1)
- **Published**: 2020-03-18 15:28:51+00:00
- **Updated**: 2020-03-18 15:28:51+00:00
- **Authors**: Ya Lu, Thomai Stathopoulou, Maria F. Vasiloglou, Stergios Christodoulidis, Zeno Stanga, Stavroula Mougiakakou
- **Comment**: None
- **Journal**: None
- **Summary**: Regular monitoring of nutrient intake in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition. Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve data accuracy and reduce both the burden on participants and health costs. In this paper, we propose a novel system based on artificial intelligence (AI) to accurately estimate nutrient intake, by simply processing RGB Depth (RGB-D) image pairs captured before and after meal consumption. The system includes a novel multi-task contextual network for food segmentation, a few-shot learning-based classifier built by limited training samples for food recognition, and an algorithm for 3D surface construction. This allows sequential food segmentation, recognition, and estimation of the consumed food volume, permitting fully automatic estimation of the nutrient intake for each meal. For the development and evaluation of the system, a dedicated new database containing images and nutrient recipes of 322 meals is assembled, coupled to data annotation using innovative strategies. Experimental results demonstrate that the estimated nutrient intake is highly correlated (> 0.91) to the ground truth and shows very small mean relative errors (< 20%), outperforming existing techniques proposed for nutrient intake assessment.



### PIC: Permutation Invariant Convolution for Recognizing Long-range Activities
- **Arxiv ID**: http://arxiv.org/abs/2003.08275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08275v1)
- **Published**: 2020-03-18 15:30:03+00:00
- **Updated**: 2020-03-18 15:30:03+00:00
- **Authors**: Noureldien Hussein, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: Neural operations as convolutions, self-attention, and vector aggregation are the go-to choices for recognizing short-range actions. However, they have three limitations in modeling long-range activities. This paper presents PIC, Permutation Invariant Convolution, a novel neural layer to model the temporal structure of long-range activities. It has three desirable properties. i. Unlike standard convolution, PIC is invariant to the temporal permutations of features within its receptive field, qualifying it to model the weak temporal structures. ii. Different from vector aggregation, PIC respects local connectivity, enabling it to learn long-range temporal abstractions using cascaded layers. iii. In contrast to self-attention, PIC uses shared weights, making it more capable of detecting the most discriminant visual evidence across long and noisy videos. We study the three properties of PIC and demonstrate its effectiveness in recognizing the long-range activities of Charades, Breakfast, and MultiThumos.



### Event Probability Mask (EPM) and Event Denoising Convolutional Neural Network (EDnCNN) for Neuromorphic Cameras
- **Arxiv ID**: http://arxiv.org/abs/2003.08282v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08282v2)
- **Published**: 2020-03-18 15:44:05+00:00
- **Updated**: 2020-03-23 13:56:36+00:00
- **Authors**: R. Wes Baldwin, Mohammed Almatrafi, Vijayan Asari, Keigo Hirakawa
- **Comment**: submitted to CVPR 2020
- **Journal**: None
- **Summary**: This paper presents a novel method for labeling real-world neuromorphic camera sensor data by calculating the likelihood of generating an event at each pixel within a short time window, which we refer to as "event probability mask" or EPM. Its applications include (i) objective benchmarking of event denoising performance, (ii) training convolutional neural networks for noise removal called "event denoising convolutional neural network" (EDnCNN), and (iii) estimating internal neuromorphic camera parameters. We provide the first dataset (DVSNOISE20) of real-world labeled neuromorphic camera events for noise removal.



### Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways
- **Arxiv ID**: http://arxiv.org/abs/2003.08284v3
- **DOI**: 10.1109/CVPRW50498.2020.00109
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08284v3)
- **Published**: 2020-03-18 15:45:06+00:00
- **Updated**: 2020-04-16 14:48:42+00:00
- **Authors**: Weikai Tan, Nannan Qin, Lingfei Ma, Ying Li, Jing Du, Guorong Cai, Ke Yang, Jonathan Li
- **Comment**: Toronto-3D dataset can be downloaded at
  https://github.com/WeikaiTan/Toronto-3D
- **Journal**: None
- **Summary**: Semantic segmentation of large-scale outdoor point clouds is essential for urban scene understanding in various applications, especially autonomous driving and urban high-definition (HD) mapping. With rapid developments of mobile laser scanning (MLS) systems, massive point clouds are available for scene understanding, but publicly accessible large-scale labeled datasets, which are essential for developing learning-based methods, are still limited. This paper introduces Toronto-3D, a large-scale urban outdoor point cloud dataset acquired by a MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of point clouds and consists of about 78.3 million points with 8 labeled object classes. Baseline experiments for semantic segmentation were conducted and the results confirmed the capability of this dataset to train deep learning models effectively. Toronto-3D is released to encourage new research, and the labels will be improved and updated with feedback from the research community.



### Triplet Permutation Method for Deep Learning of Single-Shot Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2003.08303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08303v1)
- **Published**: 2020-03-18 15:57:13+00:00
- **Updated**: 2020-03-18 15:57:13+00:00
- **Authors**: M. J. Gmez-Silva, J. M. Armingol, A. de la Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Solving Single-Shot Person Re-Identification (Re-Id) by training Deep Convolutional Neural Networks is a daunting challenge, due to the lack of training data, since only two images per person are available. This causes the overfitting of the models, leading to degenerated performance. This paper formulates the Triplet Permutation method to generate multiple training sets, from a certain re-id dataset. This is a novel strategy for feeding triplet networks, which reduces the overfitting of the Single-Shot Re-Id model. The improved performance has been demonstrated over one of the most challenging Re-Id datasets, PRID2011, proving the effectiveness of the method.



### On the Distribution of Minima in Intrinsic-Metric Rotation Averaging
- **Arxiv ID**: http://arxiv.org/abs/2003.08310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08310v1)
- **Published**: 2020-03-18 16:03:22+00:00
- **Updated**: 2020-03-18 16:03:22+00:00
- **Authors**: Kyle Wilson, David Bindel
- **Comment**: To be published in CVPR2020
- **Journal**: None
- **Summary**: Rotation Averaging is a non-convex optimization problem that determines orientations of a collection of cameras from their images of a 3D scene. The problem has been studied using a variety of distances and robustifiers. The intrinsic (or geodesic) distance on SO(3) is geometrically meaningful; but while some extrinsic distance-based solvers admit (conditional) guarantees of correctness, no comparable results have been found under the intrinsic metric.   In this paper, we study the spatial distribution of local minima. First, we do a novel empirical study to demonstrate sharp transitions in qualitative behavior: as problems become noisier, they transition from a single (easy-to-find) dominant minimum to a cost surface filled with minima. In the second part of this paper we derive a theoretical bound for when this transition occurs. This is an extension of the results of [24], which used local convexity as a proxy to study the difficulty of problem. By recognizing the underlying quotient manifold geometry of the problem we achieve an n-fold improvement over prior work. Incidentally, our analysis also extends the prior $l_2$ work to general $l_p$ costs. Our results suggest using algebraic connectivity as an indicator of problem difficulty.



### DeepCap: Monocular Human Performance Capture Using Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2003.08325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08325v1)
- **Published**: 2020-03-18 16:39:56+00:00
- **Updated**: 2020-03-18 16:39:56+00:00
- **Authors**: Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: Human performance capture is a highly important computer vision problem with many applications in movie production and virtual/augmented reality. Many previous performance capture approaches either required expensive multi-view setups or did not recover dense space-time coherent geometry with frame-to-frame correspondences. We propose a novel deep learning approach for monocular dense human performance capture. Our method is trained in a weakly supervised manner based on multi-view supervision completely removing the need for training data with 3D ground truth annotations. The network architecture is based on two separate networks that disentangle the task into a pose estimation and a non-rigid surface deformation step. Extensive qualitative and quantitative evaluations show that our approach outperforms the state of the art in terms of quality and robustness.



### A new geodesic-based feature for characterization of 3D shapes: application to soft tissue organ temporal deformations
- **Arxiv ID**: http://arxiv.org/abs/2003.08332v1
- **DOI**: None
- **Categories**: **cs.CV**, math.AP
- **Links**: [PDF](http://arxiv.org/pdf/2003.08332v1)
- **Published**: 2020-03-18 16:56:41+00:00
- **Updated**: 2020-03-18 16:56:41+00:00
- **Authors**: Karim Makki, Amine Bohi, Augustin C. Ogier, Marc-Emmanuel Bellemare
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method for characterizing 3D shapes from point clouds and we show a direct application on a study of organ temporal deformations. As an example, we characterize the behavior of a bladder during a forced respiratory motion with a reduced number of 3D surface points: first, a set of equidistant points representing the vertices of quadrilateral mesh for the surface in the first time frame are tracked throughout a long dynamic MRI sequence using a Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework. Second, a novel geometric feature which is invariant to scaling and rotation is proposed for characterizing the temporal organ deformations by employing an Eulerian Partial Differential Equations (PDEs) methodology. We demonstrate the robustness of our feature on both synthetic 3D shapes and realistic dynamic MRI data portraying the bladder deformation during forced respiratory motions. Promising results are obtained, showing that the proposed feature may be useful for several computer vision applications such as medical imaging, aerodynamics and robotics.



### Collaborative Video Object Segmentation by Foreground-Background Integration
- **Arxiv ID**: http://arxiv.org/abs/2003.08333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08333v2)
- **Published**: 2020-03-18 16:59:46+00:00
- **Updated**: 2020-07-23 11:31:22+00:00
- **Authors**: Zongxin Yang, Yunchao Wei, Yi Yang
- **Comment**: ECCV 2020, Spotlight
- **Journal**: None
- **Summary**: This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Different from previous practices that only explore the embedding learning using pixels from foreground object (s), we consider background should be equally treated and thus propose Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. Our CFBI implicitly imposes the feature embedding from the target foreground object and its corresponding background to be contrastive, promoting the segmentation results accordingly. With the feature embedding from both foreground and background, our CFBI performs the matching process between the reference and the predicted sequence from both pixel and instance levels, making the CFBI be robust to various object scales. We conduct extensive experiments on three popular benchmarks, i.e., DAVIS 2016, DAVIS 2017, and YouTube-VOS. Our CFBI achieves the performance (J$F) of 89.4%, 81.9%, and 81.4%, respectively, outperforming all the other state-of-the-art methods. Code: https://github.com/z-x-yang/CFBI.



### Weakly Supervised PET Tumor Detection Using Class Response
- **Arxiv ID**: http://arxiv.org/abs/2003.08337v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08337v2)
- **Published**: 2020-03-18 17:06:08+00:00
- **Updated**: 2020-03-19 08:01:06+00:00
- **Authors**: Amine Amyar, Romain Modzelewski, Pierre Vera, Vincent Morard, Su Ruan
- **Comment**: Submitted to MICCAI 2020
- **Journal**: None
- **Summary**: One of the most challenges in medical imaging is the lack of data and annotated data. It is proven that classical segmentation methods such as U-NET are useful but still limited due to the lack of annotated data. Using a weakly supervised learning is a promising way to address this problem, however, it is challenging to train one model to detect and locate efficiently different type of lesions due to the huge variation in images. In this paper, we present a novel approach to locate different type of lesions in positron emission tomography (PET) images using only a class label at the image-level. First, a simple convolutional neural network classifier is trained to predict the type of cancer on two 2D MIP images. Then, a pseudo-localization of the tumor is generated using class activation maps, back-propagated and corrected in a multitask learning approach with prior knowledge, resulting in a tumor detection mask. Finally, we use the mask generated from the two 2D images to detect the tumor in the 3D image. The advantage of our proposed method consists of detecting the whole tumor volume in 3D images, using only two 2D images of PET image, and showing a very promising results. It can be used as a tool to locate very efficiently tumors in a PET scan, which is a time-consuming task for physicians. In addition, we show that our proposed method can be used to conduct a radiomics study with state of the art results.



### Multi-View Optimization of Local Feature Geometry
- **Arxiv ID**: http://arxiv.org/abs/2003.08348v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08348v2)
- **Published**: 2020-03-18 17:22:11+00:00
- **Updated**: 2020-07-22 15:23:43+00:00
- **Authors**: Mihai Dusmanu, Johannes L. Schnberger, Marc Pollefeys
- **Comment**: Accepted at ECCV 2020. 28 pages, 11 figures, 6 tables
- **Journal**: None
- **Summary**: In this work, we address the problem of refining the geometry of local image features from multiple views without known scene or camera geometry. Current approaches to local feature detection are inherently limited in their keypoint localization accuracy because they only operate on a single view. This limitation has a negative impact on downstream tasks such as Structure-from-Motion, where inaccurate keypoints lead to large errors in triangulation and camera localization. Our proposed method naturally complements the traditional feature extraction and matching paradigm. We first estimate local geometric transformations between tentative matches and then optimize the keypoint locations over multiple views jointly according to a non-linear least squares formulation. Throughout a variety of experiments, we show that our method consistently improves the triangulation and camera localization performance for both hand-crafted and learned local features.



### Deep Quaternion Features for Privacy Protection
- **Arxiv ID**: http://arxiv.org/abs/2003.08365v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08365v2)
- **Published**: 2020-03-18 17:38:24+00:00
- **Updated**: 2020-06-21 09:37:52+00:00
- **Authors**: Hao Zhang, Yiting Chen, Liyao Xiang, Haotian Ma, Jie Shi, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to revise the neural network to construct the quaternion-valued neural network (QNN), in order to prevent intermediate-layer features from leaking input information. The QNN uses quaternion-valued features, where each element is a quaternion. The QNN hides input information into a random phase of quaternion-valued features. Even if attackers have obtained network parameters and intermediate-layer features, they cannot extract input information without knowing the target phase. In this way, the QNN can effectively protect the input privacy. Besides, the output accuracy of QNNs only degrades mildly compared to traditional neural networks, and the computational cost is much less than other privacy-preserving methods.



### Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination
- **Arxiv ID**: http://arxiv.org/abs/2003.08367v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2003.08367v2)
- **Published**: 2020-03-18 17:46:30+00:00
- **Updated**: 2020-05-13 17:04:29+00:00
- **Authors**: Pratul P. Srinivasan, Ben Mildenhall, Matthew Tancik, Jonathan T. Barron, Richard Tucker, Noah Snavely
- **Comment**: CVPR 2020. Project page:
  https://people.eecs.berkeley.edu/~pratul/lighthouse/ [Updates: typos
  corrected]
- **Journal**: None
- **Summary**: We present a deep learning solution for estimating the incident illumination at any 3D location within a scene from an input narrow-baseline stereo image pair. Previous approaches for predicting global illumination from images either predict just a single illumination for the entire scene, or separately estimate the illumination at each 3D location without enforcing that the predictions are consistent with the same 3D scene. Instead, we propose a deep learning model that estimates a 3D volumetric RGBA model of a scene, including content outside the observed field of view, and then uses standard volume rendering to estimate the incident illumination at any 3D location within that volume. Our model is trained without any ground truth 3D data and only requires a held-out perspective view near the input stereo pair and a spherical panorama taken within each scene as supervision, as opposed to prior methods for spatially-varying lighting estimation, which require ground truth scene geometry for training. We demonstrate that our method can predict consistent spatially-varying lighting that is convincing enough to plausibly relight and insert highly specular virtual objects into real images.



### Pairwise Similarity Knowledge Transfer for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2003.08375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08375v2)
- **Published**: 2020-03-18 17:53:33+00:00
- **Updated**: 2020-07-19 14:09:18+00:00
- **Authors**: Amir Rahimi, Amirreza Shaban, Thalaiyasingam Ajanthan, Richard Hartley, Byron Boots
- **Comment**: ECCV 2020. formerly "In Defense of Graph Inference Algorithms for
  Weakly Supervised Object Localization"
- **Journal**: None
- **Summary**: Weakly Supervised Object Localization (WSOL) methods only require image level labels as opposed to expensive bounding box annotations required by fully supervised algorithms. We study the problem of learning localization model on target classes with weakly supervised image labels, helped by a fully annotated source dataset. Typically, a WSOL model is first trained to predict class generic objectness scores on an off-the-shelf fully supervised source dataset and then it is progressively adapted to learn the objects in the weakly supervised target dataset. In this work, we argue that learning only an objectness function is a weak form of knowledge transfer and propose to learn a classwise pairwise similarity function that directly compares two input proposals as well. The combined localization model and the estimated object annotations are jointly learned in an alternating optimization paradigm as is typically done in standard WSOL methods. In contrast to the existing work that learns pairwise similarities, our approach optimizes a unified objective with convergence guarantee and it is computationally efficient for large-scale applications. Experiments on the COCO and ILSVRC 2013 detection datasets show that the performance of the localization model improves significantly with the inclusion of pairwise similarity function. For instance, in the ILSVRC dataset, the Correct Localization (CorLoc) performance improves from 72.8% to 78.2% which is a new state-of-the-art for WSOL task in the context of knowledge transfer.



### Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud Forecasting for Sequential Pose Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2003.08376v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.08376v3)
- **Published**: 2020-03-18 17:54:28+00:00
- **Updated**: 2020-11-07 02:48:22+00:00
- **Authors**: Xinshuo Weng, Jianren Wang, Sergey Levine, Kris Kitani, Nicholas Rhinehart
- **Comment**: Published in Conference on Robot Learning (CoRL), 2020. Project
  webpage: http://www.xinshuoweng.com/projects/SPF2/
- **Journal**: None
- **Summary**: Many autonomous systems forecast aspects of the future in order to aid decision-making. For example, self-driving vehicles and robotic manipulation systems often forecast future object poses by first detecting and tracking objects. However, this detect-then-forecast pipeline is expensive to scale, as pose forecasting algorithms typically require labeled sequences of object poses, which are costly to obtain in 3D space. Can we scale performance without requiring additional labels? We hypothesize yes, and propose inverting the detect-then-forecast pipeline. Instead of detecting, tracking and then forecasting the objects, we propose to first forecast 3D sensor data (e.g., point clouds with $100$k points) and then detect/track objects on the predicted point cloud sequences to obtain future poses, i.e., a forecast-then-detect pipeline. This inversion makes it less expensive to scale pose forecasting, as the sensor data forecasting task requires no labels. Part of this work's focus is on the challenging first step -- Sequential Pointcloud Forecasting (SPF), for which we also propose an effective approach, SPFNet. To compare our forecast-then-detect pipeline relative to the detect-then-forecast pipeline, we propose an evaluation procedure and two metrics. Through experiments on a robotic manipulation dataset and two driving datasets, we show that SPFNet is effective for the SPF task, our forecast-then-detect pipeline outperforms the detect-then-forecast approaches to which we compared, and that pose forecasting performance improves with the addition of unlabeled data.



### Confronting the Constraints for Optical Character Segmentation from Printed Bangla Text Image
- **Arxiv ID**: http://arxiv.org/abs/2003.08384v5
- **DOI**: 10.1145/3428363.3428367
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08384v5)
- **Published**: 2020-03-18 17:58:05+00:00
- **Updated**: 2021-01-05 18:11:50+00:00
- **Authors**: Abu Saleh Md. Abir, Sanjana Rahman, Samia Ellin, Maisha Farzana, Md Hridoy Manik, Chowdhury Rafeed Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: In a world of digitization, optical character recognition holds the automation to written history. Optical character recognition system basically converts printed images into editable texts for better storage and usability. To be completely functional, the system needs to go through some crucial methods such as pre-processing and segmentation. Pre-processing helps printed data to be noise free and gets rid of skewness efficiently whereas segmentation helps the image fragment into line, word and character precisely for better conversion. These steps hold the door to better accuracy and consistent results for a printed image to be ready for conversion. Our proposed algorithm is able to segment characters both from ideal and non-ideal cases of scanned or captured images giving a sustainable outcome. The implementation of our work is provided here: https://cutt.ly/rgdfBIa



### DLow: Diversifying Latent Flows for Diverse Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.08386v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08386v2)
- **Published**: 2020-03-18 17:58:11+00:00
- **Updated**: 2020-07-22 17:53:24+00:00
- **Authors**: Ye Yuan, Kris Kitani
- **Comment**: ECCV 2020. Project Page: https://www.ye-yuan.com/dlow
- **Journal**: None
- **Summary**: Deep generative models are often used for human motion prediction as they are able to model multi-modal data distributions and characterize diverse human behavior. While much care has been taken into designing and learning deep generative models, how to efficiently produce diverse samples from a deep generative model after it has been trained is still an under-explored problem. To obtain samples from a pretrained generative model, most existing generative human motion prediction methods draw a set of independent Gaussian latent codes and convert them to motion samples. Clearly, this random sampling strategy is not guaranteed to produce diverse samples for two reasons: (1) The independent sampling cannot force the samples to be diverse; (2) The sampling is based solely on likelihood which may only produce samples that correspond to the major modes of the data distribution. To address these problems, we propose a novel sampling method, Diversifying Latent Flows (DLow), to produce a diverse set of samples from a pretrained deep generative model. Unlike random (independent) sampling, the proposed DLow sampling method samples a single random variable and then maps it with a set of learnable mapping functions to a set of correlated latent codes. The correlated latent codes are then decoded into a set of correlated samples. During training, DLow uses a diversity-promoting prior over samples as an objective to optimize the latent mappings to improve sample diversity. The design of the prior is highly flexible and can be customized to generate diverse motions with common features (e.g., similar leg motion but diverse upper-body motion). Our experiments demonstrate that DLow outperforms state-of-the-art baseline methods in terms of sample diversity and accuracy. Our code is released on the project page: https://www.ye-yuan.com/dlow.



### Adversarial Texture Optimization from RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/2003.08400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08400v1)
- **Published**: 2020-03-18 18:00:05+00:00
- **Updated**: 2020-03-18 18:00:05+00:00
- **Authors**: Jingwei Huang, Justus Thies, Angela Dai, Abhijit Kundu, Chiyu Max Jiang, Leonidas Guibas, Matthias Niener, Thomas Funkhouser
- **Comment**: None
- **Journal**: None
- **Summary**: Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts.   In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views.   Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors.   The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments.   Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism.   We train the discriminator by providing as `real' examples pairs of input views and their misaligned versions -- so that the learned adversarial loss will tolerate errors from the scans.   Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art. Our code is publicly available with video demonstration.



### A Content Transformation Block For Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.08407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08407v1)
- **Published**: 2020-03-18 18:00:23+00:00
- **Updated**: 2020-03-18 18:00:23+00:00
- **Authors**: Dmytro Kotovenko, Artsiom Sanakoyeu, Pingchuan Ma, Sabine Lang, Bjrn Ommer
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Style transfer has recently received a lot of attention, since it allows to study fundamental challenges in image understanding and synthesis. Recent work has significantly improved the representation of color and texture and computational speed and image resolution. The explicit transformation of image content has, however, been mostly neglected: while artistic style affects formal characteristics of an image, such as color, shape or texture, it also deforms, adds or removes content details. This paper explicitly focuses on a content-and style-aware stylization of a content image. Therefore, we introduce a content transformation module between the encoder and decoder. Moreover, we utilize similar content appearing in photographs and style samples to learn how style alters content details and we generalize this to other class details. Additionally, this work presents a novel normalization layer critical for high resolution image synthesis. The robustness and speed of our model enables a video stylization in real-time and high definition. We perform extensive qualitative and quantitative evaluations to demonstrate the validity of our approach.



### Oral-3D: Reconstructing the 3D Bone Structure of Oral Cavity from 2D Panoramic X-ray
- **Arxiv ID**: http://arxiv.org/abs/2003.08413v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08413v4)
- **Published**: 2020-03-18 18:02:57+00:00
- **Updated**: 2021-01-09 00:22:42+00:00
- **Authors**: Weinan Song, Yuan Liang, Jiawei Yang, Kun Wang, Lei He
- **Comment**: None
- **Journal**: None
- **Summary**: Panoramic X-ray (PX) provides a 2D picture of the patient's mouth in a panoramic view to help dentists observe the invisible disease inside the gum. However, it provides limited 2D information compared with cone-beam computed tomography (CBCT), another dental imaging method that generates a 3D picture of the oral cavity but with more radiation dose and a higher price. Consequently, it is of great interest to reconstruct the 3D structure from a 2D X-ray image, which can greatly explore the application of X-ray imaging in dental surgeries. In this paper, we propose a framework, named Oral-3D, to reconstruct the 3D oral cavity from a single PX image and prior information of the dental arch. Specifically, we first train a generative model to learn the cross-dimension transformation from 2D to 3D. Then we restore the shape of the oral cavity with a deformation module with the dental arch curve, which can be obtained simply by taking a photo of the patient's mouth. To be noted, Oral-3D can restore both the density of bony tissues and the curved mandible surface. Experimental results show that Oral-3D can efficiently and effectively reconstruct the 3D oral structure and show critical information in clinical applications, e.g., tooth pulling and dental implants. To the best of our knowledge, we are the first to explore this domain transformation problem between these two imaging methods.



### Volumetric parcellation of the right ventricle for regional geometric and functional assessment
- **Arxiv ID**: http://arxiv.org/abs/2003.08423v3
- **DOI**: 10.1016/j.media.2021.102044
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08423v3)
- **Published**: 2020-03-18 18:31:26+00:00
- **Updated**: 2021-04-06 17:35:20+00:00
- **Authors**: Gabriel Bernardino, Amir Hodzic, Helene Langet, Damien LeGallois, Mathieu De Craene, Miguel Angel Gonzlez Ballester, Eric Saloux, Bart Bijnens
- **Comment**: None
- **Journal**: Medical Image Analysis, Available online 6 April 2021, 102044
- **Summary**: 3D echocardiography is an increasingly popular tool for assessing cardiac remodelling in the right ventricle (RV). It allows quantification of the cardiac chambers without any geometric assumptions, which is the main weakness of 2D echocardiography. However, regional quantification of geometry and function is limited by the lower spatial and temporal resolution and the scarcity of identifiable anatomical landmarks. We developed a technique for regionally assessing the 3 relevant RV regions: apical, inlet and outflow. The method's inputs are end-diastolic (ED) and end-systolic (ES) segmented 3D surface models. The method first defines a partition of the ED endocardium using the geodesic distances from each surface point to apex, tricuspid valve and pulmonary valve: the landmarks that define the 3 regions. The ED surface mesh is then tetrahedralised, and the endocardial-defined partition is interpolated in the blood cavity via the Laplace equation. For obtaining an ES partition, the endocardial partition is transported from ED to ES using a commercial image-based tracking, and then interpolated towards the endocardium, similarly to ED, for computing volumes and ejection fraction (EF). We present a full assessment of the method's validity and reproducibility. First, we assess reproducibility under segmentation variability, obtaining intra- and inter- observer errors (4-10% and 10-23% resp.). Finally, we use a synthetic remodelling dataset to identify the situations in which our method is able to correctly determine the region that has remodelled. This dataset is generated by a novel mesh reconstruction method that deforms a reference mesh, locally imposing a given strain, expressed in anatomical coordinates. We show that the parcellation method is adequate for capturing local circumferential and global circumferential and longitudinal RV remodelling.



### STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.08429v3
- **DOI**: 10.1007/978-3-030-58621-8_10
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T45, 68T10, 62H30, I.2.10; I.4.6; I.4.8; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2003.08429v3)
- **Published**: 2020-03-18 18:40:52+00:00
- **Updated**: 2020-08-18 17:05:06+00:00
- **Authors**: Ali Athar, Sabarinath Mahadevan, Aljoa Oep, Laura Leal-Taix, Bastian Leibe
- **Comment**: 28 pages, 6 figures
- **Journal**: None
- **Summary**: Existing methods for instance segmentation in videos typi-cally involve multi-stage pipelines that follow the tracking-by-detectionparadigm and model a video clip as a sequence of images. Multiple net-works are used to detect objects in individual frames, and then associatethese detections over time. Hence, these methods are often non-end-to-end trainable and highly tailored to specific tasks. In this paper, we pro-pose a different approach that is well-suited to a variety of tasks involvinginstance segmentation in videos. In particular, we model a video clip asa single 3D spatio-temporal volume, and propose a novel approach thatsegments and tracks instances across space and time in a single stage. Ourproblem formulation is centered around the idea of spatio-temporal em-beddings which are trained to cluster pixels belonging to a specific objectinstance over an entire video clip. To this end, we introduce (i) novel mix-ing functions that enhance the feature representation of spatio-temporalembeddings, and (ii) a single-stage, proposal-free network that can rea-son about temporal context. Our network is trained end-to-end to learnspatio-temporal embeddings as well as parameters required to clusterthese embeddings, thus simplifying inference. Our method achieves state-of-the-art results across multiple datasets and tasks. Code and modelsare available at https://github.com/sabarim/STEm-Seg.



### Collaborative Distillation for Ultra-Resolution Universal Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2003.08436v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08436v2)
- **Published**: 2020-03-18 18:59:31+00:00
- **Updated**: 2020-03-24 15:09:17+00:00
- **Authors**: Huan Wang, Yijun Li, Yuehai Wang, Haoji Hu, Ming-Hsuan Yang
- **Comment**: Accepted by CVPR 2020, higher-resolution images than the camera-ready
  version
- **Journal**: None
- **Summary**: Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher's features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.



### Synthesize then Compare: Detecting Failures and Anomalies for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.08440v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08440v2)
- **Published**: 2020-03-18 19:02:57+00:00
- **Updated**: 2020-09-08 02:42:54+00:00
- **Authors**: Yingda Xia, Yi Zhang, Fengze Liu, Wei Shen, Alan Yuille
- **Comment**: ECCV 2020 Oral. The first two authors contributed equally to this
  work. Code available at https://github.com/YingdaXia/SynthCP
- **Journal**: None
- **Summary**: The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, \emph{i.e.}, 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation.



### Detecting Pancreatic Ductal Adenocarcinoma in Multi-phase CT Scans via Alignment Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2003.08441v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08441v3)
- **Published**: 2020-03-18 19:06:27+00:00
- **Updated**: 2020-07-01 19:34:38+00:00
- **Authors**: Yingda Xia, Qihang Yu, Wei Shen, Yuyin Zhou, Elliot K. Fishman, Alan L. Yuille
- **Comment**: The first two authors contributed equally to this work. Accepted to
  MICCAI 2020
- **Journal**: None
- **Summary**: Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers among the population. Screening for PDACs in dynamic contrast-enhanced CT is beneficial for early diagnosis. In this paper, we investigate the problem of automated detecting PDACs in multi-phase (arterial and venous) CT scans. Multiple phases provide more information than single phase, but they are unaligned and inhomogeneous in texture, making it difficult to combine cross-phase information seamlessly. We study multiple phase alignment strategies, i.e., early alignment (image registration), late alignment (high-level feature registration), and slow alignment (multi-level feature registration), and suggest an ensemble of all these alignments as a promising way to boost the performance of PDAC detection. We provide an extensive empirical evaluation on two PDAC datasets and show that the proposed alignment ensemble significantly outperforms previous state-of-the-art approaches, illustrating the strong potential for clinical use.



### Semi-supervised few-shot learning for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.08462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08462v2)
- **Published**: 2020-03-18 20:37:18+00:00
- **Updated**: 2020-04-09 01:40:33+00:00
- **Authors**: Abdur R Feyjie, Reza Azad, Marco Pedersoli, Claude Kauffman, Ismail Ben Ayed, Jose Dolz
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the great progress of deep neural networks on semantic segmentation, particularly in medical imaging. Nevertheless, training high-performing models require large amounts of pixel-level ground truth masks, which can be prohibitive to obtain in the medical domain. Furthermore, training such models in a low-data regime highly increases the risk of overfitting. Recent attempts to alleviate the need for large annotated datasets have developed training strategies under the few-shot learning paradigm, which addresses this shortcoming by learning a novel class from only a few labeled examples. In this context, a segmentation model is trained on episodes, which represent different segmentation problems, each of them trained with a very small labeled dataset. In this work, we propose a novel few-shot learning framework for semantic segmentation, where unlabeled images are also made available at each episode. To handle this new learning paradigm, we propose to include surrogate tasks that can leverage very powerful supervisory signals --derived from the data itself-- for semantic feature learning. We show that including unlabeled surrogate tasks in the episodic training leads to more powerful feature representations, which ultimately results in better generability to unseen tasks. We demonstrate the efficiency of our method in the task of skin lesion segmentation in two publicly available datasets. Furthermore, our approach is general and model-agnostic, which can be combined with different deep architectures.



### Train, Learn, Expand, Repeat
- **Arxiv ID**: http://arxiv.org/abs/2003.08469v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08469v2)
- **Published**: 2020-03-18 20:55:38+00:00
- **Updated**: 2020-04-19 12:25:11+00:00
- **Authors**: Abhijeet Parida, Aadhithya Sankar, Rami Eisawy, Tom Finck, Benedikt Wiestler, Franz Pfister, Julia Moosbauer
- **Comment**: Published as a workshop paper at AI4AH, ICLR 2020
- **Journal**: None
- **Summary**: High-quality labeled data is essential to successfully train supervised machine learning models. Although a large amount of unlabeled data is present in the medical domain, labeling poses a major challenge: medical professionals who can expertly label the data are a scarce and expensive resource. Making matters worse, voxel-wise delineation of data (e.g. for segmentation tasks) is tedious and suffers from high inter-rater variance, thus dramatically limiting available training data. We propose a recursive training strategy to perform the task of semantic segmentation given only very few training samples with pixel-level annotations. We expand on this small training set having cheaper image-level annotations using a recursive training strategy. We apply this technique on the segmentation of intracranial hemorrhage (ICH) in CT (computed tomography) scans of the brain, where typically few annotated data is available.



### MINT: Deep Network Compression via Mutual Information-based Neuron Trimming
- **Arxiv ID**: http://arxiv.org/abs/2003.08472v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2003.08472v1)
- **Published**: 2020-03-18 21:05:02+00:00
- **Updated**: 2020-03-18 21:05:02+00:00
- **Authors**: Madan Ravi Ganesh, Jason J. Corso, Salimeh Yasaei Sekeh
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Most approaches to deep neural network compression via pruning either evaluate a filter's importance using its weights or optimize an alternative objective function with sparsity constraints. While these methods offer a useful way to approximate contributions from similar filters, they often either ignore the dependency between layers or solve a more difficult optimization objective than standard cross-entropy. Our method, Mutual Information-based Neuron Trimming (MINT), approaches deep compression via pruning by enforcing sparsity based on the strength of the relationship between filters of adjacent layers, across every pair of layers. The relationship is calculated using conditional geometric mutual information which evaluates the amount of similar information exchanged between the filters using a graph-based criterion. When pruning a network, we ensure that retained filters contribute the majority of the information towards succeeding layers which ensures high performance. Our novel approach outperforms existing state-of-the-art compression-via-pruning methods on the standard benchmarks for this task: MNIST, CIFAR-10, and ILSVRC2012, across a variety of network architectures. In addition, we discuss our observations of a common denominator between our pruning methodology's response to adversarial attacks and calibration statistics when compared to the original network.



### Visual link retrieval and knowledge discovery in painting datasets
- **Arxiv ID**: http://arxiv.org/abs/2003.08476v2
- **DOI**: 10.1007/s11042-020-09995-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08476v2)
- **Published**: 2020-03-18 21:16:33+00:00
- **Updated**: 2020-10-22 15:31:07+00:00
- **Authors**: Giovanna Castellano, Eufemia Lella, Gennaro Vessio
- **Comment**: Published on Multimedia Tools and Applications. Modified references.
  Corrected typos. Added observations according to reviewers
- **Journal**: None
- **Summary**: Visual arts are of inestimable importance for the cultural, historic and economic growth of our society. One of the building blocks of most analysis in visual arts is to find similarity relationships among paintings of different artists and painting schools. To help art historians better understand visual arts, this paper presents a framework for visual link retrieval and knowledge discovery in digital painting datasets. Visual link retrieval is accomplished by using a deep convolutional neural network to perform feature extraction and a fully unsupervised nearest neighbor mechanism to retrieve links among digitized paintings. Historical knowledge discovery is achieved by performing a graph analysis that makes it possible to study influences among artists. An experimental evaluation on a database collecting paintings by very popular artists shows the effectiveness of the method. The unsupervised strategy makes the method interesting especially in cases where metadata are scarce, unavailable or difficult to collect.



### Self-Supervised Contextual Bandits in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2003.08485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.08485v1)
- **Published**: 2020-03-18 22:06:34+00:00
- **Updated**: 2020-03-18 22:06:34+00:00
- **Authors**: Aniket Anand Deshmukh, Abhimanu Kumar, Levi Boyles, Denis Charles, Eren Manavoglu, Urun Dogan
- **Comment**: None
- **Journal**: None
- **Summary**: Contextual bandits are a common problem faced by machine learning practitioners in domains as diverse as hypothesis testing to product recommendations. There have been a lot of approaches in exploiting rich data representations for contextual bandit problems with varying degree of success. Self-supervised learning is a promising approach to find rich data representations without explicit labels. In a typical self-supervised learning scheme, the primary task is defined by the problem objective (e.g. clustering, classification, embedding generation etc.) and the secondary task is defined by the self-supervision objective (e.g. rotation prediction, words in neighborhood, colorization, etc.). In the usual self-supervision, we learn implicit labels from the training data for a secondary task. However, in the contextual bandit setting, we don't have the advantage of getting implicit labels due to lack of data in the initial phase of learning. We provide a novel approach to tackle this issue by combining a contextual bandit objective with a self supervision objective. By augmenting contextual bandit learning with self-supervision we get a better cumulative reward. Our results on eight popular computer vision datasets show substantial gains in cumulative reward. We provide cases where the proposed scheme doesn't perform optimally and give alternative methods for better learning in these cases.



### Gaze-Sensing LEDs for Head Mounted Displays
- **Arxiv ID**: http://arxiv.org/abs/2003.08499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.08499v1)
- **Published**: 2020-03-18 23:03:06+00:00
- **Updated**: 2020-03-18 23:03:06+00:00
- **Authors**: Kaan Akit, Jan Kautz, David Luebke
- **Comment**: 14 pages, 7 figures. THIS WORK WAS CONDUCTED IN 2015
- **Journal**: None
- **Summary**: We introduce a new gaze tracker for Head Mounted Displays (HMDs). We modify two off-the-shelf HMDs to be gaze-aware using Light Emitting Diodes (LEDs). Our key contribution is to exploit the sensing capability of LEDs to create low-power gaze tracker for virtual reality (VR) applications. This yields a simple approach using minimal hardware to achieve good accuracy and low latency using light-weight supervised Gaussian Process Regression (GPR) running on a mobile device. With our hardware, we show that Minkowski distance measure based GPR implementation outperforms the commonly used radial basis function-based support vector regression (SVR) without the need to precisely determine free parameters. We show that our gaze estimation method does not require complex dimension reduction techniques, feature extraction, or distortion corrections due to off-axis optical paths. We demonstrate two complete HMD prototypes with a sample eye-tracked application, and report on a series of subjective tests using our prototypes.



### Reconstructing Sinus Anatomy from Endoscopic Video -- Towards a Radiation-free Approach for Quantitative Longitudinal Assessment
- **Arxiv ID**: http://arxiv.org/abs/2003.08502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08502v2)
- **Published**: 2020-03-18 23:11:10+00:00
- **Updated**: 2020-07-03 03:34:07+00:00
- **Authors**: Xingtong Liu, Maia Stiber, Jindan Huang, Masaru Ishii, Gregory D. Hager, Russell H. Taylor, Mathias Unberath
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: Reconstructing accurate 3D surface models of sinus anatomy directly from an endoscopic video is a promising avenue for cross-sectional and longitudinal analysis to better understand the relationship between sinus anatomy and surgical outcomes. We present a patient-specific, learning-based method for 3D reconstruction of sinus surface anatomy directly and only from endoscopic videos. We demonstrate the effectiveness and accuracy of our method on in and ex vivo data where we compare to sparse reconstructions from Structure from Motion, dense reconstruction from COLMAP, and ground truth anatomy from CT. Our textured reconstructions are watertight and enable measurement of clinically relevant parameters in good agreement with CT. The source code is available at https://github.com/lppllppl920/DenseReconstruction-Pytorch.



### A Metric Learning Reality Check
- **Arxiv ID**: http://arxiv.org/abs/2003.08505v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08505v3)
- **Published**: 2020-03-18 23:28:04+00:00
- **Updated**: 2020-09-16 01:18:33+00:00
- **Authors**: Kevin Musgrave, Serge Belongie, Ser-Nam Lim
- **Comment**: Visit https://www.github.com/KevinMusgrave/powerful-benchmarker for
  supplementary material, including the source code, configuration files, log
  files, and interactive bayesian optimization plots
- **Journal**: None
- **Summary**: Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best.




# Arxiv Papers in cs.CV on 2020-03-12
### SeqXY2SeqZ: Structure Learning for 3D Shapes by Sequentially Predicting 1D Occupancy Segments From 2D Coordinates
- **Arxiv ID**: http://arxiv.org/abs/2003.05559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05559v2)
- **Published**: 2020-03-12 00:24:36+00:00
- **Updated**: 2020-03-16 15:06:39+00:00
- **Authors**: Zhizhong Han, Guanhui Qiao, Yu-Shen Liu, Matthias Zwicker
- **Comment**: None
- **Journal**: None
- **Summary**: Structure learning for 3D shapes is vital for 3D computer vision. State-of-the-art methods show promising results by representing shapes using implicit functions in 3D that are learned using discriminative neural networks. However, learning implicit functions requires dense and irregular sampling in 3D space, which also makes the sampling methods affect the accuracy of shape reconstruction during test. To avoid dense and irregular sampling in 3D, we propose to represent shapes using 2D functions, where the output of the function at each 2D location is a sequence of line segments inside the shape. Our approach leverages the power of functional representations, but without the disadvantage of 3D sampling. Specifically, we use a voxel tubelization to represent a voxel grid as a set of tubes along any one of the X, Y, or Z axes. Each tube can be indexed by its 2D coordinates on the plane spanned by the other two axes. We further simplify each tube into a sequence of occupancy segments. Each occupancy segment consists of successive voxels occupied by the shape, which leads to a simple representation of its 1D start and end location. Given the 2D coordinates of the tube and a shape feature as condition, this representation enables us to learn 3D shape structures by sequentially predicting the start and end locations of each occupancy segment in the tube. We implement this approach using a Seq2Seq model with attention, called SeqXY2SeqZ, which learns the mapping from a sequence of 2D coordinates along two arbitrary axes to a sequence of 1D locations along the third axis. SeqXY2SeqZ not only benefits from the regularity of voxel grids in training and testing, but also achieves high memory efficiency. Our experiments show that SeqXY2SeqZ outperforms the state-ofthe-art methods under widely used benchmarks.



### Extended Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2003.05569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05569v1)
- **Published**: 2020-03-12 01:53:15+00:00
- **Updated**: 2020-03-12 01:53:15+00:00
- **Authors**: Chunjie Luo, Jianfeng Zhan, Lei Wang, Wanling Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Batch normalization (BN) has become a standard technique for training the modern deep networks. However, its effectiveness diminishes when the batch size becomes smaller, since the batch statistics estimation becomes inaccurate. That hinders batch normalization's usage for 1) training larger model which requires small batches constrained by memory consumption, 2) training on mobile or embedded devices of which the memory resource is limited. In this paper, we propose a simple but effective method, called extended batch normalization (EBN). For NCHW format feature maps, extended batch normalization computes the mean along the (N, H, W) dimensions, as the same as batch normalization, to maintain the advantage of batch normalization. To alleviate the problem caused by small batch size, extended batch normalization computes the standard deviation along the (N, C, H, W) dimensions, thus enlarges the number of samples from which the standard deviation is computed. We compare extended batch normalization with batch normalization and group normalization on the datasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. The experiments show that extended batch normalization alleviates the problem of batch normalization with small batch size while achieving close performances to batch normalization with large batch size.



### ZSTAD: Zero-Shot Temporal Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.05583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05583v1)
- **Published**: 2020-03-12 02:40:36+00:00
- **Updated**: 2020-03-12 02:40:36+00:00
- **Authors**: Lingling Zhang, Xiaojun Chang, Jun Liu, Minnan Luo, Sen Wang, Zongyuan Ge, Alexander Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: An integral part of video analysis and surveillance is temporal activity detection, which means to simultaneously recognize and localize activities in long untrimmed videos. Currently, the most effective methods of temporal activity detection are based on deep learning, and they typically perform very well with large scale annotated videos for training. However, these methods are limited in real applications due to the unavailable videos about certain activity classes and the time-consuming data annotation. To solve this challenging problem, we propose a novel task setting called zero-shot temporal activity detection (ZSTAD), where activities that have never been seen in training can still be detected. We design an end-to-end deep network based on R-C3D as the architecture for this solution. The proposed network is optimized with an innovative loss function that considers the embeddings of activity labels and their super-classes while learning the common semantics of seen and unseen activities. Experiments on both the THUMOS14 and the Charades datasets show promising performance in terms of detecting unseen activities.



### Encoder-Decoder Based Convolutional Neural Networks with Multi-Scale-Aware Modules for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2003.05586v5
- **DOI**: 10.1109/ICPR48806.2021.9413286
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.05586v5)
- **Published**: 2020-03-12 03:00:26+00:00
- **Updated**: 2020-11-25 12:35:21+00:00
- **Authors**: Pongpisit Thanasutives, Ken-ichi Fukui, Masayuki Numao, Boonserm Kijsirikul
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: In this paper, we propose two modified neural networks based on dual path multi-scale fusion networks (SFANet) and SegNet for accurate and efficient crowd counting. Inspired by SFANet, the first model, which is named M-SFANet, is attached with atrous spatial pyramid pooling (ASPP) and context-aware module (CAN). The encoder of M-SFANet is enhanced with ASPP containing parallel atrous convolutional layers with different sampling rates and hence able to extract multi-scale features of the target object and incorporate larger context. To further deal with scale variation throughout an input image, we leverage the CAN module which adaptively encodes the scales of the contextual information. The combination yields an effective model for counting in both dense and sparse crowd scenes. Based on the SFANet decoder structure, M-SFANet's decoder has dual paths, for density map and attention map generation. The second model is called M-SegNet, which is produced by replacing the bilinear upsampling in SFANet with max unpooling that is used in SegNet. This change provides a faster model while providing competitive counting performance. Designed for high-speed surveillance applications, M-SegNet has no additional multi-scale-aware module in order to not increase the complexity. Both models are encoder-decoder based architectures and are end-to-end trainable. We conduct extensive experiments on five crowd counting datasets and one vehicle counting dataset to show that these modifications yield algorithms that could improve state-of-the-art crowd counting methods. Codes are available at https://github.com/Pongpisit-Thanasutives/Variations-of-SFANet-for-Crowd-Counting.



### Learning to Segment 3D Point Clouds in 2D Image Space
- **Arxiv ID**: http://arxiv.org/abs/2003.05593v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05593v4)
- **Published**: 2020-03-12 03:18:59+00:00
- **Updated**: 2020-10-07 23:27:31+00:00
- **Authors**: Yecheng Lyu, Xinming Huang, Ziming Zhang
- **Comment**: Accepted to CVPR 2020 as oral
- **Journal**: None
- **Summary**: In contrast to the literature where local patterns in 3D point clouds are captured by customized convolutional operators, in this paper we study the problem of how to effectively and efficiently project such point clouds into a 2D image space so that traditional 2D convolutional neural networks (CNNs) such as U-Net can be applied for segmentation. To this end, we are motivated by graph drawing and reformulate it as an integer programming problem to learn the topology-preserving graph-to-grid mapping for each individual point cloud. To accelerate the computation in practice, we further propose a novel hierarchical approximate algorithm. With the help of the Delaunay triangulation for graph construction from point clouds and a multi-scale U-Net for segmentation, we manage to demonstrate the state-of-the-art performance on ShapeNet and PartNet, respectively, with significant improvement over the literature. Code is available at https://github.com/Zhang-VISLab.



### On the Arbitrary-Oriented Object Detection: Classification based Approaches Revisited
- **Arxiv ID**: http://arxiv.org/abs/2003.05597v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2003.05597v4)
- **Published**: 2020-03-12 03:23:54+00:00
- **Updated**: 2022-03-23 14:58:55+00:00
- **Authors**: Xue Yang, Junchi Yan
- **Comment**: 19 pages, 16 figures, 18 tables, journal version of CSL (ECCV2020)
  and DCL (CVPR2021), accepted by IJCV2022
- **Journal**: None
- **Summary**: Arbitrary-oriented object detection has been a building block for rotation sensitive tasks. We first show that the boundary problem suffered in existing dominant regression-based rotation detectors, is caused by angular periodicity or corner ordering, according to the parameterization protocol. We also show that the root cause is that the ideal predictions can be out of the defined range. Accordingly, we transform the angular prediction task from a regression problem to a classification one. For the resulting circularly distributed angle classification problem, we first devise a Circular Smooth Label technique to handle the periodicity of angle and increase the error tolerance to adjacent angles. To reduce the excessive model parameters by Circular Smooth Label, we further design a Densely Coded Labels, which greatly reduces the length of the encoding. Finally, we further develop an object heading detection module, which can be useful when the exact heading orientation information is needed e.g. for ship and plane heading detection. We release our OHD-SJTU dataset and OHDet detector for heading detection. Extensive experimental results on three large-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU, and face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show the effectiveness of our approach.



### Beyond the Camera: Neural Networks in World Coordinates
- **Arxiv ID**: http://arxiv.org/abs/2003.05614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05614v1)
- **Published**: 2020-03-12 04:29:34+00:00
- **Updated**: 2020-03-12 04:29:34+00:00
- **Authors**: Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Karteek Alahari
- **Comment**: None
- **Journal**: None
- **Summary**: Eye movement and strategic placement of the visual field onto the retina, gives animals increased resolution of the scene and suppresses distracting information. This fundamental system has been missing from video understanding with deep networks, typically limited to 224 by 224 pixel content locked to the camera frame. We propose a simple idea, WorldFeatures, where each feature at every layer has a spatial transformation, and the feature map is only transformed as needed. We show that a network built with these WorldFeatures, can be used to model eye movements, such as saccades, fixation, and smooth pursuit, even in a batch setting on pre-recorded video. That is, the network can for example use all 224 by 224 pixels to look at a small detail one moment, and the whole scene the next. We show that typical building blocks, such as convolutions and pooling, can be adapted to support WorldFeatures using available tools. Experiments are presented on the Charades, Olympic Sports, and Caltech-UCSD Birds-200-2011 datasets, exploring action recognition, fine-grained recognition, and video stabilization.



### Understanding Crowd Flow Movements Using Active-Langevin Model
- **Arxiv ID**: http://arxiv.org/abs/2003.05626v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05626v3)
- **Published**: 2020-03-12 05:32:59+00:00
- **Updated**: 2020-08-18 07:31:57+00:00
- **Authors**: Shreetam Behera, Debi Prosad Dogra, Malay Kumar Bandyopadhyay, Partha Pratim Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd flow describes the elementary group behavior of crowds. Understanding the dynamics behind these movements can help to identify various abnormalities in crowds. However, developing a crowd model describing these flows is a challenging task. In this paper, a physics-based model is proposed to describe the movements in dense crowds. The crowd model is based on active Langevin equation where the motion points are assumed to be similar to active colloidal particles in fluids. The model is further augmented with computer-vision techniques to segment both linear and non-linear motion flows in a dense crowd. The evaluation of the active Langevin equation-based crowd segmentation has been done on publicly available crowd videos and on our own videos. The proposed method is able to segment the flow with lesser optical flow error and better accuracy in comparison to existing state-of-the-art methods.



### Highly Efficient Salient Object Detection with 100K Parameters
- **Arxiv ID**: http://arxiv.org/abs/2003.05643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05643v2)
- **Published**: 2020-03-12 07:00:46+00:00
- **Updated**: 2020-08-02 01:36:34+00:00
- **Authors**: Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, Shuicheng Yan
- **Comment**: Accepted by ECCV 2020. Source code: https://mmcheng.net/sod100k/
- **Journal**: ECCV 2020
- **Summary**: Salient object detection models often demand a considerable amount of computation cost to make precise prediction for each pixel, making them hardly applicable on low-power devices. In this paper, we aim to relieve the contradiction between computation cost and model performance by improving the network efficiency to a higher degree. We propose a flexible convolutional module, namely generalized OctConv (gOctConv), to efficiently utilize both in-stage and cross-stages multi-scale features, while reducing the representation redundancy by a novel dynamic weight decay scheme. The effective dynamic weight decay scheme stably boosts the sparsity of parameters during training, supports learnable number of channels for each scale in gOctConv, allowing 80% of parameters reduce with negligible performance drop. Utilizing gOctConv, we build an extremely light-weighted model, namely CSNet, which achieves comparable performance with about 0.2% parameters (100k) of large models on popular salient object detection benchmarks.



### Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images Using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.05653v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05653v3)
- **Published**: 2020-03-12 08:06:04+00:00
- **Updated**: 2020-07-13 08:41:09+00:00
- **Authors**: Jiangke Lin, Yi Yuan, Tianjia Shao, Kun Zhou
- **Comment**: Accepted to CVPR 2020. The source code is available at
  https://github.com/FuxiCV/3D-Face-GCNs
- **Journal**: None
- **Summary**: 3D Morphable Model (3DMM) based methods have achieved great success in recovering 3D face shapes from single-view images. However, the facial textures recovered by such methods lack the fidelity as exhibited in the input images. Recent work demonstrates high-quality facial texture recovering with generative networks trained from a large-scale database of high-resolution UV maps of face textures, which is hard to prepare and not publicly available. In this paper, we introduce a method to reconstruct 3D facial shapes with high-fidelity textures from single-view images in-the-wild, without the need to capture a large-scale face texture database. The main idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image. To this end, we propose to use graph convolutional networks to reconstruct the detailed colors for the mesh vertices instead of reconstructing the UV map. Experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.



### AirSim Drone Racing Lab
- **Arxiv ID**: http://arxiv.org/abs/2003.05654v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05654v1)
- **Published**: 2020-03-12 08:06:06+00:00
- **Updated**: 2020-03-12 08:06:06+00:00
- **Authors**: Ratnesh Madaan, Nicholas Gyde, Sai Vemprala, Matthew Brown, Keiko Nagami, Tim Taubner, Eric Cristofalo, Davide Scaramuzza, Mac Schwager, Ashish Kapoor
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: Autonomous drone racing is a challenging research problem at the intersection of computer vision, planning, state estimation, and control. We introduce AirSim Drone Racing Lab, a simulation framework for enabling fast prototyping of algorithms for autonomy and enabling machine learning research in this domain, with the goal of reducing the time, money, and risks associated with field robotics. Our framework enables generation of racing tracks in multiple photo-realistic environments, orchestration of drone races, comes with a suite of gate assets, allows for multiple sensor modalities (monocular, depth, neuromorphic events, optical flow), different camera models, and benchmarking of planning, control, computer vision, and learning-based algorithms. We used our framework to host a simulation based drone racing competition at NeurIPS 2019. The competition binaries are available at our github repository.



### Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.05656v1
- **DOI**: 10.1109/ICRA40945.2020.9196764
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05656v1)
- **Published**: 2020-03-12 08:11:09+00:00
- **Updated**: 2020-03-12 08:11:09+00:00
- **Authors**: Han Wang, Chen Wang, Lihua Xie
- **Comment**: Accepted in International Conference on Robotics and Automation
  (ICRA) 2020
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: Loop closure detection is an essential and challenging problem in simultaneous localization and mapping (SLAM). It is often tackled with light detection and ranging (LiDAR) sensor due to its view-point and illumination invariant properties. Existing works on 3D loop closure detection often leverage the matching of local or global geometrical-only descriptors, but without considering the intensity reading. In this paper we explore the intensity property from LiDAR scan and show that it can be effective for place recognition. Concretely, we propose a novel global descriptor, intensity scan context (ISC), that explores both geometry and intensity characteristics. To improve the efficiency for loop closure detection, an efficient two-stage hierarchical re-identification process is proposed, including a binary-operation based fast geometric relation retrieval and an intensity structure re-identification. Thorough experiments including both local experiment and public datasets test have been conducted to evaluate the performance of the proposed method. Our method achieves higher recall rate and recall precision than existing geometric-only methods.



### Open Source Computer Vision-based Layer-wise 3D Printing Analysis
- **Arxiv ID**: http://arxiv.org/abs/2003.05660v1
- **DOI**: 10.1016/j.addma.2020.101473
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05660v1)
- **Published**: 2020-03-12 08:33:10+00:00
- **Updated**: 2020-03-12 08:33:10+00:00
- **Authors**: Aliaksei L. Petsiuk, Joshua M. Pearce
- **Comment**: 29 pages, 19 figures
- **Journal**: None
- **Summary**: The paper describes an open source computer vision-based hardware structure and software algorithm, which analyzes layer-wise the 3-D printing processes, tracks printing errors, and generates appropriate printer actions to improve reliability. This approach is built upon multiple-stage monocular image examination, which allows monitoring both the external shape of the printed object and internal structure of its layers. Starting with the side-view height validation, the developed program analyzes the virtual top view for outer shell contour correspondence using the multi-template matching and iterative closest point algorithms, as well as inner layer texture quality clustering the spatial-frequency filter responses with Gaussian mixture models and segmenting structural anomalies with the agglomerative hierarchical clustering algorithm. This allows evaluation of both global and local parameters of the printing modes. The experimentally-verified analysis time per layer is less than one minute, which can be considered a quasi-real-time process for large prints. The systems can work as an intelligent printing suspension tool designed to save time and material. However, the results show the algorithm provides a means to systematize in situ printing data as a first step in a fully open source failure correction algorithm for additive manufacturing.



### Conditional Convolutions for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.05664v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05664v4)
- **Published**: 2020-03-12 08:42:36+00:00
- **Updated**: 2020-07-26 02:18:32+00:00
- **Authors**: Zhi Tian, Chunhua Shen, Hao Chen
- **Comment**: Accepted to Proc. European Conf. Computer Vision (ECCV) 2020 as oral
  presentation
- **Journal**: None
- **Summary**: We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: 1) Instance segmentation is solved by a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask RCNN baselines, without longer training schedules needed.   Code is available: https://github.com/aim-uofa/adet



### ARAE: Adversarially Robust Training of Autoencoders Improves Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.05669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05669v2)
- **Published**: 2020-03-12 09:06:41+00:00
- **Updated**: 2020-10-24 19:42:01+00:00
- **Authors**: Mohammadreza Salehi, Atrin Arya, Barbod Pajoum, Mohammad Otoofi, Amirreza Shaeiri, Mohammad Hossein Rohban, Hamid R. Rabiee
- **Comment**: None
- **Journal**: None
- **Summary**: Autoencoders (AE) have recently been widely employed to approach the novelty detection problem. Trained only on the normal data, the AE is expected to reconstruct the normal data effectively while fail to regenerate the anomalous data, which could be utilized for novelty detection. However, in this paper, it is demonstrated that this does not always hold. AE often generalizes so perfectly that it can also reconstruct the anomalous data well. To address this problem, we propose a novel AE that can learn more semantically meaningful features. Specifically, we exploit the fact that adversarial robustness promotes learning of meaningful features. Therefore, we force the AE to learn such features by penalizing networks with a bottleneck layer that is unstable against adversarial perturbations. We show that despite using a much simpler architecture in comparison to the prior methods, the proposed AE outperforms or is competitive to state-of-the-art on three benchmark datasets.



### Skeleton Based Action Recognition using a Stacked Denoising Autoencoder with Constraints of Privileged Information
- **Arxiv ID**: http://arxiv.org/abs/2003.05684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05684v1)
- **Published**: 2020-03-12 09:56:22+00:00
- **Updated**: 2020-03-12 09:56:22+00:00
- **Authors**: Zhize Wu, Thomas Weise, Le Zou, Fei Sun, Ming Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, with the availability of cost-effective depth cameras coupled with real-time skeleton estimation, the interest in skeleton-based human action recognition is renewed. Most of the existing skeletal representation approaches use either the joint location or the dynamics model. Differing from the previous studies, we propose a new method called Denoising Autoencoder with Temporal and Categorical Constraints (DAE_CTC)} to study the skeletal representation in a view of skeleton reconstruction. Based on the concept of learning under privileged information, we integrate action categories and temporal coordinates into a stacked denoising autoencoder in the training phase, to preserve category and temporal feature, while learning the hidden representation from a skeleton. Thus, we are able to improve the discriminative validity of the hidden representation. In order to mitigate the variation resulting from temporary misalignment, a new method of temporal registration, called Locally-Warped Sequence Registration (LWSR), is proposed for registering the sequences of inter- and intra-class actions. We finally represent the sequences using a Fourier Temporal Pyramid (FTP) representation and perform classification using a combination of LWSR registration, FTP representation, and a linear Support Vector Machine (SVM). The experimental results on three action data sets, namely MSR-Action3D, UTKinect-Action, and Florence3D-Action, show that our proposal performs better than many existing methods and comparably to the state of the art.



### Low-Rank and Total Variation Regularization and Its Application to Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/2003.05698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05698v1)
- **Published**: 2020-03-12 10:37:49+00:00
- **Updated**: 2020-03-12 10:37:49+00:00
- **Authors**: Pawan Goyal, Hussam Al Daas, Peter Benner
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of image recovery from given partial (corrupted) observations. Recovering an image using a low-rank model has been an active research area in data analysis and machine learning. But often, images are not only of low-rank but they also exhibit sparsity in a transformed space. In this work, we propose a new problem formulation in such a way that we seek to recover an image that is of low-rank and has sparsity in a transformed domain. We further discuss various non-convex non-smooth surrogates of the rank function, leading to a relaxed problem. Then, we present an efficient iterative scheme to solve the relaxed problem that essentially employs the (weighted) singular value thresholding at each iteration. Furthermore, we discuss the convergence properties of the proposed iterative method. We perform extensive experiments, showing that the proposed algorithm outperforms state-of-the-art methodologies in recovering images.



### Fairness by Learning Orthogonal Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2003.05707v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05707v3)
- **Published**: 2020-03-12 11:09:15+00:00
- **Updated**: 2020-07-04 09:04:10+00:00
- **Authors**: Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, Shadi Albarqouni
- **Comment**: None
- **Journal**: None
- **Summary**: Learning discriminative powerful representations is a crucial step for machine learning systems. Introducing invariance against arbitrary nuisance or sensitive attributes while performing well on specific tasks is an important problem in representation learning. This is mostly approached by purging the sensitive information from learned representations. In this paper, we propose a novel disentanglement approach to invariant representation problem. We disentangle the meaningful and sensitive representations by enforcing orthogonality constraints as a proxy for independence. We explicitly enforce the meaningful representation to be agnostic to sensitive information by entropy maximization. The proposed approach is evaluated on five publicly available datasets and compared with state of the art methods for learning fairness and invariance achieving the state of the art performance on three datasets and comparable performance on the rest. Further, we perform an ablative study to evaluate the effect of each component.



### Deformation Flow Based Two-Stream Network for Lip Reading
- **Arxiv ID**: http://arxiv.org/abs/2003.05709v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2003.05709v2)
- **Published**: 2020-03-12 11:13:44+00:00
- **Updated**: 2020-03-13 00:54:46+00:00
- **Authors**: Jingyun Xiao, Shuang Yang, Yuanhang Zhang, Shiguang Shan, Xilin Chen
- **Comment**: 7 pages, FG 2020
- **Journal**: None
- **Summary**: Lip reading is the task of recognizing the speech content by analyzing movements in the lip region when people are speaking. Observing on the continuity in adjacent frames in the speaking process, and the consistency of the motion patterns among different speakers when they pronounce the same phoneme, we model the lip movements in the speaking process as a sequence of apparent deformations in the lip region. Specifically, we introduce a Deformation Flow Network (DFN) to learn the deformation flow between adjacent frames, which directly captures the motion information within the lip region. The learned deformation flow is then combined with the original grayscale frames with a two-stream network to perform lip reading. Different from previous two-stream networks, we make the two streams learn from each other in the learning process by introducing a bidirectional knowledge distillation loss to train the two branches jointly. Owing to the complementary cues provided by different branches, the two-stream network shows a substantial improvement over using either single branch. A thorough experimental evaluation on two large-scale lip reading benchmarks is presented with detailed analysis. The results accord with our motivation, and show that our method achieves state-of-the-art or comparable performance on these two challenging datasets.



### EDC3: Ensemble of Deep-Classifiers using Class-specific Copula functions to Improve Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.05710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05710v1)
- **Published**: 2020-03-12 11:18:45+00:00
- **Updated**: 2020-03-12 11:18:45+00:00
- **Authors**: Somenath Kuiry, Nibaran Das, Alaka Das, Mita Nasipuri
- **Comment**: None
- **Journal**: None
- **Summary**: In the literature, many fusion techniques are registered for the segmentation of images, but they primarily focus on observed output or belief score or probability score of the output classes. In the present work, we have utilized inter source statistical dependency among different classifiers for ensembling of different deep learning techniques for semantic segmentation of images. For this purpose, in the present work, a class-wise Copula-based ensembling method is newly proposed for solving the multi-class segmentation problem. Experimentally, it is observed that the performance has improved more for semantic image segmentation using the proposed class-specific Copula function than the traditionally used single Copula function for the problem. The performance is also compared with three state-of-the-art ensembling methods.



### SynCGAN: Using learnable class specific priors to generate synthetic data for improving classifier performance on cytological images
- **Arxiv ID**: http://arxiv.org/abs/2003.05712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05712v1)
- **Published**: 2020-03-12 11:23:23+00:00
- **Updated**: 2020-03-12 11:23:23+00:00
- **Authors**: Soumyajyoti Dey, Soham Das, Swarnendu Ghosh, Shyamali Mitra, Sukanta Chakrabarty, Nibaran Das
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most challenging aspects of medical image analysis is the lack of a high quantity of annotated data. This makes it difficult for deep learning algorithms to perform well due to a lack of variations in the input space. While generative adversarial networks have shown promise in the field of synthetic data generation, but without a carefully designed prior the generation procedure can not be performed well. In the proposed approach we have demonstrated the use of automatically generated segmentation masks as learnable class-specific priors to guide a conditional GAN for the generation of patho-realistic samples for cytology image. We have observed that augmentation of data using the proposed pipeline called "SynCGAN" improves the performance of state of the art classifiers such as ResNet-152, DenseNet-161, Inception-V3 significantly.



### A Power-Efficient Binary-Weight Spiking Neural Network Architecture for Real-Time Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2003.06310v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AR, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.06310v1)
- **Published**: 2020-03-12 11:25:00+00:00
- **Updated**: 2020-03-12 11:25:00+00:00
- **Authors**: Pai-Yu Tan, Po-Yao Chuang, Yen-Ting Lin, Cheng-Wen Wu, Juin-Ming Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network hardware is considered an essential part of future edge devices. In this paper, we propose a binary-weight spiking neural network (BW-SNN) hardware architecture for low-power real-time object classification on edge platforms. This design stores a full neural network on-chip, and hence requires no off-chip bandwidth. The proposed systolic array maximizes data reuse for a typical convolutional layer. A 5-layer convolutional BW-SNN hardware is implemented in 90nm CMOS. Compared with state-of-the-art designs, the area cost and energy per classification are reduced by 7$\times$ and 23$\times$, respectively, while also achieving a higher accuracy on the MNIST benchmark. This is also a pioneering SNN hardware architecture that supports advanced CNN architectures.



### VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/2003.07289v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.07289v5)
- **Published**: 2020-03-12 14:52:10+00:00
- **Updated**: 2023-06-22 11:55:07+00:00
- **Authors**: Kaichen Zhou, Changhao Chen, Bing Wang, Muhamad Risqi U. Saputra, Niki Trigoni, Andrew Markham
- **Comment**: None
- **Journal**: The Thirty-Fifth AAAI Conference on Artificial Intelligence
  (AAAI-2021)
- **Summary**: Recent learning-based approaches have achieved impressive results in the field of single-shot camera localization. However, how best to fuse multiple modalities (e.g., image and depth) and to deal with degraded or missing input are less well studied. In particular, we note that previous approaches towards deep fusion do not perform significantly better than models employing a single modality. We conjecture that this is because of the naive approaches to feature space fusion through summation or concatenation which do not take into account the different strengths of each modality. To address this, we propose an end-to-end framework, termed VMLoc, to fuse different sensor inputs into a common latent space through a variational Product-of-Experts (PoE) followed by attention-based fusion. Unlike previous multimodal variational works directly adapting the objective function of vanilla variational auto-encoder, we show how camera localization can be accurately estimated through an unbiased objective function based on importance weighting. Our model is extensively evaluated on RGB-D datasets and the results prove the efficacy of our model. The source code is available at https://github.com/kaichen-z/VMLoc.



### Top-1 Solution of Multi-Moments in Time Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/2003.05837v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05837v2)
- **Published**: 2020-03-12 15:11:38+00:00
- **Updated**: 2020-03-13 11:53:24+00:00
- **Authors**: Manyuan Zhang, Hao Shao, Guanglu Song, Yu Liu, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we briefly introduce the solutions of our team 'Efficient' for the Multi-Moments in Time challenge in ICCV 2019. We first conduct several experiments with popular Image-Based action recognition methods TRN, TSN, and TSM. Then a novel temporal interlacing network is proposed towards fast and accurate recognition. Besides, the SlowFast network and its variants are explored. Finally, we ensemble all the above models and achieve 67.22\% on the validation set and 60.77\% on the test set, which ranks 1st on the final leaderboard. In addition, we release a new code repository for video understanding which unifies state-of-the-art 2D and 3D methods based on PyTorch. The solution of the challenge is also included in the repository, which is available at https://github.com/Sense-X/X-Temporal.



### Customized Video QoE Estimation with Algorithm-Agnostic Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.08730v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, I.2.6; I.2.11
- **Links**: [PDF](http://arxiv.org/pdf/2003.08730v1)
- **Published**: 2020-03-12 15:28:10+00:00
- **Updated**: 2020-03-12 15:28:10+00:00
- **Authors**: Selim Ickin, Markus Fiedler, Konstantinos Vandikas
- **Comment**: 6 pages, 4 figures, 6 tables, 18 references
- **Journal**: None
- **Summary**: The development of QoE models by means of Machine Learning (ML) is challenging, amongst others due to small-size datasets, lack of diversity in user profiles in the source domain, and too much diversity in the target domains of QoE models. Furthermore, datasets can be hard to share between research entities, as the machine learning models and the collected user data from the user studies may be IPR- or GDPR-sensitive. This makes a decentralized learning-based framework appealing for sharing and aggregating learned knowledge in-between the local models that map the obtained metrics to the user QoE, such as Mean Opinion Scores (MOS). In this paper, we present a transfer learning-based ML model training approach, which allows decentralized local models to share generic indicators on MOS to learn a generic base model, and then customize the generic base model further using additional features that are unique to those specific localized (and potentially sensitive) QoE nodes. We show that the proposed approach is agnostic to specific ML algorithms, stacked upon each other, as it does not necessitate the collaborating localized nodes to run the same ML algorithm. Our reproducible results reveal the advantages of stacking various generic and specific models with corresponding weight factors. Moreover, we identify the optimal combination of algorithms and weight factors for the corresponding localized QoE nodes.



### CPS++: Improving Class-level 6D Pose and Shape Estimation From Monocular Images With Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.05848v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05848v3)
- **Published**: 2020-03-12 15:28:13+00:00
- **Updated**: 2020-09-11 10:20:19+00:00
- **Authors**: Fabian Manhardt, Gu Wang, Benjamin Busam, Manuel Nickel, Sven Meier, Luca Minciullo, Xiangyang Ji, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Contemporary monocular 6D pose estimation methods can only cope with a handful of object instances. This naturally hampers possible applications as, for instance, robots seamlessly integrated in everyday processes necessarily require the ability to work with hundreds of different objects. To tackle this problem of immanent practical relevance, we propose a novel method for class-level monocular 6D pose estimation, coupled with metric shape retrieval. Unfortunately, acquiring adequate annotations is very time-consuming and labor intensive. This is especially true for class-level 6D pose estimation, as one is required to create a highly detailed reconstruction for all objects and then annotate each object and scene using these models. To overcome this shortcoming, we additionally propose the idea of synthetic-to-real domain transfer for class-level 6D poses by means of self-supervised learning, which removes the burden of collecting numerous manual annotations. In essence, after training our proposed method fully supervised with synthetic data, we leverage recent advances in differentiable rendering to self-supervise the model with unannotated real RGB-D data to improve latter inference. We experimentally demonstrate that we can retrieve precise 6D poses and metric shapes from a single RGB image.



### End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2003.05855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05855v2)
- **Published**: 2020-03-12 15:41:34+00:00
- **Updated**: 2020-03-16 14:32:05+00:00
- **Authors**: Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, Chiew-Lan Tai
- **Comment**: CVPR 2020. Webpage:
  https://github.com/craigleili/3DLocalMultiViewDesc
- **Journal**: None
- **Summary**: In this work, we propose an end-to-end framework to learn local multi-view descriptors for 3D point clouds. To adopt a similar multi-view representation, existing studies use hand-crafted viewpoints for rendering in a preprocessing stage, which is detached from the subsequent descriptor learning stage. In our framework, we integrate the multi-view rendering into neural networks by using a differentiable renderer, which allows the viewpoints to be optimizable parameters for capturing more informative local context of interest points. To obtain discriminative descriptors, we also design a soft-view pooling module to attentively fuse convolutional features across views. Extensive experiments on existing 3D registration benchmarks show that our method outperforms existing local descriptors both quantitatively and qualitatively.



### Towards Photo-Realistic Virtual Try-On by Adaptively Generating$\leftrightarrow$Preserving Image Content
- **Arxiv ID**: http://arxiv.org/abs/2003.05863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05863v1)
- **Published**: 2020-03-12 15:55:39+00:00
- **Updated**: 2020-03-12 15:55:39+00:00
- **Authors**: Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, Ping Luo
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Image visual try-on aims at transferring a target clothing image onto a reference person, and has become a hot topic in recent years. Prior arts usually focus on preserving the character of a clothing image (e.g. texture, logo, embroidery) when warping it to arbitrary human pose. However, it remains a big challenge to generate photo-realistic try-on images when large occlusions and human poses are presented in the reference person. To address this issue, we propose a novel visual try-on network, namely Adaptive Content Generating and Preserving Network (ACGPN). In particular, ACGPN first predicts semantic layout of the reference image that will be changed after try-on (e.g. long sleeve shirt$\rightarrow$arm, arm$\rightarrow$jacket), and then determines whether its image content needs to be generated or preserved according to the predicted semantic layout, leading to photo-realistic try-on and rich clothing details. ACGPN generally involves three major modules. First, a semantic layout generation module utilizes semantic segmentation of the reference image to progressively predict the desired semantic layout after try-on. Second, a clothes warping module warps clothing images according to the generated semantic layout, where a second-order difference constraint is introduced to stabilize the warping process during training. Third, an inpainting module for content fusion integrates all information (e.g. reference image, semantic layout, warped clothes) to adaptively produce each semantic part of human body. In comparison to the state-of-the-art methods, ACGPN can generate photo-realistic images with much better perceptual quality and richer fine-details.



### Fast Distance-based Anomaly Detection in Images Using an Inception-like Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2003.08731v1
- **DOI**: 10.1007/978-3-030-33778-0_37
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08731v1)
- **Published**: 2020-03-12 16:10:53+00:00
- **Updated**: 2020-03-12 16:10:53+00:00
- **Authors**: Natasa Sarafijanovic-Djukic, Jesse Davis
- **Comment**: 22nd International Conference on Discovery Science, DS 2019
- **Journal**: InInternational Conference on Discovery Science 2019 Oct 28 (pp.
  493-508). Springer, Cham
- **Summary**: The goal of anomaly detection is to identify examples that deviate from normal or expected behavior. We tackle this problem for images. We consider a two-phase approach. First, using normal examples, a convolutional autoencoder (CAE) is trained to extract a low-dimensional representation of the images. Here, we propose a novel architectural choice when designing the CAE, an Inception-like CAE. It combines convolutional filters of different kernel sizes and it uses a Global Average Pooling (GAP) operation to extract the representations from the CAE's bottleneck layer. Second, we employ a distanced-based anomaly detector in the low-dimensional space of the learned representation for the images. However, instead of computing the exact distance, we compute an approximate distance using product quantization. This alleviates the high memory and prediction time costs of distance-based anomaly detectors. We compare our proposed approach to a number of baselines and state-of-the-art methods on four image datasets, and we find that our approach resulted in improved predictive performance.



### Truncated Inference for Latent Variable Optimization Problems: Application to Robust Estimation and Learning
- **Arxiv ID**: http://arxiv.org/abs/2003.05886v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05886v1)
- **Published**: 2020-03-12 16:32:06+00:00
- **Updated**: 2020-03-12 16:32:06+00:00
- **Authors**: Christopher Zach, Huu Le
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Optimization problems with an auxiliary latent variable structure in addition to the main model parameters occur frequently in computer vision and machine learning. The additional latent variables make the underlying optimization task expensive, either in terms of memory (by maintaining the latent variables), or in terms of runtime (repeated exact inference of latent variables). We aim to remove the need to maintain the latent variables and propose two formally justified methods, that dynamically adapt the required accuracy of latent variable inference. These methods have applications in large scale robust estimation and in learning energy-based models from labeled data.



### SASL: Saliency-Adaptive Sparsity Learning for Neural Network Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2003.05891v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05891v3)
- **Published**: 2020-03-12 16:49:37+00:00
- **Updated**: 2020-07-30 02:40:13+00:00
- **Authors**: Jun Shi, Jianfeng Xu, Kazuyuki Tasaka, Zhibo Chen
- **Comment**: Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Accelerating the inference speed of CNNs is critical to their deployment in real-world applications. Among all the pruning approaches, those implementing a sparsity learning framework have shown to be effective as they learn and prune the models in an end-to-end data-driven manner. However, these works impose the same sparsity regularization on all filters indiscriminately, which can hardly result in an optimal structure-sparse network. In this paper, we propose a Saliency-Adaptive Sparsity Learning (SASL) approach for further optimization. A novel and effective estimation of each filter, i.e., saliency, is designed, which is measured from two aspects: the importance for the prediction performance and the consumed computational resources. During sparsity learning, the regularization strength is adjusted according to the saliency, so our optimized format can better preserve the prediction performance while zeroing out more computation-heavy filters. The calculation for saliency introduces minimum overhead to the training process, which means our SASL is very efficient. During the pruning phase, in order to optimize the proposed data-dependent criterion, a hard sample mining strategy is utilized, which shows higher effectiveness and efficiency. Extensive experiments demonstrate the superior performance of our method. Notably, on ILSVRC-2012 dataset, our approach can reduce 49.7% FLOPs of ResNet-50 with very negligible 0.39% top-1 and 0.05% top-5 accuracy degradation.



### Cascade EF-GAN: Progressive Facial Expression Editing with Local Focuses
- **Arxiv ID**: http://arxiv.org/abs/2003.05905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.05905v2)
- **Published**: 2020-03-12 17:11:44+00:00
- **Updated**: 2020-03-25 15:08:06+00:00
- **Authors**: Rongliang Wu, Gongjie Zhang, Shijian Lu, Tao Chen
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Recent advances in Generative Adversarial Nets (GANs) have shown remarkable improvements for facial expression editing. However, current methods are still prone to generate artifacts and blurs around expression-intensive regions, and often introduce undesired overlapping artifacts while handling large-gap expression transformations such as transformation from furious to laughing. To address these limitations, we propose Cascade Expression Focal GAN (Cascade EF-GAN), a novel network that performs progressive facial expression editing with local expression focuses. The introduction of the local focus enables the Cascade EF-GAN to better preserve identity-related features and details around eyes, noses and mouths, which further helps reduce artifacts and blurs within the generated facial images. In addition, an innovative cascade transformation strategy is designed by dividing a large facial expression transformation into multiple small ones in cascade, which helps suppress overlapping artifacts and produce more realistic editing while dealing with large-gap expression transformations. Extensive experiments over two publicly available facial expression datasets show that our proposed Cascade EF-GAN achieves superior performance for facial expression editing.



### Optimal HDR and Depth from Dual Cameras
- **Arxiv ID**: http://arxiv.org/abs/2003.05907v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05907v1)
- **Published**: 2020-03-12 17:14:08+00:00
- **Updated**: 2020-03-12 17:14:08+00:00
- **Authors**: Pradyumna Chari, Anil Kumar Vadathya, Kaushik Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Dual camera systems have assisted in the proliferation of various applications, such as optical zoom, low-light imaging and High Dynamic Range (HDR) imaging. In this work, we explore an optimal method for capturing the scene HDR and disparity map using dual camera setups. Hasinoff et al. (2010) have developed a noise optimal framework for HDR capture from a single camera. We generalize this to the dual camera set-up for estimating both HDR and disparity map. It may seem that dual camera systems can capture HDR in a shorter time. However, disparity estimation is a necessary step, which requires overlap among the images captured by the two cameras. This may lead to an increase in the capture time. To address this conflicting requirement, we propose a novel framework to find the optimal exposure and ISO sequence by minimizing the capture time under the constraints of an upper bound on the disparity error and a lower bound on the per-exposure SNR. We show that the resulting optimization problem is non-convex in general and propose an appropriate initialization technique. To obtain the HDR and disparity map from the optimal capture sequence, we propose a pipeline which alternates between estimating the camera ICRFs and the scene disparity map. We demonstrate that our optimal capture sequence leads to better results than other possible capture sequences. Our results are also close to those obtained by capturing the full stereo stack spanning the entire dynamic range. Finally, we present for the first time a stereo HDR dataset consisting of dense ISO and exposure stack captured from a smartphone dual camera. The dataset consists of 6 scenes, with an average of 142 exposure-ISO image sequence per scene.



### W2S: Microscopy Data with Joint Denoising and Super-Resolution for Widefield to SIM Mapping
- **Arxiv ID**: http://arxiv.org/abs/2003.05961v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05961v2)
- **Published**: 2020-03-12 18:15:09+00:00
- **Updated**: 2020-08-24 11:17:40+00:00
- **Authors**: Ruofan Zhou, Majed El Helou, Daniel Sage, Thierry Laroche, Arne Seitz, Sabine Ssstrunk
- **Comment**: ECCVW 2020. Project page: \<https://github.com/ivrl/w2s>
- **Journal**: None
- **Summary**: In fluorescence microscopy live-cell imaging, there is a critical trade-off between the signal-to-noise ratio and spatial resolution on one side, and the integrity of the biological sample on the other side. To obtain clean high-resolution (HR) images, one can either use microscopy techniques, such as structured-illumination microscopy (SIM), or apply denoising and super-resolution (SR) algorithms. However, the former option requires multiple shots that can damage the samples, and although efficient deep learning based algorithms exist for the latter option, no benchmark exists to evaluate these algorithms on the joint denoising and SR (JDSR) tasks. To study JDSR on microscopy data, we propose such a novel JDSR dataset, Widefield2SIM (W2S), acquired using a conventional fluorescence widefield and SIM imaging. W2S includes 144,000 real fluorescence microscopy images, resulting in a total of 360 sets of images. A set is comprised of noisy low-resolution (LR) widefield images with different noise levels, a noise-free LR image, and a corresponding high-quality HR SIM image. W2S allows us to benchmark the combinations of 6 denoising methods and 6 SR methods. We show that state-of-the-art SR networks perform very poorly on noisy inputs. Our evaluation also reveals that applying the best denoiser in terms of reconstruction error followed by the best SR method does not necessarily yield the best final result. Both quantitative and qualitative results show that SR networks are sensitive to noise and the sequential application of denoising and SR algorithms is sub-optimal. Lastly, we demonstrate that SR networks retrained end-to-end for JDSR outperform any combination of state-of-the-art deep denoising and SR networks



### LiDAR guided Small obstacle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.05970v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.05970v1)
- **Published**: 2020-03-12 18:34:46+00:00
- **Updated**: 2020-03-12 18:34:46+00:00
- **Authors**: Aasheesh Singh, Aditya Kamireddypalli, Vineet Gandhi, K Madhava Krishna
- **Comment**: 8 pages, Submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems, IROS 2020
- **Journal**: None
- **Summary**: Detecting small obstacles on the road is critical for autonomous driving. In this paper, we present a method to reliably detect such obstacles through a multi-modal framework of sparse LiDAR(VLP-16) and Monocular vision. LiDAR is employed to provide additional context in the form of confidence maps to monocular segmentation networks. We show significant performance gains when the context is fed as an additional input to monocular semantic segmentation frameworks. We further present a new semantic segmentation dataset to the community, comprising of over 3000 image frames with corresponding LiDAR observations. The images come with pixel-wise annotations of three classes off-road, road, and small obstacle. We stress that precise calibration between LiDAR and camera is crucial for this task and thus propose a novel Hausdorff distance based calibration refinement method over extrinsic parameters. As a first benchmark over this dataset, we report our results with 73% instance detection up to a distance of 50 meters on challenging scenarios. Qualitatively by showcasing accurate segmentation of obstacles less than 15 cms at 50m depth and quantitatively through favourable comparisons vis a vis prior art, we vindicate the method's efficacy. Our project-page and Dataset is hosted at https://small-obstacle-dataset.github.io/



### LaserFlow: Efficient and Probabilistic Object Detection and Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2003.05982v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.05982v4)
- **Published**: 2020-03-12 19:13:12+00:00
- **Updated**: 2020-10-15 20:57:25+00:00
- **Authors**: Gregory P. Meyer, Jake Charland, Shreyash Pandey, Ankit Laddha, Shivam Gautam, Carlos Vallespi-Gonzalez, Carl K. Wellington
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present LaserFlow, an efficient method for 3D object detection and motion forecasting from LiDAR. Unlike the previous work, our approach utilizes the native range view representation of the LiDAR, which enables our method to operate at the full range of the sensor in real-time without voxelization or compression of the data. We propose a new multi-sweep fusion architecture, which extracts and merges temporal features directly from the range images. Furthermore, we propose a novel technique for learning a probability distribution over future trajectories inspired by curriculum learning. We evaluate LaserFlow on two autonomous driving datasets and demonstrate competitive results when compared to the existing state-of-the-art methods.



### Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2003.05991v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.05991v2)
- **Published**: 2020-03-12 19:38:47+00:00
- **Updated**: 2021-04-03 11:18:12+00:00
- **Authors**: Dor Bank, Noam Koenigstein, Raja Giryes
- **Comment**: Book chapter
- **Journal**: None
- **Summary**: An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.



### Analyzing Visual Representations in Embodied Navigation Tasks
- **Arxiv ID**: http://arxiv.org/abs/2003.05993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.05993v1)
- **Published**: 2020-03-12 19:43:59+00:00
- **Updated**: 2020-03-12 19:43:59+00:00
- **Authors**: Erik Wijmans, Julian Straub, Dhruv Batra, Irfan Essa, Judy Hoffman, Ari Morcos
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep reinforcement learning require a large amount of training data and generally result in representations that are often over specialized to the target task. In this work, we present a methodology to study the underlying potential causes for this specialization. We use the recently proposed projection weighted Canonical Correlation Analysis (PWCCA) to measure the similarity of visual representations learned in the same environment by performing different tasks.   We then leverage our proposed methodology to examine the task dependence of visual representations learned on related but distinct embodied navigation tasks. Surprisingly, we find that slight differences in task have no measurable effect on the visual representation for both SqueezeNet and ResNet architectures. We then empirically demonstrate that visual representations learned on one task can be effectively transferred to a different task.



### Human Grasp Classification for Reactive Human-to-Robot Handovers
- **Arxiv ID**: http://arxiv.org/abs/2003.06000v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.06000v1)
- **Published**: 2020-03-12 19:58:03+00:00
- **Updated**: 2020-03-12 19:58:03+00:00
- **Authors**: Wei Yang, Chris Paxton, Maya Cakmak, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer of objects between humans and robots is a critical capability for collaborative robots. Although there has been a recent surge of interest in human-robot handovers, most prior research focus on robot-to-human handovers. Further, work on the equally critical human-to-robot handovers often assumes humans can place the object in the robot's gripper. In this paper, we propose an approach for human-to-robot handovers in which the robot meets the human halfway, by classifying the human's grasp of the object and quickly planning a trajectory accordingly to take the object from the human's hand according to their intent. To do this, we collect a human grasp dataset which covers typical ways of holding objects with various hand shapes and poses, and learn a deep model on this dataset to classify the hand grasps into one of these categories. We present a planning and execution approach that takes the object from the human hand according to the detected grasp and hand position, and replans as necessary when the handover is interrupted. Through a systematic evaluation, we demonstrate that our system results in more fluent handovers versus two baselines. We also present findings from a user study (N = 9) demonstrating the effectiveness and usability of our approach with naive users in different scenarios. More results and videos can be found at http://wyang.me/handovers.



### Dynamic Spatiotemporal Graph Neural Network with Tensor Network
- **Arxiv ID**: http://arxiv.org/abs/2003.08729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08729v1)
- **Published**: 2020-03-12 20:47:22+00:00
- **Updated**: 2020-03-12 20:47:22+00:00
- **Authors**: Chengcheng Jia, Bo Wu, Xiao-Ping Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic spatial graph construction is a challenge in graph neural network (GNN) for time series data problems. Although some adaptive graphs are conceivable, only a 2D graph is embedded in the network to reflect the current spatial relation, regardless of all the previous situations. In this work, we generate a spatial tensor graph (STG) to collect all the dynamic spatial relations, as well as a temporal tensor graph (TTG) to find the latent pattern along time at each node. These two tensor graphs share the same nodes and edges, which leading us to explore their entangled correlations by Projected Entangled Pair States (PEPS) to optimize the two graphs. We experimentally compare the accuracy and time costing with the state-of-the-art GNN based methods on the public traffic datasets.



### Interaction Graphs for Object Importance Estimation in On-road Driving Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.06045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.06045v1)
- **Published**: 2020-03-12 22:28:56+00:00
- **Updated**: 2020-03-12 22:28:56+00:00
- **Authors**: Zehua Zhang, Ashish Tawari, Sujitha Martin, David Crandall
- **Comment**: Accepted by ICRA 2020
- **Journal**: None
- **Summary**: A vehicle driving along the road is surrounded by many objects, but only a small subset of them influence the driver's decisions and actions. Learning to estimate the importance of each object on the driver's real-time decision-making may help better understand human driving behavior and lead to more reliable autonomous driving systems. Solving this problem requires models that understand the interactions between the ego-vehicle and the surrounding objects. However, interactions among other objects in the scene can potentially also be very helpful, e.g., a pedestrian beginning to cross the road between the ego-vehicle and the car in front will make the car in front less important. We propose a novel framework for object importance estimation using an interaction graph, in which the features of each object node are updated by interacting with others through graph convolution. Experiments show that our model outperforms state-of-the-art baselines with much less input and pre-processing.



### Deep Domain-Adversarial Image Generation for Domain Generalisation
- **Arxiv ID**: http://arxiv.org/abs/2003.06054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.06054v1)
- **Published**: 2020-03-12 23:17:47+00:00
- **Updated**: 2020-03-12 23:17:47+00:00
- **Authors**: Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on \emph{Deep Domain-Adversarial Image Generation} (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach.




# Arxiv Papers in cs.CV on 2020-03-03
### Single-Shot Pose Estimation of Surgical Robot Instruments' Shafts from Monocular Endoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2003.01267v1
- **DOI**: 10.1109/ICRA40945.2020.9196779
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01267v1)
- **Published**: 2020-03-03 00:38:48+00:00
- **Updated**: 2020-03-03 00:38:48+00:00
- **Authors**: Masakazu Yoshimura, Murilo M. Marinho, Kanako Harada, Mamoru Mitsuishi
- **Comment**: Accepted on ICRA 2020, 7 pages
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA), Paris, Palais des Congres de Paris, 2020, pp. 9960-9966
- **Summary**: Surgical robots are used to perform minimally invasive surgery and alleviate much of the burden imposed on surgeons. Our group has developed a surgical robot to aid in the removal of tumors at the base of the skull via access through the nostrils. To avoid injuring the patients, a collision-avoidance algorithm that depends on having an accurate model for the poses of the instruments' shafts is used. Given that the model's parameters can change over time owing to interactions between instruments and other disturbances, the online estimation of the poses of the instrument's shaft is essential. In this work, we propose a new method to estimate the pose of the surgical instruments' shafts using a monocular endoscope. Our method is based on the use of an automatically annotated training dataset and an improved pose-estimation deep-learning architecture. In preliminary experiments, we show that our method can surpass state of the art vision-based marker-less pose estimation techniques (providing an error decrease of 55% in position estimation, 64% in pitch, and 69% in yaw) by using artificial images.



### Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems
- **Arxiv ID**: http://arxiv.org/abs/2003.01279v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01279v3)
- **Published**: 2020-03-03 01:18:16+00:00
- **Updated**: 2020-04-27 19:58:25+00:00
- **Authors**: Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff
- **Comment**: Accepted at CVPR 2020 Workshop on Adversarial Machine Learning in
  Computer Vision
- **Journal**: None
- **Summary**: Face modification systems using deep learning have become increasingly powerful and accessible. Given images of a person's face, such systems can generate new images of that same person under different expressions and poses. Some systems can also modify targeted attributes such as hair color or age. This type of manipulated images and video have been coined Deepfakes. In order to prevent a malicious user from generating modified images of a person without their consent we tackle the new problem of generating adversarial attacks against such image translation systems, which disrupt the resulting output image. We call this problem disrupting deepfakes. Most image translation architectures are generative models conditioned on an attribute (e.g. put a smile on this person's face). We are first to propose and successfully apply (1) class transferable adversarial attacks that generalize to different classes, which means that the attacker does not need to have knowledge about the conditioning class, and (2) adversarial training for generative adversarial networks (GANs) as a first step towards robust image translation networks. Finally, in gray-box scenarios, blurring can mount a successful defense against disruption. We present a spread-spectrum adversarial attack, which evades blur defenses. Our open-source code can be found at https://github.com/natanielruiz/disrupting-deepfakes.



### Towards Noise-resistant Object Detection with Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/2003.01285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01285v1)
- **Published**: 2020-03-03 01:32:16+00:00
- **Updated**: 2020-03-03 01:32:16+00:00
- **Authors**: Junnan Li, Caiming Xiong, Richard Socher, Steven Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Training deep object detectors requires significant amount of human-annotated images with accurate object labels and bounding box coordinates, which are extremely expensive to acquire. Noisy annotations are much more easily accessible, but they could be detrimental for learning. We address the challenging problem of training object detectors with noisy annotations, where the noise contains a mixture of label noise and bounding box noise. We propose a learning framework which jointly optimizes object labels, bounding box coordinates, and model parameters by performing alternating noise correction and model training. To disentangle label noise and bounding box noise, we propose a two-step noise correction method. The first step performs class-agnostic bounding box correction by minimizing classifier discrepancy and maximizing region objectness. The second step distils knowledge from dual detection heads for soft label correction and class-specific bounding box refinement. We conduct experiments on PASCAL VOC and MS-COCO dataset with both synthetic noise and machine-generated noise. Our method achieves state-of-the-art performance by effectively cleaning both label noise and bounding box noise. Code to reproduce all results will be released.



### Trained Model Fusion for Object Detection using Gating Network
- **Arxiv ID**: http://arxiv.org/abs/2003.01288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01288v1)
- **Published**: 2020-03-03 01:38:20+00:00
- **Updated**: 2020-03-03 01:38:20+00:00
- **Authors**: Tetsuo Inoshita, Yuichi Nakatani, Katsuhiko Takahashi, Asuka Ishii, Gaku Nakano
- **Comment**: Accepted to ACPR 2019
- **Journal**: None
- **Summary**: The major approaches of transfer learning in computer vision have tried to adapt the source domain to the target domain one-to-one. However, this scenario is difficult to apply to real applications such as video surveillance systems. As those systems have many cameras installed at each location regarded as source domains, it is difficult to identify the proper source domain. In this paper, we introduce a new transfer learning scenario that has various source domains and one target domain, assuming video surveillance system integration. Also, we propose a novel method for automatically producing a high accuracy model by fusing models trained at various source domains. In particular, we show how to apply a gating network to fuse source domains for object detection tasks, which is a new approach. We demonstrate the effectiveness of our method through experiments on traffic surveillance datasets.



### Visualizing intestines for diagnostic assistance of ileus based on intestinal region segmentation from 3D CT images
- **Arxiv ID**: http://arxiv.org/abs/2003.01290v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01290v1)
- **Published**: 2020-03-03 01:40:51+00:00
- **Updated**: 2020-03-03 01:40:51+00:00
- **Authors**: Hirohisa Oda, Kohei Nishio, Takayuki Kitasaka, Hizuru Amano, Aitaro Takimoto, Hiroo Uchida, Kojiro Suzuki, Hayato Itoh, Masahiro Oda, Kensaku Mori
- **Comment**: None
- **Journal**: SPIE Medical Imaging 2020, 11314-109
- **Summary**: This paper presents a visualization method of intestine (the small and large intestines) regions and their stenosed parts caused by ileus from CT volumes. Since it is difficult for non-expert clinicians to find stenosed parts, the intestine and its stenosed parts should be visualized intuitively. Furthermore, the intestine regions of ileus cases are quite hard to be segmented. The proposed method segments intestine regions by 3D FCN (3D U-Net). Intestine regions are quite difficult to be segmented in ileus cases since the inside the intestine is filled with fluids. These fluids have similar intensities with intestinal wall on 3D CT volumes. We segment the intestine regions by using 3D U-Net trained by a weak annotation approach. Weak-annotation makes possible to train the 3D U-Net with small manually-traced label images of the intestine. This avoids us to prepare many annotation labels of the intestine that has long and winding shape. Each intestine segment is volume-rendered and colored based on the distance from its endpoint in volume rendering. Stenosed parts (disjoint points of an intestine segment) can be easily identified on such visualization. In the experiments, we showed that stenosed parts were intuitively visualized as endpoints of segmented regions, which are colored by red or blue.



### Dense Crowds Detection and Surveillance with Drones using Density Maps
- **Arxiv ID**: http://arxiv.org/abs/2003.08766v1
- **DOI**: 10.1109/ICUAS48674.2020.9213886
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2003.08766v1)
- **Published**: 2020-03-03 02:05:47+00:00
- **Updated**: 2020-03-03 02:05:47+00:00
- **Authors**: Javier Gonzalez-Trejo, Diego Mercado-Ravell
- **Comment**: 2020 International Conference on Unmanned Aircraft Systems (ICUAS),
  Athens, Greece, 2020
- **Journal**: None
- **Summary**: Detecting and Counting people in a human crowd from a moving drone present challenging problems that arisefrom the constant changing in the image perspective andcamera angle. In this paper, we test two different state-of-the-art approaches, density map generation with VGG19 trainedwith the Bayes loss function and detect-then-count with FasterRCNN with ResNet50-FPN as backbone, in order to comparetheir precision for counting and detecting people in differentreal scenarios taken from a drone flight. We show empiricallythat both proposed methodologies perform especially well fordetecting and counting people in sparse crowds when thedrone is near the ground. Nevertheless, VGG19 provides betterprecision on both tasks while also being lighter than FasterRCNN. Furthermore, VGG19 outperforms Faster RCNN whendealing with dense crowds, proving to be more robust toscale variations and strong occlusions, being more suitable forsurveillance applications using drones



### Data-Free Adversarial Perturbations for Practical Black-Box Attack
- **Arxiv ID**: http://arxiv.org/abs/2003.01295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01295v1)
- **Published**: 2020-03-03 02:22:12+00:00
- **Updated**: 2020-03-03 02:22:12+00:00
- **Authors**: ZhaoXin Huan, Yulong Wang, Xiaolu Zhang, Lin Shang, Chilin Fu, Jun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial examples, which are malicious inputs crafted to fool pre-trained models. Adversarial examples often exhibit black-box attacking transferability, which allows that adversarial examples crafted for one model can fool another model. However, existing black-box attack methods require samples from the training data distribution to improve the transferability of adversarial examples across different models. Because of the data dependence, the fooling ability of adversarial perturbations is only applicable when training data are accessible. In this paper, we present a data-free method for crafting adversarial perturbations that can fool a target model without any knowledge about the training data distribution. In the practical setting of a black-box attack scenario where attackers do not have access to target models and training data, our method achieves high fooling rates on target models and outperforms other universal adversarial perturbation methods. Our method empirically shows that current deep learning models are still at risk even when the attackers do not have access to training data.



### Gastric histopathology image segmentation using a hierarchical conditional random field
- **Arxiv ID**: http://arxiv.org/abs/2003.01302v5
- **DOI**: 10.1016/j.bbe.2020.09.008
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01302v5)
- **Published**: 2020-03-03 02:44:31+00:00
- **Updated**: 2020-10-19 14:03:28+00:00
- **Authors**: Changhao Sun, Chen Li, Jinghua Zhang, Muhammad Rahaman, Shiliang Ai, Hao Chen, Frank Kulwa, Yixin Li, Xiaoyan Li, Tao Jiang
- **Comment**: None
- **Journal**: Biocybernetics and Biomedical Engineering, 2020, 40(4): 1535-1555
- **Summary**: For the Convolutional Neural Networks (CNNs) applied in the intelligent diagnosis of gastric cancer, existing methods mostly focus on individual characteristics or network frameworks without a policy to depict the integral information. Mainly, Conditional Random Field (CRF), an efficient and stable algorithm for analyzing images containing complicated contents, can characterize spatial relation in images. In this paper, a novel Hierarchical Conditional Random Field (HCRF) based Gastric Histopathology Image Segmentation (GHIS) method is proposed, which can automatically localize abnormal (cancer) regions in gastric histopathology images obtained by an optical microscope to assist histopathologists in medical work. This HCRF model is built up with higher order potentials, including pixel-level and patch-level potentials, and graph-based post-processing is applied to further improve its segmentation performance. Especially, a CNN is trained to build up the pixel-level potentials and another three CNNs are fine-tuned to build up the patch-level potentials for sufficient spatial segmentation information. In the experiment, a hematoxylin and eosin (H&E) stained gastric histopathological dataset with 560 abnormal images are divided into training, validation and test sets with a ratio of 1 : 1 : 2. Finally, segmentation accuracy, recall and specificity of 78.91%, 65.59%, and 81.33% are achieved on the test set. Our HCRF model demonstrates high segmentation performance and shows its effectiveness and future potential in the GHIS field.



### Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for Facial and Emotion-Specified Expressions in Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2003.08759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.08759v2)
- **Published**: 2020-03-03 03:42:36+00:00
- **Updated**: 2020-09-08 18:36:44+00:00
- **Authors**: Marie Alaghband, Niloofar Yousefi, Ivan Garibay
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression datasets in the context of sign language are still scarce resources. In this manuscript, we introduce an annotated sequenced facial expression dataset in the context of sign language, comprising over $3000$ facial images extracted from the daily news and weather forecast of the public tv-station PHOENIX. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of "sad", "surprise", "fear", "angry", "neutral", "disgust", and "happy". We also considered the "None" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh as a facial expression dataset of signers in sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems.



### DDU-Nets: Distributed Dense Model for 3D MRI Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.01337v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01337v1)
- **Published**: 2020-03-03 05:08:34+00:00
- **Updated**: 2020-03-03 05:08:34+00:00
- **Authors**: Hanxiao Zhang, Jingxiong Li, Mali Shen, Yaqi Wang, Guang-Zhong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of brain tumors and their subregions remains a challenging task due to their weak features and deformable shapes. In this paper, three patterns (cross-skip, skip-1 and skip-2) of distributed dense connections (DDCs) are proposed to enhance feature reuse and propagation of CNNs by constructing tunnels between key layers of the network. For better detecting and segmenting brain tumors from multi-modal 3D MR images, CNN-based models embedded with DDCs (DDU-Nets) are trained efficiently from pixel to pixel with a limited number of parameters. Postprocessing is then applied to refine the segmentation results by reducing the false-positive samples. The proposed method is evaluated on the BraTS 2019 dataset with results demonstrating the effectiveness of the DDU-Nets while requiring less computational cost.



### ElixirNet: Relation-aware Network Architecture Adaptation for Medical Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.08770v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.08770v1)
- **Published**: 2020-03-03 05:29:49+00:00
- **Updated**: 2020-03-03 05:29:49+00:00
- **Authors**: Chenhan Jiang, Shaoju Wang, Hang Xu, Xiaodan Liang, Nong Xiao
- **Comment**: 7 pages, 5 figure, AAAI2020
- **Journal**: None
- **Summary**: Most advances in medical lesion detection network are limited to subtle modification on the conventional detection network designed for natural images. However, there exists a vast domain gap between medical images and natural images where the medical image detection often suffers from several domain-specific challenges, such as high lesion/background similarity, dominant tiny lesions, and severe class imbalance. Is a hand-crafted detection network tailored for natural image undoubtedly good enough over a discrepant medical lesion domain? Is there more powerful operations, filters, and sub-networks that better fit the medical lesion detection problem to be discovered? In this paper, we introduce a novel ElixirNet that includes three components: 1) TruncatedRPN balances positive and negative data for false positive reduction; 2) Auto-lesion Block is automatically customized for medical images to incorporate relation-aware operations among region proposals, and leads to more suitable and efficient classification and localization. 3) Relation transfer module incorporates the semantic relationship and transfers the relevant contextual information with an interpretable the graph thus alleviates the problem of lack of annotations for all types of lesions. Experiments on DeepLesion and Kits19 prove the effectiveness of ElixirNet, achieving improvement of both sensitivity and precision over FPN with fewer parameters.



### Shape analysis via inconsistent surface registration
- **Arxiv ID**: http://arxiv.org/abs/2003.01357v1
- **DOI**: 10.1098/rspa.2020.0147
- **Categories**: **cs.CG**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2003.01357v1)
- **Published**: 2020-03-03 06:58:16+00:00
- **Updated**: 2020-03-03 06:58:16+00:00
- **Authors**: Gary P. T. Choi, Di Qiu, Lok Ming Lui
- **Comment**: None
- **Journal**: Proceedings of the Royal Society A, 476(2242), 20200147 (2020)
- **Summary**: In this work, we develop a framework for shape analysis using inconsistent surface mapping. Traditional landmark-based geometric morphometrics methods suffer from the limited degrees of freedom, while most of the more advanced non-rigid surface mapping methods rely on a strong assumption of the global consistency of two surfaces. From a practical point of view, given two anatomical surfaces with prominent feature landmarks, it is more desirable to have a method that automatically detects the most relevant parts of the two surfaces and finds the optimal landmark-matching alignment between those parts, without assuming any global 1-1 correspondence between the two surfaces. Our method is capable of solving this problem using inconsistent surface registration based on quasi-conformal theory. It further enables us to quantify the dissimilarity of two shapes using quasi-conformal distortion and differences in mean and Gaussian curvatures, thereby providing a natural way for shape classification. Experiments on Platyrrhine molars demonstrate the effectiveness of our method and shed light on the interplay between function and shape in nature.



### DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-motion from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2003.01360v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2003.01360v3)
- **Published**: 2020-03-03 07:05:15+00:00
- **Updated**: 2020-11-20 06:31:22+00:00
- **Authors**: Hualie Jiang, Laiyan Ding, Zhenglong Sun, Rui Huang
- **Comment**: accepted by IROS 2020
- **Journal**: None
- **Summary**: Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn great attention, which avoids the use of expensive ground truth in the supervised one. It achieves this by using the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Despite significant progress, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difficulties better. The primary improvement is achieved by a statistical technique that can mask out the invisible or nonstationary pixels in the photometric error map and thus prevents misleading the networks. With this outlier masking approach, the depth of objects moving in the opposite direction to the camera can be estimated more accurately. To the best of our knowledge, such scenarios have not been seriously considered in the previous works, even though they pose a higher risk in applications like autonomous driving. We also propose an efficient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-of-theart performance on both depth and ego-motion estimation.



### multi-patch aggregation models for resampling detection
- **Arxiv ID**: http://arxiv.org/abs/2003.01364v1
- **DOI**: 10.1109/ICASSP40776.2020.9053005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01364v1)
- **Published**: 2020-03-03 07:19:56+00:00
- **Updated**: 2020-03-03 07:19:56+00:00
- **Authors**: Mohit Lamba, Kaushik Mitra
- **Comment**: 6 pages; 6 tables; 4 figures
- **Journal**: ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 2967-2971
- **Summary**: Images captured nowadays are of varying dimensions with smartphones and DSLR's allowing users to choose from a list of available image resolutions. It is therefore imperative for forensic algorithms such as resampling detection to scale well for images of varying dimensions. However, in our experiments, we observed that many state-of-the-art forensic algorithms are sensitive to image size and their performance quickly degenerates when operated on images of diverse dimensions despite re-training them using multiple image sizes. To handle this issue, we propose a novel pooling strategy called ITERATIVE POOLING. This pooling strategy can dynamically adjust input tensors in a discrete without much loss of information as in ROI Max-pooling. This pooling strategy can be used with any of the existing deep models and for demonstration purposes, we show its utility on Resnet-18 for the case of resampling detection a fundamental operation for any image sought of image manipulation. Compared to existing strategies and Max-pooling it gives up to 7-8% improvement on public datasets.



### Curriculum By Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2003.01367v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01367v5)
- **Published**: 2020-03-03 07:27:44+00:00
- **Updated**: 2021-01-05 04:53:44+00:00
- **Authors**: Samarth Sinha, Animesh Garg, Hugo Larochelle
- **Comment**: NeurIPS 2020 (Spotlight)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have shown impressive performance in computer vision tasks such as image classification, detection, and segmentation. Moreover, recent work in Generative Adversarial Networks (GANs) has highlighted the importance of learning by progressively increasing the difficulty of a learning task [26]. When learning a network from scratch, the information propagated within the network during the earlier stages of training can contain distortion artifacts due to noise which can be detrimental to training. In this paper, we propose an elegant curriculum based scheme that smoothes the feature embedding of a CNN using anti-aliasing or low-pass filters. We propose to augment the train-ing of CNNs by controlling the amount of high frequency information propagated within the CNNs as training progresses, by convolving the output of a CNN feature map of each layer with a Gaussian kernel. By decreasing the variance of the Gaussian kernel, we gradually increase the amount of high-frequency information available within the network for inference. As the amount of information in the feature maps increases during training, the network is able to progressively learn better representations of the data. Our proposed augmented training scheme significantly improves the performance of CNNs on various vision tasks without either adding additional trainable parameters or an auxiliary regularization objective. The generality of our method is demonstrated through empirical performance gains in CNN architectures across four different tasks: transfer learning, cross-task transfer learning, and generative models.



### Fully Convolutional Networks for Automatically Generating Image Masks to Train Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2003.01383v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01383v2)
- **Published**: 2020-03-03 08:09:29+00:00
- **Updated**: 2021-05-20 06:53:45+00:00
- **Authors**: Hao Wu, Jan Paul Siebert, Xiangrong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel automatically generating image masks method for the state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method achieves the best results in object detection until now, however, it is very time-consuming and laborious to get the object Masks for training, the proposed method is composed by a two-stage design, to automatically generating image masks, the first stage implements a fully convolutional networks (FCN) based segmentation network, the second stage network, a Mask R-CNN based object detection network, which is trained on the object image masks from FCN output, the original input image, and additional label information. Through experimentation, our proposed method can obtain the image masks automatically to train Mask R-CNN, and it can achieve very high classification accuracy with an over 90% mean of average precision (mAP) for segmentation



### DeepSperm: A robust and real-time bull sperm-cell detection in densely populated semen videos
- **Arxiv ID**: http://arxiv.org/abs/2003.01395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01395v1)
- **Published**: 2020-03-03 09:05:05+00:00
- **Updated**: 2020-03-03 09:05:05+00:00
- **Authors**: Priyanto Hidayatullah, Xueting Wang, Toshihiko Yamasaki, Tati L. E. R. Mengko, Rinaldi Munir, Anggraini Barlian, Eros Sukmawati, Supraptono Supraptono
- **Comment**: 22 pages, 8 figures, 6 tables, submitted to Computer Methods and
  Programs in Biomedicine
- **Journal**: None
- **Summary**: Background and Objective: Object detection is a primary research interest in computer vision. Sperm-cell detection in a densely populated bull semen microscopic observation video presents challenges such as partial occlusion, vast number of objects in a single video frame, tiny size of the object, artifacts, low contrast, and blurry objects because of the rapid movement of the sperm cells. This study proposes an architecture, called DeepSperm, that solves the aforementioned challenges and is more accurate and faster than state-of-the-art architectures. Methods: In the proposed architecture, we use only one detection layer, which is specific for small object detection. For handling overfitting and increasing accuracy, we set a higher network resolution, use a dropout layer, and perform data augmentation on hue, saturation, and exposure. Several hyper-parameters are tuned to achieve better performance. We compare our proposed method with those of a conventional image processing-based object-detection method, you only look once (YOLOv3), and mask region-based convolutional neural network (Mask R-CNN). Results: In our experiment, we achieve 86.91 mAP on the test dataset and a processing speed of 50.3 fps. In comparison with YOLOv3, we achieve an increase of 16.66 mAP point, 3.26 x faster on testing, and 1.4 x faster on training with a small training dataset, which contains 40 video frames. The weights file size was also reduced significantly, with 16.94 x smaller than that of YOLOv3. Moreover, it requires 1.3 x less graphical processing unit (GPU) memory than YOLOv3. Conclusions: This study proposes DeepSperm, which is a simple, effective, and efficient architecture with its hyper-parameters and configuration to detect bull sperm cells robustly in real time. In our experiment, we surpass the state of the art in terms of accuracy, speed, and resource needs.



### What's the relationship between CNNs and communication systems?
- **Arxiv ID**: http://arxiv.org/abs/2003.01413v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP, 68T45, I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/2003.01413v1)
- **Published**: 2020-03-03 09:50:46+00:00
- **Updated**: 2020-03-03 09:50:46+00:00
- **Authors**: Hao Ge, Xiaoguang Tu, Yanxiang Gong, Mei Xie, Zheng Ma
- **Comment**: Deep learning, adversarial example, interpretability
- **Journal**: None
- **Summary**: The interpretability of Convolutional Neural Networks (CNNs) is an important topic in the field of computer vision. In recent years, works in this field generally adopt a mature model to reveal the internal mechanism of CNNs, helping to understand CNNs thoroughly. In this paper, we argue the working mechanism of CNNs can be revealed through a totally different interpretation, by comparing the communication systems and CNNs. This paper successfully obtained the corresponding relationship between the modules of the two, and verified the rationality of the corresponding relationship with experiments. Finally, through the analysis of some cutting-edge research on neural networks, we find the inherent relation between these two tasks can be of help in explaining these researches reasonably, as well as helping us discover the correct research direction of neural networks.



### A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing
- **Arxiv ID**: http://arxiv.org/abs/2003.01446v2
- **DOI**: 10.1109/TCSVT.2021.3100059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01446v2)
- **Published**: 2020-03-03 10:57:52+00:00
- **Updated**: 2021-07-28 01:32:42+00:00
- **Authors**: Chongwei Liu, Zhihui Wang, Shijie Wang, Tao Tang, Yulong Tao, Caifei Yang, Haojie Li, Xing Liu, Xin Fan
- **Comment**: 14 pages, 10 figures
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology
  2021
- **Summary**: To boost the object grabbing capability of underwater robots for open-sea farming, we propose a new dataset (UDD) consisting of three categories (seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our knowledge, it is the first 4K HD dataset collected in a real open-sea farm. We also propose a novel Poisson-blending Generative Adversarial Network (Poisson GAN) and an efficient object detection network (AquaNet) to address two common issues within related datasets: the class-imbalance problem and the problem of mass small object, respectively. Specifically, Poisson GAN combines Poisson blending into its generator and employs a new loss called Dual Restriction loss (DR loss), which supervises both implicit space features and image-level features during training to generate more realistic images. By utilizing Poisson GAN, objects of minority class like seacucumber or scallop could be added into an image naturally and annotated automatically, which could increase the loss of minority classes during training detectors to eliminate the class-imbalance problem; AquaNet is a high-efficiency detector to address the problem of detecting mass small objects from cloudy underwater pictures. Within it, we design two efficient components: a depth-wise-convolution-based Multi-scale Contextual Features Fusion (MFF) block and a Multi-scale Blursampling (MBP) module to reduce the parameters of the network to 1.3 million. Both two components could provide multi-scale features of small objects under a short backbone configuration without any loss of accuracy. In addition, we construct a large-scale augmented dataset (AUDD) and a pre-training dataset via Poisson GAN from UDD. Extensive experiments show the effectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training dataset.



### 3D dynamic hand gestures recognition using the Leap Motion sensor and convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2003.01450v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01450v3)
- **Published**: 2020-03-03 11:05:35+00:00
- **Updated**: 2020-09-02 15:13:22+00:00
- **Authors**: Katia Lupinetti, Andrea Ranieri, Franca Giannini, Marina Monti
- **Comment**: Conference paper, 19 pages. UPDATE 20200311: changed in ref [19]
  'International Journal of Engineering and Technology' -> ' International
  Journal of Engineering and Technology Innovation'. UPDATE 20200902: changed
  every LMHGD occurrence to LMDHG
- **Journal**: None
- **Summary**: Defining methods for the automatic understanding of gestures is of paramount importance in many application contexts and in Virtual Reality applications for creating more natural and easy-to-use human-computer interaction methods. In this paper, we present a method for the recognition of a set of non-static gestures acquired through the Leap Motion sensor. The acquired gesture information is converted in color images, where the variation of hand joint positions during the gesture are projected on a plane and temporal information is represented with color intensity of the projected points. The classification of the gestures is performed using a deep Convolutional Neural Network (CNN). A modified version of the popular ResNet-50 architecture is adopted, obtained by removing the last fully connected layer and adding a new layer with as many neurons as the considered gesture classes. The method has been successfully applied to the existing reference dataset and preliminary tests have already been performed for the real-time recognition of dynamic gestures performed by users.



### Rethinking Zero-shot Video Classification: End-to-end Training for Realistic Applications
- **Arxiv ID**: http://arxiv.org/abs/2003.01455v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01455v4)
- **Published**: 2020-03-03 11:09:59+00:00
- **Updated**: 2020-06-20 08:22:45+00:00
- **Authors**: Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, Krzysztof Chalupka
- **Comment**: Accepted for publication at CVPR 2020
- **Journal**: None
- **Summary**: Trained on large datasets, deep learning (DL) can accurately classify videos into hundreds of diverse classes. However, video data is expensive to annotate. Zero-shot learning (ZSL) proposes one solution to this problem. ZSL trains a model once, and generalizes to new tasks whose classes are not present in the training dataset. We propose the first end-to-end algorithm for ZSL in video classification. Our training procedure builds on insights from recent video classification literature and uses a trainable 3D CNN to learn the visual features. This is in contrast to previous video ZSL methods, which use pretrained feature extractors. We also extend the current benchmarking paradigm: Previous techniques aim to make the test task unknown at training time but fall short of this goal. We encourage domain shift across training and test data and disallow tailoring a ZSL model to a specific test dataset. We outperform the state-of-the-art by a wide margin. Our code, evaluation procedure and model weights are available at github.com/bbrattoli/ZeroShotVideoClassification.



### Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion
- **Arxiv ID**: http://arxiv.org/abs/2003.01456v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01456v2)
- **Published**: 2020-03-03 11:14:29+00:00
- **Updated**: 2020-04-15 14:47:27+00:00
- **Authors**: Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll
- **Comment**: {IEEE} Conference on Computer Vision and Pattern Recognition
  (CVPR)2020
- **Journal**: {IEEE} Conference on Computer Vision and Pattern Recognition
  (CVPR) 2020
- **Summary**: While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.



### Disentangling Physical Dynamics from Unknown Factors for Unsupervised Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2003.01460v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01460v2)
- **Published**: 2020-03-03 11:26:40+00:00
- **Updated**: 2020-03-16 20:51:34+00:00
- **Authors**: Vincent Le Guen, Nicolas Thome
- **Comment**: None
- **Journal**: None
- **Summary**: Leveraging physical knowledge described by partial differential equations (PDEs) is an appealing way to improve unsupervised video prediction methods. Since physics is too restrictive for describing the full visual content of generic videos, we introduce PhyDNet, a two-branch deep architecture, which explicitly disentangles PDE dynamics from unknown complementary information. A second contribution is to propose a new recurrent physical cell (PhyCell), inspired from data assimilation techniques, for performing PDE-constrained prediction in latent space. Extensive experiments conducted on four various datasets show the ability of PhyDNet to outperform state-of-the-art methods. Ablation studies also highlight the important gain brought out by both disentanglement and PDE-constrained prediction. Finally, we show that PhyDNet presents interesting features for dealing with missing data and long-term forecasting.



### XGPT: Cross-modal Generative Pre-Training for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2003.01473v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01473v2)
- **Published**: 2020-03-03 12:13:06+00:00
- **Updated**: 2020-03-04 07:56:09+00:00
- **Authors**: Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon Bharti, Xin Liu, Ming Zhou
- **Comment**: 12 pages, 3 figures, 7 tables
- **Journal**: None
- **Summary**: While many BERT-based cross-modal pre-trained models produce excellent results on downstream understanding tasks like image-text retrieval and VQA, they cannot be applied to generation tasks directly. In this paper, we propose XGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning that is designed to pre-train text-to-image caption generators through three novel generation tasks, including Image-conditioned Masked Language Modeling (IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned Image Feature Generation (TIFG). As a result, the pre-trained XGPT can be fine-tuned without any task-specific architecture modifications to create state-of-the-art models for image captioning. Experiments show that XGPT obtains new state-of-the-art results on the benchmark datasets, including COCO Captions and Flickr30k Captions. We also use XGPT to generate new image captions as data augmentation for the image retrieval task and achieve significant improvement on all recall metrics.



### Anytime Inference with Distilled Hierarchical Neural Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2003.01474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01474v3)
- **Published**: 2020-03-03 12:13:38+00:00
- **Updated**: 2020-12-14 07:26:50+00:00
- **Authors**: Adria Ruiz, Jakob Verbeek
- **Comment**: None
- **Journal**: AAAI Conference on Artificial Intelligence 2021 (AAAI2021)
- **Summary**: Inference in deep neural networks can be computationally expensive, and networks capable of anytime inference are important in mscenarios where the amount of compute or quantity of input data varies over time. In such networks the inference process can interrupted to provide a result faster, or continued to obtain a more accurate result. We propose Hierarchical Neural Ensembles (HNE), a novel framework to embed an ensemble of multiple networks in a hierarchical tree structure, sharing intermediate layers. In HNE we control the complexity of inference on-the-fly by evaluating more or less models in the ensemble. Our second contribution is a novel hierarchical distillation method to boost the prediction accuracy of small ensembles. This approach leverages the nested structure of our ensembles, to optimally allocate accuracy and diversity across the individual models. Our experiments show that, compared to previous anytime inference models, HNE provides state-of-the-art accuracy-computate trade-offs on the CIFAR-10/100 and ImageNet datasets.



### BUSU-Net: An Ensemble U-Net Framework for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2003.01581v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01581v2)
- **Published**: 2020-03-03 15:18:01+00:00
- **Updated**: 2020-03-08 11:51:51+00:00
- **Authors**: Wei Hao Khoong
- **Comment**: GitHub link to the model scripts and trained model weights can be
  found in the manuscript. Version 2: Added S-UNet's Mi-UNet results for
  comparison and reference
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) have revolutionized medical image analysis. One of the most well-known CNN architectures in semantic segmentation is the U-net, which has achieved much success in several medical image segmentation applications. Also more recently, with the rise of autoML ad advancements in neural architecture search (NAS), methods like NAS-Unet have been proposed for NAS in medical image segmentation. In this paper, with inspiration from LadderNet, U-Net, autoML and NAS, we propose an ensemble deep neural network with an underlying U-Net framework consisting of bi-directional convolutional LSTMs and dense connections, where the first (from left) U-Net-like network is deeper than the second (from left). We show that this ensemble network outperforms recent state-of-the-art networks in several evaluation metrics, and also evaluate a lightweight version of this ensemble network, which also outperforms recent state-of-the-art networks in some evaluation metrics.



### Image Matching across Wide Baselines: From Paper to Practice
- **Arxiv ID**: http://arxiv.org/abs/2003.01587v5
- **DOI**: 10.1007/s11263-020-01385-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01587v5)
- **Published**: 2020-03-03 15:20:57+00:00
- **Updated**: 2021-02-11 13:50:17+00:00
- **Authors**: Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, Eduard Trulls
- **Comment**: Added: KeyNet-SOSNet, AffNet-HardNet, TFeat, MKD from kornia
- **Journal**: None
- **Summary**: We introduce a comprehensive benchmark for local features and robust estimation algorithms, focusing on the downstream task -- the accuracy of the reconstructed camera pose -- as our primary metric. Our pipeline's modular structure allows easy integration, configuration, and combination of different methods and heuristics. This is demonstrated by embedding dozens of popular algorithms and evaluating them, from seminal works to the cutting edge of machine learning research. We show that with proper settings, classical solutions may still outperform the perceived state of the art.   Besides establishing the actual state of the art, the conducted experiments reveal unexpected properties of Structure from Motion (SfM) pipelines that can help improve their performance, for both algorithmic and learned methods. Data and code are online https://github.com/vcg-uvic/image-matching-benchmark, providing an easy-to-use and flexible framework for the benchmarking of local features and robust estimation methods, both alongside and against top-performing methods. This work provides a basis for the Image Matching Challenge https://vision.uvic.ca/image-matching-challenge.



### Image-based OoD-Detector Principles on Graph-based Input Data in Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.01719v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01719v1)
- **Published**: 2020-03-03 15:38:43+00:00
- **Updated**: 2020-03-03 15:38:43+00:00
- **Authors**: Jens Bayer, David MÃ¼nch, Michael Arens
- **Comment**: None
- **Journal**: None
- **Summary**: Living in a complex world like ours makes it unacceptable that a practical implementation of a machine learning system assumes a closed world. Therefore, it is necessary for such a learning-based system in a real world environment, to be aware of its own capabilities and limits and to be able to distinguish between confident and unconfident results of the inference, especially if the sample cannot be explained by the underlying distribution. This knowledge is particularly essential in safety-critical environments and tasks e.g. self-driving cars or medical applications. Towards this end, we transfer image-based Out-of-Distribution (OoD)-methods to graph-based data and show the applicability in action recognition. The contribution of this work is (i) the examination of the portability of recent image-based OoD-detectors for graph-based input data, (ii) a Metric Learning-based approach to detect OoD-samples, and (iii) the introduction of a novel semi-synthetic action recognition dataset. The evaluation shows that image-based OoD-methods can be applied to graph-based data. Additionally, there is a gap between the performance on intraclass and intradataset results. First methods as the examined baseline or ODIN provide reasonable results. More sophisticated network architectures - in contrast to their image-based application - were surpassed in the intradataset comparison and even lead to less classification accuracy.



### Deep Multi-Modal Sets
- **Arxiv ID**: http://arxiv.org/abs/2003.01607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01607v1)
- **Published**: 2020-03-03 15:48:44+00:00
- **Updated**: 2020-03-03 15:48:44+00:00
- **Authors**: Austin Reiter, Menglin Jia, Pu Yang, Ser-Nam Lim
- **Comment**: 10 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Many vision-related tasks benefit from reasoning over multiple modalities to leverage complementary views of data in an attempt to learn robust embedding spaces. Most deep learning-based methods rely on a late fusion technique whereby multiple feature types are encoded and concatenated and then a multi layer perceptron (MLP) combines the fused embedding to make predictions. This has several limitations, such as an unnatural enforcement that all features be present at all times as well as constraining only a constant number of occurrences of a feature modality at any given time. Furthermore, as more modalities are added, the concatenated embedding grows. To mitigate this, we propose Deep Multi-Modal Sets: a technique that represents a collection of features as an unordered set rather than one long ever-growing fixed-size vector. The set is constructed so that we have invariance both to permutations of the feature modalities as well as to the cardinality of the set. We will also show that with particular choices in our model architecture, we can yield interpretable feature performance such that during inference time we can observe which modalities are most contributing to the prediction.With this in mind, we demonstrate a scalable, multi-modal framework that reasons over different modalities to learn various types of tasks. We demonstrate new state-of-the-art performance on two multi-modal datasets (Ads-Parallelity [34] and MM-IMDb [1]).



### Volumetric landmark detection with a multi-scale shift equivariant neural network
- **Arxiv ID**: http://arxiv.org/abs/2003.01639v2
- **DOI**: 10.1109/ISBI45749.2020.9098620
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01639v2)
- **Published**: 2020-03-03 17:06:19+00:00
- **Updated**: 2020-10-16 18:40:06+00:00
- **Authors**: Tianyu Ma, Ajay Gupta, Mert R. Sabuncu
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Deep neural networks yield promising results in a wide range of computer vision applications, including landmark detection. A major challenge for accurate anatomical landmark detection in volumetric images such as clinical CT scans is that large-scale data often constrain the capacity of the employed neural network architecture due to GPU memory limitations, which in turn can limit the precision of the output. We propose a multi-scale, end-to-end deep learning method that achieves fast and memory-efficient landmark detection in 3D images. Our architecture consists of blocks of shift-equivariant networks, each of which performs landmark detection at a different spatial scale. These blocks are connected from coarse to fine-scale, with differentiable resampling layers, so that all levels can be trained together. We also present a noise injection strategy that increases the robustness of the model and allows us to quantify uncertainty at test time. We evaluate our method for carotid artery bifurcations detection on 263 CT volumes and achieve a better than state-of-the-art accuracy with mean Euclidean distance error of 2.81mm.



### Unsupervised Learning of Intrinsic Structural Representation Points
- **Arxiv ID**: http://arxiv.org/abs/2003.01661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01661v2)
- **Published**: 2020-03-03 17:40:00+00:00
- **Updated**: 2020-03-26 11:54:35+00:00
- **Authors**: Nenglun Chen, Lingjie Liu, Zhiming Cui, Runnan Chen, Duygu Ceylan, Changhe Tu, Wenping Wang
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Learning structures of 3D shapes is a fundamental problem in the field of computer graphics and geometry processing. We present a simple yet interpretable unsupervised method for learning a new structural representation in the form of 3D structure points. The 3D structure points produced by our method encode the shape structure intrinsically and exhibit semantic consistency across all the shape instances with similar structures. This is a challenging goal that has not fully been achieved by other methods. Specifically, our method takes a 3D point cloud as input and encodes it as a set of local features. The local features are then passed through a novel point integration module to produce a set of 3D structure points. The chamfer distance is used as reconstruction loss to ensure the structure points lie close to the input point cloud. Extensive experiments have shown that our method outperforms the state-of-the-art on the semantic shape correspondence task and achieves comparable performance with the state-of-the-art on the segmentation label transfer task. Moreover, the PCA based shape embedding built upon consistent structure points demonstrates good performance in preserving the shape structures. Code is available at https://github.com/NolenChen/3DStructurePoints



### Holistically-Attracted Wireframe Parsing
- **Arxiv ID**: http://arxiv.org/abs/2003.01663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01663v1)
- **Published**: 2020-03-03 17:43:57+00:00
- **Updated**: 2020-03-03 17:43:57+00:00
- **Authors**: Nan Xue, Tianfu Wu, Song Bai, Fu-Dong Wang, Gui-Song Xia, Liangpei Zhang, Philip H. S. Torr
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: This paper presents a fast and parsimonious parsing method to accurately and robustly detect a vectorized wireframe in an input image with a single forward pass. The proposed method is end-to-end trainable, consisting of three components: (i) line segment and junction proposal generation, (ii) line segment and junction matching, and (iii) line segment and junction verification. For computing line segment proposals, a novel exact dual representation is proposed which exploits a parsimonious geometric reparameterization for line segments and forms a holistic 4-dimensional attraction field map for an input image. Junctions can be treated as the "basins" in the attraction field. The proposed method is thus called Holistically-Attracted Wireframe Parser (HAWP). In experiments, the proposed method is tested on two benchmarks, the Wireframe dataset, and the YorkUrban dataset. On both benchmarks, it obtains state-of-the-art performance in terms of accuracy and efficiency. For example, on the Wireframe dataset, compared to the previous state-of-the-art method L-CNN, it improves the challenging mean structural average precision (msAP) by a large margin ($2.8\%$ absolute improvements) and achieves 29.5 FPS on single GPU ($89\%$ relative improvement). A systematic ablation study is performed to further justify the proposed method.



### Discriminative Multi-level Reconstruction under Compact Latent Space for One-Class Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.01665v3
- **DOI**: 10.1109/ICPR48806.2021.9413248
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01665v3)
- **Published**: 2020-03-03 17:45:54+00:00
- **Updated**: 2021-02-17 14:00:42+00:00
- **Authors**: Jaewoo Park, Yoon Gyo Jung, Andrew Beng Jin Teoh
- **Comment**: Accepted to ICPR 2020 Oral (acceptance rate 4.4%)
- **Journal**: None
- **Summary**: In one-class novelty detection, a model learns solely on the in-class data to single out out-class instances. Autoencoder (AE) variants aim to compactly model the in-class data to reconstruct it exclusively, thus differentiating the in-class from out-class by the reconstruction error. However, compact modeling in an improper way might collapse the latent representations of the in-class data and thus their reconstruction, which would lead to performance deterioration. Moreover, to properly measure the reconstruction error of high-dimensional data, a metric is required that captures high-level semantics of the data. To this end, we propose Discriminative Compact AE (DCAE) that learns both compact and collapse-free latent representations of the in-class data, thereby reconstructing them both finely and exclusively. In DCAE, (a) we force a compact latent space to bijectively represent the in-class data by reconstructing them through internal discriminative layers of generative adversarial nets. (b) Based on the deep encoder's vulnerability to open set risk, out-class instances are encoded into the same compact latent space and reconstructed poorly without sacrificing the quality of in-class data reconstruction. (c) In inference, the reconstruction error is measured by a novel metric that computes the dissimilarity between a query and its reconstruction based on the class semantics captured by the internal discriminator. Extensive experiments on public image datasets validate the effectiveness of our proposed model on both novelty and adversarial example detection, delivering state-of-the-art performance.



### Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks
- **Arxiv ID**: http://arxiv.org/abs/2003.01690v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.01690v2)
- **Published**: 2020-03-03 18:15:55+00:00
- **Updated**: 2020-08-04 18:31:08+00:00
- **Authors**: Francesco Croce, Matthias Hein
- **Comment**: In ICML 2020
- **Journal**: None
- **Summary**: The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\%$, identifying several broken defenses.



### BATS: Binary ArchitecTure Search
- **Arxiv ID**: http://arxiv.org/abs/2003.01711v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01711v2)
- **Published**: 2020-03-03 18:57:02+00:00
- **Updated**: 2020-07-23 21:57:18+00:00
- **Authors**: Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos
- **Comment**: accepted to ECCV 2020
- **Journal**: None
- **Summary**: This paper proposes Binary ArchitecTure Search (BATS), a framework that drastically reduces the accuracy gap between binary neural networks and their real-valued counterparts by means of Neural Architecture Search (NAS). We show that directly applying NAS to the binary domain provides very poor results. To alleviate this, we describe, to our knowledge, for the first time, the 3 key ingredients for successfully applying NAS to the binary domain. Specifically, we (1) introduce and design a novel binary-oriented search space, (2) propose a new mechanism for controlling and stabilising the resulting searched topologies, (3) propose and validate a series of new search strategies for binary networks that lead to faster convergence and lower search times. Experimental results demonstrate the effectiveness of the proposed approach and the necessity of searching in the binary space directly. Moreover, (4) we set a new state-of-the-art for binary neural networks on CIFAR10, CIFAR100 and ImageNet datasets. Code will be made available https://github.com/1adrianb/binary-nas



### Blind Image Restoration without Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2003.01764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01764v2)
- **Published**: 2020-03-03 19:57:33+00:00
- **Updated**: 2020-03-08 18:36:09+00:00
- **Authors**: Noam Elron, Shahar S. Yuval, Dmitry Rudoy, Noam Levy
- **Comment**: Submitted to ECCV2020
- **Journal**: None
- **Summary**: Many image restoration techniques are highly dependent on the degradation used during training, and their performance declines significantly when applied to slightly different input. Blind and universal techniques attempt to mitigate this by producing a trained model that can adapt to varying conditions. However, blind techniques to date require prior knowledge of the degradation process, and assumptions regarding its parameter-space. In this paper we present the Self-Normalization Side-Chain (SCNC), a novel approach to blind universal restoration in which no prior knowledge of the degradation is needed. This module can be added to any existing CNN topology, and is trained along with the rest of the network in an end-to-end manner. The imaging parameters relevant to the task, as well as their dynamics, are deduced from the variety in the training data. We apply our solution to several image restoration tasks, and demonstrate that the SNSC encodes the degradation-parameters, improving restoration performance.



### A Robust Imbalanced SAR Image Change Detection Approach Based on Deep Difference Image and PCANet
- **Arxiv ID**: http://arxiv.org/abs/2003.01768v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01768v1)
- **Published**: 2020-03-03 20:05:49+00:00
- **Updated**: 2020-03-03 20:05:49+00:00
- **Authors**: Xinzheng Zhang, Hang Su, Ce Zhang, Peter M. Atkinson, Xiaoheng Tan, Xiaoping Zeng, Xin Jian
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: In this research, a novel robust change detection approach is presented for imbalanced multi-temporal synthetic aperture radar (SAR) image based on deep learning. Our main contribution is to develop a novel method for generating difference image and a parallel fuzzy c-means (FCM) clustering method. The main steps of our proposed approach are as follows: 1) Inspired by convolution and pooling in deep learning, a deep difference image (DDI) is obtained based on parameterized pooling leading to better speckle suppression and feature enhancement than traditional difference images. 2) Two different parameter Sigmoid nonlinear mapping are applied to the DDI to get two mapped DDIs. Parallel FCM are utilized on these two mapped DDIs to obtain three types of pseudo-label pixels, namely, changed pixels, unchanged pixels, and intermediate pixels. 3) A PCANet with support vector machine (SVM) are trained to classify intermediate pixels to be changed or unchanged. Three imbalanced multi-temporal SAR image sets are used for change detection experiments. The experimental results demonstrate that the proposed approach is effective and robust for imbalanced SAR data, and achieve up to 99.52% change detection accuracy superior to most state-of-the-art methods.



### Security of Deep Learning based Lane Keeping System under Physical-World Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2003.01782v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01782v1)
- **Published**: 2020-03-03 20:35:25+00:00
- **Updated**: 2020-03-03 20:35:25+00:00
- **Authors**: Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi Alfred Chen
- **Comment**: Project page: https://sites.google.com/view/lane-keeping-adv-attack/
- **Journal**: None
- **Summary**: Lane-Keeping Assistance System (LKAS) is convenient and widely available today, but also extremely security and safety critical. In this work, we design and implement the first systematic approach to attack real-world DNN-based LKASes. We identify dirty road patches as a novel and domain-specific threat model for practicality and stealthiness. We formulate the attack as an optimization problem, and address the challenge from the inter-dependencies among attacks on consecutive camera frames. We evaluate our approach on a state-of-the-art LKAS and our preliminary results show that our attack can successfully cause it to drive off lane boundaries within as short as 1.3 seconds.



### TimeConvNets: A Deep Time Windowed Convolution Neural Network Design for Real-time Video Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2003.01791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01791v1)
- **Published**: 2020-03-03 20:58:52+00:00
- **Updated**: 2020-03-03 20:58:52+00:00
- **Authors**: James Ren Hou Lee, Alexander Wong
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: A core challenge faced by the majority of individuals with Autism Spectrum Disorder (ASD) is an impaired ability to infer other people's emotions based on their facial expressions. With significant recent advances in machine learning, one potential approach to leveraging technology to assist such individuals to better recognize facial expressions and reduce the risk of possible loneliness and depression due to social isolation is the design of computer vision-driven facial expression recognition systems. Motivated by this social need as well as the low latency requirement of such systems, this study explores a novel deep time windowed convolutional neural network design (TimeConvNets) for the purpose of real-time video facial expression recognition. More specifically, we explore an efficient convolutional deep neural network design for spatiotemporal encoding of time windowed video frame sub-sequences and study the respective balance between speed and accuracy. Furthermore, to evaluate the proposed TimeConvNet design, we introduce a more difficult dataset called BigFaceX, composed of a modified aggregation of the extended Cohn-Kanade (CK+), BAUM-1, and the eNTERFACE public datasets. Different variants of the proposed TimeConvNet design with different backbone network architectures were evaluated using BigFaceX alongside other network designs for capturing spatiotemporal information, and experimental results demonstrate that TimeConvNets can better capture the transient nuances of facial expressions and boost classification accuracy while maintaining a low inference time.



### RODNet: Radar Object Detection Using Cross-Modal Supervision
- **Arxiv ID**: http://arxiv.org/abs/2003.01816v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2003.01816v2)
- **Published**: 2020-03-03 22:33:16+00:00
- **Updated**: 2021-02-08 07:00:42+00:00
- **Authors**: Yizhou Wang, Zhongyu Jiang, Xiangyu Gao, Jenq-Neng Hwang, Guanbin Xing, Hui Liu
- **Comment**: Accepted by WACV 2021, 10 pages, 9 figures, 3 tables. Proceedings of
  the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021
- **Journal**: None
- **Summary**: Radar is usually more robust than the camera in severe driving scenarios, e.g., weak/strong lighting and bad weather. However, unlike RGB images captured by a camera, the semantic information from the radar signals is noticeably difficult to extract. In this paper, we propose a deep radar object detection network (RODNet), to effectively detect objects purely from the carefully processed radar frequency data in the format of range-azimuth frequency heatmaps (RAMaps). Three different 3D autoencoder based architectures are introduced to predict object confidence distribution from each snippet of the input RAMaps. The final detection results are then calculated using our post-processing method, called location-based non-maximum suppression (L-NMS). Instead of using burdensome human-labeled ground truth, we train the RODNet using the annotations generated automatically by a novel 3D localization method using a camera-radar fusion (CRF) strategy. To train and evaluate our method, we build a new dataset -- CRUW, containing synchronized videos and RAMaps in various driving scenarios. After intensive experiments, our RODNet shows favorable object detection performance without the presence of the camera.



### Implicitly Defined Layers in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2003.01822v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.01822v2)
- **Published**: 2020-03-03 22:44:42+00:00
- **Updated**: 2020-06-03 01:36:06+00:00
- **Authors**: Qianggong Zhang, Yanyang Gu, Michalkiewicz Mateusz, Mahsa Baktashmotlagh, Anders Eriksson
- **Comment**: None
- **Journal**: None
- **Summary**: In conventional formulations of multilayer feedforward neural networks, the individual layers are customarily defined by explicit functions. In this paper we demonstrate that defining individual layers in a neural network \emph{implicitly} provide much richer representations over the standard explicit one, consequently enabling a vastly broader class of end-to-end trainable architectures. We present a general framework of implicitly defined layers, where much of the theoretical analysis of such layers can be addressed through the implicit function theorem. We also show how implicitly defined layers can be seamlessly incorporated into existing machine learning libraries. In particular with respect to current automatic differentiation techniques for use in backpropagation based training. Finally, we demonstrate the versatility and relevance of our proposed approach on a number of diverse example problems with promising results.



### Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral Distributions
- **Arxiv ID**: http://arxiv.org/abs/2003.01826v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2003.01826v1)
- **Published**: 2020-03-03 23:04:33+00:00
- **Updated**: 2020-03-03 23:04:33+00:00
- **Authors**: Ricard Durall, Margret Keuper, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Generative convolutional deep neural networks, e.g. popular GAN architectures, are relying on convolution based up-sampling methods to produce non-scalar outputs like images or video sequences. In this paper, we show that common up-sampling methods, i.e. known as up-convolution or transposed convolution, are causing the inability of such models to reproduce spectral distributions of natural training data correctly. This effect is independent of the underlying architecture and we show that it can be used to easily detect generated data like deepfakes with up to 100% accuracy on public benchmarks.   To overcome this drawback of current generative models, we propose to add a novel spectral regularization term to the training optimization objective. We show that this approach not only allows to train spectral consistent GANs that are avoiding high frequency errors. Also, we show that a correct approximation of the frequency spectrum has positive effects on the training stability and output quality of generative networks.



### Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data
- **Arxiv ID**: http://arxiv.org/abs/2003.01835v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2003.01835v1)
- **Published**: 2020-03-03 23:43:05+00:00
- **Updated**: 2020-03-03 23:43:05+00:00
- **Authors**: Priya Sundaresan, Jennifer Grannen, Brijen Thananjeyan, Ashwin Balakrishna, Michael Laskey, Kevin Stone, Joseph E. Gonzalez, Ken Goldberg
- **Comment**: None
- **Journal**: 2020 International Conference on Robotics and Automation
- **Summary**: Robotic manipulation of deformable 1D objects such as ropes, cables, and hoses is challenging due to the lack of high-fidelity analytic models and large configuration spaces. Furthermore, learning end-to-end manipulation policies directly from images and physical interaction requires significant time on a robot and can fail to generalize across tasks. We address these challenges using interpretable deep visual representations for rope, extending recent work on dense object descriptors for robot manipulation. This facilitates the design of interpretable and transferable geometric policies built on top of the learned representations, decoupling visual reasoning and control. We present an approach that learns point-pair correspondences between initial and goal rope configurations, which implicitly encodes geometric structure, entirely in simulation from synthetic depth images. We demonstrate that the learned representation -- dense depth object descriptors (DDODs) -- can be used to manipulate a real rope into a variety of different arrangements either by learning from demonstrations or using interpretable geometric policies. In 50 trials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66% knot-tying success rate from previously unseen configurations. See https://tinyurl.com/rope-learning for supplementary material and videos.




# Arxiv Papers in cs.CV on 2020-09-22
### ALICE: Active Learning with Contrastive Natural Language Explanations
- **Arxiv ID**: http://arxiv.org/abs/2009.10259v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10259v1)
- **Published**: 2020-09-22 01:02:07+00:00
- **Updated**: 2020-09-22 01:02:07+00:00
- **Authors**: Weixin Liang, James Zou, Zhou Yu
- **Comment**: None
- **Journal**: EMNLP 2020
- **Summary**: Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides several bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points.



### Semantic Workflows and Machine Learning for the Assessment of Carbon Storage by Urban Trees
- **Arxiv ID**: http://arxiv.org/abs/2009.10263v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10263v1)
- **Published**: 2020-09-22 01:30:29+00:00
- **Updated**: 2020-09-22 01:30:29+00:00
- **Authors**: Juan Carrillo, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda Gil, Katherine Borda
- **Comment**: Previously published as part of the SciKnow 2019 Workshop, November
  19th, 2019. Los Angeles, California, USA. Collocated with the tenth
  International Conference on Knowledge Capture (K-CAP)
- **Journal**: Proceedings of the Third International Workshop on Capturing
  Scientific Knowledge co-located with the 10th International Conference on
  Knowledge Capture (K-CAP 2019)
- **Summary**: Climate science is critical for understanding both the causes and consequences of changes in global temperatures and has become imperative for decisive policy-making. However, climate science studies commonly require addressing complex interoperability issues between data, software, and experimental approaches from multiple fields. Scientific workflow systems provide unparalleled advantages to address these issues, including reproducibility of experiments, provenance capture, software reusability and knowledge sharing. In this paper, we introduce a novel workflow with a series of connected components to perform spatial data preparation, classification of satellite imagery with machine learning algorithms, and assessment of carbon stored by urban trees. To the best of our knowledge, this is the first study that estimates carbon storage for a region in Africa following the guidelines from the Intergovernmental Panel on Climate Change (IPCC).



### Design of Efficient Deep Learning models for Determining Road Surface Condition from Roadside Camera Images and Weather Data
- **Arxiv ID**: http://arxiv.org/abs/2009.10282v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10282v1)
- **Published**: 2020-09-22 02:30:32+00:00
- **Updated**: 2020-09-22 02:30:32+00:00
- **Authors**: Juan Carrillo, Mark Crowley, Guangyuan Pan, Liping Fu
- **Comment**: Source code for experiments is available at
  https://github.com/jmcarrillog/deep-learning-for-road-surface-condition
- **Journal**: Published also in proceedings of the TAC-ITS 2019 Conference
- **Summary**: Road maintenance during the Winter season is a safety critical and resource demanding operation. One of its key activities is determining road surface condition (RSC) in order to prioritize roads and allocate cleaning efforts such as plowing or salting. Two conventional approaches for determining RSC are: visual examination of roadside camera images by trained personnel and patrolling the roads to perform on-site inspections. However, with more than 500 cameras collecting images across Ontario, visual examination becomes a resource-intensive activity, difficult to scale especially during periods of snowstorms. This paper presents the results of a study focused on improving the efficiency of road maintenance operations. We use multiple Deep Learning models to automatically determine RSC from roadside camera images and weather variables, extending previous research where similar methods have been used to deal with the problem. The dataset we use was collected during the 2017-2018 Winter season from 40 stations connected to the Ontario Road Weather Information System (RWIS), it includes 14.000 labeled images and 70.000 weather measurements. We train and evaluate the performance of seven state-of-the-art models from the Computer Vision literature, including the recent DenseNet, NASNet, and MobileNet. Moreover, by following systematic ablation experiments we adapt previously published Deep Learning models and reduce their number of parameters to about ~1.3% compared to their original parameter count, and by integrating observations from weather variables the models are able to better ascertain RSC under poor visibility conditions.



### PennSyn2Real: Training Object Recognition Models without Human Labeling
- **Arxiv ID**: http://arxiv.org/abs/2009.10292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10292v2)
- **Published**: 2020-09-22 02:53:40+00:00
- **Updated**: 2020-10-16 04:58:40+00:00
- **Authors**: Ty Nguyen, Ian D. Miller, Avi Cohen, Dinesh Thakur, Shashank Prasad, Camillo J. Taylor, Pratik Chaudrahi, Vijay Kumar
- **Comment**: 7 pages, 9 figures, 3 tables. Submitted to R-AL and ICRA 2021
- **Journal**: None
- **Summary**: Scalable training data generation is a critical problem in deep learning. We propose PennSyn2Real - a photo-realistic synthetic dataset consisting of more than 100,000 4K images of more than 20 types of micro aerial vehicles (MAVs). The dataset can be used to generate arbitrary numbers of training images for high-level computer vision tasks such as MAV detection and classification. Our data generation framework bootstraps chroma-keying, a mature cinematography technique with a motion tracking system, providing artifact-free and curated annotated images where object orientations and lighting are controlled. This framework is easy to set up and can be applied to a broad range of objects, reducing the gap between synthetic and real-world data. We show that synthetic data generated using this framework can be directly used to train CNN models for common object recognition tasks such as detection and segmentation. We demonstrate competitive performance in comparison with training using only real images. Furthermore, bootstrapping the generated synthetic data in few-shot learning can significantly improve the overall performance, reducing the number of required training data samples to achieve the desired accuracy.



### Differential Viewpoints for Ground Terrain Material Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.11072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11072v1)
- **Published**: 2020-09-22 02:57:28+00:00
- **Updated**: 2020-09-22 02:57:28+00:00
- **Authors**: Jia Xue, Hang Zhang, Ko Nishino, Kristin J. Dana
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI). arXiv admin note: substantial text overlap with arXiv:1612.02372
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI) 2020
- **Summary**: Computational surface modeling that underlies material recognition has transitioned from reflectance modeling using in-lab controlled radiometric measurements to image-based representations based on internet-mined single-view images captured in the scene. We take a middle-ground approach for material recognition that takes advantage of both rich radiometric cues and flexible image capture. A key concept is differential angular imaging, where small angular variations in image capture enables angular-gradient features for an enhanced appearance representation that improves recognition. We build a large-scale material database, Ground Terrain in Outdoor Scenes (GTOS) database, to support ground terrain recognition for applications such as autonomous driving and robot navigation. The database consists of over 30,000 images covering 40 classes of outdoor ground terrain under varying weather and lighting conditions. We develop a novel approach for material recognition called texture-encoded angular network (TEAN) that combines deep encoding pooling of RGB information and differential angular images for angular-gradient features to fully leverage this large dataset. With this novel network architecture, we extract characteristics of materials encoded in the angular and spatial gradients of their appearance. Our results show that TEAN achieves recognition performance that surpasses single view performance and standard (non-differential/large-angle sampling) multiview performance.



### Beyond Triplet Loss: Person Re-identification with Fine-grained Difference-aware Pairwise Loss
- **Arxiv ID**: http://arxiv.org/abs/2009.10295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2009.10295v1)
- **Published**: 2020-09-22 03:04:12+00:00
- **Updated**: 2020-09-22 03:04:12+00:00
- **Authors**: Cheng Yan, Guansong Pang, Xiao Bai, Jun Zhou, Lin Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Person Re-IDentification (ReID) aims at re-identifying persons from different viewpoints across multiple cameras. Capturing the fine-grained appearance differences is often the key to accurate person ReID, because many identities can be differentiated only when looking into these fine-grained differences. However, most state-of-the-art person ReID approaches, typically driven by a triplet loss, fail to effectively learn the fine-grained features as they are focused more on differentiating large appearance differences. To address this issue, we introduce a novel pairwise loss function that enables ReID models to learn the fine-grained features by adaptively enforcing an exponential penalization on the images of small differences and a bounded penalization on the images of large differences. The proposed loss is generic and can be used as a plugin to replace the triplet loss to significantly enhance different types of state-of-the-art approaches. Experimental results on four benchmark datasets show that the proposed loss substantially outperforms a number of popular loss functions by large margins; and it also enables significantly improved data efficiency.



### Stochastic Neighbor Embedding with Gaussian and Student-t Distributions: Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/2009.10301v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10301v2)
- **Published**: 2020-09-22 03:32:05+00:00
- **Updated**: 2022-08-03 04:42:14+00:00
- **Authors**: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley
- **Comment**: To appear as a part of an upcoming academic book on dimensionality
  reduction and manifold learning. v2: applied readers' feedback
- **Journal**: None
- **Summary**: Stochastic Neighbor Embedding (SNE) is a manifold learning and dimensionality reduction method with a probabilistic approach. In SNE, every point is consider to be the neighbor of all other points with some probability and this probability is tried to be preserved in the embedding space. SNE considers Gaussian distribution for the probability in both the input and embedding spaces. However, t-SNE uses the Student-t and Gaussian distributions in these spaces, respectively. In this tutorial and survey paper, we explain SNE, symmetric SNE, t-SNE (or Cauchy-SNE), and t-SNE with general degrees of freedom. We also cover the out-of-sample extension and acceleration for these methods.



### Learning Image Labels On-the-fly for Training Robust Classification Models
- **Arxiv ID**: http://arxiv.org/abs/2009.10325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.10325v2)
- **Published**: 2020-09-22 05:38:44+00:00
- **Updated**: 2020-10-02 04:35:55+00:00
- **Authors**: Xiaosong Wang, Ziyue Xu, Dong Yang, Leo Tam, Holger Roth, Daguang Xu
- **Comment**: v2: Minor Corrections
- **Journal**: None
- **Summary**: Current deep learning paradigms largely benefit from the tremendous amount of annotated data. However, the quality of the annotations often varies among labelers. Multi-observer studies have been conducted to study these annotation variances (by labeling the same data for multiple times) and its effects on critical applications like medical image analysis. This process indeed adds an extra burden to the already tedious annotation work that usually requires professional training and expertise in the specific domains. On the other hand, automated annotation methods based on NLP algorithms have recently shown promise as a reasonable alternative, relying on the existing diagnostic reports of those images that are widely available in the clinical system. Compared to human labelers, different algorithms provide labels with varying qualities that are even noisier. In this paper, we show how noisy annotations (e.g., from different algorithm-based labelers) can be utilized together and mutually benefit the learning of classification tasks. Specifically, the concept of attention-on-label is introduced to sample better label sets on-the-fly as the training data. A meta-training based label-sampling module is designed to attend the labels that benefit the model learning the most through additional back-propagation processes. We apply the attention-on-label scheme on the classification task of a synthetic noisy CIFAR-10 dataset to prove the concept, and then demonstrate superior results (3-5% increase on average in multiple disease classification AUCs) on the chest x-ray images from a hospital-scale dataset (MIMIC-CXR) and hand-labeled dataset (OpenI) in comparison to regular training paradigms.



### SAMOT: Switcher-Aware Multi-Object Tracking and Still Another MOT Measure
- **Arxiv ID**: http://arxiv.org/abs/2009.10338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10338v1)
- **Published**: 2020-09-22 06:22:21+00:00
- **Updated**: 2020-09-22 06:22:21+00:00
- **Authors**: Weitao Feng, Zhihao Hu, Baopu Li, Weihao Gan, Wei Wu, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Object Tracking (MOT) is a popular topic in computer vision. However, identity issue, i.e., an object is wrongly associated with another object of a different identity, still remains to be a challenging problem. To address it, switchers, i.e., confusing targets thatmay cause identity issues, should be focused. Based on this motivation,this paper proposes a novel switcher-aware framework for multi-object tracking, which consists of Spatial Conflict Graph model (SCG) and Switcher-Aware Association (SAA). The SCG eliminates spatial switch-ers within one frame by building a conflict graph and working out the optimal subgraph. The SAA utilizes additional information from potential temporal switcher across frames, enabling more accurate data association. Besides, we propose a new MOT evaluation measure, Still Another IDF score (SAIDF), aiming to focus more on identity issues.This new measure may overcome some problems of the previous measures and provide a better insight for identity issues in MOT. Finally,the proposed framework is tested under both the traditional measures and the new measure we proposed. Extensive experiments show that ourmethod achieves competitive results on all measure.



### Neural Face Models for Example-Based Visual Speech Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2009.10361v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3.5; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2009.10361v1)
- **Published**: 2020-09-22 07:35:33+00:00
- **Updated**: 2020-09-22 07:35:33+00:00
- **Authors**: Wolfgang Paier, Anna Hilsmann, Peter Eisert
- **Comment**: None
- **Journal**: None
- **Summary**: Creating realistic animations of human faces with computer graphic models is still a challenging task. It is often solved either with tedious manual work or motion capture based techniques that require specialised and costly hardware. Example based animation approaches circumvent these problems by re-using captured data of real people. This data is split into short motion samples that can be looped or concatenated in order to create novel motion sequences. The obvious advantages of this approach are the simplicity of use and the high realism, since the data exhibits only real deformations. Rather than tuning weights of a complex face rig, the animation task is performed on a higher level by arranging typical motion samples in a way such that the desired facial performance is achieved. Two difficulties with example based approaches, however, are high memory requirements as well as the creation of artefact-free and realistic transitions between motion samples. We solve these problems by combining the realism and simplicity of example-based animations with the advantages of neural face models. Our neural face model is capable of synthesising high quality 3D face geometry and texture according to a compact latent parameter vector. This latent representation reduces memory requirements by a factor of 100 and helps creating seamless transitions between concatenated motion samples. In this paper, we present a marker-less approach for facial motion capture based on multi-view video. Based on the captured data, we learn a neural representation of facial expressions, which is used to seamlessly concatenate facial performances during the animation procedure. We demonstrate the effectiveness of our approach by synthesising mouthings for Swiss-German sign language based on viseme query sequences.



### Visual Methods for Sign Language Recognition: A Modality-Based Review
- **Arxiv ID**: http://arxiv.org/abs/2009.10370v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.10370v1)
- **Published**: 2020-09-22 07:56:02+00:00
- **Updated**: 2020-09-22 07:56:02+00:00
- **Authors**: Bassem Seddik, Najoua Essoukri Ben Amara
- **Comment**: This survey paper is accepted as Springer book chapter, currently
  under edition
- **Journal**: None
- **Summary**: Sign language visual recognition from continuous multi-modal streams is still one of the most challenging fields.   Recent advances in human actions recognition are exploiting the ascension of GPU-based learning from massive data, and are getting closer to human-like performances.   They are then prone to creating interactive services for the deaf and hearing-impaired communities.   A population that is expected to grow considerably in the years to come.   This paper aims at reviewing the human actions recognition literature with the sign-language visual understanding as a scope.   The methods analyzed will be mainly organized according to the different types of unimodal inputs exploited, their relative multi-modal combinations and pipeline steps.   In each section, we will detail and compare the related datasets, approaches then distinguish the still open contribution paths suitable for the creation of sign language related services.   Special attention will be paid to the approaches and commercial solutions handling facial expressions and continuous signing.



### BoMuDANet: Unsupervised Adaptation for Visual Scene Understanding in Unstructured Driving Environments
- **Arxiv ID**: http://arxiv.org/abs/2010.03523v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03523v3)
- **Published**: 2020-09-22 08:25:44+00:00
- **Updated**: 2021-05-23 15:27:04+00:00
- **Authors**: Divya Kothandaraman, Rohan Chandra, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present an unsupervised adaptation approach for visual scene understanding in unstructured traffic environments. Our method is designed for unstructured real-world scenarios with dense and heterogeneous traffic consisting of cars, trucks, two-and three-wheelers, and pedestrians. We describe a new semantic segmentation technique based on unsupervised domain adaptation (DA), that can identify the class or category of each region in RGB images or videos. We also present a novel self-training algorithm (Alt-Inc) for multi-source DA that improves the accuracy. Our overall approach is a deep learning-based technique and consists of an unsupervised neural network that achieves 87.18% accuracy on the challenging India Driving Dataset. Our method works well on roads that may not be well-marked or may include dirt, unidentifiable debris, potholes, etc. A key aspect of our approach is that it can also identify objects that are encountered by the model for the fist time during the testing phase. We compare our method against the state-of-the-art methods and show an improvement of 5.17% - 42.9%. Furthermore, we also conduct user studies that qualitatively validate the improvements in visual scene understanding of unstructured driving environments.



### Conditional Sequential Modulation for Efficient Global Image Retouching
- **Arxiv ID**: http://arxiv.org/abs/2009.10390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10390v1)
- **Published**: 2020-09-22 08:32:04+00:00
- **Updated**: 2020-09-22 08:32:04+00:00
- **Authors**: Jingwen He, Yihao Liu, Yu Qiao, Chao Dong
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Photo retouching aims at enhancing the aesthetic visual quality of images that suffer from photographic defects such as over/under exposure, poor contrast, inharmonious saturation. Practically, photo retouching can be accomplished by a series of image processing operations. In this paper, we investigate some commonly-used retouching operations and mathematically find that these pixel-independent operations can be approximated or formulated by multi-layer perceptrons (MLPs). Based on this analysis, we propose an extremely light-weight framework - Conditional Sequential Retouching Network (CSRNet) - for efficient global image retouching. CSRNet consists of a base network and a condition network. The base network acts like an MLP that processes each pixel independently and the condition network extracts the global features of the input image to generate a condition vector. To realize retouching operations, we modulate the intermediate features using Global Feature Modulation (GFM), of which the parameters are transformed by condition vector. Benefiting from the utilization of $1\times1$ convolution, CSRNet only contains less than 37k trainable parameters, which is orders of magnitude smaller than existing learning-based methods. Extensive experiments show that our method achieves state-of-the-art performance on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. Code is available at https://github.com/hejingwenhejingwen/CSRNet.



### Frame-wise Cross-modal Matching for Video Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2009.10434v2
- **DOI**: 10.1109/TMM.2021.3063631
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.10434v2)
- **Published**: 2020-09-22 10:25:41+00:00
- **Updated**: 2021-07-22 07:32:20+00:00
- **Authors**: Haoyu Tang, Jihua Zhu, Meng Liu, Zan Gao, Zhiyong Cheng
- **Comment**: 12 pages; accepted by IEEE TMM
- **Journal**: IEEE Transactions on Multimedia 2021
- **Summary**: Video moment retrieval targets at retrieving a moment in a video for a given language query. The challenges of this task include 1) the requirement of localizing the relevant moment in an untrimmed video, and 2) bridging the semantic gap between textual query and video contents. To tackle those problems, early approaches adopt the sliding window or uniform sampling to collect video clips first and then match each clip with the query. Obviously, these strategies are time-consuming and often lead to unsatisfied accuracy in localization due to the unpredictable length of the golden moment. To avoid the limitations, researchers recently attempt to directly predict the relevant moment boundaries without the requirement to generate video clips first. One mainstream approach is to generate a multimodal feature vector for the target query and video frames (e.g., concatenation) and then use a regression approach upon the multimodal feature vector for boundary detection. Although some progress has been achieved by this approach, we argue that those methods have not well captured the cross-modal interactions between the query and video frames.   In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM) model which predicts the temporal boundaries based on an interaction modeling. In addition, an attention module is introduced to assign higher weights to query words with richer semantic cues, which are considered to be more important for finding relevant video contents. Another contribution is that we propose an additional predictor to utilize the internal frames in the model training to improve the localization accuracy. Extensive experiments on two datasets TACoS and Charades-STA demonstrate the superiority of our method over several state-of-the-art methods. Ablation studies have been also conducted to examine the effectiveness of different modules in our ACRM model.



### Performance Indicator in Multilinear Compressive Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.10456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10456v1)
- **Published**: 2020-09-22 11:27:50+00:00
- **Updated**: 2020-09-22 11:27:50+00:00
- **Authors**: Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: accepted in 2020 IEEE Symposium Series on Computational Intelligence
- **Journal**: None
- **Summary**: Recently, the Multilinear Compressive Learning (MCL) framework was proposed to efficiently optimize the sensing and learning steps when working with multidimensional signals, i.e. tensors. In Compressive Learning in general, and in MCL in particular, the number of compressed measurements captured by a compressive sensing device characterizes the storage requirement or the bandwidth requirement for transmission. This number, however, does not completely characterize the learning performance of a MCL system. In this paper, we analyze the relationship between the input signal resolution, the number of compressed measurements and the learning performance of MCL. Our empirical analysis shows that the reconstruction error obtained at the initialization step of MCL strongly correlates with the learning performance, thus can act as a good indicator to efficiently characterize learning performances obtained from different sensor configurations without optimizing the entire system.



### Deep N-ary Error Correcting Output Codes
- **Arxiv ID**: http://arxiv.org/abs/2009.10465v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2009.10465v4)
- **Published**: 2020-09-22 11:35:03+00:00
- **Updated**: 2020-12-15 02:33:15+00:00
- **Authors**: Hao Zhang, Joey Tianyi Zhou, Tianying Wang, Ivor W. Tsang, Rick Siow Mong Goh
- **Comment**: EAI MOBIMEDIA 2020
- **Journal**: None
- **Summary**: Ensemble learning consistently improves the performance of multi-class classification through aggregating a series of base classifiers. To this end, data-independent ensemble methods like Error Correcting Output Codes (ECOC) attract increasing attention due to its easiness of implementation and parallelization. Specifically, traditional ECOCs and its general extension N-ary ECOC decompose the original multi-class classification problem into a series of independent simpler classification subproblems. Unfortunately, integrating ECOCs, especially N-ary ECOC with deep neural networks, termed as deep N-ary ECOC, is not straightforward and yet fully exploited in the literature, due to the high expense of training base learners. To facilitate the training of N-ary ECOC with deep learning base learners, we further propose three different variants of parameter sharing architectures for deep N-ary ECOC. To verify the generalization ability of deep N-ary ECOC, we conduct experiments by varying the backbone with different deep neural network architectures for both image and text classification tasks. Furthermore, extensive ablation studies on deep N-ary ECOC show its superior performance over other deep data-independent ensemble methods.



### Self-Supervised Learning of Non-Rigid Residual Flow and Ego-Motion
- **Arxiv ID**: http://arxiv.org/abs/2009.10467v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10467v2)
- **Published**: 2020-09-22 11:39:19+00:00
- **Updated**: 2020-10-19 15:21:21+00:00
- **Authors**: Ivan Tishchenko, Sandro Lombardi, Martin R. Oswald, Marc Pollefeys
- **Comment**: Accepted to 3DV 2020 (oral)
- **Journal**: None
- **Summary**: Most of the current scene flow methods choose to model scene flow as a per point translation vector without differentiating between static and dynamic components of 3D motion. In this work we present an alternative method for end-to-end scene flow learning by joint estimation of non-rigid residual flow and ego-motion flow for dynamic 3D scenes. We propose to learn the relative rigid transformation from a pair of point clouds followed by an iterative refinement. We then learn the non-rigid flow from transformed inputs with the deducted rigid part of the flow. Furthermore, we extend the supervised framework with self-supervisory signals based on the temporal consistency property of a point cloud sequence. Our solution allows both training in a supervised mode complemented by self-supervisory loss terms as well as training in a fully self-supervised mode. We demonstrate that decomposition of scene flow into non-rigid flow and ego-motion flow along with an introduction of the self-supervisory signals allowed us to outperform the current state-of-the-art supervised methods.



### Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories Prediction
- **Arxiv ID**: http://arxiv.org/abs/2009.10468v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.10468v2)
- **Published**: 2020-09-22 11:43:40+00:00
- **Updated**: 2020-09-23 07:51:39+00:00
- **Authors**: Xiong Dan
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is a critical to avoid autonomous driving collision. But this prediction is a challenging problem due to social forces and cluttered scenes. Such human-human and human-space interactions lead to many socially plausible trajectories. In this paper, we propose a novel LSTM-based algorithm. We tackle the problem by considering the static scene and pedestrian which combine the Graph Convolutional Networks and Temporal Convolutional Networks to extract features from pedestrians. Each pedestrian in the scene is regarded as a node, and we can obtain the relationship between each node and its neighborhoods by graph embedding. It is LSTM that encode the relationship so that our model predicts nodes trajectories in crowd scenarios simultaneously. To effectively predict multiple possible future trajectories, we further introduce Spatio-Temporal Convolutional Block to make the network flexible. Experimental results on two public datasets, i.e. ETH and UCY, demonstrate the effectiveness of our proposed ST-Block and we achieve state-of-the-art approaches in human trajectory prediction.



### Classification of COVID-19 in CT Scans using Multi-Source Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.10474v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10474v1)
- **Published**: 2020-09-22 11:53:06+00:00
- **Updated**: 2020-09-22 11:53:06+00:00
- **Authors**: Alejandro R. Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: Since December of 2019, novel coronavirus disease COVID-19 has spread around the world infecting millions of people and upending the global economy. One of the driving reasons behind its high rate of infection is due to the unreliability and lack of RT-PCR testing. At times the turnaround results span as long as a couple of days, only to yield a roughly 70% sensitivity rate. As an alternative, recent research has investigated the use of Computer Vision with Convolutional Neural Networks (CNNs) for the classification of COVID-19 from CT scans. Due to an inherent lack of available COVID-19 CT data, these research efforts have been forced to leverage the use of Transfer Learning. This commonly employed Deep Learning technique has shown to improve model performance on tasks with relatively small amounts of data, as long as the Source feature space somewhat resembles the Target feature space. Unfortunately, a lack of similarity is often encountered in the classification of medical images as publicly available Source datasets usually lack the visual features found in medical images. In this study, we propose the use of Multi-Source Transfer Learning (MSTL) to improve upon traditional Transfer Learning for the classification of COVID-19 from CT scans. With our multi-source fine-tuning approach, our models outperformed baseline models fine-tuned with ImageNet. We additionally, propose an unsupervised label creation process, which enhances the performance of our Deep Residual Networks. Our best performing model was able to achieve an accuracy of 0.893 and a Recall score of 0.897, outperforming its baseline Recall score by 9.3%.



### OpenREALM: Real-time Mapping for Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2009.10492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10492v1)
- **Published**: 2020-09-22 12:28:14+00:00
- **Updated**: 2020-09-22 12:28:14+00:00
- **Authors**: Alexander Kern, Markus Bobbe, Yogesh Khedar, Ulf Bestmann
- **Comment**: Full source code on https://github.com/laxnpander/OpenREALM 2020
  International Conference on Unmanned Aircraft Systems (ICUAS)
- **Journal**: None
- **Summary**: This paper presents OpenREALM, a real-time mapping framework for Unmanned Aerial Vehicles (UAVs). A camera attached to the onboard computer of a moving UAV is utilized to acquire high resolution image mosaics of a targeted area of interest. Different modes of operation allow OpenREALM to perform simple stitching assuming an approximate plane ground, or to fully recover complex 3D surface information to extract both elevation maps and geometrically corrected orthophotos. Additionally, the global position of the UAV is used to georeference the data. In all modes incremental progress of the resulting map can be viewed live by an operator on the ground. Obtained, up-to-date surface information will be a push forward to a variety of UAV applications. For the benefit of the community, source code is public at https://github.com/laxnpander/OpenREALM.



### CA-Net: Comprehensive Attention Convolutional Neural Networks for Explainable Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.10549v2
- **DOI**: 10.1109/TMI.2020.3035253
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10549v2)
- **Published**: 2020-09-22 13:41:06+00:00
- **Updated**: 2020-09-23 01:03:45+00:00
- **Authors**: Ran Gu, Guotai Wang, Tao Song, Rui Huang, Michael Aertsen, Jan Deprest, SÃ©bastien Ourselin, Tom Vercauteren, Shaoting Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate medical image segmentation is essential for diagnosis and treatment planning of diseases. Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they are still challenged by complicated conditions where the segmentation target has large variations of position, shape and scale, and existing CNNs have a poor explainability that limits their application to clinical decisions. In this work, we make extensive use of multiple attentions in a CNN architecture and propose a comprehensive attention-based CNN (CA-Net) for more accurate and explainable medical image segmentation that is aware of the most important spatial positions, channels and scales at the same time. In particular, we first propose a joint spatial attention module to make the network focus more on the foreground region. Then, a novel channel attention module is proposed to adaptively recalibrate channel-wise feature responses and highlight the most relevant feature channels. Also, we propose a scale attention module implicitly emphasizing the most salient feature maps among multiple scales so that the CNN is adaptive to the size of an object. Extensive experiments on skin lesion segmentation from ISIC 2018 and multi-class segmentation of fetal MRI found that our proposed CA-Net significantly improved the average segmentation Dice score from 87.77% to 92.08% for skin lesion, 84.79% to 87.08% for the placenta and 93.20% to 95.88% for the fetal brain respectively compared with U-Net. It reduced the model size to around 15 times smaller with close or even better accuracy compared with state-of-the-art DeepLabv3+. In addition, it has a much higher explainability than existing networks by visualizing the attention weight maps. Our code is available at https://github.com/HiLab-git/CA-Net



### Improving Point Cloud Semantic Segmentation by Learning 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.10569v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10569v3)
- **Published**: 2020-09-22 14:17:40+00:00
- **Updated**: 2020-11-07 15:58:19+00:00
- **Authors**: Ozan Unal, Luc Van Gool, Dengxin Dai
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer Vision
  2021 (WACV'21)
- **Journal**: None
- **Summary**: Point cloud semantic segmentation plays an essential role in autonomous driving, providing vital information about drivable surfaces and nearby objects that can aid higher level tasks such as path planning and collision avoidance. While current 3D semantic segmentation networks focus on convolutional architectures that perform great for well represented classes, they show a significant drop in performance for underrepresented classes that share similar geometric features. We propose a novel Detection Aware 3D Semantic Segmentation (DASS) framework that explicitly leverages localization features from an auxiliary 3D object detection task. By utilizing multitask training, the shared feature representation of the network is guided to be aware of per class detection features that aid tackling the differentiation of geometrically similar classes. We additionally provide a pipeline that uses DASS to generate high recall proposals for existing 2-stage detectors and demonstrate that the added supervisory signal can be used to improve 3D orientation estimation capabilities. Extensive experiments on both the SemanticKITTI and KITTI object datasets show that DASS can improve 3D semantic segmentation results of geometrically similar classes up to 37.8% IoU in image FOV while maintaining high precision bird's-eye view (BEV) detection results.



### Heuristic Rank Selection with Progressively Searching Tensor Ring Network
- **Arxiv ID**: http://arxiv.org/abs/2009.10580v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.10580v2)
- **Published**: 2020-09-22 14:44:27+00:00
- **Updated**: 2021-05-30 08:44:25+00:00
- **Authors**: Nannan Li, Yu Pan, Yaran Chen, Zixiang Ding, Dongbin Zhao, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Tensor Ring Networks (TRNs) have been applied in deep networks, achieving remarkable successes in compression ratio and accuracy. Although highly related to the performance of TRNs, rank selection is seldom studied in previous works and usually set to equal in experiments. Meanwhile, there is not any heuristic method to choose the rank, and an enumerating way to find appropriate rank is extremely time-consuming. Interestingly, we discover that part of the rank elements is sensitive and usually aggregate in a narrow region, namely an interest region. Therefore, based on the above phenomenon, we propose a novel progressive genetic algorithm named Progressively Searching Tensor Ring Network Search (PSTRN), which has the ability to find optimal rank precisely and efficiently. Through the evolutionary phase and progressive phase, PSTRN can converge to the interest region quickly and harvest good performance. Experimental results show that PSTRN can significantly reduce the complexity of seeking rank, compared with the enumerating method. Furthermore, our method is validated on public benchmarks like MNIST, CIFAR10/100, UCF11 and HMDB51, achieving the state-of-the-art performance.



### The Use of AI for Thermal Emotion Recognition: A Review of Problems and Limitations in Standard Design and Data
- **Arxiv ID**: http://arxiv.org/abs/2009.10589v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10589v1)
- **Published**: 2020-09-22 14:58:59+00:00
- **Updated**: 2020-09-22 14:58:59+00:00
- **Authors**: Catherine Ordun, Edward Raff, Sanjay Purushotham
- **Comment**: Presented at AAAI FSS-20: Artificial Intelligence in Government and
  Public Sector, Washington, DC, USA
- **Journal**: 2020, AAAI FSS-20 AI in Government and Public Sector Applications
- **Summary**: With the increased attention on thermal imagery for Covid-19 screening, the public sector may believe there are new opportunities to exploit thermal as a modality for computer vision and AI. Thermal physiology research has been ongoing since the late nineties. This research lies at the intersections of medicine, psychology, machine learning, optics, and affective computing. We will review the known factors of thermal vs. RGB imaging for facial emotion recognition. But we also propose that thermal imagery may provide a semi-anonymous modality for computer vision, over RGB, which has been plagued by misuse in facial recognition. However, the transition to adopting thermal imagery as a source for any human-centered AI task is not easy and relies on the availability of high fidelity data sources across multiple demographics and thorough validation. This paper takes the reader on a short review of machine learning in thermal FER and the limitations of collecting and developing thermal FER data for AI training. Our motivation is to provide an introductory overview into recent advances for thermal FER and stimulate conversation about the limitations in current datasets.



### Detection Of Concrete Cracks using Dual-channel Deep Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2009.10612v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10612v1)
- **Published**: 2020-09-22 15:17:02+00:00
- **Updated**: 2020-09-22 15:17:02+00:00
- **Authors**: Babloo Kumar, Sayantari Ghosh
- **Comment**: 7 pages, 7 figures, Accepted and presented in IEEE-ICCCNT 2020
  (https://11icccnt.com/)
- **Journal**: None
- **Summary**: Due to cyclic loading and fatigue stress cracks are generated, which affect the safety of any civil infrastructure. Nowadays machine vision is being used to assist us for appropriate maintenance, monitoring and inspection of concrete structures by partial replacement of human-conducted onsite inspections. The current study proposes a crack detection method based on deep convolutional neural network (CNN) for detection of concrete cracks without explicitly calculating the defect features. In the course of the study, a database of 3200 labelled images with concrete cracks has been created, where the contrast, lighting conditions, orientations and severity of the cracks were extremely variable. In this paper, starting from a deep CNN trained with these images of 256 x 256 pixel-resolution, we have gradually optimized the model by identifying the difficulties. Using an augmented dataset, which takes into account the variations and degradations compatible to drone videos, like, random zooming, rotation and intensity scaling and exhaustive ablation studies, we have designed a dual-channel deep CNN which shows high accuracy (~ 92.25%) as well as robustness in finding concrete cracks in realis-tic situations. The model has been tested on the basis of performance and analyzed with the help of feature maps, which establishes the importance of the dual-channel structure.



### Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time
- **Arxiv ID**: http://arxiv.org/abs/2009.10623v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.10623v5)
- **Published**: 2020-09-22 15:26:24+00:00
- **Updated**: 2021-09-06 15:26:52+00:00
- **Authors**: Ferran Alet, Maria Bauza, Kenji Kawaguchi, Nurullah Giray Kuru, Tomas Lozano-Perez, Leslie Pack Kaelbling
- **Comment**: NeurIPS 2020 workshops on Interpretable Inductive Biases and
  Meta-learning
- **Journal**: None
- **Summary**: From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Adding auxiliary losses to the main objective function is a general way of encoding biases that can help networks learn better representations. However, since auxiliary losses are minimized only on training data, they suffer from the same generalization gap as regular task losses. Moreover, by adding a term to the loss function, the model optimizes a different objective than the one we care about. In this work we address both problems: first, we take inspiration from \textit{transductive learning} and note that after receiving an input but before making a prediction, we can fine-tune our networks on any unsupervised loss. We call this process {\em tailoring}, because we customize the model to each input to ensure our prediction satisfies the inductive bias. Second, we formulate {\em meta-tailoring}, a nested optimization similar to that in meta-learning, and train our models to perform well on the task objective after adapting them using an unsupervised loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on a diverse set of examples.



### Curriculum Learning with Diversity for Supervised Computer Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2009.10625v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10625v1)
- **Published**: 2020-09-22 15:32:49+00:00
- **Updated**: 2020-09-22 15:32:49+00:00
- **Authors**: Petru Soviany
- **Comment**: Accepted at MRC 2020 @ ECAI
- **Journal**: None
- **Summary**: Curriculum learning techniques are a viable solution for improving the accuracy of automatic models, by replacing the traditional random training with an easy-to-hard strategy. However, the standard curriculum methodology does not automatically provide improved results, but it is constrained by multiple elements like the data distribution or the proposed model. In this paper, we introduce a novel curriculum sampling strategy which takes into consideration the diversity of the training data together with the difficulty of the inputs. We determine the difficulty using a state-of-the-art estimator based on the human time required for solving a visual search task. We consider this kind of difficulty metric to be better suited for solving general problems, as it is not based on certain task-dependent elements, but more on the context of each image. We ensure the diversity during training, giving higher priority to elements from less visited classes. We conduct object detection and instance segmentation experiments on Pascal VOC 2007 and Cityscapes data sets, surpassing both the randomly-trained baseline and the standard curriculum approach. We prove that our strategy is very efficient for unbalanced data sets, leading to faster convergence and more accurate results, when other curriculum-based strategies fail.



### Whole page recognition of historical handwriting
- **Arxiv ID**: http://arxiv.org/abs/2009.10634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.10634v1)
- **Published**: 2020-09-22 15:46:33+00:00
- **Updated**: 2020-09-22 15:46:33+00:00
- **Authors**: Hans J. G. A. Dolfing
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Historical handwritten documents guard an important part of human knowledge only within reach of a few scholars and experts. Recent developments in machine learning and handwriting research have the potential of rendering this information accessible and searchable to a larger audience. To this end, we investigate an end-to-end inference approach without text localization which takes a handwritten page and transcribes its full text. No explicit character, word or line segmentation is involved in inference which is why we call this approach "segmentation free". We explore its robustness and accuracy compared to a line-by-line segmented approach based on the IAM, RODRIGO and ScribbleLens corpora, in three languages with handwriting styles spanning 400 years. We concentrate on model types and sizes which can be deployed on a hand-held or embedded device. We conclude that a whole page inference approach without text localization and segmentation is competitive.



### What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors
- **Arxiv ID**: http://arxiv.org/abs/2009.10639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10639v1)
- **Published**: 2020-09-22 15:53:19+00:00
- **Updated**: 2020-09-22 15:53:19+00:00
- **Authors**: Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik
- **Comment**: None
- **Journal**: None
- **Summary**: EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a specific target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-specific posthoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We discovered six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.



### A Generative Adversarial Approach with Residual Learning for Dust and Scratches Artifacts Removal
- **Arxiv ID**: http://arxiv.org/abs/2009.10663v1
- **DOI**: 10.1145/3423323.3423411
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10663v1)
- **Published**: 2020-09-22 16:32:57+00:00
- **Updated**: 2020-09-22 16:32:57+00:00
- **Authors**: IonuÅ£ MironicÄ
- **Comment**: None
- **Journal**: None
- **Summary**: Retouching can significantly elevate the visual appeal of photos, but many casual photographers lack the expertise to operate in a professional manner. One particularly challenging task for old photo retouching remains the removal of dust and scratches artifacts. Traditionally, this task has been completed manually with special image enhancement software and represents a tedious task that requires special know-how of photo editing applications.   However, recent research utilizing Generative Adversarial Networks (GANs) has been proven to obtain good results in various automated image enhancement tasks compared to traditional methods. This motivated us to explore the use of GANs in the context of film photo editing. In this paper, we present a GAN based method that is able to remove dust and scratches errors from film scans. Specifically, residual learning is utilized to speed up the training process, as well as boost the denoising performance.   An extensive evaluation of our model on a community provided dataset shows that it generalizes remarkably well, not being dependent on any particular type of image. Finally, we significantly outperform the state-of-the-art methods and software applications, providing superior results.



### An embedded deep learning system for augmented reality in firefighting applications
- **Arxiv ID**: http://arxiv.org/abs/2009.10679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.10679v1)
- **Published**: 2020-09-22 16:55:44+00:00
- **Updated**: 2020-09-22 16:55:44+00:00
- **Authors**: Manish Bhattarai, Aura Rose Jensen-Curtis, Manel MartÃ­Nez-RamÃ³n
- **Comment**: Accepted to ICMLA Special Session on Deep Learning
- **Journal**: None
- **Summary**: Firefighting is a dynamic activity, in which numerous operations occur simultaneously. Maintaining situational awareness (i.e., knowledge of current conditions and activities at the scene) is critical to the accurate decision-making necessary for the safe and successful navigation of a fire environment by firefighters. Conversely, the disorientation caused by hazards such as smoke and extreme heat can lead to injury or even fatality. This research implements recent advancements in technology such as deep learning, point cloud and thermal imaging, and augmented reality platforms to improve a firefighter's situational awareness and scene navigation through improved interpretation of that scene. We have designed and built a prototype embedded system that can leverage data streamed from cameras built into a firefighter's personal protective equipment (PPE) to capture thermal, RGB color, and depth imagery and then deploy already developed deep learning models to analyze the input data in real time. The embedded system analyzes and returns the processed images via wireless streaming, where they can be viewed remotely and relayed back to the firefighter using an augmented reality platform that visualizes the results of the analyzed inputs and draws the firefighter's attention to objects of interest, such as doors and windows otherwise invisible through smoke and flames.



### Deep Learning based NAS Score and Fibrosis Stage Prediction from CT and Pathology Data
- **Arxiv ID**: http://arxiv.org/abs/2009.10687v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10687v1)
- **Published**: 2020-09-22 17:02:31+00:00
- **Updated**: 2020-09-22 17:02:31+00:00
- **Authors**: Ananya Jana, Hui Qu, Puru Rattan, Carlos D. Minacapelli, Vinod Rustgi, Dimitris Metaxas
- **Comment**: 6 pages, 3 figures. Accepted in IEEE BIBE 2020
- **Journal**: None
- **Summary**: Non-Alcoholic Fatty Liver Disease (NAFLD) is becoming increasingly prevalent in the world population. Without diagnosis at the right time, NAFLD can lead to non-alcoholic steatohepatitis (NASH) and subsequent liver damage. The diagnosis and treatment of NAFLD depend on the NAFLD activity score (NAS) and the liver fibrosis stage, which are usually evaluated from liver biopsies by pathologists. In this work, we propose a novel method to automatically predict NAS score and fibrosis stage from CT data that is non-invasive and inexpensive to obtain compared with liver biopsy. We also present a method to combine the information from CT and H\&E stained pathology data to improve the performance of NAS score and fibrosis stage prediction, when both types of data are available. This is of great value to assist the pathologists in computer-aided diagnosis process. Experiments on a 30-patient dataset illustrate the effectiveness of our method.



### TSV Extrusion Morphology Classification Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.10692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10692v1)
- **Published**: 2020-09-22 17:05:55+00:00
- **Updated**: 2020-09-22 17:05:55+00:00
- **Authors**: Brendan Reidy, Golareh Jalilvand, Tengfei Jiang, Ramtin Zand
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we utilize deep convolutional neural networks (CNNs) to classify the morphology of through-silicon via (TSV) extrusion in three dimensional (3D) integrated circuits (ICs). TSV extrusion is a crucial reliability concern which can deform and crack interconnect layers in 3D ICs and cause device failures. Herein, the white light interferometry (WLI) technique is used to obtain the surface profile of the extruded TSVs. We have developed a program that uses raw data obtained from WLI to create a TSV extrusion morphology dataset, including TSV images with 54x54 pixels that are labeled and categorized into three morphology classes. Four CNN architectures with different network complexities are implemented and trained for TSV extrusion morphology classification application. Data augmentation and dropout approaches are utilized to realize a balance between overfitting and underfitting in the CNN models. Results obtained show that the CNN model with optimized complexity, dropout, and data augmentation can achieve a classification accuracy comparable to that of a human expert.



### MonoClothCap: Towards Temporally Coherent Clothing Capture from Monocular RGB Video
- **Arxiv ID**: http://arxiv.org/abs/2009.10711v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2009.10711v2)
- **Published**: 2020-09-22 17:54:38+00:00
- **Updated**: 2020-11-23 16:23:04+00:00
- **Authors**: Donglai Xiang, Fabian Prada, Chenglei Wu, Jessica Hodgins
- **Comment**: 3DV 2020 Camera Ready
- **Journal**: None
- **Summary**: We present a method to capture temporally coherent dynamic clothing deformation from a monocular RGB video input. In contrast to the existing literature, our method does not require a pre-scanned personalized mesh template, and thus can be applied to in-the-wild videos. To constrain the output to a valid deformation space, we build statistical deformation models for three types of clothing: T-shirt, short pants and long pants. A differentiable renderer is utilized to align our captured shapes to the input frames by minimizing the difference in both silhouette, segmentation, and texture. We develop a UV texture growing method which expands the visible texture region of the clothing sequentially in order to minimize drift in deformation tracking. We also extract fine-grained wrinkle detail from the input videos by fitting the clothed surface to the normal maps estimated by a convolutional neural network. Our method produces temporally coherent reconstruction of body and clothing from monocular video. We demonstrate successful clothing capture results from a variety of challenging videos. Extensive quantitative experiments demonstrate the effectiveness of our method on metrics including body pose error and surface reconstruction error of the clothing.



### Role of Orthogonality Constraints in Improving Properties of Deep Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.10762v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10762v1)
- **Published**: 2020-09-22 18:46:05+00:00
- **Updated**: 2020-09-22 18:46:05+00:00
- **Authors**: Hongjun Choi, Anirudh Som, Pavan Turaga
- **Comment**: 8 figures, 4 tables, 1 pseudo-code
- **Journal**: None
- **Summary**: Standard deep learning models that employ the categorical cross-entropy loss are known to perform well at image classification tasks. However, many standard models thus obtained often exhibit issues like feature redundancy, low interpretability, and poor calibration. A body of recent work has emerged that has tried addressing some of these challenges by proposing the use of new regularization functions in addition to the cross-entropy loss. In this paper, we present some surprising findings that emerge from exploring the role of simple orthogonality constraints as a means of imposing physics-motivated constraints common in imaging. We propose an Orthogonal Sphere (OS) regularizer that emerges from physics-based latent-representations under simplifying assumptions. Under further simplifying assumptions, the OS constraint can be written in closed-form as a simple orthonormality term and be used along with the cross-entropy loss function. The findings indicate that orthonormality loss function results in a) rich and diverse feature representations, b) robustness to feature sub-selection, c) better semantic localization in the class activation maps, and d) reduction in model calibration error. We demonstrate the effectiveness of the proposed OS regularization by providing quantitative and qualitative results on four benchmark datasets - CIFAR10, CIFAR100, SVHN and tiny ImageNet.



### Age-Net: An MRI-Based Iterative Framework for Brain Biological Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.10765v2
- **DOI**: 10.1109/TMI.2021.3066857
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10765v2)
- **Published**: 2020-09-22 19:04:02+00:00
- **Updated**: 2021-03-15 15:48:23+00:00
- **Authors**: Karim Armanious, Sherif Abdulatif, Wenbin Shi, Shashank Salian, Thomas KÃ¼stner, Daniel Weiskopf, Tobias Hepp, Sergios Gatidis, Bin Yang
- **Comment**: Accepted to IEEE Transcations on Medical Imaging 2021. 13 pages, 14
  figures, 4 tables
- **Journal**: None
- **Summary**: The concept of biological age (BA), although important in clinical practice, is hard to grasp mainly due to the lack of a clearly defined reference standard. For specific applications, especially in pediatrics, medical image data are used for BA estimation in a routine clinical context. Beyond this young age group, BA estimation is mostly restricted to whole-body assessment using non-imaging indicators such as blood biomarkers, genetic and cellular data. However, various organ systems may exhibit different aging characteristics due to lifestyle and genetic factors. Thus, a whole-body assessment of the BA does not reflect the deviations of aging behavior between organs. To this end, we propose a new imaging-based framework for organ-specific BA estimation. In this initial study, we focus mainly on brain MRI. As a first step, we introduce a chronological age (CA) estimation framework using deep convolutional neural networks (Age-Net). We quantitatively assess the performance of this framework in comparison to existing state-of-the-art CA estimation approaches. Furthermore, we expand upon Age-Net with a novel iterative data-cleaning algorithm to segregate atypical-aging patients (BA $\not \approx$ CA) from the given population. We hypothesize that the remaining population should approximate the true BA behavior. We apply the proposed methodology on a brain magnetic resonance image (MRI) dataset containing healthy individuals as well as Alzheimer's patients with different dementia ratings. We demonstrate the correlation between the predicted BAs and the expected cognitive deterioration in Alzheimer's patients. A statistical and visualization-based analysis has provided evidence regarding the potential and current challenges of the proposed methodology.



### Cranial Implant Prediction using Low-Resolution 3D Shape Completion and High-Resolution 2D Refinement
- **Arxiv ID**: http://arxiv.org/abs/2009.10769v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10769v3)
- **Published**: 2020-09-22 19:16:16+00:00
- **Updated**: 2020-09-27 23:19:08+00:00
- **Authors**: Amirhossein Bayat, Suprosanna Shit, Adrian Kilian, JÃ¼rgen T. Liechtenstein, Jan S. Kirschke, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Designing of a cranial implant needs a 3D understanding of the complete skull shape. Thus, taking a 2D approach is sub-optimal, since a 2D model lacks a holistic 3D view of both the defective and healthy skulls. Further, loading the whole 3D skull shapes at its original image resolution is not feasible in commonly available GPUs. To mitigate these issues, we propose a fully convolutional network composed of two subnetworks. The first subnetwork is designed to complete the shape of the downsampled defective skull. The second subnetwork upsamples the reconstructed shape slice-wise. We train the 3D and 2D networks together end-to-end, with a hierarchical loss function. Our proposed solution accurately predicts a high-resolution 3D implant in the challenge test case in terms of dice-score and the Hausdorff distance.



### Efficient DWT-based fusion techniques using genetic algorithm for optimal parameter estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.10777v1
- **DOI**: 10.1007/s00500-015-2009-6
- **Categories**: **cs.CV**, cs.AI, eess.IV, I.2.6; I.2.10; I.4.6; I.4.7; I.4.10; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2009.10777v1)
- **Published**: 2020-09-22 19:28:57+00:00
- **Updated**: 2020-09-22 19:28:57+00:00
- **Authors**: S. Kavitha, K. K. Thyagharajan
- **Comment**: 17 pages, 3 figures
- **Journal**: None
- **Summary**: Image fusion plays a vital role in medical imaging. Image fusion aims to integrate complementary as well as redundant information from multiple modalities into a single fused image without distortion or loss of information. In this research work, discrete wavelet transform (DWT)and undecimated discrete wavelet transform (UDWT)-based fusion techniques using genetic algorithm (GA)foroptimalparameter(weight)estimationinthefusionprocessareimplemented and analyzed with multi-modality brain images. The lack of shift variance while performing image fusion using DWT is addressed using UDWT. The proposed fusion model uses an efficient, modified GA in DWT and UDWT for optimal parameter estimation, to improve the image quality and contrast. The complexity of the basic GA (pixel level) has been reduced in the modified GA (feature level), by limiting the search space. It is observed from our experiments that fusion using DWT and UDWT techniques with GA for optimal parameter estimation resulted in a better fused image in the aspects of retaining the information and contrast without error, both in human perception as well as evaluation using objective metrics. The contributions of this research work are (1) reduced time and space complexity in estimating the weight values using GA for fusion (2) system is scalable for input image of any size with similar time complexity, owing to feature level GA implementation and (3) identification of source image that contributes more to the fused image, from the weight values estimated.



### Adaptive Debanding Filter
- **Arxiv ID**: http://arxiv.org/abs/2009.10804v1
- **DOI**: 10.1109/LSP.2020.3024985
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10804v1)
- **Published**: 2020-09-22 20:44:20+00:00
- **Updated**: 2020-09-22 20:44:20+00:00
- **Authors**: Zhengzhong Tu, Jessie Lin, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: 4 pages, 7 figures, 1 table. Accepted to IEEE Signal Processing
  Letters
- **Journal**: None
- **Summary**: Banding artifacts, which manifest as staircase-like color bands on pictures or video frames, is a common distortion caused by compression of low-textured smooth regions. These false contours can be very noticeable even on high-quality videos, especially when displayed on high-definition screens. Yet, relatively little attention has been applied to this problem. Here we consider banding artifact removal as a visual enhancement problem, and accordingly, we solve it by applying a form of content-adaptive smoothing filtering followed by dithered quantization, as a post-processing module. The proposed debanding filter is able to adaptively smooth banded regions while preserving image edges and details, yielding perceptually enhanced gradient rendering with limited bit-depths. Experimental results show that our proposed debanding filter outperforms state-of-the-art false contour removing algorithms both visually and quantitatively.



### Kernelized dense layers for facial expression recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.10814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10814v1)
- **Published**: 2020-09-22 21:02:00+00:00
- **Updated**: 2020-09-22 21:02:00+00:00
- **Authors**: M. Amine Mahmoudi, Aladine Chetouani, Fatma Boufera, Hedi Tabia
- **Comment**: None
- **Journal**: None
- **Summary**: Fully connected layer is an essential component of Convolutional Neural Networks (CNNs), which demonstrates its efficiency in computer vision tasks. The CNN process usually starts with convolution and pooling layers that first break down the input images into features, and then analyze them independently. The result of this process feeds into a fully connected neural network structure which drives the final classification decision. In this paper, we propose a Kernelized Dense Layer (KDL) which captures higher order feature interactions instead of conventional linear relations. We apply this method to Facial Expression Recognition (FER) and evaluate its performance on RAF, FER2013 and ExpW datasets. The experimental results demonstrate the benefits of such layer and show that our model achieves competitive results with respect to the state-of-the-art approaches.



### Angular Luminance for Material Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.10825v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.10825v1)
- **Published**: 2020-09-22 21:15:27+00:00
- **Updated**: 2020-09-22 21:15:27+00:00
- **Authors**: Jia Xue, Matthew Purri, Kristin Dana
- **Comment**: IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
  2020
- **Journal**: IEEE International Geoscience and Remote Sensing Symposium
  (IGARSS) 2020
- **Summary**: Moving cameras provide multiple intensity measurements per pixel, yet often semantic segmentation, material recognition, and object recognition do not utilize this information. With basic alignment over several frames of a moving camera sequence, a distribution of intensities over multiple angles is obtained. It is well known from prior work that luminance histograms and the statistics of natural images provide a strong material recognition cue. We utilize per-pixel {\it angular luminance distributions} as a key feature in discriminating the material of the surface. The angle-space sampling in a multiview satellite image sequence is an unstructured sampling of the underlying reflectance function of the material. For real-world materials there is significant intra-class variation that can be managed by building a angular luminance network (AngLNet). This network combines angular reflectance cues from multiple images with spatial cues as input to fully convolutional networks for material segmentation. We demonstrate the increased performance of AngLNet over prior state-of-the-art in material segmentation from satellite imagery.



### Improving Medical Annotation Quality to Decrease Labeling Burden Using Stratified Noisy Cross-Validation
- **Arxiv ID**: http://arxiv.org/abs/2009.10858v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10858v1)
- **Published**: 2020-09-22 23:32:59+00:00
- **Updated**: 2020-09-22 23:32:59+00:00
- **Authors**: Joy Hsu, Sonia Phene, Akinori Mitani, Jieying Luo, Naama Hammel, Jonathan Krause, Rory Sayres
- **Comment**: None
- **Journal**: ACM Conference on Health, Inference, and Learning, April 02-04,
  2020, Toronto, Canada
- **Summary**: As machine learning has become increasingly applied to medical imaging data, noise in training labels has emerged as an important challenge. Variability in diagnosis of medical images is well established; in addition, variability in training and attention to task among medical labelers may exacerbate this issue. Methods for identifying and mitigating the impact of low quality labels have been studied, but are not well characterized in medical imaging tasks. For instance, Noisy Cross-Validation splits the training data into halves, and has been shown to identify low-quality labels in computer vision tasks; but it has not been applied to medical imaging tasks specifically. In this work we introduce Stratified Noisy Cross-Validation (SNCV), an extension of noisy cross validation. SNCV can provide estimates of confidence in model predictions by assigning a quality score to each example; stratify labels to handle class imbalance; and identify likely low-quality labels to analyze the causes. We assess performance of SNCV on diagnosis of glaucoma suspect risk from retinal fundus photographs, a clinically important yet nuanced labeling task. Using training data from a previously-published deep learning model, we compute a continuous quality score (QS) for each training example. We relabel 1,277 low-QS examples using a trained glaucoma specialist; the new labels agree with the SNCV prediction over the initial label >85% of the time, indicating that low-QS examples mostly reflect labeler errors. We then quantify the impact of training with only high-QS labels, showing that strong model performance may be obtained with many fewer examples. By applying the method to randomly sub-sampled training dataset, we show that our method can reduce labelling burden by approximately 50% while achieving model performance non-inferior to using the full dataset on multiple held-out test sets.




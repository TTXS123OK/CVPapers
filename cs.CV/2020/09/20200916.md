# Arxiv Papers in cs.CV on 2020-09-16
### Geometric Uncertainty in Patient-Specific Cardiovascular Modeling with Convolutional Dropout Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.07395v1
- **DOI**: 10.1016/j.cma.2021.114038
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07395v1)
- **Published**: 2020-09-16 00:13:12+00:00
- **Updated**: 2020-09-16 00:13:12+00:00
- **Authors**: Gabriel Maher, Casey Fleeter, Daniele Schiavazzi, Alison Marsden
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to generate samples from the conditional distribution of patient-specific cardiovascular models given a clinically aquired image volume. A convolutional neural network architecture with dropout layers is first trained for vessel lumen segmentation using a regression approach, to enable Bayesian estimation of vessel lumen surfaces. This network is then integrated into a path-planning patient-specific modeling pipeline to generate families of cardiovascular models. We demonstrate our approach by quantifying the effect of geometric uncertainty on the hemodynamics for three patient-specific anatomies, an aorto-iliac bifurcation, an abdominal aortic aneurysm and a sub-model of the left coronary arteries. A key innovation introduced in the proposed approach is the ability to learn geometric uncertainty directly from training data. The results show how geometric uncertainty produces coefficients of variation comparable to or larger than other sources of uncertainty for wall shear stress and velocity magnitude, but has limited impact on pressure. Specifically, this is true for anatomies characterized by small vessel sizes, and for local vessel lesions seen infrequently during network training.



### EfficientNet-eLite: Extremely Lightweight and Efficient CNN Models for Edge Devices by Network Candidate Search
- **Arxiv ID**: http://arxiv.org/abs/2009.07409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07409v1)
- **Published**: 2020-09-16 01:11:10+00:00
- **Updated**: 2020-09-16 01:11:10+00:00
- **Authors**: Ching-Chen Wang, Ching-Te Chiu, Jheng-Yi Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Embedding Convolutional Neural Network (CNN) into edge devices for inference is a very challenging task because such lightweight hardware is not born to handle this heavyweight software, which is the common overhead from the modern state-of-the-art CNN models. In this paper, targeting at reducing the overhead with trading the accuracy as less as possible, we propose a novel of Network Candidate Search (NCS), an alternative way to study the trade-off between the resource usage and the performance through grouping concepts and elimination tournament. Besides, NCS can also be generalized across any neural network. In our experiment, we collect candidate CNN models from EfficientNet-B0 to be scaled down in varied way through width, depth, input resolution and compound scaling down, applying NCS to research the scaling-down trade-off. Meanwhile, a family of extremely lightweight EfficientNet is obtained, called EfficientNet-eLite. For further embracing the CNN edge application with Application-Specific Integrated Circuit (ASIC), we adjust the architectures of EfficientNet-eLite to build the more hardware-friendly version, EfficientNet-HF. Evaluation on ImageNet dataset, both proposed EfficientNet-eLite and EfficientNet-HF present better parameter usage and accuracy than the previous start-of-the-art CNNs. Particularly, the smallest member of EfficientNet-eLite is more lightweight than the best and smallest existing MnasNet with 1.46x less parameters and 0.56% higher accuracy. Code is available at https://github.com/Ching-Chen-Wang/EfficientNet-eLite



### Parallax Attention for Unsupervised Stereo Correspondence Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.08250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08250v2)
- **Published**: 2020-09-16 01:30:13+00:00
- **Updated**: 2021-10-12 16:24:19+00:00
- **Authors**: Longguang Wang, Yulan Guo, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An
- **Comment**: Accepted by IEEE TPAMI 2020. arXiv admin note: text overlap with
  arXiv:1903.05784
- **Journal**: None
- **Summary**: Stereo image pairs encode 3D scene cues into stereo correspondences between the left and right images. To exploit 3D cues within stereo images, recent CNN based methods commonly use cost volume techniques to capture stereo correspondence over large disparities. However, since disparities can vary significantly for stereo cameras with different baselines, focal lengths and resolutions, the fixed maximum disparity used in cost volume techniques hinders them to handle different stereo image pairs with large disparity variations. In this paper, we propose a generic parallax-attention mechanism (PAM) to capture stereo correspondence regardless of disparity variations. Our PAM integrates epipolar constraints with attention mechanism to calculate feature similarities along the epipolar line to capture stereo correspondence. Based on our PAM, we propose a parallax-attention stereo matching network (PASMnet) and a parallax-attention stereo image super-resolution network (PASSRnet) for stereo matching and stereo image super-resolution tasks. Moreover, we introduce a new and large-scale dataset named Flickr1024 for stereo image super-resolution. Experimental results show that our PAM is generic and can effectively learn stereo correspondence under large disparity variations in an unsupervised manner. Comparative results show that our PASMnet and PASSRnet achieve the state-of-the-art performance.



### Ground-truth or DAER: Selective Re-query of Secondary Information
- **Arxiv ID**: http://arxiv.org/abs/2009.07414v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07414v3)
- **Published**: 2020-09-16 01:44:19+00:00
- **Updated**: 2021-09-03 00:56:24+00:00
- **Authors**: Stephan J. Lemmer, Jason J. Corso
- **Comment**: Accepted to ICCV2021 Main: 12 pages, 7 figures. Supplementary: 4
  pages, 4 figures
- **Journal**: None
- **Summary**: Many vision tasks use secondary information at inference time -- a seed -- to assist a computer vision model in solving a problem. For example, an initial bounding box is needed to initialize visual object tracking. To date, all such work makes the assumption that the seed is a good one. However, in practice, from crowdsourcing to noisy automated seeds, this is often not the case. We hence propose the problem of seed rejection -- determining whether to reject a seed based on the expected performance degradation when it is provided in place of a gold-standard seed. We provide a formal definition to this problem, and focus on two meaningful subgoals: understanding causes of error and understanding the model's response to noisy seeds conditioned on the primary input. With these goals in mind, we propose a novel training method and evaluation metrics for the seed rejection problem. We then use seeded versions of the viewpoint estimation and fine-grained classification tasks to evaluate these contributions. In these experiments, we show our method can reduce the number of seeds that need to be reviewed for a target performance by over 23% compared to strong baselines.



### Multi-Label Activity Recognition using Activity-specific Features and Activity Correlations
- **Arxiv ID**: http://arxiv.org/abs/2009.07420v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07420v2)
- **Published**: 2020-09-16 01:57:34+00:00
- **Updated**: 2021-03-04 22:37:16+00:00
- **Authors**: Yanyi Zhang, Xinyu Li, Ivan Marsic
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label activity recognition is designed for recognizing multiple activities that are performed simultaneously or sequentially in each video. Most recent activity recognition networks focus on single-activities, that assume only one activity in each video. These networks extract shared features for all the activities, which are not designed for multi-label activities. We introduce an approach to multi-label activity recognition that extracts independent feature descriptors for each activity and learns activity correlations. This structure can be trained end-to-end and plugged into any existing network structures for video classification. Our method outperformed state-of-the-art approaches on four multi-label activity recognition datasets. To better understand the activity-specific features that the system generated, we visualized these activity-specific features in the Charades dataset.



### Classification and Region Analysis of COVID-19 Infection using Lung CT Images and Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.08864v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08864v1)
- **Published**: 2020-09-16 02:28:46+00:00
- **Updated**: 2020-09-16 02:28:46+00:00
- **Authors**: Saddam Hussain Khan, Anabia Sohail, Asifullah Khan, Yeon Soo Lee
- **Comment**: Pages: 32, Tables: 6, Figures: 14
- **Journal**: None
- **Summary**: COVID-19 is a global health problem. Consequently, early detection and analysis of the infection patterns are crucial for controlling infection spread as well as devising a treatment plan. This work proposes a two-stage deep Convolutional Neural Networks (CNNs) based framework for delineation of COVID-19 infected regions in Lung CT images. In the first stage, initially, COVID-19 specific CT image features are enhanced using a two-level discrete wavelet transformation. These enhanced CT images are then classified using the proposed custom-made deep CoV-CTNet. In the second stage, the CT images classified as infectious images are provided to the segmentation models for the identification and analysis of COVID-19 infectious regions. In this regard, we propose a novel semantic segmentation model CoV-RASeg, which systematically uses average and max pooling operations in the encoder and decoder blocks. This systematic utilization of max and average pooling operations helps the proposed CoV-RASeg in simultaneously learning both the boundaries and region homogeneity. Moreover, the idea of attention is incorporated to deal with mildly infected regions. The proposed two-stage framework is evaluated on a standard Lung CT image dataset, and its performance is compared with the existing deep CNN models. The performance of the proposed CoV-CTNet is evaluated using Mathew Correlation Coefficient (MCC) measure (0.98) and that of proposed CoV-RASeg using Dice Similarity (DS) score (0.95). The promising results on an unseen test set suggest that the proposed framework has the potential to help the radiologists in the identification and analysis of COVID-19 infected regions.



### Surgical Video Motion Magnification with Suppression of Instrument Artefacts
- **Arxiv ID**: http://arxiv.org/abs/2009.07432v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07432v1)
- **Published**: 2020-09-16 02:42:46+00:00
- **Updated**: 2020-09-16 02:42:46+00:00
- **Authors**: Mirek Janatka, Hani J. Marcus, Neil L. Dorward, Danail Stoyanov
- **Comment**: Early accept to the Internation Conference on Medical Imaging
  Computing and Computer Assisted Intervention (MICCAI) 2020 Presentation
  available here: https://www.youtube.com/watch?v=kKI_Ygny76Q Supplementary
  video available here: https://www.youtube.com/watch?v=8DUkcHI149Y
- **Journal**: None
- **Summary**: Video motion magnification could directly highlight subsurface blood vessels in endoscopic video in order to prevent inadvertent damage and bleeding. Applying motion filters to the full surgical image is however sensitive to residual motion from the surgical instruments and can impede practical application due to aberration motion artefacts. By storing the temporal filter response from local spatial frequency information for a single cardiovascular cycle prior to tool introduction to the scene, a filter can be used to determine if motion magnification should be active for a spatial region of the surgical image. In this paper, we propose a strategy to reduce aberration due to non-physiological motion for surgical video motion magnification. We present promising results on endoscopic transnasal transsphenoidal pituitary surgery with a quantitative comparison to recent methods using Structural Similarity (SSIM), as well as qualitative analysis by comparing spatio-temporal cross sections of the videos and individual frames.



### Handwritten Script Identification from Text Lines
- **Arxiv ID**: http://arxiv.org/abs/2009.07433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07433v1)
- **Published**: 2020-09-16 02:43:24+00:00
- **Updated**: 2020-09-16 02:43:24+00:00
- **Authors**: Pawan Kumar Singh, Iman Chatterjee, Ram Sarkar, Mita Nasipuri
- **Comment**: 12 pages, 4 figures, conference
- **Journal**: Proc. of 7th International Conference on Advances in
  Communication, Network and Computing (CNC), 2016
- **Summary**: In a multilingual country like India where 12 different official scripts are in use, automatic identification of handwritten script facilitates many important applications such as automatic transcription of multilingual documents, searching for documents on the web/digital archives containing a particular script and for the selection of script specific Optical Character Recognition (OCR) system in a multilingual environment. In this paper, we propose a robust method towards identifying scripts from the handwritten documents at text line-level. The recognition is based upon features extracted using Chain Code Histogram (CCH) and Discrete Fourier Transform (DFT). The proposed method is experimented on 800 handwritten text lines written in seven Indic scripts namely, Gujarati, Kannada, Malayalam, Oriya, Tamil, Telugu, Urdu along with Roman script and yielded an average identification rate of 95.14% using Support Vector Machine (SVM) classifier.



### A New Approach for Texture based Script Identification At Block Level using Quad Tree Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2009.07435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07435v1)
- **Published**: 2020-09-16 02:50:03+00:00
- **Updated**: 2020-09-16 02:50:03+00:00
- **Authors**: Pawan Kumar Singh, Supratim Das, Ram Sarkar, Mita Nasipuri
- **Comment**: 13 pages, 5 figures, conference
- **Journal**: 7th International Conference on Advances in Communication, Network
  and Computing (CNC), pp. 247-259, 2016
- **Summary**: A considerable amount of success has been achieved in developing monolingual OCR systems for Indic scripts. But in a country like India, where multi-script scenario is prevalent, identifying scripts beforehand becomes obligatory. In this paper, we present the significance of Gabor wavelets filters in extracting directional energy and entropy distributions for 11 official handwritten scripts namely, Bangla, Devanagari, Gujarati, Gurumukhi, Kannada, Malayalam, Oriya, Tamil, Telugu, Urdu and Roman. The experimentation is conducted at block level based on a quad-tree decomposition approach and evaluated using six different well-known classifiers. Finally, the best identification accuracy of 96.86% has been achieved by Multi Layer Perceptron (MLP) classifier for 3-fold cross validation at level-2 decomposition. The results serve to establish the efficacy of the present approach to the classification of handwritten Indic scripts



### Weakly-Supervised Online Hashing
- **Arxiv ID**: http://arxiv.org/abs/2009.07436v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07436v2)
- **Published**: 2020-09-16 02:50:22+00:00
- **Updated**: 2021-07-08 13:30:36+00:00
- **Authors**: Yu-Wei Zhan, Xin Luo, Yu Sun, Yongxin Wang, Zhen-Duo Chen, Xin-Shun Xu
- **Comment**: Accepted by ICME 2021
- **Journal**: None
- **Summary**: With the rapid development of social websites, recent years have witnessed an explosive growth of social images with user-provided tags which continuously arrive in a streaming fashion. Due to the fast query speed and low storage cost, hashing-based methods for image search have attracted increasing attention. However, existing hashing methods for social image retrieval are based on batch mode which violates the nature of social images, i.e., social images are usually generated periodically or collected in a stream fashion. Although there exist many online image hashing methods, they either adopt unsupervised learning which ignore the relevant tags, or are designed in the supervised manner which needs high-quality labels. In this paper, to overcome the above limitations, we propose a new method named Weakly-supervised Online Hashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak supervision by considering the semantics of tags and removing the noise. Besides, We develop a discrete online optimization algorithm for WOH, which is efficient and scalable. Extensive experiments conducted on two real-world datasets demonstrate the superiority of WOH compared with several state-of-the-art hashing baselines.



### Exploring Font-independent Features for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.07447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07447v1)
- **Published**: 2020-09-16 03:36:59+00:00
- **Updated**: 2020-09-16 03:36:59+00:00
- **Authors**: Yizhi Wang, Zhouhui Lian
- **Comment**: accepted by ACM MM 2020. Code: https://actasidiot.github.io/EFIFSTR/
- **Journal**: None
- **Summary**: Scene text recognition (STR) has been extensively studied in last few years. Many recently-proposed methods are specially designed to accommodate the arbitrary shape, layout and orientation of scene texts, but ignoring that various font (or writing) styles also pose severe challenges to STR. These methods, where font features and content features of characters are tangled, perform poorly in text recognition on scene images with texts in novel font styles. To address this problem, we explore font-independent features of scene texts via attentional generation of glyphs in a large number of font styles. Specifically, we introduce trainable font embeddings to shape the font styles of generated glyphs, with the image feature of scene text only representing its essential patterns. The generation process is directed by the spatial attention mechanism, which effectively copes with irregular texts and generates higher-quality glyphs than existing image-to-image translation methods. Experiments conducted on several STR benchmarks demonstrate the superiority of our method compared to the state of the art.



### Information Bottleneck Constrained Latent Bidirectional Embedding for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.07451v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07451v3)
- **Published**: 2020-09-16 03:54:12+00:00
- **Updated**: 2021-03-04 07:50:10+00:00
- **Authors**: Yang Liu, Lei Zhou, Xiao Bai, Lin Gu, Tatsuya Harada, Jun Zhou
- **Comment**: The new version is not complete
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen classes. Though many ZSL methods rely on a direct mapping between the visual and the semantic space, the calibration deviation and hubness problem limit the generalization capability to unseen classes. Recently emerged generative ZSL methods generate unseen image features to transform ZSL into a supervised classification problem. However, most generative models still suffer from the seen-unseen bias problem as only seen data is used for training. To address these issues, we propose a novel bidirectional embedding based generative model with a tight visual-semantic coupling constraint. We learn a unified latent space that calibrates the embedded parametric distributions of both visual and semantic spaces. Since the embedding from high-dimensional visual features comprise much non-semantic information, the alignment of visual and semantic in latent space would inevitably been deviated. Therefore, we introduce information bottleneck (IB) constraint to ZSL for the first time to preserve essential attribute information during the mapping. Specifically, we utilize the uncertainty estimation and the wake-sleep procedure to alleviate the feature noises and improve model abstraction capability. In addition, our method can be easily extended to transductive ZSL setting by generating labels for unseen images. We then introduce a robust loss to solve this label noise problem. Extensive experimental results show that our method outperforms the state-of-the-art methods in different ZSL settings on most benchmark datasets. The code will be available at https://github.com/osierboy/IBZSL.



### MSP: An FPGA-Specific Mixed-Scheme, Multi-Precision Deep Neural Network Quantization Framework
- **Arxiv ID**: http://arxiv.org/abs/2009.07460v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2009.07460v2)
- **Published**: 2020-09-16 04:24:18+00:00
- **Updated**: 2020-10-17 01:58:38+00:00
- **Authors**: Sung-En Chang, Yanyu Li, Mengshu Sun, Weiwen Jiang, Runbin Shi, Xue Lin, Yanzhi Wang
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: With the tremendous success of deep learning, there exists imminent need to deploy deep learning models onto edge devices. To tackle the limited computing and storage resources in edge devices, model compression techniques have been widely used to trim deep neural network (DNN) models for on-device inference execution. This paper targets the commonly used FPGA (field programmable gate array) devices as the hardware platforms for DNN edge computing. We focus on the DNN quantization as the main model compression technique, since DNN quantization has been of great importance for the implementations of DNN models on the hardware platforms. The novelty of this work comes in twofold: (i) We propose a mixed-scheme DNN quantization method that incorporates both the linear and non-linear number systems for quantization, with the aim to boost the utilization of the heterogeneous computing resources, i.e., LUTs (look up tables) and DSPs (digital signal processors) on an FPGA. Note that all the existing (single-scheme) quantization methods can only utilize one type of resources (either LUTs or DSPs for the MAC (multiply-accumulate) operations in deep learning computations. (ii) We use a quantization method that supports multiple precisions along the intra-layer dimension, while the existing quantization methods apply multi-precision quantization along the inter-layer dimension. The intra-layer multi-precision method can uniform the hardware configurations for different layers to reduce computation overhead and at the same time preserve the model accuracy as the inter-layer approach.



### PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line Features
- **Arxiv ID**: http://arxiv.org/abs/2009.07462v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.4.9; I.3.8
- **Links**: [PDF](http://arxiv.org/pdf/2009.07462v3)
- **Published**: 2020-09-16 04:27:33+00:00
- **Updated**: 2022-04-15 01:33:10+00:00
- **Authors**: Qiang Fu, Jialong Wang, Hongshan Yu, Islam Ali, Feng Guo, Yijia He, Hong Zhang
- **Comment**: Visual-Inertial SLAM, LSD, Lines, SLAM, VINS-Mono
- **Journal**: None
- **Summary**: Leveraging line features to improve localization accuracy of point-based visual-inertial SLAM (VINS) is gaining interest as they provide additional constraints on scene structure. However, real-time performance when incorporating line features in VINS has not been addressed. This paper presents PL-VINS, a real-time optimization-based monocular VINS method with point and line features, developed based on the state-of-the-art point-based VINS-Mono \cite{vins}. We observe that current works use the LSD \cite{lsd} algorithm to extract line features; however, LSD is designed for scene shape representation instead of the pose estimation problem, which becomes the bottleneck for the real-time performance due to its high computational cost. In this paper, a modified LSD algorithm is presented by studying a hidden parameter tuning and length rejection strategy. The modified LSD can run at least three times as fast as LSD. Further, by representing space lines with the Pl\"{u}cker coordinates, the residual error in line estimation is modeled in terms of the point-to-line distance, which is then minimized by iteratively updating the minimum four-parameter orthonormal representation of the Pl\"{u}cker coordinates. Experiments in a public benchmark dataset show that the localization error of our method is 12-16\% less than that of VINS-Mono at the same pose update frequency. %For the benefit of the community, The source code of our method is available at: https://github.com/cnqiangfu/PL-VINS.



### Deep Sinogram Completion with Image Prior for Metal Artifact Reduction in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2009.07469v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07469v1)
- **Published**: 2020-09-16 04:43:35+00:00
- **Updated**: 2020-09-16 04:43:35+00:00
- **Authors**: Lequan Yu, Zhicheng Zhang, Xiaomeng Li, Lei Xing
- **Comment**: Accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Computed tomography (CT) has been widely used for medical diagnosis, assessment, and therapy planning and guidance. In reality, CT images may be affected adversely in the presence of metallic objects, which could lead to severe metal artifacts and influence clinical diagnosis or dose calculation in radiation therapy. In this paper, we propose a generalizable framework for metal artifact reduction (MAR) by simultaneously leveraging the advantages of image domain and sinogram domain-based MAR techniques. We formulate our framework as a sinogram completion problem and train a neural network (SinoNet) to restore the metal-affected projections. To improve the continuity of the completed projections at the boundary of metal trace and thus alleviate new artifacts in the reconstructed CT images, we train another neural network (PriorNet) to generate a good prior image to guide sinogram learning, and further design a novel residual sinogram learning strategy to effectively utilize the prior image information for better sinogram completion. The two networks are jointly trained in an end-to-end fashion with a differentiable forward projection (FP) operation so that the prior image generation and deep sinogram completion procedures can benefit from each other. Finally, the artifact-reduced CT images are reconstructed using the filtered backward projection (FBP) from the completed sinogram. Extensive experiments on simulated and real artifacts data demonstrate that our method produces superior artifact-reduced results while preserving the anatomical structures and outperforms other MAR methods.



### Knowledge Guided Learning: Towards Open Domain Egocentric Action Recognition with Zero Supervision
- **Arxiv ID**: http://arxiv.org/abs/2009.07470v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.07470v2)
- **Published**: 2020-09-16 04:44:51+00:00
- **Updated**: 2022-03-12 00:02:25+00:00
- **Authors**: Sathyanarayanan N. Aakur, Sanjoy Kundu, Nikhil Gunti
- **Comment**: Pattern Recognition Lettters
- **Journal**: None
- **Summary**: Advances in deep learning have enabled the development of models that have exhibited a remarkable tendency to recognize and even localize actions in videos. However, they tend to experience errors when faced with scenes or examples beyond their initial training environment. Hence, they fail to adapt to new domains without significant retraining with large amounts of annotated data. In this paper, we propose to overcome these limitations by moving to an open-world setting by decoupling the ideas of recognition and reasoning. Building upon the compositional representation offered by Grenander's Pattern Theory formalism, we show that attention and commonsense knowledge can be used to enable the self-supervised discovery of novel actions in egocentric videos in an open-world setting, where data from the observed environment (the target domain) is open i.e., the vocabulary is partially known and training examples (both labeled and unlabeled) are not available. We show that our approach can infer and learn novel classes for open vocabulary classification in egocentric videos and novel object detection with zero supervision. Extensive experiments show its competitive performance on two publicly available egocentric action recognition datasets (GTEA Gaze and GTEA Gaze+) under open-world conditions.



### A Convolutional LSTM based Residual Network for Deepfake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.07480v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.07480v1)
- **Published**: 2020-09-16 05:57:06+00:00
- **Updated**: 2020-09-16 05:57:06+00:00
- **Authors**: Shahroz Tariq, Sangyup Lee, Simon S. Woo
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning-based video manipulation methods have become widely accessible to masses. With little to no effort, people can easily learn how to generate deepfake videos with only a few victims or target images. This creates a significant social problem for everyone whose photos are publicly available on the Internet, especially on social media websites. Several deep learning-based detection methods have been developed to identify these deepfakes. However, these methods lack generalizability, because they perform well only for a specific type of deepfake method. Therefore, those methods are not transferable to detect other deepfake methods. Also, they do not take advantage of the temporal information of the video. In this paper, we addressed these limitations. We developed a Convolutional LSTM based Residual Network (CLRNet), which takes a sequence of consecutive images as an input from a video to learn the temporal information that helps in detecting unnatural looking artifacts that are present between frames of deepfake videos. We also propose a transfer learning-based approach to generalize different deepfake methods. Through rigorous experimentations using the FaceForensics++ dataset, we showed that our method outperforms five of the previously proposed state-of-the-art deepfake detection methods by better generalizing at detecting different deepfake methods using the same model.



### Pooling Methods in Deep Neural Networks, a Review
- **Arxiv ID**: http://arxiv.org/abs/2009.07485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07485v1)
- **Published**: 2020-09-16 06:11:40+00:00
- **Updated**: 2020-09-16 06:11:40+00:00
- **Authors**: Hossein Gholamalinezhad, Hossein Khosravi
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Nowadays, Deep Neural Networks are among the main tools used in various sciences. Convolutional Neural Network is a special type of DNN consisting of several convolution layers, each followed by an activation function and a pooling layer. The pooling layer is an important layer that executes the down-sampling on the feature maps coming from the previous layer and produces new feature maps with a condensed resolution. This layer drastically reduces the spatial dimension of input. It serves two main purposes. The first is to reduce the number of parameters or weights, thus lessening the computational cost. The second is to control the overfitting of the network. An ideal pooling method is expected to extract only useful information and discard irrelevant details. There are a lot of methods for the implementation of pooling operation in Deep Neural Networks. In this paper, we reviewed some of the famous and useful pooling methods.



### Robust Person Re-Identification through Contextual Mutual Boosting
- **Arxiv ID**: http://arxiv.org/abs/2009.07491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07491v1)
- **Published**: 2020-09-16 06:33:35+00:00
- **Updated**: 2020-09-16 06:33:35+00:00
- **Authors**: Zhikang Wang, Lihuo He, Xinbo Gao, Jane Shen
- **Comment**: 8 pages, 4 figures, conference
- **Journal**: None
- **Summary**: Person Re-Identification (Re-ID) has witnessed great advance, driven by the development of deep learning. However, modern person Re-ID is still challenged by background clutter, occlusion and large posture variation which are common in practice. Previous methods tackle these challenges by localizing pedestrians through external cues (e.g., pose estimation, human parsing) or attention mechanism, suffering from high computation cost and increased model complexity. In this paper, we propose the Contextual Mutual Boosting Network (CMBN). It localizes pedestrians and recalibrates features by effectively exploiting contextual information and statistical inference. Firstly, we construct two branches with a shared convolutional frontend to learn the foreground and background features respectively. By enabling interaction between these two branches, they boost the accuracy of the spatial localization mutually. Secondly, starting from a statistical perspective, we propose the Mask Generator that exploits the activation distribution of the transformation matrix for generating the static channel mask to the representations. The mask recalibrates the features to amplify the valuable characteristics and diminish the noise. Finally, we propose the Contextual-Detachment Strategy to optimize the two branches jointly and independently, which further enhances the localization precision. Experiments on the benchmarks demonstrate the superiority of the architecture compared the state-of-the-art.



### Dual Semantic Fusion Network for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.07498v1
- **DOI**: 10.1145/3394171.3413583
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07498v1)
- **Published**: 2020-09-16 06:49:17+00:00
- **Updated**: 2020-09-16 06:49:17+00:00
- **Authors**: Lijian Lin, Haosheng Chen, Honglun Zhang, Jun Liang, Yu Li, Ying Shan, Hanzi Wang
- **Comment**: 9 pages,6 figures
- **Journal**: ACM Multimedia 2020
- **Summary**: Video object detection is a tough task due to the deteriorated quality of video sequences captured under complex environments. Currently, this area is dominated by a series of feature enhancement based methods, which distill beneficial semantic information from multiple frames and generate enhanced features through fusing the distilled information. However, the distillation and fusion operations are usually performed at either frame level or instance level with external guidance using additional information, such as optical flow and feature memory. In this work, we propose a dual semantic fusion network (abbreviated as DSFNet) to fully exploit both frame-level and instance-level semantics in a unified fusion framework without external guidance. Moreover, we introduce a geometric similarity measure into the fusion process to alleviate the influence of information distortion caused by noise. As a result, the proposed DSFNet can generate more robust features through the multi-granularity fusion and avoid being affected by the instability of external guidance. To evaluate the proposed DSFNet, we conduct extensive experiments on the ImageNet VID dataset. Notably, the proposed dual semantic fusion network achieves, to the best of our knowledge, the best performance of 84.1\% mAP among the current state-of-the-art video object detectors with ResNet-101 and 85.4\% mAP with ResNeXt-101 without using any post-processing steps.



### UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.07501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07501v1)
- **Published**: 2020-09-16 06:50:57+00:00
- **Updated**: 2020-09-16 06:50:57+00:00
- **Authors**: Yuanfeng Ji, Ruimao Zhang, Zhen Li, Jiamin Ren, Shaoting Zhang, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Aggregating multi-level feature representation plays a critical role in achieving robust volumetric medical image segmentation, which is important for the auxiliary diagnosis and treatment. Unlike the recent neural architecture search (NAS) methods that typically searched the optimal operators in each network layer, but missed a good strategy to search for feature aggregations, this paper proposes a novel NAS method for 3D medical image segmentation, named UXNet, which searches both the scale-wise feature aggregation strategies as well as the block-wise operators in the encoder-decoder network. UXNet has several appealing benefits. (1) It significantly improves flexibility of the classical UNet architecture, which only aggregates feature representations of encoder and decoder in equivalent resolution. (2) A continuous relaxation of UXNet is carefully designed, enabling its searching scheme performed in an efficient differentiable manner. (3) Extensive experiments demonstrate the effectiveness of UXNet compared with recent NAS methods for medical image segmentation. The architecture discovered by UXNet outperforms existing state-of-the-art models in terms of Dice on several public 3D medical image segmentation benchmarks, especially for the boundary locations and tiny tissues. The searching computational complexity of UXNet is cheap, enabling to search a network with the best performance less than 1.5 days on two TitanXP GPUs.



### The 1st Tiny Object Detection Challenge:Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2009.07506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07506v2)
- **Published**: 2020-09-16 07:01:38+00:00
- **Updated**: 2020-10-06 06:31:17+00:00
- **Authors**: Xuehui Yu, Zhenjun Han, Yuqi Gong, Nan Jiang, Jian Zhao, Qixiang Ye, Jie Chen, Yuan Feng, Bin Zhang, Xiaodi Wang, Ying Xin, Jingwei Liu, Mingyuan Mao, Sheng Xu, Baochang Zhang, Shumin Han, Cheng Gao, Wei Tang, Lizuo Jin, Mingbo Hong, Yuchao Yang, Shuiwang Li, Huan Luo, Qijun Zhao, Humphrey Shi
- **Comment**: ECCV2020 Workshop on Real-world Computer Vision from Inputs with
  Limited Quality (RLQ) and Tiny Object Detection Challenge
- **Journal**: None
- **Summary**: The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in developing novel and accurate methods for tiny object detection in images which have wide views, with a current focus on tiny person detection. The TinyPerson dataset was used for the TOD Challenge and is publicly released. It has 1610 images and 72651 box-levelannotations. Around 36 participating teams from the globe competed inthe 1st TOD Challenge. In this paper, we provide a brief summary of the1st TOD Challenge including brief introductions to the top three methods.The submission leaderboard will be reopened for researchers that areinterested in the TOD challenge. The benchmark dataset and other information can be found at: https://github.com/ucas-vg/TinyBenchmark.



### CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.07526v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.07526v2)
- **Published**: 2020-09-16 07:47:26+00:00
- **Updated**: 2021-06-08 06:27:33+00:00
- **Authors**: Jing Yu, Yuan Chai, Yujing Wang, Yue Hu, Qi Wu
- **Comment**: Accepted by IJCAI 2021. SOLE copyright holder is IJCAI (International
  Joint Conferences on Artificial Intelligence)
- **Journal**: None
- **Summary**: Scene graphs are semantic abstraction of images that encourage visual understanding and reasoning. However, the performance of Scene Graph Generation (SGG) is unsatisfactory when faced with biased data in real-world scenarios. Conventional debiasing research mainly studies from the view of balancing data distribution or learning unbiased models and representations, ignoring the correlations among the biased classes. In this work, we analyze this problem from a novel cognition perspective: automatically building a hierarchical cognitive structure from the biased predictions and navigating that hierarchy to locate the relationships, making the tail relationships receive more attention in a coarse-to-fine mode. To this end, we propose a novel debiasing Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive structure CogTree to organize the relationships based on the prediction of a biased SGG model. The CogTree distinguishes remarkably different relationships at first and then focuses on a small portion of easily confused ones. Then, we propose a debiasing loss specially for this cognitive structure, which supports coarse-to-fine distinction for the correct relationships. The loss is model-agnostic and consistently boosting the performance of several state-of-the-art models. The code is available at: https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.



### DRL-FAS: A Novel Framework Based on Deep Reinforcement Learning for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2009.07529v2
- **DOI**: 10.1109/TIFS.2020.3026553
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07529v2)
- **Published**: 2020-09-16 07:58:01+00:00
- **Updated**: 2020-09-18 06:08:06+00:00
- **Authors**: Rizhao Cai, Haoliang Li, Shiqi Wang, Changsheng Chen, Alex Chichung Kot
- **Comment**: Accepted by IEEE Transactions on Information Forensics and Security.
  Code will be released soon
- **Journal**: None
- **Summary**: Inspired by the philosophy employed by human beings to determine whether a presented face example is genuine or not, i.e., to glance at the example globally first and then carefully observe the local regions to gain more discriminative information, for the face anti-spoofing problem, we propose a novel framework based on the Convolutional Neural Network (CNN) and the Recurrent Neural Network (RNN). In particular, we model the behavior of exploring face-spoofing-related information from image sub-patches by leveraging deep reinforcement learning. We further introduce a recurrent mechanism to learn representations of local information sequentially from the explored sub-patches with an RNN. Finally, for the classification purpose, we fuse the local information with the global one, which can be learned from the original input image through a CNN. Moreover, we conduct extensive experiments, including ablation study and visualization analysis, to evaluate our proposed framework on various public databases. The experiment results show that our method can generally achieve state-of-the-art performance among all scenarios, demonstrating its effectiveness.



### m-arcsinh: An Efficient and Reliable Function for SVM and MLP in scikit-learn
- **Arxiv ID**: http://arxiv.org/abs/2009.07530v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MS, stat.ML, I.2.1; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2009.07530v1)
- **Published**: 2020-09-16 07:59:15+00:00
- **Updated**: 2020-09-16 07:59:15+00:00
- **Authors**: Luca Parisi
- **Comment**: 20 pages, 4 listings/Python code snippets, 2 figures, 15 tables
- **Journal**: None
- **Summary**: This paper describes the 'm-arcsinh', a modified ('m-') version of the inverse hyperbolic sine function ('arcsinh'). Kernel and activation functions enable Machine Learning (ML)-based algorithms, such as Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP), to learn from data in a supervised manner. m-arcsinh, implemented in the open source Python library 'scikit-learn', is hereby presented as an efficient and reliable kernel and activation function for SVM and MLP respectively. Improvements in reliability and speed to convergence in classification tasks on fifteen (N = 15) datasets available from scikit-learn and the University California Irvine (UCI) Machine Learning repository are discussed. Experimental results demonstrate the overall competitive classification performance of both SVM and MLP, achieved via the proposed function. This function is compared to gold standard kernel and activation functions, demonstrating its overall competitive reliability regardless of the complexity of the classification tasks involved.



### RCNN for Region of Interest Detection in Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2009.07532v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07532v2)
- **Published**: 2020-09-16 08:00:17+00:00
- **Updated**: 2020-09-18 01:14:25+00:00
- **Authors**: A Nugaliyadde, Kok Wai Wong, Jeremy Parry, Ferdous Sohel, Hamid Laga, Upeka V. Somaratne, Chris Yeomans, Orchid Foster
- **Comment**: This paper was accepted to the 27th International Conference on
  Neural Information Processing (ICONIP 2020) and will be published in the
  Springer CCIS Series
- **Journal**: None
- **Summary**: Digital pathology has attracted significant attention in recent years. Analysis of Whole Slide Images (WSIs) is challenging because they are very large, i.e., of Giga-pixel resolution. Identifying Regions of Interest (ROIs) is the first step for pathologists to analyse further the regions of diagnostic interest for cancer detection and other anomalies. In this paper, we investigate the use of RCNN, which is a deep machine learning technique, for detecting such ROIs only using a small number of labelled WSIs for training. For experimentation, we used real WSIs from a public hospital pathology service in Western Australia. We used 60 WSIs for training the RCNN model and another 12 WSIs for testing. The model was further tested on a new set of unseen WSIs. The results show that RCNN can be effectively used for ROI detection from WSIs.



### Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.07536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07536v2)
- **Published**: 2020-09-16 08:12:42+00:00
- **Updated**: 2021-06-06 03:05:03+00:00
- **Authors**: Guoqing Zhang, Junchuan Yang, Yuhui Zheng, Yi Wu, Shengyong Chen
- **Comment**: 11 pages, 8 figures, 66 conferences
- **Journal**: None
- **Summary**: Extracting effective and discriminative features is very important for addressing the challenging person re-identification (re-ID) task. Prevailing deep convolutional neural networks (CNNs) usually use high-level features for identifying pedestrian. However, some essential spatial information resided in low-level features such as shape, texture and color will be lost when learning the high-level features, due to extensive padding and pooling operations in the training stage. In addition, most existing person re-ID methods are mainly based on hand-craft bounding boxes where images are precisely aligned. It is unrealistic in practical applications, since the exploited object detection algorithms often produce inaccurate bounding boxes. This will inevitably degrade the performance of existing algorithms. To address these problems, we put forward a novel person re-ID model that fuses high- and low-level embeddings to reduce the information loss caused in learning high-level features. Then we divide the fused embedding into several parts and reconnect them to obtain the global feature and more significant local features, so as to alleviate the affect caused by the inaccurate bounding boxes. In addition, we also introduce the spatial and channel attention mechanisms in our model, which aims to mine more discriminative features related to the target. Finally, we reconstruct the feature extractor to ensure that our model can obtain more richer and robust features. Extensive experiments display the superiority of our approach compared with existing approaches. Our code is available at https://github.com/libraflower/MutipleFeature-for-PRID.



### SLGAN: Style- and Latent-guided Generative Adversarial Network for Desirable Makeup Transfer and Removal
- **Arxiv ID**: http://arxiv.org/abs/2009.07557v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.07557v3)
- **Published**: 2020-09-16 08:54:20+00:00
- **Updated**: 2020-09-24 13:08:51+00:00
- **Authors**: Daichi Horita, Kiyoharu Aizawa
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: There are five features to consider when using generative adversarial networks to apply makeup to photos of the human face. These features include (1) facial components, (2) interactive color adjustments, (3) makeup variations, (4) robustness to poses and expressions, and the (5) use of multiple reference images. Several related works have been proposed, mainly using generative adversarial networks (GAN). Unfortunately, none of them have addressed all five features simultaneously. This paper closes the gap with an innovative style- and latent-guided GAN (SLGAN). We provide a novel, perceptual makeup loss and a style-invariant decoder that can transfer makeup styles based on histogram matching to avoid the identity-shift problem. In our experiments, we show that our SLGAN is better than or comparable to state-of-the-art methods. Furthermore, we show that our proposal can interpolate facial makeup images to determine the unique features, compare existing methods, and help users find desirable makeup configurations.



### Similarity-based data mining for online domain adaptation of a sonar ATR system
- **Arxiv ID**: http://arxiv.org/abs/2009.07560v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2009.07560v1)
- **Published**: 2020-09-16 09:07:54+00:00
- **Updated**: 2020-09-16 09:07:54+00:00
- **Authors**: Jean de Bodinat, Thomas Guerneve, Jose Vazquez, Marija Jegorova
- **Comment**: Accepted for publication in IEEE OCEANS2020
- **Journal**: IEEE OCEANS2020
- **Summary**: Due to the expensive nature of field data gathering, the lack of training data often limits the performance of Automatic Target Recognition (ATR) systems. This problem is often addressed with domain adaptation techniques, however the currently existing methods fail to satisfy the constraints of resource and time-limited underwater systems. We propose to address this issue via an online fine-tuning of the ATR algorithm using a novel data-selection method. Our proposed data-mining approach relies on visual similarity and outperforms the traditionally employed hard-mining methods. We present a comparative performance analysis in a wide range of simulated environments and highlight the benefits of using our method for the rapid adaptation to previously unseen environments.



### Domain Adaptation for Outdoor Robot Traversability Estimation from RGB data with Safety-Preserving Loss
- **Arxiv ID**: http://arxiv.org/abs/2009.07565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.07565v1)
- **Published**: 2020-09-16 09:19:33+00:00
- **Updated**: 2020-09-16 09:19:33+00:00
- **Authors**: Simone Palazzo, Dario C. Guastella, Luciano Cantelli, Paolo Spadaro, Francesco Rundo, Giovanni Muscato, Daniela Giordano, Concetto Spampinato
- **Comment**: Accepted at IROS 2020
- **Journal**: None
- **Summary**: Being able to estimate the traversability of the area surrounding a mobile robot is a fundamental task in the design of a navigation algorithm. However, the task is often complex, since it requires evaluating distances from obstacles, type and slope of terrain, and dealing with non-obvious discontinuities in detected distances due to perspective. In this paper, we present an approach based on deep learning to estimate and anticipate the traversing score of different routes in the field of view of an on-board RGB camera. The backbone of the proposed model is based on a state-of-the-art deep segmentation model, which is fine-tuned on the task of predicting route traversability. We then enhance the model's capabilities by a) addressing domain shifts through gradient-reversal unsupervised adaptation, and b) accounting for the specific safety requirements of a mobile robot, by encouraging the model to err on the safe side, i.e., penalizing errors that would cause collisions with obstacles more than those that would cause the robot to stop in advance. Experimental results show that our approach is able to satisfactorily identify traversable areas and to generalize to unseen locations.



### Hierarchical brain parcellation with uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2009.07573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07573v1)
- **Published**: 2020-09-16 09:37:53+00:00
- **Updated**: 2020-09-16 09:37:53+00:00
- **Authors**: Mark S. Graham, Carole H. Sudre, Thomas Varsavsky, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: To be published in the MICCAI 2020 workshop: Uncertainty for Safe
  Utilization of Machine Learning in Medical Imaging
- **Journal**: None
- **Summary**: Many atlases used for brain parcellation are hierarchically organised, progressively dividing the brain into smaller sub-regions. However, state-of-the-art parcellation methods tend to ignore this structure and treat labels as if they are `flat'. We introduce a hierarchically-aware brain parcellation method that works by predicting the decisions at each branch in the label tree. We further show how this method can be used to model uncertainty separately for every branch in this label tree. Our method exceeds the performance of flat uncertainty methods, whilst also providing decomposed uncertainty estimates that enable us to obtain self-consistent parcellations and uncertainty maps at any level of the label hierarchy. We demonstrate a simple way these decision-specific uncertainty maps may be used to provided uncertainty-thresholded tissue maps at any level of the label tree.



### Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.07576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07576v1)
- **Published**: 2020-09-16 09:45:33+00:00
- **Updated**: 2020-09-16 09:45:33+00:00
- **Authors**: Yunus Can Bilge, Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis, Pinar Duygulu
- **Comment**: To appear in WACV 2021
- **Journal**: None
- **Summary**: In many real-world problems, there is typically a large discrepancy between the characteristics of data used in training versus deployment. A prime example is the analysis of aggression videos: in a criminal incidence, typically suspects need to be identified based on their clean portrait-like photos, instead of their prior video recordings. This results in three major challenges; large domain discrepancy between violence videos and ID-photos, the lack of video examples for most individuals and limited training data availability. To mimic such scenarios, we formulate a realistic domain-transfer problem, where the goal is to transfer the recognition model trained on clean posed images to the target domain of violent videos, where training videos are available only for a subset of subjects. To this end, we introduce the WildestFaces dataset, tailored to study cross-domain recognition under a variety of adverse conditions. We divide the task of transferring a recognition model from the domain of clean images to the violent videos into two sub-problems and tackle them using (i) stacked affine-transforms for classifier-transfer, (ii) attention-driven pooling for temporal-adaptation. We additionally formulate a self-attention based model for domain-transfer. We establish a rigorous evaluation protocol for this clean-to-violent recognition task, and present a detailed analysis of the proposed dataset and the methods. Our experiments highlight the unique challenges introduced by the WildestFaces dataset and the advantages of the proposed approach.



### Compressing Facial Makeup Transfer Networks by Collaborative Distillation and Kernel Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2009.07604v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.07604v1)
- **Published**: 2020-09-16 11:07:04+00:00
- **Updated**: 2020-09-16 11:07:04+00:00
- **Authors**: Bianjiang Yang, Zi Hui, Haoji Hu, Xinyi Hu, Lu Yu
- **Comment**: This paper will be published on 2020 IEEE International Conference on
  Visual Communications and Image Processing (VCIP)
- **Journal**: None
- **Summary**: Although the facial makeup transfer network has achieved high-quality performance in generating perceptually pleasing makeup images, its capability is still restricted by the massive computation and storage of the network architecture. We address this issue by compressing facial makeup transfer networks with collaborative distillation and kernel decomposition. The main idea of collaborative distillation is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for low-level vision tasks. For kernel decomposition, we apply the depth-wise separation of convolutional kernels to build a light-weighted Convolutional Neural Network (CNN) from the original network. Extensive experiments show the effectiveness of the compression method when applied to the state-of-the-art facial makeup transfer network -- BeautyGAN.



### Deep Learning in Photoacoustic Tomography: Current approaches and future directions
- **Arxiv ID**: http://arxiv.org/abs/2009.07608v1
- **DOI**: 10.1117/1.JBO.25.11.112903
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2009.07608v1)
- **Published**: 2020-09-16 11:33:29+00:00
- **Updated**: 2020-09-16 11:33:29+00:00
- **Authors**: Andreas Hauptmann, Ben Cox
- **Comment**: None
- **Journal**: J. of Biomedical Optics, 25(11), 112903 (2020)
- **Summary**: Biomedical photoacoustic tomography, which can provide high resolution 3D soft tissue images based on the optical absorption, has advanced to the stage at which translation from the laboratory to clinical settings is becoming possible. The need for rapid image formation and the practical restrictions on data acquisition that arise from the constraints of a clinical workflow are presenting new image reconstruction challenges. There are many classical approaches to image reconstruction, but ameliorating the effects of incomplete or imperfect data through the incorporation of accurate priors is challenging and leads to slow algorithms. Recently, the application of Deep Learning, or deep neural networks, to this problem has received a great deal of attention. This paper reviews the literature on learned image reconstruction, summarising the current trends, and explains how these new approaches fit within, and to some extent have arisen from, a framework that encompasses classical reconstruction methods. In particular, it shows how these new techniques can be understood from a Bayesian perspective, providing useful insights. The paper also provides a concise tutorial demonstration of three prototypical approaches to learned image reconstruction. The code and data sets for these demonstrations are available to researchers. It is anticipated that it is in in vivo applications - where data may be sparse, fast imaging critical and priors difficult to construct by hand - that Deep Learning will have the most impact. With this in mind, the paper concludes with some indications of possible future research directions.



### Perceiving Traffic from Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2009.07611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07611v1)
- **Published**: 2020-09-16 11:37:43+00:00
- **Updated**: 2020-09-16 11:37:43+00:00
- **Authors**: George Adaimi, Sven Kreiss, Alexandre Alahi
- **Comment**: None
- **Journal**: None
- **Summary**: Drones or UAVs, equipped with different sensors, have been deployed in many places especially for urban traffic monitoring or last-mile delivery. It provides the ability to control the different aspects of traffic given real-time obeservations, an important pillar for the future of transportation and smart cities. With the increasing use of such machines, many previous state-of-the-art object detectors, who have achieved high performance on front facing cameras, are being used on UAV datasets. When applied to high-resolution aerial images captured from such datasets, they fail to generalize to the wide range of objects' scales. In order to address this limitation, we propose an object detection method called Butterfly Detector that is tailored to detect objects in aerial images. We extend the concept of fields and introduce butterfly fields, a type of composite field that describes the spatial information of output features as well as the scale of the detected object. To overcome occlusion and viewing angle variations that can hinder the localization process, we employ a voting mechanism between related butterfly vectors pointing to the object center. We evaluate our Butterfly Detector on two publicly available UAV datasets (UAVDT and VisDrone2019) and show that it outperforms previous state-of-the-art methods while remaining real-time.



### Multi-Stage CNN Architecture for Face Mask Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.07627v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07627v2)
- **Published**: 2020-09-16 12:23:21+00:00
- **Updated**: 2020-09-17 09:08:18+00:00
- **Authors**: Amit Chavda, Jason Dsouza, Sumeet Badgujar, Ankit Damani
- **Comment**: None
- **Journal**: None
- **Summary**: The end of 2019 witnessed the outbreak of Coronavirus Disease 2019 (COVID-19), which has continued to be the cause of plight for millions of lives and businesses even in 2020. As the world recovers from the pandemic and plans to return to a state of normalcy, there is a wave of anxiety among all individuals, especially those who intend to resume in-person activity. Studies have proved that wearing a face mask significantly reduces the risk of viral transmission as well as provides a sense of protection. However, it is not feasible to manually track the implementation of this policy. Technology holds the key here. We introduce a Deep Learning based system that can detect instances where face masks are not used properly. Our system consists of a dual-stage Convolutional Neural Network (CNN) architecture capable of detecting masked and unmasked faces and can be integrated with pre-installed CCTV cameras. This will help track safety violations, promote the use of face masks, and ensure a safe working environment.



### ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action Unit
- **Arxiv ID**: http://arxiv.org/abs/2009.07637v1
- **DOI**: 10.1145/3394171.3414005
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.07637v1)
- **Published**: 2020-09-16 12:38:19+00:00
- **Updated**: 2020-09-16 12:38:19+00:00
- **Authors**: Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, Yanfeng Wang
- **Comment**: 10 pages, 5 figures, Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Dance and music are two highly correlated artistic forms. Synthesizing dance motions has attracted much attention recently. Most previous works conduct music-to-dance synthesis via directly music to human skeleton keypoints mapping. Meanwhile, human choreographers design dance motions from music in a two-stage manner: they firstly devise multiple choreographic dance units (CAUs), each with a series of dance motions, and then arrange the CAU sequence according to the rhythm, melody and emotion of the music. Inspired by these, we systematically study such two-stage choreography approach and construct a dataset to incorporate such choreography knowledge. Based on the constructed dataset, we design a two-stage music-to-dance synthesis framework ChoreoNet to imitate human choreography procedure. Our framework firstly devises a CAU prediction model to learn the mapping relationship between music and CAU sequences. Afterwards, we devise a spatial-temporal inpainting model to convert the CAU sequence into continuous dance motions. Experimental results demonstrate that the proposed ChoreoNet outperforms baseline methods (0.622 in terms of CAU BLEU score and 1.59 in terms of user study score).



### Eating Habits Discovery in Egocentric Photo-streams
- **Arxiv ID**: http://arxiv.org/abs/2009.07646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07646v1)
- **Published**: 2020-09-16 12:46:35+00:00
- **Updated**: 2020-09-16 12:46:35+00:00
- **Authors**: Estefania Talavera, Andreea Glavan, Alina Matei, Petia Radeva
- **Comment**: None
- **Journal**: None
- **Summary**: Eating habits are learned throughout the early stages of our lives. However, it is not easy to be aware of how our food-related routine affects our healthy living. In this work, we address the unsupervised discovery of nutritional habits from egocentric photo-streams. We build a food-related behavioural pattern discovery model, which discloses nutritional routines from the activities performed throughout the days. To do so, we rely on Dynamic-Time-Warping for the evaluation of similarity among the collected days. Within this framework, we present a simple, but robust and fast novel classification pipeline that outperforms the state-of-the-art on food-related image classification with a weighted accuracy and F-score of 70% and 63%, respectively. Later, we identify days composed of nutritional activities that do not describe the habits of the person as anomalies in the daily life of the user with the Isolation Forest method. Furthermore, we show an application for the identification of food-related scenes when the camera wearer eats in isolation. Results have shown the good performance of the proposed model and its relevance to visualize the nutritional habits of individuals.



### Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery
- **Arxiv ID**: http://arxiv.org/abs/2009.07683v1
- **DOI**: 10.1109/TGRS.2020.3024744
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07683v1)
- **Published**: 2020-09-16 13:40:42+00:00
- **Updated**: 2020-09-16 13:40:42+00:00
- **Authors**: Patrick Ebel, Andrea Meraner, Michael Schmitt, Xiaoxiang Zhu
- **Comment**: This work has been accepted by IEEE TGRS for publication
- **Journal**: None
- **Summary**: This work has been accepted by IEEE TGRS for publication. The majority of optical observations acquired via spaceborne earth imagery are affected by clouds. While there is numerous prior work on reconstructing cloud-covered information, previous studies are oftentimes confined to narrowly-defined regions of interest, raising the question of whether an approach can generalize to a diverse set of observations acquired at variable cloud coverage or in different regions and seasons. We target the challenge of generalization by curating a large novel data set for training new cloud removal approaches and evaluate on two recently proposed performance metrics of image quality and diversity. Our data set is the first publically available to contain a global sample of co-registered radar and optical observations, cloudy as well as cloud-free. Based on the observation that cloud coverage varies widely between clear skies and absolute coverage, we propose a novel model that can deal with either extremes and evaluate its performance on our proposed data set. Finally, we demonstrate the superiority of training models on real over synthetic data, underlining the need for a carefully curated data set of real observations. To facilitate future research, our data set is made available online



### Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News
- **Arxiv ID**: http://arxiv.org/abs/2009.07698v5
- **DOI**: 10.18653/v1/2020.emnlp-main.163
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07698v5)
- **Published**: 2020-09-16 14:13:15+00:00
- **Updated**: 2020-10-21 15:16:20+00:00
- **Authors**: Reuben Tan, Bryan A. Plummer, Kate Saenko
- **Comment**: Accepted at EMNLP 2020
- **Journal**: None
- **Summary**: Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study experiments, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation.



### Calibrating Self-supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.07714v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07714v2)
- **Published**: 2020-09-16 14:35:45+00:00
- **Updated**: 2021-10-13 12:41:43+00:00
- **Authors**: Robert McCraith, Lukas Neumann, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent years, many methods demonstrated the ability of neural networks to learn depth and pose changes in a sequence of images, using only self-supervision as the training signal. Whilst the networks achieve good performance, the often over-looked detail is that due to the inherent ambiguity of monocular vision they predict depth up to an unknown scaling factor. The scaling factor is then typically obtained from the LiDAR ground truth at test time, which severely limits practical applications of these methods. In this paper, we show that incorporating prior information about the camera configuration and the environment, we can remove the scale ambiguity and predict depth directly, still using the self-supervised formulation and not relying on any additional sensors.



### Domain-invariant Similarity Activation Map Contrastive Learning for Retrieval-based Long-term Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2009.07719v4
- **DOI**: 10.1109/JAS.2021.1003907
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07719v4)
- **Published**: 2020-09-16 14:43:22+00:00
- **Updated**: 2021-10-14 17:27:32+00:00
- **Authors**: Hanjiang Hu, Hesheng Wang, Zhe Liu, Weidong Chen
- **Comment**: Published in IEEE/CAA Journal of Automatica Sinica
- **Journal**: None
- **Summary**: Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g. illumination, seasonal and weather changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain invariant feature through multi-domain image translation. And then a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models without and with Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMUSeasons dataset. The strong generalization ability of our approach is verified on RobotCar dataset using models pre-trained on urban part of CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under the challenging environments with illumination variance, vegetation and night-time images. The code and pretrained models are available on https://github.com/HanjiangHu/DISAM.



### SelfAugment: Automatic Augmentation Policies for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.07724v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07724v3)
- **Published**: 2020-09-16 14:49:03+00:00
- **Updated**: 2021-05-17 15:11:06+00:00
- **Authors**: Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, Kurt Keutzer
- **Comment**: Computer Vision and Pattern Recognition (CVPR), 2021
- **Journal**: None
- **Summary**: A common practice in unsupervised representation learning is to use labeled data to evaluate the quality of the learned representations. This supervised evaluation is then used to guide critical aspects of the training process such as selecting the data augmentation policy. However, guiding an unsupervised training process through supervised evaluations is not possible for real-world data that does not actually contain labels (which may be the case, for example, in privacy sensitive fields such as medical imaging). Therefore, in this work we show that evaluating the learned representations with a self-supervised image rotation task is highly correlated with a standard set of supervised evaluations (rank correlation $> 0.94$). We establish this correlation across hundreds of augmentation policies, training settings, and network architectures and provide an algorithm (SelfAugment) to automatically and efficiently select augmentation policies without using supervised evaluations. Despite not using any labeled data, the learned augmentation policies perform comparably with augmentation policies that were determined using exhaustive supervised evaluations.



### TreeGAN: Incorporating Class Hierarchy into Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.07734v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.07734v2)
- **Published**: 2020-09-16 15:06:52+00:00
- **Updated**: 2021-11-30 04:29:09+00:00
- **Authors**: Ruisi Zhang, Luntian Mou, Pengtao Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional image generation (CIG) is a widely studied problem in computer vision and machine learning. Given a class, CIG takes the name of this class as input and generates a set of images that belong to this class. In existing CIG works, for different classes, their corresponding images are generated independently, without considering the relationship among classes. In real-world applications, the classes are organized into a hierarchy and their hierarchical relationships are informative for generating high-fidelity images. In this paper, we aim to leverage the class hierarchy for conditional image generation. We propose two ways of incorporating class hierarchy: prior control and post constraint. In prior control, we first encode the class hierarchy, then feed it as a prior into the conditional generator to generate images. In post constraint, after the images are generated, we measure their consistency with the class hierarchy and use the consistency score to guide the training of the generator. Based on these two ideas, we propose a TreeGAN model which consists of three modules: (1) a class hierarchy encoder (CHE) which takes the hierarchical structure of classes and their textual names as inputs and learns an embedding for each class; the embedding captures the hierarchical relationship among classes; (2) a conditional image generator (CIG) which takes the CHE-generated embedding of a class as input and generates a set of images belonging to this class; (3) a consistency checker which performs hierarchical classification on the generated images and checks whether the generated images are compatible with the class hierarchy; the consistency score is used to guide the CIG to generate hierarchy-compatible images. Experiments on various datasets demonstrate the effectiveness of our method.



### HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.07736v2
- **DOI**: 10.1007/s11263-020-01375-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07736v2)
- **Published**: 2020-09-16 15:11:30+00:00
- **Updated**: 2020-09-29 10:40:09+00:00
- **Authors**: Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixe, Bastian Leibe
- **Comment**: Pre-print. Accepted for Publication in the International Journal of
  Computer Vision, 19 August 2020. Code is available at
  https://github.com/JonathonLuiten/HOTA-metrics
- **Journal**: International Journal of Computer Vision (2020)
- **Summary**: Multi-Object Tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, HOTA (Higher Order Tracking Accuracy), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.



### GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2009.07823v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07823v4)
- **Published**: 2020-09-16 17:33:01+00:00
- **Updated**: 2021-04-05 14:00:51+00:00
- **Authors**: Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: code: https://github.com/PruneTruong/GOCor. Website:
  https://prunetruong.com/research/gocor. Video:
  https://www.youtube.com/watch?v=V22MyFChBCs. NeurIPS 2020
- **Journal**: Annual Conference on Neural Information Processing Systems,
  NeurIPS, 2020
- **Summary**: The feature correlation layer serves as a key neural network module in numerous computer vision problems that involve dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors extracted from pairs of locations in two images. However, this point-to-point feature comparison is insufficient when disambiguating multiple similar regions in an image, severely affecting the performance of the end task. We propose GOCor, a fully differentiable dense matching module, acting as a direct replacement to the feature correlation layer. The correspondence volume generated by our module is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene. Moreover, our approach is capable of effectively learning spatial matching priors to resolve further matching ambiguities. We analyze our GOCor module in extensive ablative experiments. When integrated into state-of-the-art networks, our approach significantly outperforms the feature correlation layer for the tasks of geometric matching, optical flow, and dense semantic matching. The code and trained models will be made available at github.com/PruneTruong/GOCor.



### Multiple Exemplars-based Hallucinationfor Face Super-resolution and Editing
- **Arxiv ID**: http://arxiv.org/abs/2009.07827v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07827v3)
- **Published**: 2020-09-16 17:35:26+00:00
- **Updated**: 2021-01-14 09:55:02+00:00
- **Authors**: Kaili Wang, Jose Oramas, Tinne Tuytelaars
- **Comment**: accepted in ACCV 2020
- **Journal**: None
- **Summary**: Given a really low-resolution input image of a face (say 16x16 or 8x8 pixels), the goal of this paper is to reconstruct a high-resolution version thereof. This, by itself, is an ill-posed problem, as the high-frequency information is missing in the low-resolution input and needs to be hallucinated, based on prior knowledge about the image content. Rather than relying on a generic face prior, in this paper, we explore the use of a set of exemplars, i.e. other high-resolution images of the same person. These guide the neural network as we condition the output on them. Multiple exemplars work better than a single one. To combine the information from multiple exemplars effectively, we introduce a pixel-wise weight generation module. Besides standard face super-resolution, our method allows to perform subtle face editing simply by replacing the exemplars with another set with different facial features. A user study is conducted and shows the super-resolved images can hardly be distinguished from real images on the CelebA dataset. A qualitative comparison indicates our model outperforms methods proposed in the literature on the CelebA and WebFace dataset.



### Layered Neural Rendering for Retiming People in Video
- **Arxiv ID**: http://arxiv.org/abs/2009.07833v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2009.07833v2)
- **Published**: 2020-09-16 17:48:26+00:00
- **Updated**: 2021-10-01 01:15:41+00:00
- **Authors**: Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T. Freeman, Michael Rubinstein
- **Comment**: In SIGGRAPH Asia 2020. Project webpage: https://retiming.github.io/.
  Added references
- **Journal**: None
- **Summary**: We present a method for retiming people in an ordinary, natural video -- manipulating and editing the time in which different motions of individuals in the video occur. We can temporally align different motions, change the speed of certain actions (speeding up/slowing down, or entirely "freezing" people), or "erase" selected people from the video altogether. We achieve these effects computationally via a dedicated learning-based layered video representation, where each frame in the video is decomposed into separate RGBA layers, representing the appearance of different people in the video. A key property of our model is that it not only disentangles the direct motions of each person in the input video, but also correlates each person automatically with the scene changes they generate -- e.g., shadows, reflections, and motion of loose clothing. The layers can be individually retimed and recombined into a new video, allowing us to achieve realistic, high-quality renderings of retiming effects for real-world videos depicting complex actions and involving multiple individuals, including dancing, trampoline jumping, or group running.



### FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.07838v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07838v2)
- **Published**: 2020-09-16 17:56:22+00:00
- **Updated**: 2020-12-02 17:17:48+00:00
- **Authors**: Tom Sixta, Julio C. S. Jacques Junior, Pau Buch-Cardona, Neil M. Robertson, Eduard Vazquez, Sergio Escalera
- **Comment**: accepted on ECCV'2020 Fair Face Recognition and Analysis Workshop
- **Journal**: None
- **Summary**: This work summarizes the 2020 ChaLearn Looking at People Fair Face Recognition and Analysis Challenge and provides a description of the top-winning solutions and analysis of the results. The aim of the challenge was to evaluate accuracy and bias in gender and skin colour of submitted algorithms on the task of 1:1 face verification in the presence of other confounding attributes. Participants were evaluated using an in-the-wild dataset based on reannotated IJB-C, further enriched by 12.5K new images and additional labels. The dataset is not balanced, which simulates a real world scenario where AI-based models supposed to present fair outcomes are trained and evaluated on imbalanced data. The challenge attracted 151 participants, who made more than 1.8K submissions in total. The final phase of the challenge attracted 36 active teams out of which 10 exceeded 0.999 AUC-ROC while achieving very low scores in the proposed bias metrics. Common strategies by the participants were face pre-processing, homogenization of data distributions, the use of bias aware loss functions and ensemble models. The analysis of top-10 teams shows higher false positive rates (and lower false negative rates) for females with dark skin tone as well as the potential of eyeglasses and young age to increase the false positive rates too.



### Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning
- **Arxiv ID**: http://arxiv.org/abs/2009.07879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.07879v1)
- **Published**: 2020-09-16 18:18:49+00:00
- **Updated**: 2020-09-16 18:18:49+00:00
- **Authors**: Qiong Liu, Yanxia Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As data from IoT (Internet of Things) sensors become ubiquitous, state-of-the-art machine learning algorithms face many challenges on directly using sensor data. To overcome these challenges, methods must be designed to learn directly from sensors without manual annotations. This paper introduces Sensory Time-cue for Unsupervised Meta-learning (STUM). Different from traditional learning approaches that either heavily depend on labels or on time-independent feature extraction assumptions, such as Gaussian distribution features, the STUM system uses time relation of inputs to guide the feature space formation within and across modalities. The fact that STUM learns from a variety of small tasks may put this method in the camp of Meta-Learning. Different from existing Meta-Learning approaches, STUM learning tasks are composed within and across multiple modalities based on time-cue co-exist with the IoT streaming data. In an audiovisual learning example, because consecutive visual frames usually comprise the same object, this approach provides a unique way to organize features from the same object together. The same method can also organize visual object features with the object's spoken-name features together if the spoken name is presented with the object at about the same time. This cross-modality feature organization may further help the organization of visual features that belong to similar objects but acquired at different location and time. Promising results are achieved through evaluations.



### Skeletonization and Reconstruction based on Graph Morphological Transformations
- **Arxiv ID**: http://arxiv.org/abs/2009.07970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07970v1)
- **Published**: 2020-09-16 22:58:06+00:00
- **Updated**: 2020-09-16 22:58:06+00:00
- **Authors**: Hossein Memarzadeh Sharifipour, Bardia Yousefi, Xavier P. V. Maldague
- **Comment**: None
- **Journal**: None
- **Summary**: Multiscale shape skeletonization on pixel adjacency graphs is an advanced intriguing research subject in the field of image processing, computer vision and data mining. The previous works in this area almost focused on the graph vertices. We proposed novel structured based graph morphological transformations based on edges opposite to the current node based transformations and used them for deploying skeletonization and reconstruction of infrared thermal images represented by graphs. The advantage of this method is that many widely used path based approaches become available within this definition of morphological operations. For instance, we use distance maps and image foresting transform (IFT) as two main path based methods are utilized for computing the skeleton of an image. Moreover, In addition, the open question proposed by Maragos et al (2013) about connectivity of graph skeletonization method are discussed and shown to be quite difficult to decide in general case.



### Analysis of Generalizability of Deep Neural Networks Based on the Complexity of Decision Boundary
- **Arxiv ID**: http://arxiv.org/abs/2009.07974v1
- **DOI**: 10.1109/ICMLA51294.2020.00025
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.07974v1)
- **Published**: 2020-09-16 23:25:52+00:00
- **Updated**: 2020-09-16 23:25:52+00:00
- **Authors**: Shuyue Guan, Murray Loew
- **Comment**: 7 pages, 11 figures. Accepted by ICMLA 2020
- **Journal**: 19th IEEE International Conference on Machine Learning and
  Applications (ICMLA), 2020, pp. 101-106
- **Summary**: For supervised learning models, the analysis of generalization ability (generalizability) is vital because the generalizability expresses how well a model will perform on unseen data. Traditional generalization methods, such as the VC dimension, do not apply to deep neural network (DNN) models. Thus, new theories to explain the generalizability of DNNs are required. In this study, we hypothesize that the DNN with a simpler decision boundary has better generalizability by the law of parsimony (Occam's Razor). We create the decision boundary complexity (DBC) score to define and measure the complexity of decision boundary of DNNs. The idea of the DBC score is to generate data points (called adversarial examples) on or near the decision boundary. Our new approach then measures the complexity of the boundary using the entropy of eigenvalues of these data. The method works equally well for high-dimensional data. We use training data and the trained model to compute the DBC score. And, the ground truth for model's generalizability is its test accuracy. Experiments based on the DBC score have verified our hypothesis. The DBC is shown to provide an effective method to measure the complexity of a decision boundary and gives a quantitative measure of the generalizability of DNNs.



### Noise-Aware Merging of High Dynamic Range Image Stacks without Camera Calibration
- **Arxiv ID**: http://arxiv.org/abs/2009.07975v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07975v1)
- **Published**: 2020-09-16 23:26:17+00:00
- **Updated**: 2020-09-16 23:26:17+00:00
- **Authors**: Param Hanji, Fangcheng Zhong, Rafal K. Mantiuk
- **Comment**: None
- **Journal**: None
- **Summary**: A near-optimal reconstruction of the radiance of a High Dynamic Range scene from an exposure stack can be obtained by modeling the camera noise distribution. The latent radiance is then estimated using Maximum Likelihood Estimation. But this requires a well-calibrated noise model of the camera, which is difficult to obtain in practice. We show that an unbiased estimation of comparable variance can be obtained with a simpler Poisson noise estimator, which does not require the knowledge of camera-specific noise parameters. We demonstrate this empirically for four different cameras, ranging from a smartphone camera to a full-frame mirrorless camera. Our experimental results are consistent for simulated as well as real images, and across different camera settings.




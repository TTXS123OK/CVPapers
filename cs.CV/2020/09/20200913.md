# Arxiv Papers in cs.CV on 2020-09-13
### TCDesc: Learning Topology Consistent Descriptors for Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2009.07036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07036v1)
- **Published**: 2020-09-13 03:32:37+00:00
- **Updated**: 2020-09-13 03:32:37+00:00
- **Authors**: Honghu Pan, Fanyang Meng, Nana Fan, Zhenyu He
- **Comment**: arXiv admin note: text overlap with arXiv:2006.03254
- **Journal**: None
- **Summary**: The constraint of neighborhood consistency or local consistency is widely used for robust image matching. In this paper, we focus on learning neighborhood topology consistent descriptors (TCDesc), while former works of learning descriptors, such as HardNet and DSM, only consider point-to-point Euclidean distance among descriptors and totally neglect neighborhood information of descriptors. To learn topology consistent descriptors, first we propose the linear combination weights to depict the topological relationship between center descriptor and its kNN descriptors, where the difference between center descriptor and the linear combination of its kNN descriptors is minimized. Then we propose the global mapping function which maps the local linear combination weights to the global topology vector and define the topology distance of matching descriptors as l1 distance between their topology vectors. Last we employ adaptive weighting strategy to jointly minimize topology distance and Euclidean distance, which automatically adjust the weight or attention of two distances in triplet loss. Our method has the following two advantages: (1) We are the first to consider neighborhood information of descriptors, while former works mainly focus on neighborhood consistency of feature points; (2) Our method can be applied in any former work of learning descriptors by triplet loss. Experimental results verify the generalization of our method: We can improve the performances of both HardNet and DSM on several benchmarks.



### Attention Cube Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2009.05907v3
- **DOI**: 10.1145/3394171.3413564
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.05907v3)
- **Published**: 2020-09-13 03:42:14+00:00
- **Updated**: 2021-01-24 11:32:04+00:00
- **Authors**: Yucheng Hang, Qingmin Liao, Wenming Yang, Yupeng Chen, Jie Zhou
- **Comment**: Accepted by the 28th ACM International Conference on Multimedia (ACM
  MM 2020); Code is available at https://github.com/YCHang686/A-CubeNet
- **Journal**: None
- **Summary**: Recently, deep convolutional neural network (CNN) have been widely used in image restoration and obtained great success. However, most of existing methods are limited to local receptive field and equal treatment of different types of information. Besides, existing methods always use a multi-supervised method to aggregate different feature maps, which can not effectively aggregate hierarchical feature information. To address these issues, we propose an attention cube network (A-CubeNet) for image restoration for more powerful feature expression and feature correlation learning. Specifically, we design a novel attention mechanism from three dimensions, namely spatial dimension, channel-wise dimension and hierarchical dimension. The adaptive spatial attention branch (ASAB) and the adaptive channel attention branch (ACAB) constitute the adaptive dual attention module (ADAM), which can capture the long-range spatial and channel-wise contextual information to expand the receptive field and distinguish different types of information for more effective feature representations. Furthermore, the adaptive hierarchical attention module (AHAM) can capture the long-range hierarchical contextual information to flexibly aggregate different feature maps by weights depending on the global context. The ADAM and AHAM cooperate to form an "attention in attention" structure, which means AHAM's inputs are enhanced by ASAB and ACAB. Experiments demonstrate the superiority of our method over state-of-the-art image restoration methods in both quantitative comparison and visual analysis. Code is available at https://github.com/YCHang686/A-CubeNet.



### Deep Detection for Face Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2009.05934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.05934v1)
- **Published**: 2020-09-13 06:48:34+00:00
- **Updated**: 2020-09-13 06:48:34+00:00
- **Authors**: Disheng Feng, Xuequan Lu, Xufeng Lin
- **Comment**: accepted to the 27th International Conference on Neural Information
  Processing Xuequan Lu is the corresponding author (see www.xuequanlu.com for
  more information)
- **Journal**: None
- **Summary**: It has become increasingly challenging to distinguish real faces from their visually realistic fake counterparts, due to the great advances of deep learning based face manipulation techniques in recent years. In this paper, we introduce a deep learning method to detect face manipulation. It consists of two stages: feature extraction and binary classification. To better distinguish fake faces from real faces, we resort to the triplet loss function in the first stage. We then design a simple linear classification network to bridge the learned contrastive features with the real/fake faces. Experimental results on public benchmark datasets demonstrate the effectiveness of this method, and show that it generates better performance than state-of-the-art techniques in most cases.



### Coding Facial Expressions with Gabor Wavelets (IVC Special Issue)
- **Arxiv ID**: http://arxiv.org/abs/2009.05938v1
- **DOI**: 10.5281/zenodo.4029679
- **Categories**: **cs.CV**, cs.HC, I.0
- **Links**: [PDF](http://arxiv.org/pdf/2009.05938v1)
- **Published**: 2020-09-13 07:01:16+00:00
- **Updated**: 2020-09-13 07:01:16+00:00
- **Authors**: Michael J. Lyons, Miyuki Kamachi, Jiro Gyoba
- **Comment**: 13 pages, 7 figures, 23 references
- **Journal**: None
- **Summary**: We present a method for extracting information about facial expressions from digital images. The method codes facial expression images using a multi-orientation, multi-resolution set of Gabor filters that are topographically ordered and approximately aligned with the face. A similarity space derived from this code is compared with one derived from semantic ratings of the images by human observers. Interestingly the low-dimensional structure of the image-derived similarity space shares organizational features with the circumplex model of affect, suggesting a bridge between categorical and dimensional representations of facial expression. Our results also indicate that it would be possible to construct a facial expression classifier based on a topographically-linked multi-orientation, multi-resolution Gabor coding of the facial images at the input stage. The significant degree of psychological plausibility exhibited by the proposed code may also be useful in the design of human-computer interfaces.



### PolSAR Image Classification Based on Robust Low-Rank Feature Extraction and Markov Random Field
- **Arxiv ID**: http://arxiv.org/abs/2009.05942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05942v1)
- **Published**: 2020-09-13 07:38:12+00:00
- **Updated**: 2020-09-13 07:38:12+00:00
- **Authors**: Haixia Bi, Jing Yao, Zhiqiang Wei, Danfeng Hong, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: Polarimetric synthetic aperture radar (PolSAR) image classification has been investigated vigorously in various remote sensing applications. However, it is still a challenging task nowadays. One significant barrier lies in the speckle effect embedded in the PolSAR imaging process, which greatly degrades the quality of the images and further complicates the classification. To this end, we present a novel PolSAR image classification method, which removes speckle noise via low-rank (LR) feature extraction and enforces smoothness priors via Markov random field (MRF). Specifically, we employ the mixture of Gaussian-based robust LR matrix factorization to simultaneously extract discriminative features and remove complex noises. Then, a classification map is obtained by applying convolutional neural network with data augmentation on the extracted features, where local consistency is implicitly involved, and the insufficient label issue is alleviated. Finally, we refine the classification map by MRF to enforce contextual smoothness. We conduct experiments on two benchmark PolSAR datasets. Experimental results indicate that the proposed method achieves promising classification performance and preferable spatial consistency.



### Synthesizing brain tumor images and annotations by combining progressive growing GAN and SPADE
- **Arxiv ID**: http://arxiv.org/abs/2009.05946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05946v1)
- **Published**: 2020-09-13 07:56:10+00:00
- **Updated**: 2020-09-13 07:56:10+00:00
- **Authors**: Mehdi Foroozandeh, Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: Training segmentation networks requires large annotated datasets, but manual annotation is time consuming and costly. We here investigate if the combination of a noise-to-image GAN and an image-to-image GAN can be used to synthesize realistic brain tumor images as well as the corresponding tumor annotations (labels), to substantially increase the number of training images. The noise-to-image GAN is used to synthesize new label images, while the image-to-image GAN generates the corresponding MR image from the label image. Our results indicate that the two GANs can synthesize label images and MR images that look realistic, and that adding synthetic images improves the segmentation performance, although the effect is small.



### Interpretation of smartphone-captured radiographs utilizing a deep learning-based approach
- **Arxiv ID**: http://arxiv.org/abs/2009.05951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.05951v1)
- **Published**: 2020-09-13 08:26:10+00:00
- **Updated**: 2020-09-13 08:26:10+00:00
- **Authors**: Hieu X. Le, Phuong D. Nguyen, Thang H. Nguyen, Khanh N. Q. Le, Thanh T. Nguyen
- **Comment**: 10 pages, 5 tables, 4 figures
- **Journal**: None
- **Summary**: Recently, computer-aided diagnostic systems (CADs) that could automatically interpret medical images effectively have been the emerging subject of recent academic attention. For radiographs, several deep learning-based systems or models have been developed to study the multi-label diseases recognition tasks. However, none of them have been trained to work on smartphone-captured chest radiographs. In this study, we proposed a system that comprises a sequence of deep learning-based neural networks trained on the newly released CheXphoto dataset to tackle this issue. The proposed approach achieved promising results of 0.684 in AUC and 0.699 in average F1 score. To the best of our knowledge, this is the first published study that showed to be capable of processing smartphone-captured radiographs.



### Semi-supervised dictionary learning with graph regularization and active points
- **Arxiv ID**: http://arxiv.org/abs/2009.05964v1
- **DOI**: 10.1137/19M1285469
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05964v1)
- **Published**: 2020-09-13 09:24:51+00:00
- **Updated**: 2020-09-13 09:24:51+00:00
- **Authors**: Khanh-Hung Tran, Fred-Maurice Ngole-Mboula, Jean-Luc Starck, Vincent Prost
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences, 2020, Vol. 13, No. 2 : pp.
  724-745
- **Summary**: Supervised Dictionary Learning has gained much interest in the recent decade and has shown significant performance improvements in image classification. However, in general, supervised learning needs a large number of labelled samples per class to achieve an acceptable result. In order to deal with databases which have just a few labelled samples per class, semi-supervised learning, which also exploits unlabelled samples in training phase is used. Indeed, unlabelled samples can help to regularize the learning model, yielding an improvement of classification accuracy. In this paper, we propose a new semi-supervised dictionary learning method based on two pillars: on one hand, we enforce manifold structure preservation from the original data into sparse code space using Locally Linear Embedding, which can be considered a regularization of sparse code; on the other hand, we train a semi-supervised classifier in sparse code space. We show that our approach provides an improvement over state-of-the-art semi-supervised dictionary learning methods.



### SSKD: Self-Supervised Knowledge Distillation for Cross Domain Adaptive Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.05972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05972v1)
- **Published**: 2020-09-13 10:12:02+00:00
- **Updated**: 2020-09-13 10:12:02+00:00
- **Authors**: Junhui Yin, Jiayan Qiu, Siqing Zhang, Zhanyu Ma, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptive person re-identification (re-ID) is a challenging task due to the large discrepancy between the source domain and the target domain. To reduce the domain discrepancy, existing methods mainly attempt to generate pseudo labels for unlabeled target images by clustering algorithms. However, clustering methods tend to bring noisy labels and the rich fine-grained details in unlabeled images are not sufficiently exploited. In this paper, we seek to improve the quality of labels by capturing feature representation from multiple augmented views of unlabeled images. To this end, we propose a Self-Supervised Knowledge Distillation (SSKD) technique containing two modules, the identity learning and the soft label learning. Identity learning explores the relationship between unlabeled samples and predicts their one-hot labels by clustering to give exact information for confidently distinguished images. Soft label learning regards labels as a distribution and induces an image to be associated with several related classes for training peer network in a self-supervised manner, where the slowly evolving network is a core to obtain soft labels as a gentle constraint for reliable images. Finally, the two modules can resist label noise for re-ID by enhancing each other and systematically integrating label information from unlabeled images. Extensive experiments on several adaptation tasks demonstrate that the proposed method outperforms the current state-of-the-art approaches by large margins.



### Improving Deep Video Compression by Resolution-adaptive Flow Coding
- **Arxiv ID**: http://arxiv.org/abs/2009.05982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05982v1)
- **Published**: 2020-09-13 12:10:34+00:00
- **Updated**: 2020-09-13 12:10:34+00:00
- **Authors**: Zhihao Hu, Zhenghao Chen, Dong Xu, Guo Lu, Wanli Ouyang, Shuhang Gu
- **Comment**: ECCV 2020(oral)
- **Journal**: None
- **Summary**: In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.



### Calibration Venus: An Interactive Camera Calibration Method Based on Search Algorithm and Pose Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2009.05983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05983v1)
- **Published**: 2020-09-13 12:12:10+00:00
- **Updated**: 2020-09-13 12:12:10+00:00
- **Authors**: Wentai Lei, Mengdi Xu, Feifei Hou, Wensi Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In many scenarios where cameras are applied, such as robot positioning and unmanned driving, camera calibration is one of the most important pre-work. The interactive calibration method based on the plane board is becoming popular in camera calibration field due to its repeatability and operation advantages. However, the existing methods select suggestions from a fixed dataset of pre-defined poses based on subjective experience, which leads to a certain degree of one-sidedness. Moreover, they does not give users clear instructions on how to place the board in the specified pose.



### Semantic Segmentation of Surface from Lidar Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2009.05994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05994v1)
- **Published**: 2020-09-13 13:06:26+00:00
- **Updated**: 2020-09-13 13:06:26+00:00
- **Authors**: Aritra Mukherjee, Sourya Dipta Das, Jasorsi Ghosh, Ananda S. Chowdhury, Sanjoy Kumar Saha
- **Comment**: Accepted in Multimedia Tools and Applications
- **Journal**: None
- **Summary**: In the field of SLAM (Simultaneous Localization And Mapping) for robot navigation, mapping the environment is an important task. In this regard the Lidar sensor can produce near accurate 3D map of the environment in the format of point cloud, in real time. Though the data is adequate for extracting information related to SLAM, processing millions of points in the point cloud is computationally quite expensive. The methodology presented proposes a fast algorithm that can be used to extract semantically labelled surface segments from the cloud, in real time, for direct navigational use or higher level contextual scene reconstruction. First, a single scan from a spinning Lidar is used to generate a mesh of subsampled cloud points online. The generated mesh is further used for surface normal computation of those points on the basis of which surface segments are estimated. A novel descriptor to represent the surface segments is proposed and utilized to determine the surface class of the segments (semantic label) with the help of classifier. These semantic surface segments can be further utilized for geometric reconstruction of objects in the scene, or can be used for optimized trajectory planning by a robot. The proposed methodology is compared with number of point cloud segmentation methods and state of the art semantic segmentation methods to emphasize its efficacy in terms of speed and accuracy.



### A Review of Visual Descriptors and Classification Techniques Used in Leaf Species Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.06001v1
- **DOI**: 10.1007/s11831-018-9266-3
- **Categories**: **cs.CV**, cs.AI, cs.MM, I.2.6; I.2.10; I.4.6; I.4.7; I.4.10; I.5; K.3.2
- **Links**: [PDF](http://arxiv.org/pdf/2009.06001v1)
- **Published**: 2020-09-13 14:11:00+00:00
- **Updated**: 2020-09-13 14:11:00+00:00
- **Authors**: K. K. Thyagharajan, I. Kiruba Raji
- **Comment**: 44 pages, 7 figures, "for final published version, see
  https://link.springer.com/article/10.1007/s11831-018-9266-3"
- **Journal**: Sept. 2019
- **Summary**: Plants are fundamentally important to life. Key research areas in plant science include plant species identification, weed classification using hyper spectral images, monitoring plant health and tracing leaf growth, and the semantic interpretation of leaf information. Botanists easily identify plant species by discriminating between the shape of the leaf, tip, base, leaf margin and leaf vein, as well as the texture of the leaf and the arrangement of leaflets of compound leaves. Because of the increasing demand for experts and calls for biodiversity, there is a need for intelligent systems that recognize and characterize leaves so as to scrutinize a particular species, the diseases that affect them, the pattern of leaf growth, and so on. We review several image processing methods in the feature extraction of leaves, given that feature extraction is a crucial technique in computer vision. As computers cannot comprehend images, they are required to be converted into features by individually analysing image shapes, colours, textures and moments. Images that look the same may deviate in terms of geometric and photometric variations. In our study, we also discuss certain machine learning classifiers for an analysis of different species of leaves.



### Extracting Optimal Solution Manifolds using Constrained Neural Optimization
- **Arxiv ID**: http://arxiv.org/abs/2009.06024v4
- **DOI**: None
- **Categories**: **cs.NE**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.06024v4)
- **Published**: 2020-09-13 15:37:44+00:00
- **Updated**: 2021-01-03 18:46:54+00:00
- **Authors**: Gurpreet Singh, Soumyajit Gupta, Matthew Lease
- **Comment**: None
- **Journal**: None
- **Summary**: Constrained Optimization solution algorithms are restricted to point based solutions. In practice, single or multiple objectives must be satisfied, wherein both the objective function and constraints can be non-convex resulting in multiple optimal solutions. Real world scenarios include intersecting surfaces as Implicit Functions, Hyperspectral Unmixing and Pareto Optimal fronts. Local or global convexification is a common workaround when faced with non-convex forms. However, such an approach is often restricted to a strict class of functions, deviation from which results in sub-optimal solution to the original problem. We present neural solutions for extracting optimal sets as approximate manifolds, where unmodified, non-convex objectives and constraints are defined as modeler guided, domain-informed $L_2$ loss function. This promotes interpretability since modelers can confirm the results against known analytical forms in their specific domains. We present synthetic and realistic cases to validate our approach and compare against known solvers for bench-marking in terms of accuracy and computational efficiency.



### Pairwise-GAN: Pose-based View Synthesis through Pair-Wise Training
- **Arxiv ID**: http://arxiv.org/abs/2009.06053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.06053v1)
- **Published**: 2020-09-13 18:02:15+00:00
- **Updated**: 2020-09-13 18:02:15+00:00
- **Authors**: Xuyang Shen, Jo Plested, Yue Yao, Tom Gedeon
- **Comment**: The 27th International Conference on Neural Information
  Processing(ICONIP2020)
- **Journal**: None
- **Summary**: Three-dimensional face reconstruction is one of the popular applications in computer vision. However, even state-of-the-art models still require frontal face as inputs, which restricts its usage scenarios in the wild. A similar dilemma also happens in face recognition. New research designed to recover the frontal face from a single side-pose facial image has emerged. The state-of-the-art in this area is the Face-Transformation generative adversarial network, which is based on the CycleGAN. This inspired our research which explores the performance of two models from pixel transformation in frontal facial synthesis, Pix2Pix and CycleGAN. We conducted the experiments on five different loss functions on Pix2Pix to improve its performance, then followed by proposing a new network Pairwise-GAN in frontal facial synthesis. Pairwise-GAN uses two parallel U-Nets as the generator and PatchGAN as the discriminator. The detailed hyper-parameters are also discussed. Based on the quantitative measurement by face similarity comparison, our results showed that Pix2Pix with L1 loss, gradient difference loss, and identity loss results in 2.72% of improvement at average similarity compared to the default Pix2Pix model. Additionally, the performance of Pairwise-GAN is 5.4% better than the CycleGAN and 9.1% than the Pix2Pix at average similarity.



### Efficient Folded Attention for 3D Medical Image Reconstruction and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.05576v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05576v1)
- **Published**: 2020-09-13 19:18:04+00:00
- **Updated**: 2020-09-13 19:18:04+00:00
- **Authors**: Hang Zhang, Jinwei Zhang, Rongguang Wang, Qihao Zhang, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Recently, 3D medical image reconstruction (MIR) and segmentation (MIS) based on deep neural networks have been developed with promising results, and attention mechanism has been further designed to capture global contextual information for performance enhancement. However, the large size of 3D volume images poses a great computational challenge to traditional attention methods. In this paper, we propose a folded attention (FA) approach to improve the computational efficiency of traditional attention methods on 3D medical images. The main idea is that we apply tensor folding and unfolding operations with four permutations to build four small sub-affinity matrices to approximate the original affinity matrix. Through four consecutive sub-attention modules of FA, each element in the feature tensor can aggregate spatial-channel information from all other elements. Compared to traditional attention methods, with moderate improvement of accuracy, FA can substantially reduce the computational complexity and GPU memory consumption. We demonstrate the superiority of our method on two challenging tasks for 3D MIR and MIS, which are quantitative susceptibility mapping and multiple sclerosis lesion segmentation.



### Deep Transparent Prediction through Latent Representation Analysis
- **Arxiv ID**: http://arxiv.org/abs/2009.07044v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.07044v2)
- **Published**: 2020-09-13 19:21:40+00:00
- **Updated**: 2020-09-20 22:06:43+00:00
- **Authors**: D. Kollias, N. Bouas, Y. Vlaxos, V. Brillakis, M. Seferis, I. Kollia, L. Sukissian, J. Wingate, S. Kollias
- **Comment**: 16 pages, 8 figures, to be published at Foundations of Trustworthy AI
  integrating Learning, Optimisation and Reasoning (TAILOR) Workshop of
  European Conference on Artificial Intelligence (ECAI) 2020. arXiv admin note:
  substantial text overlap with arXiv:1911.10653
- **Journal**: None
- **Summary**: The paper presents a novel deep learning approach, which extracts latent information from trained Deep Neural Networks (DNNs) and derives concise representations that are analyzed in an effective, unified way for prediction purposes. It is well known that DNNs are capable of analyzing complex data; however, they lack transparency in their decision making, in the sense that it is not straightforward to justify their prediction, or to visualize the features on which the decision was based. Moreover, they generally require large amounts of data in order to learn and become able to adapt to different environments. This makes their use difficult in healthcare, where trust and personalization are key issues. Transparency combined with high prediction accuracy are the targeted goals of the proposed approach. It includes both supervised DNN training and unsupervised learning of latent variables extracted from the trained DNNs. Domain Adaptation from multiple sources is also presented as an extension, where the extracted latent variable representations are used to generate predictions in other, non-annotated, environments. Successful application is illustrated through a large experimental study in various fields: prediction of Parkinson's disease from MRI and DaTScans; prediction of COVID-19 and pneumonia from CT scans and X-rays; optical character verification in retail food packaging.



### Cosine meets Softmax: A tough-to-beat baseline for visual grounding
- **Arxiv ID**: http://arxiv.org/abs/2009.06066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2009.06066v1)
- **Published**: 2020-09-13 19:35:43+00:00
- **Updated**: 2020-09-13 19:35:43+00:00
- **Authors**: Nivedita Rufus, Unni Krishnan R Nair, K. Madhava Krishna, Vineet Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a simple baseline for visual grounding for autonomous driving which outperforms the state of the art methods, while retaining minimal design choices. Our framework minimizes the cross-entropy loss over the cosine distance between multiple image ROI features with a text embedding (representing the give sentence/phrase). We use pre-trained networks for obtaining the initial embeddings and learn a transformation layer on top of the text embedding. We perform experiments on the Talk2Car dataset and achieve 68.7% AP50 accuracy, improving upon the previous state of the art by 8.6%. Our investigation suggests reconsideration towards more approaches employing sophisticated attention mechanisms or multi-stage reasoning or complex metric learning loss functions by showing promise in simpler alternatives.



### Towards the Quantification of Safety Risks in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.06114v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.06114v1)
- **Published**: 2020-09-13 23:30:09+00:00
- **Updated**: 2020-09-13 23:30:09+00:00
- **Authors**: Peipei Xu, Wenjie Ruan, Xiaowei Huang
- **Comment**: 19 pages, 10 figures
- **Journal**: None
- **Summary**: Safety concerns on the deep neural networks (DNNs) have been raised when they are applied to critical sectors. In this paper, we define safety risks by requesting the alignment of the network's decision with human perception. To enable a general methodology for quantifying safety risks, we define a generic safety property and instantiate it to express various safety risks. For the quantification of risks, we take the maximum radius of safe norm balls, in which no safety risk exists. The computation of the maximum safe radius is reduced to the computation of their respective Lipschitz metrics - the quantities to be computed. In addition to the known adversarial example, reachability example, and invariant example, in this paper we identify a new class of risk - uncertainty example - on which humans can tell easily but the network is unsure. We develop an algorithm, inspired by derivative-free optimization techniques and accelerated by tensor-based parallelization on GPUs, to support efficient computation of the metrics. We perform evaluations on several benchmark neural networks, including ACSC-Xu, MNIST, CIFAR-10, and ImageNet networks. The experiments show that, our method can achieve competitive performance on safety quantification in terms of the tightness and the efficiency of computation. Importantly, as a generic approach, our method can work with a broad class of safety risks and without restrictions on the structure of neural networks.



### Multi-channel MRI Embedding: An EffectiveStrategy for Enhancement of Human Brain WholeTumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.06115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.06115v1)
- **Published**: 2020-09-13 23:44:16+00:00
- **Updated**: 2020-09-13 23:44:16+00:00
- **Authors**: Apurva Pandya, Catherine Samuel, Nisargkumar Patel, Vaibhavkumar Patel, Thangarajah Akilan
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most important tasks in medical image processing is the brain's whole tumor segmentation. It assists in quicker clinical assessment and early detection of brain tumors, which is crucial for lifesaving treatment procedures of patients. Because, brain tumors often can be malignant or benign, if they are detected at an early stage. A brain tumor is a collection or a mass of abnormal cells in the brain. The human skull encloses the brain very rigidly and any growth inside this restricted place can cause severe health issues. The detection of brain tumors requires careful and intricate analysis for surgical planning and treatment. Most physicians employ Magnetic Resonance Imaging (MRI) to diagnose such tumors. A manual diagnosis of the tumors using MRI is known to be time-consuming; approximately, it takes up to eighteen hours per sample. Thus, the automatic segmentation of tumors has become an optimal solution for this problem. Studies have shown that this technique provides better accuracy and it is faster than manual analysis resulting in patients receiving the treatment at the right time. Our research introduces an efficient strategy called Multi-channel MRI embedding to improve the result of deep learning-based tumor segmentation. The experimental analysis on the Brats-2019 dataset wrt the U-Net encoder-decoder (EnDec) model shows significant improvement. The embedding strategy surmounts the state-of-the-art approaches with an improvement of 2% without any timing overheads.



### Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2009.06116v1
- **DOI**: 10.3390/app11020672
- **Categories**: **cs.CV**, cs.DB, cs.DL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.06116v1)
- **Published**: 2020-09-13 23:52:03+00:00
- **Updated**: 2020-09-13 23:52:03+00:00
- **Authors**: Jannis Born, Nina Wiedemann, Gabriel Br√§ndle, Charlotte Buhre, Bastian Rieck, Karsten Borgwardt
- **Comment**: 8 pages, 4 figures
- **Journal**: Applied Sciences 2021 (special issue on: "Fighting COVID-19:
  Emerging Techniques and Aid Systems for Prevention, Forecasting and
  Diagnosis")
- **Summary**: Controlling the COVID-19 pandemic largely hinges upon the existence of fast, safe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or X-Ray, has many practical advantages and can serve as a globally-applicable first-line examination technique. We provide the largest publicly available lung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three classes (COVID-19, bacterial pneumonia, and healthy controls); curated and approved by medical experts. On this dataset, we perform an in-depth study of the value of deep learning methods for differential diagnosis of COVID-19. We propose a frame-based convolutional neural network that correctly classifies COVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of 0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We further employ class activation maps for the spatio-temporal localization of pulmonary biomarkers, which we subsequently validate for human-in-the-loop scenarios in a blindfolded study with medical experts. Aiming for scalability and robustness, we perform ablation studies comparing mobile-friendly, frame- and video-based architectures and show reliability of the best model by aleatoric and epistemic uncertainty estimates. We hope to pave the road for a community effort toward an accessible, efficient and interpretable screening method and we have started to work on a clinical validation of the proposed method. Data and code are publicly available.




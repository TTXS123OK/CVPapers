# Arxiv Papers in cs.CV on 2020-09-10
### QuantNet: Learning to Quantize by Learning within Fully Differentiable Framework
- **Arxiv ID**: http://arxiv.org/abs/2009.04626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04626v1)
- **Published**: 2020-09-10 01:41:05+00:00
- **Updated**: 2020-09-10 01:41:05+00:00
- **Authors**: Junjie Liu, Dongchao Wen, Deyu Wang, Wei Tao, Tse-Wei Chen, Kinya Osa, Masami Kato
- **Comment**: Accepted for publication in ECCV Workshop 2020
- **Journal**: None
- **Summary**: Despite the achievements of recent binarization methods on reducing the performance degradation of Binary Neural Networks (BNNs), gradient mismatching caused by the Straight-Through-Estimator (STE) still dominates quantized networks. This paper proposes a meta-based quantizer named QuantNet, which utilizes a differentiable sub-network to directly binarize the full-precision weights without resorting to STE and any learnable gradient estimators. Our method not only solves the problem of gradient mismatching, but also reduces the impact of discretization errors, caused by the binarizing operation in the deployment, on performance. Generally, the proposed algorithm is implemented within a fully differentiable framework, and is easily extended to the general network quantization with any bits. The quantitative experiments on CIFAR-100 and ImageNet demonstrate that QuantNet achieves the signifficant improvements comparing with previous binarization methods, and even bridges gaps of accuracies between binarized models and full-precision models.



### Adjusting Bias in Long Range Stereo Matching: A semantics guided approach
- **Arxiv ID**: http://arxiv.org/abs/2009.04629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04629v2)
- **Published**: 2020-09-10 01:47:53+00:00
- **Updated**: 2020-11-10 01:30:54+00:00
- **Authors**: WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, Alireza Bab-Hadiashar, David Suter
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Stereo vision generally involves the computation of pixel correspondences and estimation of disparities between rectified image pairs. In many applications, including simultaneous localization and mapping (SLAM) and 3D object detection, the disparities are primarily needed to calculate depth values and the accuracy of depth estimation is often more compelling than disparity estimation. The accuracy of disparity estimation, however, does not directly translate to the accuracy of depth estimation, especially for faraway objects. In the context of learning-based stereo systems, this is largely due to biases imposed by the choices of the disparity-based loss function and the training data. Consequently, the learning algorithms often produce unreliable depth estimates of foreground objects, particularly at large distances~($>50$m). To resolve this issue, we first analyze the effect of those biases and then propose a pair of novel depth-based loss functions for foreground and background, separately. These loss functions are tunable and can balance the inherent bias of the stereo learning algorithms. The efficacy of our solution is demonstrated by an extensive set of experiments, which are benchmarked against state of the art. We show on KITTI~2015 benchmark that our proposed solution yields substantial improvements in disparity and depth estimation, particularly for objects located at distances beyond 50 meters, outperforming the previous state of the art by $10\%$.



### Assignment Flow for Order-Constrained OCT Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.04632v1
- **DOI**: 10.1007/s11263-021-01520-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04632v1)
- **Published**: 2020-09-10 01:57:53+00:00
- **Updated**: 2020-09-10 01:57:53+00:00
- **Authors**: D. Sitenko, B. Boll, C. Schnörr
- **Comment**: None
- **Journal**: International Journal of Computer Vision, 129(11), 3088-3118,2021
- **Summary**: At the present time Optical Coherence Tomography (OCT) is among the most commonly used non-invasive imaging methods for the acquisition of large volumetric scans of human retinal tissues and vasculature. To resolve decisive information from extracted OCT volumes and to make it applicable for further diagnostic analysis, the exact identification of retinal layer thicknesses serves as an essential task be done for each patient separately. However, the manual examination of multiple OCT scans in a row is a demanding and time consuming task, which results in a lengthy qualification process and is frequently confounded in the presence of tissue-dependent speckle noise. Therefore, the elaboration of automated segmentation models has become an important task in the field of medical image processing. We propose a novel, purely data driven \textit{geometric approach to order-constrained 3D OCT retinal cell layer segmentation} which takes as input data in any metric space and comes along with basic operations that can be effectively computed in parallel. As opposed to many established retina detection methods, our presented formulation avoids the use of any shape prior and accomplishes the natural order of the retina in a purely geometric way. This makes the approach unbiased and hence suited for the detection of local anatomical changes of retinal tissue structure. To demonstrate robustness of the proposed approach, we compare two different choices of features on a data set of manually annotated 3D OCT volumes of healthy human retina. The quality of computed segmentations is compared to the state of the art in terms of mean absolute error and the Dice similarity coefficient. The results indicate a great potential for applying our method to the classification of diseased retina and opens a new research direction regarding the joint segmentation of retinal cell layers and blood vessel structures.



### Multimodal Noisy Segmentation based fragmented burn scars identification in Amazon Rainforest
- **Arxiv ID**: http://arxiv.org/abs/2009.04634v1
- **DOI**: 10.1109/SMC42975.2020.9283432
- **Categories**: **cs.CV**, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04634v1)
- **Published**: 2020-09-10 02:04:50+00:00
- **Updated**: 2020-09-10 02:04:50+00:00
- **Authors**: Satyam Mohla, Sidharth Mohla, Anupam Guha, Biplab Banerjee
- **Comment**: 5 pages, 5 figures. Accepted at IEEE International Conference on
  Systems, Man and Cybernetics 2020. Earlier draft presented at Harvard CRCS AI
  for Social Good Workshop 2020
- **Journal**: 2020 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)
- **Summary**: Detection of burn marks due to wildfires in inaccessible rain forests is important for various disaster management and ecological studies. The fragmented nature of arable landscapes and diverse cropping patterns often thwart the precise mapping of burn scars. Recent advances in remote-sensing and availability of multimodal data offer a viable solution to this mapping problem. However, the task to segment burn marks is difficult because of its indistinguishably with similar looking land patterns, severe fragmented nature of burn marks and partially labelled noisy datasets. In this work we present AmazonNET -- a convolutional based network that allows extracting of burn patters from multimodal remote sensing images. The network consists of UNet: a well-known encoder decoder type of architecture with skip connections commonly used in biomedical segmentation. The proposed framework utilises stacked RGB-NIR channels to segment burn scars from the pastures by training on a new weakly labelled noisy dataset from Amazonia. Our model illustrates superior performance by correctly identifying partially labelled burn scars and rejecting incorrectly labelled samples, demonstrating our approach as one of the first to effectively utilise deep learning based segmentation models in multimodal burn scar identification.



### Enhanced Quadratic Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2009.04642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04642v1)
- **Published**: 2020-09-10 02:31:50+00:00
- **Updated**: 2020-09-10 02:31:50+00:00
- **Authors**: Yihao Liu, Liangbin Xie, Li Siyao, Wenxiu Sun, Yu Qiao, Chao Dong
- **Comment**: Winning solution of AIM2020 VTSR Challenge (in conjunction with ECCV
  2020)
- **Journal**: None
- **Summary**: With the prosperity of digital video industry, video frame interpolation has arisen continuous attention in computer vision community and become a new upsurge in industry. Many learning-based methods have been proposed and achieved progressive results. Among them, a recent algorithm named quadratic video interpolation (QVI) achieves appealing performance. It exploits higher-order motion information (e.g. acceleration) and successfully models the estimation of interpolated flow. However, its produced intermediate frames still contain some unsatisfactory ghosting, artifacts and inaccurate motion, especially when large and complex motion occurs. In this work, we further improve the performance of QVI from three facets and propose an enhanced quadratic video interpolation (EQVI) model. In particular, we adopt a rectified quadratic flow prediction (RQFP) formulation with least squares method to estimate the motion more accurately. Complementary with image pixel-level blending, we introduce a residual contextual synthesis network (RCSN) to employ contextual information in high-dimensional feature space, which could help the model handle more complicated scenes and motion patterns. Moreover, to further boost the performance, we devise a novel multi-scale fusion network (MS-Fusion) which can be regarded as a learnable augmentation process. The proposed EQVI model won the first place in the AIM2020 Video Temporal Super-Resolution Challenge.



### Non-contact Real time Eye Gaze Mapping System Based on Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2009.04645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.04645v1)
- **Published**: 2020-09-10 02:37:37+00:00
- **Updated**: 2020-09-10 02:37:37+00:00
- **Authors**: Hoyeon Ahn
- **Comment**: None
- **Journal**: None
- **Summary**: Human-Computer Interaction(HCI) is a field that studies interactions between human users and computer systems. With the development of HCI, individuals or groups of people can use various digital technologies to achieve the optimal user experience. Human visual attention and visual intelligence are related to cognitive science, psychology, and marketing informatics, and are used in various applications of HCI. Gaze recognition is closely related to the HCI field because it is meaningful in that it can enhance understanding of basic human behavior. We can obtain reliable visual attention by the Gaze Matching method that finds the area the user is staring at. In the previous methods, the user wears a glasses-type device which in the form of glasses equipped with a gaze tracking function and performs gaze tracking within a limited monitor area. Also, the gaze estimation within a limited range is performed while the user's posture is fixed. We overcome the physical limitations of the previous method in this paper and propose a non-contact gaze mapping system applicable in real-world environments. In addition, we introduce the GIST Gaze Mapping (GGM) dataset, a Gaze mapping dataset created to learn and evaluate gaze mapping.



### Towards Fine-grained Large Object Segmentation 1st Place Solution to 3D AI Challenge 2020 -- Instance Segmentation Track
- **Arxiv ID**: http://arxiv.org/abs/2009.04650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04650v1)
- **Published**: 2020-09-10 02:55:27+00:00
- **Updated**: 2020-09-10 02:55:27+00:00
- **Authors**: Zehui Chen, Qiaofei Li, Feng Zhao
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: This technical report introduces our solutions of Team 'FineGrainedSeg' for Instance Segmentation track in 3D AI Challenge 2020. In order to handle extremely large objects in 3D-FUTURE, we adopt PointRend as our basic framework, which outputs more fine-grained masks compared to HTC and SOLOv2. Our final submission is an ensemble of 5 PointRend models, which achieves the 1st place on both validation and test leaderboards. The code is available at https://github.com/zehuichen123/3DFuture_ins_seg.



### Improved Robustness to Open Set Inputs via Tempered Mixup
- **Arxiv ID**: http://arxiv.org/abs/2009.04659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04659v1)
- **Published**: 2020-09-10 04:01:31+00:00
- **Updated**: 2020-09-10 04:01:31+00:00
- **Authors**: Ryne Roady, Tyler L. Hayes, Christopher Kanan
- **Comment**: Proceedings of the ECCV 2020 Workshop on Adversarial Robustness in
  the Real World
- **Journal**: None
- **Summary**: Supervised classification methods often assume that evaluation data is drawn from the same distribution as training data and that all classes are present for training. However, real-world classifiers must handle inputs that are far from the training distribution including samples from unknown classes. Open set robustness refers to the ability to properly label samples from previously unseen categories as novel and avoid high-confidence, incorrect predictions. Existing approaches have focused on either novel inference methods, unique training architectures, or supplementing the training data with additional background samples. Here, we propose a simple regularization technique easily applied to existing convolutional neural network architectures that improves open set robustness without a background dataset. Our method achieves state-of-the-art results on open set classification baselines and easily scales to large-scale open set classification problems.



### CAD-PU: A Curvature-Adaptive Deep Learning Solution for Point Set Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2009.04660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04660v1)
- **Published**: 2020-09-10 04:03:19+00:00
- **Updated**: 2020-09-10 04:03:19+00:00
- **Authors**: Jiehong Lin, Xian Shi, Yuan Gao, Ke Chen, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Point set is arguably the most direct approximation of an object or scene surface, yet its practical acquisition often suffers from the shortcoming of being noisy, sparse, and possibly incomplete, which restricts its use for a high-quality surface recovery. Point set upsampling aims to increase its density and regularity such that a better surface recovery could be achieved. The problem is severely ill-posed and challenging, considering that the upsampling target itself is only an approximation of the underlying surface. Motivated to improve the surface approximation via point set upsampling, we identify the factors that are critical to the objective, by pairing the surface approximation error bounds of the input and output point sets. It suggests that given a fixed budget of points in the upsampling result, more points should be distributed onto the surface regions where local curvatures are relatively high. To implement the motivation, we propose a novel design of Curvature-ADaptive Point set Upsampling network (CAD-PU), the core of which is a module of curvature-adaptive feature expansion. To train CAD-PU, we follow the same motivation and propose geometrically intuitive surrogates that approximate discrete notions of surface curvature for the upsampled point set. We further integrate the proposed surrogates into an adversarial learning based curvature minimization objective, which gives a practically effective learning of CAD-PU. We conduct thorough experiments that show the efficacy of our contributions and the advantages of our method over existing ones. Our implementation codes are publicly available at https://github.com/JiehongLin/CAD-PU.



### Prune Responsibly
- **Arxiv ID**: http://arxiv.org/abs/2009.09936v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09936v1)
- **Published**: 2020-09-10 04:43:11+00:00
- **Updated**: 2020-09-10 04:43:11+00:00
- **Authors**: Michela Paganini
- **Comment**: None
- **Journal**: None
- **Summary**: Irrespective of the specific definition of fairness in a machine learning application, pruning the underlying model affects it. We investigate and document the emergence and exacerbation of undesirable per-class performance imbalances, across tasks and architectures, for almost one million categories considered across over 100K image classification models that undergo a pruning process.We demonstrate the need for transparent reporting, inclusive of bias, fairness, and inclusion metrics, in real-life engineering decision-making around neural network pruning. In response to the calls for quantitative evaluation of AI models to be population-aware, we present neural network pruning as a tangible application domain where the ways in which accuracy-efficiency trade-offs disproportionately affect underrepresented or outlier groups have historically been overlooked. We provide a simple, Pareto-based framework to insert fairness considerations into value-based operating point selection processes, and to re-evaluate pruning technique choices.



### Virtual Image Correlation uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2009.04693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04693v1)
- **Published**: 2020-09-10 07:04:05+00:00
- **Updated**: 2020-09-10 07:04:05+00:00
- **Authors**: M. L. M. François
- **Comment**: None
- **Journal**: None
- **Summary**: The Virtual Image Correlation method applies for the measurement of silhouettes boundaries with sub-pixel precision. It consists in a correlation between the image of interest and a virtual image based on a parametrized curve. Thanks to a new formulation, it is shown that the method is exact in 1D, insensitive to local curvature and to contrast variation, and that the bias induced by luminance variation can be easily corrected. Optimal value of the virtual image width, the sole parameter of the method, and optimal numerical settings are established. An estimator is proposed to assess the relevance of the user-chosen curve to describe the contour with a sub-pixel precision. Analytical formulas are given for the measurement uncertainty in both cases of noiseless and noisy images and their prediction is successfully compared to numerical tests.



### Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/2009.04709v5
- **DOI**: 10.1016/j.patcog.2023.109430
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.04709v5)
- **Published**: 2020-09-10 07:48:42+00:00
- **Updated**: 2023-04-20 02:03:18+00:00
- **Authors**: Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen
- **Comment**: This paper was published in Pattern Recognition
- **Journal**: None
- **Summary**: Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this alignment increases the robustness of models.



### Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.04724v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04724v3)
- **Published**: 2020-09-10 08:38:32+00:00
- **Updated**: 2021-02-03 07:26:49+00:00
- **Authors**: Siteng Huang, Min Zhang, Yachen Kang, Donglin Wang
- **Comment**: An expanded version of the same-name paper accepted by AAAI-2021
- **Journal**: None
- **Summary**: The purpose of few-shot recognition is to recognize novel categories with a limited number of labeled examples in each class. To encourage learning from a supplementary view, recent approaches have introduced auxiliary semantic modalities into effective metric-learning frameworks that aim to learn a feature similarity between training samples (support set) and test samples (query set). However, these approaches only augment the representations of samples with available semantics while ignoring the query set, which loses the potential for the improvement and may lead to a shift between the modalities combination and the pure-visual representation. In this paper, we devise an attributes-guided attention module (AGAM) to utilize human-annotated attributes and learn more discriminative features. This plug-and-play module enables visual contents and corresponding attributes to collectively focus on important channels and regions for the support set. And the feature selection is also achieved for query set with only visual information while the attributes are not available. Therefore, representations from both sets are improved in a fine-grained manner. Moreover, an attention alignment mechanism is proposed to distill knowledge from the guidance of attributes to the pure-visual branch for samples without attributes. Extensive experiments and analysis show that our proposed module can significantly improve simple metric-based approaches to achieve state-of-the-art performance on different datasets and settings.



### 3D Facial Matching by Spiral Convolutional Metric Learning and a Biometric Fusion-Net of Demographic Properties
- **Arxiv ID**: http://arxiv.org/abs/2009.04746v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04746v2)
- **Published**: 2020-09-10 09:31:47+00:00
- **Updated**: 2020-10-16 14:44:31+00:00
- **Authors**: Soha Sadat Mahdi, Nele Nauwelaers, Philip Joris, Giorgos Bouritsas, Shunwang Gong, Sergiy Bokhnyak, Susan Walsh, Mark D. Shriver, Michael Bronstein, Peter Claes, .
- **Comment**: Accepted at International Conference on Pattern Recognition (ICPR
  2020). Mahdi and Nauwelaers contributed equally and are ordered
  alphabetically
- **Journal**: None
- **Summary**: Face recognition is a widely accepted biometric verification tool, as the face contains a lot of information about the identity of a person. In this study, a 2-step neural-based pipeline is presented for matching 3D facial shape to multiple DNA-related properties (sex, age, BMI and genomic background). The first step consists of a triplet loss-based metric learner that compresses facial shape into a lower dimensional embedding while preserving information about the property of interest. Most studies in the field of metric learning have only focused on 2D Euclidean data. In this work, geometric deep learning is employed to learn directly from 3D facial meshes. To this end, spiral convolutions are used along with a novel mesh-sampling scheme that retains uniformly sampled 3D points at different levels of resolution. The second step is a multi-biometric fusion by a fully connected neural network. The network takes an ensemble of embeddings and property labels as input and returns genuine and imposter scores. Since embeddings are accepted as an input, there is no need to train classifiers for the different properties and available data can be used more efficiently. Results obtained by a 10-fold cross-validation for biometric verification show that combining multiple properties leads to stronger biometric systems. Furthermore, the proposed neural-based pipeline outperforms a linear baseline, which consists of principal component analysis, followed by classification with linear support vector machines and a Naive Bayes-based score-fuser.



### Phase Sampling Profilometry
- **Arxiv ID**: http://arxiv.org/abs/2009.05406v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2009.05406v1)
- **Published**: 2020-09-10 09:36:05+00:00
- **Updated**: 2020-09-10 09:36:05+00:00
- **Authors**: Zhenzhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Structured light 3D surface imaging is a school of techniques in which structured light patterns are used for measuring the depth map of the object. Among all the designed structured light patterns, phase pattern has become most popular because of its high resolution and high accuracy. Accordingly, phase measuring profolimetry (PMP) has become the mainstream of structured light technology. In this letter, we introduce the concept of phase sampling profilometry (PSP) that calculates the phase unambiguously in the spatial-frequency domain with only one pattern image. Therefore, PSP is capable of measuring the 3D shapes of the moving objects robustly with single-shot.



### Critical analysis on the reproducibility of visual quality assessment using deep features
- **Arxiv ID**: http://arxiv.org/abs/2009.05369v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05369v3)
- **Published**: 2020-09-10 09:51:18+00:00
- **Updated**: 2021-03-01 10:59:22+00:00
- **Authors**: Franz Götz-Hahn, Vlad Hosu, Dietmar Saupe
- **Comment**: 27 pages, 8 figures, PLOS ONE journal. arXiv admin note: text overlap
  with arXiv:2005.04400
- **Journal**: None
- **Summary**: Data used to train supervised machine learning models are commonly split into independent training, validation, and test sets. This paper illustrates that complex data leakage cases have occurred in the no-reference image and video quality assessment literature. Recently, papers in several journals reported performance results well above the best in the field. However, our analysis shows that information from the test set was inappropriately used in the training process in different ways and that the claimed performance results cannot be achieved. When correcting for the data leakage, the performances of the approaches drop even below the state-of-the-art by a large margin. Additionally, we investigate end-to-end variations to the discussed approaches, which do not improve upon the original.



### Activate or Not: Learning Customized Activation
- **Arxiv ID**: http://arxiv.org/abs/2009.04759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04759v2)
- **Published**: 2020-09-10 09:59:19+00:00
- **Updated**: 2021-04-16 09:56:15+00:00
- **Authors**: Ningning Ma, Xiangyu Zhang, Ming Liu, Jian Sun
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present a simple, effective, and general activation function we term ACON which learns to activate the neurons or not. Interestingly, we find Swish, the recent popular NAS-searched activation, can be interpreted as a smooth approximation to ReLU. Intuitively, in the same way, we approximate the more general Maxout family to our novel ACON family, which remarkably improves the performance and makes Swish a special case of ACON. Next, we present meta-ACON, which explicitly learns to optimize the parameter switching between non-linear (activate) and linear (inactivate) and provides a new design space. By simply changing the activation function, we show its effectiveness on both small models and highly optimized large models (e.g. it improves the ImageNet top-1 accuracy rate by 6.7% and 1.8% on MobileNet-0.25 and ResNet-152, respectively). Moreover, our novel ACON can be naturally transferred to object detection and semantic segmentation, showing that ACON is an effective alternative in a variety of tasks. Code is available at https://github.com/nmaac/acon.



### Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors
- **Arxiv ID**: http://arxiv.org/abs/2009.04776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04776v2)
- **Published**: 2020-09-10 11:18:11+00:00
- **Updated**: 2020-09-13 18:45:15+00:00
- **Authors**: Akhmedkhan Shabanov, Ilya Krotov, Nikolay Chinaev, Vsevolod Poletaev, Sergei Kozlukov, Igor Pasechnik, Bulat Yakupov, Artsiom Sanakoyeu, Vadim Lebedev, Dmitry Ulyanov
- **Comment**: None
- **Journal**: None
- **Summary**: Consumer-level depth cameras and depth sensors embedded in mobile devices enable numerous applications, such as AR games and face identification. However, the quality of the captured depth is sometimes insufficient for 3D reconstruction, tracking and other computer vision tasks. In this paper, we propose a self-supervised depth denoising approach to denoise and refine depth coming from a low quality sensor. We record simultaneous RGB-D sequences with unzynchronized lower- and higher-quality cameras and solve a challenging problem of aligning sequences both temporally and spatially. We then learn a deep neural network to denoise the lower-quality depth using the matched higher-quality data as a source of supervision signal. We experimentally validate our method against state-of-the-art filtering-based and deep denoising techniques and show its application for 3D object reconstruction tasks where our approach leads to more detailed fused surfaces and better tracking.



### Hard Occlusions in Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.04787v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04787v1)
- **Published**: 2020-09-10 11:42:21+00:00
- **Updated**: 2020-09-10 11:42:21+00:00
- **Authors**: Thijs P. Kuipers, Devanshu Arya, Deepak K. Gupta
- **Comment**: Accepted at ECCV 2020 Workshop RLQ-TOD
- **Journal**: None
- **Summary**: Visual object tracking is among the hardest problems in computer vision, as trackers have to deal with many challenging circumstances such as illumination changes, fast motion, occlusion, among others. A tracker is assessed to be good or not based on its performance on the recent tracking datasets, e.g., VOT2019, and LaSOT. We argue that while the recent datasets contain large sets of annotated videos that to some extent provide a large bandwidth for training data, the hard scenarios such as occlusion and in-plane rotation are still underrepresented. For trackers to be brought closer to the real-world scenarios and deployed in safety-critical devices, even the rarest hard scenarios must be properly addressed. In this paper, we particularly focus on hard occlusion cases and benchmark the performance of recent state-of-the-art trackers (SOTA) on them. We created a small-scale dataset containing different categories within hard occlusions, on which the selected trackers are evaluated. Results show that hard occlusions remain a very challenging problem for SOTA trackers. Furthermore, it is observed that tracker performance varies wildly between different categories of hard occlusions, where a top-performing tracker on one category performs significantly worse on a different category. The varying nature of tracker performance based on specific categories suggests that the common tracker rankings using averaged single performance scores are not adequate to gauge tracker performance in real-world scenarios.



### Fully automated analysis of muscle architecture from B-mode ultrasound images with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2009.04790v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04790v1)
- **Published**: 2020-09-10 11:44:00+00:00
- **Updated**: 2020-09-10 11:44:00+00:00
- **Authors**: Neil J. Cronin, Taija Finni, Olivier Seynnes
- **Comment**: None
- **Journal**: None
- **Summary**: B-mode ultrasound is commonly used to image musculoskeletal tissues, but one major bottleneck is data interpretation, and analyses of muscle thickness, pennation angle and fascicle length are often still performed manually. In this study we trained deep neural networks (based on U-net) to detect muscle fascicles and aponeuroses using a set of labelled musculoskeletal ultrasound images. We then compared neural network predictions on new, unseen images to those obtained via manual analysis and two existing semi/automated analysis approaches (SMA and Ultratrack). With a GPU, inference time for a single image with the new approach was around 0.7s, compared to 4.6s with a CPU. Our method detects the locations of the superficial and deep aponeuroses, as well as multiple fascicle fragments per image. For single images, the method gave similar results to those produced by a non-trainable automated method (SMA; mean difference in fascicle length: 1.1 mm) or human manual analysis (mean difference: 2.1 mm). Between-method differences in pennation angle were within 1$^\circ$, and mean differences in muscle thickness were less than 0.2 mm. Similarly, for videos, there was strong overlap between the results produced with Ultratrack and our method, with a mean ICC of 0.73, despite the fact that the analysed trials included hundreds of frames. Our method is fully automated and open source, and can estimate fascicle length, pennation angle and muscle thickness from single images or videos, as well as from multiple superficial muscles. We also provide all necessary code and training data for custom model development.



### MAT: Motion-Aware Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.04794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04794v2)
- **Published**: 2020-09-10 11:51:33+00:00
- **Updated**: 2020-09-18 05:13:06+00:00
- **Authors**: Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu, Xiaofeng Pan, Jun Zhao
- **Comment**: 13 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: Modern multi-object tracking (MOT) systems usually model the trajectories by associating per-frame detections. However, when camera motion, fast motion, and occlusion challenges occur, it is difficult to ensure long-range tracking or even the tracklet purity, especially for small objects. Although re-identification is often employed, due to noisy partial-detections, similar appearance, and lack of temporal-spatial constraints, it is not only unreliable and time-consuming, but still cannot address the false negatives for occluded and blurred objects. In this paper, we propose an enhanced MOT paradigm, namely Motion-Aware Tracker (MAT), focusing more on various motion patterns of different objects. The rigid camera motion and nonrigid pedestrian motion are blended compatibly to form the integrated motion localization module. Meanwhile, we introduce the dynamic reconnection context module, which aims to balance the robustness of long-range motion-based reconnection, and includes the cyclic pseudo-observation updating strategy to smoothly fill in the tracking fragments caused by occlusion or blur. Additionally, the 3D integral image module is presented to efficiently cut useless track-detection association connections with temporal-spatial constraints. Extensive experiments on MOT16 and MOT17 challenging benchmarks demonstrate that our MAT approach can achieve the superior performance by a large margin with high efficiency, in contrast to other state-of-the-art trackers.



### Distributed Variable-Baseline Stereo SLAM from two UAVs
- **Arxiv ID**: http://arxiv.org/abs/2009.04801v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2009.04801v1)
- **Published**: 2020-09-10 12:16:10+00:00
- **Updated**: 2020-09-10 12:16:10+00:00
- **Authors**: Marco Karrer, Margarita Chli
- **Comment**: None
- **Journal**: None
- **Summary**: VIO has been widely used and researched to control and aid the automation of navigation of robots especially in the absence of absolute position measurements, such as GPS. However, when observable landmarks in the scene lie far away from the robot's sensor suite, as it is the case at high altitude flights, the fidelity of estimates and the observability of the metric scale degrades greatly for these methods. Aiming to tackle this issue, in this article, we employ two UAVs equipped with one monocular camera and one IMU each, to exploit their view overlap and relative distance measurements between them using UWB modules onboard to enable collaborative VIO. In particular, we propose a novel, distributed fusion scheme enabling the formation of a virtual stereo camera rig with adjustable baseline from the two UAVs. In order to control the \gls{uav} agents autonomously, we propose a decentralized collaborative estimation scheme, where each agent hold its own local map, achieving an average pose estimation latency of 11ms, while ensuring consistency of the agents' estimates via consensus based optimization. Following a thorough evaluation on photorealistic simulations, we demonstrate the effectiveness of the approach at high altitude flights of up to 160m, going significantly beyond the capabilities of state-of-the-art VIO methods. Finally, we show the advantage of actively adjusting the baseline on-the-fly over a fixed, target baseline, reducing the error in our experiments by a factor of two.



### Globally-scalable Automated Target Recognition (GATR)
- **Arxiv ID**: http://arxiv.org/abs/2009.04836v1
- **DOI**: 10.1109/AIPR47015.2019.9174585
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04836v1)
- **Published**: 2020-09-10 13:20:50+00:00
- **Updated**: 2020-09-10 13:20:50+00:00
- **Authors**: Gary Chern, Austen Groener, Michael Harner, Tyler Kuhns, Andy Lam, Stephen O'Neill, Mark Pritt
- **Comment**: 7 pages, 18 figures, 2019 IEEE Applied Imagery Pattern Recognition
  Workshop (AIPR)
- **Journal**: 2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),
  Washington, DC, USA, 2019, pp. 1-7
- **Summary**: GATR (Globally-scalable Automated Target Recognition) is a Lockheed Martin software system for real-time object detection and classification in satellite imagery on a worldwide basis. GATR uses GPU-accelerated deep learning software to quickly search large geographic regions. On a single GPU it processes imagery at a rate of over 16 square km/sec (or more than 10 Mpixels/sec), and it requires only two hours to search the entire state of Pennsylvania for gas fracking wells. The search time scales linearly with the geographic area, and the processing rate scales linearly with the number of GPUs. GATR has a modular, cloud-based architecture that uses the Maxar GBDX platform and provides an ATR analytic as a service. Applications include broad area search, watch boxes for monitoring ports and airfields, and site characterization. ATR is performed by deep learning models including RetinaNet and Faster R-CNN. Results are presented for the detection of aircraft and fracking wells and show that the recalls exceed 90% even in geographic regions never seen before. GATR is extensible to new targets, such as cars and ships, and it also handles radar and infrared imagery.



### A Comparison of Deep Learning Object Detection Models for Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2009.04857v1
- **DOI**: 10.1109/AIPR47015.2019.9174593
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04857v1)
- **Published**: 2020-09-10 13:43:14+00:00
- **Updated**: 2020-09-10 13:43:14+00:00
- **Authors**: Austen Groener, Gary Chern, Mark Pritt
- **Comment**: 10 pages, 9 figures, 3 tables. 2019 IEEE Applied Imagery Pattern
  Recognition Workshop (AIPR)
- **Journal**: 2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),
  Washington, DC, USA, 2019, pp. 1-10
- **Summary**: In this work, we compare the detection accuracy and speed of several state-of-the-art models for the task of detecting oil and gas fracking wells and small cars in commercial electro-optical satellite imagery. Several models are studied from the single-stage, two-stage, and multi-stage object detection families of techniques. For the detection of fracking well pads (50m - 250m), we find single-stage detectors provide superior prediction speed while also matching detection performance of their two and multi-stage counterparts. However, for detecting small cars, two-stage and multi-stage models provide substantially higher accuracies at the cost of some speed. We also measure timing results of the sliding window object detection algorithm to provide a baseline for comparison. Some of these models have been incorporated into the Lockheed Martin Globally-Scalable Automated Target Recognition (GATR) framework.



### Detecting the Presence of Vehicles and Equipment in SAR Imagery Using Image Texture Features
- **Arxiv ID**: http://arxiv.org/abs/2009.04866v1
- **DOI**: 10.1109/AIPR47015.2019.9174598
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04866v1)
- **Published**: 2020-09-10 13:59:52+00:00
- **Updated**: 2020-09-10 13:59:52+00:00
- **Authors**: Michael Harner, Austen Groener, Mark Pritt
- **Comment**: 6 pages, 6 figures, 2019 IEEE Applied Imagery Pattern Recognition
  Workshop (AIPR)
- **Journal**: 2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),
  Washington, DC, USA, 2019, pp. 1-6
- **Summary**: In this work, we present a methodology for monitoring man-made, construction-like activities in low-resolution SAR imagery. Our source of data is the European Space Agency Sentinel-l satellite which provides global coverage at a 12-day revisit rate. Despite limitations in resolution, our methodology enables us to monitor activity levels (i.e. presence of vehicles, equipment) of a pre-defined location by analyzing the texture of detected SAR imagery. Using an exploratory dataset, we trained a support vector machine (SVM), a random binary forest, and a fully-connected neural network for classification. We use Haralick texture features in the VV and VH polarization channels as the input features to our classifiers. Each classifier showed promising results in being able to distinguish between two possible types of construction-site activity levels. This paper documents a case study that is centered around monitoring the construction process for oil and gas fracking wells.



### Text-independent writer identification using convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2009.04877v1
- **DOI**: 10.1016/j.patrec.2018.07.022
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.04877v1)
- **Published**: 2020-09-10 14:18:03+00:00
- **Updated**: 2020-09-10 14:18:03+00:00
- **Authors**: Hung Tuan Nguyen, Cuong Tuan Nguyen, Takeya Ino, Bipin Indurkhya, Masaki Nakagawa
- **Comment**: 12 pages
- **Journal**: Pattern Recognition Letters, Volume 121, 2019, Pages 104-112
- **Summary**: The text-independent approach to writer identification does not require the writer to write some predetermined text. Previous research on text-independent writer identification has been based on identifying writer-specific features designed by experts. However, in the last decade, deep learning methods have been successfully applied to learn features from data automatically. We propose here an end-to-end deep-learning method for text-independent writer identification that does not require prior identification of features. A Convolutional Neural Network (CNN) is trained initially to extract local features, which represent characteristics of individual handwriting in the whole character images and their sub-regions. Randomly sampled tuples of images from the training set are used to train the CNN and aggregate the extracted local features of images from the tuples to form global features. For every training epoch, the process of randomly sampling tuples is repeated, which is equivalent to a large number of training patterns being prepared for training the CNN for text-independent writer identification. We conducted experiments on the JEITA-HP database of offline handwritten Japanese character patterns. With 200 characters, our method achieved an accuracy of 99.97% to classify 100 writers. Even when using 50 characters for 100 writers or 100 characters for 400 writers, our method achieved accuracy levels of 92.80% or 93.82%, respectively. We conducted further experiments on the Firemaker and IAM databases of offline handwritten English text. Using only one page per writer to train, our method achieved over 91.81% accuracy to classify 900 writers. Overall, we achieved a better performance than the previously published best result based on handcrafted features and clustering algorithms, which demonstrates the effectiveness of our method for handwritten English text also.



### A leak in PRNU based source identification. Questioning fingerprint uniqueness
- **Arxiv ID**: http://arxiv.org/abs/2009.04878v2
- **DOI**: 10.1109/ACCESS.2021.3070478
- **Categories**: **eess.IV**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.04878v2)
- **Published**: 2020-09-10 14:18:38+00:00
- **Updated**: 2021-04-12 12:51:11+00:00
- **Authors**: Massimo Iuliani, Marco Fontani, Alessandro Piva
- **Comment**: Final paper in : https://ieeexplore.ieee.org/document/9393356
- **Journal**: IEEE Access, 2021
- **Summary**: Photo Response Non-Uniformity (PRNU) is considered the most effective trace for the image source attribution task. Its uniqueness ensures that the sensor pattern noises extracted from different cameras are strongly uncorrelated, even when they belong to the same camera model. However, with the advent of computational photography, most recent devices heavily process the acquired pixels, possibly introducing non-unique artifacts that may reduce PRNU noise's distinctiveness, especially when several exemplars of the same device model are involved in the analysis. Considering that PRNU is an image forensic technology that finds actual and wide use by law enforcement agencies worldwide, it is essential to keep validating such technology on recent devices as they appear. In this paper, we perform an extensive testing campaign on over 33.000 Flickr images belonging to 45 smartphone and 25 DSLR camera models released recently to determine how widespread the issue is and which is the plausible cause. Experiments highlight that most brands, like Samsung, Huawei, Canon, Nikon, Fujifilm, Sigma, and Leica, are strongly affected by this issue. We show that the primary cause of high false alarm rates cannot be directly related to specific camera models, firmware, nor image contents. It is evident that the effectiveness of \prnu based source identification on the most recent devices must be reconsidered in light of these results. Therefore, this paper is intended as a call to action for the scientific community rather than a complete treatment of the subject. Moreover, we believe publishing these data is important to raise awareness about a possible issue with PRNU reliability in the law enforcement world.



### MedMeshCNN -- Enabling MeshCNN for Medical Surface Models
- **Arxiv ID**: http://arxiv.org/abs/2009.04893v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.8, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.04893v1)
- **Published**: 2020-09-10 14:40:28+00:00
- **Updated**: 2020-09-10 14:40:28+00:00
- **Authors**: Lisa Schneider, Annika Niemann, Oliver Beuing, Bernhard Preim, Sylvia Saalfeld
- **Comment**: 7 pages, 7 figures, 1 table, Submitted to Computer Methods and
  Programs in Biomedicine
- **Journal**: None
- **Summary**: Background and objective: MeshCNN is a recently proposed Deep Learning framework that drew attention due to its direct operation on irregular, non-uniform 3D meshes. On selected benchmarking datasets, it outperformed state-of-the-art methods within classification and segmentation tasks. Especially, the medical domain provides a large amount of complex 3D surface models that may benefit from processing with MeshCNN. However, several limitations prevent outstanding performances of MeshCNN on highly diverse medical surface models. Within this work, we propose MedMeshCNN as an expansion for complex, diverse, and fine-grained medical data. Methods: MedMeshCNN follows the functionality of MeshCNN with a significantly increased memory efficiency that allows retaining patient-specific properties during the segmentation process. Furthermore, it enables the segmentation of pathological structures that often come with highly imbalanced class distributions. Results: We tested the performance of MedMeshCNN on a complex part segmentation task of intracranial aneurysms and their surrounding vessel structures and reached a mean Intersection over Union of 63.24\%. The pathological aneurysm is segmented with an Intersection over Union of 71.4\%. Conclusions: These results demonstrate that MedMeshCNN enables the application of MeshCNN on complex, fine-grained medical surface meshes. The imbalanced class distribution deriving from the pathological finding is considered by MedMeshCNN and patient-specific properties are mostly retained during the segmentation process.



### Ultrasound Liver Fibrosis Diagnosis using Multi-indicator guided Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.04924v1
- **DOI**: 10.1007/978-3-030-32692-0_27
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04924v1)
- **Published**: 2020-09-10 15:05:45+00:00
- **Updated**: 2020-09-10 15:05:45+00:00
- **Authors**: Jiali Liu, Wenxuan Wang, Tianyao Guan, Ningbo Zhao, Xiaoguang Han, Zhen Li
- **Comment**: Jiali Liu and Wenxuan Wang are equal contribution
- **Journal**: Machine Learning in Medical Imaging 2019
- **Summary**: Accurate analysis of the fibrosis stage plays very important roles in follow-up of patients with chronic hepatitis B infection. In this paper, a deep learning framework is presented for automatically liver fibrosis prediction. On contrary of previous works, our approach can take use of the information provided by multiple ultrasound images. An indicator-guided learning mechanism is further proposed to ease the training of the proposed model. This follows the workflow of clinical diagnosis and make the prediction procedure interpretable. To support the training, a dataset is well-collected which contains the ultrasound videos/images, indicators and labels of 229 patients. As demonstrated in the experimental results, our proposed model shows its effectiveness by achieving the state-of-the-art performance, specifically, the accuracy is 65.6%(20% higher than previous best).



### Orientation Keypoints for 6D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.04930v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04930v2)
- **Published**: 2020-09-10 15:15:12+00:00
- **Updated**: 2021-12-16 17:28:44+00:00
- **Authors**: Martin Fisch, Ronald Clark
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). Video: https://youtu.be/1EBUrfu_CaE
- **Journal**: None
- **Summary**: Most realtime human pose estimation approaches are based on detecting joint positions. Using the detected joint positions, the yaw and pitch of the limbs can be computed. However, the roll along the limb, which is critical for application such as sports analysis and computer animation, cannot be computed as this axis of rotation remains unobserved. In this paper we therefore introduce orientation keypoints, a novel approach for estimating the full position and rotation of skeletal joints, using only single-frame RGB images. Inspired by how motion-capture systems use a set of point markers to estimate full bone rotations, our method uses virtual markers to generate sufficient information to accurately infer rotations with simple post processing. The rotation predictions improve upon the best reported mean error for joint angles by 48% and achieves 93% accuracy across 15 bone rotations. The method also improves the current state-of-the-art results for joint positions by 14% as measured by MPJPE on the principle dataset, and generalizes well to in-the-wild datasets.



### Performance of object recognition in wearable videos
- **Arxiv ID**: http://arxiv.org/abs/2009.04932v1
- **DOI**: 10.1109/ETFA.2019.8869019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04932v1)
- **Published**: 2020-09-10 15:20:17+00:00
- **Updated**: 2020-09-10 15:20:17+00:00
- **Authors**: Alberto Sabater, Luis Montesano, Ana C. Murillo
- **Comment**: Emerging Technologies and Factory Automation, ETFA, 2019
- **Journal**: None
- **Summary**: Wearable technologies are enabling plenty of new applications of computer vision, from life logging to health assistance. Many of them are required to recognize the elements of interest in the scene captured by the camera. This work studies the problem of object detection and localization on videos captured by this type of camera. Wearable videos are a much more challenging scenario for object detection than standard images or even another type of videos, due to lower quality images (e.g. poor focus) or high clutter and occlusion common in wearable recordings. Existing work typically focuses on detecting the objects of focus or those being manipulated by the user wearing the camera. We perform a more general evaluation of the task of object detection in this type of video, because numerous applications, such as marketing studies, also need detecting objects which are not in focus by the user. This work presents a thorough study of the well known YOLO architecture, that offers an excellent trade-off between accuracy and speed, for the particular case of object detection in wearable video. We focus our study on the public ADL Dataset, but we also use additional public data for complementary evaluations. We run an exhaustive set of experiments with different variations of the original architecture and its training strategy. Our experiments drive to several conclusions about the most promising directions for our goal and point us to further research steps to improve detection in wearable videos.



### Dual Encoding for Video Retrieval by Text
- **Arxiv ID**: http://arxiv.org/abs/2009.05381v2
- **DOI**: 10.1109/TPAMI.2021.3059295
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.05381v2)
- **Published**: 2020-09-10 15:49:39+00:00
- **Updated**: 2021-02-18 09:26:20+00:00
- **Authors**: Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang, Meng Wang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence. Code and data will be available at
  https://github.com/danieljf24/hybrid_space. Conference version:
  arXiv:1809.06181
- **Journal**: None
- **Summary**: This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method.



### Prototype Completion with Primitive Knowledge for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.04960v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04960v6)
- **Published**: 2020-09-10 16:09:34+00:00
- **Updated**: 2021-06-24 03:41:34+00:00
- **Authors**: Baoquan Zhang, Xutao Li, Yunming Ye, Zhichao Huang, Lisai Zhang
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Few-shot learning is a challenging task, which aims to learn a classifier for novel classes with few examples. Pre-training based meta-learning methods effectively tackle the problem by pre-training a feature extractor and then fine-tuning it through the nearest centroid based meta-learning. However, results show that the fine-tuning step makes very marginal improvements. In this paper, 1) we figure out the key reason, i.e., in the pre-trained feature space, the base classes already form compact clusters while novel classes spread as groups with large variances, which implies that fine-tuning the feature extractor is less meaningful; 2) instead of fine-tuning the feature extractor, we focus on estimating more representative prototypes during meta-learning. Consequently, we propose a novel prototype completion based meta-learning framework. This framework first introduces primitive knowledge (i.e., class-level part or attribute annotations) and extracts representative attribute features as priors. Then, we design a prototype completion network to learn to complete prototypes with these priors. To avoid the prototype completion error caused by primitive knowledge noises or class differences, we further develop a Gaussian based prototype fusion strategy that combines the mean-based and completed prototypes by exploiting the unlabeled samples. Extensive experiments show that our method: (i) can obtain more accurate prototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of classification accuracy. Our code is available online.



### Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations
- **Arxiv ID**: http://arxiv.org/abs/2009.04965v3
- **DOI**: 10.1109/ACCESS.2021.3069041
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.04965v3)
- **Published**: 2020-09-10 16:15:09+00:00
- **Updated**: 2021-04-05 07:48:10+00:00
- **Authors**: Meng-Jiun Chiou, Roger Zimmermann, Jiashi Feng
- **Comment**: Published in IEEE Access
- **Journal**: IEEE Access, 2021
- **Summary**: Visual relationship detection aims to reason over relationships among salient objects in images, which has drawn increasing attention over the past few years. Inspired by human reasoning mechanisms, it is believed that external visual commonsense knowledge is beneficial for reasoning visual relationships of objects in images, which is however rarely considered in existing methods. In this paper, we propose a novel approach named Relational Visual-Linguistic Bidirectional Encoder Representations from Transformers (RVL-BERT), which performs relational reasoning with both visual and language commonsense knowledge learned via self-supervised pre-training with multimodal representations. RVL-BERT also uses an effective spatial module and a novel mask attention module to explicitly capture spatial information among the objects. Moreover, our model decouples object detection from visual relationship recognition by taking in object names directly, enabling it to be used on top of any object detection system. We show through quantitative and qualitative experiments that, with the transferred knowledge and novel modules, RVL-BERT achieves competitive results on two challenging visual relationship detection datasets. The source code is available at https://github.com/coldmanck/RVL-BERT.



### Unsupervised Domain Adaptation via CycleGAN for White Matter Hyperintensity Segmentation in Multicenter MR Images
- **Arxiv ID**: http://arxiv.org/abs/2009.04985v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.04985v1)
- **Published**: 2020-09-10 16:48:19+00:00
- **Updated**: 2020-09-10 16:48:19+00:00
- **Authors**: Julian Alberto Palladino, Diego Fernandez Slezak, Enzo Ferrante
- **Comment**: Accepted for publication in the International Seminar on Medical
  Information Processing and Analysis (SIPAIM 2020)
- **Journal**: None
- **Summary**: Automatic segmentation of white matter hyperintensities in magnetic resonance images is of paramount clinical and research importance. Quantification of these lesions serve as a predictor for risk of stroke, dementia and mortality. During the last years, convolutional neural networks (CNN) specifically tailored for biomedical image segmentation have outperformed all previous techniques in this task. However, they are extremely data-dependent, and maintain a good performance only when data distribution between training and test datasets remains unchanged. When such distribution changes but we still aim at performing the same task, we incur in a domain adaptation problem (e.g. using a different MR machine or different acquisition parameters for training and test data). In this work, we explore the use of cycle-consistent adversarial networks (CycleGAN) to perform unsupervised domain adaptation on multicenter MR images with brain lesions. We aim at learning a mapping function to transform volumetric MR images between domains, which are characterized by different medical centers and MR machines with varying brand, model and configuration parameters. Our experiments show that CycleGAN allows us to reduce the Jensen-Shannon divergence between MR domains, enabling automatic segmentation with CNN models on domains where no labeled data was available.



### Semi-Anchored Detector for One-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.04989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04989v1)
- **Published**: 2020-09-10 16:57:09+00:00
- **Updated**: 2020-09-10 16:57:09+00:00
- **Authors**: Lei Chen, Qi Qian, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: A standard one-stage detector is comprised of two tasks: classification and regression. Anchors of different shapes are introduced for each location in the feature map to mitigate the challenge of regression for multi-scale objects. However, the performance of classification can degrade due to the highly class-imbalanced problem in anchors. Recently, many anchor-free algorithms have been proposed to classify locations directly. The anchor-free strategy benefits the classification task but can lead to sup-optimum for the regression task due to the lack of prior bounding boxes. In this work, we propose a semi-anchored framework. Concretely, we identify positive locations in classification, and associate multiple anchors to the positive locations in regression. With ResNet-101 as the backbone, the proposed semi-anchored detector achieves 43.6% mAP on COCO data set, which demonstrates the state-of-art performance among one-stage detectors.



### Proposal-Free Volumetric Instance Segmentation from Latent Single-Instance Masks
- **Arxiv ID**: http://arxiv.org/abs/2009.04998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04998v1)
- **Published**: 2020-09-10 17:09:23+00:00
- **Updated**: 2020-09-10 17:09:23+00:00
- **Authors**: Alberto Bailoni, Constantin Pape, Steffen Wolf, Anna Kreshuk, Fred A. Hamprecht
- **Comment**: Presented at GCPR 2020
- **Journal**: None
- **Summary**: This work introduces a new proposal-free instance segmentation method that builds on single-instance segmentation masks predicted across the entire image in a sliding window style. In contrast to related approaches, our method concurrently predicts all masks, one for each pixel, and thus resolves any conflict jointly across the entire image. Specifically, predictions from overlapping masks are combined into edge weights of a signed graph that is subsequently partitioned to obtain all final instances concurrently. The result is a parameter-free method that is strongly robust to noise and prioritizes predictions with the highest consensus across overlapping masks. All masks are decoded from a low dimensional latent representation, which results in great memory savings strictly required for applications to large volumetric images. We test our method on the challenging CREMI 2016 neuron segmentation benchmark where it achieves competitive scores.



### HSolo: Homography from a single affine aware correspondence
- **Arxiv ID**: http://arxiv.org/abs/2009.05004v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05004v1)
- **Published**: 2020-09-10 17:13:23+00:00
- **Updated**: 2020-09-10 17:13:23+00:00
- **Authors**: Antonio Gonzales, Cara Monical, Tony Perkins
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of existing robust homography estimation algorithms is highly dependent on the inlier rate of feature point correspondences. In this paper, we present a novel procedure for homography estimation that is particularly well suited for inlier-poor domains. By utilizing the scale and rotation byproducts created by affine aware feature detectors such as SIFT and SURF, we obtain an initial homography estimate from a single correspondence pair. This estimate allows us to filter the correspondences to an inlier-rich subset for use with a robust estimator. Especially at low inlier rates, our novel algorithm provides dramatic performance improvements.



### OrthoReg: Robust Network Pruning Using Orthonormality Regularization
- **Arxiv ID**: http://arxiv.org/abs/2009.05014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05014v1)
- **Published**: 2020-09-10 17:21:21+00:00
- **Updated**: 2020-09-10 17:21:21+00:00
- **Authors**: Ekdeep Singh Lubana, Puja Trivedi, Conrad Hougen, Robert P. Dick, Alfred O. Hero
- **Comment**: None
- **Journal**: None
- **Summary**: Network pruning in Convolutional Neural Networks (CNNs) has been extensively investigated in recent years. To determine the impact of pruning a group of filters on a network's accuracy, state-of-the-art pruning methods consistently assume filters of a CNN are independent. This allows the importance of a group of filters to be estimated as the sum of importances of individual filters. However, overparameterization in modern networks results in highly correlated filters that invalidate this assumption, thereby resulting in incorrect importance estimates. To address this issue, we propose OrthoReg, a principled regularization strategy that enforces orthonormality on a network's filters to reduce inter-filter correlation, thereby allowing reliable, efficient determination of group importance estimates, improved trainability of pruned networks, and efficient, simultaneous pruning of large groups of filters. When used for iterative pruning on VGG-13, MobileNet-V1, and ResNet-34, OrthoReg consistently outperforms five baseline techniques, including the state-of-the-art, on CIFAR-100 and Tiny-ImageNet. For the recently proposed Early-Bird Ticket hypothesis, which claims networks become amenable to pruning early-on in training and can be pruned after a few epochs to minimize training expenditure, we find OrthoReg significantly outperforms prior work. Code available at https://github.com/EkdeepSLubana/OrthoReg.



### Learning Shape Features and Abstractions in 3D Convolutional Neural Networks for Detecting Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2009.05023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05023v1)
- **Published**: 2020-09-10 17:41:03+00:00
- **Updated**: 2020-09-10 17:41:03+00:00
- **Authors**: Md Motiur Rahman Sagar, Martin Dyrba
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks - especially Convolutional Neural Network (ConvNet) has become the state-of-the-art for image classification, pattern recognition and various computer vision tasks. ConvNet has a huge potential in medical domain for analyzing medical data to diagnose diseases in an efficient way. Based on extracted features by ConvNet model from MRI data, early diagnosis is very crucial for preventing progress and treating the Alzheimer's disease. Despite having the ability to deliver great performance, absence of interpretability of the model's decision can lead to misdiagnosis which can be life threatening. In this thesis, learned shape features and abstractions by 3D ConvNets for detecting Alzheimer's disease were investigated using various visualization techniques. How changes in network structures, used filters sizes and filters shapes affects the overall performance and learned features of the model were also inspected. LRP relevance map of different models revealed which parts of the brain were more relevant for the classification decision. Comparing the learned filters by Activation Maximization showed how patterns were encoded in different layers of the network. Finally, transfer learning from a convolutional autoencoder was implemented to check whether increasing the number of training samples with patches of input to extract the low-level features improves learned features and the model performance.



### Understanding the Role of Individual Units in a Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2009.05041v2
- **DOI**: 10.1073/pnas.1907375117
- **Categories**: **cs.CV**, cs.LG, cs.NE, 68T07, I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2009.05041v2)
- **Published**: 2020-09-10 17:59:10+00:00
- **Updated**: 2020-09-12 18:58:32+00:00
- **Authors**: David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba
- **Comment**: Proceedings of the National Academy of Sciences 2020. Code at
  https://github.com/davidbau/dissect/ and website at
  https://dissect.csail.mit.edu/
- **Journal**: None
- **Summary**: Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.



### COVID CT-Net: Predicting Covid-19 From Chest CT Images Using Attentional Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2009.05096v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05096v1)
- **Published**: 2020-09-10 19:00:51+00:00
- **Updated**: 2020-09-10 19:00:51+00:00
- **Authors**: Shakib Yazdani, Shervin Minaee, Rahele Kafieh, Narges Saeedizadeh, Milan Sonka
- **Comment**: None
- **Journal**: None
- **Summary**: The novel corona-virus disease (COVID-19) pandemic has caused a major outbreak in more than 200 countries around the world, leading to a severe impact on the health and life of many people globally. As of Aug 25th of 2020, more than 20 million people are infected, and more than 800,000 death are reported. Computed Tomography (CT) images can be used as a as an alternative to the time-consuming "reverse transcription polymerase chain reaction (RT-PCR)" test, to detect COVID-19. In this work we developed a deep learning framework to predict COVID-19 from CT images. We propose to use an attentional convolution network, which can focus on the infected areas of chest, enabling it to perform a more accurate prediction. We trained our model on a dataset of more than 2000 CT images, and report its performance in terms of various popular metrics, such as sensitivity, specificity, area under the curve, and also precision-recall curve, and achieve very promising results. We also provide a visualization of the attention maps of the model for several test images, and show that our model is attending to the infected regions as intended. In addition to developing a machine learning modeling framework, we also provide the manual annotation of the potentionally infected regions of chest, with the help of a board-certified radiologist, and make that publicly available for other researchers.



### Multi-Task Learning with Deep Neural Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2009.09796v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09796v1)
- **Published**: 2020-09-10 19:31:04+00:00
- **Updated**: 2020-09-10 19:31:04+00:00
- **Authors**: Michael Crawshaw
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning (MTL) is a subfield of machine learning in which multiple tasks are simultaneously learned by a shared model. Such approaches offer advantages like improved data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information. However, the simultaneous learning of multiple tasks presents new design and optimization challenges, and choosing which tasks should be learned jointly is in itself a non-trivial problem. In this survey, we give an overview of multi-task learning methods for deep neural networks, with the aim of summarizing both the well-established and most recent directions within the field. Our discussion is structured according to a partition of the existing deep MTL techniques into three groups: architectures, optimization methods, and task relationship learning. We also provide a summary of common multi-task benchmarks.



### SWP-LeafNET: A novel multistage approach for plant leaf identification based on deep CNN
- **Arxiv ID**: http://arxiv.org/abs/2009.05139v2
- **DOI**: 10.1016/j.eswa.2022.117470
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05139v2)
- **Published**: 2020-09-10 20:28:57+00:00
- **Updated**: 2022-08-17 20:52:20+00:00
- **Authors**: Ali Beikmohammadi, Karim Faez, Ali Motallebi
- **Comment**: None
- **Journal**: Expert Systems with Applications 202(2022)
- **Summary**: Modern scientific and technological advances allow botanists to use computer vision-based approaches for plant identification tasks. These approaches have their own challenges. Leaf classification is a computer-vision task performed for the automated identification of plant species, a serious challenge due to variations in leaf morphology, including its size, texture, shape, and venation. Researchers have recently become more inclined toward deep learning-based methods rather than conventional feature-based methods due to the popularity and successful implementation of deep learning methods in image analysis, object recognition, and speech recognition.   In this paper, to have an interpretable and reliable system, a botanist's behavior is modeled in leaf identification by proposing a highly-efficient method of maximum behavioral resemblance developed through three deep learning-based models. Different layers of the three models are visualized to ensure that the botanist's behavior is modeled accurately. The first and second models are designed from scratch. Regarding the third model, the pre-trained architecture MobileNetV2 is employed along with the transfer-learning technique. The proposed method is evaluated on two well-known datasets: Flavia and MalayaKew. According to a comparative analysis, the suggested approach is more accurate than hand-crafted feature extraction methods and other deep learning techniques in terms of 99.67% and 99.81% accuracy. Unlike conventional techniques that have their own specific complexities and depend on datasets, the proposed method requires no hand-crafted feature extraction. Also, it increases accuracy as compared with other deep learning techniques. Moreover, SWP-LeafNET is distributable and considerably faster than other methods because of using shallower models with fewer parameters asynchronously.



### Auto-encoders for Track Reconstruction in Drift Chambers for CLAS12
- **Arxiv ID**: http://arxiv.org/abs/2009.05144v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2009.05144v2)
- **Published**: 2020-09-10 20:46:14+00:00
- **Updated**: 2022-06-13 19:33:53+00:00
- **Authors**: Gagik Gavalian
- **Comment**: None
- **Journal**: None
- **Summary**: In this article we describe the development of machine learning models to assist the CLAS12 tracking algorithm by identifying tracks through inferring missing segments in the drift chambers. Auto encoders are used to reconstruct missing segments from track trajectory. Implemented neural network was able to reliably reconstruct missing segment positions with accuracy of $\approx 0.35$ wires, and lead to recovery of missing tracks with accuracy of $>99.8\%$.



### OCR Graph Features for Manipulation Detection in Documents
- **Arxiv ID**: http://arxiv.org/abs/2009.05158v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.05158v2)
- **Published**: 2020-09-10 21:50:45+00:00
- **Updated**: 2020-09-14 15:52:09+00:00
- **Authors**: Hailey Joren, Otkrist Gupta, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting manipulations in digital documents is becoming increasingly important for information verification purposes. Due to the proliferation of image editing software, altering key information in documents has become widely accessible. Nearly all approaches in this domain rely on a procedural approach, using carefully generated features and a hand-tuned scoring system, rather than a data-driven and generalizable approach. We frame this issue as a graph comparison problem using the character bounding boxes, and propose a model that leverages graph features using OCR (Optical Character Recognition). Our model relies on a data-driven approach to detect alterations by training a random forest classifier on the graph-based OCR features. We evaluate our algorithm's forgery detection performance on dataset constructed from real business documents with slight forgery imperfections. Our proposed model dramatically outperforms the most closely-related document manipulation detection model on this task.



### Denoising Large-Scale Image Captioning from Alt-text Data using Content Selection Models
- **Arxiv ID**: http://arxiv.org/abs/2009.05175v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05175v3)
- **Published**: 2020-09-10 23:31:38+00:00
- **Updated**: 2022-10-30 21:46:51+00:00
- **Authors**: Khyathi Raghavi Chandu, Piyush Sharma, Soravit Changpinyo, Ashish Thapliyal, Radu Soricut
- **Comment**: None
- **Journal**: None
- **Summary**: Training large-scale image captioning (IC) models demands access to a rich and diverse set of training examples, gathered from the wild, often from noisy alt-text data. However, recent modeling approaches to IC often fall short in terms of performance in this case, because they assume a clean annotated dataset (as opposed to the noisier alt-text--based annotations), and employ an end-to-end generation approach, which often lacks both controllability and interpretability. We address these problems by breaking down the task into two simpler, more controllable tasks -- skeleton prediction and skeleton-based caption generation. Specifically, we show that selecting content words as skeletons} helps in generating improved and denoised captions when leveraging rich yet noisy alt-text--based uncurated datasets. We also show that the predicted English skeletons can be further cross-lingually leveraged to generate non-English captions, and present experimental results covering caption generation in French, Italian, German, Spanish and Hindi. We also show that skeleton-based prediction allows for better control of certain caption properties, such as length, content, and gender expression, providing a handle to perform human-in-the-loop semi-automatic corrections.




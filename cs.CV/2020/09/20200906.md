# Arxiv Papers in cs.CV on 2020-09-06
### A Genetic Feature Selection Based Two-stream Neural Network for Anger Veracity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.02650v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.02650v3)
- **Published**: 2020-09-06 05:52:41+00:00
- **Updated**: 2020-09-12 03:03:43+00:00
- **Authors**: Chaoxing Huang, Xuanying Zhu, Tom Gedeon
- **Comment**: This paper has been accepted by the 27th International Conference on
  Neural Information Processing
- **Journal**: None
- **Summary**: People can manipulate emotion expressions when interacting with others. For example, acted anger can be expressed when stimuli is not genuinely angry with an aim to manipulate the observer. In this paper, we aim to examine if the veracity of anger can be recognized with observers' pupillary data with computational approaches. We use Genetic-based Feature Selection (GFS) methods to select time-series pupillary features of of observers who observe acted and genuine anger of the video stimuli. We then use the selected features to train a simple fully connected neural work and a two-stream neural network. Our results show that the two-stream architecture is able to achieve a promising recognition result with an accuracy of 93.58% when the pupillary responses from both eyes are available. It also shows that genetic algorithm based feature selection method can effectively improve the classification accuracy by 3.07%. We hope our work could help daily research such as human machine interaction and psychology studies that require emotion recognition .



### A Survey on Machine Learning from Few Samples
- **Arxiv ID**: http://arxiv.org/abs/2009.02653v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02653v3)
- **Published**: 2020-09-06 06:13:09+00:00
- **Updated**: 2023-02-27 13:52:42+00:00
- **Authors**: Jiang Lu, Pinghua Gong, Jieping Ye, Jianwei Zhang, Changshui Zhang
- **Comment**: 30 pages, Accepted by Pattern Recognition, 2023
- **Journal**: Pattern Recognition, 2023
- **Summary**: Few sample learning (FSL) is significant and challenging in the field of machine learning. The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence since humans can readily establish their cognition to novelty from just a single or a handful of examples whereas machine learning algorithms typically entail hundreds or thousands of supervised samples to guarantee generalization ability. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning technologies, little surveys or reviews for FSL are available until now. In this context, we extensively review 300+ papers of FSL spanning from the 2000s to 2019 and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history as well as the current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review the latest advances on these topics. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.



### Approaches, Challenges, and Applications for Deep Visual Odometry: Toward to Complicated and Emerging Areas
- **Arxiv ID**: http://arxiv.org/abs/2009.02672v1
- **DOI**: 10.1109/TCDS.2020.3038898
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.02672v1)
- **Published**: 2020-09-06 08:25:23+00:00
- **Updated**: 2020-09-06 08:25:23+00:00
- **Authors**: Ke Wang, Sai Ma, Junlan Chen, Fan Ren
- **Comment**: None
- **Journal**: IEEE Transactions on Cognitive and Developmental Systems. 2020
- **Summary**: Visual odometry (VO) is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep learning-based VO (Deep VO). Therefore, this paper aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented reality and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.



### Deep Learning for Automatic Spleen Length Measurement in Sickle Cell Disease Patients
- **Arxiv ID**: http://arxiv.org/abs/2009.02704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.02704v1)
- **Published**: 2020-09-06 10:47:49+00:00
- **Updated**: 2020-09-06 10:47:49+00:00
- **Authors**: Zhen Yuan, Esther Puyol-Anton, Haran Jogeesvaran, Catriona Reid, Baba Inusa, Andrew P. King
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Sickle Cell Disease (SCD) is one of the most common genetic diseases in the world. Splenomegaly (abnormal enlargement of the spleen) is frequent among children with SCD. If left untreated, splenomegaly can be life-threatening. The current workflow to measure spleen size includes palpation, possibly followed by manual length measurement in 2D ultrasound imaging. However, this manual measurement is dependent on operator expertise and is subject to intra- and inter-observer variability. We investigate the use of deep learning to perform automatic estimation of spleen length from ultrasound images. We investigate two types of approach, one segmentation-based and one based on direct length estimation, and compare the results against measurements made by human experts. Our best model (segmentation-based) achieved a percentage length error of 7.42%, which is approaching the level of inter-observer variability (5.47%-6.34%). To the best of our knowledge, this is the first attempt to measure spleen size in a fully automated way from ultrasound images.



### Efficient Pedestrian Detection in Top-View Fisheye Images Using Compositions of Perspective View Patches
- **Arxiv ID**: http://arxiv.org/abs/2009.02711v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02711v2)
- **Published**: 2020-09-06 11:19:00+00:00
- **Updated**: 2020-10-23 04:48:30+00:00
- **Authors**: Sheng-Ho Chiang, Tsaipei Wang, Yi-Fu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection in images is a topic that has been studied extensively, but existing detectors designed for perspective images do not perform as successfully on images taken with top-view fisheye cameras, mainly due to the orientation variation of people in such images. In our proposed approach, several perspective views are generated from a fisheye image and then concatenated to form a composite image. As pedestrians in this composite image are more likely to be upright, existing detectors designed and trained for perspective images can be applied directly without additional training. We also describe a new method of mapping detection bounding boxes from the perspective views to the fisheye frame. The detection performance on several public datasets compare favorably with state-of-the-art results.



### DeePSD: Automatic Deep Skinning And Pose Space Deformation For 3D Garment Animation
- **Arxiv ID**: http://arxiv.org/abs/2009.02715v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02715v2)
- **Published**: 2020-09-06 11:52:17+00:00
- **Updated**: 2021-04-07 09:19:24+00:00
- **Authors**: Hugo Bertiche, Meysam Madadi, Emilio Tylson, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel solution to the garment animation problem through deep learning. Our contribution allows animating any template outfit with arbitrary topology and geometric complexity. Recent works develop models for garment edition, resizing and animation at the same time by leveraging the support body model (encoding garments as body homotopies). This leads to complex engineering solutions that suffer from scalability, applicability and compatibility. By limiting our scope to garment animation only, we are able to propose a simple model that can animate any outfit, independently of its topology, vertex order or connectivity. Our proposed architecture maps outfits to animated 3D models into the standard format for 3D animation (blend weights and blend shapes matrices), automatically providing of compatibility with any graphics engine. We also propose a methodology to complement supervised learning with an unsupervised physically based learning that implicitly solves collisions and enhances cloth quality.



### Detection Defense Against Adversarial Attacks with Saliency Map
- **Arxiv ID**: http://arxiv.org/abs/2009.02738v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02738v1)
- **Published**: 2020-09-06 13:57:17+00:00
- **Updated**: 2020-09-06 13:57:17+00:00
- **Authors**: Dengpan Ye, Chuanxi Chen, Changrui Liu, Hao Wang, Shunzhi Jiang
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: It is well established that neural networks are vulnerable to adversarial examples, which are almost imperceptible on human vision and can cause the deep models misbehave. Such phenomenon may lead to severely inestimable consequences in the safety and security critical applications. Existing defenses are trend to harden the robustness of models against adversarial attacks, e.g., adversarial training technology. However, these are usually intractable to implement due to the high cost of re-training and the cumbersome operations of altering the model architecture or parameters. In this paper, we discuss the saliency map method from the view of enhancing model interpretability, it is similar to introducing the mechanism of the attention to the model, so as to comprehend the progress of object identification by the deep networks. We then propose a novel method combined with additional noises and utilize the inconsistency strategy to detect adversarial examples. Our experimental results of some representative adversarial attacks on common datasets including ImageNet and popular models show that our method can detect all the attacks with high detection success rate effectively. We compare it with the existing state-of-the-art technique, and the experiments indicate that our method is more general.



### Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2009.02755v8
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02755v8)
- **Published**: 2020-09-06 15:35:53+00:00
- **Updated**: 2021-09-28 09:14:49+00:00
- **Authors**: Boris Lorbeer, Max Botler
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder EnSemble), a new method for unsupervised outlier detection (UOD). More precisely, given any autoencoder for UOD, this technique can be used to improve its accuracy while at the same time removing the burden of tuning its regularization. The idea is to not regularize at all, but to rather randomly partition the data into sufficiently many equally sized parts, overfit each part with its own autoencoder, and to use the maximum over all autoencoder reconstruction errors as the anomaly score. We apply our model to various realistic datasets and show that if the set of inliers is dense enough, our method indeed improves the UOD performance of a given autoencoder significantly. For reproducibility, the code is made available on github so the reader can recreate the results in this paper as well as apply the method to other autoencoders and datasets.



### Edge-variational Graph Convolutional Networks for Uncertainty-aware Disease Prediction
- **Arxiv ID**: http://arxiv.org/abs/2009.02759v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02759v1)
- **Published**: 2020-09-06 15:53:17+00:00
- **Updated**: 2020-09-06 15:53:17+00:00
- **Authors**: Yongxiang Huang, Albert C. S. Chung
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: There is a rising need for computational models that can complementarily leverage data of different modalities while investigating associations between subjects for population-based disease analysis. Despite the success of convolutional neural networks in representation learning for imaging data, it is still a very challenging task. In this paper, we propose a generalizable framework that can automatically integrate imaging data with non-imaging data in populations for uncertainty-aware disease prediction. At its core is a learnable adaptive population graph with variational edges, which we mathematically prove that it is optimizable in conjunction with graph convolutional neural networks. To estimate the predictive uncertainty related to the graph topology, we propose the novel concept of Monte-Carlo edge dropout. Experimental results on four databases show that our method can consistently and significantly improve the diagnostic accuracy for Autism spectrum disorder, Alzheimer's disease, and ocular diseases, indicating its generalizability in leveraging multimodal data for computer-aided diagnosis.



### Why Spectral Normalization Stabilizes GANs: Analysis and Improvements
- **Arxiv ID**: http://arxiv.org/abs/2009.02773v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02773v2)
- **Published**: 2020-09-06 16:51:42+00:00
- **Updated**: 2021-04-08 00:29:30+00:00
- **Authors**: Zinan Lin, Vyas Sekar, Giulia Fanti
- **Comment**: 54 pages, 74 figures
- **Journal**: None
- **Summary**: Spectral normalization (SN) is a widely-used technique for improving the stability and sample quality of Generative Adversarial Networks (GANs). However, there is currently limited understanding of why SN is effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization. This connection helps to explain why the most popular implementation of SN for GANs requires no hyper-parameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, SN preserves this property throughout training. Building on this theoretical understanding, we propose a new spectral normalization technique: Bidirectional Scaled Spectral Normalization (BSSN), which incorporates insights from later improvements to LeCun initialization: Xavier initialization and Kaiming initialization. Theoretically, we show that BSSN gives better gradient control than SN. Empirically, we demonstrate that it outperforms SN in sample quality and training stability on several benchmark datasets.



### Perfusion Imaging: A Data Assimilation Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.02796v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02796v1)
- **Published**: 2020-09-06 18:44:44+00:00
- **Updated**: 2020-09-06 18:44:44+00:00
- **Authors**: Peirong Liu, Yueh Z. Lee, Stephen R. Aylward, Marc Niethammer
- **Comment**: Submitted to IEEE-TMI 2020
- **Journal**: None
- **Summary**: Perfusion imaging (PI) is clinically used to assess strokes and brain tumors. Commonly used PI approaches based on magnetic resonance imaging (MRI) or computed tomography (CT) measure the effect of a contrast agent moving through blood vessels and into tissue. Contrast-agent free approaches, for example, based on intravoxel incoherent motion, also exist, but are so far not routinely used clinically. These methods rely on estimating on the arterial input function (AIF) to approximately model tissue perfusion, neglecting spatial dependencies, and reliably estimating the AIF is also non-trivial, leading to difficulties with standardizing perfusion measures. In this work we therefore propose a data-assimilation approach (PIANO) which estimates the velocity and diffusion fields of an advection-diffusion model that best explains the contrast dynamics. PIANO accounts for spatial dependencies and neither requires estimating the AIF nor relies on a particular contrast agent bolus shape. Specifically, we propose a convenient parameterization of the estimation problem, a numerical estimation approach, and extensively evaluate PIANO. We demonstrate that PIANO can successfully resolve velocity and diffusion field ambiguities and results in sensitive measures for the assessment of stroke, comparing favorably to conventional measures of perfusion.



### Deep Modeling of Growth Trajectories for Longitudinal Prediction of Missing Infant Cortical Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2009.02797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02797v2)
- **Published**: 2020-09-06 18:46:04+00:00
- **Updated**: 2020-09-12 03:34:36+00:00
- **Authors**: Peirong Liu, Zhengwang Wu, Gang Li, Pew-Thian Yap, Dinggang Shen
- **Comment**: Accepted as oral presentation at IPMI 2019
- **Journal**: None
- **Summary**: Charting cortical growth trajectories is of paramount importance for understanding brain development. However, such analysis necessitates the collection of longitudinal data, which can be challenging due to subject dropouts and failed scans. In this paper, we will introduce a method for longitudinal prediction of cortical surfaces using a spatial graph convolutional neural network (GCNN), which extends conventional CNNs from Euclidean to curved manifolds. The proposed method is designed to model the cortical growth trajectories and jointly predict inner and outer cortical surfaces at multiple time points. Adopting a binary flag in loss calculation to deal with missing data, we fully utilize all available cortical surfaces for training our deep learning model, without requiring a complete collection of longitudinal data. Predicting the surfaces directly allows cortical attributes such as cortical thickness, curvature, and convexity to be computed for subsequent analysis. We will demonstrate with experimental results that our method is capable of capturing the nonlinearity of spatiotemporal cortical growth patterns and can predict cortical surfaces with improved accuracy.



### The 2ST-UNet for Pneumothorax Segmentation in Chest X-Rays using ResNet34 as a Backbone for U-Net
- **Arxiv ID**: http://arxiv.org/abs/2009.02805v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02805v1)
- **Published**: 2020-09-06 19:39:05+00:00
- **Updated**: 2020-09-06 19:39:05+00:00
- **Authors**: Ayat Abedalla, Malak Abdullah, Mahmoud Al-Ayyoub, Elhadj Benkhelifa
- **Comment**: None
- **Journal**: None
- **Summary**: Pneumothorax, also called a collapsed lung, refers to the presence of the air in the pleural space between the lung and chest wall. It can be small (no need for treatment), or large and causes death if it is not identified and treated on time. It is easily seen and identified by experts using a chest X-ray. Although this method is mostly error-free, it is time-consuming and needs expert radiologists. Recently, Computer Vision has been providing great assistance in detecting and segmenting pneumothorax. In this paper, we propose a 2-Stage Training system (2ST-UNet) to segment images with pneumothorax. This system is built based on U-Net with Residual Networks (ResNet-34) backbone that is pre-trained on the ImageNet dataset. We start with training the network at a lower resolution before we load the trained model weights to retrain the network with a higher resolution. Moreover, we utilize different techniques including Stochastic Weight Averaging (SWA), data augmentation, and Test-Time Augmentation (TTA). We use the chest X-ray dataset that is provided by the 2019 SIIM-ACR Pneumothorax Segmentation Challenge, which contains 12,047 training images and 3,205 testing images. Our experiments show that 2-Stage Training leads to better and faster network convergence. Our method achieves 0.8356 mean Dice Similarity Coefficient (DSC) placing it among the top 9% of models with a rank of 124 out of 1,475.



### Goal-Directed Occupancy Prediction for Lane-Following Actors
- **Arxiv ID**: http://arxiv.org/abs/2009.12174v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.12174v1)
- **Published**: 2020-09-06 20:44:59+00:00
- **Updated**: 2020-09-06 20:44:59+00:00
- **Authors**: Poornima Kaniarasu, Galen Clark Haynes, Micol Marchetti-Bowick
- **Comment**: Published at ICRA 2020
- **Journal**: None
- **Summary**: Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.



### Rain rendering for evaluating and improving robustness to bad weather
- **Arxiv ID**: http://arxiv.org/abs/2009.03683v1
- **DOI**: 10.1007/s11263-020-01366-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03683v1)
- **Published**: 2020-09-06 21:08:41+00:00
- **Updated**: 2020-09-06 21:08:41+00:00
- **Authors**: Maxime Tremblay, Shirsendu Sukanta Halder, Raoul de Charette, Jean-Fran√ßois Lalonde
- **Comment**: 19 pages, 19 figures, IJCV 2020 preprint. arXiv admin note: text
  overlap with arXiv:1908.10335
- **Journal**: None
- **Summary**: Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73% more realistic than the state-of-theart. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection, 60% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21% on object detection, 37% on semantic segmentation, and 8% on depth estimation.



### TRANSPR: Transparency Ray-Accumulating Neural 3D Scene Point Renderer
- **Arxiv ID**: http://arxiv.org/abs/2009.02819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02819v1)
- **Published**: 2020-09-06 21:19:18+00:00
- **Updated**: 2020-09-06 21:19:18+00:00
- **Authors**: Maria Kolos, Artem Sevastopolsky, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and evaluate a neural point-based graphics method that can model semi-transparent scene parts. Similarly to its predecessor pipeline, ours uses point clouds to model proxy geometry, and augments each point with a neural descriptor. Additionally, a learnable transparency value is introduced in our approach for each point.   Our neural rendering procedure consists of two steps. Firstly, the point cloud is rasterized using ray grouping into a multi-channel image. This is followed by the neural rendering step that "translates" the rasterized image into an RGB output using a learnable convolutional network. New scenes can be modeled using gradient-based optimization of neural descriptors and of the rendering network.   We show that novel views of semi-transparent point cloud scenes can be generated after training with our approach. Our experiments demonstrate the benefit of introducing semi-transparency into the neural point-based modeling for a range of scenes with semi-transparent parts.



### MFL_COVID19: Quantifying Country-based Factors affecting Case Fatality Rate in Early Phase of COVID-19 Epidemic via Regularised Multi-task Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.02827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02827v1)
- **Published**: 2020-09-06 22:34:14+00:00
- **Updated**: 2020-09-06 22:34:14+00:00
- **Authors**: Po Yang, Jun Qi, Xulong Wang, Yun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent outbreak of COVID-19 has led a rapid global spread around the world. Many countries have implemented timely intensive suppression to minimize the infections, but resulted in high case fatality rate (CFR) due to critical demand of health resources. Other country-based factors such as sociocultural issues, ageing population etc., has also influenced practical effectiveness of taking interventions to improve morality in early phase. To better understand the relationship of these factors across different countries with COVID-19 CFR is of primary importance to prepare for potentially second wave of COVID-19 infections. In the paper, we propose a novel regularized multi-task learning based factor analysis approach for quantifying country-based factors affecting CFR in early phase of COVID-19 epidemic. We formulate the prediction of CFR progression as a ML regression problem with observed CFR and other countries-based factors. In this formulation, all CFR related factors were categorized into 6 sectors with 27 indicators. We proposed a hybrid feature selection method combining filter, wrapper and tree-based models to calibrate initial factors for a preliminary feature interaction. Then we adopted two typical single task model (Ridge and Lasso regression) and one state-of-the-art MTFL method (fused sparse group lasso) in our formulation. The fused sparse group Lasso (FSGL) method allows the simultaneous selection of a common set of country-based factors for multiple time points of COVID-19 epidemic and also enables incorporating temporal smoothness of each factor over the whole early phase period. Finally, we proposed one novel temporal voting feature selection scheme to balance the weight instability of multiple factors in our MTFL model.



### Unsupervised Wasserstein Distance Guided Domain Adaptation for 3D Multi-Domain Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.02831v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02831v1)
- **Published**: 2020-09-06 23:48:27+00:00
- **Updated**: 2020-09-06 23:48:27+00:00
- **Authors**: Chenyu You, Junlin Yang, Julius Chapiro, James S. Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have shown exceptional learning capability and generalizability in the source domain when massive labeled data is provided. However, the well-trained models often fail in the target domain due to the domain shift. Unsupervised domain adaptation aims to improve network performance when applying robust models trained on medical images from source domains to a new target domain. In this work, we present an approach based on the Wasserstein distance guided disentangled representation to achieve 3D multi-domain liver segmentation. Concretely, we embed images onto a shared content space capturing shared feature-level information across domains and domain-specific appearance spaces. The existing mutual information-based representation learning approaches often fail to capture complete representations in multi-domain medical imaging tasks. To mitigate these issues, we utilize Wasserstein distance to learn more complete representation, and introduces a content discriminator to further facilitate the representation disentanglement. Experiments demonstrate that our method outperforms the state-of-the-art on the multi-modality liver segmentation task.




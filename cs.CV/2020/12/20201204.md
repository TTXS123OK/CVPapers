# Arxiv Papers in cs.CV on 2020-12-04
### Understanding Guided Image Captioning Performance across Domains
- **Arxiv ID**: http://arxiv.org/abs/2012.02339v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2012.02339v3)
- **Published**: 2020-12-04 00:05:02+00:00
- **Updated**: 2021-11-10 23:52:50+00:00
- **Authors**: Edwin G. Ng, Bo Pang, Piyush Sharma, Radu Soricut
- **Comment**: Proceedings of CoNLL 2021
- **Journal**: None
- **Summary**: Image captioning models generally lack the capability to take into account user interest, and usually default to global descriptions that try to balance readability, informativeness, and information overload. On the other hand, VQA models generally lack the ability to provide long descriptive answers, while expecting the textual question to be quite precise. We present a method to control the concepts that an image caption should focus on, using an additional input called the guiding text that refers to either groundable or ungroundable concepts in the image. Our model consists of a Transformer-based multimodal encoder that uses the guiding text together with global and object-level image features to derive early-fusion representations used to generate the guided caption. While models trained on Visual Genome data have an in-domain advantage of fitting well when guided with automatic object labels, we find that guided captioning models trained on Conceptual Captions generalize better on out-of-domain images and guiding texts. Our human-evaluation results indicate that attempting in-the-wild guided image captioning requires access to large, unrestricted-domain training datasets, and that increased style diversity (even without increasing the number of unique tokens) is a key factor for improved performance.



### ChartPointFlow for Topology-Aware 3D Point Cloud Generation
- **Arxiv ID**: http://arxiv.org/abs/2012.02346v2
- **DOI**: 10.1145/3474085.3475589
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02346v2)
- **Published**: 2020-12-04 00:49:25+00:00
- **Updated**: 2021-08-07 10:14:33+00:00
- **Authors**: Takumi Kimura, Takashi Matsubara, Kuniaki Uehara
- **Comment**: Accepted to ACM International Conference on Multimedia (ACMMM2021) as
  an oral presentation
- **Journal**: ACM International Conference on Multimedia (ACMMM2021)
- **Summary**: A point cloud serves as a representation of the surface of a three-dimensional (3D) shape. Deep generative models have been adapted to model their variations typically using a map from a ball-like set of latent variables. However, previous approaches did not pay much attention to the topological structure of a point cloud, despite that a continuous map cannot express the varying numbers of holes and intersections. Moreover, a point cloud is often composed of multiple subparts, and it is also difficult to express. In this study, we propose ChartPointFlow, a flow-based generative model with multiple latent labels for 3D point clouds. Each label is assigned to points in an unsupervised manner. Then, a map conditioned on a label is assigned to a continuous subset of a point cloud, similar to a chart of a manifold. This enables our proposed model to preserve the topological structure with clear boundaries, whereas previous approaches tend to generate blurry point clouds and fail to generate holes. The experimental results demonstrate that ChartPointFlow achieves state-of-the-art performance in terms of generation and reconstruction compared with other point cloud generators. Moreover, ChartPointFlow divides an object into semantic subparts using charts, and it demonstrates superior performance in case of unsupervised segmentation.



### WeaQA: Weak Supervision via Captions for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2012.02356v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2012.02356v2)
- **Published**: 2020-12-04 01:22:05+00:00
- **Updated**: 2021-05-28 07:09:48+00:00
- **Authors**: Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Chitta Baral
- **Comment**: Accepted in Findings of ACL 2021
- **Journal**: None
- **Summary**: Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated \textit{Image-Question-Answer} (I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic Q-A pairs generated procedurally from captions. Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models. Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors.



### Copyspace: Where to Write on Images?
- **Arxiv ID**: http://arxiv.org/abs/2012.08933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.08933v1)
- **Published**: 2020-12-04 01:58:39+00:00
- **Updated**: 2020-12-04 01:58:39+00:00
- **Authors**: Jessica M. Lundin, Michael Sollami, Brian Lonsdorf, Alan Ross, Owen Schoppe, David Woodward, Sönke Rohde
- **Comment**: 4th Workshop on Machine Learning for Creativity and Design, NeurIPS
  2020, Vancouver, Canada
- **Journal**: None
- **Summary**: The placement of text over an image is an important part of producing high-quality visual designs. Automating this work by determining appropriate position, orientation, and style for textual elements requires understanding the contents of the background image. We refer to the search for aesthetic parameters of text rendered over images as "copyspace detection", noting that this task is distinct from foreground-background separation. We have developed solutions using one and two stage object detection methodologies trained on an expertly labeled data. This workshop will examine such algorithms for copyspace detection and demonstrate their application in generative design models and pipelines such as Einstein Designer.



### Deep Learning for Medical Anomaly Detection -- A Survey
- **Arxiv ID**: http://arxiv.org/abs/2012.02364v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.02364v2)
- **Published**: 2020-12-04 02:09:37+00:00
- **Updated**: 2021-04-13 04:43:59+00:00
- **Authors**: Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Preprint submitted to ACM Computing Surveys
- **Journal**: None
- **Summary**: Machine learning-based medical anomaly detection is an important problem that has been extensively studied. Numerous approaches have been proposed across various medical application domains and we observe several similarities across these distinct applications. Despite this comparability, we observe a lack of structured organisation of these diverse research applications such that their advantages and limitations can be studied. The principal aim of this survey is to provide a thorough theoretical analysis of popular deep learning techniques in medical anomaly detection. In particular, we contribute a coherent and systematic review of state-of-the-art techniques, comparing and contrasting their architectural differences as well as training algorithms. Furthermore, we provide a comprehensive overview of deep model interpretation strategies that can be used to interpret model decisions. In addition, we outline the key limitations of existing deep medical anomaly detection techniques and propose key research directions for further investigation.



### DenserNet: Weakly Supervised Visual Localization Using Multi-scale Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2012.02366v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02366v4)
- **Published**: 2020-12-04 02:16:47+00:00
- **Updated**: 2021-03-11 21:06:01+00:00
- **Authors**: Dongfang Liu, Yiming Cui, Liqi Yan, Christos Mousas, Baijian Yang, Yingjie Chen
- **Comment**: Proceeding with The Thirty-Fifth AAAI Conference on Artificial
  Intelligence (AAAI-21)
- **Journal**: None
- **Summary**: In this work, we introduce a Denser Feature Network (DenserNet) for visual localization. Our work provides three principal contributions. First, we develop a convolutional neural network (CNN) architecture which aggregates feature maps at different semantic levels for image representations. Using denser feature maps, our method can produce more keypoint features and increase image retrieval accuracy. Second, our model is trained end-to-end without pixel-level annotation other than positive and negative GPS-tagged image pairs. We use a weakly supervised triplet ranking loss to learn discriminative features and encourage keypoint feature repeatability for image representation. Finally, our method is computationally efficient as our architecture has shared features and parameters during computation. Our method can perform accurate large-scale localization under challenging conditions while remaining the computational constraint. Extensive experiment results indicate that our method sets a new state-of-the-art on four challenging large-scale localization benchmarks and three image retrieval benchmarks.



### Optical Wavelength Guided Self-Supervised Feature Learning For Galaxy Cluster Richness Estimate
- **Arxiv ID**: http://arxiv.org/abs/2012.02368v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.CO, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02368v1)
- **Published**: 2020-12-04 02:21:00+00:00
- **Updated**: 2020-12-04 02:21:00+00:00
- **Authors**: Gongbo Liang, Yuanyuan Su, Sheng-Chieh Lin, Yu Zhang, Yuanyuan Zhang, Nathan Jacobs
- **Comment**: Accepted to NeurIPS 2020 Workshop on Machine Learning and the
  Physical Sciences
- **Journal**: None
- **Summary**: Most galaxies in the nearby Universe are gravitationally bound to a cluster or group of galaxies. Their optical contents, such as optical richness, are crucial for understanding the co-evolution of galaxies and large-scale structures in modern astronomy and cosmology. The determination of optical richness can be challenging. We propose a self-supervised approach for estimating optical richness from multi-band optical images. The method uses the data properties of the multi-band optical images for pre-training, which enables learning feature representations from a large but unlabeled dataset. We apply the proposed method to the Sloan Digital Sky Survey. The result shows our estimate of optical richness lowers the mean absolute error and intrinsic scatter by 11.84% and 20.78%, respectively, while reducing the need for labeled training data by up to 60%. We believe the proposed method will benefit astronomy and cosmology, where a large number of unlabeled multi-band images are available, but acquiring image labels is costly.



### Scale-aware Insertion of Virtual Objects in Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2012.02371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.02371v1)
- **Published**: 2020-12-04 02:25:24+00:00
- **Updated**: 2020-12-04 02:25:24+00:00
- **Authors**: Songhai Zhang, Xiangli Li, Yingtian Liu, Hongbo Fu
- **Comment**: Accepted at ISMAR 2020
- **Journal**: None
- **Summary**: In this paper, we propose a scale-aware method for inserting virtual objects with proper sizes into monocular videos. To tackle the scale ambiguity problem of geometry recovery from monocular videos, we estimate the global scale objects in a video with a Bayesian approach incorporating the size priors of objects, where the scene objects sizes should strictly conform to the same global scale and the possibilities of global scales are maximized according to the size distribution of object categories. To do so, we propose a dataset of sizes of object categories: Metric-Tree, a hierarchical representation of sizes of more than 900 object categories with the corresponding images. To handle the incompleteness of objects recovered from videos, we propose a novel scale estimation method that extracts plausible dimensions of objects for scale optimization. Experiments have shown that our method for scale estimation performs better than the state-of-the-art methods, and has considerable validity and robustness for different video scenes. Metric-Tree has been made available at: https://metric-tree.github.io



### CIT-GAN: Cyclic Image Translation Generative Adversarial Network With Application in Iris Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.02374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02374v1)
- **Published**: 2020-12-04 02:44:54+00:00
- **Updated**: 2020-12-04 02:44:54+00:00
- **Authors**: Shivangi Yadav, Arun Ross
- **Comment**: 10 pages (8 pages + 2 reference pages) and 10 figures
- **Journal**: WACV 2020
- **Summary**: In this work, we propose a novel Cyclic Image Translation Generative Adversarial Network (CIT-GAN) for multi-domain style transfer. To facilitate this, we introduce a Styling Network that has the capability to learn style characteristics of each domain represented in the training dataset. The Styling Network helps the generator to drive the translation of images from a source domain to a reference domain and generate synthetic images with style characteristics of the reference domain. The learned style characteristics for each domain depend on both the style loss and domain classification loss. This induces variability in style characteristics within each domain. The proposed CIT-GAN is used in the context of iris presentation attack detection (PAD) to generate synthetic presentation attack (PA) samples for classes that are under-represented in the training set. Evaluation using current state-of-the-art iris PAD methods demonstrates the efficacy of using such synthetically generated PA samples for training PAD methods. Further, the quality of the synthetically generated samples is evaluated using Frechet Inception Distance (FID) score. Results show that the quality of synthetic images generated by the proposed method is superior to that of other competing methods, including StarGan.



### Generator Pyramid for High-Resolution Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2012.02381v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02381v1)
- **Published**: 2020-12-04 03:27:48+00:00
- **Updated**: 2020-12-04 03:27:48+00:00
- **Authors**: Leilei Cao, Tong Yang, Yixu Wang, Bo Yan, Yandong Guo
- **Comment**: Under review
- **Journal**: None
- **Summary**: Inpainting high-resolution images with large holes challenges existing deep learning based image inpainting methods. We present a novel framework -- PyramidFill for high-resolution image inpainting task, which explicitly disentangles content completion and texture synthesis. PyramidFill attempts to complete the content of unknown regions in a lower-resolution image, and synthesis the textures of unknown regions in a higher-resolution image, progressively. Thus, our model consists of a pyramid of fully convolutional GANs, wherein the content GAN is responsible for completing contents in the lowest-resolution masked image, and each texture GAN is responsible for synthesizing textures in a higher-resolution image. Since completing contents and synthesising textures demand different abilities from generators, we customize different architectures for the content GAN and texture GAN. Experiments on multiple datasets including CelebA-HQ, Places2 and a new natural scenery dataset (NSHQ) with different resolutions demonstrate that PyramidFill generates higher-quality inpainting results than the state-of-the-art methods. To better assess high-resolution image inpainting methods, we will release NSHQ, high-quality natural scenery images with high-resolution 1920$\times$1080.



### SAM: Self-supervised Learning of Pixel-wise Anatomical Embeddings in Radiological Images
- **Arxiv ID**: http://arxiv.org/abs/2012.02383v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02383v2)
- **Published**: 2020-12-04 03:31:20+00:00
- **Updated**: 2022-04-21 05:45:09+00:00
- **Authors**: Ke Yan, Jinzheng Cai, Dakai Jin, Shun Miao, Dazhou Guo, Adam P. Harrison, Youbao Tang, Jing Xiao, Jingjing Lu, Le Lu
- **Comment**: 16 pages including appendix, accepted by IEEE Trans on Medical
  Imaging: https://ieeexplore.ieee.org/document/9760421
- **Journal**: None
- **Summary**: Radiological images such as computed tomography (CT) and X-rays render anatomy with intrinsic structures. Being able to reliably locate the same anatomical structure across varying images is a fundamental task in medical image analysis. In principle it is possible to use landmark detection or semantic segmentation for this task, but to work well these require large numbers of labeled data for each anatomical structure and sub-structure of interest. A more universal approach would learn the intrinsic structure from unlabeled images. We introduce such an approach, called Self-supervised Anatomical eMbedding (SAM). SAM generates semantic embeddings for each image pixel that describes its anatomical location or body part. To produce such embeddings, we propose a pixel-level contrastive learning framework. A coarse-to-fine strategy ensures both global and local anatomical information are encoded. Negative sample selection strategies are designed to enhance the embedding's discriminability. Using SAM, one can label any point of interest on a template image and then locate the same body part in other images by simple nearest neighbor searching. We demonstrate the effectiveness of SAM in multiple tasks with 2D and 3D image modalities. On a chest CT dataset with 19 landmarks, SAM outperforms widely-used registration algorithms while only taking 0.23 seconds for inference. On two X-ray datasets, SAM, with only one labeled template image, surpasses supervised methods trained on 50 labeled images. We also apply SAM on whole-body follow-up lesion matching in CT and obtain an accuracy of 91%. SAM can also be applied for improving image registration and initializing CNN weights.



### XraySyn: Realistic View Synthesis From a Single Radiograph Through CT Priors
- **Arxiv ID**: http://arxiv.org/abs/2012.02407v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02407v2)
- **Published**: 2020-12-04 05:08:53+00:00
- **Updated**: 2022-03-24 03:36:59+00:00
- **Authors**: Cheng Peng, Haofu Liao, Gina Wong, Jiebo Luo, Shaohua Kevin Zhou, Rama Chellappa
- **Comment**: Accepted to AAAI2021, https://github.com/cpeng93/XraySyn
- **Journal**: None
- **Summary**: A radiograph visualizes the internal anatomy of a patient through the use of X-ray, which projects 3D information onto a 2D plane. Hence, radiograph analysis naturally requires physicians to relate the prior about 3D human anatomy to 2D radiographs. Synthesizing novel radiographic views in a small range can assist physicians in interpreting anatomy more reliably; however, radiograph view synthesis is heavily ill-posed, lacking in paired data, and lacking in differentiable operations to leverage learning-based approaches. To address these problems, we use Computed Tomography (CT) for radiograph simulation and design a differentiable projection algorithm, which enables us to achieve geometrically consistent transformations between the radiography and CT domains. Our method, XraySyn, can synthesize novel views on real radiographs through a combination of realistic simulation and finetuning on real radiographs. To the best of our knowledge, this is the first work on radiograph view synthesis. We show that by gaining an understanding of radiography in 3D space, our method can be applied to radiograph bone extraction and suppression without groundtruth bone labels.



### PeR-ViS: Person Retrieval in Video Surveillance using Semantic Description
- **Arxiv ID**: http://arxiv.org/abs/2012.02408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02408v1)
- **Published**: 2020-12-04 05:11:21+00:00
- **Updated**: 2020-12-04 05:11:21+00:00
- **Authors**: Parshwa Shah, Arpit Garg, Vandit Gajjar
- **Comment**: 10 pages, 6 figures, 3 tables; Human Activity Detection in
  multi-camera, Continuous, long-duration Video (HADCV'21)under the IEEE Winter
  Conf. on Applications of Computer Vision (WACV), Virtual Conference, January
  5, 2021
- **Journal**: None
- **Summary**: A person is usually characterized by descriptors like age, gender, height, cloth type, pattern, color, etc. Such descriptors are known as attributes and/or soft-biometrics. They link the semantic gap between a person's description and retrieval in video surveillance. Retrieving a specific person with the query of semantic description has an important application in video surveillance. Using computer vision to fully automate the person retrieval task has been gathering interest within the research community. However, the Current, trend mainly focuses on retrieving persons with image-based queries, which have major limitations for practical usage. Instead of using an image query, in this paper, we study the problem of person retrieval in video surveillance with a semantic description. To solve this problem, we develop a deep learning-based cascade filtering approach (PeR-ViS), which uses Mask R-CNN [14] (person detection and instance segmentation) and DenseNet-161 [16] (soft-biometric classification). On the standard person retrieval dataset of SoftBioSearch [6], we achieve 0.566 Average IoU and 0.792 %w $IoU > 0.4$, surpassing the current state-of-the-art by a large margin. We hope our simple, reproducible, and effective approach will help ease future research in the domain of person retrieval in video surveillance. The source code and pretrained weights available at https://parshwa1999.github.io/PeR-ViS/.



### Spatial-Temporal Alignment Network for Action Recognition and Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.02426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02426v1)
- **Published**: 2020-12-04 06:23:40+00:00
- **Updated**: 2020-12-04 06:23:40+00:00
- **Authors**: Junwei Liang, Liangliang Cao, Xuehan Xiong, Ting Yu, Alexander Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies how to introduce viewpoint-invariant feature representations that can help action recognition and detection. Although we have witnessed great progress of action recognition in the past decade, it remains challenging yet interesting how to efficiently model the geometric variations in large scale datasets. This paper proposes a novel Spatial-Temporal Alignment Network (STAN) that aims to learn geometric invariant representations for action recognition and action detection. The STAN model is very light-weighted and generic, which could be plugged into existing action recognition models like ResNet3D and the SlowFast with a very low extra computational cost. We test our STAN model extensively on AVA, Kinetics-400, AVA-Kinetics, Charades, and Charades-Ego datasets. The experimental results show that the STAN model can consistently improve the state of the arts in both action detection and action recognition tasks. We will release our data, models and code.



### Towards Natural Robustness Against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2012.02452v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02452v1)
- **Published**: 2020-12-04 08:12:38+00:00
- **Updated**: 2020-12-04 08:12:38+00:00
- **Authors**: Haoyu Chu, Shikui Wei, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that deep neural networks are vulnerable to adversarial examples, but most of the methods proposed to defense adversarial examples cannot solve this problem fundamentally. In this paper, we theoretically prove that there is an upper bound for neural networks with identity mappings to constrain the error caused by adversarial noises. However, in actual computations, this kind of neural network no longer holds any upper bound and is therefore susceptible to adversarial examples. Following similar procedures, we explain why adversarial examples can fool other deep neural networks with skip connections. Furthermore, we demonstrate that a new family of deep neural networks called Neural ODEs (Chen et al., 2018) holds a weaker upper bound. This weaker upper bound prevents the amount of change in the result from being too large. Thus, Neural ODEs have natural robustness against adversarial examples. We evaluate the performance of Neural ODEs compared with ResNet under three white-box adversarial attacks (FGSM, PGD, DI2-FGSM) and one black-box adversarial attack (Boundary Attack). Finally, we show that the natural robustness of Neural ODEs is even better than the robustness of neural networks that are trained with adversarial training methods, such as TRADES and YOPO.



### Multiscale Mesh Deformation Component Analysis with Attention-based Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2012.02459v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02459v1)
- **Published**: 2020-12-04 08:30:57+00:00
- **Updated**: 2020-12-04 08:30:57+00:00
- **Authors**: Jie Yang, Lin Gao, Qingyang Tan, Yihua Huang, Shihong Xia, Yu-Kun Lai
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Deformation component analysis is a fundamental problem in geometry processing and shape understanding. Existing approaches mainly extract deformation components in local regions at a similar scale while deformations of real-world objects are usually distributed in a multi-scale manner. In this paper, we propose a novel method to exact multiscale deformation components automatically with a stacked attention-based autoencoder. The attention mechanism is designed to learn to softly weight multi-scale deformation components in active deformation regions, and the stacked attention-based autoencoder is learned to represent the deformation components at different scales. Quantitative and qualitative evaluations show that our method outperforms state-of-the-art methods. Furthermore, with the multiscale deformation components extracted by our method, the user can edit shapes in a coarse-to-fine fashion which facilitates effective modeling of new shapes.



### Offset Curves Loss for Imbalanced Problem in Medical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.02463v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02463v1)
- **Published**: 2020-12-04 08:35:21+00:00
- **Updated**: 2020-12-04 08:35:21+00:00
- **Authors**: Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu, Marios Savides
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Medical image segmentation has played an important role in medical analysis and widely developed for many clinical applications. Deep learning-based approaches have achieved high performance in semantic segmentation but they are limited to pixel-wise setting and imbalanced classes data problem. In this paper, we tackle those limitations by developing a new deep learning-based model which takes into account both higher feature level i.e. region inside contour, intermediate feature level i.e. offset curves around the contour and lower feature level i.e. contour. Our proposed Offset Curves (OsC) loss consists of three main fitting terms. The first fitting term focuses on pixel-wise level segmentation whereas the second fitting term acts as attention model which pays attention to the area around the boundaries (offset curves). The third terms plays a role as regularization term which takes the length of boundaries into account. We evaluate our proposed OsC loss on both 2D network and 3D network. Two common medical datasets, i.e. retina DRIVE and brain tumor BRATS 2018 datasets are used to benchmark our proposed loss performance. The experiments have shown that our proposed OsC loss function outperforms other mainstream loss functions such as Cross-Entropy, Dice, Focal on the most common segmentation networks Unet, FCN.



### A Jointed Feature Fusion Framework for Photoacoustic Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2012.02472v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.02472v3)
- **Published**: 2020-12-04 08:53:46+00:00
- **Updated**: 2021-05-25 15:44:00+00:00
- **Authors**: Hengrong Lan, Changchun Yang, Fei Gao
- **Comment**: under the peer-review procedure
- **Journal**: None
- **Summary**: Photoacoustic (PA) computed tomography (PACT) reconstructs the initial pressure distribution from raw PA signals. The standard reconstruction of medical image could cause the artifacts due to interferences or ill-posed setup. Recently, deep learning has been used to reconstruct the PA image with ill-posed conditions. Most works remove the artifacts from image domain, and compensate the limited-view from dataset. In this paper, we propose a jointed feature fusion framework (JEFF-Net) based on deep learning to reconstruct the PA image using limited-view data. The cross-domain features from limited-view position-wise data and the reconstructed image are fused by a backtracked supervision. Specifically, our results could generate superior performance, whose artifacts are drastically reduced in the output compared to ground-truth (full-view reconstructed result). In this paper, a quarter position-wise data (32 channels) is fed into model, which outputs another 3-quarters-view data (96 channels). Moreover, two novel losses are designed to restrain the artifacts by sufficiently manipulating superposed data. The numerical and in-vivo results have demonstrated the superior performance of our method to reconstruct the full-view image without artifacts. Finally, quantitative evaluations show that our proposed method outperformed the ground-truth in some metrics.



### Is It a Plausible Colour? UCapsNet for Image Colourisation
- **Arxiv ID**: http://arxiv.org/abs/2012.02478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02478v1)
- **Published**: 2020-12-04 09:07:13+00:00
- **Updated**: 2020-12-04 09:07:13+00:00
- **Authors**: Rita Pucci, Christian Micheloni, Gian Luca Foresti, Niki Martinel
- **Comment**: None
- **Journal**: None
- **Summary**: Human beings can imagine the colours of a grayscale image with no particular effort thanks to their ability of semantic feature extraction. Can an autonomous system achieve that? Can it hallucinate plausible and vibrant colours? This is the colourisation problem. Different from existing works relying on convolutional neural network models pre-trained with supervision, we cast such colourisation problem as a self-supervised learning task. We tackle the problem with the introduction of a novel architecture based on Capsules trained following the adversarial learning paradigm. Capsule networks are able to extract a semantic representation of the entities in the image but loose details about their spatial information, which is important for colourising a grayscale image. Thus our UCapsNet structure comes with an encoding phase that extracts entities through capsules and spatial details through convolutional neural networks. A decoding phase merges the entity features with the spatial features to hallucinate a plausible colour version of the input datum. Results on the ImageNet benchmark show that our approach is able to generate more vibrant and plausible colours than exiting solutions and achieves superior performance than models pre-trained with supervision.



### Compositionally Generalizable 3D Structure Prediction
- **Arxiv ID**: http://arxiv.org/abs/2012.02493v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02493v3)
- **Published**: 2020-12-04 09:53:14+00:00
- **Updated**: 2021-04-22 02:15:38+00:00
- **Authors**: Songfang Han, Jiayuan Gu, Kaichun Mo, Li Yi, Siyu Hu, Xuejin Chen, Hao Su
- **Comment**: None
- **Journal**: None
- **Summary**: Single-image 3D shape reconstruction is an important and long-standing problem in computer vision. A plethora of existing works is constantly pushing the state-of-the-art performance in the deep learning era. However, there remains a much more difficult and under-explored issue on how to generalize the learned skills over unseen object categories that have very different shape geometry distributions. In this paper, we bring in the concept of compositional generalizability and propose a novel framework that could better generalize to these unseen categories. We factorize the 3D shape reconstruction problem into proper sub-problems, each of which is tackled by a carefully designed neural sub-module with generalizability concerns. The intuition behind our formulation is that object parts (slates and cylindrical parts), their relationships (adjacency and translation symmetry), and shape substructures (T-junctions and a symmetric group of parts) are mostly shared across object categories, even though object geometries may look very different (e.g. chairs and cabinets). Experiments on PartNet show that we achieve superior performance than state-of-the-art. This validates our problem factorization and network designs.



### How Many Annotators Do We Need? -- A Study on the Influence of Inter-Observer Variability on the Reliability of Automatic Mitotic Figure Assessment
- **Arxiv ID**: http://arxiv.org/abs/2012.02495v2
- **DOI**: 10.1007/978-3-658-33198-6_56
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02495v2)
- **Published**: 2020-12-04 09:54:00+00:00
- **Updated**: 2021-01-08 12:03:01+00:00
- **Authors**: Frauke Wilm, Christof A. Bertram, Christian Marzahl, Alexander Bartel, Taryn A. Donovan, Charles-Antoine Assenmacher, Kathrin Becker, Mark Bennett, Sarah Corner, Brieuc Cossic, Daniela Denk, Martina Dettwiler, Beatriz Garcia Gonzalez, Corinne Gurtner, Annika Lehmbecker, Sophie Merz, Stephanie Plog, Anja Schmidt, Rebecca C. Smedley, Marco Tecilla, Tuddow Thaiwong, Katharina Breininger, Matti Kiupel, Andreas Maier, Robert Klopfleisch, Marc Aubreville
- **Comment**: Due to data inconsistencies experiments had to be repeated with a
  reduced number of annotators (17 in version 1). All findings of the previous
  version were reproducible. 7 pages, 2 figures, accepted at BVM workshop 2021
- **Journal**: None
- **Summary**: Density of mitotic figures in histologic sections is a prognostically relevant characteristic for many tumours. Due to high inter-pathologist variability, deep learning-based algorithms are a promising solution to improve tumour prognostication. Pathologists are the gold standard for database development, however, labelling errors may hamper development of accurate algorithms. In the present work we evaluated the benefit of multi-expert consensus (n = 3, 5, 7, 9, 11) on algorithmic performance. While training with individual databases resulted in highly variable F$_1$ scores, performance was notably increased and more consistent when using the consensus of three annotators. Adding more annotators only resulted in minor improvements. We conclude that databases by few pathologists and high label accuracy may be the best compromise between high algorithmic performance and time investment.



### SAFFIRE: System for Autonomous Feature Filtering and Intelligent ROI Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.02502v2
- **DOI**: 10.1007/978-3-030-68799-1_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02502v2)
- **Published**: 2020-12-04 10:07:24+00:00
- **Updated**: 2021-03-04 20:29:14+00:00
- **Authors**: Marco Boschi, Luigi Di Stefano, Martino Alessandrini
- **Comment**: 14 pages, 23 figures, 2 tables
- **Journal**: ICPR International Workshops and Challenges. ICPR 2021. Lecture
  Notes in Computer Science, vol 12664, pp 552-565
- **Summary**: This work introduces a new framework, named SAFFIRE, to automatically extract a dominant recurrent image pattern from a set of image samples. Such a pattern shall be used to eliminate pose variations between samples, which is a common requirement in many computer vision and machine learning tasks. The framework is specialized here in the context of a machine vision system for automated product inspection. Here, it is customary to ask the user for the identification of an anchor pattern, to be used by the automated system to normalize data before further processing. Yet, this is a very sensitive operation which is intrinsically subjective and requires high expertise. Hereto, SAFFIRE provides a unique and disruptive framework for unsupervised identification of an optimal anchor pattern in a way which is fully transparent to the user. SAFFIRE is thoroughly validated on several realistic case studies for a machine vision inspection pipeline.



### ID-Reveal: Identity-aware DeepFake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.02512v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02512v3)
- **Published**: 2020-12-04 10:43:16+00:00
- **Updated**: 2021-08-20 21:33:53+00:00
- **Authors**: Davide Cozzolino, Andreas Rössler, Justus Thies, Matthias Nießner, Luisa Verdoliva
- **Comment**: Video: https://www.youtube.com/watch?v=RsFxsOLvRdY
- **Journal**: None
- **Summary**: A major challenge in DeepFake forgery detection is that state-of-the-art algorithms are mostly trained to detect a specific fake method. As a result, these approaches show poor generalization across different types of facial manipulations, e.g., from face swapping to facial reenactment. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how a person moves while talking, by means of metric learning coupled with an adversarial training strategy. The advantage is that we do not need any training data of fakes, but only train on real videos. Moreover, we utilize high-level semantic features, which enables robustness to widespread and disruptive forms of post-processing. We perform a thorough experimental analysis on several publicly available benchmarks. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.



### AuthNet: A Deep Learning based Authentication Mechanism using Temporal Facial Feature Movements
- **Arxiv ID**: http://arxiv.org/abs/2012.02515v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02515v2)
- **Published**: 2020-12-04 10:46:12+00:00
- **Updated**: 2020-12-19 08:52:14+00:00
- **Authors**: Mohit Raghavendra, Pravan Omprakash, B R Mukesh, Sowmya Kamath
- **Comment**: 2-page version accepted in AAAI-21 Student Abstract and Poster
  Program
- **Journal**: None
- **Summary**: Biometric systems based on Machine learning and Deep learning are being extensively used as authentication mechanisms in resource-constrained environments like smartphones and other small computing devices. These AI-powered facial recognition mechanisms have gained enormous popularity in recent years due to their transparent, contact-less and non-invasive nature. While they are effective to a large extent, there are ways to gain unauthorized access using photographs, masks, glasses, etc. In this paper, we propose an alternative authentication mechanism that uses both facial recognition and the unique movements of that particular face while uttering a password, that is, the temporal facial feature movements. The proposed model is not inhibited by language barriers because a user can set a password in any language. When evaluated on the standard MIRACL-VC1 dataset, the proposed model achieved an accuracy of 98.1%, underscoring its effectiveness as an effective and robust system. The proposed method is also data-efficient since the model gave good results even when trained with only 10 positive video samples. The competence of the training of the network is also demonstrated by benchmarking the proposed system against various compounded Facial recognition and Lip reading models.



### A Note on Data Biases in Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2012.02516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02516v1)
- **Published**: 2020-12-04 10:46:37+00:00
- **Updated**: 2020-12-04 10:46:37+00:00
- **Authors**: Patrick Esser, Robin Rombach, Björn Ommer
- **Comment**: Extended Abstract for the NeurIPS 2020 Workshop on Machine Learning
  for Creativity and Design
- **Journal**: None
- **Summary**: It is tempting to think that machines are less prone to unfairness and prejudice. However, machine learning approaches compute their outputs based on data. While biases can enter at any stage of the development pipeline, models are particularly receptive to mirror biases of the datasets they are trained on and therefore do not necessarily reflect truths about the world but, primarily, truths about the data. To raise awareness about the relationship between modern algorithms and the data that shape them, we use a conditional invertible neural network to disentangle the dataset-specific information from the information which is shared across different datasets. In this way, we can project the same image onto different datasets, thereby revealing their inherent biases. We use this methodology to (i) investigate the impact of dataset quality on the performance of generative models, (ii) show how societal biases of datasets are replicated by generative models, and (iii) present creative applications through unpaired transfer between diverse datasets such as photographs, oil portraits, and animes. Our code and an interactive demonstration are available at https://github.com/CompVis/net2net .



### Practical No-box Adversarial Attacks against DNNs
- **Arxiv ID**: http://arxiv.org/abs/2012.02525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02525v1)
- **Published**: 2020-12-04 11:10:03+00:00
- **Updated**: 2020-12-04 11:10:03+00:00
- **Authors**: Qizhang Li, Yiwen Guo, Hao Chen
- **Comment**: Accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: The study of adversarial vulnerabilities of deep neural networks (DNNs) has progressed rapidly. Existing attacks require either internal access (to the architecture, parameters, or training set of the victim model) or external access (to query the model). However, both the access may be infeasible or expensive in many scenarios. We investigate no-box adversarial examples, where the attacker can neither access the model information or the training set nor query the model. Instead, the attacker can only gather a small number of examples from the same problem domain as that of the victim model. Such a stronger threat model greatly expands the applicability of adversarial attacks. We propose three mechanisms for training with a very small dataset (on the order of tens of examples) and find that prototypical reconstruction is the most effective. Our experiments show that adversarial examples crafted on prototypical auto-encoding models transfer well to a variety of image classification and face verification models. On a commercial celebrity recognition system held by clarifai.com, our approach significantly diminishes the average prediction accuracy of the system to only 15.40%, which is on par with the attack that transfers adversarial examples from a pre-trained Arcface model.



### Rethinking supervised learning: insights from biological learning and from calling it by its name
- **Arxiv ID**: http://arxiv.org/abs/2012.02526v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02526v2)
- **Published**: 2020-12-04 11:13:31+00:00
- **Updated**: 2021-06-22 18:18:44+00:00
- **Authors**: Alex Hernandez-Garcia
- **Comment**: Perspective paper. 8 pages + references. Earlier, shorter version
  accepted at the workshop SVRHM, NeurIPS 2020
- **Journal**: None
- **Summary**: The renaissance of artificial neural networks was catalysed by the success of classification models, tagged by the community with the broader term supervised learning. The extraordinary results gave rise to a hype loaded with ambitious promises and overstatements. Soon the community realised that the success owed much to the availability of thousands of labelled examples and supervised learning went, for many, from glory to shame: Some criticised deep learning as a whole and others proclaimed that the way forward had to be alternatives to supervised learning: predictive, unsupervised, semi-supervised and, more recently, self-supervised learning. However, all these seem brand names, rather than actual categories of a theoretically grounded taxonomy. Moreover, the call to banish supervised learning was motivated by the questionable claim that humans learn with little or no supervision and are capable of robust out-of-distribution generalisation. Here, we review insights about learning and supervision in nature, revisit the notion that learning and generalisation are not possible without supervision or inductive biases and argue that we will make better progress if we just call it by its name.



### Matching Distributions via Optimal Transport for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.03790v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03790v2)
- **Published**: 2020-12-04 11:15:14+00:00
- **Updated**: 2021-10-21 22:13:54+00:00
- **Authors**: Fariborz Taherkhani, Hadi Kazemi, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) approaches have been an influential framework for the usage of unlabeled data when there is not a sufficient amount of labeled data available over the course of training. SSL methods based on Convolutional Neural Networks (CNNs) have recently provided successful results on standard benchmark tasks such as image classification. In this work, we consider the general setting of SSL problem where the labeled and unlabeled data come from the same underlying probability distribution. We propose a new approach that adopts an Optimal Transport (OT) technique serving as a metric of similarity between discrete empirical probability measures to provide pseudo-labels for the unlabeled data, which can then be used in conjunction with the initial labeled data to train the CNN model in an SSL manner. We have evaluated and compared our proposed method with state-of-the-art SSL algorithms on standard datasets to demonstrate the superiority and effectiveness of our SSL algorithm.



### F2Net: Learning to Focus on the Foreground for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.02534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02534v1)
- **Published**: 2020-12-04 11:30:50+00:00
- **Updated**: 2020-12-04 11:30:50+00:00
- **Authors**: Daizong Liu, Dongdong Yu, Changhu Wang, Pan Zhou
- **Comment**: Accepted by AAAI2021
- **Journal**: None
- **Summary**: Although deep learning based methods have achieved great progress in unsupervised video object segmentation, difficult scenarios (e.g., visual similarity, occlusions, and appearance changing) are still not well-handled. To alleviate these issues, we propose a novel Focus on Foreground Network (F2Net), which delves into the intra-inter frame details for the foreground objects and thus effectively improve the segmentation performance. Specifically, our proposed network consists of three main parts: Siamese Encoder Module, Center Guiding Appearance Diffusion Module, and Dynamic Information Fusion Module. Firstly, we take a siamese encoder to extract the feature representations of paired frames (reference frame and current frame). Then, a Center Guiding Appearance Diffusion Module is designed to capture the inter-frame feature (dense correspondences between reference frame and current frame), intra-frame feature (dense correspondences in current frame), and original semantic feature of current frame. Specifically, we establish a Center Prediction Branch to predict the center location of the foreground object in current frame and leverage the center point information as spatial guidance prior to enhance the inter-frame and intra-frame feature extraction, and thus the feature representation considerably focus on the foreground objects. Finally, we propose a Dynamic Information Fusion Module to automatically select relatively important features through three aforementioned different level features. Extensive experiments on DAVIS2016, Youtube-object, and FBMS datasets show that our proposed F2Net achieves the state-of-the-art performance with significant improvement.



### Crop Classification under Varying Cloud Cover with Neural Ordinary Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2012.02542v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02542v2)
- **Published**: 2020-12-04 11:56:50+00:00
- **Updated**: 2021-08-16 10:47:19+00:00
- **Authors**: Nando Metzger, Mehmet Ozgur Turkoglu, Stefano D'Aronco, Jan Dirk Wegner, Konrad Schindler
- **Comment**: None
- **Journal**: None
- **Summary**: Optical satellite sensors cannot see the Earth's surface through clouds. Despite the periodic revisit cycle, image sequences acquired by Earth observation satellites are therefore irregularly sampled in time. State-of-the-art methods for crop classification (and other time series analysis tasks) rely on techniques that implicitly assume regular temporal spacing between observations, such as recurrent neural networks (RNNs). We propose to use neural ordinary differential equations (NODEs) in combination with RNNs to classify crop types in irregularly spaced image sequences. The resulting ODE-RNN models consist of two steps: an update step, where a recurrent unit assimilates new input data into the model's hidden state; and a prediction step, in which NODE propagates the hidden state until the next observation arrives. The prediction step is based on a continuous representation of the latent dynamics, which has several advantages. At the conceptual level, it is a more natural way to describe the mechanisms that govern the phenological cycle. From a practical point of view, it makes it possible to sample the system state at arbitrary points in time, such that one can integrate observations whenever they are available, and extrapolate beyond the last observation. Our experiments show that ODE-RNN indeed improves classification accuracy over common baselines such as LSTM, GRU, and temporal convolution. The gains are most prominent in the challenging scenario where only few observations are available (i.e., frequent cloud cover). Moreover, we show that the ability to extrapolate translates to better classification performance early in the season, which is important for forecasting.



### Boosting offline handwritten text recognition in historical documents with few labeled lines
- **Arxiv ID**: http://arxiv.org/abs/2012.02544v1
- **DOI**: 10.1109/ACCESS.2021.3082689
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02544v1)
- **Published**: 2020-12-04 11:59:35+00:00
- **Updated**: 2020-12-04 11:59:35+00:00
- **Authors**: José Carlos Aradillas, Juan José Murillo-Fuentes, Pablo M. Olmos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we face the problem of offline handwritten text recognition (HTR) in historical documents when few labeled samples are available and some of them contain errors in the train set. Three main contributions are developed. First we analyze how to perform transfer learning (TL) from a massive database to a smaller historical database, analyzing which layers of the model need a fine-tuning process. Second, we analyze methods to efficiently combine TL and data augmentation (DA). Finally, an algorithm to mitigate the effects of incorrect labelings in the training set is proposed. The methods are analyzed over the ICFHR 2018 competition database, Washington and Parzival. Combining all these techniques, we demonstrate a remarkable reduction of CER (up to 6% in some cases) in the test set with little complexity overhead.



### Critical Evaluation of Deep Neural Networks for Wrist Fracture Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.02577v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2012.02577v2)
- **Published**: 2020-12-04 13:35:36+00:00
- **Updated**: 2021-03-05 08:32:54+00:00
- **Authors**: Abu Mohammed Raisuddin, Elias Vaattovaara, Mika Nevalainen, Marko Nikki, Elina Järvenpää, Kaisa Makkonen, Pekka Pinola, Tuula Palsio, Arttu Niemensivu, Osmo Tervonen, Aleksei Tiulpin
- **Comment**: None
- **Journal**: None
- **Summary**: Wrist Fracture is the most common type of fracture with a high incidence rate. Conventional radiography (i.e. X-ray imaging) is used for wrist fracture detection routinely, but occasionally fracture delineation poses issues and an additional confirmation by computed tomography (CT) is needed for diagnosis. Recent advances in the field of Deep Learning (DL), a subfield of Artificial Intelligence (AI), have shown that wrist fracture detection can be automated using Convolutional Neural Networks. However, previous studies did not pay close attention to the difficult cases which can only be confirmed via CT imaging. In this study, we have developed and analyzed a state-of-the-art DL-based pipeline for wrist (distal radius) fracture detection -- DeepWrist, and evaluated it against one general population test set, and one challenging test set comprising only cases requiring confirmation by CT. Our results reveal that a typical state-of-the-art approach, such as DeepWrist, while having a near-perfect performance on the general independent test set, has a substantially lower performance on the challenging test set -- average precision of 0.99 (0.99-0.99) vs 0.64 (0.46-0.83), respectively. Similarly, the area under the ROC curve was of 0.99 (0.98-0.99) vs 0.84 (0.72-0.93), respectively. Our findings highlight the importance of a meticulous analysis of DL-based models before clinical use, and unearth the need for more challenging settings for testing medical AI systems.



### A high performance approach to detecting small targets in long range low quality infrared videos
- **Arxiv ID**: http://arxiv.org/abs/2012.02579v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02579v1)
- **Published**: 2020-12-04 13:36:49+00:00
- **Updated**: 2020-12-04 13:36:49+00:00
- **Authors**: Chiman Kwan, Bence Budavari
- **Comment**: Submitted to Journal of Signal, Image and Video Processing
- **Journal**: None
- **Summary**: Since targets are small in long range infrared (IR) videos, it is challenging to accurately detect targets in those videos. In this paper, we propose a high performance approach to detecting small targets in long range and low quality infrared videos. Our approach consists of a video resolution enhancement module, a proven small target detector based on local intensity and gradient (LIG), a connected component (CC) analysis module, and a track association module to connect detections from multiple frames. Extensive experiments using actual mid-wave infrared (MWIR) videos in ranges between 3500 m and 5000 m from a benchmark dataset clearly demonstrated the efficacy of the proposed approach.



### Towards Good Practices of U-Net for Traffic Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2012.02598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02598v1)
- **Published**: 2020-12-04 13:54:49+00:00
- **Updated**: 2020-12-04 13:54:49+00:00
- **Authors**: Jingwei Xu, Jianjin Zhang, Zhiyu Yao, Yunbo Wang
- **Comment**: Code is available at
  \<https://github.com/ZJianjin/Traffic4cast2020_LDS>
- **Journal**: None
- **Summary**: This technical report presents a solution for the 2020 Traffic4Cast Challenge. We consider the traffic forecasting problem as a future frame prediction task with relatively weak temporal dependencies (might be due to stochastic urban traffic dynamics) and strong prior knowledge, \textit{i.e.}, the roadmaps of the cities. For these reasons, we use the U-Net as the backbone model, and we propose a roadmap generation method to make the predicted traffic flows more rational. Meanwhile, we use a fine-tuning strategy based on the validation set to prevent overfitting, which effectively improves the prediction results. At the end of this report, we further discuss several approaches that we have considered or could be explored in future work: (1) harnessing inherent data patterns, such as seasonality; (2) distilling and transferring common knowledge between different cities. We also analyze the validity of the evaluation metric.



### Prediction of Lane Number Using Results From Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.02604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02604v1)
- **Published**: 2020-12-04 14:01:00+00:00
- **Updated**: 2020-12-04 14:01:00+00:00
- **Authors**: Panumate Chetprayoon, Fumihiko Takahashi, Yusuke Uchida
- **Comment**: GCCE 2020
- **Journal**: None
- **Summary**: The lane number that the vehicle is traveling in is a key factor in intelligent vehicle fields. Many lane detection algorithms were proposed and if we can perfectly detect the lanes, we can directly calculate the lane number from the lane detection results. However, in fact, lane detection algorithms sometimes underperform. Therefore, we propose a new approach for predicting the lane number, where we combine the drive recorder image with the lane detection results to predict the lane number. Experiments on our own dataset confirmed that our approach delivered outstanding results without significantly increasing computational cost.



### Effective Label Propagation for Discriminative Semi-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2012.02621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02621v1)
- **Published**: 2020-12-04 14:28:19+00:00
- **Updated**: 2020-12-04 14:28:19+00:00
- **Authors**: Zhiyong Huang, Kekai Sheng, Weiming Dong, Xing Mei, Chongyang Ma, Feiyue Huang, Dengwen Zhou, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised domain adaptation (SSDA) methods have demonstrated great potential in large-scale image classification tasks when massive labeled data are available in the source domain but very few labeled samples are provided in the target domain. Existing solutions usually focus on feature alignment between the two domains while paying little attention to the discrimination capability of learned representations in the target domain. In this paper, we present a novel and effective method, namely Effective Label Propagation (ELP), to tackle this problem by using effective inter-domain and intra-domain semantic information propagation. For inter-domain propagation, we propose a new cycle discrepancy loss to encourage consistency of semantic information between the two domains. For intra-domain propagation, we propose an effective self-training strategy to mitigate the noises in pseudo-labeled target domain data and improve the feature discriminability in the target domain. As a general method, our ELP can be easily applied to various domain adaptation approaches and can facilitate their feature discrimination in the target domain. Experiments on Office-Home and DomainNet benchmarks show ELP consistently improves the classification accuracy of mainstream SSDA methods by 2%~3%. Additionally, ELP also improves the performance of UDA methods as well (81.5% vs 86.1%), based on UDA experiments on the VisDA-2017 benchmark. Our source code and pre-trained models will be released soon.



### Global Context Aware RCNN for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.02637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02637v1)
- **Published**: 2020-12-04 14:56:46+00:00
- **Updated**: 2020-12-04 14:56:46+00:00
- **Authors**: Wenchao Zhang, Chong Fu, Haoyu Xie, Mai Zhu, Ming Tie, Junxin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: RoIPool/RoIAlign is an indispensable process for the typical two-stage object detection algorithm, it is used to rescale the object proposal cropped from the feature pyramid to generate a fixed size feature map. However, these cropped feature maps of local receptive fields will heavily lose global context information. To tackle this problem, we propose a novel end-to-end trainable framework, called Global Context Aware (GCA) RCNN, aiming at assisting the neural network in strengthening the spatial correlation between the background and the foreground by fusing global context information. The core component of our GCA framework is a context aware mechanism, in which both global feature pyramid and attention strategies are used for feature extraction and feature refinement, respectively. Specifically, we leverage the dense connection to improve the information flow of the global context at different stages in the top-down process of FPN, and further use the attention mechanism to refine the global context at each level in the feature pyramid. In the end, we also present a lightweight version of our method, which only slightly increases model complexity and computational burden. Experimental results on COCO benchmark dataset demonstrate the significant advantages of our approach.



### Rethinking movie genre classification with fine-grained semantic clustering
- **Arxiv ID**: http://arxiv.org/abs/2012.02639v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.02639v3)
- **Published**: 2020-12-04 14:58:31+00:00
- **Updated**: 2021-01-20 16:46:09+00:00
- **Authors**: Edward Fish, Jon Weinbren, Andrew Gilbert
- **Comment**: None
- **Journal**: None
- **Summary**: Movie genre classification is an active research area in machine learning. However, due to the limited labels available, there can be large semantic variations between movies within a single genre definition. We expand these 'coarse' genre labels by identifying 'fine-grained' semantic information within the multi-modal content of movies. By leveraging pre-trained 'expert' networks, we learn the influence of different combinations of modes for multi-label genre classification. Using a contrastive loss, we continue to fine-tune this 'coarse' genre classification network to identify high-level intertextual similarities between the movies across all genre labels. This leads to a more 'fine-grained' and detailed clustering, based on semantic similarities while still retaining some genre information. Our approach is demonstrated on a newly introduced multi-modal 37,866,450 frame, 8,800 movie trailer dataset, MMX-Trailer-20, which includes pre-computed audio, location, motion, and image embeddings.



### Predicting Emotions Perceived from Sounds
- **Arxiv ID**: http://arxiv.org/abs/2012.02643v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2012.02643v1)
- **Published**: 2020-12-04 15:01:59+00:00
- **Updated**: 2020-12-04 15:01:59+00:00
- **Authors**: Faranak Abri, Luis Felipe Gutiérrez, Akbar Siami Namin, David R. W. Sears, Keith S. Jones
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Sonification is the science of communication of data and events to users through sounds. Auditory icons, earcons, and speech are the common auditory display schemes utilized in sonification, or more specifically in the use of audio to convey information. Once the captured data are perceived, their meanings, and more importantly, intentions can be interpreted more easily and thus can be employed as a complement to visualization techniques. Through auditory perception it is possible to convey information related to temporal, spatial, or some other context-oriented information. An important research question is whether the emotions perceived from these auditory icons or earcons are predictable in order to build an automated sonification platform. This paper conducts an experiment through which several mainstream and conventional machine learning algorithms are developed to study the prediction of emotions perceived from sounds. To do so, the key features of sounds are captured and then are modeled using machine learning algorithms using feature reduction techniques. We observe that it is possible to predict perceived emotions with high accuracy. In particular, the regression based on Random Forest demonstrated its superiority compared to other machine learning algorithms.



### Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/2012.02646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02646v2)
- **Published**: 2020-12-04 15:09:35+00:00
- **Updated**: 2021-10-23 00:24:26+00:00
- **Authors**: Songyang Zhang, Houwen Peng, Jianlong Fu, Yijuan Lu, Jiebo Luo
- **Comment**: Accepted By TPAMI. arXiv admin note: text overlap with
  arXiv:1912.03590
- **Journal**: None
- **Summary**: We address the problem of retrieving a specific moment from an untrimmed video by natural language. It is a challenging problem because a target moment may take place in the context of other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they do not fully consider the temporal contexts between temporal moments. In this paper, we model the temporal context between video moments by a set of predefined two-dimensional maps under different temporal scales. For each map, one dimension indicates the starting time of a moment and the other indicates the duration. These 2D temporal maps can cover diverse video moments with different lengths, while representing their adjacent contexts at different temporal scales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal Adjacent Network (MS-2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal contexts at each scale, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed MS-2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our MS-2D-TAN outperforms the state of the art.



### Detecting 32 Pedestrian Attributes for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2012.02647v2
- **DOI**: 10.1109/TITS.2021.3107587
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02647v2)
- **Published**: 2020-12-04 15:10:12+00:00
- **Updated**: 2021-08-27 17:59:26+00:00
- **Authors**: Taylor Mordan, Matthieu Cord, Patrick Pérez, Alexandre Alahi
- **Comment**: Accepted to IEEE Transactions on Intelligent Transportation Systems
  (T-ITS). Code available at
  https://github.com/vita-epfl/detection-attributes-fields
- **Journal**: None
- **Summary**: Pedestrians are arguably one of the most safety-critical road users to consider for autonomous vehicles in urban areas. In this paper, we address the problem of jointly detecting pedestrians and recognizing 32 pedestrian attributes from a single image. These encompass visual appearance and behavior, and also include the forecasting of road crossing, which is a main safety concern. For this, we introduce a Multi-Task Learning (MTL) model relying on a composite field framework, which achieves both goals in an efficient way. Each field spatially locates pedestrian instances and aggregates attribute predictions over them. This formulation naturally leverages spatial context, making it well suited to low resolution scenarios such as autonomous driving. By increasing the number of attributes jointly learned, we highlight an issue related to the scales of gradients, which arises in MTL with numerous tasks. We solve it by normalizing the gradients coming from different objective functions when they join at the fork in the network architecture during the backward pass, referred to as fork-normalization. Experimental validation is performed on JAAD, a dataset providing numerous attributes for pedestrian analysis from autonomous vehicles, and shows competitive detection and attribute recognition results, as well as a more stable MTL training.



### Accelerating Road Sign Ground Truth Construction with Knowledge Graph and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.02672v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02672v1)
- **Published**: 2020-12-04 15:42:08+00:00
- **Updated**: 2020-12-04 15:42:08+00:00
- **Authors**: Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin
- **Comment**: 12 pages, 5 figures
- **Journal**: Computing Conference 2021
- **Summary**: Having a comprehensive, high-quality dataset of road sign annotation is critical to the success of AI-based Road Sign Recognition (RSR) systems. In practice, annotators often face difficulties in learning road sign systems of different countries; hence, the tasks are often time-consuming and produce poor results. We propose a novel approach using knowledge graphs and a machine learning algorithm - variational prototyping-encoder (VPE) - to assist human annotators in classifying road signs effectively. Annotators can query the Road Sign Knowledge Graph using visual attributes and receive closest matching candidates suggested by the VPE model. The VPE model uses the candidates from the knowledge graph and a real sign image patch as inputs. We show that our knowledge graph approach can reduce sign search space by 98.9%. Furthermore, with VPE, our system can propose the correct single candidate for 75% of signs in the tested datasets, eliminating the human search effort entirely in those cases.



### Isometric Multi-Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2012.02689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02689v1)
- **Published**: 2020-12-04 15:58:34+00:00
- **Updated**: 2020-12-04 15:58:34+00:00
- **Authors**: Maolin Gao, Zorah Lähner, Johan Thunberg, Daniel Cremers, Florian Bernard
- **Comment**: None
- **Journal**: None
- **Summary**: Finding correspondences between shapes is a fundamental problem in computer vision and graphics, which is relevant for many applications, including 3D reconstruction, object tracking, and style transfer. The vast majority of correspondence methods aim to find a solution between pairs of shapes, even if multiple instances of the same class are available. While isometries are often studied in shape correspondence problems, they have not been considered explicitly in the multi-matching setting. This paper closes this gap by proposing a novel optimisation formulation for isometric multi-shape matching. We present a suitable optimisation algorithm for solving our formulation and provide a convergence and complexity analysis. Our algorithm obtains multi-matchings that are by construction provably cycle-consistent. We demonstrate the superior performance of our method on various datasets and set the new state-of-the-art in isometric multi-shape matching.



### Super-Selfish: Self-Supervised Learning on Images with PyTorch
- **Arxiv ID**: http://arxiv.org/abs/2012.02706v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02706v2)
- **Published**: 2020-12-04 16:30:02+00:00
- **Updated**: 2020-12-07 07:22:08+00:00
- **Authors**: Nicolas Wagner, Anirban Mukhopadhyay
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Super-Selfish is an easy to use PyTorch framework for image-based self-supervised learning. Features can be learned with 13 algorithms that span from simple classification to more complex state of theart contrastive pretext tasks. The framework is easy to use and allows for pretraining any PyTorch neural network with only two lines of code. Simultaneously, full flexibility is maintained through modular design choices. The code can be found at https://github.com/MECLabTUDA/Super_Selfish and installed using pip install super-selfish.



### Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.02733v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02733v2)
- **Published**: 2020-12-04 17:26:24+00:00
- **Updated**: 2021-04-07 08:44:47+00:00
- **Authors**: Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai Xiong, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning based on instance discrimination has shown remarkable progress. In particular, contrastive learning, which regards each image as well as its augmentations as an individual class and tries to distinguish them from all other images, has been verified effective for representation learning. However, pushing away two images that are de facto similar is suboptimal for general representation. In this paper, we propose a hierarchical semantic alignment strategy via expanding the views generated by a single image to \textbf{Cross-samples and Multi-level} representation, and models the invariance to semantically similar images in a hierarchical way. This is achieved by extending the contrastive loss to allow for multiple positives per anchor, and explicitly pulling semantically similar images/patches together at different layers of the network. Our method, termed as CsMl, has the ability to integrate multi-level visual representations across samples in a robust way. CsMl is applicable to current contrastive learning based methods and consistently improves the performance. Notably, using the moco as an instantiation, CsMl achieves a \textbf{76.6\% }top-1 accuracy with linear evaluation using ResNet-50 as backbone, and \textbf{66.7\%} and \textbf{75.1\%} top-1 accuracy with only 1\% and 10\% labels, respectively. \textbf{All these numbers set the new state-of-the-art.}



### Ultrasound Scatterer Density Classification Using Convolutional Neural Networks by Exploiting Patch Statistics
- **Arxiv ID**: http://arxiv.org/abs/2012.02738v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02738v1)
- **Published**: 2020-12-04 17:36:57+00:00
- **Updated**: 2020-12-04 17:36:57+00:00
- **Authors**: Ali K. Z. Tehrani, Mina Amiri, Ivan M. Rosado-Mendez, Timothy J. Hall, Hassan Rivaz
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Quantitative ultrasound (QUS) can reveal crucial information on tissue properties such as scatterer density. If the scatterer density per resolution cell is above or below 10, the tissue is considered as fully developed speckle (FDS) or low-density scatterers (LDS), respectively. Conventionally, the scatterer density has been classified using estimated statistical parameters of the amplitude of backscattered echoes. However, if the patch size is small, the estimation is not accurate. These parameters are also highly dependent on imaging settings. In this paper, we propose a convolutional neural network (CNN) architecture for QUS, and train it using simulation data. We further improve the network performance by utilizing patch statistics as additional input channels. We evaluate the network using simulation data, experimental phantoms and in vivo data. We also compare our proposed network with different classic and deep learning models, and demonstrate its superior performance in classification of tissues with different scatterer density values. The results also show that the proposed network is able to work with different imaging parameters with no need for a reference phantom. This work demonstrates the potential of CNNs in classifying scatterer density in ultrasound images.



### SMPLy Benchmarking 3D Human Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2012.02743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02743v1)
- **Published**: 2020-12-04 17:48:32+00:00
- **Updated**: 2020-12-04 17:48:32+00:00
- **Authors**: Vincent Leroy, Philippe Weinzaepfel, Romain Brégier, Hadrien Combaluzier, Grégory Rogez
- **Comment**: 3DV 2020 Oral presentation
- **Journal**: None
- **Summary**: Predicting 3D human pose from images has seen great recent improvements. Novel approaches that can even predict both pose and shape from a single input image have been introduced, often relying on a parametric model of the human body such as SMPL. While qualitative results for such methods are often shown for images captured in-the-wild, a proper benchmark in such conditions is still missing, as it is cumbersome to obtain ground-truth 3D poses elsewhere than in a motion capture room. This paper presents a pipeline to easily produce and validate such a dataset with accurate ground-truth, with which we benchmark recent 3D human pose estimation methods in-the-wild. We make use of the recently introduced Mannequin Challenge dataset which contains in-the-wild videos of people frozen in action like statues and leverage the fact that people are static and the camera moving to accurately fit the SMPL model on the sequences. A total of 24,428 frames with registered body models are then selected from 567 scenes at almost no cost, using only online RGB videos. We benchmark state-of-the-art SMPL-based human pose estimation methods on this dataset. Our results highlight that challenges remain, in particular for difficult poses or for scenes where the persons are partially truncated or occluded.



### An Empirical Method to Quantify the Peripheral Performance Degradation in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.02749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02749v1)
- **Published**: 2020-12-04 18:00:47+00:00
- **Updated**: 2020-12-04 18:00:47+00:00
- **Authors**: Calden Wloka, John K. Tsotsos
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: When applying a convolutional kernel to an image, if the output is to remain the same size as the input then some form of padding is required around the image boundary, meaning that for each layer of convolution in a convolutional neural network (CNN), a strip of pixels equal to the half-width of the kernel size is produced with a non-veridical representation. Although most CNN kernels are small to reduce the parameter load of a network, this non-veridical area compounds with each convolutional layer. The tendency toward deeper and deeper networks combined with stride-based down-sampling means that the propagation of this region can end up covering a non-negligable portion of the image. Although this issue with convolutions has been well acknowledged over the years, the impact of this degraded peripheral representation on modern network behavior has not been fully quantified. What are the limits of translation invariance? Does image padding successfully mitigate the issue, or is performance affected as an object moves between the image border and center? Using Mask R-CNN as an experimental model, we design a dataset and methodology to quantify the spatial dependency of network performance. Our dataset is constructed by inserting objects into high resolution backgrounds, thereby allowing us to crop sub-images which place target objects at specific locations relative to the image border. By probing the behaviour of Mask R-CNN across a selection of target locations, we see clear patterns of performance degredation near the image boundary, and in particular in the image corners. Quantifying both the extent and magnitude of this spatial anisotropy in network performance is important for the deployment of deep networks into unconstrained and realistic environments in which the location of objects or regions of interest are not guaranteed to be well localized within a given image.



### Statistical inference of the inter-sample Dice distribution for discriminative CNN brain lesion segmentation models
- **Arxiv ID**: http://arxiv.org/abs/2012.02755v2
- **DOI**: 10.5220/0010286201680173
- **Categories**: **eess.IV**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2012.02755v2)
- **Published**: 2020-12-04 18:18:24+00:00
- **Updated**: 2021-02-19 14:45:24+00:00
- **Authors**: Kevin Raina
- **Comment**: Proceedings of the 14th International Joint Conference on Biomedical
  Engineering Systems and Technologies, Volume 2: BIOIMAGING 2021
- **Journal**: None
- **Summary**: Discriminative convolutional neural networks (CNNs), for which a voxel-wise conditional Multinoulli distribution is assumed, have performed well in many brain lesion segmentation tasks. For a trained discriminative CNN to be used in clinical practice, the patient's radiological features are inputted into the model, in which case a conditional distribution of segmentations is produced. Capturing the uncertainty of the predictions can be useful in deciding whether to abandon a model, or choose amongst competing models. In practice, however, we never know the ground truth segmentation, and therefore can never know the true model variance. In this work, segmentation sampling on discriminative CNNs is used to assess a trained model's robustness by analyzing the inter-sample Dice distribution on a new patient solely based on their magnetic resonance (MR) images. Furthermore, by demonstrating the inter-sample Dice observations are independent and identically distributed with a finite mean and variance under certain conditions, a rigorous confidence based decision rule is proposed to decide whether to reject or accept a CNN model for a particular patient. Applied to the ISLES 2015 (SISS) dataset, the model identified 7 predictions as non-robust, and the average Dice coefficient calculated on the remaining brains improved by 12 percent.



### Learning Equivariant Representations
- **Arxiv ID**: http://arxiv.org/abs/2012.02771v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02771v1)
- **Published**: 2020-12-04 18:46:17+00:00
- **Updated**: 2020-12-04 18:46:17+00:00
- **Authors**: Carlos Esteves
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art deep learning systems often require large amounts of data and computation. For this reason, leveraging known or unknown structure of the data is paramount. Convolutional neural networks (CNNs) are successful examples of this principle, their defining characteristic being the shift-equivariance. By sliding a filter over the input, when the input shifts, the response shifts by the same amount, exploiting the structure of natural images where semantic content is independent of absolute pixel positions. This property is essential to the success of CNNs in audio, image and video recognition tasks. In this thesis, we extend equivariance to other kinds of transformations, such as rotation and scaling. We propose equivariant models for different transformations defined by groups of symmetries. The main contributions are (i) polar transformer networks, achieving equivariance to the group of similarities on the plane, (ii) equivariant multi-view networks, achieving equivariance to the group of symmetries of the icosahedron, (iii) spherical CNNs, achieving equivariance to the continuous 3D rotation group, (iv) cross-domain image embeddings, achieving equivariance to 3D rotations for 2D inputs, and (v) spin-weighted spherical CNNs, generalizing the spherical CNNs and achieving equivariance to 3D rotations for spherical vector fields. Applications include image classification, 3D shape classification and retrieval, panoramic image classification and segmentation, shape alignment and pose estimation. What these models have in common is that they leverage symmetries in the data to reduce sample and model complexity and improve generalization performance. The advantages are more significant on (but not limited to) challenging tasks where data is limited or input perturbations such as arbitrary rotations are present.



### Representation Based Complexity Measures for Predicting Generalization in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.02775v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02775v1)
- **Published**: 2020-12-04 18:53:44+00:00
- **Updated**: 2020-12-04 18:53:44+00:00
- **Authors**: Parth Natekar, Manik Sharma
- **Comment**: Winning Solution of the NeurIPS 2020 Competition on Predicting
  Generalization in Deep Learning
- **Journal**: None
- **Summary**: Deep Neural Networks can generalize despite being significantly overparametrized. Recent research has tried to examine this phenomenon from various view points and to provide bounds on the generalization error or measures predictive of the generalization gap based on these viewpoints, such as norm-based, PAC-Bayes based, and margin-based analysis. In this work, we provide an interpretation of generalization from the perspective of quality of internal representations of deep neural networks, based on neuroscientific theories of how the human visual system creates invariant and untangled object representations. Instead of providing theoretical bounds, we demonstrate practical complexity measures which can be computed ad-hoc to uncover generalization behaviour in deep models. We also provide a detailed description of our solution that won the NeurIPS competition on Predicting Generalization in Deep Learning held at NeurIPS 2020. An implementation of our solution is available at https://github.com/parthnatekar/pgdl.



### Learning to Fuse Asymmetric Feature Maps in Siamese Trackers
- **Arxiv ID**: http://arxiv.org/abs/2012.02776v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02776v2)
- **Published**: 2020-12-04 18:55:53+00:00
- **Updated**: 2021-03-31 03:57:46+00:00
- **Authors**: Wencheng Han, Xingping Dong, Fahad Shahbaz Khan, Ling Shao, Jianbing Shen
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Recently, Siamese-based trackers have achieved promising performance in visual tracking. Most recent Siamese-based trackers typically employ a depth-wise cross-correlation (DW-XCorr) to obtain multi-channel correlation information from the two feature maps (target and search region). However, DW-XCorr has several limitations within Siamese-based tracking: it can easily be fooled by distractors, has fewer activated channels, and provides weak discrimination of object boundaries. Further, DW-XCorr is a handcrafted parameter-free module and cannot fully benefit from offline learning on large-scale data. We propose a learnable module, called the asymmetric convolution (ACM), which learns to better capture the semantic correlation information in offline training on large-scale data. Different from DW-XCorr and its predecessor(XCorr), which regard a single feature map as the convolution kernel, our ACM decomposes the convolution operation on a concatenated feature map into two mathematically equivalent operations, thereby avoiding the need for the feature maps to be of the same size (width and height)during concatenation. Our ACM can incorporate useful prior information, such as bounding-box size, with standard visual features. Furthermore, ACM can easily be integrated into existing Siamese trackers based on DW-XCorror XCorr. To demonstrate its generalization ability, we integrate ACM into three representative trackers: SiamFC, SiamRPN++, and SiamBAN. Our experiments reveal the benefits of the proposed ACM, which outperforms existing methods on six tracking benchmarks. On the LaSOT test set, our ACM-based tracker obtains a significant improvement of 5.8% in terms of success (AUC), over the baseline.



### Few-shot Image Generation with Elastic Weight Consolidation
- **Arxiv ID**: http://arxiv.org/abs/2012.02780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02780v1)
- **Published**: 2020-12-04 18:57:13+00:00
- **Updated**: 2020-12-04 18:57:13+00:00
- **Authors**: Yijun Li, Richard Zhang, Jingwan Lu, Eli Shechtman
- **Comment**: Accepted by NeurIPS 2020, see
  https://yijunmaverick.github.io/publications/ewc/
- **Journal**: None
- **Summary**: Few-shot image generation seeks to generate more data of a given domain, with only few available training examples. As it is unreasonable to expect to fully infer the distribution from just a few observations (e.g., emojis), we seek to leverage a large, related source domain as pretraining (e.g., human faces). Thus, we wish to preserve the diversity of the source domain, while adapting to the appearance of the target. We adapt a pretrained model, without introducing any additional parameters, to the few examples of the target domain. Crucially, we regularize the changes of the weights during this adaptation, in order to best preserve the information of the source dataset, while fitting the target. We demonstrate the effectiveness of our algorithm by generating high-quality results of different target domains, including those with extremely few examples (e.g., <10). We also analyze the performance of our method with respect to some important factors, such as the number of examples and the dissimilarity between the source and target domain.



### Batch Group Normalization
- **Arxiv ID**: http://arxiv.org/abs/2012.02782v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02782v2)
- **Published**: 2020-12-04 18:57:52+00:00
- **Updated**: 2020-12-09 01:26:51+00:00
- **Authors**: Xiao-Yun Zhou, Jiacheng Sun, Nanyang Ye, Xu Lan, Qijun Luo, Bo-Lin Lai, Pedro Esperanca, Guang-Zhong Yang, Zhenguo Li
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming to train. Normalization is one of the effective solutions. Among previous normalization methods, Batch Normalization (BN) performs well at medium and large batch sizes and is with good generalizability to multiple vision tasks, while its performance degrades significantly at small batch sizes. In this paper, we find that BN saturates at extreme large batch sizes, i.e., 128 images per worker, i.e., GPU, as well and propose that the degradation/saturation of BN at small/extreme large batch sizes is caused by noisy/confused statistic calculation. Hence without adding new trainable parameters, using multiple-layer or multi-iteration information, or introducing extra computation, Batch Group Normalization (BGN) is proposed to solve the noisy/confused statistic calculation of BN at small/extreme large batch sizes with introducing the channel, height and width dimension to compensate. The group technique in Group Normalization (GN) is used and a hyper-parameter G is used to control the number of feature instances used for statistic calculation, hence to offer neither noisy nor confused statistic for different batch sizes. We empirically demonstrate that BGN consistently outperforms BN, Instance Normalization (IN), Layer Normalization (LN), GN, and Positional Normalization (PN), across a wide spectrum of vision tasks, including image classification, Neural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL) and Unsupervised Domain Adaptation (UDA), indicating its good performance, robust stability to batch size and wide generalizability. For example, for training ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1 accuracy of 66.512% while BGN achieves 76.096% with notable improvement.



### Neural Dynamic Policies for End-to-End Sensorimotor Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.02788v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.02788v1)
- **Published**: 2020-12-04 18:59:32+00:00
- **Updated**: 2020-12-04 18:59:32+00:00
- **Authors**: Shikhar Bahl, Mustafa Mukadam, Abhinav Gupta, Deepak Pathak
- **Comment**: NeurIPS 2020 (Spotlight). Code and videos at
  https://shikharbahl.github.io/neural-dynamic-policies/
- **Journal**: None
- **Summary**: The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decisions individually at each timestep in training, and hence, limits the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed the structure of a dynamical system into deep neural network-based policies by reparameterizing action spaces via second-order differential equations. We propose Neural Dynamic Policies (NDPs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where actions represent the raw control space. The embedded structure allows end-to-end policy learning for both reinforcement and imitation learning setups. We show that NDPs outperform the prior state-of-the-art in terms of either efficiency or performance across several robotic control tasks for both imitation and reinforcement learning setups. Project video and code are available at https://shikharbahl.github.io/neural-dynamic-policies/



### Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment
- **Arxiv ID**: http://arxiv.org/abs/2012.02813v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02813v1)
- **Published**: 2020-12-04 19:27:26+00:00
- **Updated**: 2020-12-04 19:27:26+00:00
- **Authors**: Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov
- **Comment**: None
- **Journal**: None
- **Summary**: The natural world is abundant with concepts expressed via visual, acoustic, tactile, and linguistic modalities. Much of the existing progress in multimodal learning, however, focuses primarily on problems where the same set of modalities are present at train and test time, which makes learning in low-resource modalities particularly difficult. In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can (1) quickly perform new tasks in a target modality (i.e. meta-learning) and (2) doing so while being trained on a different source modality. We study a key research question: how can we ensure generalization across modalities despite using separate encoders for different source and target modalities? Our solution is based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. We study this problem on 3 classification tasks: text to image, image to audio, and text to speech. Our results demonstrate strong performance even when the new target modality has only a few (1-10) labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities.



### Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification
- **Arxiv ID**: http://arxiv.org/abs/2012.02818v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.02818v2)
- **Published**: 2020-12-04 19:50:09+00:00
- **Updated**: 2021-03-25 12:12:19+00:00
- **Authors**: Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, Isabelle Bloch
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Bayesian neural networks (BNNs) have been long considered an ideal, yet unscalable solution for improving the robustness and the predictive uncertainty of deep neural networks. While they could capture more accurately the posterior distribution of the network parameters, most BNN approaches are either limited to small networks or rely on constraining assumptions such as parameter independence. These drawbacks have enabled prominence of simple, but computationally heavy approaches such as Deep Ensembles, whose training and testing costs increase linearly with the number of networks. In this work we aim for efficient deep BNNs amenable to complex computer vision architectures, e.g. ResNet50 DeepLabV3+, and tasks, e.g. semantic segmentation, with fewer assumptions on the parameters. We achieve this by leveraging variational autoencoders (VAEs) to learn the interaction and the latent distribution of the parameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN), is compatible with the recent BatchEnsemble method, leading to highly efficient ({in terms of computation and} memory during both training and testing) ensembles. LP-BNN s attain competitive results across multiple metrics in several challenging benchmarks for image classification, semantic segmentation and out-of-distribution detection.



### MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs
- **Arxiv ID**: http://arxiv.org/abs/2012.02821v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2012.02821v2)
- **Published**: 2020-12-04 19:51:31+00:00
- **Updated**: 2021-10-06 00:48:37+00:00
- **Authors**: Fangda Han, Guoyao Hao, Ricardo Guerrero, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: Multilabel conditional image generation is a challenging problem in computer vision. In this work we propose Multi-ingredient Pizza Generator (MPG), a conditional Generative Neural Network (GAN) framework for synthesizing multilabel images. We design MPG based on a state-of-the-art GAN structure called StyleGAN2, in which we develop a new conditioning technique by enforcing intermediate feature maps to learn scalewise label information. Because of the complex nature of the multilabel image generation problem, we also regularize synthetic image by predicting the corresponding ingredients as well as encourage the discriminator to distinguish between matched image and mismatched image. To verify the efficacy of MPG, we test it on Pizza10, which is a carefully annotated multi-ingredient pizza image dataset. MPG can successfully generate photo-realist pizza images with desired ingredients. The framework can be easily extend to other multilabel image generation scenarios.



### Discovering Underground Maps from Fashion
- **Arxiv ID**: http://arxiv.org/abs/2012.02897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02897v1)
- **Published**: 2020-12-04 23:40:59+00:00
- **Updated**: 2020-12-04 23:40:59+00:00
- **Authors**: Utkarsh Mall, Kavita Bala, Tamara Berg, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: The fashion sense -- meaning the clothing styles people wear -- in a geographical region can reveal information about that region. For example, it can reflect the kind of activities people do there, or the type of crowds that frequently visit the region (e.g., tourist hot spot, student neighborhood, business center). We propose a method to automatically create underground neighborhood maps of cities by analyzing how people dress. Using publicly available images from across a city, our method finds neighborhoods with a similar fashion sense and segments the map without supervision. For 37 cities worldwide, we show promising results in creating good underground maps, as evaluated using experiments with human judges and underground map benchmarks derived from non-image data. Our approach further allows detecting distinct neighborhoods (what is the most unique region of LA?) and answering analogy questions between cities (what is the "Downtown LA" of Bogota?).



### Automated Calibration of Mobile Cameras for 3D Reconstruction of Mechanical Pipes
- **Arxiv ID**: http://arxiv.org/abs/2012.02899v1
- **DOI**: 10.1111/phor.12364
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02899v1)
- **Published**: 2020-12-04 23:41:25+00:00
- **Updated**: 2020-12-04 23:41:25+00:00
- **Authors**: Reza Maalek, Derek Lichti
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript provides a new framework for calibration of optical instruments, in particular mobile cameras, using large-scale circular black and white target fields. New methods were introduced for (i) matching targets between images; (ii) adjusting the systematic eccentricity error of target centers; and (iii) iteratively improving the calibration solution through a free-network self-calibrating bundle adjustment. It was observed that the proposed target matching effectively matched circular targets in 270 mobile phone images from a complete calibration laboratory with robustness to Type II errors. The proposed eccentricity adjustment, which requires only camera projective matrices from two views, behaved synonymous to available closed-form solutions, which require several additional object space target information a priori. Finally, specifically for the case of the mobile devices, the calibration parameters obtained using our framework was found superior compared to in-situ calibration for estimating the 3D reconstructed radius of a mechanical pipe (approximately 45% improvement).




# Arxiv Papers in cs.CV on 2020-08-09
### Model Generalization in Deep Learning Applications for Land Cover Mapping
- **Arxiv ID**: http://arxiv.org/abs/2008.10351v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.4.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.10351v3)
- **Published**: 2020-08-09 01:50:52+00:00
- **Updated**: 2021-06-17 19:04:16+00:00
- **Authors**: Lucas Hu, Caleb Robinson, Bistra Dilkina
- **Comment**: 9 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Recent work has shown that deep learning models can be used to classify land-use data from geospatial satellite imagery. We show that when these deep learning models are trained on data from specific continents/seasons, there is a high degree of variability in model performance on out-of-sample continents/seasons. This suggests that just because a model accurately predicts land-use classes in one continent or season does not mean that the model will accurately predict land-use classes in a different continent or season. We then use clustering techniques on satellite imagery from different continents to visualize the differences in landscapes that make geospatial generalization particularly difficult, and summarize our takeaways for future satellite imagery-related applications.



### Appearance-free Tripartite Matching for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.03628v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2008.03628v2)
- **Published**: 2020-08-09 02:16:44+00:00
- **Updated**: 2021-10-07 18:05:56+00:00
- **Authors**: Lijun Wang, Yanting Zhu, Jue Shi, Xiaodan Fan
- **Comment**: 36 pages, 14 figures
- **Journal**: None
- **Summary**: Multiple Object Tracking (MOT) detects the trajectories of multiple objects given an input video. It has become more and more important for various research and industry areas, such as cell tracking for biomedical research and human tracking in video surveillance. Most existing algorithms depend on the uniqueness of the object's appearance, and the dominating bipartite matching scheme ignores the speed smoothness. Although several methods have incorporated the velocity smoothness for tracking, they either fail to pursue global smooth velocity or are often trapped in local optimums. We focus on the general MOT problem regardless of the appearance and propose an appearance-free tripartite matching to avoid the irregular velocity problem of the bipartite matching. The tripartite matching is formulated as maximizing the likelihood of the state vectors constituted of the position and velocity of objects, which results in a chain-dependent structure. We resort to the dynamic programming algorithm to find such a maximum likelihood estimate. To overcome the high computational cost induced by the vast search space of dynamic programming when many objects are to be tracked, we decompose the space by the number of disappearing objects and propose a reduced-space approach by truncating the decomposition. Extensive simulations have shown the superiority and efficiency of our proposed method, and the comparisons with top methods on Cell Tracking Challenge also demonstrate our competence. We also applied our method to track the motion of natural killer cells around tumor cells in a cancer study.\footnote{The source code is available on \url{https://github.com/szcf-weiya/TriMatchMOT}



### A Review on Deep Learning Techniques for the Diagnosis of Novel Coronavirus (COVID-19)
- **Arxiv ID**: http://arxiv.org/abs/2008.04815v1
- **DOI**: 10.1109/ACCESS.2021.3058537
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.04815v1)
- **Published**: 2020-08-09 02:37:50+00:00
- **Updated**: 2020-08-09 02:37:50+00:00
- **Authors**: Md. Milon Islam, Fakhri Karray, Reda Alhajj, Jia Zeng
- **Comment**: 18 pages, 2 figures, 4 Tables
- **Journal**: IEEE Access C9:30551-30572,2021
- **Summary**: Novel coronavirus (COVID-19) outbreak, has raised a calamitous situation all over the world and has become one of the most acute and severe ailments in the past hundred years. The prevalence rate of COVID-19 is rapidly rising every day throughout the globe. Although no vaccines for this pandemic have been discovered yet, deep learning techniques proved themselves to be a powerful tool in the arsenal used by clinicians for the automatic diagnosis of COVID-19. This paper aims to overview the recently developed systems based on deep learning techniques using different medical imaging modalities like Computer Tomography (CT) and X-ray. This review specifically discusses the systems developed for COVID-19 diagnosis using deep learning techniques and provides insights on well-known data sets used to train these networks. It also highlights the data partitioning techniques and various performance measures developed by researchers in this field. A taxonomy is drawn to categorize the recent works for proper insight. Finally, we conclude by addressing the challenges associated with the use of deep learning methods for COVID-19 detection and probable future trends in this research area. This paper is intended to provide experts (medical or otherwise) and technicians with new insights into the ways deep learning techniques are used in this regard and how they potentially further works in combatting the outbreak of COVID-19.



### Encoding Structure-Texture Relation with P-Net for Anomaly Detection in Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/2008.03632v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03632v1)
- **Published**: 2020-08-09 02:59:33+00:00
- **Updated**: 2020-08-09 02:59:33+00:00
- **Authors**: Kang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Zaiwang Gu, Jiang Liu, Shenghua Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in retinal image refers to the identification of abnormality caused by various retinal diseases/lesions, by only leveraging normal images in training phase. Normal images from healthy subjects often have regular structures (e.g., the structured blood vessels in the fundus image, or structured anatomy in optical coherence tomography image). On the contrary, the diseases and lesions often destroy these structures. Motivated by this, we propose to leverage the relation between the image texture and structure to design a deep neural network for anomaly detection. Specifically, we first extract the structure of the retinal images, then we combine both the structure features and the last layer features extracted from original health image to reconstruct the original input healthy image. The image feature provides the texture information and guarantees the uniqueness of the image recovered from the structure. In the end, we further utilize the reconstructed image to extract the structure and measure the difference between structure extracted from original and the reconstructed image. On the one hand, minimizing the reconstruction difference behaves like a regularizer to guarantee that the image is corrected reconstructed. On the other hand, such structure difference can also be used as a metric for normality measurement. The whole network is termed as P-Net because it has a ``P'' shape. Extensive experiments on RESC dataset and iSee dataset validate the effectiveness of our approach for anomaly detection in retinal images. Further, our method also generalizes well to novel class discovery in retinal images and anomaly detection in real-world images.



### Forget About the LiDAR: Self-Supervised Depth Estimators with MED Probability Volumes
- **Arxiv ID**: http://arxiv.org/abs/2008.03633v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03633v2)
- **Published**: 2020-08-09 03:03:00+00:00
- **Updated**: 2020-09-26 15:07:29+00:00
- **Authors**: Juan Luis Gonzalez, Munchurl Kim
- **Comment**: Accepted to NeurIPS2020
- **Journal**: None
- **Summary**: Self-supervised depth estimators have recently shown results comparable to the supervised methods on the challenging single image depth estimation (SIDE) task, by exploiting the geometrical relations between target and reference views in the training data. However, previous methods usually learn forward or backward image synthesis, but not depth estimation, as they cannot effectively neglect occlusions between the target and the reference images. Previous works rely on rigid photometric assumptions or the SIDE network to infer depth and occlusions, resulting in limited performance. On the other hand, we propose a method to "Forget About the LiDAR" (FAL), for the training of depth estimators, with Mirrored Exponential Disparity (MED) probability volumes, from which we obtain geometrically inspired occlusion maps with our novel Mirrored Occlusion Module (MOM). Our MOM does not impose a burden on our FAL-net. Contrary to the previous methods that learn SIDE from stereo pairs by regressing disparity in the linear space, our FAL-net regresses disparity by binning it into the exponential space, which allows for better detection of distant and nearby objects. We define a two-step training strategy for our FAL-net: It is first trained for view synthesis and then fine-tuned for depth estimation with our MOM. Our FAL-net is remarkably light-weight and outperforms the previous state-of-the-art methods with 8x fewer parameters and 3x faster inference speeds on the challenging KITTI dataset. We present extensive experimental results on the KITTI, CityScapes, and Make3D datasets to verify our method's effectiveness. To the authors' best knowledge, the presented method performs the best among all the previous self-supervised methods until now.



### Augmenting Molecular Images with Vector Representations as a Featurization Technique for Drug Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.03646v1
- **DOI**: 10.1109/ICASSP40776.2020.9053425
- **Categories**: **cs.CV**, q-bio.BM
- **Links**: [PDF](http://arxiv.org/pdf/2008.03646v1)
- **Published**: 2020-08-09 04:26:16+00:00
- **Updated**: 2020-08-09 04:26:16+00:00
- **Authors**: Daniel de Marchi, Amarjit Budhiraja
- **Comment**: None
- **Journal**: ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)
- **Summary**: One of the key steps in building deep learning systems for drug classification and generation is the choice of featurization for the molecules. Previous featurization methods have included molecular images, binary strings, graphs, and SMILES strings. This paper proposes the creation of molecular images captioned with binary vectors that encode information not contained in or easily understood from a molecular image alone. Specifically, we use Morgan fingerprints, which encode higher level structural information, and MACCS keys, which encode yes or no questions about a molecules properties and structure. We tested our method on the HIV dataset published by the Pande lab, which consists of 41,127 molecules labeled by if they inhibit the HIV virus. Our final model achieved a state of the art AUC ROC on the HIV dataset, outperforming all other methods. Moreover, the model converged significantly faster than most other methods, requiring dramatically less computational power than unaugmented images.



### Accelerating Evolutionary Construction Tree Extraction via Graph Partitioning
- **Arxiv ID**: http://arxiv.org/abs/2008.03669v1
- **DOI**: 10.24132/CSRN.2018.2802.5
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03669v1)
- **Published**: 2020-08-09 06:11:33+00:00
- **Updated**: 2020-08-09 06:11:33+00:00
- **Authors**: Markus Friedrich, Sebastian Feld, Thomy Phan, Pierre-Alain Fayolle
- **Comment**: The 26th International Conference in Central Europe on Computer
  Graphics, Visualization and Computer Vision 2016 in co-operation with
  EUROGRAPHICS: University of West Bohemia, Plzen, Czech Republic May 28 - June
  1 2018, p. 29-37
- **Journal**: None
- **Summary**: Extracting a Construction Tree from potentially noisy point clouds is an important aspect of Reverse Engineering tasks in Computer Aided Design. Solutions based on algorithmic geometry impose constraints on usable model representations (e.g. quadric surfaces only) and noise robustness. Re-formulating the problem as a combinatorial optimization problem and solving it with an Evolutionary Algorithm can mitigate some of these constraints at the cost of increased computational complexity. This paper proposes a graph-based search space partitioning scheme that is able to accelerate Evolutionary Construction Tree extraction while exploiting parallelization capabilities of modern CPUs. The evaluation indicates a speed-up up to a factor of $46.6$ compared to the baseline approach while resulting tree sizes increased by $25.2\%$ to $88.6\%$.



### Feature Space Augmentation for Long-Tailed Data
- **Arxiv ID**: http://arxiv.org/abs/2008.03673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03673v1)
- **Published**: 2020-08-09 06:38:00+00:00
- **Updated**: 2020-08-09 06:38:00+00:00
- **Authors**: Peng Chu, Xiao Bian, Shaopeng Liu, Haibin Ling
- **Comment**: To be appeared in ECCV 2020
- **Journal**: None
- **Summary**: Real-world data often follow a long-tailed distribution as the frequency of each class is typically different. For example, a dataset can have a large number of under-represented classes and a few classes with more than sufficient data. However, a model to represent the dataset is usually expected to have reasonably homogeneous performances across classes. Introducing class-balanced loss and advanced methods on data re-sampling and augmentation are among the best practices to alleviate the data imbalance problem. However, the other part of the problem about the under-represented classes will have to rely on additional knowledge to recover the missing information.   In this work, we present a novel approach to address the long-tailed problem by augmenting the under-represented classes in the feature space with the features learned from the classes with ample samples. In particular, we decompose the features of each class into a class-generic component and a class-specific component using class activation maps. Novel samples of under-represented classes are then generated on the fly during training stages by fusing the class-specific features from the under-represented classes with the class-generic features from confusing classes. Our results on different datasets such as iNaturalist, ImageNet-LT, Places-LT and a long-tailed version of CIFAR have shown the state of the art performances.



### Semantic scene synthesis: Application to assistive systems
- **Arxiv ID**: http://arxiv.org/abs/2008.03685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03685v2)
- **Published**: 2020-08-09 08:13:22+00:00
- **Updated**: 2021-04-14 21:14:03+00:00
- **Authors**: Chayma Zatout, Slimane Larabi
- **Comment**: paper Not published
- **Journal**: None
- **Summary**: The aim of this work is to provide a semantic scene synthesis from a single depth image. This is used in assistive aid systems for visually impaired and blind people that allow them to understand their surroundings by the touch sense. The fact that blind people use touch to recognize objects and rely on listening to replace sight, motivated us to propose this work. First, the acquired depth image is segmented and each segment is classified in the context of assistive systems using a deep learning network. Second, inspired by the Braille system and the Japanese writing system Kanji, the obtained classes are coded with semantic labels. The scene is then synthesized using these labels and the extracted geometric features. Our system is able to predict more than 17 classes only by understanding the provided illustrative labels. For the remaining objects, their geometric features are transmitted. The labels and the geometric features are mapped on a synthesis area to be sensed by the touch sense. Experiments are conducted on noisy and incomplete data including acquired depth images of indoor scenes and public datasets. The obtained results are reported and discussed.



### LiDAR Data Enrichment Using Deep Learning Based on High-Resolution Image: An Approach to Achieve High-Performance LiDAR SLAM Using Low-cost LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2008.03694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.03694v1)
- **Published**: 2020-08-09 09:20:47+00:00
- **Updated**: 2020-08-09 09:20:47+00:00
- **Authors**: Jiang Yue, Weisong Wen, Jing Han, Li-Ta Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR-based SLAM algorithms are extensively studied to providing robust and accurate positioning for autonomous driving vehicles (ADV) in the past decades. Satisfactory performance can be obtained using high-grade 3D LiDAR with 64 channels, which can provide dense point clouds. Unfortunately, the high price significantly prevents its extensive commercialization in ADV. The cost-effective 3D LiDAR with 16 channels is a promising replacement. However, only limited and sparse point clouds can be provided by the 16 channels LiDAR, which cannot guarantee sufficient positioning accuracy for ADV in challenging dynamic environments. The high-resolution image from the low-cost camera can provide ample information about the surroundings. However, the explicit depth information is not available from the image. Inspired by the complementariness of 3D LiDAR and camera, this paper proposes to make use of the high-resolution images from a camera to enrich the raw 3D point clouds from the low-cost 16 channels LiDAR based on a state-of-the-art deep learning algorithm. An ERFNet is firstly employed to segment the image with the aid of the raw sparse 3D point clouds. Meanwhile, the sparse convolutional neural network is employed to predict the dense point clouds based on raw sparse 3D point clouds. Then, the predicted dense point clouds are fused with the segmentation outputs from ERFnet using a novel multi-layer convolutional neural network to refine the predicted 3D point clouds. Finally, the enriched point clouds are employed to perform LiDAR SLAM based on the state-of-the-art normal distribution transform (NDT). We tested our approach on the re-edited KITTI datasets: (1)the sparse 3D point clouds are significantly enriched with a mean square error of 1.1m MSE. (2)the map generated from the LiDAR SLAM is denser which includes more details without significant accuracy loss.



### Radar-based Dynamic Occupancy Grid Mapping and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.03696v1
- **DOI**: 10.1109/ITSC45102.2020.9294626
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03696v1)
- **Published**: 2020-08-09 09:26:30+00:00
- **Updated**: 2020-08-09 09:26:30+00:00
- **Authors**: Christopher Diehl, Eduard Feicho, Alexander Schwambach, Thomas Dammeier, Eric Mares, Torsten Bertram
- **Comment**: Accepted to be published as part of the 23rd IEEE International
  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,
  September 20-23, 2020
- **Journal**: None
- **Summary**: Environment modeling utilizing sensor data fusion and object tracking is crucial for safe automated driving. In recent years, the classical occupancy grid map approach, which assumes a static environment, has been extended to dynamic occupancy grid maps, which maintain the possibility of a low-level data fusion while also estimating the position and velocity distribution of the dynamic local environment. This paper presents the further development of a previous approach. To the best of the author's knowledge, there is no publication about dynamic occupancy grid mapping with subsequent analysis based only on radar data. Therefore in this work, the data of multiple radar sensors are fused, and a grid-based object tracking and mapping method is applied. Subsequently, the clustering of dynamic areas provides high-level object information. For comparison, also a lidar-based method is developed. The approach is evaluated qualitatively and quantitatively with real-world data from a moving vehicle in urban environments. The evaluation illustrates the advantages of the radar-based dynamic occupancy grid map, considering different comparison metrics.



### Fully Automated Photogrammetric Data Segmentation and Object Information Extraction Approach for Creating Simulation Terrain
- **Arxiv ID**: http://arxiv.org/abs/2008.03697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03697v1)
- **Published**: 2020-08-09 09:32:09+00:00
- **Updated**: 2020-08-09 09:32:09+00:00
- **Authors**: Meida Chen, Andrew Feng, Kyle McCullough, Pratusha Bhuvana Prasad, Ryan McAlinden, Lucio Soibelman, Mike Enloe
- **Comment**: None
- **Journal**: Interservice/Industry Training, Simulation, and Education
  Conference (I/ITSEC) 2019
- **Summary**: Our previous works have demonstrated that visually realistic 3D meshes can be automatically reconstructed with low-cost, off-the-shelf unmanned aerial systems (UAS) equipped with capable cameras, and efficient photogrammetric software techniques. However, such generated data do not contain semantic information/features of objects (i.e., man-made objects, vegetation, ground, object materials, etc.) and cannot allow the sophisticated user-level and system-level interaction. Considering the use case of the data in creating realistic virtual environments for training and simulations (i.e., mission planning, rehearsal, threat detection, etc.), segmenting the data and extracting object information are essential tasks. Thus, the objective of this research is to design and develop a fully automated photogrammetric data segmentation and object information extraction framework. To validate the proposed framework, the segmented data and extracted features were used to create virtual environments in the authors previously designed simulation tool i.e., Aerial Terrain Line of Sight Analysis System (ATLAS). The results showed that 3D mesh trees could be replaced with geo-typical 3D tree models using the extracted individual tree locations. The extracted tree features (i.e., color, width, height) are valuable for selecting the appropriate tree species and enhance visual quality. Furthermore, the identified ground material information can be taken into consideration for pathfinding. The shortest path can be computed not only considering the physical distance, but also considering the off-road vehicle performance capabilities on different ground surface materials.



### Learning Consistency Pursued Correlation Filters for Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2008.03704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03704v1)
- **Published**: 2020-08-09 10:22:52+00:00
- **Updated**: 2020-08-09 10:22:52+00:00
- **Authors**: Changhong Fu, Xiaoxiao Yang, Fan Li, Juntao Xu, Changjing Liu, Peng Lu
- **Comment**: IROS 2020 accepted, 8 pages, 7 figures, and 2 tables
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2020), Las Vegas, USA
- **Summary**: Correlation filter (CF)-based methods have demonstrated exceptional performance in visual object tracking for unmanned aerial vehicle (UAV) applications, but suffer from the undesirable boundary effect. To solve this issue, spatially regularized correlation filters (SRDCF) proposes the spatial regularization to penalize filter coefficients, thereby significantly improving the tracking performance. However, the temporal information hidden in the response maps is not considered in SRDCF, which limits the discriminative power and the robustness for accurate tracking. This work proposes a novel approach with dynamic consistency pursued correlation filters, i.e., the CPCF tracker. Specifically, through a correlation operation between adjacent response maps, a practical consistency map is generated to represent the consistency level across frames. By minimizing the difference between the practical and the scheduled ideal consistency map, the consistency level is constrained to maintain temporal smoothness, and rich temporal information contained in response maps is introduced. Besides, a dynamic constraint strategy is proposed to further improve the adaptability of the proposed tracker in complex situations. Comprehensive experiments are conducted on three challenging UAV benchmarks, i.e., UAV123@10FPS, UAVDT, and DTB70. Based on the experimental results, the proposed tracker favorably surpasses the other 25 state-of-the-art trackers with real-time running speed ($\sim$43FPS) on a single CPU.



### Block Shuffle: A Method for High-resolution Fast Style Transfer with Limited Memory
- **Arxiv ID**: http://arxiv.org/abs/2008.03706v1
- **DOI**: 10.1109/ACCESS.2020.3020053
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03706v1)
- **Published**: 2020-08-09 10:33:21+00:00
- **Updated**: 2020-08-09 10:33:21+00:00
- **Authors**: Weifeng Ma, Zhe Chen, Caoting Ji
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Fast Style Transfer is a series of Neural Style Transfer algorithms that use feed-forward neural networks to render input images. Because of the high dimension of the output layer, these networks require much memory for computation. Therefore, for high-resolution images, most mobile devices and personal computers cannot stylize them, which greatly limits the application scenarios of Fast Style Transfer. At present, the two existing solutions are purchasing more memory and using the feathering-based method, but the former requires additional cost, and the latter has poor image quality. To solve this problem, we propose a novel image synthesis method named \emph{block shuffle}, which converts a single task with high memory consumption to multiple subtasks with low memory consumption. This method can act as a plug-in for Fast Style Transfer without any modification to the network architecture. We use the most popular Fast Style Transfer repository on GitHub as the baseline. Experiments show that the quality of high-resolution images generated by our method is better than that of the feathering-based method. Although our method is an order of magnitude slower than the baseline, it can stylize high-resolution images with limited memory, which is impossible with the baseline. The code and models will be made available on \url{https://github.com/czczup/block-shuffle}.



### A Boundary Based Out-of-Distribution Classifier for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.04872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.04872v2)
- **Published**: 2020-08-09 11:27:19+00:00
- **Updated**: 2022-01-03 09:00:51+00:00
- **Authors**: Xingyu Chen, Xuguang Lan, Fuchun Sun, Nanning Zheng
- **Comment**: We found that the results in the first version are incorrect, due to
  the data leak problem in the original dataset splits. In this version, we fix
  this problem in the experiments
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) is a challenging topic that has promising prospects in many realistic scenarios. Using a gating mechanism that discriminates the unseen samples from the seen samples can decompose the GZSL problem to a conventional Zero-Shot Learning (ZSL) problem and a supervised classification problem. However, training the gate is usually challenging due to the lack of data in the unseen domain. To resolve this problem, in this paper, we propose a boundary based Out-of-Distribution (OOD) classifier which classifies the unseen and seen domains by only using seen samples for training. First, we learn a shared latent space on a unit hyper-sphere where the latent distributions of visual features and semantic attributes are aligned class-wisely. Then we find the boundary and the center of the manifold for each class. By leveraging the class centers and boundaries, the unseen samples can be separated from the seen samples. After that, we use two experts to classify the seen and unseen samples separately. We extensively validate our approach on five popular benchmark datasets including AWA1, AWA2, CUB, FLO and SUN. The experimental results demonstrate the advantages of our approach over state-of-the-art methods.



### I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2008.03713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03713v2)
- **Published**: 2020-08-09 12:13:31+00:00
- **Updated**: 2020-11-01 11:39:58+00:00
- **Authors**: Gyeongsik Moon, Kyoung Mu Lee
- **Comment**: Published at ECCV 2020
- **Journal**: None
- **Summary**: Most of the previous image-based 3D human pose and mesh estimation methods estimate parameters of the human mesh model from an input image. However, directly regressing the parameters from the input image is a highly non-linear mapping because it breaks the spatial relationship between pixels in the input image. In addition, it cannot model the prediction uncertainty, which can make training harder. To resolve the above issues, we propose I2L-MeshNet, an image-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet predicts the per-lixel likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Our lixel-based 1D heatmap preserves the spatial relationship in the input image and models the prediction uncertainty. We demonstrate the benefit of the image-to-lixel prediction and show that the proposed I2L-MeshNet outperforms previous methods. The code is publicly available https://github.com/mks0601/I2L-MeshNet_RELEASE.



### 1-Point RANSAC-Based Method for Ground Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.03718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03718v2)
- **Published**: 2020-08-09 12:58:58+00:00
- **Updated**: 2021-06-10 07:22:42+00:00
- **Authors**: Jeong-Kyun Lee, Young-Ki Baik, Hankyu Cho, Kang Kim, Duck Hoon Kim
- **Comment**: Accepted in the workshop on Autonomous Driving: Perception,
  Prediction and Planning in conjunction with CVPR 2021
- **Journal**: None
- **Summary**: Solving Perspective-n-Point (PnP) problems is a traditional way of estimating object poses. Given outlier-contaminated data, a pose of an object is calculated with PnP algorithms of n = {3, 4} in the RANSAC-based scheme. However, the computational complexity considerably increases along with n and the high complexity imposes a severe strain on devices which should estimate multiple object poses in real time. In this paper, we propose an efficient method based on 1-point RANSAC for estimating a pose of an object on the ground. In the proposed method, a pose is calculated with 1-DoF parameterization by using a ground object assumption and a 2D object bounding box as an additional observation, thereby achieving the fastest performance among the RANSAC-based methods. In addition, since the method suffers from the errors of the additional information, we propose a hierarchical robust estimation method for polishing a rough pose estimate and discovering more inliers in a coarse-to-fine manner. The experiments in synthetic and real-world datasets demonstrate the superiority of the proposed method.



### Online Extrinsic Camera Calibration for Temporally Consistent IPM Using Lane Boundary Observations with a Lane Width Prior
- **Arxiv ID**: http://arxiv.org/abs/2008.03722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03722v1)
- **Published**: 2020-08-09 13:11:17+00:00
- **Updated**: 2020-08-09 13:11:17+00:00
- **Authors**: Jeong-Kyun Lee, Young-Ki Baik, Hankyu Cho, Seungwoo Yoo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method for online extrinsic camera calibration, i.e., estimating pitch, yaw, roll angles and camera height from road surface in sequential driving scene images. The proposed method estimates the extrinsic camera parameters in two steps: 1) pitch and yaw angles are estimated simultaneously using a vanishing point computed from a set of lane boundary observations, and then 2) roll angle and camera height are computed by minimizing difference between lane width observations and a lane width prior. The extrinsic camera parameters are sequentially updated using extended Kalman filtering (EKF) and are finally used to generate a temporally consistent bird-eye-view (BEV) image by inverse perspective mapping (IPM). We demonstrate the superiority of the proposed method in synthetic and real-world datasets.



### SOFA-Net: Second-Order and First-order Attention Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2008.03723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03723v1)
- **Published**: 2020-08-09 13:13:04+00:00
- **Updated**: 2020-08-09 13:13:04+00:00
- **Authors**: Haoran Duan, Shidong Wang, Yu Guan
- **Comment**: Accepted by BMVC 2020
- **Journal**: None
- **Summary**: Automated crowd counting from images/videos has attracted more attention in recent years because of its wide application in smart cities. But modelling the dense crowd heads is challenging and most of the existing works become less reliable. To obtain the appropriate crowd representation, in this work we proposed SOFA-Net(Second-Order and First-order Attention Network): second-order statistics were extracted to retain selectivity of the channel-wise spatial information for dense heads while first-order statistics, which can enhance the feature discrimination for the heads' areas, were used as complementary information. Via a multi-stream architecture, the proposed second/first-order statistics were learned and transformed into attention for robust representation refinement. We evaluated our method on four public datasets and the performance reached state-of-the-art on most of them. Extensive experiments were also conducted to study the components in the proposed SOFA-Net, and the results suggested the high-capability of second/first-order statistics on modelling crowd in challenging scenarios. To the best of our knowledge, we are the first work to explore the second/first-order statistics for crowd counting.



### Representative elementary volume via averaged scalar Minkowski functionals
- **Arxiv ID**: http://arxiv.org/abs/2008.03727v2
- **DOI**: 10.1007/978-3-030-92144-6_40
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03727v2)
- **Published**: 2020-08-09 13:46:32+00:00
- **Updated**: 2021-07-31 15:54:48+00:00
- **Authors**: M. V. Andreeva, A. V. Kalyuzhnyuk, V. V. Krutko, N. E. Russkikh, I. A. Taimanov
- **Comment**: 7 pages
- **Journal**: Advanced Problem in Mechanics II. APM 2020, 533-539. Lecture Notes
  in Mechanical Engineering. Springer, Cham
- **Summary**: Representative Elementary Volume (REV) at which the material properties do not vary with change in volume is an important quantity for making measurements or simulations which represent the whole. We discuss the geometrical method to evaluation of REV based on the quantities coming in the Steiner formula from convex geometry. For bodies in the three-space this formula gives us four scalar functionals known as scalar Minkowski functionals. We demonstrate on certain samples that the values of such averaged functionals almost stabilize for cells for which the length of edges are greater than certain threshold value R. Therefore, from this point of view, it is reasonable to consider cubes of volume R^3 as representative elementary volumes.



### Recurrent Feature Reasoning for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2008.03737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03737v1)
- **Published**: 2020-08-09 14:40:04+00:00
- **Updated**: 2020-08-09 14:40:04+00:00
- **Authors**: Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, Dacheng Tao
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Existing inpainting methods have achieved promising performance for recovering regular or small image defects. However, filling in large continuous holes remains difficult due to the lack of constraints for the hole center. In this paper, we devise a Recurrent Feature Reasoning (RFR) network which is mainly constructed by a plug-and-play Recurrent Feature Reasoning module and a Knowledge Consistent Attention (KCA) module. Analogous to how humans solve puzzles (i.e., first solve the easier parts and then use the results as additional information to solve difficult parts), the RFR module recurrently infers the hole boundaries of the convolutional feature maps and then uses them as clues for further inference. The module progressively strengthens the constraints for the hole center and the results become explicit. To capture information from distant places in the feature map for RFR, we further develop KCA and incorporate it in RFR. Empirically, we first compare the proposed RFR-Net with existing backbones, demonstrating that RFR-Net is more efficient (e.g., a 4\% SSIM improvement for the same model size). We then place the network in the context of the current state-of-the-art, where it exhibits improved performance. The corresponding source code is available at: https://github.com/jingyuanli001/RFR-Inpainting



### Depth image denoising using nuclear norm and learning graph model
- **Arxiv ID**: http://arxiv.org/abs/2008.03741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03741v1)
- **Published**: 2020-08-09 15:12:16+00:00
- **Updated**: 2020-08-09 15:12:16+00:00
- **Authors**: Chenggang Yan, Zhisheng Li, Yongbing Zhang, Yutao Liu, Xiangyang Ji, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The depth images denoising are increasingly becoming the hot research topic nowadays because they reflect the three-dimensional (3D) scene and can be applied in various fields of computer vision. But the depth images obtained from depth camera usually contain stains such as noise, which greatly impairs the performance of depth related applications. In this paper, considering that group-based image restoration methods are more effective in gathering the similarity among patches, a group based nuclear norm and learning graph (GNNLG) model was proposed. For each patch, we find and group the most similar patches within a searching window. The intrinsic low-rank property of the grouped patches is exploited in our model. In addition, we studied the manifold learning method and devised an effective optimized learning strategy to obtain the graph Laplacian matrix, which reflects the topological structure of image, to further impose the smoothing priors to the denoised depth image. To achieve fast speed and high convergence, the alternating direction method of multipliers (ADMM) is proposed to solve our GNNLG. The experimental results show that the proposed method is superior to other current state-of-the-art denoising methods in both subjective and objective criterion.



### Switching Loss for Generalized Nucleus Detection in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2008.03750v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03750v1)
- **Published**: 2020-08-09 15:42:50+00:00
- **Updated**: 2020-08-09 15:42:50+00:00
- **Authors**: Deepak Anand, Gaurav Patel, Yaman Dang, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: The accuracy of deep learning methods for two foundational tasks in medical image analysis -- detection and segmentation -- can suffer from class imbalance. We propose a `switching loss' function that adaptively shifts the emphasis between foreground and background classes. While the existing loss functions to address this problem were motivated by the classification task, the switching loss is based on Dice loss, which is better suited for segmentation and detection. Furthermore, to get the most out the training samples, we adapt the loss with each mini-batch, unlike previous proposals that adapt once for the entire training set. A nucleus detector trained using the proposed loss function on a source dataset outperformed those trained using cross-entropy, Dice, or focal losses. Remarkably, without retraining on target datasets, our pre-trained nucleus detector also outperformed existing nucleus detectors that were trained on at least some of the images from the target datasets. To establish a broad utility of the proposed loss, we also confirmed that it led to more accurate ventricle segmentation in MRI as compared to the other loss functions. Our GPU-enabled pre-trained nucleus detection software is also ready to process whole slide images right out-of-the-box and is usably fast.



### A methodology for the measurement of track geometry based on computer vision and inertial sensors
- **Arxiv ID**: http://arxiv.org/abs/2008.03763v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03763v1)
- **Published**: 2020-08-09 16:57:51+00:00
- **Updated**: 2020-08-09 16:57:51+00:00
- **Authors**: Jos√© L. Escalona
- **Comment**: None
- **Journal**: None
- **Summary**: This document describes the theory used for the calculation of track geometric irregularities on a Track Geometry Measuring System (TGMS) to be installed in railway vehicles. The TGMS includes a computer for data acquisition and process, a set of sensors including an inertial measuring unit (IMU, 3D gyroscope and 3D accelerometer), two video cameras and an encoder. The main features of the proposed system are: 1. It is capable to measure track alignment, vertical profile, cross-level, gauge, twist and rail-head profile using non-contact technology. 2. It can be installed in line railway vehicles. It is compact and low cost. Provided that the equipment sees the rail heads when the vehicle is moving, it can be installed in any body of the vehicle: at the wheelsets level, above primary suspension (bogie frame) or above the secondary suspension (car body).



### Low-Light Maritime Image Enhancement with Regularized Illumination Optimization and Deep Noise Suppression
- **Arxiv ID**: http://arxiv.org/abs/2008.03765v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03765v1)
- **Published**: 2020-08-09 17:05:23+00:00
- **Updated**: 2020-08-09 17:05:23+00:00
- **Authors**: Yu Guo, Yuxu Lu, Ryan Wen Liu, Meifang Yang, Kwok Tai Chui
- **Comment**: 17 pages, 14 figures
- **Journal**: None
- **Summary**: Maritime images captured under low-light imaging condition easily suffer from low visibility and unexpected noise, leading to negative effects on maritime traffic supervision and management. To promote imaging performance, it is necessary to restore the important visual information from degraded low-light images. In this paper, we propose to enhance the low-light images through regularized illumination optimization and deep noise suppression. In particular, a hybrid regularized variational model, which combines L0-norm gradient sparsity prior with structure-aware regularization, is presented to refine the coarse illumination map originally estimated using Max-RGB. The adaptive gamma correction method is then introduced to adjust the refined illumination map. Based on the assumption of Retinex theory, a guided filter-based detail boosting method is introduced to optimize the reflection map. The adjusted illumination and optimized reflection maps are finally combined to generate the enhanced maritime images. To suppress the effect of unwanted noise on imaging performance, a deep learning-based blind denoising framework is further introduced to promote the visual quality of enhanced image. In particular, this framework is composed of two sub-networks, i.e., E-Net and D-Net adopted for noise level estimation and non-blind noise reduction, respectively. The main benefit of our image enhancement method is that it takes full advantage of the regularized illumination optimization and deep blind denoising. Comprehensive experiments have been conducted on both synthetic and realistic maritime images to compare our proposed method with several state-of-the-art imaging methods. Experimental results have illustrated its superior performance in terms of both quantitative and qualitative evaluations.



### SemEval-2020 Task 8: Memotion Analysis -- The Visuo-Lingual Metaphor!
- **Arxiv ID**: http://arxiv.org/abs/2008.03781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03781v1)
- **Published**: 2020-08-09 18:17:33+00:00
- **Updated**: 2020-08-09 18:17:33+00:00
- **Authors**: Chhavi Sharma, Deepesh Bhageria, William Scott, Srinivas PYKL, Amitava Das, Tanmoy Chakraborty, Viswanath Pulabaigari, Bjorn Gamback
- **Comment**: None
- **Journal**: None
- **Summary**: Information on social media comprises of various modalities such as textual, visual and audio. NLP and Computer Vision communities often leverage only one prominent modality in isolation to study social media. However, the computational processing of Internet memes needs a hybrid approach. The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twiter further suggests that we can not ignore such multimodal content anymore. To the best of our knowledge, there is not much attention towards meme emotion analysis. The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes. The task Memotion analysis released approx 10K annotated memes, with human-annotated labels namely sentiment (positive, negative, neutral), type of emotion (sarcastic, funny, offensive, motivation) and their corresponding intensity. The challenge consisted of three subtasks: sentiment (positive, negative, and neutral) analysis of memes, overall emotion (humour, sarcasm, offensive, and motivational) classification of memes, and classifying intensity of meme emotion. The best performances achieved were F1 (macro average) scores of 0.35, 0.51 and 0.32, respectively for each of the three subtasks.



### A Flow-Guided Mutual Attention Network for Video-Based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.03788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03788v2)
- **Published**: 2020-08-09 18:58:11+00:00
- **Updated**: 2020-10-04 23:56:23+00:00
- **Authors**: Madhu Kiran, Amran Bhuiyan, Louis-Antoine Blais-Morin, Mehrsan Javan, Ismail Ben Ayed, Eric Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Person Re-Identification (ReID) is a challenging problem in many video analytics and surveillance applications, where a person's identity must be associated across a distributed non-overlapping network of cameras. Video-based person ReID has recently gained much interest because it allows capturing discriminant spatio-temporal information from video clips that is unavailable for image-based ReID. Despite recent advances, deep learning (DL) models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, a flow-guided Mutual Attention network is proposed for fusion of image and optical flow sequences using any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information. Our Mutual Attention network relies on the joint spatial attention between image and optical flow features maps to activate a common set of salient features across them. In addition to flow-guided attention, we introduce a method to aggregate features from longer input streams for better video sequence-level representation. Our extensive experiments on three challenging video ReID datasets indicate that using the proposed Mutual Attention network allows to improve recognition accuracy considerably with respect to conventional gated-attention networks, and state-of-the-art methods for video-based person ReID.



### 3D Human Motion Estimation via Motion Compression and Refinement
- **Arxiv ID**: http://arxiv.org/abs/2008.03789v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03789v2)
- **Published**: 2020-08-09 19:02:29+00:00
- **Updated**: 2020-10-05 20:24:59+00:00
- **Authors**: Zhengyi Luo, S. Alireza Golestaneh, Kris M. Kitani
- **Comment**: Accepted by ACCV 2020 (Oral). Project page:
  https://zhengyiluo.github.io/projects/meva/
- **Journal**: None
- **Summary**: We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our method, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement. This two-step encoding of human motion captures human motion in two stages: a general human motion estimation step that captures the coarse overall motion, and a residual estimation that adds back person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates.



### Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.03791v2
- **DOI**: 10.1109/TCSVT.2020.3015051
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.03791v2)
- **Published**: 2020-08-09 19:06:29+00:00
- **Updated**: 2020-11-26 02:07:30+00:00
- **Authors**: Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang
- **Comment**: Accepted by IEEE T-CSVT, 11 pages, 6 figures, 10 tables
- **Journal**: None
- **Summary**: Current methods for skeleton-based human action recognition usually work with complete skeletons. However, in real scenarios, it is inevitable to capture incomplete or noisy skeletons, which could significantly deteriorate the performance of current methods when some informative joints are occluded or disturbed. To improve the robustness of action recognition models, a multi-stream graph convolutional network (GCN) is proposed to explore sufficient discriminative features spreading over all skeleton joints, so that the distributed redundant representation reduces the sensitivity of the action models to non-standard skeletons. Concretely, the backbone GCN is extended by a series of ordered streams which is responsible for learning discriminative features from the joints less activated by preceding streams. Here, the activation degrees of skeleton joints of each GCN stream are measured by the class activation maps (CAM), and only the information from the unactivated joints will be passed to the next stream, by which rich features over all active joints are obtained. Thus, the proposed method is termed richly activated GCN (RA-GCN). Compared to the state-of-the-art (SOTA) methods, the RA-GCN achieves comparable performance on the standard NTU RGB+D 60 and 120 datasets. More crucially, on the synthetic occlusion and jittering datasets, the performance deterioration due to the occluded and disturbed joints can be significantly alleviated by utilizing the proposed RA-GCN.



### Spatiotemporal Contrastive Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.03800v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.03800v4)
- **Published**: 2020-08-09 19:58:45+00:00
- **Updated**: 2021-04-05 19:51:20+00:00
- **Authors**: Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, Yin Cui
- **Comment**: CVPR2021 Camera ready
- **Journal**: None
- **Summary**: We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentations for video self-supervised learning and find that both spatial and temporal information are crucial. We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on clips that are distant in time. On Kinetics-600, a linear classifier trained on the representations learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated R3D-50. The performance of CVRL can be further improved to 72.9% with a larger R3D-152 (2x filters) backbone, significantly closing the gap between unsupervised and supervised video representation learning. Our code and models will be available at https://github.com/tensorflow/models/tree/master/official/.



### Neural Light Transport for Relighting and View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.03806v3
- **DOI**: 10.1145/3446328
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.03806v3)
- **Published**: 2020-08-09 20:13:15+00:00
- **Updated**: 2021-01-20 15:45:52+00:00
- **Authors**: Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul Debevec, Jonathan T. Barron, Ravi Ramamoorthi, William T. Freeman
- **Comment**: Camera-ready version for TOG 2021. Project Page:
  http://nlt.csail.mit.edu/
- **Journal**: None
- **Summary**: The light transport (LT) of a scene describes how it appears under different lighting and viewing directions, and complete knowledge of a scene's LT enables the synthesis of novel views under arbitrary lighting. In this paper, we focus on image-based LT acquisition, primarily for human bodies within a light stage setup. We propose a semi-parametric approach to learn a neural representation of LT that is embedded in the space of a texture atlas of known geometric properties, and model all non-diffuse and global LT as residuals added to a physically-accurate diffuse base rendering. In particular, we show how to fuse previously seen observations of illuminants and views to synthesize a new image of the same scene under a desired lighting condition from a chosen viewpoint. This strategy allows the network to learn complex material effects (such as subsurface scattering) and global illumination, while guaranteeing the physical correctness of the diffuse LT (such as hard shadows). With this learned LT, one can relight the scene photorealistically with a directional light or an HDRI map, synthesize novel views with view-dependent effects, or do both simultaneously, all in a unified framework using a set of sparse, previously seen observations. Qualitative and quantitative experiments demonstrate that our neural LT (NLT) outperforms state-of-the-art solutions for relighting and view synthesis, without separate treatment for both problems that prior work requires.



### Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2008.03813v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.03813v5)
- **Published**: 2020-08-09 21:13:13+00:00
- **Updated**: 2021-05-16 03:11:23+00:00
- **Authors**: Xudong Wang, Ziwei Liu, Stella X. Yu
- **Comment**: Accepted at CVPR 2021; Project page:
  http://people.eecs.berkeley.edu/~xdwang/projects/CLD/
- **Journal**: None
- **Summary**: Unsupervised feature learning has made great strides with contrastive learning based on instance discrimination and invariant mapping, as benchmarked on curated class-balanced datasets. However, natural data could be highly correlated and long-tail distributed. Natural between-instance similarity conflicts with the presumed instance distinction, causing unstable training and poor performance.   Our idea is to discover and integrate between-instance similarity into contrastive learning, not directly by instance grouping, but by cross-level discrimination (CLD) between instances and local instance groups. While invariant mapping of each instance is imposed by attraction within its augmented views, between-instance similarity could emerge from common repulsion against instance groups.   Our batch-wise and cross-view comparisons also greatly improve the positive/negative sample ratio of contrastive learning and achieve better invariant mapping. To effect both grouping and discrimination objectives, we impose them on features separately derived from a shared representation. In addition, we propose normalized projection heads and unsupervised hyper-parameter tuning for the first time.   Our extensive experimentation demonstrates that CLD is a lean and powerful add-on to existing methods such as NPID, MoCo, InfoMin, and BYOL on highly correlated, long-tail, or balanced datasets. It not only achieves new state-of-the-art on self-supervision, semi-supervision, and transfer learning benchmarks, but also beats MoCo v2 and SimCLR on every reported performance attained with a much larger compute. CLD effectively brings unsupervised learning closer to natural data and real-world applications. Our code is publicly available at: https://github.com/frank-xwang/CLD-UnsupervisedLearning.



### Neural Reflectance Fields for Appearance Acquisition
- **Arxiv ID**: http://arxiv.org/abs/2008.03824v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.03824v2)
- **Published**: 2020-08-09 22:04:36+00:00
- **Updated**: 2020-08-16 08:39:07+00:00
- **Authors**: Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo≈° Ha≈°an, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi
- **Comment**: None
- **Journal**: None
- **Summary**: We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.



### Dual In-painting Model for Unsupervised Gaze Correction and Animation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2008.03834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03834v1)
- **Published**: 2020-08-09 23:14:16+00:00
- **Updated**: 2020-08-09 23:14:16+00:00
- **Authors**: Jichao Zhang, Jingjing Chen, Hao Tang, Wei Wang, Yan Yan, Enver Sangineto, Nicu Sebe
- **Comment**: Accepted By ACMMM 2020
- **Journal**: None
- **Summary**: In this paper we address the problem of unsupervised gaze correction in the wild, presenting a solution that works without the need for precise annotations of the gaze angle and the head pose. We have created a new dataset called CelebAGaze, which consists of two domains X, Y, where the eyes are either staring at the camera or somewhere else. Our method consists of three novel modules: the Gaze Correction module (GCM), the Gaze Animation module (GAM), and the Pretrained Autoencoder module (PAM). Specifically, GCM and GAM separately train a dual in-painting network using data from the domain $X$ for gaze correction and data from the domain $Y$ for gaze animation. Additionally, a Synthesis-As-Training method is proposed when training GAM to encourage the features encoded from the eye region to be correlated with the angle information, resulting in a gaze animation which can be achieved by interpolation in the latent space. To further preserve the identity information~(e.g., eye shape, iris color), we propose the PAM with an Autoencoder, which is based on Self-Supervised mirror learning where the bottleneck features are angle-invariant and which works as an extra input to the dual in-painting models. Extensive experiments validate the effectiveness of the proposed method for gaze correction and gaze animation in the wild and demonstrate the superiority of our approach in producing more compelling results than state-of-the-art baselines. Our code, the pretrained models and the supplementary material are available at: https://github.com/zhangqianhui/GazeAnimation.




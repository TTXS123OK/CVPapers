# Arxiv Papers in cs.CV on 2020-08-05
### Multimodality Biomedical Image Registration using Free Point Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.01885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2008.01885v1)
- **Published**: 2020-08-05 00:13:04+00:00
- **Updated**: 2020-08-05 00:13:04+00:00
- **Authors**: Zachary M. C. Baum, Yipeng Hu, Dean C. Barratt
- **Comment**: 10 pages, 4 figures. Accepted for publication at International
  Conference on Medical Image Computing and Computer Assisted Intervention
  (MICCAI) workshop on Advances in Simplifying Medical UltraSound (ASMUS) 2020
- **Journal**: None
- **Summary**: We describe a point-set registration algorithm based on a novel free point transformer (FPT) network, designed for points extracted from multimodal biomedical images for registration tasks, such as those frequently encountered in ultrasound-guided interventional procedures. FPT is constructed with a global feature extractor which accepts unordered source and target point-sets of variable size. The extracted features are conditioned by a shared multilayer perceptron point transformer module to predict a displacement vector for each source point, transforming it into the target space. The point transformer module assumes no vicinity or smoothness in predicting spatial transformation and, together with the global feature extractor, is trained in a data-driven fashion with an unsupervised loss function. In a multimodal registration task using prostate MR and sparsely acquired ultrasound images, FPT yields comparable or improved results over other rigid and non-rigid registration methods. This demonstrates the versatility of FPT to learn registration directly from real, clinical training data and to generalize to a challenging task, such as the interventional application presented.



### A coarse-to-fine framework for unsupervised multi-contrast MR image deformable registration with dual consistency constraint
- **Arxiv ID**: http://arxiv.org/abs/2008.01896v3
- **DOI**: 10.1109/TMI.2021.3059282
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01896v3)
- **Published**: 2020-08-05 01:16:45+00:00
- **Updated**: 2021-02-16 06:07:20+00:00
- **Authors**: Weijian Huang, Hao Yang, Xinfeng Liu, Cheng Li, Ian Zhang, Rongpin Wang, Hairong Zheng, Shanshan Wang
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging (2021)
- **Summary**: Multi-contrast magnetic resonance (MR) image registration is useful in the clinic to achieve fast and accurate imaging-based disease diagnosis and treatment planning. Nevertheless, the efficiency and performance of the existing registration algorithms can still be improved. In this paper, we propose a novel unsupervised learning-based framework to achieve accurate and efficient multi-contrast MR image registrations. Specifically, an end-to-end coarse-to-fine network architecture consisting of affine and deformable transformations is designed to improve the robustness and achieve end-to-end registration. Furthermore, a dual consistency constraint and a new prior knowledge-based loss function are developed to enhance the registration performances. The proposed method has been evaluated on a clinical dataset containing 555 cases, and encouraging performances have been achieved. Compared to the commonly utilized registration methods, including VoxelMorph, SyN, and LT-Net, the proposed method achieves better registration performance with a Dice score of 0.8397 in identifying stroke lesions. With regards to the registration speed, our method is about 10 times faster than the most competitive method of SyN (Affine) when testing on a CPU. Moreover, we prove that our method can still perform well on more challenging tasks with lacking scanning information data, showing high robustness for the clinical application.



### Counterfactual Explanation Based on Gradual Construction for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.01897v2
- **DOI**: 10.1016/j.patcog.2022.108958
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.01897v2)
- **Published**: 2020-08-05 01:18:31+00:00
- **Updated**: 2021-09-06 02:27:47+00:00
- **Authors**: Hong-Gyu Jung, Sin-Han Kang, Hee-Dong Kim, Dong-Ok Won, Seong-Whan Lee
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.



### Hierarchical Amortized Training for Memory-efficient High Resolution 3D GAN
- **Arxiv ID**: http://arxiv.org/abs/2008.01910v4
- **DOI**: 10.1109/JBHI.2022.3172976
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01910v4)
- **Published**: 2020-08-05 02:33:04+00:00
- **Updated**: 2022-09-12 17:04:07+00:00
- **Authors**: Li Sun, Junxiang Chen, Yanwu Xu, Mingming Gong, Ke Yu, Kayhan Batmanghelich
- **Comment**: Paper accepted to IEEE Journal of Biomedical and Health Informatics,
  code available at https://github.com/batmanlab/HA-GAN
- **Journal**: in IEEE Journal of Biomedical and Health Informatics, vol. 26, no.
  8, pp. 3966-3975, Aug. 2022
- **Summary**: Generative Adversarial Networks (GAN) have many potential medical imaging applications, including data augmentation, domain adaptation, and model explanation. Due to the limited memory of Graphical Processing Units (GPUs), most current 3D GAN models are trained on low-resolution medical images, these models either cannot scale to high-resolution or are prone to patchy artifacts. In this work, we propose a novel end-to-end GAN architecture that can generate high-resolution 3D images. We achieve this goal by using different configurations between training and inference. During training, we adopt a hierarchical structure that simultaneously generates a low-resolution version of the image and a randomly selected sub-volume of the high-resolution image. The hierarchical design has two advantages: First, the memory demand for training on high-resolution images is amortized among sub-volumes. Furthermore, anchoring the high-resolution sub-volumes to a single low-resolution image ensures anatomical consistency between sub-volumes. During inference, our model can directly generate full high-resolution images. We also incorporate an encoder with a similar hierarchical structure into the model to extract features from the images. Experiments on 3D thorax CT and brain MRI demonstrate that our approach outperforms state of the art in image generation. We also demonstrate clinical applications of the proposed model in data augmentation and clinical-relevant feature extraction.



### Graph Signal Processing for Geometric Data and Beyond: Theory and Applications
- **Arxiv ID**: http://arxiv.org/abs/2008.01918v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01918v3)
- **Published**: 2020-08-05 03:20:16+00:00
- **Updated**: 2021-09-04 17:35:02+00:00
- **Authors**: Wei Hu, Jiahao Pang, Xianming Liu, Dong Tian, Chia-Wen Lin, Anthony Vetro
- **Comment**: Accepted at IEEE TMM
- **Journal**: None
- **Summary**: Geometric data acquired from real-world scenes, e.g., 2D depth images, 3D point clouds, and 4D dynamic point clouds, have found a wide range of applications including immersive telepresence, autonomous driving, surveillance, etc. Due to irregular sampling patterns of most geometric data, traditional image/video processing methodologies are limited, while Graph Signal Processing (GSP) -- a fast-developing field in the signal processing community -- enables processing signals that reside on irregular domains and plays a critical role in numerous applications of geometric data from low-level processing to high-level analysis. To further advance the research in this field, we provide the first timely and comprehensive overview of GSP methodologies for geometric data in a unified manner by bridging the connections between geometric data and graphs, among the various geometric data modalities, and with spectral/nodal graph filtering techniques. We also discuss the recently developed Graph Neural Networks (GNNs) and interpret the operation of these networks from the perspective of GSP. We conclude with a brief discussion of open problems and challenges.



### Component Divide-and-Conquer for Real-World Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2008.01928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01928v1)
- **Published**: 2020-08-05 04:26:26+00:00
- **Updated**: 2020-08-05 04:26:26+00:00
- **Authors**: Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, Liang Lin
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), 2020
- **Summary**: In this paper, we present a large-scale Diverse Real-world image Super-Resolution dataset, i.e., DRealSR, as well as a divide-and-conquer Super-Resolution (SR) network, exploring the utility of guiding SR model with low-level image components. DRealSR establishes a new SR benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. In general, the targets of SR vary with image regions with different low-level image components, e.g., smoothness preserving for flat regions, sharpening for edges, and detail enhancing for textures. Learning an SR model with conventional pixel-wise loss usually is easily dominated by flat regions and edges, and fails to infer realistic details of complex textures. We propose a Component Divide-and-Conquer (CDC) model and a Gradient-Weighted (GW) loss for SR. Our CDC parses an image with three components, employs three Component-Attentive Blocks (CABs) to learn attentive masks and intermediate SR predictions with an intermediate supervision learning strategy, and trains an SR model following a divide-and-conquer learning principle. Our GW loss also provides a feasible way to balance the difficulties of image components for SR. Extensive experiments validate the superior performance of our CDC and the challenging aspects of our DRealSR dataset related to diverse real-world scenarios. Our dataset and codes are publicly available at https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution



### COALESCE: Component Assembly by Learning to Synthesize Connections
- **Arxiv ID**: http://arxiv.org/abs/2008.01936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01936v2)
- **Published**: 2020-08-05 05:12:06+00:00
- **Updated**: 2020-11-08 08:11:55+00:00
- **Authors**: Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher, Vladimir G. Kim, Hao Zhang
- **Comment**: 20 pages: paper + supplementary
- **Journal**: None
- **Summary**: We introduce COALESCE, the first data-driven framework for component-based shape assembly which employs deep learning to synthesize part connections. To handle geometric and topological mismatches between parts, we remove the mismatched portions via erosion, and rely on a joint synthesis step, which is learned from data, to fill the gap and arrive at a natural and plausible part joint. Given a set of input parts extracted from different objects, COALESCE automatically aligns them and synthesizes plausible joints to connect the parts into a coherent 3D object represented by a mesh. The joint synthesis network, designed to focus on joint regions, reconstructs the surface between the parts by predicting an implicit shape representation that agrees with existing parts, while generating a smooth and topologically meaningful connection. We employ test-time optimization to further ensure that the synthesized joint region closely aligns with the input parts to create realistic component assemblies from diverse input parts. We demonstrate that our method significantly outperforms prior approaches including baseline deep models for 3D shape synthesis, as well as state-of-the-art methods for shape completion.



### A feature-supervised generative adversarial network for environmental monitoring during hazy days
- **Arxiv ID**: http://arxiv.org/abs/2008.01942v1
- **DOI**: 10.1016/j.scitotenv.2020.141445
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01942v1)
- **Published**: 2020-08-05 05:27:15+00:00
- **Updated**: 2020-08-05 05:27:15+00:00
- **Authors**: Ke Wang, Siyuan Zhang, Junlan Chen, Fan Ren, Lei Xiao
- **Comment**: None
- **Journal**: Science of the Total Environment (2020),748, 141445
- **Summary**: The adverse haze weather condition has brought considerable difficulties in vision-based environmental applications. While, until now, most of the existing environmental monitoring studies are under ordinary conditions, and the studies of complex haze weather conditions have been ignored. Thence, this paper proposes a feature-supervised learning network based on generative adversarial networks (GAN) for environmental monitoring during hazy days. Its main idea is to train the model under the supervision of feature maps from the ground truth. Four key technical contributions are made in the paper. First, pairs of hazy and clean images are used as inputs to supervise the encoding process and obtain high-quality feature maps. Second, the basic GAN formulation is modified by introducing perception loss, style loss, and feature regularization loss to generate better results. Third, multi-scale images are applied as the input to enhance the performance of discriminator. Finally, a hazy remote sensing dataset is created for testing our dehazing method and environmental detection. Extensive experimental results show that the proposed method has achieved better performance than current state-of-the-art methods on both synthetic datasets and real-world remote sensing images.



### MultiCheXNet: A Multi-Task Learning Deep Network For Pneumonia-like Diseases Diagnosis From X-ray Scans
- **Arxiv ID**: http://arxiv.org/abs/2008.01973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01973v1)
- **Published**: 2020-08-05 07:45:24+00:00
- **Updated**: 2020-08-05 07:45:24+00:00
- **Authors**: Abdullah Tarek Farag, Ahmed Raafat Abd El-Wahab, Mahmoud Nada, Mohamed Yasser Abd El-Hakeem, Omar Sayed Mahmoud, Reem Khaled Rashwan, Ahmad El Sallab
- **Comment**: None
- **Journal**: None
- **Summary**: We present MultiCheXNet, an end-to-end Multi-task learning model, that is able to take advantage of different X-rays data sets of Pneumonia-like diseases in one neural architecture, performing three tasks at the same time; diagnosis, segmentation and localization. The common encoder in our architecture can capture useful common features present in the different tasks. The common encoder has another advantage of efficient computations, which speeds up the inference time compared to separate models. The specialized decoders heads can then capture the task-specific features. We employ teacher forcing to address the issue of negative samples that hurt the segmentation and localization performance. Finally,we employ transfer learning to fine tune the classifier on unseen pneumonia-like diseases. The MTL architecture can be trained on joint or dis-joint labeled data sets. The training of the architecture follows a carefully designed protocol, that pre trains different sub-models on specialized datasets, before being integrated in the joint MTL model. Our experimental setup involves variety of data sets, where the baseline performance of the 3 tasks is compared to the MTL architecture performance. Moreover, we evaluate the transfer learning mode to COVID-19 data set,both from individual classifier model, and from MTL architecture classification head.



### More Than Accuracy: Towards Trustworthy Machine Learning Interfaces for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.01980v1
- **DOI**: 10.1145/3340631.3394873
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01980v1)
- **Published**: 2020-08-05 07:56:37+00:00
- **Updated**: 2020-08-05 07:56:37+00:00
- **Authors**: Hendrik Heuer, Andreas Breiter
- **Comment**: UMAP '20: Proceedings of the 28th ACM Conference on User Modeling,
  Adaptation and Personalization
- **Journal**: UMAP 2020: Proceedings of the 28th ACM Conference on User
  Modeling, Adaptation and Personalization
- **Summary**: This paper investigates the user experience of visualizations of a machine learning (ML) system that recognizes objects in images. This is important since even good systems can fail in unexpected ways as misclassifications on photo-sharing websites showed. In our study, we exposed users with a background in ML to three visualizations of three systems with different levels of accuracy. In interviews, we explored how the visualization helped users assess the accuracy of systems in use and how the visualization and the accuracy of the system affected trust and reliance. We found that participants do not only focus on accuracy when assessing ML systems. They also take the perceived plausibility and severity of misclassification into account and prefer seeing the probability of predictions. Semantically plausible errors are judged as less severe than errors that are implausible, which means that system accuracy could be communicated through the types of errors.



### Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field
- **Arxiv ID**: http://arxiv.org/abs/2008.11014v1
- **DOI**: 10.1109/TIP.2020.2992177
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.11014v1)
- **Published**: 2020-08-05 08:28:18+00:00
- **Updated**: 2020-08-05 08:28:18+00:00
- **Authors**: Haixia Bi, Lin Xu, Xiangyong Cao, Yong Xue, Zongben Xu
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing (2020)
- **Summary**: Polarimetric synthetic aperture radar (PolSAR) image segmentation is currently of great importance in image processing for remote sensing applications. However, it is a challenging task due to two main reasons. Firstly, the label information is difficult to acquire due to high annotation costs. Secondly, the speckle effect embedded in the PolSAR imaging process remarkably degrades the segmentation performance. To address these two issues, we present a contextual PolSAR image semantic segmentation method in this paper.With a newly defined channelwise consistent feature set as input, the three-dimensional discrete wavelet transform (3D-DWT) technique is employed to extract discriminative multi-scale features that are robust to speckle noise. Then Markov random field (MRF) is further applied to enforce label smoothness spatially during segmentation. By simultaneously utilizing 3D-DWT features and MRF priors for the first time, contextual information is fully integrated during the segmentation to ensure accurate and smooth segmentation. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on three real benchmark PolSAR image data sets. Experimental results indicate that the proposed method achieves promising segmentation accuracy and preferable spatial consistency using a minimal number of labeled pixels.



### Subclass Contrastive Loss for Injured Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.01993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01993v1)
- **Published**: 2020-08-05 08:30:29+00:00
- **Updated**: 2020-08-05 08:30:29+00:00
- **Authors**: Puspita Majumdar, Saheb Chhabra, Richa Singh, Mayank Vatsa
- **Comment**: Accepted in BTAS 2019
- **Journal**: None
- **Summary**: Deaths and injuries are common in road accidents, violence, and natural disaster. In such cases, one of the main tasks of responders is to retrieve the identity of the victims to reunite families and ensure proper identification of deceased/ injured individuals. Apart from this, identification of unidentified dead bodies due to violence and accidents is crucial for the police investigation. In the absence of identification cards, current practices for this task include DNA profiling and dental profiling. Face is one of the most commonly used and widely accepted biometric modalities for recognition. However, face recognition is challenging in the presence of facial injuries such as swelling, bruises, blood clots, laceration, and avulsion which affect the features used in recognition. In this paper, for the first time, we address the problem of injured face recognition and propose a novel Subclass Contrastive Loss (SCL) for this task. A novel database, termed as Injured Face (IF) database, is also created to instigate research in this direction. Experimental analysis shows that the proposed loss function surpasses existing algorithm for injured face recognition.



### Unsupervised seismic facies classification using deep convolutional autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2008.01995v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.01995v1)
- **Published**: 2020-08-05 08:33:09+00:00
- **Updated**: 2020-08-05 08:33:09+00:00
- **Authors**: Vladimir Puzyrev, Chris Elders
- **Comment**: None
- **Journal**: None
- **Summary**: With the increased size and complexity of seismic surveys, manual labeling of seismic facies has become a significant challenge. Application of automatic methods for seismic facies interpretation could significantly reduce the manual labor and subjectivity of a particular interpreter present in conventional methods. A recently emerged group of methods is based on deep neural networks. These approaches are data-driven and require large labeled datasets for network training. We apply a deep convolutional autoencoder for unsupervised seismic facies classification, which does not require manually labeled examples. The facies maps are generated by clustering the deep-feature vectors obtained from the input data. Our method yields accurate results on real data and provides them instantaneously. The proposed approach opens up possibilities to analyze geological patterns in real time without human intervention.



### F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.01999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.01999v2)
- **Published**: 2020-08-05 08:47:42+00:00
- **Updated**: 2020-08-06 04:36:47+00:00
- **Authors**: Yan Hong, Li Niu, Jianfu Zhang, Weijie Zhao, Chen Fu, Liqing Zhang
- **Comment**: This paper is accepted by ACM Multimedia 2020
- **Journal**: None
- **Summary**: In order to generate images for a given category, existing deep generative models generally rely on abundant training images. However, extensive data acquisition is expensive and fast learning ability from limited data is necessarily required in real-world applications. Also, these existing methods are not well-suited for fast adaptation to a new category.   Few-shot image generation, aiming to generate images from only a few images for a new category, has attracted some research interest. In this paper, we propose a Fusing-and-Filling Generative Adversarial Network (F2GAN) to generate realistic and diverse images for a new category with only a few images. In our F2GAN, a fusion generator is designed to fuse the high-level features of conditional images with random interpolation coefficients, and then fills in attended low-level details with non-local attention module to produce a new image. Moreover, our discriminator can ensure the diversity of generated images by a mode seeking loss and an interpolation regression loss. Extensive experiments on five datasets demonstrate the effectiveness of our proposed method for few-shot image generation.



### Multiple Sclerosis Lesion Activity Segmentation with Attention-Guided Two-Path CNNs
- **Arxiv ID**: http://arxiv.org/abs/2008.02001v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02001v1)
- **Published**: 2020-08-05 08:49:20+00:00
- **Updated**: 2020-08-05 08:49:20+00:00
- **Authors**: Nils Gessert, Julia Kr√ºger, Roland Opfer, Ann-Christin Ostwaldt, Praveena Manogaran, Hagen H. Kitzler, Sven Schippling, Alexander Schlaefer
- **Comment**: Accepted for publication in Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Multiple sclerosis is an inflammatory autoimmune demyelinating disease that is characterized by lesions in the central nervous system. Typically, magnetic resonance imaging (MRI) is used for tracking disease progression. Automatic image processing methods can be used to segment lesions and derive quantitative lesion parameters. So far, methods have focused on lesion segmentation for individual MRI scans. However, for monitoring disease progression, \textit{lesion activity} in terms of new and enlarging lesions between two time points is a crucial biomarker. For this problem, several classic methods have been proposed, e.g., using difference volumes. Despite their success for single-volume lesion segmentation, deep learning approaches are still rare for lesion activity segmentation. In this work, convolutional neural networks (CNNs) are studied for lesion activity segmentation from two time points. For this task, CNNs are designed and evaluated that combine the information from two points in different ways. In particular, two-path architectures with attention-guided interactions are proposed that enable effective information exchange between the two time point's processing paths. It is demonstrated that deep learning-based methods outperform classic approaches and it is shown that attention-guided interactions significantly improve performance. Furthermore, the attention modules produce plausible attention maps that have a masking effect that suppresses old, irrelevant lesions. A lesion-wise false positive rate of 26.4% is achieved at a true positive rate of 74.2%, which is not significantly different from the interrater performance.



### Fast top-K Cosine Similarity Search through XOR-Friendly Binary Quantization on GPUs
- **Arxiv ID**: http://arxiv.org/abs/2008.02002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2008.02002v1)
- **Published**: 2020-08-05 08:50:21+00:00
- **Updated**: 2020-08-05 08:50:21+00:00
- **Authors**: Xiaozheng Jian, Jianqiu Lu, Zexi Yuan, Ao Li
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: We explore the use of GPU for accelerating large scale nearest neighbor search and we propose a fast vector-quantization-based exhaustive nearest neighbor search algorithm that can achieve high accuracy without any indexing construction specifically designed for cosine similarity. This algorithm uses a novel XOR-friendly binary quantization method to encode floating-point numbers such that high-complexity multiplications can be optimized as low-complexity bitwise operations. Experiments show that, our quantization method takes short preprocessing time, and helps make the search speed of our exhaustive search method much more faster than that of popular approximate nearest neighbor algorithms when high accuracy is needed.



### Beyond Controlled Environments: 3D Camera Re-Localization in Changing Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2008.02004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02004v1)
- **Published**: 2020-08-05 09:02:12+00:00
- **Updated**: 2020-08-05 09:02:12+00:00
- **Authors**: Johanna Wald, Torsten Sattler, Stuart Golodetz, Tommaso Cavallari, Federico Tombari
- **Comment**: ECCV 2020, project website https://waldjohannau.github.io/RIO10
- **Journal**: None
- **Summary**: Long-term camera re-localization is an important task with numerous computer vision and robotics applications. Whilst various outdoor benchmarks exist that target lighting, weather and seasonal changes, far less attention has been paid to appearance changes that occur indoors. This has led to a mismatch between popular indoor benchmarks, which focus on static scenes, and indoor environments that are of interest for many real-world applications. In this paper, we adapt 3RScan - a recently introduced indoor RGB-D dataset designed for object instance re-localization - to create RIO10, a new long-term camera re-localization benchmark focused on indoor scenes. We propose new metrics for evaluating camera re-localization and explore how state-of-the-art camera re-localizers perform according to these metrics. We also examine in detail how different types of scene change affect the performance of different methods, based on novel ways of detecting such changes in a given RGB-D frame. Our results clearly show that long-term indoor re-localization is an unsolved problem. Our benchmark and tools are publicly available at waldjohannau.github.io/RIO10



### Extracting and Leveraging Nodule Features with Lung Inpainting for Local Feature Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.02030v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02030v1)
- **Published**: 2020-08-05 10:13:41+00:00
- **Updated**: 2020-08-05 10:13:41+00:00
- **Authors**: Sebastian Guendel, Arnaud Arindra Adiyoso Setio, Sasa Grbic, Andreas Maier, Dorin Comaniciu
- **Comment**: Accepted at MICCAI MLMI 2020
- **Journal**: None
- **Summary**: Chest X-ray (CXR) is the most common examination for fast detection of pulmonary abnormalities. Recently, automated algorithms have been developed to classify multiple diseases and abnormalities in CXR scans. However, because of the limited availability of scans containing nodules and the subtle properties of nodules in CXRs, state-of-the-art methods do not perform well on nodule classification. To create additional data for the training process, standard augmentation techniques are applied. However, the variance introduced by these methods are limited as the images are typically modified globally. In this paper, we propose a method for local feature augmentation by extracting local nodule features using a generative inpainting network. The network is applied to generate realistic, healthy tissue and structures in patches containing nodules. The nodules are entirely removed in the inpainted representation. The extraction of the nodule features is processed by subtraction of the inpainted patch from the nodule patch. With arbitrary displacement of the extracted nodules in the lung area across different CXR scans and further local modifications during training, we significantly increase the nodule classification performance and outperform state-of-the-art augmentation methods.



### Pose-based Modular Network for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.02042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02042v1)
- **Published**: 2020-08-05 10:56:09+00:00
- **Updated**: 2020-08-05 10:56:09+00:00
- **Authors**: Zhijun Liang, Junfa Liu, Yisheng Guan, Juan Rojas
- **Comment**: None
- **Journal**: None
- **Summary**: Human-object interaction(HOI) detection is a critical task in scene understanding. The goal is to infer the triplet <subject, predicate, object> in a scene. In this work, we note that the human pose itself as well as the relative spatial information of the human pose with respect to the target object can provide informative cues for HOI detection. We contribute a Pose-based Modular Network (PMN) which explores the absolute pose features and relative spatial pose features to improve HOI detection and is fully compatible with existing networks. Our module consists of a branch that first processes the relative spatial pose features of each joint independently. Another branch updates the absolute pose features via fully connected graph structures. The processed pose features are then fed into an action classifier. To evaluate our proposed method, we combine the module with the state-of-the-art model named VS-GATs and obtain significant improvement on two public benchmarks: V-COCO and HICO-DET, which shows its efficacy and flexibility. Code is available at \url{https://github.com/birlrobotics/PMN}.



### Learning Boost by Exploiting the Auxiliary Task in Multi-task Domain
- **Arxiv ID**: http://arxiv.org/abs/2008.02043v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.02043v1)
- **Published**: 2020-08-05 10:56:56+00:00
- **Updated**: 2020-08-05 10:56:56+00:00
- **Authors**: Jonghwa Yim, Sang Hwan Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Learning two tasks in a single shared function has some benefits. Firstly by acquiring information from the second task, the shared function leverages useful information that could have been neglected or underestimated in the first task. Secondly, it helps to generalize the function that can be learned using generally applicable information for both tasks. To fully enjoy these benefits, Multi-task Learning (MTL) has long been researched in various domains such as computer vision, language understanding, and speech synthesis. While MTL benefits from the positive transfer of information from multiple tasks, in a real environment, tasks inevitably have a conflict between them during the learning phase, called negative transfer. The negative transfer hampers function from achieving the optimality and degrades the performance. To solve the problem of the task conflict, previous works only suggested partial solutions that are not fundamental, but ad-hoc. A common approach is using a weighted sum of losses. The weights are adjusted to induce positive transfer. Paradoxically, this kind of solution acknowledges the problem of negative transfer and cannot remove it unless the weight of the task is set to zero. Therefore, these previous methods had limited success. In this paper, we introduce a novel approach that can drive positive transfer and suppress negative transfer by leveraging class-wise weights in the learning process. The weights act as an arbitrator of the fundamental unit of information to determine its positive or negative status to the main task.



### Compact Graph Architecture for Speech Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.02063v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.02063v4)
- **Published**: 2020-08-05 12:09:09+00:00
- **Updated**: 2021-02-02 10:34:47+00:00
- **Authors**: A. Shirian, T. Guha
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architecture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters (~30K) indicating its applicability in resource-constrained devices.



### Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.02086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02086v1)
- **Published**: 2020-08-05 12:41:59+00:00
- **Updated**: 2020-08-05 12:41:59+00:00
- **Authors**: Jinpeng Wang, Yiqi Lin, Andy J. Ma
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Self-supervised learning has shown great potentials in improving the deep learning model in an unsupervised manner by constructing surrogate supervision signals directly from the unlabeled data. Different from existing works, we present a novel way to obtain the surrogate supervision signal based on high-level feature maps under consistency regularization. In this paper, we propose a Spatio-Temporal Consistency Regularization between different output features generated from a siamese network including a clean path fed with original video and a noise path fed with the corresponding augmented video. Based on the Spatio-Temporal characteristics of video, we develop two video-based data augmentation methods, i.e., Spatio-Temporal Transformation and Intra-Video Mixup. Consistency of the former one is proposed to model transformation consistency of features, while the latter one aims at retaining spatial invariance to extract action-related features. Extensive experiments demonstrate that our method achieves substantial improvements compared with state-of-the-art self-supervised learning methods for action recognition. When using our method as an additional regularization term and combine with current surrogate supervision signals, we achieve 22% relative improvement over the previous state-of-the-art on HMDB51 and 7% on UCF101.



### Point Proposal Network: Accelerating Point Source Detection Through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02093v2
- **DOI**: 10.1109/SSCI50451.2021.9660085
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02093v2)
- **Published**: 2020-08-05 12:54:04+00:00
- **Updated**: 2021-02-04 13:40:28+00:00
- **Authors**: Duncan Tilley, Christopher W. Cleghorn, Kshitij Thorat, Roger Deane
- **Comment**: None
- **Journal**: 2021 IEEE Symposium Series on Computational Intelligence (SSCI),
  2021, pp. 1-8
- **Summary**: Point source detection techniques are used to identify and localise point sources in radio astronomical surveys. With the development of the Square Kilometre Array (SKA) telescope, survey images will see a massive increase in size from Gigapixels to Terapixels. Point source detection has already proven to be a challenge in recent surveys performed by SKA pathfinder telescopes. This paper proposes the Point Proposal Network (PPN): a point source detector that utilises deep convolutional neural networks for fast source detection. Results measured on simulated MeerKAT images show that, although less precise when compared to leading alternative approaches, PPN performs source detection faster and is able to scale to large images, unlike the alternative approaches.



### Structure Preserving Stain Normalization of Histopathology Images Using Self-Supervised Semantic Guidance
- **Arxiv ID**: http://arxiv.org/abs/2008.02101v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02101v3)
- **Published**: 2020-08-05 12:59:15+00:00
- **Updated**: 2021-06-03 15:35:06+00:00
- **Authors**: Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Although generative adversarial network (GAN) based style transfer is state of the art in histopathology color-stain normalization, they do not explicitly integrate structural information of tissues. We propose a self-supervised approach to incorporate semantic guidance into a GAN based stain normalization framework and preserve detailed structural information. Our method does not require manual segmentation maps which is a significant advantage over existing methods. We integrate semantic information at different layers between a pre-trained semantic network and the stain color normalization network. The proposed scheme outperforms other color normalization methods leading to better classification and segmentation performance.



### Duality Diagram Similarity: a generic framework for initialization selection in task transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02107v1)
- **Published**: 2020-08-05 13:00:34+00:00
- **Updated**: 2020-08-05 13:00:34+00:00
- **Authors**: Kshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, Gemma Roig
- **Comment**: accepted at ECCV 2020. Code available here:
  https://github.com/cvai-repo/duality-diagram-similarity
- **Journal**: None
- **Summary**: In this paper, we tackle an open research question in transfer learning, which is selecting a model initialization to achieve high performance on a new task, given several pre-trained models. We propose a new highly efficient and accurate approach based on duality diagram similarity (DDS) between deep neural networks (DNNs). DDS is a generic framework to represent and compare data of different feature dimensions. We validate our approach on the Taskonomy dataset by measuring the correspondence between actual transfer learning performance rankings on 17 taskonomy tasks and predicted rankings. Computing DDS based ranking for $17\times17$ transfers requires less than 2 minutes and shows a high correlation ($0.86$) with actual transfer learning rankings, outperforming state-of-the-art methods by a large margin ($10\%$) on the Taskonomy benchmark. We also demonstrate the robustness of our model selection approach to a new task, namely Pascal VOC semantic segmentation. Additionally, we show that our method can be applied to select the best layer locations within a DNN for transfer learning on 2D, 3D and semantic tasks on NYUv2 and Pascal VOC datasets.



### Self-supervised Temporal Discriminative Learning for Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.02129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02129v1)
- **Published**: 2020-08-05 13:36:59+00:00
- **Updated**: 2020-08-05 13:36:59+00:00
- **Authors**: Jinpeng Wang, Yiqi Lin, Andy J. Ma, Pong C. Yuen
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Temporal cues in videos provide important information for recognizing actions accurately. However, temporal-discriminative features can hardly be extracted without using an annotated large-scale video action dataset for training. This paper proposes a novel Video-based Temporal-Discriminative Learning (VTDL) framework in self-supervised manner. Without labelled data for network pretraining, temporal triplet is generated for each anchor video by using segment of the same or different time interval so as to enhance the capacity for temporal feature representation. Measuring temporal information by time derivative, Temporal Consistent Augmentation (TCA) is designed to ensure that the time derivative (in any order) of the augmented positive is invariant except for a scaling constant. Finally, temporal-discriminative features are learnt by minimizing the distance between each anchor and its augmented positive, while the distance between each anchor and its augmented negative as well as other videos saved in the memory bank is maximized to enrich the representation diversity. In the downstream action recognition task, the proposed method significantly outperforms existing related works. Surprisingly, the proposed self-supervised approach is better than fully-supervised methods on UCF101 and HMDB51 when a small-scale video dataset (with only thousands of videos) is used for pre-training. The code has been made publicly available on https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition.



### Tiny-YOLO object detection supplemented with geometrical data
- **Arxiv ID**: http://arxiv.org/abs/2008.02170v2
- **DOI**: 10.1109/VTC2020-Spring48590.2020.9128749
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.02170v2)
- **Published**: 2020-08-05 14:45:19+00:00
- **Updated**: 2020-10-15 19:15:01+00:00
- **Authors**: Ivan Khokhlov, Egor Davydenko, Ilya Osokin, Ilya Ryakin, Azer Babaev, Vladimir Litvinenko, Roman Gorbachev
- **Comment**: 5 pages, 5 figures, published in 2020 IEEE 91st Vehicular Technology
  Conference (VTC2020-Spring)
- **Journal**: None
- **Summary**: We propose a method of improving detection precision (mAP) with the help of the prior knowledge about the scene geometry: we assume the scene to be a plane with objects placed on it. We focus our attention on autonomous robots, so given the robot's dimensions and the inclination angles of the camera, it is possible to predict the spatial scale for each pixel of the input frame. With slightly modified YOLOv3-tiny we demonstrate that the detection supplemented by the scale channel, further referred as S, outperforms standard RGB-based detection with small computational overhead.



### Active Perception using Light Curtains for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2008.02191v1
- **DOI**: 10.1007/978-3-030-58558-7_44
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.02191v1)
- **Published**: 2020-08-05 15:38:18+00:00
- **Updated**: 2020-08-05 15:38:18+00:00
- **Authors**: Siddharth Ancha, Yaadhav Raaj, Peiyun Hu, Srinivasa G. Narasimhan, David Held
- **Comment**: Published at the European Conference on Computer Vision (ECCV), 2020
- **Journal**: None
- **Summary**: Most real-world 3D sensors such as LiDARs perform fixed scans of the entire environment, while being decoupled from the recognition system that processes the sensor data. In this work, we propose a method for 3D object recognition using light curtains, a resource-efficient controllable sensor that measures depth at user-specified locations in the environment. Crucially, we propose using prediction uncertainty of a deep learning based 3D point cloud detector to guide active perception. Given a neural network's uncertainty, we derive an optimization objective to place light curtains using the principle of maximizing information gain. Then, we develop a novel and efficient optimization algorithm to maximize this objective by encoding the physical constraints of the device into a constraint graph and optimizing with dynamic programming. We show how a 3D detector can be trained to detect objects in a scene by sequentially placing uncertainty-guided light curtains to successively improve detection accuracy. Code and details can be found on the project webpage: http://siddancha.github.io/projects/active-perception-light-curtains.



### Domain-Specific Mappings for Generative Adversarial Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2008.02198v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02198v1)
- **Published**: 2020-08-05 15:55:25+00:00
- **Updated**: 2020-08-05 15:55:25+00:00
- **Authors**: Hsin-Yu Chang, Zhixiang Wang, Yung-Yu Chuang
- **Comment**: ECCV 2020, Project url: https://acht7111020.github.io/DSMAP-demo/
- **Journal**: None
- **Summary**: Style transfer generates an image whose content comes from one image and style from the other. Image-to-image translation approaches with disentangled representations have been shown effective for style transfer between two image categories. However, previous methods often assume a shared domain-invariant content space, which could compromise the content representation power. For addressing this issue, this paper leverages domain-specific mappings for remapping latent features in the shared content space to domain-specific content spaces. This way, images can be encoded more properly for style transfer. Experiments show that the proposed method outperforms previous style transfer methods, particularly on challenging scenarios that would require semantic correspondences between images. Code and results are available at https://acht7111020.github.io/DSMAP-demo/.



### Can You Read Me Now? Content Aware Rectification using Angle Supervision
- **Arxiv ID**: http://arxiv.org/abs/2008.02231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.02231v1)
- **Published**: 2020-08-05 16:58:13+00:00
- **Updated**: 2020-08-05 16:58:13+00:00
- **Authors**: Amir Markovitz, Inbal Lavi, Or Perel, Shai Mazor, Roee Litman
- **Comment**: Presented in ECCV 2020
- **Journal**: None
- **Summary**: The ubiquity of smartphone cameras has led to more and more documents being captured by cameras rather than scanned. Unlike flatbed scanners, photographed documents are often folded and crumpled, resulting in large local variance in text structure. The problem of document rectification is fundamental to the Optical Character Recognition (OCR) process on documents, and its ability to overcome geometric distortions significantly affects recognition accuracy. Despite the great progress in recent OCR systems, most still rely on a pre-process that ensures the text lines are straight and axis aligned. Recent works have tackled the problem of rectifying document images taken in-the-wild using various supervision signals and alignment means. However, they focused on global features that can be extracted from the document's boundaries, ignoring various signals that could be obtained from the document's content.   We present CREASE: Content Aware Rectification using Angle Supervision, the first learned method for document rectification that relies on the document's content, the location of the words and specifically their orientation, as hints to assist in the rectification process. We utilize a novel pixel-wise angle regression approach and a curvature estimation side-task for optimizing our rectification model. Our method surpasses previous approaches in terms of OCR accuracy, geometric error and visual similarity.



### Fully Automated and Standardized Segmentation of Adipose Tissue Compartments by Deep Learning in Three-dimensional Whole-body MRI of Epidemiological Cohort Studies
- **Arxiv ID**: http://arxiv.org/abs/2008.02251v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2008.02251v1)
- **Published**: 2020-08-05 17:30:14+00:00
- **Updated**: 2020-08-05 17:30:14+00:00
- **Authors**: Thomas K√ºstner, Tobias Hepp, Marc Fischer, Martin Schwartz, Andreas Fritsche, Hans-Ulrich H√§ring, Konstantin Nikolaou, Fabian Bamberg, Bin Yang, Fritz Schick, Sergios Gatidis, J√ºrgen Machann
- **Comment**: This manuscript has been accepted for publication in Radiology:
  Artificial Intelligence (https://pubs.rsna.org/journal/ai), which is
  published by the Radiological Society of North America (RSNA)
- **Journal**: None
- **Summary**: Purpose: To enable fast and reliable assessment of subcutaneous and visceral adipose tissue compartments derived from whole-body MRI. Methods: Quantification and localization of different adipose tissue compartments from whole-body MR images is of high interest to examine metabolic conditions. For correct identification and phenotyping of individuals at increased risk for metabolic diseases, a reliable automatic segmentation of adipose tissue into subcutaneous and visceral adipose tissue is required. In this work we propose a 3D convolutional neural network (DCNet) to provide a robust and objective segmentation. In this retrospective study, we collected 1000 cases (66$\pm$ 13 years; 523 women) from the Tuebingen Family Study and from the German Center for Diabetes research (TUEF/DZD), as well as 300 cases (53$\pm$ 11 years; 152 women) from the German National Cohort (NAKO) database for model training, validation, and testing with a transfer learning between the cohorts. These datasets had variable imaging sequences, imaging contrasts, receiver coil arrangements, scanners and imaging field strengths. The proposed DCNet was compared against a comparable 3D UNet segmentation in terms of sensitivity, specificity, precision, accuracy, and Dice overlap. Results: Fast (5-7seconds) and reliable adipose tissue segmentation can be obtained with high Dice overlap (0.94), sensitivity (96.6%), specificity (95.1%), precision (92.1%) and accuracy (98.4%) from 3D whole-body MR datasets (field of view coverage 450x450x2000mm${}^3$). Segmentation masks and adipose tissue profiles are automatically reported back to the referring physician. Conclusion: Automatic adipose tissue segmentation is feasible in 3D whole-body MR data sets and is generalizable to different epidemiological cohort studies with the proposed DCNet.



### Performance Improvement of Path Planning algorithms with Deep Learning Encoder Model
- **Arxiv ID**: http://arxiv.org/abs/2008.02254v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02254v1)
- **Published**: 2020-08-05 17:34:31+00:00
- **Updated**: 2020-08-05 17:34:31+00:00
- **Authors**: Janderson Ferreira, Agostinho A. F. J√∫nior, Yves M. Galv√£o, Pablo Barros, Sergio Murilo Maciel Fernandes, Bruno J. T. Fernandes
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, path planning algorithms are used in many daily tasks. They are relevant to find the best route in traffic and make autonomous robots able to navigate. The use of path planning presents some issues in large and dynamic environments. Large environments make these algorithms spend much time finding the shortest path. On the other hand, dynamic environments request a new execution of the algorithm each time a change occurs in the environment, and it increases the execution time. The dimensionality reduction appears as a solution to this problem, which in this context means removing useless paths present in those environments. Most of the algorithms that reduce dimensionality are limited to the linear correlation of the input data. Recently, a Convolutional Neural Network (CNN) Encoder was used to overcome this situation since it can use both linear and non-linear information to data reduction. This paper analyzes in-depth the performance to eliminate the useless paths using this CNN Encoder model. To measure the mentioned model efficiency, we combined it with different path planning algorithms. Next, the final algorithms (combined and not combined) are checked in a database that is composed of five scenarios. Each scenario contains fixed and dynamic obstacles. Their proposed model, the CNN Encoder, associated to other existent path planning algorithms in the literature, was able to obtain a time decrease to find the shortest path in comparison to all path planning algorithms analyzed. the average decreased time was 54.43 %.



### Learning Long-term Visual Dynamics with Region Proposal Interaction Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.02265v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02265v5)
- **Published**: 2020-08-05 17:48:00+00:00
- **Updated**: 2021-04-02 20:12:04+00:00
- **Authors**: Haozhi Qi, Xiaolong Wang, Deepak Pathak, Yi Ma, Jitendra Malik
- **Comment**: ICLR 2021; Code: https://github.com/HaozhiQi/RPIN Website:
  https://haozhiqi.github.io/RPIN/ v5: update PHYRE results of each evaluation
  fold
- **Journal**: None
- **Summary**: Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long-range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object's trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Code, pre-trained models, and more visualization results are available at https://haozhi.io/RPIN.



### NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections
- **Arxiv ID**: http://arxiv.org/abs/2008.02268v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02268v3)
- **Published**: 2020-08-05 17:51:16+00:00
- **Updated**: 2021-01-06 13:45:14+00:00
- **Authors**: Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth
- **Comment**: Project website: https://nerf-w.github.io. Ricardo Martin-Brualla,
  Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work.
  Updated with results for three additional scenes
- **Journal**: None
- **Summary**: We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.



### Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs
- **Arxiv ID**: http://arxiv.org/abs/2008.02312v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02312v4)
- **Published**: 2020-08-05 18:42:33+00:00
- **Updated**: 2020-08-19 06:04:28+00:00
- **Authors**: Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li
- **Comment**: BMVC 2020 (Oral presentation). Code is avaliable at:
  https://github.com/Fu0511/XGrad-CAM
- **Journal**: None
- **Summary**: To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. In spite of the reasonable visualization, lack of clear and sufficient theoretical support is the main limitation of these methods. In this paper, we introduce two axioms -- Conservation and Sensitivity -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of conservation and sensitivity. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. The code is available at https://github.com/Fu0511/XGrad-CAM.



### Can I Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations
- **Arxiv ID**: http://arxiv.org/abs/2008.02321v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02321v2)
- **Published**: 2020-08-05 19:00:36+00:00
- **Updated**: 2021-02-25 03:04:37+00:00
- **Authors**: Hongtao Wu, Gregory S. Chirikjian
- **Comment**: IEEE Robotics and Automation Letters. Video demos are available on
  https://chirikjianlab.github.io/realcontainerimagination/
- **Journal**: None
- **Summary**: Open containers, i.e., containers without covers, are an important and ubiquitous class of objects in human life. In this letter, we propose a novel method for robots to "imagine" the open containability affordance of a previously unseen object via physical simulations. We implement our imagination method on a UR5 manipulator. The robot autonomously scans the object with an RGB-D camera. The scanned 3D model is used for open containability imagination which quantifies the open containability affordance by physically simulating dropping particles onto the object and counting how many particles are retained in it. This quantification is used for open-container vs. non-open-container binary classification (hereafter referred to as open container classification). If the object is classified as an open container, the robot further imagines pouring into the object, again using physical simulations, to obtain the pouring position and orientation for real robot autonomous pouring. We evaluate our method on open container classification and autonomous pouring of granular material on a dataset containing 130 previously unseen objects with 57 object categories. Although our proposed method uses only 11 objects for simulation calibration (training), its open container classification aligns well with human judgements. In addition, our method endows the robot with the capability to autonomously pour into the 55 containers in the dataset with a very high success rate. We also compare to a deep learning method. Results show that our method achieves the same performance as the deep learning method on open container classification and outperforms it on autonomous pouring. Moreover, our method is fully explainable.



### Global Voxel Transformer Networks for Augmented Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2008.02340v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.02340v2)
- **Published**: 2020-08-05 20:11:15+00:00
- **Updated**: 2020-11-23 16:45:20+00:00
- **Authors**: Zhengyang Wang, Yaochen Xie, Shuiwang Ji
- **Comment**: Supplementary Material:
  https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:9fcf9e0d-6ea2-470b-8a89-ed09ac634ef8
- **Journal**: None
- **Summary**: Advances in deep learning have led to remarkable success in augmented microscopy, enabling us to obtain high-quality microscope images without using expensive microscopy hardware and sample preparation techniques. However, current deep learning models for augmented microscopy are mostly U-Net based neural networks, thus sharing certain drawbacks that limit the performance. In this work, we introduce global voxel transformer networks (GVTNets), an advanced deep learning tool for augmented microscopy that overcomes intrinsic limitations of the current U-Net based models and achieves improved performance. GVTNets are built on global voxel transformer operators (GVTOs), which are able to aggregate global information, as opposed to local operators like convolutions. We apply the proposed methods on existing datasets for three different augmented microscopy tasks under various settings. The performance is significantly and consistently better than previous U-Net based approaches.



### Exploiting Temporal Attention Features for Effective Denoising in Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.02344v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02344v2)
- **Published**: 2020-08-05 20:17:18+00:00
- **Updated**: 2020-08-27 05:04:50+00:00
- **Authors**: Aryansh Omray, Samyak Jain, Utsav Krishnan, Pratik Chattopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Video Denoising is one of the fundamental tasks of any videoprocessing pipeline. It is different from image denoising due to the tem-poral aspects of video frames, and any image denoising approach appliedto videos will result in flickering. The proposed method makes use oftemporal as well as spatial dimensions of video frames as part of a two-stage pipeline. Each stage in the architecture named as Spatio-TemporalNetwork uses a channel-wise attention mechanism to forward the encodersignal to the decoder side. The Attention Block used in this paper usessoft attention to ranks the filters for better training.



### A Neural-Symbolic Framework for Mental Simulation
- **Arxiv ID**: http://arxiv.org/abs/2008.02356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2008.02356v1)
- **Published**: 2020-08-05 20:43:55+00:00
- **Updated**: 2020-08-05 20:43:55+00:00
- **Authors**: Michael Kissner
- **Comment**: Dissertation
- **Journal**: None
- **Summary**: We present a neural-symbolic framework for observing the environment and continuously learning visual semantics and intuitive physics to reproduce them in an interactive simulation. The framework consists of five parts, a neural-symbolic hybrid network based on capsules for inverse graphics, an episodic memory to store observations, an interaction network for intuitive physics, a meta-learning agent that continuously improves the framework and a querying language that acts as the framework's interface for simulation. By means of lifelong meta-learning, the capsule network is expanded and trained continuously, in order to better adapt to its environment with each iteration. This enables it to learn new semantics using a few-shot approach and with minimal input from an oracle over its lifetime. From what it learned through observation, the part for intuitive physics infers all the required physical properties of the objects in a scene, enabling predictions. Finally, a custom query language ties all parts together, which allows to perform various mental simulation tasks, such as navigation, sorting and simulation of a game environment, with which we illustrate the potential of our novel approach.



### A robot that counts like a child: a developmental model of counting and pointing
- **Arxiv ID**: http://arxiv.org/abs/2008.02366v2
- **DOI**: 10.1007/s00426-020-01428-8
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.02366v2)
- **Published**: 2020-08-05 21:06:27+00:00
- **Updated**: 2020-11-02 23:48:30+00:00
- **Authors**: Leszek Pecyna, Angelo Cangelosi, Alessandro Di Nuovo
- **Comment**: 28 pages, 13 figures. This is a pre-print of an article published in
  Psychological Research. The final authenticated version is available online
  at: https://doi.org/10.1007/s00426-020-01428-8
- **Journal**: Psychological Research (2020)
- **Summary**: In this paper, a novel neuro-robotics model capable of counting real items is introduced. The model allows us to investigate the interaction between embodiment and numerical cognition. This is composed of a deep neural network capable of image processing and sequential tasks performance, and a robotic platform providing the embodiment - the iCub humanoid robot. The network is trained using images from the robot's cameras and proprioceptive signals from its joints. The trained model is able to count a set of items and at the same time points to them. We investigate the influence of pointing on the counting process and compare our results with those from studies with children. Several training approaches are presented in this paper all of them uses pre-training routine allowing the network to gain the ability of pointing and number recitation (from 1 to 10) prior to counting training. The impact of the counted set size and distance to the objects are investigated. The obtained results on counting performance show similarities with those from human studies.



### OverNet: Lightweight Multi-Scale Super-Resolution with Overscaling Network
- **Arxiv ID**: http://arxiv.org/abs/2008.02382v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02382v2)
- **Published**: 2020-08-05 22:10:29+00:00
- **Updated**: 2020-11-09 17:11:58+00:00
- **Authors**: Parichehr Behjati, Pau Rodriguez, Armin Mehri, Isabelle Hupont, Jordi Gonzalez, Carles Fernandez Tena
- **Comment**: 10 pages, 4 figures, conference, accepted by WACV2021
- **Journal**: None
- **Summary**: Super-resolution (SR) has achieved great success due to the development of deep convolutional neural networks (CNNs). However, as the depth and width of the networks increase, CNN-based SR methods have been faced with the challenge of computational complexity in practice. Moreover, most of them train a dedicated model for each target resolution, losing generality and increasing memory requirements. To address these limitations we introduce OverNet, a deep but lightweight convolutional network to solve SISR at arbitrary scale factors with a single model. We make the following contributions: first, we introduce a lightweight recursive feature extractor that enforces efficient reuse of information through a novel recursive structure of skip and dense connections. Second, to maximize the performance of the feature extractor we propose a reconstruction module that generates accurate high-resolution images from overscaled feature maps and can be independently used to improve existing architectures. Third, we introduce a multi-scale loss function to achieve generalization across scales. Through extensive experiments, we demonstrate that our network outperforms previous state-of-the-art results in standard benchmarks while using fewer parameters than previous approaches.



### Learning Illumination from Diverse Portraits
- **Arxiv ID**: http://arxiv.org/abs/2008.02396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2008.02396v1)
- **Published**: 2020-08-05 23:41:23+00:00
- **Updated**: 2020-08-05 23:41:23+00:00
- **Authors**: Chloe LeGendre, Wan-Chun Ma, Rohit Pandey, Sean Fanello, Christoph Rhemann, Jason Dourgarian, Jay Busch, Paul Debevec
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning-based technique for estimating high dynamic range (HDR), omnidirectional illumination from a single low dynamic range (LDR) portrait image captured under arbitrary indoor or outdoor lighting conditions. We train our model using portrait photos paired with their ground truth environmental illumination. We generate a rich set of such photos by using a light stage to record the reflectance field and alpha matte of 70 diverse subjects in various expressions. We then relight the subjects using image-based relighting with a database of one million HDR lighting environments, compositing the relit subjects onto paired high-resolution background imagery recorded during the lighting acquisition. We train the lighting estimation model using rendering-based loss functions and add a multi-scale adversarial loss to estimate plausible high frequency lighting detail. We show that our technique outperforms the state-of-the-art technique for portrait-based lighting estimation, and we also show that our method reliably handles the inherent ambiguity between overall lighting strength and surface albedo, recovering a similar scale of illumination for subjects with diverse skin tones. We demonstrate that our method allows virtual objects and digital characters to be added to a portrait photograph with consistent illumination. Our lighting inference runs in real-time on a smartphone, enabling realistic rendering and compositing of virtual objects into live video for augmented reality applications.




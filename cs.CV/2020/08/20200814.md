# Arxiv Papers in cs.CV on 2020-08-14
### Landmark detection in Cardiac Magnetic Resonance Imaging Using A Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.06142v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2008.06142v1)
- **Published**: 2020-08-14 00:25:59+00:00
- **Updated**: 2020-08-14 00:25:59+00:00
- **Authors**: Hui Xue, Jessica Artico, Marianna Fontana, James C Moon, Rhodri H Davies, Peter Kellman
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To develop a convolutional neural network (CNN) solution for robust landmark detection in cardiac MR images.   Methods: This retrospective study included cine, LGE and T1 mapping scans from two hospitals. The training set included 2,329 patients and 34,019 images. A hold-out test set included 531 patients and 7,723 images. CNN models were developed to detect two mitral valve plane and apical points on long-axis (LAX) images. On short-axis (SAX) images, anterior and posterior RV insertion points and LV center were detected. Model outputs were compared to manual labels by two operators for accuracy with a t-test for statistical significance. The trained model was deployed to MR scanners.   Results: For the LAX images, success detection was 99.8% for cine, 99.4% for LGE. For the SAX, success rate was 96.6%, 97.6% and 98.9% for cine, LGE and T1-mapping. The L2 distances between model and manual labels were 2 to 3.5 mm, indicating close agreement between model landmarks to manual labels. No significant differences were found for the anterior RV insertion angle and LV length by the models and operators for all views and imaging sequences. Model inference on MR scanner took 610ms/5.6s on GPU/CPU, respectively, for a typical cardiac cine series.   Conclusions: This study developed, validated and deployed a CNN solution for robust landmark detection in both long and short-axis CMR images for cine, LGE and T1 mapping sequences, with the accuracy comparable to the inter-operator variation.



### Interpretation of Brain Morphology in Association to Alzheimer's Disease Dementia Classification Using Graph Convolutional Networks on Triangulated Meshes
- **Arxiv ID**: http://arxiv.org/abs/2008.06151v3
- **DOI**: 10.1007/978-3-030-61056-2_8
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.SP, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2008.06151v3)
- **Published**: 2020-08-14 01:10:39+00:00
- **Updated**: 2020-08-20 06:58:20+00:00
- **Authors**: Emanuel A. Azcona, Pierre Besson, Yunan Wu, Arjun Punjabi, Adam Martersteck, Amil Dravid, Todd B. Parrish, S. Kathleen Bandt, Aggelos K. Katsaggelos
- **Comment**: Accepted for the Shape in Medical Imaging (ShapeMI) workshop at
  MICCAI International Conference 2020
- **Journal**: None
- **Summary**: We propose a mesh-based technique to aid in the classification of Alzheimer's disease dementia (ADD) using mesh representations of the cortex and subcortical structures. Deep learning methods for classification tasks that utilize structural neuroimaging often require extensive learning parameters to optimize. Frequently, these approaches for automated medical diagnosis also lack visual interpretability for areas in the brain involved in making a diagnosis. This work: (a) analyzes brain shape using surface information of the cortex and subcortical structures, (b) proposes a residual learning framework for state-of-the-art graph convolutional networks which offer a significant reduction in learnable parameters, and (c) offers visual interpretability of the network via class-specific gradient information that localizes important regions of interest in our inputs. With our proposed method leveraging the use of cortical and subcortical surface information, we outperform other machine learning methods with a 96.35% testing accuracy for the ADD vs. healthy control problem. We confirm the validity of our model by observing its performance in a 25-trial Monte Carlo cross-validation. The generated visualization maps in our study show correspondences with current knowledge regarding the structural localization of pathological changes in the brain associated to dementia of the Alzheimer's type.



### Unsupervised Image Restoration Using Partially Linear Denoisers
- **Arxiv ID**: http://arxiv.org/abs/2008.06164v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06164v2)
- **Published**: 2020-08-14 02:13:19+00:00
- **Updated**: 2021-03-30 13:56:06+00:00
- **Authors**: Rihuan Ke, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network based methods are the state of the art in various image restoration problems. Standard supervised learning frameworks require a set of noisy measurement and clean image pairs for which a distance between the output of the restoration model and the ground truth, clean images is minimized. The ground truth images, however, are often unavailable or very expensive to acquire in real-world applications. We circumvent this problem by proposing a class of structured denoisers that can be decomposed as the sum of a nonlinear image-dependent mapping, a linear noise-dependent term and a small residual term. We show that these denoisers can be trained with only noisy images under the condition that the noise has zero mean and known variance. The exact distribution of the noise, however, is not assumed to be known. We show the superiority of our approach for image denoising, and demonstrate its extension to solving other restoration problems such as blind deblurring where the ground truth is not available. Our method outperforms some recent unsupervised and self-supervised deep denoising models that do not require clean images for their training. For blind deblurring problems, the method, using only one noisy and blurry observation per image, reaches a quality not far away from its fully supervised counterparts on a benchmark dataset.



### A Multimodal Late Fusion Model for E-Commerce Product Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.06179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2008.06179v1)
- **Published**: 2020-08-14 03:46:24+00:00
- **Updated**: 2020-08-14 03:46:24+00:00
- **Authors**: Ye Bi, Shuo Wang, Zhongrui Fan
- **Comment**: 4 pages, SIGIR 2020 E-commerce Workshop Data Challenge Technical
  Report
- **Journal**: None
- **Summary**: The cataloging of product listings is a fundamental problem for most e-commerce platforms. Despite promising results obtained by unimodal-based methods, it can be expected that their performance can be further boosted by the consideration of multimodal product information. In this study, we investigated a multimodal late fusion approach based on text and image modalities to categorize e-commerce products on Rakuten. Specifically, we developed modal specific state-of-the-art deep neural networks for each input modal, and then fused them at the decision level. Experimental results on Multimodal Product Classification Task of SIGIR 2020 E-Commerce Workshop Data Challenge demonstrate the superiority and effectiveness of our proposed method compared with unimodal and other multimodal methods. Our team named pa_curis won the 1st place with a macro-F1 of 0.9144 on the final leaderboard.



### Apparel-invariant Feature Learning for Apparel-changed Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2008.06181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06181v2)
- **Published**: 2020-08-14 03:49:14+00:00
- **Updated**: 2020-08-17 03:14:12+00:00
- **Authors**: Zhengxu Yu, Yilun Zhao, Bin Hong, Zhongming Jin, Jianqiang Huang, Deng Cai, Xiaofei He, Xian-Sheng Hua
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons' appearance rarely changes. In real-world applications such as in a shopping mall, the same person's clothing may change, and different persons may wearing similar clothes. All these cases can result in an inconsistent ReID performance, revealing a critical problem that current ReID models heavily rely on person's apparels. Therefore, it is critical to learn an apparel-invariant person representation under cases like cloth changing or several persons wearing similar clothes. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth changing images according to the target cloth embedding. It's worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth changing images. We conduct extensive experiments on several datasets comparing with several baselines. Experimental results demonstrate that our proposal can improve the ReID performance of the baseline models.



### An Improved Deep Convolutional Neural Network-Based Autonomous Road Inspection Scheme Using Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2008.06189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06189v1)
- **Published**: 2020-08-14 04:35:10+00:00
- **Updated**: 2020-08-14 04:35:10+00:00
- **Authors**: Syed Ali Hassan, Tariq Rahim, Soo Young Shin
- **Comment**: 10 pages, 11 figures, journal
- **Journal**: None
- **Summary**: Advancements in artificial intelligence (AI) gives a great opportunity to develop an autonomous devices. The contribution of this work is an improved convolutional neural network (CNN) model and its implementation for the detection of road cracks, potholes, and yellow lane in the road. The purpose of yellow lane detection and tracking is to realize autonomous navigation of unmanned aerial vehicle (UAV) by following yellow lane while detecting and reporting the road cracks and potholes to the server through WIFI or 5G medium. The fabrication of own data set is a hectic and time-consuming task. The data set is created, labeled and trained using default and an improved model. The performance of both these models is benchmarked with respect to accuracy, mean average precision (mAP) and detection time. In the testing phase, it was observed that the performance of the improved model is better in respect of accuracy and mAP. The improved model is implemented in UAV using the robot operating system for the autonomous detection of potholes and cracks in roads via UAV front camera vision in real-time.



### Structure-Aware Network for Lane Marker Extraction with Dynamic Vision Sensor
- **Arxiv ID**: http://arxiv.org/abs/2008.06204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06204v1)
- **Published**: 2020-08-14 06:28:20+00:00
- **Updated**: 2020-08-14 06:28:20+00:00
- **Authors**: Wensheng Cheng, Hao Luo, Wen Yang, Lei Yu, Wei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Lane marker extraction is a basic yet necessary task for autonomous driving. Although past years have witnessed major advances in lane marker extraction with deep learning models, they all aim at ordinary RGB images generated by frame-based cameras, which limits their performance in extreme cases, like huge illumination change. To tackle this problem, we introduce Dynamic Vision Sensor (DVS), a type of event-based sensor to lane marker extraction task and build a high-resolution DVS dataset for lane marker extraction. We collect the raw event data and generate 5,424 DVS images with a resolution of 1280$\times$800 pixels, the highest one among all DVS datasets available now. All images are annotated with multi-class semantic segmentation format. We then propose a structure-aware network for lane marker extraction in DVS images. It can capture directional information comprehensively with multidirectional slice convolution. We evaluate our proposed network with other state-of-the-art lane marker extraction models on this dataset. Experimental results demonstrate that our method outperforms other competitors. The dataset is made publicly available, including the raw event data, accumulated images and labels.



### Parameter Sharing Exploration and Hetero-Center based Triplet Loss for Visible-Thermal Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.06223v2
- **DOI**: 10.1109/TMM.2020.3042080
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06223v2)
- **Published**: 2020-08-14 07:40:35+00:00
- **Updated**: 2020-12-04 01:36:49+00:00
- **Authors**: Haijun Liu, Xiaoheng Tan, Xichuan Zhou
- **Comment**: to be published in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: This paper focuses on the visible-thermal cross-modality person re-identification (VT Re-ID) task, whose goal is to match person images between the daytime visible modality and the nighttime thermal modality. The two-stream network is usually adopted to address the cross-modality discrepancy, the most challenging problem for VT Re-ID, by learning the multi-modality person features. In this paper, we explore how many parameters of two-stream network should share, which is still not well investigated in the existing literature. By well splitting the ResNet50 model to construct the modality-specific feature extracting network and modality-sharing feature embedding network, we experimentally demonstrate the effect of parameters sharing of two-stream network for VT Re-ID. Moreover, in the framework of part-level person feature learning, we propose the hetero-center based triplet loss to relax the strict constraint of traditional triplet loss through replacing the comparison of anchor to all the other samples by anchor center to all the other centers. With the extremely simple means, the proposed method can significantly improve the VT Re-ID performance. The experimental results on two datasets show that our proposed method distinctly outperforms the state-of-the-art methods by large margins, especially on RegDB dataset achieving superior performance, rank1/mAP/mINP 91.05%/83.28%/68.84%. It can be a new baseline for VT Re-ID, with a simple but effective strategy.



### BriNet: Towards Bridging the Intra-class and Inter-class Gaps in One-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06226v1)
- **Published**: 2020-08-14 07:45:50+00:00
- **Updated**: 2020-08-14 07:45:50+00:00
- **Authors**: Xianghui Yang, Bairun Wang, Kaige Chen, Xinchi Zhou, Shuai Yi, Wanli Ouyang, Luping Zhou
- **Comment**: 14 pages, 6 figures, BMVC2020(Oral)
- **Journal**: None
- **Summary**: Few-shot segmentation focuses on the generalization of models to segment unseen object instances with limited training samples. Although tremendous improvements have been achieved, existing methods are still constrained by two factors. (1) The information interaction between query and support images is not adequate, leaving intra-class gap. (2) The object categories at the training and inference stages have no overlap, leaving the inter-class gap. Thus, we propose a framework, BriNet, to bridge these gaps. First, more information interactions are encouraged between the extracted features of the query and support images, i.e., using an Information Exchange Module to emphasize the common objects. Furthermore, to precisely localize the query objects, we design a multi-path fine-grained strategy which is able to make better use of the support feature representations. Second, a new online refinement strategy is proposed to help the trained model adapt to unseen classes, achieved by switching the roles of the query and the support images at the inference stage. The effectiveness of our framework is demonstrated by experimental results, which outperforms other competitive methods and leads to a new state-of-the-art on both PASCAL VOC and MSCOCO dataset.



### Deep Atrous Guided Filter for Image Restoration in Under Display Cameras
- **Arxiv ID**: http://arxiv.org/abs/2008.06229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06229v2)
- **Published**: 2020-08-14 07:54:52+00:00
- **Updated**: 2020-09-01 06:15:45+00:00
- **Authors**: Varun Sundar, Sumanth Hegde, Divya Kothandaraman, Kaushik Mitra
- **Comment**: To appear in ECCV 2020 RLQ Workshop. Supplementary material attached.
  For project website, see
  https://varun19299.github.io/deep-atrous-guided-filter/
- **Journal**: None
- **Summary**: Under Display Cameras present a promising opportunity for phone manufacturers to achieve bezel-free displays by positioning the camera behind semi-transparent OLED screens. Unfortunately, such imaging systems suffer from severe image degradation due to light attenuation and diffraction effects. In this work, we present Deep Atrous Guided Filter (DAGF), a two-stage, end-to-end approach for image restoration in UDC systems. A Low-Resolution Network first restores image quality at low-resolution, which is subsequently used by the Guided Filter Network as a filtering input to produce a high-resolution output. Besides the initial downsampling, our low-resolution network uses multiple, parallel atrous convolutions to preserve spatial resolution and emulates multi-scale processing. Our approach's ability to directly train on megapixel images results in significant performance improvement. We additionally propose a simple simulation scheme to pre-train our model and boost performance. Our overall framework ranks 2nd and 5th in the RLQ-TOD'20 UDC Challenge for POLED and TOLED displays, respectively.



### Generating Image Adversarial Examples by Embedding Digital Watermarks
- **Arxiv ID**: http://arxiv.org/abs/2009.05107v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2009.05107v2)
- **Published**: 2020-08-14 09:03:26+00:00
- **Updated**: 2022-08-03 18:00:06+00:00
- **Authors**: Yuexin Xiang, Tiantian Li, Wei Ren, Tianqing Zhu, Kim-Kwang Raymond Choo
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: With the increasing attention to deep neural network (DNN) models, attacks are also upcoming for such models. For example, an attacker may carefully construct images in specific ways (also referred to as adversarial examples) aiming to mislead the DNN models to output incorrect classification results. Similarly, many efforts are proposed to detect and mitigate adversarial examples, usually for certain dedicated attacks. In this paper, we propose a novel digital watermark-based method to generate image adversarial examples to fool DNN models. Specifically, partial main features of the watermark image are embedded into the host image almost invisibly, aiming to tamper with and damage the recognition capabilities of the DNN models. We devise an efficient mechanism to select host images and watermark images and utilize the improved discrete wavelet transform (DWT) based Patchwork watermarking algorithm with a set of valid hyperparameters to embed digital watermarks from the watermark image dataset into original images for generating image adversarial examples. The experimental results illustrate that the attack success rate on common DNN models can reach an average of 95.47% on the CIFAR-10 dataset and the highest at 98.71%. Besides, our scheme is able to generate a large number of adversarial examples efficiently, concretely, an average of 1.17 seconds for completing the attacks on each image on the CIFAR-10 dataset. In addition, we design a baseline experiment using the watermark images generated by Gaussian noise as the watermark image dataset that also displays the effectiveness of our scheme. Similarly, we also propose the modified discrete cosine transform (DCT) based Patchwork watermarking algorithm. To ensure repeatability and reproducibility, the source code is available on GitHub.



### ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.06254v4
- **DOI**: 10.1145/3394171.3413600
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06254v4)
- **Published**: 2020-08-14 09:11:18+00:00
- **Updated**: 2022-03-27 07:49:43+00:00
- **Authors**: Ye Liu, Junsong Yuan, Chang Wen Chen
- **Comment**: Accepted to Proceedings of the 28th ACM International Conference on
  Multimedia (MM 2020)
- **Journal**: None
- **Summary**: We consider the problem of Human-Object Interaction (HOI) Detection, which aims to locate and recognize HOI instances in the form of <human, action, object> in images. Most existing works treat HOIs as individual interaction categories, thus can not handle the problem of long-tail distribution and polysemy of action labels. We argue that multi-level consistencies among objects, actions and interactions are strong cues for generating semantic representations of rare or previously unseen HOIs. Leveraging the compositional and relational peculiarities of HOI labels, we propose ConsNet, a knowledge-aware framework that explicitly encodes the relations among objects, actions and interactions into an undirected graph called consistency graph, and exploits Graph Attention Networks (GATs) to propagate knowledge among HOI categories as well as their constituents. Our model takes visual features of candidate human-object pairs and word embeddings of HOI labels as inputs, maps them into visual-semantic joint embedding space and obtains detection results by measuring their similarities. We extensively evaluate our model on the challenging V-COCO and HICO-DET datasets, and results validate that our approach outperforms state-of-the-arts under both fully-supervised and zero-shot settings. Code is available at https://github.com/yeliudev/ConsNet.



### WAN: Watermarking Attack Network
- **Arxiv ID**: http://arxiv.org/abs/2008.06255v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06255v3)
- **Published**: 2020-08-14 09:11:46+00:00
- **Updated**: 2021-10-20 13:32:30+00:00
- **Authors**: Seung-Hun Nam, In-Jae Yu, Seung-Min Mun, Daesik Kim, Wonhyuk Ahn
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Multi-bit watermarking (MW) has been developed to improve robustness against signal processing operations and geometric distortions. To this end, benchmark tools that test robustness by applying simulated attacks on watermarked images are available. However, limitations in these general attacks exist since they cannot exploit specific characteristics of the targeted MW. In addition, these attacks are usually devised without consideration of visual quality, which rarely occurs in the real world. To address these limitations, we propose a watermarking attack network (WAN), a fully trainable watermarking benchmark tool that utilizes the weak points of the target MW and induces an inversion of the watermark bit, thereby considerably reducing the watermark extractability. To hinder the extraction of hidden information while ensuring high visual quality, we utilize a residual dense blocks-based architecture specialized in local and global feature learning. A novel watermarking attack loss is introduced to break the MW systems. We empirically demonstrate that the WAN can successfully fool various block-based MW systems. Moreover, we show that existing MW methods can be improved with the help of the WAN as an add-on module.



### Unsupervised vs. transfer learning for multimodal one-shot matching of speech and images
- **Arxiv ID**: http://arxiv.org/abs/2008.06258v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.06258v1)
- **Published**: 2020-08-14 09:13:37+00:00
- **Updated**: 2020-08-14 09:13:37+00:00
- **Authors**: Leanne Nortje, Herman Kamper
- **Comment**: Accepted at Interspeech 2020
- **Journal**: None
- **Summary**: We consider the task of multimodal one-shot speech-image matching. An agent is shown a picture along with a spoken word describing the object in the picture, e.g. cookie, broccoli and ice-cream. After observing one paired speech-image example per class, it is shown a new set of unseen pictures, and asked to pick the "ice-cream". Previous work attempted to tackle this problem using transfer learning: supervised models are trained on labelled background data not containing any of the one-shot classes. Here we compare transfer learning to unsupervised models trained on unlabelled in-domain data. On a dataset of paired isolated spoken and visual digits, we specifically compare unsupervised autoencoder-like models to supervised classifier and Siamese neural networks. In both unimodal and multimodal few-shot matching experiments, we find that transfer learning outperforms unsupervised training. We also present experiments towards combining the two methodologies, but find that transfer learning still performs best (despite idealised experiments showing the benefits of unsupervised learning).



### A Learning-based Method for Online Adjustment of C-arm Cone-Beam CT Source Trajectories for Artifact Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2008.06262v1
- **DOI**: 10.1007/s11548-020-02249-1
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06262v1)
- **Published**: 2020-08-14 09:23:50+00:00
- **Updated**: 2020-08-14 09:23:50+00:00
- **Authors**: Mareike Thies, Jan-Nico Zäch, Cong Gao, Russell Taylor, Nassir Navab, Andreas Maier, Mathias Unberath
- **Comment**: 12 pages
- **Journal**: Int. J. CARS 15 (2020) 1787-1796
- **Summary**: During spinal fusion surgery, screws are placed close to critical nerves suggesting the need for highly accurate screw placement. Verifying screw placement on high-quality tomographic imaging is essential. C-arm Cone-beam CT (CBCT) provides intraoperative 3D tomographic imaging which would allow for immediate verification and, if needed, revision. However, the reconstruction quality attainable with commercial CBCT devices is insufficient, predominantly due to severe metal artifacts in the presence of pedicle screws. These artifacts arise from a mismatch between the true physics of image formation and an idealized model thereof assumed during reconstruction. Prospectively acquiring views onto anatomy that are least affected by this mismatch can, therefore, improve reconstruction quality. We propose to adjust the C-arm CBCT source trajectory during the scan to optimize reconstruction quality with respect to a certain task, i.e. verification of screw placement. Adjustments are performed on-the-fly using a convolutional neural network that regresses a quality index for possible next views given the current x-ray image. Adjusting the CBCT trajectory to acquire the recommended views results in non-circular source orbits that avoid poor images, and thus, data inconsistencies. We demonstrate that convolutional neural networks trained on realistically simulated data are capable of predicting quality metrics that enable scene-specific adjustments of the CBCT source trajectory. Using both realistically simulated data and real CBCT acquisitions of a semi-anthropomorphic phantom, we show that tomographic reconstructions of the resulting scene-specific CBCT acquisitions exhibit improved image quality particularly in terms of metal artifacts. Since the optimization objective is implicitly encoded in a neural network, the proposed approach overcomes the need for 3D information at run-time.



### Optimized Deep Encoder-Decoder Methods for Crack Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06266v2
- **DOI**: 10.1016/j.dsp.2020.102907
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06266v2)
- **Published**: 2020-08-14 09:43:43+00:00
- **Updated**: 2021-08-26 09:57:55+00:00
- **Authors**: Jacob König, Mark Jenkins, Mike Mannion, Peter Barrie, Gordon Morison
- **Comment**: Accepted Manuscript at Digital Signal Processing (Elsevier)
- **Journal**: None
- **Summary**: Surface crack segmentation poses a challenging computer vision task as background, shape, colour and size of cracks vary. In this work we propose optimized deep encoder-decoder methods consisting of a combination of techniques which yield an increase in crack segmentation performance. Specifically we propose a decoder-part for an encoder-decoder based deep learning architecture for semantic segmentation and study its components to achieve increased performance. We also examine the use of different encoder strategies and introduce a data augmentation policy to increase the amount of available training data. The performance evaluation of our method is carried out on four publicly available crack segmentation datasets. Additionally, we introduce two techniques into the field of surface crack segmentation, previously not used there: Generating results using test-time-augmentation and performing a statistical result analysis over multiple training runs. The former approach generally yields increased performance results, whereas the latter allows for more reproducible and better representability of a methods results. Using those aforementioned strategies with our proposed encoder-decoder architecture we are able to achieve new state of the art results in all datasets.



### Homotopic Gradients of Generative Density Priors for MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2008.06284v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06284v2)
- **Published**: 2020-08-14 10:30:12+00:00
- **Updated**: 2021-02-01 16:22:56+00:00
- **Authors**: Cong Quan, Jinjie Zhou, Yuanzheng Zhu, Yang Chen, Shanshan Wang, Dong Liang, Qiegen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning, particularly the generative model, has demonstrated tremendous potential to significantly speed up image reconstruction with reduced measurements recently. Rather than the existing generative models that often optimize the density priors, in this work, by taking advantage of the denoising score matching, homotopic gradients of generative density priors (HGGDP) are proposed for magnetic resonance imaging (MRI) reconstruction. More precisely, to tackle the low-dimensional manifold and low data density region issues in generative density prior, we estimate the target gradients in higher-dimensional space. We train a more powerful noise conditional score network by forming high-dimensional tensor as the network input at the training phase. More artificial noise is also injected in the embedding space. At the reconstruction stage, a homotopy method is employed to pursue the density prior, such as to boost the reconstruction performance. Experiment results imply the remarkable performance of HGGDP in terms of high reconstruction accuracy; only 10% of the k-space data can still generate images of high quality as effectively as standard MRI reconstruction with the fully sampled data.



### Rb-PaStaNet: A Few-Shot Human-Object Interaction Detection Based on Rules and Part States
- **Arxiv ID**: http://arxiv.org/abs/2008.06285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06285v1)
- **Published**: 2020-08-14 10:32:15+00:00
- **Updated**: 2020-08-14 10:32:15+00:00
- **Authors**: Shenyu Zhang, Zichen Zhu, Qingquan Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Existing Human-Object Interaction (HOI) Detection approaches have achieved great progress on nonrare classes while rare HOI classes are still not well-detected. In this paper, we intend to apply human prior knowledge into the existing work. So we add human-labeled rules to PaStaNet and propose Rb-PaStaNet aimed at improving rare HOI classes detection. Our results show a certain improvement of the rare classes, while the non-rare classes and the overall improvement is more considerable.



### GeoLayout: Geometry Driven Room Layout Estimation Based on Depth Maps of Planes
- **Arxiv ID**: http://arxiv.org/abs/2008.06286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06286v1)
- **Published**: 2020-08-14 10:34:24+00:00
- **Updated**: 2020-08-14 10:34:24+00:00
- **Authors**: Weidong Zhang, Wei Zhang, Yinda Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The task of room layout estimation is to locate the wall-floor, wall-ceiling, and wall-wall boundaries. Most recent methods solve this problem based on edge/keypoint detection or semantic segmentation. However, these approaches have shown limited attention on the geometry of the dominant planes and the intersection between them, which has significant impact on room layout. In this work, we propose to incorporate geometric reasoning to deep learning for layout estimation. Our approach learns to infer the depth maps of the dominant planes in the scene by predicting the pixel-level surface parameters, and the layout can be generated by the intersection of the depth maps. Moreover, we present a new dataset with pixel-level depth annotation of dominant planes. It is larger than the existing datasets and contains both cuboid and non-cuboid rooms. Experimental results show that our approach produces considerable performance gains on both 2D and 3D datasets.



### Not 3D Re-ID: a Simple Single Stream 2D Convolution for Robust Video Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2008.06318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06318v2)
- **Published**: 2020-08-14 12:19:32+00:00
- **Updated**: 2020-08-17 10:49:24+00:00
- **Authors**: Toby P. Breckon, Aishah Alsehaim
- **Comment**: have been submitted to ICPR 2020 and has been ACCEPTED for
  presentation and inclusion in the proceedings
- **Journal**: None
- **Summary**: Video-based person re-identification has received increasing attention recently, as it plays an important role within surveillance video analysis. Video-based Re-ID is an expansion of earlier image-based re-identification methods by learning features from a video via multiple image frames for each person. Most contemporary video Re-ID methods utilise complex CNNbased network architectures using 3D convolution or multibranch networks to extract spatial-temporal video features. By contrast, in this paper, we illustrate superior performance from a simple single stream 2D convolution network leveraging the ResNet50-IBN architecture to extract frame-level features followed by temporal attention for clip level features. These clip level features can be generalised to extract video level features by averaging without any significant additional cost. Our approach uses best video Re-ID practice and transfer learning between datasets to outperform existing state-of-the-art approaches on the MARS, PRID2011 and iLIDS-VID datasets with 89:62%, 97:75%, 97:33% rank-1 accuracy respectively and with 84:61% mAP for MARS, without reliance on complex and memory intensive 3D convolutions or multi-stream networks architectures as found in other contemporary work. Conversely, our work shows that global features extracted by the 2D convolution network are a sufficient representation for robust state of the art video Re-ID.



### Survey of XAI in digital pathology
- **Arxiv ID**: http://arxiv.org/abs/2008.06353v1
- **DOI**: 10.1007/978-3-030-50402-1_4
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06353v1)
- **Published**: 2020-08-14 13:11:54+00:00
- **Updated**: 2020-08-14 13:11:54+00:00
- **Authors**: Milda Pocevičiūtė, Gabriel Eilertsen, Claes Lundström
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of AI to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions.



### PointMixup: Augmentation for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2008.06374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06374v1)
- **Published**: 2020-08-14 13:57:20+00:00
- **Updated**: 2020-08-14 13:57:20+00:00
- **Authors**: Yunlu Chen, Vincent Tao Hu, Efstratios Gavves, Thomas Mensink, Pascal Mettes, Pengwan Yang, Cees G. M. Snoek
- **Comment**: Accepted as Spotlight presentation at European Conference on Computer
  Vision (ECCV), 2020
- **Journal**: None
- **Summary**: This paper introduces data augmentation for point clouds by interpolation between examples. Data augmentation by interpolation has shown to be a simple and effective approach in the image domain. Such a mixup is however not directly transferable to point clouds, as we do not have a one-to-one correspondence between the points of two different objects. In this paper, we define data augmentation between point clouds as a shortest path linear interpolation. To that end, we introduce PointMixup, an interpolation method that generates new examples through an optimal assignment of the path function between two point clouds. We prove that our PointMixup finds the shortest path between two point clouds and that the interpolation is assignment invariant and linear. With the definition of interpolation, PointMixup allows to introduce strong interpolation-based regularizers such as mixup and manifold mixup to the point cloud domain. Experimentally, we show the potential of PointMixup for point cloud classification, especially when examples are scarce, as well as increased robustness to noise and geometric transformations to points. The code for PointMixup and the experimental details are publicly available.



### Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans
- **Arxiv ID**: http://arxiv.org/abs/2008.06388v4
- **DOI**: 10.1038/s42256-021-00307-0
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06388v4)
- **Published**: 2020-08-14 14:25:21+00:00
- **Updated**: 2021-01-05 19:41:55+00:00
- **Authors**: Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Aviles-Rivero, Christian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas, James H. F. Rudd, Evis Sala, Carola-Bibiane Schönlieb
- **Comment**: 35 pages, 3 figures, 2 tables, updated to the period 1 January 2020 -
  3 October 2020
- **Journal**: Nature Machine Intelligence 3, 199-217 (2021)
- **Summary**: Machine learning methods offer great promise for fast and accurate detection and prognostication of COVID-19 from standard-of-care chest radiographs (CXR) and computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we search EMBASE via OVID, MEDLINE via PubMed, bioRxiv, medRxiv and arXiv for published papers and preprints uploaded from January 1, 2020 to October 3, 2020 which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 61 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher quality model development and well documented manuscripts.



### Renormalization for Initialization of Rolling Shutter Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2008.06399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.06399v2)
- **Published**: 2020-08-14 14:54:15+00:00
- **Updated**: 2021-03-24 19:46:11+00:00
- **Authors**: Branislav Micusik, Georgios Evangelidis
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we deal with the initialization problem of a visual-inertial odometry system with rolling shutter cameras. Initialization is a prerequisite for using inertial signals and fusing them with visual data. We propose a novel statistical solution to the initialization problem on visual and inertial data simultaneously, by casting it into the renormalization scheme of Kanatani. The renormalization is an optimization scheme which intends to reduce the inherent statistical bias of common linear systems. We derive and present the necessary steps and methodology specific to the initialization problem. Extensive evaluations on ground truth exhibit superior performance and a gain in accuracy of up to $20\%$ over the originally proposed Least Squares solution. The renormalization performs similarly to the optimal Maximum Likelihood estimate, despite arriving at the solution by different means. With this paper we are adding to the set of Computer Vision problems which can be cast into the renormalization scheme.



### SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud
- **Arxiv ID**: http://arxiv.org/abs/2008.06402v2
- **DOI**: 10.1145/3372224.3419194
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06402v2)
- **Published**: 2020-08-14 15:00:19+00:00
- **Updated**: 2020-08-24 10:24:41+00:00
- **Authors**: Stefanos Laskaridis, Stylianos I. Venieris, Mario Almeida, Ilias Leontiadis, Nicholas D. Lane
- **Comment**: Accepted at the 26th Annual International Conference on Mobile
  Computing and Networking (MobiCom), 2020
- **Journal**: None
- **Summary**: Despite the soaring use of convolutional neural networks (CNNs) in mobile applications, uniformly sustaining high-performance inference on mobile has been elusive due to the excessive computational demands of modern CNNs and the increasing diversity of deployed devices. A popular alternative comprises offloading CNN processing to powerful cloud-based servers. Nevertheless, by relying on the cloud to produce outputs, emerging mission-critical and high-mobility applications, such as drone obstacle avoidance or interactive applications, can suffer from the dynamic connectivity conditions and the uncertain availability of the cloud. In this paper, we propose SPINN, a distributed inference system that employs synergistic device-cloud computation together with a progressive inference method to deliver fast and robust CNN inference across diverse settings. The proposed system introduces a novel scheduler that co-optimises the early-exit policy and the CNN splitting at run time, in order to adapt to dynamic conditions and meet user-defined service-level requirements. Quantitative evaluation illustrates that SPINN outperforms its state-of-the-art collaborative inference counterparts by up to 2x in achieved throughput under varying network conditions, reduces the server cost by up to 6.8x and improves accuracy by 20.7% under latency constraints, while providing robust operation under uncertain connectivity conditions and significant energy savings compared to cloud-centric execution.



### The Effect of Various Strengths of Noises and Data Augmentations on Classification of Short Single-Lead ECG Signals Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.01192v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01192v1)
- **Published**: 2020-08-14 15:26:17+00:00
- **Updated**: 2020-08-14 15:26:17+00:00
- **Authors**: Faezeh Nejati Hatamian, AmirAbbas Davari, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the multiple imperfections during the signal acquisition, Electrocardiogram (ECG) datasets are typically contaminated with numerous types of noise, like salt and pepper and baseline drift. These datasets may contain different recordings with various types of noise [1] and thus, denoising may not be the easiest task. Furthermore, usually, the number of labeled bio-signals is very limited for a proper classification task.



### RODEO: Replay for Online Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.06439v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06439v1)
- **Published**: 2020-08-14 16:03:52+00:00
- **Updated**: 2020-08-14 16:03:52+00:00
- **Authors**: Manoj Acharya, Tyler L. Hayes, Christopher Kanan
- **Comment**: Accepted for poster presentation at BMVC2020
- **Journal**: None
- **Summary**: Humans can incrementally learn to do new visual detection tasks, which is a huge challenge for today's computer vision systems. Incrementally trained deep learning models lack backwards transfer to previously seen classes and suffer from a phenomenon known as $"catastrophic forgetting."$ In this paper, we pioneer online streaming learning for object detection, where an agent must learn examples one at a time with severe memory and computational constraints. In object detection, a system must output all bounding boxes for an image with the correct label. Unlike earlier work, the system described in this paper can learn this task in an online manner with new classes being introduced over time. We achieve this capability by using a novel memory replay mechanism that efficiently replays entire scenes. We achieve state-of-the-art results on both the PASCAL VOC 2007 and MS COCO datasets.



### Self-adapting confidence estimation for stereo
- **Arxiv ID**: http://arxiv.org/abs/2008.06447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06447v2)
- **Published**: 2020-08-14 16:17:28+00:00
- **Updated**: 2020-11-24 18:01:13+00:00
- **Authors**: Matteo Poggi, Filippo Aleotti, Fabio Tosi, Giulio Zaccaroni, Stefano Mattoccia
- **Comment**: ECCV 2020 (errata corrige: eq.6, k domain)
- **Journal**: None
- **Summary**: Estimating the confidence of disparity maps inferred by a stereo algorithm has become a very relevant task in the years, due to the increasing number of applications leveraging such cue. Although self-supervised learning has recently spread across many computer vision tasks, it has been barely considered in the field of confidence estimation. In this paper, we propose a flexible and lightweight solution enabling self-adapting confidence estimation agnostic to the stereo algorithm or network. Our approach relies on the minimum information available in any stereo setup (i.e., the input stereo pair and the output disparity map) to learn an effective confidence measure. This strategy allows us not only a seamless integration with any stereo system, including consumer and industrial devices equipped with undisclosed stereo perception methods, but also, due to its self-adapting capability, for its out-of-the-box deployment in the field. Exhaustive experimental results with different standard datasets support our claims, showing how our solution is the first-ever enabling online learning of accurate confidence estimation for any stereo system and without any requirement for the end-user.



### Abstracting Deep Neural Networks into Concept Graphs for Concept Level Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2008.06457v2
- **DOI**: 10.1007/978-3-030-93080-6_15
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.06457v2)
- **Published**: 2020-08-14 16:34:32+00:00
- **Updated**: 2020-11-17 07:12:01+00:00
- **Authors**: Avinash Kori, Parth Natekar, Ganapathy Krishnamurthi, Balaji Srinivasan
- **Comment**: None
- **Journal**: None
- **Summary**: The black-box nature of deep learning models prevents them from being completely trusted in domains like biomedicine. Most explainability techniques do not capture the concept-based reasoning that human beings follow. In this work, we attempt to understand the behavior of trained models that perform image processing tasks in the medical domain by building a graphical representation of the concepts they learn. Extracting such a graphical representation of the model's behavior on an abstract, higher conceptual level would unravel the learnings of these models and would help us to evaluate the steps taken by the model for predictions. We show the application of our proposed implementation on two biomedical problems - brain tumor segmentation and fundus image classification. We provide an alternative graphical representation of the model by formulating a concept level graph as discussed above, which makes the problem of intervention to find active inference trails more tractable. Understanding these trails would provide an understanding of the hierarchy of the decision-making process followed by the model. [As well as overall nature of model]. Our framework is available at https://github.com/koriavinash1/BioExp



### Self-Sampling for Neural Point Cloud Consolidation
- **Arxiv ID**: http://arxiv.org/abs/2008.06471v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06471v3)
- **Published**: 2020-08-14 17:16:02+00:00
- **Updated**: 2022-05-13 09:19:55+00:00
- **Authors**: Gal Metzer, Rana Hanocka, Raja Giryes, Daniel Cohen-Or
- **Comment**: TOG 2021
- **Journal**: None
- **Summary**: We introduce a novel technique for neural point cloud consolidation which learns from only the input point cloud. Unlike other point upsampling methods which analyze shapes via local patches, in this work, we learn from global subsets. We repeatedly self-sample the input point cloud with global subsets that are used to train a deep neural network. Specifically, we define source and target subsets according to the desired consolidation criteria (e.g., generating sharp points or points in sparse regions). The network learns a mapping from source to target subsets, and implicitly learns to consolidate the point cloud. During inference, the network is fed with random subsets of points from the input, which it displaces to synthesize a consolidated point set. We leverage the inductive bias of neural networks to eliminate noise and outliers, a notoriously difficult problem in point cloud consolidation. The shared weights of the network are optimized over the entire shape, learning non-local statistics and exploiting the recurrence of local-scale geometries. Specifically, the network encodes the distribution of the underlying shape surface within a fixed set of local kernels, which results in the best explanation of the underlying shape surface. We demonstrate the ability to consolidate point sets from a variety of shapes, while eliminating outliers and noise.



### Feedback Attention for Cell Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.06474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06474v1)
- **Published**: 2020-08-14 17:23:32+00:00
- **Updated**: 2020-08-14 17:23:32+00:00
- **Authors**: Hiroki Tsuda, Eisuke Shibuya, Kazuhiro Hotta
- **Comment**: 14 pages, 4 figures, Accepted by ECCV2020 Workshop "BioImage
  Computing (BIC)"
- **Journal**: None
- **Summary**: In this paper, we address cell image segmentation task by Feedback Attention mechanism like feedback processing. Unlike conventional neural network models of feedforward processing, we focused on the feedback processing in human brain and assumed that the network learns like a human by connecting feature maps from deep layers to shallow layers. We propose some Feedback Attentions which imitate human brain and feeds back the feature maps of output layer to close layer to the input. U-Net with Feedback Attention showed better result than the conventional methods using only feedforward processing.



### Learning Gradient Fields for Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2008.06520v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.06520v2)
- **Published**: 2020-08-14 18:06:15+00:00
- **Updated**: 2020-08-18 04:34:18+00:00
- **Authors**: Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, Bharath Hariharan
- **Comment**: Published in ECCV 2020 (Spotlight); Project page:
  https://www.cs.cornell.edu/~ruojin/ShapeGF/
- **Journal**: None
- **Summary**: In this work, we propose a novel technique to generate shapes from point cloud data. A point cloud can be viewed as samples from a distribution of 3D points whose density is concentrated near the surface of the shape. Point cloud generation thus amounts to moving randomly sampled points to high-density areas. We generate point clouds by performing stochastic gradient ascent on an unnormalized probability density, thereby moving sampled points toward the high-likelihood regions. Our model directly predicts the gradient of the log density field and can be trained with a simple objective adapted from score-based generative models. We show that our method can reach state-of-the-art performance for point cloud auto-encoding and generation, while also allowing for extraction of a high-quality implicit surface. Code is available at https://github.com/RuojinCai/ShapeGF.



### MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere Images
- **Arxiv ID**: http://arxiv.org/abs/2008.06534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06534v1)
- **Published**: 2020-08-14 18:33:05+00:00
- **Updated**: 2020-08-14 18:33:05+00:00
- **Authors**: Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian Richardt, James Tompkin
- **Comment**: 25 pages, 13 figures, Published at European Conference on Computer
  Vision (ECCV 2020), Project Page: http://visual.cs.brown.edu/matryodshka
- **Journal**: None
- **Summary**: We introduce a method to convert stereo 360{\deg} (omnidirectional stereo) imagery into a layered, multi-sphere image representation for six degree-of-freedom (6DoF) rendering. Stereo 360{\deg} imagery can be captured from multi-camera systems for virtual reality (VR), but lacks motion parallax and correct-in-all-directions disparity cues. Together, these can quickly lead to VR sickness when viewing content. One solution is to try and generate a format suitable for 6DoF rendering, such as by estimating depth. However, this raises questions as to how to handle disoccluded regions in dynamic scenes. Our approach is to simultaneously learn depth and disocclusions via a multi-sphere image representation, which can be rendered with correct 6DoF disparity and motion parallax in VR. This significantly improves comfort for the viewer, and can be inferred and rendered in real time on modern GPU hardware. Together, these move towards making VR video a more comfortable immersive medium.



### AntiDote: Attention-based Dynamic Optimization for Neural Network Runtime Efficiency
- **Arxiv ID**: http://arxiv.org/abs/2008.06543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06543v1)
- **Published**: 2020-08-14 18:48:13+00:00
- **Updated**: 2020-08-14 18:48:13+00:00
- **Authors**: Fuxun Yu, Chenchen Liu, Di Wang, Yanzhi Wang, Xiang Chen
- **Comment**: Accepted in DATE'2020 (Best Paper Nomination)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) achieved great cognitive performance at the expense of considerable computation load. To relieve the computation load, many optimization works are developed to reduce the model redundancy by identifying and removing insignificant model components, such as weight sparsity and filter pruning. However, these works only evaluate model components' static significance with internal parameter information, ignoring their dynamic interaction with external inputs. With per-input feature activation, the model component significance can dynamically change, and thus the static methods can only achieve sub-optimal results. Therefore, we propose a dynamic CNN optimization framework in this work. Based on the neural network attention mechanism, we propose a comprehensive dynamic optimization framework including (1) testing-phase channel and column feature map pruning, as well as (2) training-phase optimization by targeted dropout. Such a dynamic optimization framework has several benefits: (1) First, it can accurately identify and aggressively remove per-input feature redundancy with considering the model-input interaction; (2) Meanwhile, it can maximally remove the feature map redundancy in various dimensions thanks to the multi-dimension flexibility; (3) The training-testing co-optimization favors the dynamic pruning and helps maintain the model accuracy even with very high feature pruning ratio. Extensive experiments show that our method could bring 37.4% to 54.5% FLOPs reduction with negligible accuracy drop on various of test networks.



### Sketch-Guided Object Localization in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/2008.06551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2008.06551v1)
- **Published**: 2020-08-14 19:35:56+00:00
- **Updated**: 2020-08-14 19:35:56+00:00
- **Authors**: Aditay Tripathi, Rajath R Dani, Anand Mishra, Anirban Chakraborty
- **Comment**: ECCV 2020 accepted
- **Journal**: None
- **Summary**: We introduce the novel problem of localizing all the instances of an object (seen or unseen during training) in a natural image via sketch query. We refer to this problem as sketch-guided object localization. This problem is distinctively different from the traditional sketch-based image retrieval task where the gallery set often contains images with only one object. The sketch-guided object localization proves to be more challenging when we consider the following: (i) the sketches used as queries are abstract representations with little information on the shape and salient attributes of the object, (ii) the sketches have significant variability as they are hand-drawn by a diverse set of untrained human subjects, and (iii) there exists a domain gap between sketch queries and target natural images as these are sampled from very different data distributions. To address the problem of sketch-guided object localization, we propose a novel cross-modal attention scheme that guides the region proposal network (RPN) to generate object proposals relevant to the sketch query. These object proposals are later scored against the query to obtain final localization. Our method is effective with as little as a single sketch query. Moreover, it also generalizes well to object categories not seen during training and is effective in localizing multiple object instances present in the image. Furthermore, we extend our framework to a multi-query setting using novel feature fusion and attention fusion strategies introduced in this paper. The localization performance is evaluated on publicly available object detection benchmarks, viz. MS-COCO and PASCAL-VOC, with sketch queries obtained from `Quick, Draw!'. The proposed method significantly outperforms related baselines on both single-query and multi-query localization tasks.



### Performance characterization of a novel deep learning-based MR image reconstruction pipeline
- **Arxiv ID**: http://arxiv.org/abs/2008.06559v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.06559v1)
- **Published**: 2020-08-14 19:54:08+00:00
- **Updated**: 2020-08-14 19:54:08+00:00
- **Authors**: R. Marc Lebel
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: A novel deep learning-based magnetic resonance imaging reconstruction pipeline was designed to address fundamental image quality limitations of conventional reconstruction to provide high-resolution, low-noise MR images. This pipeline's unique aims were to convert truncation artifact into improved image sharpness while jointly denoising images to improve image quality. This new approach, now commercially available at AIR Recon DL (GE Healthcare, Waukesha, WI), includes a deep convolutional neural network (CNN) to aid in the reconstruction of raw data, ultimately producing clean, sharp images. Here we describe key features of this pipeline and its CNN, characterize its performance in digital reference objects, phantoms, and in-vivo, and present sample images and protocol optimization strategies that leverage image quality improvement for reduced scan time. This new deep learning-based reconstruction pipeline represents a powerful new tool to increase the diagnostic and operational performance of an MRI scanner.



### Audio-Visual Event Localization via Recursive Fusion by Joint Co-Attention
- **Arxiv ID**: http://arxiv.org/abs/2008.06581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2008.06581v1)
- **Published**: 2020-08-14 21:50:26+00:00
- **Updated**: 2020-08-14 21:50:26+00:00
- **Authors**: Bin Duan, Hao Tang, Wei Wang, Ziliang Zong, Guowei Yang, Yan Yan
- **Comment**: None
- **Journal**: None
- **Summary**: The major challenge in audio-visual event localization task lies in how to fuse information from multiple modalities effectively. Recent works have shown that attention mechanism is beneficial to the fusion process. In this paper, we propose a novel joint attention mechanism with multimodal fusion methods for audio-visual event localization. Particularly, we present a concise yet valid architecture that effectively learns representations from multiple modalities in a joint manner. Initially, visual features are combined with auditory features and then turned into joint representations. Next, we make use of the joint representations to attend to visual features and auditory features, respectively. With the help of this joint co-attention, new visual and auditory features are produced, and thus both features can enjoy the mutually improved benefits from each other. It is worth noting that the joint co-attention unit is recursive meaning that it can be performed multiple times for obtaining better joint representations progressively. Extensive experiments on the public AVE dataset have shown that the proposed method achieves significantly better results than the state-of-the-art methods.



### Weakly supervised cross-domain alignment with optimal transport
- **Arxiv ID**: http://arxiv.org/abs/2008.06597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06597v1)
- **Published**: 2020-08-14 22:48:36+00:00
- **Updated**: 2020-08-14 22:48:36+00:00
- **Authors**: Siyang Yuan, Ke Bai, Liqun Chen, Yizhe Zhang, Chenyang Tao, Chunyuan Li, Guoyin Wang, Ricardo Henao, Lawrence Carin
- **Comment**: Accepted to BMVC 2020 (Oral)
- **Journal**: None
- **Summary**: Cross-domain alignment between image objects and text sequences is key to many visual-language tasks, and it poses a fundamental challenge to both computer vision and natural language processing. This paper investigates a novel approach for the identification and optimization of fine-grained semantic similarities between image and text entities, under a weakly-supervised setup, improving performance over state-of-the-art solutions. Our method builds upon recent advances in optimal transport (OT) to resolve the cross-domain matching problem in a principled manner. Formulated as a drop-in regularizer, the proposed OT solution can be efficiently computed and used in combination with other existing approaches. We present empirical evidence to demonstrate the effectiveness of our approach, showing how it enables simpler model architectures to outperform or be comparable with more sophisticated designs on a range of vision-language tasks.



### Self-supervised Contrastive Video-Speech Representation Learning for Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2008.06607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.06607v1)
- **Published**: 2020-08-14 23:58:23+00:00
- **Updated**: 2020-08-14 23:58:23+00:00
- **Authors**: Jianbo Jiao, Yifan Cai, Mohammad Alsharid, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble
- **Comment**: MICCAI 2020 (early acceptance)
- **Journal**: None
- **Summary**: In medical imaging, manual annotations can be expensive to acquire and sometimes infeasible to access, making conventional deep learning-based models difficult to scale. As a result, it would be beneficial if useful representations could be derived from raw data without the need for manual annotations. In this paper, we propose to address the problem of self-supervised representation learning with multi-modal ultrasound video-speech raw data. For this case, we assume that there is a high correlation between the ultrasound video and the corresponding narrative speech audio of the sonographer. In order to learn meaningful representations, the model needs to identify such correlation and at the same time understand the underlying anatomical features. We designed a framework to model the correspondence between video and audio without any kind of human annotations. Within this framework, we introduce cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental evaluations on multi-modal fetal ultrasound video and audio show that the proposed approach is able to learn strong representations and transfers well to downstream tasks of standard plane detection and eye-gaze prediction.




# Arxiv Papers in cs.CV on 2020-08-19
### DeepHandMesh: A Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling
- **Arxiv ID**: http://arxiv.org/abs/2008.08213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08213v1)
- **Published**: 2020-08-19 00:59:51+00:00
- **Updated**: 2020-08-19 00:59:51+00:00
- **Authors**: Gyeongsik Moon, Takaaki Shiratori, Kyoung Mu Lee
- **Comment**: Published at ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: Human hands play a central role in interacting with other people and objects. For realistic replication of such hand motions, high-fidelity hand meshes have to be reconstructed. In this study, we firstly propose DeepHandMesh, a weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling. We design our system to be trained in an end-to-end and weakly-supervised manner; therefore, it does not require groundtruth meshes. Instead, it relies on weaker supervisions such as 3D joint coordinates and multi-view depth maps, which are easier to get than groundtruth meshes and do not dependent on the mesh topology. Although the proposed DeepHandMesh is trained in a weakly-supervised way, it provides significantly more realistic hand mesh than previous fully-supervised hand models. Our newly introduced penetration avoidance loss further improves results by replicating physical interaction between hand parts. Finally, we demonstrate that our system can also be applied successfully to the 3D hand mesh estimation from general images. Our hand model, dataset, and codes are publicly available at https://mks0601.github.io/DeepHandMesh/.



### Stereo Plane SLAM Based on Intersecting Lines
- **Arxiv ID**: http://arxiv.org/abs/2008.08218v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08218v3)
- **Published**: 2020-08-19 01:35:49+00:00
- **Updated**: 2021-07-29 02:25:33+00:00
- **Authors**: Xiaoyu Zhang, Wei Wang, Xianyu Qi, Ziwei Liao
- **Comment**: Accepted at IROS 2021
- **Journal**: None
- **Summary**: Plane feature is a kind of stable landmark to reduce drift error in SLAM system. It is easy and fast to extract planes from dense point cloud, which is commonly acquired from RGB-D camera or lidar. But for stereo camera, it is hard to compute dense point cloud accurately and efficiently. In this paper, we propose a novel method to compute plane parameters using intersecting lines which are extracted from the stereo image. The plane features commonly exist on the surface of man-made objects and structure, which have regular shape and straight edge lines. In 3D space, two intersecting lines can determine such a plane. Thus we extract line segments from both stereo left and right image. By stereo matching, we compute the endpoints and line directions in 3D space, and then the planes from two intersecting lines. We discard those inaccurate plane features in the frame tracking. Adding such plane features in stereo SLAM system reduces the drift error and refines the performance. We test our proposed system on public datasets and demonstrate its robust and accurate estimation results, compared with state-of-the-art SLAM systems. To benefit the research of plane-based SLAM, we release our codes at https://github.com/fishmarch/Stereo-Plane-SLAM.



### Open Source Iris Recognition Hardware and Software with Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.08220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08220v1)
- **Published**: 2020-08-19 02:02:16+00:00
- **Updated**: 2020-08-19 02:02:16+00:00
- **Authors**: Zhaoyuan Fang, Adam Czajka
- **Comment**: Accepted to IJCB 2020
- **Journal**: None
- **Summary**: This paper proposes the first known to us open source hardware and software iris recognition system with presentation attack detection (PAD), which can be easily assembled for about 75 USD using Raspberry Pi board and a few peripherals. The primary goal of this work is to offer a low-cost baseline for spoof-resistant iris recognition, which may (a) stimulate research in iris PAD and allow for easy prototyping of secure iris recognition systems, (b) offer a low-cost secure iris recognition alternative to more sophisticated systems, and (c) serve as an educational platform. We propose a lightweight image complexity-guided convolutional network for fast and accurate iris segmentation, domain-specific human-inspired Binarized Statistical Image Features (BSIF) to build an iris template, and to combine 2D (iris texture) and 3D (photometric stereo-based) features for PAD. The proposed iris recognition runs in about 3.2 seconds and the proposed PAD runs in about 4.5 seconds on Raspberry Pi 3B+. The hardware specifications and all source codes of the entire pipeline are made available along with this paper.



### A New Perspective on Stabilizing GANs training: Direct Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2008.09041v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.09041v5)
- **Published**: 2020-08-19 02:36:53+00:00
- **Updated**: 2022-07-19 14:47:32+00:00
- **Authors**: Ziqiang Li, Pengfei Xia, Rentuo Tao, Hongjing Niu, Bin Li
- **Comment**: Accepted to IEEE Transactions on Emerging Topics in Computational
  Intelligence
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are the most popular image generation models that have achieved remarkable progress on various computer vision tasks. However, training instability is still one of the open problems for all GAN-based algorithms. Quite a number of methods have been proposed to stabilize the training of GANs, the focuses of which were respectively put on the loss functions, regularization and normalization technologies, training algorithms, and model architectures. Different from the above methods, in this paper, a new perspective on stabilizing GANs training is presented. It is found that sometimes the images produced by the generator act like adversarial examples of the discriminator during the training process, which may be part of the reason causing the unstable training of GANs. With this finding, we propose the Direct Adversarial Training (DAT) method to stabilize the training process of GANs. Furthermore, we prove that the DAT method is able to minimize the Lipschitz constant of the discriminator adaptively. The advanced performance of DAT is verified on multiple loss functions, network architectures, hyper-parameters, and datasets. Specifically, DAT achieves significant improvements of 11.5% FID on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10 unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom unconditional generation based on SSGAN. Code will be available at https://github.com/iceli1007/DAT-GAN



### LIRA: Lifelong Image Restoration from Unknown Blended Distortions
- **Arxiv ID**: http://arxiv.org/abs/2008.08242v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08242v1)
- **Published**: 2020-08-19 03:35:45+00:00
- **Updated**: 2020-08-19 03:35:45+00:00
- **Authors**: Jianzhao Liu, Jianxin Lin, Xin Li, Wei Zhou, Sen Liu, Zhibo Chen
- **Comment**: ECCV2020 accepted
- **Journal**: None
- **Summary**: Most existing image restoration networks are designed in a disposable way and catastrophically forget previously learned distortions when trained on a new distortion removal task. To alleviate this problem, we raise the novel lifelong image restoration problem for blended distortions. We first design a base fork-join model in which multiple pre-trained expert models specializing in individual distortion removal task work cooperatively and adaptively to handle blended distortions. When the input is degraded by a new distortion, inspired by adult neurogenesis in human memory system, we develop a neural growing strategy where the previously trained model can incorporate a new expert branch and continually accumulate new knowledge without interfering with learned knowledge. Experimental results show that the proposed approach can not only achieve state-of-the-art performance on blended distortions removal tasks in both PSNR/SSIM metrics, but also maintain old expertise while learning new restoration tasks.



### Salient Instance Segmentation with Region and Box-level Annotations
- **Arxiv ID**: http://arxiv.org/abs/2008.08246v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08246v3)
- **Published**: 2020-08-19 03:43:45+00:00
- **Updated**: 2021-04-28 07:38:49+00:00
- **Authors**: Jialun Pei, He Tang, Tianyang Cheng, Chuanbo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Salient instance segmentation is a new challenging task that received widespread attention in the saliency detection area. The new generation of saliency detection provides a strong theoretical and technical basis for video surveillance. Due to the limited scale of the existing dataset and the high mask annotations cost, plenty of supervision source is urgently needed to train a well-performing salient instance model. In this paper, we aim to train a novel salient instance segmentation framework by an inexact supervision without resorting to laborious labeling. To this end, we present a cyclic global context salient instance segmentation network (CGCNet), which is supervised by the combination of salient regions and bounding boxes from the ready-made salient object detection datasets. To locate salient instance more accurately, a global feature refining layer is proposed that dilates the features of the region of interest (ROI) to the global context in a scene. Meanwhile, a labeling updating scheme is embedded in the proposed framework to update the coarse-grained labels for next iteration. Experiment results demonstrate that the proposed end-to-end framework trained by inexact supervised annotations can be competitive to the existing fully supervised salient instance segmentation methods. Without bells and whistles, our proposed method achieves a mask AP of 58.3% in the test set of Dataset1K that outperforms the mainstream state-of-the-art methods.



### Enhanced MRI Reconstruction Network using Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2008.08248v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08248v1)
- **Published**: 2020-08-19 03:44:31+00:00
- **Updated**: 2020-08-19 03:44:31+00:00
- **Authors**: Qiaoying Huang, Dong Yang, Yikun Xian, Pengxiang Wu, Jingru Yi, Hui Qu, Dimitris Metaxas
- **Comment**: 10 pages. Code will be released soon
- **Journal**: None
- **Summary**: The accurate reconstruction of under-sampled magnetic resonance imaging (MRI) data using modern deep learning technology, requires significant effort to design the necessary complex neural network architectures. The cascaded network architecture for MRI reconstruction has been widely used, while it suffers from the "vanishing gradient" problem when the network becomes deep. In addition, homogeneous architecture degrades the representation capacity of the network. In this work, we present an enhanced MRI reconstruction network using a residual in residual basic block. For each cell in the basic block, we use the differentiable neural architecture search (NAS) technique to automatically choose the optimal operation among eight variants of the dense block. This new heterogeneous network is evaluated on two publicly available datasets and outperforms all current state-of-the-art methods, which demonstrates the effectiveness of our proposed method.



### Face Anti-Spoofing Via Disentangled Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.08250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08250v1)
- **Published**: 2020-08-19 03:54:23+00:00
- **Updated**: 2020-08-19 03:54:23+00:00
- **Authors**: Ke-Yue Zhang, Taiping Yao, Jian Zhang, Ying Tai, Shouhong Ding, Jilin Li, Feiyue Huang, Haichuan Song, Lizhuang Ma
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Face anti-spoofing is crucial to security of face recognition systems. Previous approaches focus on developing discriminative models based on the features extracted from images, which may be still entangled between spoof patterns and real persons. In this paper, motivated by the disentangled representation learning, we propose a novel perspective of face anti-spoofing that disentangles the liveness features and content features from images, and the liveness features is further used for classification. We also put forward a Convolutional Neural Network (CNN) architecture with the process of disentanglement and combination of low-level and high-level supervision to improve the generalization capabilities. We evaluate our method on public benchmark datasets and extensive experimental results demonstrate the effectiveness of our method against the state-of-the-art competitors. Finally, we further visualize some results to help understand the effect and advantage of disentanglement.



### Spatio-temporal relationships between rainfall and convective clouds during Indian Monsoon through a discrete lens
- **Arxiv ID**: http://arxiv.org/abs/2008.08251v1
- **DOI**: 10.1002/joc.6812
- **Categories**: **physics.ao-ph**, cs.CV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2008.08251v1)
- **Published**: 2020-08-19 04:02:39+00:00
- **Updated**: 2020-08-19 04:02:39+00:00
- **Authors**: Arjun Sharma, Adway Mitra, Vishal Vasan, Rama Govindarajan
- **Comment**: 20 pages, 13 figures, 5 tables. Abstract slightly modified from the
  accepted version at International Journal of Climatology
- **Journal**: International Journal of Climatology, 2020
- **Summary**: The Indian monsoon, a multi-variable process causing heavy rains during June-September every year, is very heterogeneous in space and time. We study the relationship between rainfall and Outgoing Longwave Radiation (OLR, convective cloud cover) for monsoon between 2004-2010. To identify, classify and visualize spatial patterns of rainfall and OLR we use a discrete and spatio-temporally coherent representation of the data, created using a statistical model based on Markov Random Field. Our approach clusters the days with similar spatial distributions of rainfall and OLR into a small number of spatial patterns. We find that eight daily spatial patterns each in rainfall and OLR, and seven joint patterns of rainfall and OLR, describe over 90\% of all days. Through these patterns, we find that OLR generally has a strong negative correlation with precipitation, but with significant spatial variations. In particular, peninsular India (except west coast) is under significant convective cloud cover over a majority of days but remains rainless. We also find that much of the monsoon rainfall co-occurs with low OLR, but some amount of rainfall in Eastern and North-western India in June occurs on OLR days, presumably from shallow clouds. To study day-to-day variations of both quantities, we identify spatial patterns in the temporal gradients computed from the observations. We find that changes in convective cloud activity across India most commonly occur due to the establishment of a north-south OLR gradient which persists for 1-2 days and shifts the convective cloud cover from light to deep or vice versa. Such changes are also accompanied by changes in the spatial distribution of precipitation. The present work thus provides a highly reduced description of the complex spatial patterns and their day-to-day variations, and could form a useful tool for future simplified descriptions of this process.



### A Color Elastica Model for Vector-Valued Image Regularization
- **Arxiv ID**: http://arxiv.org/abs/2008.08255v2
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2008.08255v2)
- **Published**: 2020-08-19 04:18:35+00:00
- **Updated**: 2021-03-04 00:42:25+00:00
- **Authors**: Hao Liu, Xue-Cheng Tai, Ron Kimmel, Roland Glowinski
- **Comment**: None
- **Journal**: None
- **Summary**: Models related to the Euler's elastica energy have proven to be useful for many applications including image processing. Extending elastica models to color images and multi-channel data is a challenging task, as stable and consistent numerical solvers for these geometric models often involve high order derivatives. Like the single channel Euler's elastica model and the total variation (TV) models, geometric measures that involve high order derivatives could help when considering image formation models that minimize elastic properties. In the past, the Polyakov action from high energy physics has been successfully applied to color image processing. Here, we introduce an addition to the Polyakov action for color images that minimizes the color manifold curvature. The color image curvature is computed by applying of the Laplace-Beltrami operator to the color image channels. When reduced to gray-scale images, while selecting appropriate scaling between space and color, the proposed model minimizes the Euler's elastica operating on the image level sets. Finding a minimizer for the proposed nonlinear geometric model is a challenge we address in this paper. Specifically, we present an operator-splitting method to minimize the proposed functional. The non-linearity is decoupled by introducing three vector-valued and matrix-valued variables. The problem is then converted into solving for the steady state of an associated initial-value problem. The initial-value problem is time-split into three fractional steps, such that each sub-problem has a closed form solution, or can be solved by fast algorithms. The efficiency and robustness of the proposed method are demonstrated by systematic numerical experiments.



### Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos
- **Arxiv ID**: http://arxiv.org/abs/2008.08257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2008.08257v1)
- **Published**: 2020-08-19 04:42:46+00:00
- **Updated**: 2020-08-19 04:42:46+00:00
- **Authors**: Zhu Zhang, Zhijie Lin, Zhou Zhao, Jieming Zhu, Xiuqiang He
- **Comment**: ACM MM 2020
- **Journal**: None
- **Summary**: Video moment retrieval aims to localize the target moment in an video according to the given sentence. The weak-supervised setting only provides the video-level sentence annotations during training. Most existing weak-supervised methods apply a MIL-based framework to develop inter-sample confrontment, but ignore the intra-sample confrontment between moments with semantically similar contents. Thus, these methods fail to distinguish the target moment from plausible negative moments. In this paper, we propose a novel Regularized Two-Branch Proposal Network to simultaneously consider the inter-sample and intra-sample confrontments. Concretely, we first devise a language-aware filter to generate an enhanced video stream and a suppressed video stream. We then design the sharable two-branch proposal module to generate positive proposals from the enhanced stream and plausible negative proposals from the suppressed one for sufficient confrontment. Further, we apply the proposal regularization to stabilize the training process and improve model performance. The extensive experiments show the effectiveness of our method. Our code is released at here.



### Learning Connectivity of Neural Networks from a Topological Perspective
- **Arxiv ID**: http://arxiv.org/abs/2008.08261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08261v1)
- **Published**: 2020-08-19 04:53:31+00:00
- **Updated**: 2020-08-19 04:53:31+00:00
- **Authors**: Kun Yuan, Quanquan Li, Jing Shao, Junjie Yan
- **Comment**: 17 pages, 5 figures, to be published in ECCV2020
- **Journal**: None
- **Summary**: Seeking effective neural networks is a critical and practical field in deep learning. Besides designing the depth, type of convolution, normalization, and nonlinearities, the topological connectivity of neural networks is also important. Previous principles of rule-based modular design simplify the difficulty of building an effective architecture, but constrain the possible topologies in limited spaces. In this paper, we attempt to optimize the connectivity in neural networks. We propose a topological perspective to represent a network into a complete graph for analysis, where nodes carry out aggregation and transformation of features, and edges determine the flow of information. By assigning learnable parameters to the edges which reflect the magnitude of connections, the learning process can be performed in a differentiable manner. We further attach auxiliary sparsity constraint to the distribution of connectedness, which promotes the learned topology focus on critical connections. This learning process is compatible with existing networks and owns adaptability to larger search spaces and different tasks. Quantitative results of experiments reflect the learned connectivity is superior to traditional rule-based ones, such as random, residual, and complete. In addition, it obtains significant improvements in image classification and object detection without introducing excessive computation burden.



### DONet: Dual Objective Networks for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.08278v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08278v1)
- **Published**: 2020-08-19 06:02:46+00:00
- **Updated**: 2020-08-19 06:02:46+00:00
- **Authors**: Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Skin lesion segmentation is a crucial step in the computer-aided diagnosis of dermoscopic images. In the last few years, deep learning based semantic segmentation methods have significantly advanced the skin lesion segmentation results. However, the current performance is still unsatisfactory due to some challenging factors such as large variety of lesion scale and ambiguous difference between lesion region and background. In this paper, we propose a simple yet effective framework, named Dual Objective Networks (DONet), to improve the skin lesion segmentation. Our DONet adopts two symmetric decoders to produce different predictions for approaching different objectives. Concretely, the two objectives are actually defined by different loss functions. In this way, the two decoders are encouraged to produce differentiated probability maps to match different optimization targets, resulting in complementary predictions accordingly. The complementary information learned by these two objectives are further aggregated together to make the final prediction, by which the uncertainty existing in segmentation maps can be significantly alleviated. Besides, to address the challenge of large variety of lesion scales and shapes in dermoscopic images, we additionally propose a recurrent context encoding module (RCEM) to model the complex correlation among skin lesions, where the features with different scale contexts are efficiently integrated to form a more robust representation. Extensive experiments on two popular benchmarks well demonstrate the effectiveness of the proposed DONet. In particular, our DONet achieves 0.881 and 0.931 dice score on ISIC 2018 and $\text{PH}^2$, respectively. Code will be made public available.



### CCA: Exploring the Possibility of Contextual Camouflage Attack on Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.08281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08281v1)
- **Published**: 2020-08-19 06:16:10+00:00
- **Updated**: 2020-08-19 06:16:10+00:00
- **Authors**: Shengnan Hu, Yang Zhang, Sumit Laha, Ankit Sharma, Hassan Foroosh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network based object detection hasbecome the cornerstone of many real-world applications. Alongwith this success comes concerns about its vulnerability tomalicious attacks. To gain more insight into this issue, we proposea contextual camouflage attack (CCA for short) algorithm to in-fluence the performance of object detectors. In this paper, we usean evolutionary search strategy and adversarial machine learningin interactions with a photo-realistic simulated environment tofind camouflage patterns that are effective over a huge varietyof object locations, camera poses, and lighting conditions. Theproposed camouflages are validated effective to most of the state-of-the-art object detectors.



### Channel-wise Hessian Aware trace-Weighted Quantization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.08284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08284v1)
- **Published**: 2020-08-19 06:34:56+00:00
- **Updated**: 2020-08-19 06:34:56+00:00
- **Authors**: Xu Qian, Victor Li, Crews Darren
- **Comment**: None
- **Journal**: None
- **Summary**: Second-order information has proven to be very effective in determining the redundancy of neural network weights and activations. Recent paper proposes to use Hessian traces of weights and activations for mixed-precision quantization and achieves state-of-the-art results. However, prior works only focus on selecting bits for each layer while the redundancy of different channels within a layer also differ a lot. This is mainly because the complexity of determining bits for each channel is too high for original methods. Here, we introduce Channel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ). CW-HAWQ uses Hessian trace to determine the relative sensitivity order of different channels of activations and weights. What's more, CW-HAWQ proposes to use deep Reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agent to find the optimal ratios of different quantization bits and assign bits to channels according to the Hessian trace order. The number of states in CW-HAWQ is much smaller compared with traditional AutoML based mix-precision methods since we only need to search ratios for the quantization bits. Compare CW-HAWQ with state-of-the-art shows that we can achieve better results for multiple networks.



### Attribute Prototype Network for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2008.08290v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08290v4)
- **Published**: 2020-08-19 06:46:35+00:00
- **Updated**: 2021-05-06 09:13:08+00:00
- **Authors**: Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata
- **Comment**: NeurIPS 2020. The code is publicly available at
  https://wenjiaxu.github.io/APN-ZSL/
- **Journal**: None
- **Summary**: From the beginning of zero-shot learning research, visual attributes have been shown to play an important role. In order to better transfer attribute-based knowledge from known to unknown classes, we argue that an image representation with integrated attribute localization ability would be beneficial for zero-shot learning. To this end, we propose a novel zero-shot representation learning framework that jointly learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. We show that our locality augmented image representations achieve a new state-of-the-art on three zero-shot learning benchmarks. As an additional benefit, our model points to the visual evidence of the attributes in an image, e.g. for the CUB dataset, confirming the improved attribute localization ability of our image representation.



### TNT: Target-driveN Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2008.08294v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.08294v2)
- **Published**: 2020-08-19 06:52:46+00:00
- **Updated**: 2020-08-21 07:33:10+00:00
- **Authors**: Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li, Dragomir Anguelov
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future behavior of moving agents is essential for real world applications. It is challenging as the intent of the agent and the corresponding behavior is unknown and intrinsically multimodal. Our key insight is that for prediction within a moderate time horizon, the future modes can be effectively captured by a set of target states. This leads to our target-driven trajectory prediction (TNT) framework. TNT has three stages which are trained end-to-end. It first predicts an agent's potential target states $T$ steps into the future, by encoding its interactions with the environment and the other agents. TNT then generates trajectory state sequences conditioned on targets. A final stage estimates trajectory likelihoods and a final compact set of trajectory predictions is selected. This is in contrast to previous work which models agent intents as latent variables, and relies on test-time sampling to generate diverse trajectories. We benchmark TNT on trajectory prediction of vehicles and pedestrians, where we outperform state-of-the-art on Argoverse Forecasting, INTERACTION, Stanford Drone and an in-house Pedestrian-at-Intersection dataset.



### Deep Relighting Networks for Image Light Source Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2008.08298v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08298v2)
- **Published**: 2020-08-19 07:03:23+00:00
- **Updated**: 2020-10-15 04:02:17+00:00
- **Authors**: Li-Wen Wang, Wan-Chi Siu, Zhi-Song Liu, Chu-Tak Li, Daniel P. K. Lun
- **Comment**: The 2020 European Conference on Computer Vision
- **Journal**: None
- **Summary**: Manipulating the light source of given images is an interesting task and useful in various applications, including photography and cinematography. Existing methods usually require additional information like the geometric structure of the scene, which may not be available for most images. In this paper, we formulate the single image relighting task and propose a novel Deep Relighting Network (DRN) with three parts: 1) scene reconversion, which aims to reveal the primary scene structure through a deep auto-encoder network, 2) shadow prior estimation, to predict light effect from the new light direction through adversarial learning, and 3) re-renderer, to combine the primary structure with the reconstructed shadow view to form the required estimation under the target light source. Experimental results show that the proposed method outperforms other possible methods, both qualitatively and quantitatively. Specifically, the proposed DRN has achieved the best PSNR in the "AIM2020 - Any to one relighting challenge" of the 2020 ECCV conference.



### Towards Lightweight Lane Detection by Optimizing Spatial Embedding
- **Arxiv ID**: http://arxiv.org/abs/2008.08311v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08311v2)
- **Published**: 2020-08-19 07:37:04+00:00
- **Updated**: 2020-08-27 06:45:20+00:00
- **Authors**: Seokwoo Jung, Sungha Choi, Mohammad Azam Khan, Jaegul Choo
- **Comment**: Preprint - work in progress
- **Journal**: None
- **Summary**: A number of lane detection methods depend on a proposal-free instance segmentation because of its adaptability to flexible object shape, occlusion, and real-time application. This paper addresses the problem that pixel embedding in proposal-free instance segmentation based lane detection is difficult to optimize. A translation invariance of convolution, which is one of the supposed strengths, causes challenges in optimizing pixel embedding. In this work, we propose a lane detection method based on proposal-free instance segmentation, directly optimizing spatial embedding of pixels using image coordinate. Our proposed method allows the post-processing step for center localization and optimizes clustering in an end-to-end manner. The proposed method enables real-time lane detection through the simplicity of post-processing and the adoption of a lightweight backbone. Our proposed method demonstrates competitive performance on public lane detection datasets.



### FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression and Integration
- **Arxiv ID**: http://arxiv.org/abs/2008.08324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08324v1)
- **Published**: 2020-08-19 08:22:30+00:00
- **Updated**: 2020-08-19 08:22:30+00:00
- **Authors**: Yu Rong, Takaaki Shiratori, Hanbyul Joo
- **Comment**: Demo, Code and Models are available at
  https://penincillin.github.io/frank_mocap
- **Journal**: None
- **Summary**: Although the essential nuance of human motion is often conveyed as a combination of body movements and hand gestures, the existing monocular motion capture approaches mostly focus on either body motion capture only ignoring hand parts or hand motion capture only without considering body motion. In this paper, we present FrankMocap, a motion capture system that can estimate both 3D hand and body motion from in-the-wild monocular inputs with faster speed (9.5 fps) and better accuracy than previous work. Our method works in near real-time (9.5 fps) and produces 3D body and hand motion capture outputs as a unified parametric model structure. Our method aims to capture 3D body and hand motion simultaneously from challenging in-the-wild monocular videos. To construct FrankMocap, we build the state-of-the-art monocular 3D "hand" motion capture method by taking the hand part of the whole body parametric model (SMPL-X). Our 3D hand motion capture output can be efficiently integrated to monocular body motion capture output, producing whole body motion results in a unified parrametric model structure. We demonstrate the state-of-the-art performance of our hand motion capture system in public benchmarks, and show the high quality of our whole body motion capture result in various challenging real-world scenes, including a live demo scenario.



### CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2008.08332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08332v1)
- **Published**: 2020-08-19 08:47:50+00:00
- **Updated**: 2020-08-19 08:47:50+00:00
- **Authors**: Yuxi Li, Weiyao Lin, John See, Ning Xu, Shugong Xu, Ke Yan, Cong Yang
- **Comment**: 7 figures, 3 tables; ECCV2020
- **Journal**: None
- **Summary**: Most current pipelines for spatio-temporal action localization connect frame-wise or clip-wise detection results to generate action proposals, where only local information is exploited and the efficiency is hindered by dense per-frame localization. In this paper, we propose Coarse-to-Fine Action Detector (CFAD),an original end-to-end trainable framework for efficient spatio-temporal action localization. The CFAD introduces a new paradigm that first estimates coarse spatio-temporal action tubes from video streams, and then refines the tubes' location based on key timestamps. This concept is implemented by two key components, the Coarse and Refine Modules in our framework. The parameterized modeling of long temporal information in the Coarse Module helps obtain accurate initial tube estimation, while the Refine Module selectively adjusts the tube location under the guidance of key timestamps. Against other methods, theproposed CFAD achieves competitive results on action detection benchmarks of UCF101-24, UCFSports and JHMDB-21 with inference speed that is 3.3x faster than the nearest competitors.



### Towards Class-incremental Object Detection with Nearest Mean of Exemplars
- **Arxiv ID**: http://arxiv.org/abs/2008.08336v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08336v3)
- **Published**: 2020-08-19 08:56:04+00:00
- **Updated**: 2020-10-09 07:23:34+00:00
- **Authors**: Sheng Ren, Yan He, Neal N. Xiong, Kehua Guo
- **Comment**: There are inaccuracies in the statement of the article, and we need
  to withdraw and correct the inaccurate statement
- **Journal**: None
- **Summary**: Incremental learning is a form of online learning. Incremental learning can modify the parameters and structure of the deep learning model so that the model does not forget the old knowledge while learning new knowledge. Preventing catastrophic forgetting is the most important task of incremental learning. However, the current incremental learning is often only for one type of input. For example, if the input images are of the same type, the current incremental model can learn new knowledge while not forgetting old knowledge. However, if several categories are added to the input graphics, the current model will not be able to deal with it correctly, and the accuracy will drop significantly. Therefore, this paper proposes a kind of incremental method, which adjusts the parameters of the model by identifying the prototype vector and increasing the distance of the vector, so that the model can learn new knowledge without catastrophic forgetting. Experiments show the effectiveness of our proposed method.



### Deep Volumetric Ambient Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2008.08345v2
- **DOI**: 10.1109/TVCG.2020.3030344
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV, stat.ML, I.2; I.3
- **Links**: [PDF](http://arxiv.org/pdf/2008.08345v2)
- **Published**: 2020-08-19 09:19:08+00:00
- **Updated**: 2020-10-16 06:13:14+00:00
- **Authors**: Dominik Engel, Timo Ropinski
- **Comment**: IEEE VIS SciVis 2020
- **Journal**: None
- **Summary**: We present a novel deep learning based technique for volumetric ambient occlusion in the context of direct volume rendering. Our proposed Deep Volumetric Ambient Occlusion (DVAO) approach can predict per-voxel ambient occlusion in volumetric data sets, while considering global information provided through the transfer function. The proposed neural network only needs to be executed upon change of this global information, and thus supports real-time volume interaction. Accordingly, we demonstrate DVAOs ability to predict volumetric ambient occlusion, such that it can be applied interactively within direct volume rendering. To achieve the best possible results, we propose and analyze a variety of transfer function representations and injection strategies for deep neural networks. Based on the obtained results we also give recommendations applicable in similar volume learning scenarios. Lastly, we show that DVAO generalizes to a variety of modalities, despite being trained on computed tomography data only.



### Query Twice: Dual Mixture Attention Meta Learning for Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2008.08360v1
- **DOI**: 10.1145/3394171.3414064
- **Categories**: **cs.CV**, cs.MM, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2008.08360v1)
- **Published**: 2020-08-19 10:12:52+00:00
- **Updated**: 2020-08-19 10:12:52+00:00
- **Authors**: Junyan Wang, Yang Bai, Yang Long, Bingzhang Hu, Zhenhua Chai, Yu Guan, Xiaolin Wei
- **Comment**: This manuscript has been accepted at ACM MM 2020
- **Journal**: None
- **Summary**: Video summarization aims to select representative frames to retain high-level information, which is usually solved by predicting the segment-wise importance score via a softmax function. However, softmax function suffers in retaining high-rank representations for complex visual or sequential information, which is known as the Softmax Bottleneck problem. In this paper, we propose a novel framework named Dual Mixture Attention (DMASum) model with Meta Learning for video summarization that tackles the softmax bottleneck problem, where the Mixture of Attention layer (MoA) effectively increases the model capacity by employing twice self-query attention that can capture the second-order changes in addition to the initial query-key attention, and a novel Single Frame Meta Learning rule is then introduced to achieve more generalization to small datasets with limited training sources. Furthermore, the DMASum significantly exploits both visual and sequential attention that connects local key-frame and global attention in an accumulative way. We adopt the new evaluation protocol on two public datasets, SumMe, and TVSum. Both qualitative and quantitative experiments manifest significant improvements over the state-of-the-art methods.



### Virtual Adversarial Training in Feature Space to Improve Unsupervised Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.08369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08369v1)
- **Published**: 2020-08-19 10:30:31+00:00
- **Updated**: 2020-08-19 10:30:31+00:00
- **Authors**: Artjoms Gorpincenko, Geoffrey French, Michal Mackiewicz
- **Comment**: Submitted to the EI conference
- **Journal**: None
- **Summary**: Virtual Adversarial Training has recently seen a lot of success in semi-supervised learning, as well as unsupervised Domain Adaptation. However, so far it has been used on input samples in the pixel space, whereas we propose to apply it directly to feature vectors. We also discuss the unstable behaviour of entropy minimization and Decision-Boundary Iterative Refinement Training With a Teacher in Domain Adaptation, and suggest substitutes that achieve similar behaviour. By adding the aforementioned techniques to the state of the art model TA$^3$N, we either maintain competitive results or outperform prior art in multiple unsupervised video Domain Adaptation tasks



### Addressing Neural Network Robustness with Mixup and Targeted Labeling Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2008.08384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.08384v1)
- **Published**: 2020-08-19 11:34:11+00:00
- **Updated**: 2020-08-19 11:34:11+00:00
- **Authors**: Alfred Laugros, Alice Caplier, Matthieu Ospici
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their performance, Artificial Neural Networks are not reliable enough for most of industrial applications. They are sensitive to noises, rotations, blurs and adversarial examples. There is a need to build defenses that protect against a wide range of perturbations, covering the most traditional common corruptions and adversarial examples. We propose a new data augmentation strategy called M-TLAT and designed to address robustness in a broad sense. Our approach combines the Mixup augmentation and a new adversarial training algorithm called Targeted Labeling Adversarial Training (TLAT). The idea of TLAT is to interpolate the target labels of adversarial examples with the ground-truth labels. We show that M-TLAT can increase the robustness of image classifiers towards nineteen common corruptions and five adversarial attacks, without reducing the accuracy on clean samples.



### Robust RGB-based 6-DoF Pose Estimation without Real Pose Annotations
- **Arxiv ID**: http://arxiv.org/abs/2008.08391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08391v1)
- **Published**: 2020-08-19 12:07:01+00:00
- **Updated**: 2020-08-19 12:07:01+00:00
- **Authors**: Zhigang Li, Yinlin Hu, Mathieu Salzmann, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: While much progress has been made in 6-DoF object pose estimation from a single RGB image, the current leading approaches heavily rely on real-annotation data. As such, they remain sensitive to severe occlusions, because covering all possible occlusions with annotated data is intractable. In this paper, we introduce an approach to robustly and accurately estimate the 6-DoF pose in challenging conditions and without using any real pose annotations. To this end, we leverage the intuition that the poses predicted by a network from an image and from its counterpart synthetically altered to mimic occlusion should be consistent, and translate this to a self-supervised loss function. Our experiments on LINEMOD, Occluded-LINEMOD, YCB and new Randomization LINEMOD dataset evidence the robustness of our approach. We achieve state of the art performance on LINEMOD, and OccludedLINEMOD in without real-pose setting, even outperforming methods that rely on real annotations during training on Occluded-LINEMOD.



### Instance-Aware Graph Convolutional Network for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.08407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08407v1)
- **Published**: 2020-08-19 12:49:28+00:00
- **Updated**: 2020-08-19 12:49:28+00:00
- **Authors**: Yun Wang, Tong Zhang, Zhen Cui, Chunyan Xu, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolutional neural network (GCN) has effectively boosted the multi-label image recognition task by introducing label dependencies based on statistical label co-occurrence of data. However, in previous methods, label correlation is computed based on statistical information of data and therefore the same for all samples, and this makes graph inference on labels insufficient to handle huge variations among numerous image instances. In this paper, we propose an instance-aware graph convolutional neural network (IA-GCN) framework for multi-label classification. As a whole, two fused branches of sub-networks are involved in the framework: a global branch modeling the whole image and a region-based branch exploring dependencies among regions of interests (ROIs). For label diffusion of instance-awareness in graph convolution, rather than using the statistical label correlation alone, an image-dependent label correlation matrix (LCM), fusing both the statistical LCM and an individual one of each image instance, is constructed for graph inference on labels to inject adaptive information of label-awareness into the learned features of the model. Specifically, the individual LCM of each image is obtained by mining the label dependencies based on the scores of labels about detected ROIs. In this process, considering the contribution differences of ROIs to multi-label classification, variational inference is introduced to learn adaptive scaling factors for those ROIs by considering their complex distribution. Finally, extensive experiments on MS-COCO and VOC datasets show that our proposed approach outperforms existing state-of-the-art methods.



### A Systematic Survey of Regularization and Normalization in GANs
- **Arxiv ID**: http://arxiv.org/abs/2008.08930v7
- **DOI**: 10.1145/3569928
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08930v7)
- **Published**: 2020-08-19 12:52:10+00:00
- **Updated**: 2022-11-03 14:15:09+00:00
- **Authors**: Ziqiang Li, Muhammad Usman, Rentuo Tao, Pengfei Xia, Chaoyue Wang, Huanhuan Chen, Bin Li
- **Comment**: Accepted to Acm Computing Survey
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been widely applied in different scenarios thanks to the development of deep neural networks. The original GAN was proposed based on the non-parametric assumption of the infinite capacity of networks. However, it is still unknown whether GANs can fit the target distribution without any prior information. Due to the overconfident assumption, many issues remain unaddressed in GANs' training, such as non-convergence, mode collapses, gradient vanishing. Regularization and normalization are common methods of introducing prior information to stabilize training and improve discrimination. Although a handful number of regularization and normalization methods have been proposed for GANs, to the best of our knowledge, there exists no comprehensive survey that primarily focuses on objectives and development of these methods, apart from some in-comprehensive and limited scope studies. In this work, we conduct a comprehensive survey on the regularization and normalization techniques from different perspectives of GANs training. First, we systematically describe different perspectives of GANs training and thus obtain the different objectives of regularization and normalization. Based on these objectives, we propose a new taxonomy. Furthermore, we compare the performance of the mainstream methods on different datasets and investigate the applications of regularization and normalization techniques that have been frequently employed in state-of-the-art GANs. Finally, we highlight potential future directions of research in this domain. Code and studies related to the regularization and normalization of GANs in this work is summarized on https://github.com/iceli1007/GANs-Regularization-Review.



### Improving Text to Image Generation using Mode-seeking Function
- **Arxiv ID**: http://arxiv.org/abs/2008.08976v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08976v4)
- **Published**: 2020-08-19 12:58:32+00:00
- **Updated**: 2020-09-18 20:00:56+00:00
- **Authors**: Naitik Bhise, Zhenfei Zhang, Tien D. Bui
- **Comment**: changes : changed the title of the research for submission to CVIU
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have long been used to understand the semantic relationship between the text and image. However, there are problems with mode collapsing in the image generation that causes some preferred output modes. Our aim is to improve the training of the network by using a specialized mode-seeking loss function to avoid this issue. In the text to image synthesis, our loss function differentiates two points in latent space for the generation of distinct images. We validate our model on the Caltech Birds (CUB) dataset and the Microsoft COCO dataset by changing the intensity of the loss function during the training. Experimental results demonstrate that our model works very well compared to some state-of-the-art approaches.



### Improving Blind Spot Denoising for Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2008.08414v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2008.08414v1)
- **Published**: 2020-08-19 13:06:24+00:00
- **Updated**: 2020-08-19 13:06:24+00:00
- **Authors**: Anna S. Goncharova, Alf Honigmann, Florian Jug, Alexander Krull
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Many microscopy applications are limited by the total amount of usable light and are consequently challenged by the resulting levels of noise in the acquired images. This problem is often addressed via (supervised) deep learning based denoising. Recently, by making assumptions about the noise statistics, self-supervised methods have emerged. Such methods are trained directly on the images that are to be denoised and do not require additional paired training data. While achieving remarkable results, self-supervised methods can produce high-frequency artifacts and achieve inferior results compared to supervised approaches. Here we present a novel way to improve the quality of self-supervised denoising. Considering that light microscopy images are usually diffraction-limited, we propose to include this knowledge in the denoising process. We assume the clean image to be the result of a convolution with a point spread function (PSF) and explicitly include this operation at the end of our neural network. As a consequence, we are able to eliminate high-frequency artifacts and achieve self-supervised results that are very close to the ones achieved with traditional supervised methods.



### Gradually Applying Weakly Supervised and Active Learning for Mass Detection in Breast Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2008.08416v1
- **DOI**: 10.3390/app10134519
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08416v1)
- **Published**: 2020-08-19 13:09:00+00:00
- **Updated**: 2020-08-19 13:09:00+00:00
- **Authors**: JooYeol Yun, JungWoo Oh, IlDong Yun
- **Comment**: None
- **Journal**: Appl. Sci. 2020, 10(13), 4519
- **Summary**: We propose a method for effectively utilizing weakly annotated image data in an object detection tasks of breast ultrasound images. Given the problem setting where a small, strongly annotated dataset and a large, weakly annotated dataset with no bounding box information are available, training an object detection model becomes a non-trivial problem. We suggest a controlled weight for handling the effect of weakly annotated images in a two stage object detection model. We~also present a subsequent active learning scheme for safely assigning weakly annotated images a strong annotation using the trained model. Experimental results showed a 24\% point increase in correct localization (CorLoc) measure, which is the ratio of correctly localized and classified images, by assigning the properly controlled weight. Performing active learning after a model is trained showed an additional increase in CorLoc. We tested the proposed method on the Stanford Dog datasets to assure that it can be applied to general cases, where strong annotations are insufficient to obtain resembling results. The presented method showed that higher performance is achievable with lesser annotation effort.



### Anchor-free Small-scale Multispectral Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2008.08418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08418v2)
- **Published**: 2020-08-19 13:13:01+00:00
- **Updated**: 2020-08-20 15:01:59+00:00
- **Authors**: Alexander Wolpert, Michael Teutsch, M. Saquib Sarfraz, Rainer Stiefelhagen
- **Comment**: BMVC2020
- **Journal**: None
- **Summary**: Multispectral images consisting of aligned visual-optical (VIS) and thermal infrared (IR) image pairs are well-suited for practical applications like autonomous driving or visual surveillance. Such data can be used to increase the performance of pedestrian detection especially for weakly illuminated, small-scaled, or partially occluded instances. The current state-of-the-art is based on variants of Faster R-CNN and thus passes through two stages: a proposal generator network with handcrafted anchor boxes for object localization and a classification network for verifying the object category. In this paper we propose a method for effective and efficient multispectral fusion of the two modalities in an adapted single-stage anchor-free base architecture. We aim at learning pedestrian representations based on object center and scale rather than direct bounding box predictions. In this way, we can both simplify the network architecture and achieve higher detection performance, especially for pedestrians under occlusion or at low object resolution. In addition, we provide a study on well-suited multispectral data augmentation techniques that improve the commonly used augmentations. The results show our method's effectiveness in detecting small-scaled pedestrians. We achieve 5.68% log-average miss rate in comparison to the best current state-of-the-art of 7.49% (25% improvement) on the challenging KAIST Multispectral Pedestrian Detection Benchmark.   Code: https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection



### Unsupervised Cross-domain Image Classification by Distance Metric Guided Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2008.08433v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.08433v1)
- **Published**: 2020-08-19 13:36:57+00:00
- **Updated**: 2020-08-19 13:36:57+00:00
- **Authors**: Qingjie Meng, Daniel Rueckert, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Learning deep neural networks that are generalizable across different domains remains a challenge due to the problem of domain shift. Unsupervised domain adaptation is a promising avenue which transfers knowledge from a source domain to a target domain without using any labels in the target domain. Contemporary techniques focus on extracting domain-invariant features using domain adversarial training. However, these techniques neglect to learn discriminative class boundaries in the latent representation space on a target domain and yield limited adaptation performance. To address this problem, we propose distance metric guided feature alignment (MetFA) to extract discriminative as well as domain-invariant features on both source and target domains. The proposed MetFA method explicitly and directly learns the latent representation without using domain adversarial training. Our model integrates class distribution alignment to transfer semantic knowledge from a source domain to a target domain. We evaluate the proposed method on fetal ultrasound datasets for cross-device image classification. Experimental results demonstrate that the proposed method outperforms the state-of-the-art and enables model generalization.



### Generating Adjacency Matrix for Video Relocalization
- **Arxiv ID**: http://arxiv.org/abs/2008.08977v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08977v2)
- **Published**: 2020-08-19 13:52:36+00:00
- **Updated**: 2022-01-27 02:59:03+00:00
- **Authors**: Yuan Zhou, Mingfei Wang, Ruolin Wang, Shuwei Huo
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2007.09877
- **Journal**: None
- **Summary**: In this paper, we continue our work on video relocalization task. Based on using graph convolution to extract intra-video and inter-video frame features, we improve the method by using similarity-metric based graph convolution, whose weighted adjacency matrix is achieved by calculating similarity metric between features of any two different time steps in the graph. Experiments on ActivityNet v1.2 and Thumos14 dataset show the effectiveness of this improvement, and it outperforms the state-of-the-art methods.



### SegCodeNet: Color-Coded Segmentation Masks for Activity Detection from Wearable Cameras
- **Arxiv ID**: http://arxiv.org/abs/2008.08452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08452v1)
- **Published**: 2020-08-19 14:01:53+00:00
- **Updated**: 2020-08-19 14:01:53+00:00
- **Authors**: Asif Shahriyar Sushmit, Partho Ghosh, Md. Abrar Istiak, Nayeeb Rashid, Ahsan Habib Akash, Taufiq Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: Activity detection from first-person videos (FPV) captured using a wearable camera is an active research field with potential applications in many sectors, including healthcare, law enforcement, and rehabilitation. State-of-the-art methods use optical flow-based hybrid techniques that rely on features derived from the motion of objects from consecutive frames. In this work, we developed a two-stream network, the \emph{SegCodeNet}, that uses a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects in addition to the original RGB video-stream. We also include a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes the video frames that contain relevant features. Experiments are conducted on an FPV dataset containing $18$ activity classes in office environments. In comparison to a single-stream network, the proposed two-stream method achieves an absolute improvement of $14.366\%$ and $10.324\%$ for averaged F1 score and accuracy, respectively, when average results are compared for three different frame sizes $224\times224$, $112\times112$, and $64\times64$. The proposed method provides significant performance gains for lower-resolution images with absolute improvements of $17\%$ and $26\%$ in F1 score for input dimensions of $112\times112$ and $64\times64$, respectively. The best performance is achieved for a frame size of $224\times224$ yielding an F1 score and accuracy of $90.176\%$ and $90.799\%$ which outperforms the state-of-the-art Inflated 3D ConvNet (I3D) \cite{carreira2017quo} method by an absolute margin of $4.529\%$ and $2.419\%$, respectively.



### Exploring the Impacts from Datasets to Monocular Depth Estimation (MDE) Models with MineNavi
- **Arxiv ID**: http://arxiv.org/abs/2008.08454v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.08454v2)
- **Published**: 2020-08-19 14:03:17+00:00
- **Updated**: 2022-06-28 13:55:03+00:00
- **Authors**: Xiangtong Wang, Binbin Liang, Menglong Yang, Wei Li
- **Comment**: 6 pages,10 figures
- **Journal**: None
- **Summary**: Current computer vision tasks based on deep learning require a huge amount of data with annotations for model training or testing, especially in some dense estimation tasks, such as optical flow segmentation and depth estimation. In practice, manual labeling for dense estimation tasks is very difficult or even impossible, and the scenes of the dataset are often restricted to a small range, which dramatically limits the development of the community. To overcome this deficiency, we propose a synthetic dataset generation method to obtain the expandable dataset without burdensome manual workforce. By this method, we construct a dataset called MineNavi containing video footages from first-perspective-view of the aircraft matched with accurate ground truth for depth estimation in aircraft navigation application. We also provide quantitative experiments to prove that pre-training via our MineNavi dataset can improve the performance of depth estimation model and speed up the convergence of the model on real scene data. Since the synthetic dataset has a similar effect to the real-world dataset in the training process of deep model, we also provide additional experiments with monocular depth estimation method to demonstrate the impact of various factors in our dataset such as lighting conditions and motion mode.



### CosyPose: Consistent multi-view multi-object 6D pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2008.08465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08465v1)
- **Published**: 2020-08-19 14:11:56+00:00
- **Updated**: 2020-08-19 14:11:56+00:00
- **Authors**: Yann Labb, Justin Carpentier, Mathieu Aubry, Josef Sivic
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We introduce an approach for recovering the 6D pose of multiple known objects in a scene captured by a set of input images with unknown camera viewpoints. First, we present a single-view single-object 6D pose estimation method, which we use to generate 6D object pose hypotheses. Second, we develop a robust method for matching individual 6D object pose hypotheses across different input images in order to jointly estimate camera viewpoints and 6D poses of all objects in a single consistent scene. Our approach explicitly handles object symmetries, does not require depth measurements, is robust to missing or incorrect object hypotheses, and automatically recovers the number of objects in the scene. Third, we develop a method for global scene refinement given multiple object hypotheses and their correspondences across views. This is achieved by solving an object-level bundle adjustment problem that refines the poses of cameras and objects to minimize the reprojection error in all views. We demonstrate that the proposed method, dubbed CosyPose, outperforms current state-of-the-art results for single-view and multi-view 6D object pose estimation by a large margin on two challenging benchmarks: the YCB-Video and T-LESS datasets. Code and pre-trained models are available on the project webpage https://www.di.ens.fr/willow/research/cosypose/.



### Cross-Domain Identification for Thermal-to-Visible Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2008.08473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08473v1)
- **Published**: 2020-08-19 14:24:04+00:00
- **Updated**: 2020-08-19 14:24:04+00:00
- **Authors**: Cedric Nimpa Fondje, Shuowen Hu, Nathaniel J. Short, Benjamin S. Riggan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in domain adaptation, especially those applied to heterogeneous facial recognition, typically rely upon restrictive Euclidean loss functions (e.g., $L_2$ norm) which perform best when images from two different domains (e.g., visible and thermal) are co-registered and temporally synchronized. This paper proposes a novel domain adaptation framework that combines a new feature mapping sub-network with existing deep feature models, which are based on modified network architectures (e.g., VGG16 or Resnet50). This framework is optimized by introducing new cross-domain identity and domain invariance loss functions for thermal-to-visible face recognition, which alleviates the requirement for precisely co-registered and synchronized imagery. We provide extensive analysis of both features and loss functions used, and compare the proposed domain adaptation framework with state-of-the-art feature based domain adaptation models on a difficult dataset containing facial imagery collected at varying ranges, poses, and expressions. Moreover, we analyze the viability of the proposed framework for more challenging tasks, such as non-frontal thermal-to-visible face recognition.



### Human Body Model Fitting by Learned Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/2008.08474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08474v1)
- **Published**: 2020-08-19 14:26:47+00:00
- **Updated**: 2020-08-19 14:26:47+00:00
- **Authors**: Jie Song, Xu Chen, Otmar Hilliges
- **Comment**: First two authors contributed equally to this work
- **Journal**: None
- **Summary**: We propose a novel algorithm for the fitting of 3D human shape to images. Combining the accuracy and refinement capabilities of iterative gradient-based optimization techniques with the robustness of deep neural networks, we propose a gradient descent algorithm that leverages a neural network to predict the parameter update rule for each iteration. This per-parameter and state-aware update guides the optimizer towards a good solution in very few steps, converging in typically few steps. During training our approach only requires MoCap data of human poses, parametrized via SMPL. From this data the network learns a subspace of valid poses and shapes in which optimization is performed much more efficiently. The approach does not require any hard to acquire image-to-3D correspondences. At test time we only optimize the 2D joint re-projection error without the need for any further priors or regularization terms. We show empirically that this algorithm is fast (avg. 120ms convergence), robust to initialization and dataset, and achieves state-of-the-art results on public evaluation datasets including the challenging 3DPW in-the-wild benchmark (improvement over SMPLify 45%) and also approaches using image-to-3D correspondences



### Correcting Data Imbalance for Semi-Supervised Covid-19 Detection Using X-ray Chest Images
- **Arxiv ID**: http://arxiv.org/abs/2008.08496v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08496v2)
- **Published**: 2020-08-19 15:16:57+00:00
- **Updated**: 2020-08-20 20:53:08+00:00
- **Authors**: Saul Calderon-Ramirez, Shengxiang-Yang, Armaghan Moemeni, David Elizondo, Simon Colreavy-Donnelly, Luis Fernando Chavarria-Estrada, Miguel A. Molina-Cabello
- **Comment**: Under journal review
- **Journal**: None
- **Summary**: The Corona Virus (COVID-19) is an internationalpandemic that has quickly propagated throughout the world. The application of deep learning for image classification of chest X-ray images of Covid-19 patients, could become a novel pre-diagnostic detection methodology. However, deep learning architectures require large labelled datasets. This is often a limitation when the subject of research is relatively new as in the case of the virus outbreak, where dealing with small labelled datasets is a challenge. Moreover, in the context of a new highly infectious disease, the datasets are also highly imbalanced,with few observations from positive cases of the new disease. In this work we evaluate the performance of the semi-supervised deep learning architecture known as MixMatch using a very limited number of labelled observations and highly imbalanced labelled dataset. We propose a simple approach for correcting data imbalance, re-weight each observationin the loss function, giving a higher weight to the observationscorresponding to the under-represented class. For unlabelled observations, we propose the usage of the pseudo and augmentedlabels calculated by MixMatch to choose the appropriate weight. The MixMatch method combined with the proposed pseudo-label based balance correction improved classification accuracy by up to 10%, with respect to the non balanced MixMatch algorithm, with statistical significance. We tested our proposed approach with several available datasets using 10, 15 and 20 labelledobservations. Additionally, a new dataset is included among thetested datasets, composed of chest X-ray images of Costa Rican adult patients



### Learning Trailer Moments in Full-Length Movies
- **Arxiv ID**: http://arxiv.org/abs/2008.08502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08502v1)
- **Published**: 2020-08-19 15:23:25+00:00
- **Updated**: 2020-08-19 15:23:25+00:00
- **Authors**: Lezi Wang, Dong Liu, Rohit Puri, Dimitris N. Metaxas
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: A movie's key moments stand out of the screenplay to grab an audience's attention and make movie browsing efficient. But a lack of annotations makes the existing approaches not applicable to movie key moment detection. To get rid of human annotations, we leverage the officially-released trailers as the weak supervision to learn a model that can detect the key moments from full-length movies. We introduce a novel ranking network that utilizes the Co-Attention between movies and trailers as guidance to generate the training pairs, where the moments highly corrected with trailers are expected to be scored higher than the uncorrelated moments. Additionally, we propose a Contrastive Attention module to enhance the feature representations such that the comparative contrast between features of the key and non-key moments are maximized. We construct the first movie-trailer dataset, and the proposed Co-Attention assisted ranking network shows superior performance even over the supervised approach. The effectiveness of our Contrastive Attention module is also demonstrated by the performance improvement over the state-of-the-art on the public benchmarks.



### Scene Text Detection with Selected Anchor
- **Arxiv ID**: http://arxiv.org/abs/2008.08523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08523v1)
- **Published**: 2020-08-19 16:03:13+00:00
- **Updated**: 2020-08-19 16:03:13+00:00
- **Authors**: Anna Zhu, Hang Du, Shengwu Xiong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Object proposal technique with dense anchoring scheme for scene text detection were applied frequently to achieve high recall. It results in the significant improvement in accuracy but waste of computational searching, regression and classification. In this paper, we propose an anchor selection-based region proposal network (AS-RPN) using effective selected anchors instead of dense anchors to extract text proposals. The center, scales, aspect ratios and orientations of anchors are learnable instead of fixing, which leads to high recall and greatly reduced numbers of anchors. By replacing the anchor-based RPN in Faster RCNN, the AS-RPN-based Faster RCNN can achieve comparable performance with previous state-of-the-art text detecting approaches on standard benchmarks, including COCO-Text, ICDAR2013, ICDAR2015 and MSRA-TD500 when using single-scale and single model (ResNet50) testing only.



### "Name that manufacturer". Relating image acquisition bias with task complexity when training deep learning models: experiments on head CT
- **Arxiv ID**: http://arxiv.org/abs/2008.08525v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08525v1)
- **Published**: 2020-08-19 16:05:58+00:00
- **Updated**: 2020-08-19 16:05:58+00:00
- **Authors**: Giorgio Pietro Biondetti, Romane Gauriau, Christopher P. Bridge, Charles Lu, Katherine P. Andriole
- **Comment**: 15 pages, 4 figures. This paper has been submitted to the Journal of
  Digital Imaging (Springer Journal) and it is now in review
- **Journal**: None
- **Summary**: As interest in applying machine learning techniques for medical images continues to grow at a rapid pace, models are starting to be developed and deployed for clinical applications. In the clinical AI model development lifecycle (described by Lu et al. [1]), a crucial phase for machine learning scientists and clinicians is the proper design and collection of the data cohort. The ability to recognize various forms of biases and distribution shifts in the dataset is critical at this step. While it remains difficult to account for all potential sources of bias, techniques can be developed to identify specific types of bias in order to mitigate their impact. In this work we analyze how the distribution of scanner manufacturers in a dataset can contribute to the overall bias of deep learning models. We evaluate convolutional neural networks (CNN) for both classification and segmentation tasks, specifically two state-of-the-art models: ResNet [2] for classification and U-Net [3] for segmentation. We demonstrate that CNNs can learn to distinguish the imaging scanner manufacturer and that this bias can substantially impact model performance for both classification and segmentation tasks. By creating an original synthesis dataset of brain data mimicking the presence of more or less subtle lesions we also show that this bias is related to the difficulty of the task. Recognition of such bias is critical to develop robust, generalizable models that will be crucial for clinical applications in real-world data distributions.



### Blur-Attention: A boosting mechanism for non-uniform blurred image restoration
- **Arxiv ID**: http://arxiv.org/abs/2008.08526v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08526v1)
- **Published**: 2020-08-19 16:07:06+00:00
- **Updated**: 2020-08-19 16:07:06+00:00
- **Authors**: Xiaoguang Li, Feifan Yang, Kin Man Lam, Li Zhuo, Jiafeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic scene deblurring is a challenging problem in computer vision. It is difficult to accurately estimate the spatially varying blur kernel by traditional methods. Data-driven-based methods usually employ kernel-free end-to-end mapping schemes, which are apt to overlook the kernel estimation. To address this issue, we propose a blur-attention module to dynamically capture the spatially varying features of non-uniform blurred images. The module consists of a DenseBlock unit and a spatial attention unit with multi-pooling feature fusion, which can effectively extract complex spatially varying blur features. We design a multi-level residual connection structure to connect multiple blur-attention modules to form a blur-attention network. By introducing the blur-attention network into a conditional generation adversarial framework, we propose an end-to-end blind motion deblurring method, namely Blur-Attention-GAN (BAG), for a single image. Our method can adaptively select the weights of the extracted features according to the spatially varying blur features, and dynamically restore the images. Experimental results show that the deblurring capability of our method achieved outstanding objective performance in terms of PSNR, SSIM, and subjective visual quality. Furthermore, by visualizing the features extracted by the blur-attention module, comprehensive discussions are provided on its effectiveness.



### Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2008.08528v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.08528v1)
- **Published**: 2020-08-19 16:10:36+00:00
- **Updated**: 2020-08-19 16:10:36+00:00
- **Authors**: Boqiang Xu, Lingxiao He, Xingyu Liao, Wu Liu, Zhenan Sun, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) aims at retrieving an input person image from a set of images captured by multiple cameras. Although recent Re-ID methods have made great success, most of them extract features in terms of the attributes of clothing (e.g., color, texture). However, it is common for people to wear black clothes or be captured by surveillance systems in low light illumination, in which cases the attributes of the clothing are severely missing. We call this problem the Black Re-ID problem. To solve this problem, rather than relying on the clothing information, we propose to exploit head-shoulder features to assist person Re-ID. The head-shoulder adaptive attention network (HAA) is proposed to learn the head-shoulder feature and an innovative ensemble method is designed to enhance the generalization of our model. Given the input person image, the ensemble method would focus on the head-shoulder feature by assigning a larger weight if the individual insides the image is in black clothing. Due to the lack of a suitable benchmark dataset for studying the Black Re-ID problem, we also contribute the first Black-reID dataset, which contains 1274 identities in training set. Extensive evaluations on the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model achieves the best result compared with the state-of-the-art Re-ID methods on both Black and conventional Re-ID problems. Furthermore, our method is also proved to be effective in dealing with person Re-ID in similar clothing. Our code and dataset are avaliable on https://github.com/xbq1994/.



### STAR: Sparse Trained Articulated Human Body Regressor
- **Arxiv ID**: http://arxiv.org/abs/2008.08535v1
- **DOI**: 10.1007/978-3-030-58539-6_36
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08535v1)
- **Published**: 2020-08-19 16:27:55+00:00
- **Updated**: 2020-08-19 16:27:55+00:00
- **Authors**: Ahmed A. A. Osman, Timo Bolkart, Michael J. Black
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: The SMPL body model is widely used for the estimation, synthesis, and analysis of 3D human pose and shape. While popular, we show that SMPL has several limitations and introduce STAR, which is quantitatively and qualitatively superior to SMPL. First, SMPL has a huge number of parameters resulting from its use of global blend shapes. These dense pose-corrective offsets relate every vertex on the mesh to all the joints in the kinematic tree, capturing spurious long-range correlations. To address this, we define per-joint pose correctives and learn the subset of mesh vertices that are influenced by each joint movement. This sparse formulation results in more realistic deformations and significantly reduces the number of model parameters to 20% of SMPL. When trained on the same data as SMPL, STAR generalizes better despite having many fewer parameters. Second, SMPL factors pose-dependent deformations from body shape while, in reality, people with different shapes deform differently. Consequently, we learn shape-dependent pose-corrective blend shapes that depend on both body pose and BMI. Third, we show that the shape space of SMPL is not rich enough to capture the variation in the human population. We address this by training STAR with an additional 10,000 scans of male and female subjects, and show that this results in better model generalization. STAR is compact, generalizes better to new bodies and is a drop-in replacement for SMPL. STAR is publicly available for research purposes at http://star.is.tue.mpg.de.



### No-reference Screen Content Image Quality Assessment with Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2008.08561v4
- **DOI**: 10.1109/TIP.2021.3084750
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08561v4)
- **Published**: 2020-08-19 17:31:23+00:00
- **Updated**: 2021-05-26 03:00:56+00:00
- **Authors**: Baoliang Chen, Haoliang Li, Hongfei Fan, Shiqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we quest the capability of transferring the quality of natural scene images to the images that are not acquired by optical cameras (e.g., screen content images, SCIs), rooted in the widely accepted view that the human visual system has adapted and evolved through the perception of natural environment. Here, we develop the first unsupervised domain adaptation based no reference quality assessment method for SCIs, leveraging rich subjective ratings of the natural images (NIs). In general, it is a non-trivial task to directly transfer the quality prediction model from NIs to a new type of content (i.e., SCIs) that holds dramatically different statistical characteristics. Inspired by the transferability of pair-wise relationship, the proposed quality measure operates based on the philosophy of improving the transferability and discriminability simultaneously. In particular, we introduce three types of losses which complementarily and explicitly regularize the feature space of ranking in a progressive manner. Regarding feature discriminatory capability enhancement, we propose a center based loss to rectify the classifier and improve its prediction capability not only for source domain (NI) but also the target domain (SCI). For feature discrepancy minimization, the maximum mean discrepancy (MMD) is imposed on the extracted ranking features of NIs and SCIs. Furthermore, to further enhance the feature diversity, we introduce the correlation penalization between different feature dimensions, leading to the features with lower rank and higher diversity. Experiments show that our method can achieve higher performance on different source-target settings based on a light-weight convolution neural network. The proposed method also sheds light on learning quality assessment measures for unseen application-specific content without the cumbersome and costing subjective evaluations.



### Physically-Constrained Transfer Learning through Shared Abundance Space for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2008.08563v2
- **DOI**: 10.1109/TGRS.2020.3045790
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08563v2)
- **Published**: 2020-08-19 17:41:37+00:00
- **Updated**: 2020-08-30 11:05:13+00:00
- **Authors**: Ying Qu, Razieh Kaviani Baghbaderani, Wei Li, Lianru Gao, Hairong Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification is one of the most active research topics and has achieved promising results boosted by the recent development of deep learning. However, most state-of-the-art approaches tend to perform poorly when the training and testing images are on different domains, e.g., source domain and target domain, respectively, due to the spectral variability caused by different acquisition conditions. Transfer learning-based methods address this problem by pre-training in the source domain and fine-tuning on the target domain. Nonetheless, a considerable amount of data on the target domain has to be labeled and non-negligible computational resources are required to retrain the whole network. In this paper, we propose a new transfer learning scheme to bridge the gap between the source and target domains by projecting the HSI data from the source and target domains into a shared abundance space based on their own physical characteristics. In this way, the domain discrepancy would be largely reduced such that the model trained on the source domain could be applied on the target domain without extra efforts for data labeling or network retraining. The proposed method is referred to as physically-constrained transfer learning through shared abundance space (PCTL-SAS). Extensive experimental results demonstrate the superiority of the proposed method as compared to the state-of-the-art. The success of this endeavor would largely facilitate the deployment of HSI classification for real-world sensing scenarios.



### Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2008.08574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08574v1)
- **Published**: 2020-08-19 17:57:03+00:00
- **Updated**: 2020-08-19 17:57:03+00:00
- **Authors**: Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, Ming-Hsuan Yang
- **Comment**: Accepted in ECCV'20. Project page:
  https://chengchunhsu.github.io/EveryPixelMatters/
- **Journal**: None
- **Summary**: A domain adaptive object detector aims to adapt itself to unseen domains that may contain variations of object appearance, viewpoints or backgrounds. Most existing methods adopt feature alignment either on the image level or instance level. However, image-level alignment on global features may tangle foreground/background pixels at the same time, while instance-level alignment using proposals may suffer from the background noise. Different from existing solutions, we propose a domain adaptation framework that accounts for each pixel via predicting pixel-wise objectness and centerness. Specifically, the proposed method carries out center-aware alignment by paying more attention to foreground pixels, hence achieving better adaptation across domains. We demonstrate our method on numerous adaptation settings with extensive experimental results and show favorable performance against existing state-of-the-art algorithms.



### Slide-free MUSE Microscopy to H&E Histology Modality Conversion via Unpaired Image-to-Image Translation GAN Models
- **Arxiv ID**: http://arxiv.org/abs/2008.08579v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08579v1)
- **Published**: 2020-08-19 17:59:08+00:00
- **Updated**: 2020-08-19 17:59:08+00:00
- **Authors**: Tanishq Abraham, Andrew Shaw, Daniel O'Connor, Austin Todd, Richard Levenson
- **Comment**: 4 pages plus 1 page references. Presented at the ICML Computational
  Biology Workshop 2020
- **Journal**: None
- **Summary**: MUSE is a novel slide-free imaging technique for histological examination of tissues that can serve as an alternative to traditional histology. In order to bridge the gap between MUSE and traditional histology, we aim to convert MUSE images to resemble authentic hematoxylin- and eosin-stained (H&E) images. We evaluated four models: a non-machine-learning-based color-mapping unmixing-based tool, CycleGAN, DualGAN, and GANILLA. CycleGAN and GANILLA provided visually compelling results that appropriately transferred H&E style and preserved MUSE content. Based on training an automated critic on real and generated H&E images, we determined that CycleGAN demonstrated the best performance. We have also found that MUSE color inversion may be a necessary step for accurate modality conversion to H&E. We believe that our MUSE-to-H&E model can help improve adoption of novel slide-free methods by bridging a perceptual gap between MUSE imaging and traditional histology.



### On Qualitative Shape Inferences: a journey from geometry to topology
- **Arxiv ID**: http://arxiv.org/abs/2008.08622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08622v1)
- **Published**: 2020-08-19 18:32:54+00:00
- **Updated**: 2020-08-19 18:32:54+00:00
- **Authors**: Steven W Zucker
- **Comment**: None
- **Journal**: None
- **Summary**: Shape inference is classically ill-posed, because it involves a map from the (2D) image domain to the (3D) world. Standard approaches regularize this problem by either assuming a prior on lighting and rendering or restricting the domain, and develop differential equations or optimization solutions. While elegant, the solutions that emerge in these situations are remarkably fragile. We exploit the observation that people infer shape qualitatively; that there are quantitative differences between individuals. The consequence is a topological approach based on critical contours and the Morse-Smale complex. This paper provides a developmental review of that theory, emphasizing the motivation at different stages of the research.



### Spatio-Temporal EEG Representation Learning on Riemannian Manifold and Euclidean Space
- **Arxiv ID**: http://arxiv.org/abs/2008.08633v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08633v3)
- **Published**: 2020-08-19 18:56:49+00:00
- **Updated**: 2023-08-06 20:17:31+00:00
- **Authors**: Guangyi Zhang, Ali Etemad
- **Comment**: Accepted in IEEE Transactions on Emerging Topics in Computational
  Intelligence. 15 pages, 6 figures
- **Journal**: None
- **Summary**: We present a novel deep neural architecture for learning electroencephalogram (EEG). To learn the spatial information, our model first obtains the Riemannian mean and distance from spatial covariance matrices (SCMs) on a Riemannian manifold. We then project the spatial information onto a Euclidean space via tangent space learning. Following, two fully connected layers are used to learn the spatial information embeddings. Moreover, our proposed method learns the temporal information via differential entropy and logarithm power spectrum density features extracted from EEG signals in a Euclidean space using a deep long short-term memory network with a soft attention mechanism. To combine the spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embedding-specific features for decision making. We evaluate our proposed framework on four public datasets across three popular EEG-related tasks, notably emotion recognition, vigilance estimation, and motor imagery classification, containing various types of tasks such as binary classification, multi-class classification, and regression. Our proposed architecture outperforms other methods on SEED-VIG, and approaches the state-of-the-art on the other three datasets (SEED, BCI-IV 2A, and BCI-IV 2B), showing the robustness of our framework in EEG representation learning. The source code of our paper is publicly available at https://github.com/guangyizhangbci/EEG_Riemannian.



### Image Segmentation of Zona-Ablated Human Blastocysts
- **Arxiv ID**: http://arxiv.org/abs/2008.08673v1
- **DOI**: 10.1109/NANOMED49242.2019.9130621
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08673v1)
- **Published**: 2020-08-19 21:20:02+00:00
- **Updated**: 2020-08-19 21:20:02+00:00
- **Authors**: Md Yousuf Harun, M Arifur Rahman, Joshua Mellinger, Willy Chang, Thomas Huang, Brienne Walker, Kristen Hori, Aaron T. Ohta
- **Comment**: None
- **Journal**: IEEE 13th International Conference on Nano/Molecular Medicine &
  Engineering (NANOMED), Gwangju, Korea (South), 2019, pp. 208-213
- **Summary**: Automating human preimplantation embryo grading offers the potential for higher success rates with in vitro fertilization (IVF) by providing new quantitative and objective measures of embryo quality. Current IVF procedures typically use only qualitative manual grading, which is limited in the identification of genetically abnormal embryos. The automatic quantitative assessment of blastocyst expansion can potentially improve sustained pregnancy rates and reduce health risks from abnormal pregnancies through a more accurate identification of genetic abnormality. The expansion rate of a blastocyst is an important morphological feature to determine the quality of a developing embryo. In this work, a deep learning based human blastocyst image segmentation method is presented, with the goal of facilitating the challenging task of segmenting irregularly shaped blastocysts. The type of blastocysts evaluated here has undergone laser ablation of the zona pellucida, which is required prior to trophectoderm biopsy. This complicates the manual measurements of the expanded blastocyst's size, which shows a correlation with genetic abnormalities. The experimental results on the test set demonstrate segmentation greatly improves the accuracy of expansion measurements, resulting in up to 99.4% accuracy, 98.1% precision, 98.8% recall, a 98.4% Dice Coefficient, and a 96.9% Jaccard Index.



### Inner Cell Mass and Trophectoderm Segmentation in Human Blastocyst Images using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2008.08676v1
- **DOI**: 10.1109/NANOMED49242.2019.9130618
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08676v1)
- **Published**: 2020-08-19 21:23:16+00:00
- **Updated**: 2020-08-19 21:23:16+00:00
- **Authors**: Md Yousuf Harun, Thomas Huang, Aaron T. Ohta
- **Comment**: None
- **Journal**: IEEE 13th International Conference on Nano/Molecular Medicine &
  Engineering (NANOMED), Gwangju, Korea (South), 2019, pp. 214-219
- **Summary**: Embryo quality assessment based on morphological attributes is important for achieving higher pregnancy rates from in vitro fertilization (IVF). The accurate segmentation of the embryo's inner cell mass (ICM) and trophectoderm epithelium (TE) is important, as these parameters can help to predict the embryo viability and live birth potential. However, segmentation of the ICM and TE is difficult due to variations in their shape and similarities in their textures, both with each other and with their surroundings. To tackle this problem, a deep neural network (DNN) based segmentation approach was implemented. The DNN can identify the ICM region with 99.1% accuracy, 94.9% precision, 93.8% recall, a 94.3% Dice Coefficient, and a 89.3% Jaccard Index. It can extract the TE region with 98.3% accuracy, 91.8% precision, 93.2% recall, a 92.5% Dice Coefficient, and a 85.3% Jaccard Index.



### Self-Supervised Ultrasound to MRI Fetal Brain Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2008.08698v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.08698v1)
- **Published**: 2020-08-19 22:56:36+00:00
- **Updated**: 2020-08-19 22:56:36+00:00
- **Authors**: Jianbo Jiao, Ana I. L. Namburete, Aris T. Papageorghiou, J. Alison Noble
- **Comment**: IEEE Transactions on Medical Imaging 2020
- **Journal**: None
- **Summary**: Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the developing brain but is not suitable for second-trimester anomaly screening, for which ultrasound (US) is employed. Although expert sonographers are adept at reading US images, MR images which closely resemble anatomical images are much easier for non-experts to interpret. Thus in this paper we propose to generate MR-like images directly from clinical US images. In medical image analysis such a capability is potentially useful as well, for instance for automatic US-MRI registration and fusion. The proposed model is end-to-end trainable and self-supervised without any external annotations. Specifically, based on an assumption that the US and MRI data share a similar anatomical latent space, we first utilise a network to extract the shared latent features, which are then used for MRI synthesis. Since paired data is unavailable for our study (and rare in practice), pixel-level constraints are infeasible to apply. We instead propose to enforce the distributions to be statistically indistinguishable, by adversarial learning in both the image domain and feature space. To regularise the anatomical structures between US and MRI during synthesis, we further propose an adversarial structural constraint. A new cross-modal attention technique is proposed to utilise non-local spatial information, by encouraging multi-modal knowledge fusion and propagation. We extend the approach to consider the case where 3D auxiliary information (e.g., 3D neighbours and a 3D location index) from volumetric data is also available, and show that this improves image synthesis. The proposed approach is evaluated quantitatively and qualitatively with comparison to real fetal MR images and other approaches to synthesis, demonstrating its feasibility of synthesising realistic MR images.



### Hidden Footprints: Learning Contextual Walkability from 3D Human Trails
- **Arxiv ID**: http://arxiv.org/abs/2008.08701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.08701v1)
- **Published**: 2020-08-19 23:19:08+00:00
- **Updated**: 2020-08-19 23:19:08+00:00
- **Authors**: Jin Sun, Hadar Averbuch-Elor, Qianqian Wang, Noah Snavely
- **Comment**: European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Predicting where people can walk in a scene is important for many tasks, including autonomous driving systems and human behavior analysis. Yet learning a computational model for this purpose is challenging due to semantic ambiguity and a lack of labeled data: current datasets only tell you where people are, not where they could be. We tackle this problem by leveraging information from existing datasets, without additional labeling. We first augment the set of valid, labeled walkable regions by propagating person observations between images, utilizing 3D information to create what we call hidden footprints. However, this augmented data is still sparse. We devise a training strategy designed for such sparse labels, combining a class-balanced classification loss with a contextual adversarial loss. Using this strategy, we demonstrate a model that learns to predict a walkability map from a single image. We evaluate our model on the Waymo and Cityscapes datasets, demonstrating superior performance compared to baselines and state-of-the-art models.




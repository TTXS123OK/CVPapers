# Arxiv Papers in cs.CV on 2020-04-05
### gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity Priors
- **Arxiv ID**: http://arxiv.org/abs/2004.02052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02052v1)
- **Published**: 2020-04-05 00:01:19+00:00
- **Updated**: 2020-04-05 00:01:19+00:00
- **Authors**: Victor Fragoso, Joseph DeGol, Gang Hua
- **Comment**: None
- **Journal**: IEEE/CVF CVPR 2020
- **Summary**: Many real-world applications in augmented reality (AR), 3D mapping, and robotics require both fast and accurate estimation of camera poses and scales from multiple images captured by multiple cameras or a single moving camera. Achieving high speed and maintaining high accuracy in a pose-and-scale estimator are often conflicting goals. To simultaneously achieve both, we exploit a priori knowledge about the solution space. We present gDLS*, a generalized-camera-model pose-and-scale estimator that utilizes rotation and scale priors. gDLS* allows an application to flexibly weigh the contribution of each prior, which is important since priors often come from noisy sensors. Compared to state-of-the-art generalized-pose-and-scale estimators (e.g., gDLS), our experiments on both synthetic and real data consistently demonstrate that gDLS* accelerates the estimation process and improves scale and pose accuracy.



### Finding Covid-19 from Chest X-rays using Deep Learning on a Small Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.02060v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02060v4)
- **Published**: 2020-04-05 00:58:54+00:00
- **Updated**: 2020-05-20 19:12:46+00:00
- **Authors**: Lawrence O. Hall, Rahul Paul, Dmitry B. Goldgof, Gregory M. Goldgof
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Testing for COVID-19 has been unable to keep up with the demand. Further, the false negative rate is projected to be as high as 30% and test results can take some time to obtain. X-ray machines are widely available and provide images for diagnosis quickly. This paper explores how useful chest X-ray images can be in diagnosing COVID-19 disease. We have obtained 122 chest X-rays of COVID-19 and over 4,000 chest X-rays of viral and bacterial pneumonia. A pretrained deep convolutional neural network has been tuned on 102 COVID-19 cases and 102 other pneumonia cases in a 10-fold cross validation. The results were all 102 COVID-19 cases were correctly classified and there were 8 false positives resulting in an AUC of 0.997. On a test set of 20 unseen COVID-19 cases all were correctly classified and more than 95% of 4171 other pneumonia examples were correctly classified. This study has flaws, most critically a lack of information about where in the disease process the COVID-19 cases were and the small data set size. More COVID-19 case images will enable a better answer to the question of how useful chest X-rays can be for diagnosing COVID-19 (so please send them).



### Discriminator Contrastive Divergence: Semi-Amortized Generative Modeling by Exploring Energy of the Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2004.01704v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.01704v1)
- **Published**: 2020-04-05 01:50:16+00:00
- **Updated**: 2020-04-05 01:50:16+00:00
- **Authors**: Yuxuan Song, Qiwei Ye, Minkai Xu, Tie-Yan Liu
- **Comment**: 17 pages, 9 figures, pre-submmited to cvpr2019
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown great promise in modeling high dimensional data. The learning objective of GANs usually minimizes some measure discrepancy, \textit{e.g.}, $f$-divergence~($f$-GANs) or Integral Probability Metric~(Wasserstein GANs). With $f$-divergence as the objective function, the discriminator essentially estimates the density ratio, and the estimated ratio proves useful in further improving the sample quality of the generator. However, how to leverage the information contained in the discriminator of Wasserstein GANs (WGAN) is less explored. In this paper, we introduce the Discriminator Contrastive Divergence, which is well motivated by the property of WGAN's discriminator and the relationship between WGAN and energy-based model. Compared to standard GANs, where the generator is directly utilized to obtain new samples, our method proposes a semi-amortized generation procedure where the samples are produced with the generator's output as an initial state. Then several steps of Langevin dynamics are conducted using the gradient of the discriminator. We demonstrate the benefits of significant improved generation on both synthetic data and several real-world image generation benchmarks.



### ReADS: A Rectified Attentional Double Supervised Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.02070v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02070v2)
- **Published**: 2020-04-05 02:05:35+00:00
- **Updated**: 2020-04-07 01:44:17+00:00
- **Authors**: Qi Song, Qianyi Jiang, Nan Li, Rui Zhang, Xiaolin Wei
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: In recent years, scene text recognition is always regarded as a sequence-to-sequence problem. Connectionist Temporal Classification (CTC) and Attentional sequence recognition (Attn) are two very prevailing approaches to tackle this problem while they may fail in some scenarios respectively. CTC concentrates more on every individual character but is weak in text semantic dependency modeling. Attn based methods have better context semantic modeling ability while tends to overfit on limited training data. In this paper, we elaborately design a Rectified Attentional Double Supervised Network (ReADS) for general scene text recognition. To overcome the weakness of CTC and Attn, both of them are applied in our method but with different modules in two supervised branches which can make a complementary to each other. Moreover, effective spatial and channel attention mechanisms are introduced to eliminate background noise and extract valid foreground information. Finally, a simple rectified network is implemented to rectify irregular text. The ReADS can be trained end-to-end and only word-level annotations are required. Extensive experiments on various benchmarks verify the effectiveness of ReADS which achieves state-of-the-art performance.



### Any-Shot Sequential Anomaly Detection in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.02072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02072v1)
- **Published**: 2020-04-05 02:15:45+00:00
- **Updated**: 2020-04-05 02:15:45+00:00
- **Authors**: Keval Doshi, Yasin Yilmaz
- **Comment**: Accepted to CVPR 2020: Workshop on Continual Learning in Computer
  Vision
- **Journal**: None
- **Summary**: Anomaly detection in surveillance videos has been recently gaining attention. Even though the performance of state-of-the-art methods on publicly available data sets has been competitive, they demand a massive amount of training data. Also, they lack a concrete approach for continuously updating the trained model once new data is available. Furthermore, online decision making is an important but mostly neglected factor in this domain. Motivated by these research gaps, we propose an online anomaly detection method for surveillance videos using transfer learning and any-shot learning, which in turn significantly reduces the training complexity and provides a mechanism that can detect anomalies using only a few labeled nominal examples. Our proposed algorithm leverages the feature extraction power of neural network-based models for transfer learning and the any-shot learning capability of statistical detection methods.



### Arbitrary Scale Super-Resolution for Brain MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2004.02086v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02086v2)
- **Published**: 2020-04-05 03:53:28+00:00
- **Updated**: 2020-06-04 02:34:44+00:00
- **Authors**: Chuan Tan, Jin Zhu, Pietro Lio'
- **Comment**: 12 pages, 8 figures, 1 table, to appear as a full paper with oral
  contribution in AIAI 2020
- **Journal**: None
- **Summary**: Recent attempts at Super-Resolution for medical images used deep learning techniques such as Generative Adversarial Networks (GANs) to achieve perceptually realistic single image Super-Resolution. Yet, they are constrained by their inability to generalise to different scale factors. This involves high storage and energy costs as every integer scale factor involves a separate neural network. A recent paper has proposed a novel meta-learning technique that uses a Weight Prediction Network to enable Super-Resolution on arbitrary scale factors using only a single neural network. In this paper, we propose a new network that combines that technique with SRGAN, a state-of-the-art GAN-based architecture, to achieve arbitrary scale, high fidelity Super-Resolution for medical images. By using this network to perform arbitrary scale magnifications on images from the Multimodal Brain Tumor Segmentation Challenge (BraTS) dataset, we demonstrate that it is able to outperform traditional interpolation methods by up to 20$\%$ on SSIM scores whilst retaining generalisability on brain MRI images. We show that performance across scales is not compromised, and that it is able to achieve competitive results with other state-of-the-art methods such as EDSR whilst being fifty times smaller than them. Combining efficiency, performance, and generalisability, this can hopefully become a new foundation for tackling Super-Resolution on medical images.   Check out the webapp here: https://metasrgan.herokuapp.com/ Check out the github tutorial here: https://github.com/pancakewaffles/metasrgan-tutorial



### Feature Quantization Improves GAN Training
- **Arxiv ID**: http://arxiv.org/abs/2004.02088v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02088v2)
- **Published**: 2020-04-05 04:06:50+00:00
- **Updated**: 2020-07-15 00:06:52+00:00
- **Authors**: Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, Changyou Chen
- **Comment**: The first two authors contributed equally to this manuscript. ICML
  2020. Code: https://github.com/YangNaruto/FQ-GAN
- **Journal**: None
- **Summary**: The instability in GAN training has been a long-standing problem despite remarkable research efforts. We identify that instability issues stem from difficulties of performing feature matching with mini-batch statistics, due to a fragile balance between the fixed target distribution and the progressively generated distribution. In this work, we propose Feature Quantization (FQ) for the discriminator, to embed both true and fake data samples into a shared discrete space. The quantized values of FQ are constructed as an evolving dictionary, which is consistent with feature statistics of the recent distribution history. Hence, FQ implicitly enables robust feature matching in a compact space. Our method can be easily plugged into existing GAN models, with little computational overhead in training. We apply FQ to 3 representative GAN models on 9 benchmarks: BigGAN for image generation, StyleGAN for face synthesis, and U-GAT-IT for unsupervised image-to-image translation. Extensive experimental results show that the proposed FQ-GAN can improve the FID scores of baseline methods by a large margin on a variety of tasks, achieving new state-of-the-art performance.



### Deeply Aligned Adaptation for Cross-domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.02093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02093v2)
- **Published**: 2020-04-05 04:41:45+00:00
- **Updated**: 2020-04-09 01:39:11+00:00
- **Authors**: Minghao Fu, Zhenshan Xie, Wen Li, Lixin Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-domain object detection has recently attracted more and more attention for real-world applications, since it helps build robust detectors adapting well to new environments. In this work, we propose an end-to-end solution based on Faster R-CNN, where ground-truth annotations are available for source images (e.g., cartoon) but not for target ones (e.g., watercolor) during training. Motivated by the observation that the transferabilities of different neural network layers differ from each other, we propose to apply a number of domain alignment strategies to different layers of Faster R-CNN, where the alignment strength is gradually reduced from low to higher layers. Moreover, after obtaining region proposals in our network, we develop a foreground-background aware alignment module to further reduce the domain mismatch by separately aligning features of the foreground and background regions from the source and target domains. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed approach.



### DeepFLASH: An Efficient Network for Learning-based Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2004.02097v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02097v1)
- **Published**: 2020-04-05 05:17:07+00:00
- **Updated**: 2020-04-05 05:17:07+00:00
- **Authors**: Jian Wang, Miaomiao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents DeepFLASH, a novel network with efficient training and inference for learning-based medical image registration. In contrast to existing approaches that learn spatial transformations from training data in the high dimensional imaging space, we develop a new registration network entirely in a low dimensional bandlimited space. This dramatically reduces the computational cost and memory footprint of an expensive training and inference. To achieve this goal, we first introduce complex-valued operations and representations of neural architectures that provide key components for learning-based registration models. We then construct an explicit loss function of transformation fields fully characterized in a bandlimited space with much fewer parameterizations. Experimental results show that our method is significantly faster than the state-of-the-art deep learning based image registration methods, while producing equally accurate alignment. We demonstrate our algorithm in two different applications of image registration: 2D synthetic data and 3D real brain magnetic resonance (MR) images. Our code is available at https://github.com/jw4hv/deepflash.



### Learning and Recognizing Archeological Features from LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2004.02099v1
- **DOI**: 10.1109/BigData47090.2019.9005548
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02099v1)
- **Published**: 2020-04-05 05:36:37+00:00
- **Updated**: 2020-04-05 05:36:37+00:00
- **Authors**: Conrad M Albrecht, Chris Fisher, Marcus Freitag, Hendrik F Hamann, Sharathchandra Pankanti, Florencia Pezzutti, Francesca Rossi
- **Comment**: None
- **Journal**: 2019 IEEE International Conference on Big Data (Big Data)
- **Summary**: We present a remote sensing pipeline that processes LiDAR (Light Detection And Ranging) data through machine & deep learning for the application of archeological feature detection on big geo-spatial data platforms such as e.g. IBM PAIRS Geoscope.   Today, archeologists get overwhelmed by the task of visually surveying huge amounts of (raw) LiDAR data in order to identify areas of interest for inspection on the ground. We showcase a software system pipeline that results in significant savings in terms of expert productivity while missing only a small fraction of the artifacts.   Our work employs artificial neural networks in conjunction with an efficient spatial segmentation procedure based on domain knowledge. Data processing is constraint by a limited amount of training labels and noisy LiDAR signals due to vegetation cover and decay of ancient structures. We aim at identifying geo-spatial areas with archeological artifacts in a supervised fashion allowing the domain expert to flexibly tune parameters based on her needs.



### Empirical Upper Bound, Error Diagnosis and Invariance Analysis of Modern Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2004.02877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02877v1)
- **Published**: 2020-04-05 06:19:43+00:00
- **Updated**: 2020-04-05 06:19:43+00:00
- **Authors**: Ali Borji
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1911.12451
- **Journal**: None
- **Summary**: Object detection remains as one of the most notorious open problems in computer vision. Despite large strides in accuracy in recent years, modern object detectors have started to saturate on popular benchmarks raising the question of how far we can reach with deep learning tools and tricks. Here, by employing 2 state-of-the-art object detection benchmarks, and analyzing more than 15 models over 4 large scale datasets, we I) carefully determine the upper bound in AP, which is 91.6% on VOC (test2007), 78.2% on COCO (val2017), and 58.9% on OpenImages V4 (validation), regardless of the IOU threshold. These numbers are much better than the mAP of the best model (47.9% on VOC, and 46.9% on COCO; IOUs=.5:.05:.95), II) characterize the sources of errors in object detectors, in a novel and intuitive way, and find that classification error (confusion with other classes and misses) explains the largest fraction of errors and weighs more than localization and duplicate errors, and III) analyze the invariance properties of models when surrounding context of an object is removed, when an object is placed in an incongruent background, and when images are blurred or flipped vertically. We find that models generate a lot of boxes on empty regions and that context is more important for detecting small objects than larger ones. Our work taps into the tight relationship between object detection and object recognition and offers insights for building better models. Our code is publicly available at https://github.com/aliborji/Deetctionupper bound.git.



### Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.02108v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02108v7)
- **Published**: 2020-04-05 06:51:22+00:00
- **Updated**: 2020-08-27 13:54:22+00:00
- **Authors**: Shi Yin, Shangfei Wang, Xiaoping Chen, Enhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Although heatmap regression is considered a state-of-the-art method to locate facial landmarks, it suffers from huge spatial complexity and is prone to quantization error. To address this, we propose a novel attentive one-dimensional heatmap regression method for facial landmark localization. First, we predict two groups of 1D heatmaps to represent the marginal distributions of the x and y coordinates. These 1D heatmaps reduce spatial complexity significantly compared to current heatmap regression methods, which use 2D heatmaps to represent the joint distributions of x and y coordinates. With much lower spatial complexity, the proposed method can output high-resolution 1D heatmaps despite limited GPU memory, significantly alleviating the quantization error. Second, a co-attention mechanism is adopted to model the inherent spatial patterns existing in x and y coordinates, and therefore the joint distributions on the x and y axes are also captured. Third, based on the 1D heatmap structures, we propose a facial landmark detector capturing spatial patterns for landmark detection on an image; and a tracker further capturing temporal patterns with a temporal refinement mechanism for landmark tracking. Experimental results on four benchmark databases demonstrate the superiority of our method.



### Emotional Video to Audio Transformation Using Deep Recurrent Neural Networks and a Neuro-Fuzzy System
- **Arxiv ID**: http://arxiv.org/abs/2004.02113v1
- **DOI**: 10.1155/2020/8478527
- **Categories**: **cs.SD**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02113v1)
- **Published**: 2020-04-05 07:18:28+00:00
- **Updated**: 2020-04-05 07:18:28+00:00
- **Authors**: Gwenaelle Cunha Sergio, Minho Lee
- **Comment**: Published (https://www.hindawi.com/journals/mpe/2020/8478527/)
- **Journal**: Mathematical Problems in Engineering 2020 (2020) 1-15
- **Summary**: Generating music with emotion similar to that of an input video is a very relevant issue nowadays. Video content creators and automatic movie directors benefit from maintaining their viewers engaged, which can be facilitated by producing novel material eliciting stronger emotions in them. Moreover, there's currently a demand for more empathetic computers to aid humans in applications such as augmenting the perception ability of visually and/or hearing impaired people. Current approaches overlook the video's emotional characteristics in the music generation step, only consider static images instead of videos, are unable to generate novel music, and require a high level of human effort and skills. In this study, we propose a novel hybrid deep neural network that uses an Adaptive Neuro-Fuzzy Inference System to predict a video's emotion from its visual features and a deep Long Short-Term Memory Recurrent Neural Network to generate its corresponding audio signals with similar emotional inkling. The former is able to appropriately model emotions due to its fuzzy properties, and the latter is able to model data with dynamic time properties well due to the availability of the previous hidden state information. The novelty of our proposed method lies in the extraction of visual emotional features in order to transform them into audio signals with corresponding emotional aspects for users. Quantitative experiments show low mean absolute errors of 0.217 and 0.255 in the Lindsey and DEAP datasets respectively, and similar global features in the spectrograms. This indicates that our model is able to appropriately perform domain transformation between visual and audio features. Based on experimental results, our model can effectively generate audio that matches the scene eliciting a similar emotion from the viewer in both datasets, and music generated by our model is also chosen more often.



### Anisotropic Convolutional Networks for 3D Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2004.02122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02122v1)
- **Published**: 2020-04-05 07:57:02+00:00
- **Updated**: 2020-04-05 07:57:02+00:00
- **Authors**: Jie Li, Kai Han, Peng Wang, Yu Liu, Xia Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: As a voxel-wise labeling task, semantic scene completion (SSC) tries to simultaneously infer the occupancy and semantic labels for a scene from a single depth and/or RGB image. The key challenge for SSC is how to effectively take advantage of the 3D context to model various objects or stuffs with severe variations in shapes, layouts and visibility. To handle such variations, we propose a novel module called anisotropic convolution, which properties with flexibility and power impossible for the competing methods such as standard 3D convolution and some of its variations. In contrast to the standard 3D convolution that is limited to a fixed 3D receptive field, our module is capable of modeling the dimensional anisotropy voxel-wisely. The basic idea is to enable anisotropic 3D receptive field by decomposing a 3D convolution into three consecutive 1D convolutions, and the kernel size for each such 1D convolution is adaptively determined on the fly. By stacking multiple such anisotropic convolution modules, the voxel-wise modeling capability can be further enhanced while maintaining a controllable amount of model parameters. Extensive experiments on two SSC benchmarks, NYU-Depth-v2 and NYUCAD, show the superior performance of the proposed method. Our code is available at https://waterljwant.github.io/SSC/



### Deep Homography Estimation for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2004.02132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02132v1)
- **Published**: 2020-04-05 09:07:18+00:00
- **Updated**: 2020-04-05 09:07:18+00:00
- **Authors**: Hoang Le, Feng Liu, Shu Zhang, Aseem Agarwala
- **Comment**: CVPR 2020, https://github.com/lcmhoang/hmg-dynamics
- **Journal**: None
- **Summary**: Homography estimation is an important step in many computer vision problems. Recently, deep neural network methods have shown to be favorable for this problem when compared to traditional methods. However, these new methods do not consider dynamic content in input images. They train neural networks with only image pairs that can be perfectly aligned using homographies. This paper investigates and discusses how to design and train a deep neural network that handles dynamic scenes. We first collect a large video dataset with dynamic content. We then develop a multi-scale neural network and show that when properly trained using our new dataset, this neural network can already handle dynamic scenes to some extent. To estimate a homography of a dynamic scene in a more principled way, we need to identify the dynamic content. Since dynamic content detection and homography estimation are two tightly coupled tasks, we follow the multi-task learning principles and augment our multi-scale network such that it jointly estimates the dynamics masks and homographies. Our experiments show that our method can robustly estimate homography for challenging scenarios with dynamic scenes, blur artifacts, or lack of textures.



### Neuron Linear Transformation: Modeling the Domain Shift for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2004.02133v2
- **DOI**: 10.1109/TNNLS.2021.3051371
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02133v2)
- **Published**: 2020-04-05 09:15:47+00:00
- **Updated**: 2021-01-14 05:51:00+00:00
- **Authors**: Qi Wang, Tao Han, Junyu Gao, Yuan Yuan
- **Comment**: accepted by IEEE T-NNLS
- **Journal**: None
- **Summary**: Cross-domain crowd counting (CDCC) is a hot topic due to its importance in public safety. The purpose of CDCC is to alleviate the domain shift between the source and target domain. Recently, typical methods attempt to extract domain-invariant features via image translation and adversarial learning. When it comes to specific tasks, we find that the domain shifts are reflected on model parameters' differences. To describe the domain gap directly at the parameter-level, we propose a Neuron Linear Transformation (NLT) method, exploiting domain factor and bias weights to learn the domain shift. Specifically, for a specific neuron of a source model, NLT exploits few labeled target data to learn domain shift parameters. Finally, the target neuron is generated via a linear transformation. Extensive experiments and analysis on six real-world datasets validate that NLT achieves top performance compared with other domain adaptation methods. An ablation study also shows that the NLT is robust and more effective than supervised and fine-tune training. Code is available at: \url{https://github.com/taohan10200/NLT}.



### Adversarial-Prediction Guided Multi-task Adaptation for Semantic Segmentation of Electron Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2004.02134v1
- **DOI**: 10.1109/JSTSP.2020.3005317
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02134v1)
- **Published**: 2020-04-05 09:18:11+00:00
- **Updated**: 2020-04-05 09:18:11+00:00
- **Authors**: Jiajin Yi, Zhimin Yuan, Jialin Peng
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing 14.6 (2020):
  1199-1209
- **Summary**: Semantic segmentation is an essential step for electron microscopy (EM) image analysis. Although supervised models have achieved significant progress, the need for labor intensive pixel-wise annotation is a major limitation. To complicate matters further, supervised learning models may not generalize well on a novel dataset due to domain shift. In this study, we introduce an adversarial-prediction guided multi-task network to learn the adaptation of a well-trained model for use on a novel unlabeled target domain. Since no label is available on target domain, we learn an encoding representation not only for the supervised segmentation on source domain but also for unsupervised reconstruction of the target data. To improve the discriminative ability with geometrical cues, we further guide the representation learning by multi-level adversarial learning in semantic prediction space. Comparisons and ablation study on public benchmark demonstrated state-of-the-art performance and effectiveness of our approach.



### Fisher Discriminant Triplet and Contrastive Losses for Training Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04674v1
- **DOI**: 10.1109/IJCNN48605.2020.9206833
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04674v1)
- **Published**: 2020-04-05 09:27:05+00:00
- **Updated**: 2020-04-05 09:27:05+00:00
- **Authors**: Benyamin Ghojogh, Milad Sikaroudi, Sobhan Shafiei, H. R. Tizhoosh, Fakhri Karray, Mark Crowley
- **Comment**: Accepted (to appear) in International Joint Conference on Neural
  Networks (IJCNN) 2020, IEEE, in IEEE World Congress on Computational
  Intelligence (WCCI) 2020
- **Journal**: International Joint Conference on Neural Networks (IJCNN), IEEE,
  2020
- **Summary**: Siamese neural network is a very powerful architecture for both feature extraction and metric learning. It usually consists of several networks that share weights. The Siamese concept is topology-agnostic and can use any neural network as its backbone. The two most popular loss functions for training these networks are the triplet and contrastive loss functions. In this paper, we propose two novel loss functions, named Fisher Discriminant Triplet (FDT) and Fisher Discriminant Contrastive (FDC). The former uses anchor-neighbor-distant triplets while the latter utilizes pairs of anchor-neighbor and anchor-distant samples. The FDT and FDC loss functions are designed based on the statistical formulation of the Fisher Discriminant Analysis (FDA), which is a linear subspace learning method. Our experiments on the MNIST and two challenging and publicly available histopathology datasets show the effectiveness of the proposed loss functions.



### Adding A Filter Based on The Discriminator to Improve Unconditional Text Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.02135v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.02135v5)
- **Published**: 2020-04-05 09:34:52+00:00
- **Updated**: 2020-06-22 04:59:43+00:00
- **Authors**: Xingyuan Chen, Ping Cai, Peng Jin, Hongjun Wang, Xinyu Dai, Jiajun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The autoregressive language model (ALM) trained with maximum likelihood estimation (MLE) is widely used in unconditional text generation. Due to exposure bias, the generated texts still suffer from low quality and diversity. This presents statistically as a discrepancy between the real text and generated text. Some research shows a discriminator can detect this discrepancy. Because the discriminator can encode more information than the generator, discriminator has the potentiality to improve generator. To alleviate the exposure bias, generative adversarial networks (GAN) use the discriminator to update the generator's parameters directly, but they fail by being evaluated precisely. A critical reason for the failure is the difference between the discriminator input and the ALM input. We propose a novel mechanism by adding a filter which has the same input as the discriminator. First, discriminator detects the discrepancy signals and passes to filter directly (or by learning). Then, we use the filter to reject some generated samples with a sampling-based method. Thus, the original generative distribution is revised to reduce the discrepancy. Two ALMs, RNN-based and Transformer-based, are experimented. Evaluated precisely by three metrics, our mechanism consistently outperforms the ALMs and all kinds of GANs across two benchmark data sets.



### Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2004.02138v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02138v1)
- **Published**: 2020-04-05 09:52:04+00:00
- **Updated**: 2020-04-05 09:52:04+00:00
- **Authors**: Pengpeng Liu, Irwin King, Michael Lyu, Jia Xu
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: In this paper, we propose a unified method to jointly learn optical flow and stereo matching. Our first intuition is stereo matching can be modeled as a special case of optical flow, and we can leverage 3D geometry behind stereoscopic videos to guide the learning of these two forms of correspondences. We then enroll this knowledge into the state-of-the-art self-supervised learning framework, and train one single network to estimate both flow and stereo. Second, we unveil the bottlenecks in prior self-supervised learning approaches, and propose to create a new set of challenging proxy tasks to boost performance. These two insights yield a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. More remarkably, our self-supervised method even outperforms several state-of-the-art fully supervised methods, including PWC-Net and FlowNet2 on KITTI 2012.



### BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.02147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02147v1)
- **Published**: 2020-04-05 10:26:38+00:00
- **Updated**: 2020-04-05 10:26:38+00:00
- **Authors**: Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, Nong Sang
- **Comment**: 16 pages, 10 figures, 9 tables. Code is available at
  https://git.io/BiSeNet
- **Journal**: None
- **Summary**: The low-level details and high-level semantics are both essential to the semantic segmentation task. However, to speed up the model inference, current approaches almost always sacrifice the low-level details, which leads to a considerable accuracy decrease. We propose to treat these spatial details and categorical semantics separately to achieve high accuracy and high efficiency for realtime semantic segmentation. To this end, we propose an efficient and effective architecture with a good trade-off between speed and accuracy, termed Bilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a Detail Branch, with wide channels and shallow layers to capture low-level details and generate high-resolution feature representation; (ii) a Semantic Branch, with narrow channels and deep layers to obtain high-level semantic context. The Semantic Branch is lightweight due to reducing the channel capacity and a fast-downsampling strategy. Furthermore, we design a Guided Aggregation Layer to enhance mutual connections and fuse both types of feature representation. Besides, a booster training strategy is designed to improve the segmentation performance without any extra inference cost. Extensive quantitative and qualitative evaluations demonstrate that the proposed architecture performs favourably against a few state-of-the-art real-time semantic segmentation approaches. Specifically, for a 2,048x1,024 input, we achieve 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than existing methods, yet we achieve better segmentation accuracy.



### DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation
- **Arxiv ID**: http://arxiv.org/abs/2004.02164v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02164v5)
- **Published**: 2020-04-05 11:28:39+00:00
- **Updated**: 2020-07-20 15:26:14+00:00
- **Authors**: Xuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu Wang, Huazhong Yang
- **Comment**: The first two authors contribute equally
- **Journal**: None
- **Summary**: Budgeted pruning is the problem of pruning under resource constraints. In budgeted pruning, how to distribute the resources across layers (i.e., sparsity allocation) is the key problem. Traditional methods solve it by discretely searching for the layer-wise pruning ratios, which lacks efficiency. In this paper, we propose Differentiable Sparsity Allocation (DSA), an efficient end-to-end budgeted pruning flow. Utilizing a novel differentiable pruning process, DSA finds the layer-wise pruning ratios with gradient-based optimization. It allocates sparsity in continuous space, which is more efficient than methods based on discrete evaluation and search. Furthermore, DSA could work in a pruning-from-scratch manner, whereas traditional budgeted pruning methods are applied to pre-trained models. Experimental results on CIFAR-10 and ImageNet show that DSA could achieve superior performance than current iterative budgeted pruning methods, and shorten the time cost of the overall pruning process by at least 1.5x in the meantime.



### Comparative Analysis of Multiple Deep CNN Models for Waste Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.02168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02168v2)
- **Published**: 2020-04-05 11:50:27+00:00
- **Updated**: 2020-08-14 09:19:12+00:00
- **Authors**: Dipesh Gyawali, Alok Regmi, Aatish Shakya, Ashish Gautam, Surendra Shrestha
- **Comment**: 6 pages, 13 figures
- **Journal**: 5th International Conference on Advanced Engineering and
  ICT-Convergence 2020
- **Summary**: Waste is a wealth in a wrong place. Our research focuses on analyzing possibilities for automatic waste sorting and collecting in such a way that helps it for further recycling process. Various approaches are being practiced managing waste but not efficient and require human intervention. The automatic waste segregation would fit in to fill the gap. The project tested well known Deep Learning Network architectures for waste classification with dataset combined from own endeavors and Trash Net. The convolutional neural network is used for image classification. The hardware built in the form of dustbin is used to segregate those wastes into different compartments. Without the human exercise in segregating those waste products, the study would save the precious time and would introduce the automation in the area of waste management. Municipal solid waste is a huge, renewable source of energy. The situation is win-win for both government, society and industrialists. Because of fine-tuning of the ResNet18 Network, the best validation accuracy was found to be 87.8%.



### Approximate Manifold Defense Against Multiple Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2004.02183v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02183v2)
- **Published**: 2020-04-05 12:36:08+00:00
- **Updated**: 2020-10-15 08:08:16+00:00
- **Authors**: Jay Nandy, Wynne Hsu, Mong Li Lee
- **Comment**: Workshop on Machine Learning with Guarantees, NeurIPS 2019. IJCNN,
  2020 (full paper)
- **Journal**: None
- **Summary**: Existing defenses against adversarial attacks are typically tailored to a specific perturbation type. Using adversarial training to defend against multiple types of perturbation requires expensive adversarial examples from different perturbation types at each training step. In contrast, manifold-based defense incorporates a generative network to project an input sample onto the clean data manifold. This approach eliminates the need to generate expensive adversarial examples while achieving robustness against multiple perturbation types. However, the success of this approach relies on whether the generative network can capture the complete clean data manifold, which remains an open problem for complex input domain. In this work, we devise an approximate manifold defense mechanism, called RBF-CNN, for image classification. Instead of capturing the complete data manifold, we use an RBF layer to learn the density of small image patches. RBF-CNN also utilizes a reconstruction layer that mitigates any minor adversarial perturbations. Further, incorporating our proposed reconstruction process for training improves the adversarial robustness of our RBF-CNN models. Experiment results on MNIST and CIFAR-10 datasets indicate that RBF-CNN offers robustness for multiple perturbations without the need for expensive adversarial training.



### Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation
- **Arxiv ID**: http://arxiv.org/abs/2004.02186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02186v2)
- **Published**: 2020-04-05 12:52:29+00:00
- **Updated**: 2020-06-20 08:35:57+00:00
- **Authors**: Edoardo Remelli, Shangchen Han, Sina Honari, Pascal Fua, Robert Wang
- **Comment**: None
- **Journal**: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 6040-6049
- **Summary**: We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.



### Iterative Context-Aware Graph Inference for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/2004.02194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02194v1)
- **Published**: 2020-04-05 13:09:37+00:00
- **Updated**: 2020-04-05 13:09:37+00:00
- **Authors**: Dan Guo, Hui Wang, Hanwang Zhang, Zheng-Jun Zha, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-$K$ message passing mechanism. Specifically, in every message passing step, each node selects the most $K$ relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related $K$ neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.



### Clustering based Contrastive Learning for Improving Face Representations
- **Arxiv ID**: http://arxiv.org/abs/2004.02195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02195v1)
- **Published**: 2020-04-05 13:11:44+00:00
- **Updated**: 2020-04-05 13:11:44+00:00
- **Authors**: Vivek Sharma, Makarand Tapaswi, M. Saquib Sarfraz, Rainer Stiefelhagen
- **Comment**: To appear at IEEE International Conference on Automatic Face and
  Gesture Recognition (FG), 2020
- **Journal**: None
- **Summary**: A good clustering algorithm can discover natural groupings in data. These groupings, if used wisely, provide a form of weak supervision for learning representations. In this work, we present Clustering-based Contrastive Learning (CCL), a new clustering-based representation learning approach that uses labels obtained from clustering along with video constraints to learn discriminative face features. We demonstrate our method on the challenging task of learning representations for video face clustering. Through several ablation studies, we analyze the impact of creating pair-wise positive and negative labels from different sources. Experiments on three challenging video face clustering datasets: BBT-0101, BF-0502, and ACCIO show that CCL achieves a new state-of-the-art on all datasets.



### Confident Coreset for Active Learning in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.02200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02200v1)
- **Published**: 2020-04-05 13:46:16+00:00
- **Updated**: 2020-04-05 13:46:16+00:00
- **Authors**: Seong Tae Kim, Farrukh Mushtaq, Nassir Navab
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Recent advances in deep learning have resulted in great successes in various applications. Although semi-supervised or unsupervised learning methods have been widely investigated, the performance of deep neural networks highly depends on the annotated data. The problem is that the budget for annotation is usually limited due to the annotation time and expensive annotation cost in medical data. Active learning is one of the solutions to this problem where an active learner is designed to indicate which samples need to be annotated to effectively train a target model. In this paper, we propose a novel active learning method, confident coreset, which considers both uncertainty and distribution for effectively selecting informative samples. By comparative experiments on two medical image analysis tasks, we show that our method outperforms other active learning methods.



### Deep Multimodal Feature Encoding for Video Ordering
- **Arxiv ID**: http://arxiv.org/abs/2004.02205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.02205v1)
- **Published**: 2020-04-05 14:02:23+00:00
- **Updated**: 2020-04-05 14:02:23+00:00
- **Authors**: Vivek Sharma, Makarand Tapaswi, Rainer Stiefelhagen
- **Comment**: IEEE International Conference on Computer Vision (ICCV) Workshop on
  Large Scale Holistic Video Understanding. The datasets and code are available
  at https://github.com/vivoutlaw/tcbp
- **Journal**: None
- **Summary**: True understanding of videos comes from a joint analysis of all its modalities: the video frames, the audio track, and any accompanying text such as closed captions. We present a way to learn a compact multimodal feature representation that encodes all these modalities. Our model parameters are learned through a proxy task of inferring the temporal ordering of a set of unordered videos in a timeline. To this end, we create a new multimodal dataset for temporal ordering that consists of approximately 30K scenes (2-6 clips per scene) based on the "Large Scale Movie Description Challenge". We analyze and evaluate the individual and joint modalities on three challenging tasks: (i) inferring the temporal ordering of a set of videos; and (ii) action recognition. We demonstrate empirically that multimodal representations are indeed complementary, and can play a key role in improving the performance of many applications.



### Light Field Spatial Super-resolution via Deep Combinatorial Geometry Embedding and Structural Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/2004.02215v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02215v1)
- **Published**: 2020-04-05 14:39:57+00:00
- **Updated**: 2020-04-05 14:39:57+00:00
- **Authors**: Jing Jin, Junhui Hou, Jie Chen, Sam Kwong
- **Comment**: This paper was accepted by CVPR 2020
- **Journal**: None
- **Summary**: Light field (LF) images acquired by hand-held devices usually suffer from low spatial resolution as the limited sampling resources have to be shared with the angular dimension. LF spatial super-resolution (SR) thus becomes an indispensable part of the LF camera processing pipeline. The high-dimensionality characteristic and complex geometrical structure of LF images make the problem more challenging than traditional single-image SR. The performance of existing methods is still limited as they fail to thoroughly explore the coherence among LF views and are insufficient in accurately preserving the parallax structure of the scene. In this paper, we propose a novel learning-based LF spatial SR framework, in which each view of an LF image is first individually super-resolved by exploring the complementary information among views with combinatorial geometry embedding. For accurate preservation of the parallax structure among the reconstructed views, a regularization network trained over a structure-aware loss function is subsequently appended to enforce correct parallax relationships over the intermediate estimation. Our proposed approach is evaluated over datasets with a large number of testing images including both synthetic and real-world scenes. Experimental results demonstrate the advantage of our approach over state-of-the-art methods, i.e., our method not only improves the average PSNR by more than 1.0 dB but also preserves more accurate parallax details, at a lower computational cost.



### Structural-analogy from a Single Image Pair
- **Arxiv ID**: http://arxiv.org/abs/2004.02222v3
- **DOI**: 10.1111/cgf.14186
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02222v3)
- **Published**: 2020-04-05 14:51:10+00:00
- **Updated**: 2021-01-06 16:57:44+00:00
- **Authors**: Sagie Benaim, Ron Mokady, Amit Bermano, Daniel Cohen-Or, Lior Wolf
- **Comment**: Published in 'Computer Graphics Forum'
- **Journal**: None
- **Summary**: The task of unsupervised image-to-image translation has seen substantial advancements in recent years through the use of deep neural networks. Typically, the proposed solutions learn the characterizing distribution of two large, unpaired collections of images, and are able to alter the appearance of a given image, while keeping its geometry intact. In this paper, we explore the capabilities of neural networks to understand image structure given only a single pair of images, A and B. We seek to generate images that are structurally aligned: that is, to generate an image that keeps the appearance and style of B, but has a structural arrangement that corresponds to A. The key idea is to map between image patches at different scales. This enables controlling the granularity at which analogies are produced, which determines the conceptual distinction between style and content. In addition to structural alignment, our method can be used to generate high quality imagery in other conditional generation tasks utilizing images A and B only: guided image synthesis, style and texture transfer, text translation as well as video translation. Our code and additional results are available in https://github.com/rmokady/structural-analogy/.



### Feature Super-Resolution Based Facial Expression Recognition for Multi-scale Low-Resolution Faces
- **Arxiv ID**: http://arxiv.org/abs/2004.02234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02234v1)
- **Published**: 2020-04-05 15:38:47+00:00
- **Updated**: 2020-04-05 15:38:47+00:00
- **Authors**: Wei Jing, Feng Tian, Jizhong Zhang, Kuo-Ming Chao, Zhenxin Hong, Xu Liu
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Facial Expressions Recognition(FER) on low-resolution images is necessary for applications like group expression recognition in crowd scenarios(station, classroom etc.). Classifying a small size facial image into the right expression category is still a challenging task. The main cause of this problem is the loss of discriminative feature due to reduced resolution. Super-resolution method is often used to enhance low-resolution images, but the performance on FER task is limited when on images of very low resolution. In this work, inspired by feature super-resolution methods for object detection, we proposed a novel generative adversary network-based feature level super-resolution method for robust facial expression recognition(FSR-FER). In particular, a pre-trained FER model was employed as feature extractor, and a generator network G and a discriminator network D are trained with features extracted from images of low resolution and original high resolution. Generator network G tries to transform features of low-resolution images to more discriminative ones by making them closer to the ones of corresponding high-resolution images. For better classification performance, we also proposed an effective classification-aware loss re-weighting strategy based on the classification probability calculated by a fixed FER model to make our model focus more on samples that are easily misclassified. Experiment results on Real-World Affective Faces (RAF) Database demonstrate that our method achieves satisfying results on various down-sample factors with a single model and has better performance on low-resolution images compared with methods using image super-resolution and expression recognition separately.



### From Generalized zero-shot learning to long-tail with class descriptors
- **Arxiv ID**: http://arxiv.org/abs/2004.02235v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02235v4)
- **Published**: 2020-04-05 15:51:31+00:00
- **Updated**: 2020-11-04 15:17:56+00:00
- **Authors**: Dvir Samuel, Yuval Atzmon, Gal Chechik
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: Real-world data is predominantly unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. Often, classes can be accompanied by side information like textual descriptions, but it is not fully clear how to use them for learning with unbalanced long-tail data. Such descriptions have been mostly used in (Generalized) Zero-shot learning (ZSL), suggesting that ZSL with class descriptions may also be useful for long-tail distributions. We describe DRAGON, a late-fusion architecture for long-tail learning with class descriptors. It learns to (1) correct the bias towards head classes on a sample-by-sample basis; and (2) fuse information from class-descriptions to improve the tail-class accuracy. We also introduce new benchmarks CUB-LT, SUN-LT, AWA-LT for long-tail learning with class-descriptions, building on existing learning-with-attributes datasets and a version of Imagenet-LT with class descriptors. DRAGON outperforms state-of-the-art models on the new benchmark. It is also a new SoTA on existing benchmarks for GFSL with class descriptors (GFSL-d) and standard (vision-only) long-tailed learning ImageNet-LT, CIFAR-10, 100, and Places365.



### CondenseUNet: A Memory-Efficient Condensely-Connected Architecture for Bi-ventricular Blood Pool and Myocardium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.02249v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02249v1)
- **Published**: 2020-04-05 16:34:51+00:00
- **Updated**: 2020-04-05 16:34:51+00:00
- **Authors**: S. M. Kamrul Hasan, Cristian A. Linte
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: With the advent of Cardiac Cine Magnetic Resonance (CMR) Imaging, there has been a paradigm shift in medical technology, thanks to its capability of imaging different structures within the heart without ionizing radiation. However, it is very challenging to conduct pre-operative planning of minimally invasive cardiac procedures without accurate segmentation and identification of the left ventricle (LV), right ventricle (RV) blood-pool, and LV-myocardium. Manual segmentation of those structures, nevertheless, is time-consuming and often prone to error and biased outcomes. Hence, automatic and computationally efficient segmentation techniques are paramount. In this work, we propose a novel memory-efficient Convolutional Neural Network (CNN) architecture as a modification of both CondenseNet, as well as DenseNet for ventricular blood-pool segmentation by introducing a bottleneck block and an upsampling path. Our experiments show that the proposed architecture runs on the Automated Cardiac Diagnosis Challenge (ACDC) dataset using half (50%) the memory requirement of DenseNet and one-twelfth (~ 8%) of the memory requirements of U-Net, while still maintaining excellent accuracy of cardiac segmentation. We validated the framework on the ACDC dataset featuring one healthy and four pathology groups whose heart images were acquired throughout the cardiac cycle and achieved the mean dice scores of 96.78% (LV blood-pool), 93.46% (RV blood-pool) and 90.1% (LV-Myocardium). These results are promising and promote the proposed methods as a competitive tool for cardiac image segmentation and clinical parameter estimation that has the potential to provide fast and accurate results, as needed for pre-procedural planning and/or pre-operative applications.



### Nonparametric Data Analysis on the Space of Perceived Colors
- **Arxiv ID**: http://arxiv.org/abs/2004.03402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03402v1)
- **Published**: 2020-04-05 17:43:33+00:00
- **Updated**: 2020-04-05 17:43:33+00:00
- **Authors**: Vic Patrangenaru, Yifang Deng
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Moving around in a 3D world, requires the visual system of a living individual to rely on three channels of image recognition, which is done through three types of retinal cones. Newton, Grasmann, Helmholz and Schr$\ddot{o}$dinger laid down the basic assumptions needed to understand colored vision. Such concepts were furthered by Resnikoff, who imagined the space of perceived colors as a 3D homogeneous space.   This article is concerned with perceived colors regarded as random objects on a Resnikoff 3D homogeneous space model. Two applications to color differentiation in machine vision are illustrated for the proposed statistical methodology, applied to the Euclidean model for perceived colors.



### MNEW: Multi-domain Neighborhood Embedding and Weighting for Sparse Point Clouds Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03401v1)
- **Published**: 2020-04-05 18:02:07+00:00
- **Updated**: 2020-04-05 18:02:07+00:00
- **Authors**: Yang Zheng, Izzat H. Izzat, Sanling Song
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds have been widely adopted in 3D semantic scene understanding. However, point clouds for typical tasks such as 3D shape segmentation or indoor scenario parsing are much denser than outdoor LiDAR sweeps for the application of autonomous driving perception. Due to the spatial property disparity, many successful methods designed for dense point clouds behave depreciated effectiveness on the sparse data. In this paper, we focus on the semantic segmentation task of sparse outdoor point clouds. We propose a new method called MNEW, including multi-domain neighborhood embedding, and attention weighting based on their geometry distance, feature similarity, and neighborhood sparsity. The network architecture inherits PointNet which directly process point clouds to capture pointwise details and global semantics, and is improved by involving multi-scale local neighborhoods in static geometry domain and dynamic feature space. The distance/similarity attention and sparsity-adapted weighting mechanism of MNEW enable its capability for a wide range of data sparsity distribution. With experiments conducted on virtual and real KITTI semantic datasets, MNEW achieves the top performance for sparse point clouds, which is important to the application of LiDAR-based automated driving perception.



### Game of Learning Bloch Equation Simulations for MR Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/2004.02270v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2004.02270v1)
- **Published**: 2020-04-05 18:15:52+00:00
- **Updated**: 2020-04-05 18:15:52+00:00
- **Authors**: Mingrui Yang, Yun Jiang, Dan Ma, Bhairav B. Mehta, Mark A. Griswold
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: This work proposes a novel approach to efficiently generate MR fingerprints for MR fingerprinting (MRF) problems based on the unsupervised deep learning model generative adversarial networks (GAN). Methods: The GAN model is adopted and modified for better convergence and performance, resulting in an MRF specific model named GAN-MRF. The GAN-MRF model is trained, validated, and tested using different MRF fingerprints simulated from the Bloch equations with certain MRF sequence. The performance and robustness of the model are further tested by using in vivo data collected on a 3 Tesla scanner from a healthy volunteer together with MRF dictionaries with different sizes. T1, T2 maps are generated and compared quantitatively. Results: The validation and testing curves for the GAN-MRF model show no evidence of high bias or high variance problems. The sample MRF fingerprints generated from the trained GAN-MRF model agree well with the benchmark fingerprints simulated from the Bloch equations. The in vivo T1, T2 maps generated from the GAN-MRF fingerprints are in good agreement with those generated from the Bloch simulated fingerprints, showing good performance and robustness of the proposed GAN-MRF model. Moreover, the MRF dictionary generation time is reduced from hours to sub-second for the testing dictionary. Conclusion: The GAN-MRF model enables a fast and accurate generation of the MRF fingerprints. It significantly reduces the MRF dictionary generation process and opens the door for real-time applications and sequence optimization problems.



### Dynamic Decision Boundary for One-class Classifiers applied to non-uniformly Sampled Data
- **Arxiv ID**: http://arxiv.org/abs/2004.02273v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02273v1)
- **Published**: 2020-04-05 18:29:36+00:00
- **Updated**: 2020-04-05 18:29:36+00:00
- **Authors**: Riccardo La Grassa, Ignazio Gallo, Nicola Landro
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: A typical issue in Pattern Recognition is the non-uniformly sampled data, which modifies the general performance and capability of machine learning algorithms to make accurate predictions. Generally, the data is considered non-uniformly sampled when in a specific area of data space, they are not enough, leading us to misclassification problems. This issue cut down the goal of the one-class classifiers decreasing their performance. In this paper, we propose a one-class classifier based on the minimum spanning tree with a dynamic decision boundary (OCdmst) to make good prediction also in the case we have non-uniformly sampled data. To prove the effectiveness and robustness of our approach we compare with the most recent one-class classifier reaching the state-of-the-art in most of them.



### EfficientPS: Efficient Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.02307v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.02307v3)
- **Published**: 2020-04-05 20:15:59+00:00
- **Updated**: 2021-02-01 09:33:18+00:00
- **Authors**: Rohit Mohan, Abhinav Valada
- **Comment**: Ranked # 1 on Cityscapes panoptic segmentation benchmark, ranked # 2
  among the published methods on Cityscapes semantic segmentation benchmark,
  and ranked # 2 among the published methods on Cityscapes instance
  segmentation benchmark. Demo, code and models are available at
  https://rl.uni-freiburg.de/research/panoptic
- **Journal**: International Journal of Computer Vision (IJCV), vol. 129, no. 5,
  pp. 1551-1579, 2021
- **Summary**: Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.



### Automatic Right Ventricle Segmentation using Multi-Label Fusion in Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2004.02317v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02317v1)
- **Published**: 2020-04-05 21:06:15+00:00
- **Updated**: 2020-04-05 21:06:15+00:00
- **Authors**: Maria A. Zuluaga, M. Jorge Cardoso, Sbastien Ourselin
- **Comment**: None
- **Journal**: Workshop on RV Segmentation Challenge in Cardiac MRI in
  conjunction with Medical Image Computing and Computer-Assisted Intervention
  2012
- **Summary**: Accurate segmentation of the right ventricle (RV) is a crucial step in the assessment of the ventricular structure and function. Yet, due to its complex anatomy and motion segmentation of the RV has not been as largely studied as the left ventricle. This paper presents a fully automatic method for the segmentation of the RV in cardiac magnetic resonance images (MRI). The method uses a coarse-to-fine segmentation strategy in combination with a multi-atlas propagation segmentation framework. Based on a cross correlation metric, our method selects the best atlases for propagation allowing the refinement of the segmentation at each iteration of the propagation. The proposed method was evaluated on 32 cardiac MRI datasets provided by the RV Segmentation Challenge in Cardiac MRI.



### Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia Cases at the Era of COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2004.03399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.9; I.2.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2004.03399v1)
- **Published**: 2020-04-05 21:30:54+00:00
- **Updated**: 2020-04-05 21:30:54+00:00
- **Authors**: Karim Hammoudi, Halim Benhabiles, Mahmoud Melkemi, Fadi Dornaika, Ignacio Arganda-Carreras, Dominique Collard, Arnaud Scherpereel
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Coronavirus disease 2019 (COVID-19) is an infectious disease with first symptoms similar to the flu. COVID-19 appeared first in China and very quickly spreads to the rest of the world, causing then the 2019-20 coronavirus pandemic. In many cases, this disease causes pneumonia. Since pulmonary infections can be observed through radiography images, this paper investigates deep learning methods for automatically analyzing query chest X-ray images with the hope to bring precision tools to health professionals towards screening the COVID-19 and diagnosing confirmed patients. In this context, training datasets, deep learning architectures and analysis strategies have been experimented from publicly open sets of chest X-ray images. Tailored deep learning models are proposed to detect pneumonia infection cases, notably viral cases. It is assumed that viral pneumonia cases detected during an epidemic COVID-19 context have a high probability to presume COVID-19 infections. Moreover, easy-to-apply health indicators are proposed for estimating infection status and predicting patient status from the detected pneumonia cases. Experimental results show possibilities of training deep learning models over publicly open sets of chest X-ray images towards screening viral pneumonia. Chest X-ray test images of COVID-19 infected patients are successfully diagnosed through detection models retained for their performances. The efficiency of proposed health indicators is highlighted through simulated scenarios of patients presenting infections and health problems by combining real and synthetic health data.



### Hyper-spectral NIR and MIR data and optimal wavebands for detection of apple tree diseases
- **Arxiv ID**: http://arxiv.org/abs/2004.02325v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02325v3)
- **Published**: 2020-04-05 21:51:13+00:00
- **Updated**: 2020-04-24 07:42:03+00:00
- **Authors**: Dmitrii Shadrin, Mariia Pukalchik, Anastasia Uryasheva, Evgeny Tsykunov, Grigoriy Yashin, Nikita Rodichenko, Dzmitry Tsetserukou
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Plant diseases can lead to dramatic losses in yield and quality of food, becoming a problem of high priority for farmers. Apple scab, moniliasis, and powdery mildew are the most significant apple tree diseases worldwide and may cause between 50% and 60% in yield losses annually; they are controlled by fungicide use with huge financial and time expenses. This research proposes a modern approach for analyzing the spectral data in Near-Infrared and Mid-Infrared ranges of the apple tree diseases at different stages. Using the obtained spectra, we found optimal spectral bands for detecting particular disease and discriminating it from other diseases and healthy trees. The proposed instrument will provide farmers with accurate, real-time information on different stages of apple tree diseases, enabling more effective timing, and selecting the fungicide application, resulting in better control and increasing yield. The obtained dataset, as well as scripts in Matlab for processing data and finding optimal spectral bands, are available via the link: https://yadi.sk/d/ZqfGaNlYVR3TUA



### Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics
- **Arxiv ID**: http://arxiv.org/abs/2004.02331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02331v1)
- **Published**: 2020-04-05 22:09:08+00:00
- **Updated**: 2020-04-05 22:09:08+00:00
- **Authors**: Simon Jenni, Hailin Jin, Paolo Favaro
- **Comment**: CVPR 2020 (oral)
- **Journal**: None
- **Summary**: We introduce a novel principle for self-supervised feature learning based on the discrimination of specific transformations of an image. We argue that the generalization capability of learned features depends on what image neighborhood size is sufficient to discriminate different image transformations: The larger the required neighborhood size and the more global the image statistics that the feature can describe. An accurate description of global image statistics allows to better represent the shape and configuration of objects and their context, which ultimately generalizes better to new tasks such as object classification and detection. This suggests a criterion to choose and design image transformations. Based on this criterion, we introduce a novel image transformation that we call limited context inpainting (LCI). This transformation inpaints an image patch conditioned only on a small rectangular pixel boundary (the limited context). Because of the limited boundary information, the inpainter can learn to match local pixel statistics, but is unlikely to match the global statistics of the image. We claim that the same principle can be used to justify the performance of transformations such as image rotations and warping. Indeed, we demonstrate experimentally that learning to discriminate transformations such as LCI, image warping and rotations, yields features with state of the art generalization capabilities on several datasets such as Pascal VOC, STL-10, CelebA, and ImageNet. Remarkably, our trained features achieve a performance on Places on par with features trained through supervised learning with ImageNet labels.




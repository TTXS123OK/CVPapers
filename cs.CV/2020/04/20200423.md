# Arxiv Papers in cs.CV on 2020-04-23
### PolyLaneNet: Lane Estimation via Deep Polynomial Regression
- **Arxiv ID**: http://arxiv.org/abs/2004.10924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10924v2)
- **Published**: 2020-04-23 01:23:02+00:00
- **Updated**: 2020-07-14 17:02:54+00:00
- **Authors**: Lucas Tabelini, Rodrigo Berriel, Thiago M. Paixão, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: One of the main factors that contributed to the large advances in autonomous driving is the advent of deep learning. For safer self-driving vehicles, one of the problems that has yet to be solved completely is lane detection. Since methods for this task have to work in real-time (+30 FPS), they not only have to be effective (i.e., have high accuracy) but they also have to be efficient (i.e., fast). In this work, we present a novel method for lane detection that uses as input an image from a forward-looking camera mounted in the vehicle and outputs polynomials representing each lane marking in the image, via deep polynomial regression. The proposed method is shown to be competitive with existing state-of-the-art methods in the TuSimple dataset while maintaining its efficiency (115 FPS). Additionally, extensive qualitative results on two additional public datasets are presented, alongside with limitations in the evaluation metrics used by recent works for lane detection. Finally, we provide source code and trained models that allow others to replicate all the results shown in this paper, which is surprisingly rare in state-of-the-art lane detection methods. The full source code and pretrained models are available at https://github.com/lucastabelini/PolyLaneNet.



### YOLOv4: Optimal Speed and Accuracy of Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.10934v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10934v1)
- **Published**: 2020-04-23 02:10:02+00:00
- **Updated**: 2020-04-23 02:10:02+00:00
- **Authors**: Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao
- **Comment**: None
- **Journal**: None
- **Summary**: There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet



### PERMDNN: Efficient Compressed DNN Architecture with Permuted Diagonal Matrices
- **Arxiv ID**: http://arxiv.org/abs/2004.10936v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2004.10936v1)
- **Published**: 2020-04-23 02:26:40+00:00
- **Updated**: 2020-04-23 02:26:40+00:00
- **Authors**: Chunhua Deng, Siyu Liao, Yi Xie, Keshab K. Parhi, Xuehai Qian, Bo Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network (DNN) has emerged as the most important and popular artificial intelligent (AI) technique. The growth of model size poses a key energy efficiency challenge for the underlying computing platform. Thus, model compression becomes a crucial problem. However, the current approaches are limited by various drawbacks. Specifically, network sparsification approach suffers from irregularity, heuristic nature and large indexing overhead. On the other hand, the recent structured matrix-based approach (i.e., CirCNN) is limited by the relatively complex arithmetic computation (i.e., FFT), less flexible compression ratio, and its inability to fully utilize input sparsity. To address these drawbacks, this paper proposes PermDNN, a novel approach to generate and execute hardware-friendly structured sparse DNN models using permuted diagonal matrices. Compared with unstructured sparsification approach, PermDNN eliminates the drawbacks of indexing overhead, non-heuristic compression effects and time-consuming retraining. Compared with circulant structure-imposing approach, PermDNN enjoys the benefits of higher reduction in computational complexity, flexible compression ratio, simple arithmetic computation and full utilization of input sparsity. We propose PermDNN architecture, a multi-processing element (PE) fully-connected (FC) layer-targeted computing engine. The entire architecture is highly scalable and flexible, and hence it can support the needs of different applications with different model configurations. We implement a 32-PE design using CMOS 28nm technology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout, 5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher throughput and 3.89x better energy efficiency.



### Distilling Knowledge from Refinement in Multiple Instance Detection Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.10943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10943v1)
- **Published**: 2020-04-23 02:49:40+00:00
- **Updated**: 2020-04-23 02:49:40+00:00
- **Authors**: Luis Felipe Zeni, Claudio Jung
- **Comment**: published at CVPR 2020 Deepvision Workshop
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD) aims to tackle the object detection problem using only labeled image categories as supervision. A common approach used in WSOD to deal with the lack of localization information is Multiple Instance Learning, and in recent years methods started adopting Multiple Instance Detection Networks (MIDN), which allows training in an end-to-end fashion. In general, these methods work by selecting the best instance from a pool of candidates and then aggregating other instances based on similarity. In this work, we claim that carefully selecting the aggregation criteria can considerably improve the accuracy of the learned detector. We start by proposing an additional refinement step to an existing approach (OICR), which we call refinement knowledge distillation. Then, we present an adaptive supervision aggregation function that dynamically changes the aggregation criteria for selecting boxes related to one of the ground-truth classes, background, or even ignored during the generation of each refinement module supervision. Experiments in Pascal VOC 2007 demonstrate that our Knowledge Distillation and smooth aggregation function significantly improves the performance of OICR in the weakly supervised object detection and weakly supervised object localization tasks. These improvements make the Boosted-OICR competitive again versus other state-of-the-art approaches.



### Joint Bilateral Learning for Real-time Universal Photorealistic Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2004.10955v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10955v2)
- **Published**: 2020-04-23 03:31:24+00:00
- **Updated**: 2020-04-27 13:02:17+00:00
- **Authors**: Xide Xia, Meng Zhang, Tianfan Xue, Zheng Sun, Hui Fang, Brian Kulis, Jiawen Chen
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: Photorealistic style transfer is the task of transferring the artistic style of an image onto a content target, producing a result that is plausibly taken with a camera. Recent approaches, based on deep neural networks, produce impressive results but are either too slow to run at practical resolutions, or still contain objectionable artifacts. We propose a new end-to-end model for photorealistic style transfer that is both fast and inherently generates photorealistic results. The core of our approach is a feed-forward neural network that learns local edge-aware affine transforms that automatically obey the photorealism constraint. When trained on a diverse set of images and a variety of styles, our model can robustly apply style transfer to an arbitrary pair of input images. Compared to the state of the art, our method produces visually superior results and is three orders of magnitude faster, enabling real-time performance at 4K on a mobile phone. We validate our method with ablation and user studies.



### Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.10956v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10956v2)
- **Published**: 2020-04-23 03:38:33+00:00
- **Updated**: 2020-04-24 02:12:32+00:00
- **Authors**: Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong
- **Comment**: Accepted by CVPR 2020 (oral)
- **Journal**: None
- **Summary**: The ability to incrementally learn new classes is crucial to the development of real-world artificial intelligence systems. In this paper, we focus on a challenging but practical few-shot class-incremental learning (FSCIL) problem. FSCIL requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. To address this problem, we represent the knowledge using a neural gas (NG) network, which can learn and preserve the topology of the feature manifold formed by different classes. On this basis, we propose the TOpology-Preserving knowledge InCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old classes by stabilizing NG's topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental learning methods on CIFAR100, miniImageNet, and CUB200 datasets.



### Uncertainty Quantification for Hyperspectral Image Denoising Frameworks based on Low-rank Matrix Approximation
- **Arxiv ID**: http://arxiv.org/abs/2004.10959v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10959v4)
- **Published**: 2020-04-23 03:56:30+00:00
- **Updated**: 2022-05-06 10:57:04+00:00
- **Authors**: Jingwei Song, Shaobo Xia, Jun Wang, Mitesh Patel, Dong Chen
- **Comment**: Accepted for publication by IEEE Transactions on Geoscience and
  Remote Sensing. IEEE Transactions on Geoscience and Remote Sensing (TGRS)
- **Journal**: None
- **Summary**: Sliding-window based low-rank matrix approximation (LRMA) is a technique widely used in hyperspectral images (HSIs) denoising or completion. However, the uncertainty quantification of the restored HSI has not been addressed to date. Accurate uncertainty quantification of the denoised HSI facilitates to applications such as multi-source or multi-scale data fusion, data assimilation, and product uncertainty quantification, since these applications require an accurate approach to describe the statistical distributions of the input data. Therefore, we propose a prior-free closed-form element-wise uncertainty quantification method for LRMA-based HSI restoration. Our closed-form algorithm overcomes the difficulty of the HSI patch mixing problem caused by the sliding-window strategy used in the conventional LRMA process. The proposed approach only requires the uncertainty of the observed HSI and provides the uncertainty result relatively rapidly and with similar computational complexity as the LRMA technique. We conduct extensive experiments to validate the estimation accuracy of the proposed closed-form uncertainty approach. The method is robust to at least 10% random impulse noise at the cost of 10-20% of additional processing time compared to the LRMA. The experiments indicate that the proposed closed-form uncertainty quantification method is more applicable to real-world applications than the baseline Monte Carlo test, which is computationally expensive. The code is available in the attachment and will be released after the acceptance of this paper.



### Metric-Learning-Assisted Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.10963v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10963v3)
- **Published**: 2020-04-23 04:20:02+00:00
- **Updated**: 2020-06-11 09:41:08+00:00
- **Authors**: Yueming Yin, Zhen Yang, Haifeng Hu, Xiaofu Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Domain alignment (DA) has been widely used in unsupervised domain adaptation. Many existing DA methods assume that a low source risk, together with the alignment of distributions of source and target, means a low target risk. In this paper, we show that this does not always hold. We thus propose a novel metric-learning-assisted domain adaptation (MLA-DA) method, which employs a novel triplet loss for helping better feature alignment. We explore the relationship between the second largest probability of a target sample's prediction and its distance to the decision boundary. Based on the relationship, we propose a novel mechanism to adaptively adjust the margin in the triplet loss according to target predictions. Experimental results show that the use of proposed triplet loss can achieve clearly better results. We also demonstrate the performance improvement of MLA-DA on all four standard benchmarks compared with the state-of-the-art unsupervised domain adaptation methods. Furthermore, MLA-DA shows stable performance in robust experiments.



### Visual Question Answering Using Semantic Information from Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2004.10966v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10966v2)
- **Published**: 2020-04-23 04:35:04+00:00
- **Updated**: 2021-04-03 18:09:22+00:00
- **Authors**: Tasmia Tasrin, Md Sultan Al Nahian, Brent Harrison
- **Comment**: 6 pages, 5 figures, The 34th International FLAIRS Conference
- **Journal**: None
- **Summary**: In this work, we propose a deep neural architecture that uses an attention mechanism which utilizes region based image features, the natural language question asked, and semantic knowledge extracted from the regions of an image to produce open-ended answers for questions asked in a visual question answering (VQA) task. The combination of both region based features and region based textual information about the image bolsters a model to more accurately respond to questions and potentially do so with less required training data. We evaluate our proposed architecture on a VQA task against a strong baseline and show that our method achieves excellent results on this task.



### Real-time Detection of Clustered Events in Video-imaging data with Applications to Additive Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2004.10977v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10977v1)
- **Published**: 2020-04-23 05:32:13+00:00
- **Updated**: 2020-04-23 05:32:13+00:00
- **Authors**: Hao Yan, Marco Grasso, Kamran Paynabar, Bianca Maria Colosimo
- **Comment**: None
- **Journal**: None
- **Summary**: The use of video-imaging data for in-line process monitoring applications has become more and more popular in the industry. In this framework, spatio-temporal statistical process monitoring methods are needed to capture the relevant information content and signal possible out-of-control states. Video-imaging data are characterized by a spatio-temporal variability structure that depends on the underlying phenomenon, and typical out-of-control patterns are related to the events that are localized both in time and space. In this paper, we propose an integrated spatio-temporal decomposition and regression approach for anomaly detection in video-imaging data. Out-of-control events are typically sparse spatially clustered and temporally consistent. Therefore, the goal is to not only detect the anomaly as quickly as possible ("when") but also locate it ("where"). The proposed approach works by decomposing the original spatio-temporal data into random natural events, sparse spatially clustered and temporally consistent anomalous events, and random noise. Recursive estimation procedures for spatio-temporal regression are presented to enable the real-time implementation of the proposed methodology. Finally, a likelihood ratio test procedure is proposed to detect when and where the hotspot happens. The proposed approach was applied to the analysis of video-imaging data to detect and locate local over-heating phenomena ("hotspots") during the layer-wise process in a metal additive manufacturing process.



### COVID-19 Chest CT Image Segmentation -- A Deep Convolutional Neural Network Solution
- **Arxiv ID**: http://arxiv.org/abs/2004.10987v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10987v2)
- **Published**: 2020-04-23 06:09:16+00:00
- **Updated**: 2020-04-26 01:45:16+00:00
- **Authors**: Qingsen Yan, Bo Wang, Dong Gong, Chuan Luo, Wei Zhao, Jianhu Shen, Qinfeng Shi, Shuo Jin, Liang Zhang, Zheng You
- **Comment**: None
- **Journal**: None
- **Summary**: A novel coronavirus disease 2019 (COVID-19) was detected and has spread rapidly across various countries around the world since the end of the year 2019, Computed Tomography (CT) images have been used as a crucial alternative to the time-consuming RT-PCR test. However, pure manual segmentation of CT images faces a serious challenge with the increase of suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. In this work, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We firstly maintain a large and new chest CT image dataset consisting of 165,667 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FV block can enhance the capability of feature representation effectively and adaptively for diverse cases. We fuse features at different scales by proposing Progressive Atrous Spatial Pyramid Pooling to handle the sophisticated infection areas with diverse appearance and shapes. We conducted experiments on the data collected in China and Germany and show that the proposed deep CNN can produce impressive performance effectively.



### Cross-ethnicity Face Anti-spoofing Recognition Challenge: A Review
- **Arxiv ID**: http://arxiv.org/abs/2004.10998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10998v1)
- **Published**: 2020-04-23 06:43:08+00:00
- **Updated**: 2020-04-23 06:43:08+00:00
- **Authors**: Ajian Liu, Xuan Li, Jun Wan, Sergio Escalera, Hugo Jair Escalante, Meysam Madadi, Yi Jin, Zhuoyuan Wu, Xiaogang Yu, Zichang Tan, Qi Yuan, Ruikun Yang, Benjia Zhou, Guodong Guo, Stan Z. Li
- **Comment**: 18 figures, 6 tables, 12 pages
- **Journal**: None
- **Summary**: Face anti-spoofing is critical to prevent face recognition systems from a security breach. The biometrics community has %possessed achieved impressive progress recently due the excellent performance of deep neural networks and the availability of large datasets. Although ethnic bias has been verified to severely affect the performance of face recognition systems, it still remains an open research problem in face anti-spoofing. Recently, a multi-ethnic face anti-spoofing dataset, CASIA-SURF CeFA, has been released with the goal of measuring the ethnic bias. It is the largest up to date cross-ethnicity face anti-spoofing dataset covering $3$ ethnicities, $3$ modalities, $1,607$ subjects, 2D plus 3D attack types, and the first dataset including explicit ethnic labels among the recently released datasets for face anti-spoofing. We organized the Chalearn Face Anti-spoofing Attack Detection Challenge which consists of single-modal (e.g., RGB) and multi-modal (e.g., RGB, Depth, Infrared (IR)) tracks around this novel resource to boost research aiming to alleviate the ethnic bias. Both tracks have attracted $340$ teams in the development stage, and finally 11 and 8 teams have submitted their codes in the single-modal and multi-modal face anti-spoofing recognition challenges, respectively. All the results were verified and re-ran by the organizing team, and the results were used for the final ranking. This paper presents an overview of the challenge, including its design, evaluation protocol and a summary of results. We analyze the top ranked solutions and draw conclusions derived from the competition. In addition we outline future work directions.



### Location-Aware Feature Selection Text Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2004.10999v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2004.10999v2)
- **Published**: 2020-04-23 06:45:26+00:00
- **Updated**: 2020-05-26 02:26:21+00:00
- **Authors**: Zengyuan Guo, Zilin Wang, Zhihui Wang, Wanli Ouyang, Haojie Li, Wen Gao
- **Comment**: 10 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Regression-based text detection methods have already achieved promising performances with simple network structure and high efficiency. However, they are behind in accuracy comparing with recent segmentation-based text detectors. In this work, we discover that one important reason to this case is that regression-based methods usually utilize a fixed feature selection way, i.e. selecting features in a single location or in neighbor regions, to predict components of the bounding box, such as the distances to the boundaries or the rotation angle. The features selected through this way sometimes are not the best choices for predicting every component of a text bounding box and thus degrade the accuracy performance. To address this issue, we propose a novel Location-Aware feature Selection text detection Network (LASNet). LASNet selects suitable features from different locations to separately predict the five components of a bounding box and gets the final bounding box through the combination of these components. Specifically, instead of using the classification score map to select one feature for predicting the whole bounding box as most of the existing methods did, the proposed LASNet first learn five new confidence score maps to indicate the prediction accuracy of the bounding box components, respectively. Then, a Location-Aware Feature Selection mechanism (LAFS) is designed to weightily fuse the top-$K$ prediction results for each component according to their confidence score, and to combine the all five fused components into a final bounding box. As a result, LASNet predicts the more accurate bounding boxes by using a learnable feature selection way. The experimental results demonstrate that our LASNet achieves state-of-the-art performance with single-model and single-scale testing, outperforming all existing regression-based detectors.



### An Asymmetric Cycle-Consistency Loss for Dealing with Many-to-One Mappings in Image Translation: A Study on Thigh MR Scans
- **Arxiv ID**: http://arxiv.org/abs/2004.11001v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11001v3)
- **Published**: 2020-04-23 06:59:58+00:00
- **Updated**: 2021-01-11 18:06:13+00:00
- **Authors**: Michael Gadermayr, Maximilian Tschuchnig, Laxmi Gupta, Dorit Merhof, Nils Krämer, Daniel Truhn, Burkhard Gess
- **Comment**: Presented at IEEE ISBI'21
- **Journal**: None
- **Summary**: Generative adversarial networks using a cycle-consistency loss facilitate unpaired training of image-translation models and thereby exhibit a very high potential in manifold medical applications. However, the fact that images in one domain potentially map to more than one image in another domain (e.g. in case of pathological changes) exhibits a major challenge for training the networks. In this work, we offer a solution to improve the training process in case of many-to-one mappings by modifying the cycle-consistency loss. We show formally and empirically that the proposed method improves the performance significantly without radically changing the architecture and without increasing the overall complexity. We evaluate our method on thigh MRI scans with the final goal of segmenting the muscle in fat-infiltrated patients' data.



### SimUSR: A Simple but Strong Baseline for Unsupervised Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.11020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11020v1)
- **Published**: 2020-04-23 08:27:41+00:00
- **Updated**: 2020-04-23 08:27:41+00:00
- **Authors**: Namhyuk Ahn, Jaejun Yoo, Kyung-Ah Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle a fully unsupervised super-resolution problem, i.e., neither paired images nor ground truth HR images. We assume that low resolution (LR) images are relatively easy to collect compared to high resolution (HR) images. By allowing multiple LR images, we build a set of pseudo pairs by denoising and downsampling LR images and cast the original unsupervised problem into a supervised learning problem but in one level lower. Though this line of study is easy to think of and thus should have been investigated prior to any complicated unsupervised methods, surprisingly, there are currently none. Even more, we show that this simple method outperforms the state-of-the-art unsupervised method with a dramatically shorter latency at runtime, and significantly reduces the gap to the HR supervised models. We submitted our method in NTIRE 2020 super-resolution challenge and won 1st in PSNR, 2nd in SSIM, and 13th in LPIPS. This simple method should be used as the baseline to beat in the future, especially when multiple LR images are allowed during the training phase. However, even in the zero-shot condition, we argue that this method can serve as a useful baseline to see the gap between supervised and unsupervised frameworks.



### Virtual SAR: A Synthetic Dataset for Deep Learning based Speckle Noise Reduction Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2004.11021v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11021v1)
- **Published**: 2020-04-23 08:27:45+00:00
- **Updated**: 2020-04-23 08:27:45+00:00
- **Authors**: Shrey Dabhi, Kartavya Soni, Utkarsh Patel, Priyanka Sharma, Manojkumar Parmar
- **Comment**: 5 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) images contain a huge amount of information, however, the number of practical use-cases is limited due to the presence of speckle noise in them. In recent years, deep learning based techniques have brought significant improvement in the domain of denoising and image restoration. However, further research has been hampered by the lack of availability of data suitable for training deep neural network based systems. With this paper, we propose a standard way of generating synthetic data for the training of speckle reduction algorithms and demonstrate a use-case to advance research in this domain.



### Proceedings of the ICLR Workshop on Computer Vision for Agriculture (CV4A) 2020
- **Arxiv ID**: http://arxiv.org/abs/2004.11051v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2004.11051v3)
- **Published**: 2020-04-23 10:11:52+00:00
- **Updated**: 2020-05-17 16:50:58+00:00
- **Authors**: Yannis Kalantidis, Laura Sevilla-Lara, Ernest Mwebaze, Dina Machuve, Hamed Alemohammad, David Guerena
- **Comment**: 14 papers accepted, 4 as oral, 10 as spotlights
- **Journal**: None
- **Summary**: This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held in conjunction with the International Conference on Learning Representations (ICLR) 2020.   The Computer Vision for Agriculture (CV4A) 2020 workshop was scheduled to be held in Addis Ababa, Ethiopia, on April 26th, 2020. It was held virtually that same day due to the COVID-19 pandemic. The workshop was held in conjunction with the International Conference on Learning Representations (ICLR) 2020.



### Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2004.11676v5
- **DOI**: 10.1007/s10489-020-01900-3
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11676v5)
- **Published**: 2020-04-23 10:24:34+00:00
- **Updated**: 2020-07-21 10:27:36+00:00
- **Authors**: Narinder Singh Punn, Sonali Agarwal
- **Comment**: None
- **Journal**: Appl Intell (2020)
- **Summary**: The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that resembles pneumonia. The current diagnostic procedure of COVID-19 follows reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which however is less sensitive to identify the virus at the initial stage. Hence, a more robust and alternate diagnosis technique is desirable. Recently, with the release of publicly available datasets of corona positive patients comprising of computed tomography (CT) and chest X-ray (CXR) imaging; scientists, researchers and healthcare experts are contributing for faster and automated diagnosis of COVID-19 by identifying pulmonary infections using deep learning approaches to achieve better cure and treatment. These datasets have limited samples concerned with the positive COVID-19 cases, which raise the challenge for unbiased learning. Following from this context, this article presents the random oversampling and weighted class loss function approach for unbiased fine-tuned learning (transfer learning) in various state-of-the-art deep learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2, DenseNet169, and NASNetLarge to perform binary classification (as normal and COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia, and normal case) of posteroanterior CXR images. Accuracy, precision, recall, loss, and area under the curve (AUC) are utilized to evaluate the performance of the models. Considering the experimental results, the performance of each model is scenario dependent; however, NASNetLarge displayed better scores in contrast to other architectures, which is further compared with other recently proposed approaches. This article also added the visual explanation to illustrate the basis of model classification and perception of COVID-19 in CXR images.



### Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training with Self-Supervised Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.11072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11072v1)
- **Published**: 2020-04-23 11:03:56+00:00
- **Updated**: 2020-04-23 11:03:56+00:00
- **Authors**: Marvin Klingner, Andreas Bär, Tim Fingscheidt
- **Comment**: CVPR 2020 Workshop on Safe Artificial Intelligence for Automated
  Driving
- **Journal**: None
- **Summary**: While current approaches for neural network training often aim at improving performance, less focus is put on training methods aiming at robustness towards varying noise conditions or directed attacks by adversarial examples. In this paper, we propose to improve robustness by a multi-task training, which extends supervised semantic segmentation by a self-supervised monocular depth estimation on unlabeled videos. This additional task is only performed during training to improve the semantic segmentation model's robustness at test time under several input perturbations. Moreover, we even find that our joint training approach also improves the performance of the model on the original (supervised) semantic segmentation task. Our evaluation exhibits a particular novelty in that it allows to mutually compare the effect of input noises and adversarial attacks on the robustness of the semantic segmentation. We show the effectiveness of our method on the Cityscapes dataset, where our multi-task training approach consistently outperforms the single-task semantic segmentation baseline in terms of both robustness vs. noise and in terms of adversarial attacks, without the need for depth labels in training.



### Fast Convex Relaxations using Graph Discretizations
- **Arxiv ID**: http://arxiv.org/abs/2004.11075v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2004.11075v2)
- **Published**: 2020-04-23 11:14:38+00:00
- **Updated**: 2020-09-11 11:42:38+00:00
- **Authors**: Jonas Geiping, Fjedor Gaede, Hartmut Bauermeister, Michael Moeller
- **Comment**: 20 pages, 9 figures. BMVC 2020 publication
- **Journal**: None
- **Summary**: Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.



### DAN: A Deformation-Aware Network for Consecutive Biomedical Image Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2004.11076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11076v1)
- **Published**: 2020-04-23 11:14:44+00:00
- **Updated**: 2020-04-23 11:14:44+00:00
- **Authors**: Zejin Wang, Guoqing Li, Xi Chen, Hua Han
- **Comment**: None
- **Journal**: None
- **Summary**: The continuity of biological tissue between consecutive biomedical images makes it possible for the video interpolation algorithm, to recover large area defects and tears that are common in biomedical images. However, noise and blur differences, large deformation, and drift between biomedical images, make the task challenging. To address the problem, this paper introduces a deformation-aware network to synthesize each pixel in accordance with the continuity of biological tissue. First, we develop a deformation-aware layer for consecutive biomedical images interpolation that implicitly adopting global perceptual deformation. Second, we present an adaptive style-balance loss to take the style differences of consecutive biomedical images such as blur and noise into consideration. Guided by the deformation-aware module, we synthesize each pixel from a global domain adaptively which further improves the performance of pixel synthesis. Quantitative and qualitative experiments on the benchmark dataset show that the proposed method is superior to the state-of-the-art approaches.



### SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.11085v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11085v4)
- **Published**: 2020-04-23 11:28:27+00:00
- **Updated**: 2020-10-19 13:16:59+00:00
- **Authors**: Raphael Memmesheimer, Nick Theisen, Dietrich Paulus
- **Comment**: 8 pages, 6 figures, 7 tables
- **Journal**: None
- **Summary**: Recognizing an activity with a single reference sample using metric learning approaches is a promising research field. The majority of few-shot methods focus on object recognition or face-identification. We propose a metric learning approach to reduce the action recognition problem to a nearest neighbor search in embedding space. We encode signals into images and extract features using a deep residual CNN. Using triplet loss, we learn a feature embedding. The resulting encoder transforms features into an embedding space in which closer distances encode similar actions while higher distances encode different actions. Our approach is based on a signal level formulation and remains flexible across a variety of modalities. It further outperforms the baseline on the large scale NTU RGB+D 120 dataset for the One-Shot action recognition protocol by 5.6%. With just 60% of the training data, our approach still outperforms the baseline approach by 3.7%. With 40% of the training data, our approach performs comparably well to the second follow up. Further, we show that our approach generalizes well in experiments on the UTD-MHAD dataset for inertial, skeleton and fused data and the Simitate dataset for motion capturing data. Furthermore, our inter-joint and inter-sensor experiments suggest good capabilities on previously unseen setups.



### Conditional Variational Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2004.11373v2
- **DOI**: 10.1109/TIP.2020.2990606
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11373v2)
- **Published**: 2020-04-23 11:51:38+00:00
- **Updated**: 2020-05-08 15:29:51+00:00
- **Authors**: Ying-Jun Du, Jun Xu, Xian-Tong Zhen, Ming-Ming Cheng, Ling Shao
- **Comment**: 14pages, 11 figures, 5 tables, newly accepted by TIP
- **Journal**: None
- **Summary**: Image deraining is an important yet challenging image processing task. Though deterministic image deraining methods are developed with encouraging performance, they are infeasible to learn flexible representations for probabilistic inference and diverse predictions. Besides, rain intensity varies both in spatial locations and across color channels, making this task more difficult. In this paper, we propose a Conditional Variational Image Deraining (CVID) network for better deraining performance, leveraging the exclusive generative ability of Conditional Variational Auto-Encoder (CVAE) on providing diverse predictions for the rainy image. To perform spatially adaptive deraining, we propose a spatial density estimation (SDE) module to estimate a rain density map for each image. Since rain density varies across different color channels, we also propose a channel-wise (CW) deraining scheme. Experiments on synthesized and real-world datasets show that the proposed CVID network achieves much better performance than previous deterministic methods on image deraining. Extensive ablation studies validate the effectiveness of the proposed SDE module and CW scheme in our CVID network. The code is available at \url{https://github.com/Yingjun-Du/VID}.



### Improving the Interpretability of fMRI Decoding using Deep Neural Networks and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2004.11114v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11114v3)
- **Published**: 2020-04-23 12:56:24+00:00
- **Updated**: 2020-12-17 16:01:57+00:00
- **Authors**: Patrick McClure, Dustin Moraczewski, Ka Chun Lam, Adam Thomas, Francisco Pereira
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are being increasingly used to make predictions from functional magnetic resonance imaging (fMRI) data. However, they are widely seen as uninterpretable "black boxes", as it can be difficult to discover what input information is used by the DNN in the process, something important in both cognitive neuroscience and clinical applications. A saliency map is a common approach for producing interpretable visualizations of the relative importance of input features for a prediction. However, methods for creating maps often fail due to DNNs being sensitive to input noise, or by focusing too much on the input and too little on the model. It is also challenging to evaluate how well saliency maps correspond to the truly relevant input information, as ground truth is not always available. In this paper, we review a variety of methods for producing gradient-based saliency maps, and present a new adversarial training method we developed to make DNNs robust to input noise, with the goal of improving interpretability. We introduce two quantitative evaluation procedures for saliency map methods in fMRI, applicable whenever a DNN or linear model is being trained to decode some information from imaging data. We evaluate the procedures using a synthetic dataset where the complex activation structure is known, and on saliency maps produced for DNN and linear models for task decoding in the Human Connectome Project (HCP) dataset. Our key finding is that saliency maps produced with different methods vary widely in interpretability, in both in synthetic and HCP fMRI data. Strikingly, even when DNN and linear models decode at comparable levels of performance, DNN saliency maps score higher on interpretability than linear model saliency maps (derived via weights or gradient). Finally, saliency maps produced with our adversarial training method outperform those from other methods.



### The Creation and Detection of Deepfakes: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2004.11138v3
- **DOI**: 10.1145/3425780
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11138v3)
- **Published**: 2020-04-23 13:35:49+00:00
- **Updated**: 2020-09-13 22:44:33+00:00
- **Authors**: Yisroel Mirsky, Wenke Lee
- **Comment**: None
- **Journal**: ACM Computing Surveys (CSUR), 2020, preprint
- **Summary**: Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these `deepfakes' have advanced significantly.   In this paper, we explore the creation and detection of deepfakes and provide an in-depth view of how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas which require further research and attention.



### Cloud-Based Face and Speech Recognition for Access Control Applications
- **Arxiv ID**: http://arxiv.org/abs/2004.11168v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2004.11168v2)
- **Published**: 2020-04-23 13:57:55+00:00
- **Updated**: 2020-05-08 13:07:24+00:00
- **Authors**: Nathalie Tkauc, Thao Tran, Kevin Hernandez-Diaz, Fernando Alonso-Fernandez
- **Comment**: Published at Proc. 6th International Workshop on Security and Privacy
  in the Cloud, SPC, in conjunction with IEEE Conference on Communications and
  Network Security, CNS, Avignon, France, 29 June - 1 July 2020
- **Journal**: Proc. 6th International Workshop on Security and Privacy in the
  Cloud, SPC, in conjunction with IEEE Conference on Communications and Network
  Security, CNS, Avignon, France, 29 June - 1 July 2020
- **Summary**: This paper describes the implementation of a system to recognize employees and visitors wanting to gain access to a physical office through face images and speech-to-text recognition. The system helps employees to unlock the entrance door via face recognition without the need of tag-keys or cards. To prevent spoofing attacks and increase security, a randomly generated code is sent to the employee, who then has to type it into the screen. On the other hand, visitors and delivery persons are provided with a speech-to-text service where they utter the name of the employee that they want to meet, and the system then sends a notification to the right employee automatically. The hardware of the system is constituted by two Raspberry Pi, a 7-inch LCD-touch display, a camera, and a sound card with a microphone and speaker. To carry out face recognition and speech-to-text conversion, the cloud-based platforms Amazon Web Services and the Google Speech-to-Text API service are used respectively. The two-step face authentication mechanism for employees provides an increased level of security and protection against spoofing attacks without the need of carrying key-tags or access cards, while disturbances by visitors or couriers are minimized by notifying their arrival to the right employee, without disturbing other co-workers by means of ring-bells.



### Stage-Wise Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.11178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11178v2)
- **Published**: 2020-04-23 14:16:39+00:00
- **Updated**: 2020-10-19 21:59:00+00:00
- **Authors**: Artur Jordao, Fernando Akio, Maiko Lie, William Robson Schwartz
- **Comment**: Accepted for publication at International Conference on Pattern
  Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: Modern convolutional networks such as ResNet and NASNet have achieved state-of-the-art results in many computer vision applications. These architectures consist of stages, which are sets of layers that operate on representations in the same resolution. It has been demonstrated that increasing the number of layers in each stage improves the prediction ability of the network. However, the resulting architecture becomes computationally expensive in terms of floating point operations, memory requirements and inference time. Thus, significant human effort is necessary to evaluate different trade-offs between depth and performance. To handle this problem, recent works have proposed to automatically design high-performance architectures, mainly by means of neural architecture search (NAS). Current NAS strategies analyze a large set of possible candidate architectures and, hence, require vast computational resources and take many GPUs days. Motivated by this, we propose a NAS approach to efficiently design accurate and low-cost convolutional architectures and demonstrate that an efficient strategy for designing these architectures is to learn the depth stage-by-stage. For this purpose, our approach increases depth incrementally in each stage taking into account its importance, such that stages with low importance are kept shallow while stages with high importance become deeper. We conduct experiments on the CIFAR and different versions of ImageNet datasets, where we show that architectures discovered by our approach achieve better accuracy and efficiency than human-designed architectures. Additionally, we show that architectures discovered on CIFAR-10 can be successfully transferred to large datasets. Compared to previous NAS approaches, our method is substantially more efficient, as it evaluates one order of magnitude fewer models and yields architectures on par with the state-of-the-art.



### BIT-VO: Visual Odometry at 300 FPS using Binary Features from the Focal Plane
- **Arxiv ID**: http://arxiv.org/abs/2004.11186v1
- **DOI**: 10.1109/IROS45743.2020.9341151
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11186v1)
- **Published**: 2020-04-23 14:26:16+00:00
- **Updated**: 2020-04-23 14:26:16+00:00
- **Authors**: Riku Murai, Sajad Saeedi, Paul H. J. Kelly
- **Comment**: 8 pages, 16 figures
- **Journal**: None
- **Summary**: Focal-plane Sensor-processor (FPSP) is a next-generation camera technology which enables every pixel on the sensor chip to perform computation in parallel, on the focal plane where the light intensity is captured. SCAMP-5 is a general-purpose FPSP used in this work and it carries out computations in the analog domain before analog to digital conversion. By extracting features from the image on the focal plane, data which is digitized and transferred is reduced. As a consequence, SCAMP-5 offers a high frame rate while maintaining low energy consumption. Here, we present BIT-VO, which is, to the best of our knowledge, the first 6 Degrees of Freedom visual odometry algorithm which utilises the FPSP. Our entire system operates at 300 FPS in a natural scene, using binary edges and corner features detected by the SCAMP-5.



### Detection and Classification of Industrial Signal Lights for Factory Floors
- **Arxiv ID**: http://arxiv.org/abs/2004.11187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11187v2)
- **Published**: 2020-04-23 14:26:39+00:00
- **Updated**: 2020-05-28 14:31:53+00:00
- **Authors**: Felix Nilsson, Jens Jakobsen, Fernando Alonso-Fernandez
- **Comment**: Published at Proc International Conference on Intelligent Systems and
  Computer Vision, ISCV, Fez, Morocco, 9-11 June 2020
- **Journal**: Proc International Conference on Intelligent Systems and Computer
  Vision, ISCV, Fez, Morocco, 9-11 June 2020
- **Summary**: Industrial manufacturing has developed during the last decades from a labor-intensive manual control of machines to a fully-connected automated process. The next big leap is known as industry 4.0, or smart manufacturing. With industry 4.0 comes increased integration between IT systems and the factory floor from the customer order system to final delivery of the product. One benefit of this integration is mass production of individually customized products. However, this has proven challenging to implement into existing factories, considering that their lifetime can be up to 30 years. The single most important parameter to measure in a factory is the operating hours of each machine. Operating hours can be affected by machine maintenance as well as re-configuration for different products. For older machines without connectivity, the operating state is typically indicated by signal lights of green, yellow and red colours. Accordingly, the goal is to develop a solution which can measure the operational state using the input from a video camera capturing a factory floor. Using methods commonly employed for traffic light recognition in autonomous cars, a system with an accuracy of over 99% in the specified conditions is presented. It is believed that if more diverse video data becomes available, a system with high reliability that generalizes well could be developed using a similar methodology.



### Simulating Anisoplanatic Turbulence by Sampling Inter-modal and Spatially Correlated Zernike Coefficients
- **Arxiv ID**: http://arxiv.org/abs/2004.11210v2
- **DOI**: None
- **Categories**: **physics.optics**, astro-ph.IM, cs.CV, eess.IV, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2004.11210v2)
- **Published**: 2020-04-23 15:05:39+00:00
- **Updated**: 2020-06-23 03:03:08+00:00
- **Authors**: Nicholas Chimitt, Stanley H. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Simulating atmospheric turbulence is an essential task for evaluating turbulence mitigation algorithms and training learning-based methods. Advanced numerical simulators for atmospheric turbulence are available, but they require evaluating wave propagation which is computationally expensive. In this paper, we present a propagation-free method for simulating imaging through turbulence. The key idea behind our work is a new method to draw inter-modal and spatially correlated Zernike coefficients. By establishing the equivalence between the angle-of-arrival correlation by Basu, McCrae and Fiorino (2015) and the multi-aperture correlation by Chanan (1992), we show that the Zernike coefficients can be drawn according to a covariance matrix defining the correlations. We propose fast and scalable sampling strategies to draw these samples. The new method allows us to compress the wave propagation problem into a sampling problem, hence making the new simulator significantly faster than existing ones. Experimental results show that the simulator has an excellent match with the theory and real turbulence data.



### Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2004.11273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11273v1)
- **Published**: 2020-04-23 16:01:13+00:00
- **Updated**: 2020-04-23 16:01:13+00:00
- **Authors**: Jianhe Yuan, Zhihai He
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under powerful white-box attacks. In this paper, we develop a new method called ensemble generative cleaning with feedback loops (EGC-FL) for effective defense of deep neural networks. The proposed EGC-FL method is based on two central ideas. First, we introduce a transformed deadzone layer into the defense network, which consists of an orthonormal transform and a deadzone-based activation function, to destroy the sophisticated noise pattern of adversarial attacks. Second, by constructing a generative cleaning network with a feedback loop, we are able to generate an ensemble of diverse estimations of the original clean image. We then learn a network to fuse this set of diverse estimations together to restore the original image. Our extensive experimental results demonstrate that our approach improves the state-of-art by large margins in both white-box and black-box attacks. It significantly improves the classification accuracy for white-box PGD attacks upon the second best method by more than 29% on the SVHN dataset and more than 39% on the challenging CIFAR-10 dataset.



### Edge Detection using Stationary Wavelet Transform, HMM, and EM algorithm
- **Arxiv ID**: http://arxiv.org/abs/2004.11296v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11296v1)
- **Published**: 2020-04-23 16:27:26+00:00
- **Updated**: 2020-04-23 16:27:26+00:00
- **Authors**: S. Anand, K. Nagajothi, K. Nithya
- **Comment**: 07 pages, 5 figures
- **Journal**: None
- **Summary**: Stationary Wavelet Transform (SWT) is an efficient tool for edge analysis. This paper a new edge detection technique using SWT based Hidden Markov Model (WHMM) along with the expectation-maximization (EM) algorithm is proposed. The SWT coefficients contain a hidden state and they indicate the SWT coefficient fits into an edge model or not. Laplacian and Gaussian model is used to check the information of the state is an edge or no edge. This model is trained by an EM algorithm and the Viterbi algorithm is employed to recover the state. This algorithm can be applied to noisy images efficiently.



### CoInGP: Convolutional Inpainting with Genetic Programming
- **Arxiv ID**: http://arxiv.org/abs/2004.11300v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11300v2)
- **Published**: 2020-04-23 16:31:58+00:00
- **Updated**: 2021-04-25 10:23:28+00:00
- **Authors**: Domagoj Jakobovic, Luca Manzoni, Luca Mariot, Stjepan Picek, Mauro Castelli
- **Comment**: 21 pages, 8 figures, updated pre-print accepted at GECCO 2021
- **Journal**: None
- **Summary**: We investigate the use of Genetic Programming (GP) as a convolutional predictor for missing pixels in images. The training phase is performed by sweeping a sliding window over an image, where the pixels on the border represent the inputs of a GP tree. The output of the tree is taken as the predicted value for the central pixel. We consider two topologies for the sliding window, namely the Moore and the Von Neumann neighborhood. The best GP tree scoring the lowest prediction error over the training set is then used to predict the pixels in the test set. We experimentally assess our approach through two experiments. In the first one, we train a GP tree over a subset of 1000 complete images from the MNIST dataset. The results show that GP can learn the distribution of the pixels with respect to a simple baseline predictor, with no significant differences observed between the two neighborhoods. In the second experiment, we train a GP convolutional predictor on two degraded images, removing around 20% of their pixels. In this case, we observe that the Moore neighborhood works better, although the Von Neumann neighborhood allows for a larger training set.



### Self-supervised Learning for Astronomical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.11336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11336v2)
- **Published**: 2020-04-23 17:32:19+00:00
- **Updated**: 2020-06-25 13:49:19+00:00
- **Authors**: Ana Martinazzo, Mateus Espadoto, Nina S. T. Hirata
- **Comment**: Accepted for ICPR 2020
- **Journal**: None
- **Summary**: In Astronomy, a huge amount of image data is generated daily by photometric surveys, which scan the sky to collect data from stars, galaxies and other celestial objects. In this paper, we propose a technique to leverage unlabeled astronomical images to pre-train deep convolutional neural networks, in order to learn a domain-specific feature extractor which improves the results of machine learning techniques in setups with small amounts of labeled data available. We show that our technique produces results which are in many cases better than using ImageNet pre-training.



### Supervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11362v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11362v5)
- **Published**: 2020-04-23 17:58:56+00:00
- **Updated**: 2021-03-10 19:11:45+00:00
- **Authors**: Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.



### Single-View View Synthesis with Multiplane Images
- **Arxiv ID**: http://arxiv.org/abs/2004.11364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.11364v1)
- **Published**: 2020-04-23 17:59:19+00:00
- **Updated**: 2020-04-23 17:59:19+00:00
- **Authors**: Richard Tucker, Noah Snavely
- **Comment**: None
- **Journal**: None
- **Summary**: A recent strand of work in view synthesis uses deep learning to generate multiplane images (a camera-centric, layered 3D representation) given two or more input images at known viewpoints. We apply this representation to single-view view synthesis, a problem which is more challenging but has potentially much wider application. Our method learns to predict a multiplane image directly from a single image input, and we introduce scale-invariant view synthesis for supervision, enabling us to train on online video. We show this approach is applicable to several different datasets, that it additionally generates reasonable depth maps, and that it learns to fill in content behind the edges of foreground objects in background layers.   Project page at https://single-view-mpi.github.io/.



### Style Your Face Morph and Improve Your Face Morphing Attack Detector
- **Arxiv ID**: http://arxiv.org/abs/2004.11435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11435v1)
- **Published**: 2020-04-23 19:29:07+00:00
- **Updated**: 2020-04-23 19:29:07+00:00
- **Authors**: Clemens Seibold, Anna Hilsmann, Peter Eisert
- **Comment**: Published at BIOSIG 2019
- **Journal**: None
- **Summary**: A morphed face image is a synthetically created image that looks so similar to the faces of two subjects that both can use it for verification against a biometric verification system. It can be easily created by aligning and blending face images of the two subjects. In this paper, we propose a style transfer based method that improves the quality of morphed face images. It counters the image degeneration during the creation of morphed face images caused by blending. We analyze different state of the art face morphing attack detection systems regarding their performance against our improved morphed face images and other methods that improve the image quality. All detection systems perform significantly worse, when first confronted with our improved morphed face images. Most of them can be enhanced by adding our quality improved morphs to the training data, which further improves the robustness against other means of quality improvement.



### Device-based Image Matching with Similarity Learning by Convolutional Neural Networks that Exploit the Underlying Camera Sensor Pattern Noise
- **Arxiv ID**: http://arxiv.org/abs/2004.11443v1
- **DOI**: 10.5220/0009155505780584
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11443v1)
- **Published**: 2020-04-23 20:03:40+00:00
- **Updated**: 2020-04-23 20:03:40+00:00
- **Authors**: Guru Swaroop Bennabhaktula, Enrique Alegre, Dimka Karastoyanova, George Azzopardi
- **Comment**: 7 pages, 4 figures, conference paper
- **Journal**: In Proceedings of the 9th International Conference on Pattern
  Recognition Applications and Methods - Volume 1: ICPRAM, 578-584, 2020
- **Summary**: One of the challenging problems in digital image forensics is the capability to identify images that are captured by the same camera device. This knowledge can help forensic experts in gathering intelligence about suspects by analyzing digital images. In this paper, we propose a two-part network to quantify the likelihood that a given pair of images have the same source camera, and we evaluated it on the benchmark Dresden data set containing 1851 images from 31 different cameras. To the best of our knowledge, we are the first ones addressing the challenge of device-based image matching. Though the proposed approach is not yet forensics ready, our experiments show that this direction is worth pursuing, achieving at this moment 85 percent accuracy. This ongoing work is part of the EU-funded project 4NSEEK concerned with forensics against child sexual abuse.



### Upgrading the Newsroom: An Automated Image Selection System for News Articles
- **Arxiv ID**: http://arxiv.org/abs/2004.11449v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11449v1)
- **Published**: 2020-04-23 20:29:26+00:00
- **Updated**: 2020-04-23 20:29:26+00:00
- **Authors**: Fangyu Liu, Rémi Lebret, Didier Orel, Philippe Sordet, Karl Aberer
- **Comment**: Accepted to ACM Transactions on Multimedia Computing Communications
  and Applications (ACM TOMM)
- **Journal**: None
- **Summary**: We propose an automated image selection system to assist photo editors in selecting suitable images for news articles. The system fuses multiple textual sources extracted from news articles and accepts multilingual inputs. It is equipped with char-level word embeddings to help both modeling morphologically rich languages, e.g. German, and transferring knowledge across nearby languages. The text encoder adopts a hierarchical self-attention mechanism to attend more to both keywords within a piece of text and informative components of a news article. We extensively experiment with our system on a large-scale text-image database containing multimodal multilingual news articles collected from Swiss local news media websites. The system is compared with multiple baselines with ablation studies and is shown to beat existing text-image retrieval methods in a weakly-supervised learning setting. Besides, we also offer insights on the advantage of using multiple textual sources and multilingual data.



### Debiasing Skin Lesion Datasets and Models? Not So Fast
- **Arxiv ID**: http://arxiv.org/abs/2004.11457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11457v1)
- **Published**: 2020-04-23 21:07:49+00:00
- **Updated**: 2020-04-23 21:07:49+00:00
- **Authors**: Alceu Bissoto, Eduardo Valle, Sandra Avila
- **Comment**: Accepted to the ISIC Skin Image Analysis Workshop @ CVPR 2020
- **Journal**: None
- **Summary**: Data-driven models are now deployed in a plethora of real-world applications - including automated diagnosis - but models learned from data risk learning biases from that same data. When models learn spurious correlations not found in real-world situations, their deployment for critical tasks, such as medical decisions, can be catastrophic. In this work we address this issue for skin-lesion classification models, with two objectives: finding out what are the spurious correlations exploited by biased networks, and debiasing the models by removing such spurious correlations from them. We perform a systematic integrated analysis of 7 visual artifacts (which are possible sources of biases exploitable by networks), employ a state-of-the-art technique to prevent the models from learning spurious correlations, and propose datasets to test models for the presence of bias. We find out that, despite interesting results that point to promising future research, current debiasing methods are not ready to solve the bias issue for skin-lesion models.



### Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.11475v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11475v2)
- **Published**: 2020-04-23 22:20:10+00:00
- **Updated**: 2020-05-19 17:45:25+00:00
- **Authors**: Mamshad Nayeem Rizve, Ugur Demir, Praveen Tirupattur, Aayush Jung Rana, Kevin Duarte, Ishan Dave, Yogesh Singh Rawat, Mubarak Shah
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Activity detection in security videos is a difficult problem due to multiple factors such as large field of view, presence of multiple activities, varying scales and viewpoints, and its untrimmed nature. The existing research in activity detection is mainly focused on datasets, such as UCF-101, JHMDB, THUMOS, and AVA, which partially address these issues. The requirement of processing the security videos in real-time makes this even more challenging. In this work we propose Gabriella, a real-time online system to perform activity detection on untrimmed security videos. The proposed method consists of three stages: tubelet extraction, activity classification, and online tubelet merging. For tubelet extraction, we propose a localization network which takes a video clip as input and spatio-temporally detects potential foreground regions at multiple scales to generate action tubelets. We propose a novel Patch-Dice loss to handle large variations in actor size. Our online processing of videos at a clip level drastically reduces the computation time in detecting activities. The detected tubelets are assigned activity class scores by the classification network and merged together using our proposed Tubelet-Merge Action-Split (TMAS) algorithm to form the final action detections. The TMAS algorithm efficiently connects the tubelets in an online fashion to generate action detections which are robust against varying length activities. We perform our experiments on the VIRAT and MEVA (Multiview Extended Video with Activities) datasets and demonstrate the effectiveness of the proposed approach in terms of speed (~100 fps) and performance with state-of-the-art results. The code and models will be made publicly available.



### Pill Identification using a Mobile Phone App for Assessing Medication Adherence and Post-Market Drug Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2004.11479v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2004.11479v1)
- **Published**: 2020-04-23 22:24:01+00:00
- **Updated**: 2020-04-23 22:24:01+00:00
- **Authors**: david Prokop, Joseph Babigumira, Ashleigh Lewis
- **Comment**: 12 pages, 1 photo, 6 tables, 3 charts, 1 figure
- **Journal**: None
- **Summary**: Objectives: Medication non-adherence is an important factor in clinical practice and research methodology. There have been many methods of measuring adherence yet no recognized standard for adherence. Here we conduct a software study of the usefulness and efficacy of a mobile phone app to measure medication adherence using photographs taken by a phone app of medications and self-reported health measures.   Results: The participants were asked by the app 'would help to keep track of your medication', their response indicated 92.9% felt the app 'would you use this app every day' to improve their medication adherence. The subjects were also asked by the app if they 'would photograph their pills on a daily basis'. Subject responses indicated 63% would use the app on a daily basis. By using the data collected, we determined that subjects who used the app on daily basis were more likely to adhere to the prescribed regimen.   Conclusions: Pill photographs are a useful measure of adherence, allowing more accurate time measures and more frequent adherence assessment. Given the ubiquity of mobile telephone use, and the relative ease of this adherence measurement method, we believe it is a useful and cost-effective approach. However we feel the 'manual' nature of using the phone for taking a photograph of a pill has individual variability and an 'automatic' method is needed to reduce data inconsistency.



### Roof material classification from aerial imagery
- **Arxiv ID**: http://arxiv.org/abs/2004.11482v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11482v1)
- **Published**: 2020-04-23 22:47:50+00:00
- **Updated**: 2020-04-23 22:47:50+00:00
- **Authors**: Roman Solovyev
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes an algorithm for classification of roof materials using aerial photographs. Main advantages of the algorithm are proposed methods to improve prediction accuracy. Proposed methods includes: method of converting ImageNet weights of neural networks for using multi-channel images; special set of features of second level models that are used in addition to specific predictions of neural networks; special set of image augmentations that improve training accuracy. In addition, complete flow for solving this problem is proposed. The following content is available in open access: solution code, weight sets and architecture of the used neural networks. The proposed solution achieved second place in the competition "Open AI Caribbean Challenge".



### Adversarial Attacks and Defenses: An Interpretation Perspective
- **Arxiv ID**: http://arxiv.org/abs/2004.11488v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11488v2)
- **Published**: 2020-04-23 23:19:00+00:00
- **Updated**: 2020-10-07 15:43:26+00:00
- **Authors**: Ninghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, Xia Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent advances in a wide spectrum of applications, machine learning models, especially deep neural networks, have been shown to be vulnerable to adversarial attacks. Attackers add carefully-crafted perturbations to input, where the perturbations are almost imperceptible to humans, but can cause models to make wrong predictions. Techniques to protect models against adversarial input are called adversarial defense methods. Although many approaches have been proposed to study adversarial attacks and defenses in different scenarios, an intriguing and crucial challenge remains that how to really understand model vulnerability? Inspired by the saying that "if you know yourself and your enemy, you need not fear the battles", we may tackle the aforementioned challenge after interpreting machine learning models to open the black-boxes. The goal of model interpretation, or interpretable machine learning, is to extract human-understandable terms for the working mechanism of models. Recently, some approaches start incorporating interpretation into the exploration of adversarial attacks and defenses. Meanwhile, we also observe that many existing methods of adversarial attacks and defenses, although not explicitly claimed, can be understood from the perspective of interpretation. In this paper, we review recent work on adversarial attacks and defenses, particularly from the perspective of machine learning interpretation. We categorize interpretation into two types, feature-level interpretation and model-level interpretation. For each type of interpretation, we elaborate on how it could be used for adversarial attacks and defenses. We then briefly illustrate additional correlations between interpretation and adversaries. Finally, we discuss the challenges and future directions along tackling adversary issues with interpretation.




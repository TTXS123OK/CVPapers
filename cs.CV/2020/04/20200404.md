# Arxiv Papers in cs.CV on 2020-04-04
### Group Based Deep Shared Feature Learning for Fine-grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.01817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01817v1)
- **Published**: 2020-04-04 00:01:11+00:00
- **Updated**: 2020-04-04 00:01:11+00:00
- **Authors**: Xuelu Li, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained image classification has emerged as a significant challenge because objects in such images have small inter-class visual differences but with large variations in pose, lighting, and viewpoints, etc. Most existing work focuses on highly customized feature extraction via deep network architectures which have been shown to deliver state of the art performance. Given that images from distinct classes in fine-grained classification share significant features of interest, we present a new deep network architecture that explicitly models shared features and removes their effect to achieve enhanced classification results. Our modeling of shared features is based on a new group based learning wherein existing classes are divided into groups and multiple shared feature patterns are discovered (learned). We call this framework Group based deep Shared Feature Learning (GSFL) and the resulting learned network as GSFL-Net. Specifically, the proposed GSFL-Net develops a specially designed autoencoder which is constrained by a newly proposed Feature Expression Loss to decompose a set of features into their constituent shared and discriminative components. During inference, only the discriminative feature component is used to accomplish the classification task. A key benefit of our specialized autoencoder is that it is versatile and can be combined with state-of-the-art fine-grained feature extraction models and trained together with them to improve their performance directly. Experiments on benchmark datasets show that GSFL-Net can enhance classification accuracy over the state of the art with a more interpretable architecture.



### Temporal Shift GAN for Large Scale Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.01823v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.01823v2)
- **Published**: 2020-04-04 00:40:52+00:00
- **Updated**: 2020-11-10 19:46:08+00:00
- **Authors**: Andres Munoz, Mohammadreza Zolfaghari, Max Argus, Thomas Brox
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Video generation models have become increasingly popular in the last few years, however the standard 2D architectures used today lack natural spatio-temporal modelling capabilities. In this paper, we present a network architecture for video generation that models spatio-temporal consistency without resorting to costly 3D architectures. The architecture facilitates information exchange between neighboring time points, which improves the temporal consistency of both the high level structure as well as the low-level details of the generated frames. The approach achieves state-of-the-art quantitative performance, as measured by the inception score on the UCF-101 dataset as well as better qualitative results. We also introduce a new quantitative measure (S3) that uses downstream tasks for evaluation. Moreover, we present a new multi-label dataset MaisToy, which enables us to evaluate the generalization of the model.



### Pixel Consensus Voting for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01849v1)
- **Published**: 2020-04-04 04:33:45+00:00
- **Updated**: 2020-04-04 04:33:45+00:00
- **Authors**: Haochen Wang, Ruotian Luo, Michael Maire, Greg Shakhnarovich
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: The core of our approach, Pixel Consensus Voting, is a framework for instance segmentation based on the Generalized Hough transform. Pixels cast discretized, probabilistic votes for the likely regions that contain instance centroids. At the detected peaks that emerge in the voting heatmap, backprojection is applied to collect pixels and produce instance masks. Unlike a sliding window detector that densely enumerates object proposals, our method detects instances as a result of the consensus among pixel-wise votes. We implement vote aggregation and backprojection using native operators of a convolutional neural network. The discretization of centroid voting reduces the training of instance segmentation to pixel labeling, analogous and complementary to FCN-style semantic segmentation, leading to an efficient and unified architecture that jointly models things and stuff. We demonstrate the effectiveness of our pipeline on COCO and Cityscapes Panoptic Segmentation and obtain competitive results. Code will be open-sourced.



### Weighted Fisher Discriminant Analysis in the Input and Feature Spaces
- **Arxiv ID**: http://arxiv.org/abs/2004.01857v1
- **DOI**: 10.1007/978-3-030-50516-5_1
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01857v1)
- **Published**: 2020-04-04 05:17:53+00:00
- **Updated**: 2020-04-04 05:17:53+00:00
- **Authors**: Benyamin Ghojogh, Milad Sikaroudi, H. R. Tizhoosh, Fakhri Karray, Mark Crowley
- **Comment**: Accepted (to appear) in International Conference on Image Analysis
  and Recognition (ICIAR) 2020, Springer
- **Journal**: International Conference on Image Analysis and Recognition, vol 2,
  pp. 3-15. Springer, Cham, 2020
- **Summary**: Fisher Discriminant Analysis (FDA) is a subspace learning method which minimizes and maximizes the intra- and inter-class scatters of data, respectively. Although, in FDA, all the pairs of classes are treated the same way, some classes are closer than the others. Weighted FDA assigns weights to the pairs of classes to address this shortcoming of FDA. In this paper, we propose a cosine-weighted FDA as well as an automatically weighted FDA in which weights are found automatically. We also propose a weighted FDA in the feature space to establish a weighted kernel FDA for both existing and newly proposed weights. Our experiments on the ORL face recognition dataset show the effectiveness of the proposed weighting schemes.



### Deblurring by Realistic Blurring
- **Arxiv ID**: http://arxiv.org/abs/2004.01860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01860v2)
- **Published**: 2020-04-04 05:25:15+00:00
- **Updated**: 2020-05-07 03:34:49+00:00
- **Authors**: Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, Hongdong Li
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Existing deep learning methods for image deblurring typically train models using pairs of sharp images and their blurred counterparts. However, synthetically blurring images do not necessarily model the genuine blurring process in real-world scenarios with sufficient accuracy. To address this problem, we propose a new method which combines two GAN models, i.e., a learning-to-Blur GAN (BGAN) and learning-to-DeBlur GAN (DBGAN), in order to learn a better model for image deblurring by primarily learning how to blur images. The first model, BGAN, learns how to blur sharp images with unpaired sharp and blurry image sets, and then guides the second model, DBGAN, to learn how to correctly deblur such images. In order to reduce the discrepancy between real blur and synthesized blur, a relativistic blur loss is leveraged. As an additional contribution, this paper also introduces a Real-World Blurred Image (RWBI) dataset including diverse blurry images. Our experiments show that the proposed method achieves consistently superior quantitative performance as well as higher perceptual quality on both the newly proposed dataset and the public GOPRO dataset.



### Theoretical Insights into the Use of Structural Similarity Index In Generative Models and Inferential Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2004.01864v1
- **DOI**: 10.1007/978-3-030-50516-5_10
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.01864v1)
- **Published**: 2020-04-04 05:39:15+00:00
- **Updated**: 2020-04-04 05:39:15+00:00
- **Authors**: Benyamin Ghojogh, Fakhri Karray, Mark Crowley
- **Comment**: Accepted (to appear) in International Conference on Image Analysis
  and Recognition (ICIAR) 2020, Springer
- **Journal**: International Conference on Image Analysis and Recognition, vol 2,
  pp. 112-117. Springer, Cham, 2020
- **Summary**: Generative models and inferential autoencoders mostly make use of $\ell_2$ norm in their optimization objectives. In order to generate perceptually better images, this short paper theoretically discusses how to use Structural Similarity Index (SSIM) in generative models and inferential autoencoders. We first review SSIM, SSIM distance metrics, and SSIM kernel. We show that the SSIM kernel is a universal kernel and thus can be used in unconditional and conditional generated moment matching networks. Then, we explain how to use SSIM distance in variational and adversarial autoencoders and unconditional and conditional Generative Adversarial Networks (GANs). Finally, we propose to use SSIM distance rather than $\ell_2$ norm in least squares GAN.



### FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.01888v6
- **DOI**: 10.1007/s11263-021-01513-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01888v6)
- **Published**: 2020-04-04 08:18:00+00:00
- **Updated**: 2021-10-19 05:37:18+00:00
- **Authors**: Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu
- **Comment**: IJCV 2021
- **Journal**: International Journal of Computer Vision, 129(11), 2021, 3069-3087
- **Summary**: Multi-object tracking (MOT) is an important problem in computer vision which has a wide range of applications. Formulating MOT as multi-task learning of object detection and re-ID in a single network is appealing since it allows joint optimization of the two tasks and enjoys high computation efficiency. However, we find that the two tasks tend to compete with each other which need to be carefully addressed. In particular, previous works usually treat re-ID as a secondary task whose accuracy is heavily affected by the primary detection task. As a result, the network is biased to the primary detection task which is not fair to the re-ID task. To solve the problem, we present a simple yet effective approach termed as FairMOT based on the anchor-free object detection architecture CenterNet. Note that it is not a naive combination of CenterNet and re-ID. Instead, we present a bunch of detailed designs which are critical to achieve good tracking results by thorough empirical studies. The resulting approach achieves high accuracy for both detection and tracking. The approach outperforms the state-of-the-art methods by a large margin on several public datasets. The source code and pre-trained models are released at https://github.com/ifzhang/FairMOT.



### Understanding (Non-)Robust Feature Disentanglement and the Relationship Between Low- and High-Dimensional Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2004.01903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01903v1)
- **Published**: 2020-04-04 10:38:23+00:00
- **Updated**: 2020-04-04 10:38:23+00:00
- **Authors**: Zuowen Wang, Leo Horne
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has put forth the hypothesis that adversarial vulnerabilities in neural networks are due to them overusing "non-robust features" inherent in the training data. We show empirically that for PGD-attacks, there is a training stage where neural networks start heavily relying on non-robust features to boost natural accuracy. We also propose a mechanism reducing vulnerability to PGD-style attacks consisting of mixing in a certain amount of images contain-ing mostly "robust features" into each training batch, and then show that robust accuracy is improved, while natural accuracy is not substantially hurt. We show that training on "robust features" provides boosts in robust accuracy across various architectures and for different attacks. Finally, we demonstrate empirically that these "robust features" do not induce spatial invariance.



### Optical Flow in Dense Foggy Scenes using Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.01905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01905v1)
- **Published**: 2020-04-04 10:44:16+00:00
- **Updated**: 2020-04-04 10:44:16+00:00
- **Authors**: Wending Yan, Aashish Sharma, Robby T. Tan
- **Comment**: None
- **Journal**: None
- **Summary**: In dense foggy scenes, existing optical flow methods are erroneous. This is due to the degradation caused by dense fog particles that break the optical flow basic assumptions such as brightness and gradient constancy. To address the problem, we introduce a semi-supervised deep learning technique that employs real fog images without optical flow ground-truths in the training process. Our network integrates the domain transformation and optical flow networks in one framework. Initially, given a pair of synthetic fog images, its corresponding clean images and optical flow ground-truths, in one training batch we train our network in a supervised manner. Subsequently, given a pair of real fog images and a pair of clean images that are not corresponding to each other (unpaired), in the next training batch, we train our network in an unsupervised manner. We then alternate the training of synthetic and real data iteratively. We use real data without ground-truths, since to have ground-truths in such conditions is intractable, and also to avoid the overfitting problem of synthetic data training, where the knowledge learned on synthetic data cannot be generalized to real data testing. Together with the network architecture design, we propose a new training strategy that combines supervised synthetic-data training and unsupervised real-data training. Experimental results show that our method is effective and outperforms the state-of-the-art methods in estimating optical flow in dense foggy scenes.



### Knife and Threat Detectors
- **Arxiv ID**: http://arxiv.org/abs/2004.03366v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03366v2)
- **Published**: 2020-04-04 12:41:28+00:00
- **Updated**: 2020-04-08 14:36:29+00:00
- **Authors**: David A. Noever, Sam E. Miller Noever
- **Comment**: None
- **Journal**: None
- **Summary**: Despite rapid advances in image-based machine learning, the threat identification of a knife wielding attacker has not garnered substantial academic attention. This relative research gap appears less understandable given the high knife assault rate (>100,000 annually) and the increasing availability of public video surveillance to analyze and forensically document. We present three complementary methods for scoring automated threat identification using multiple knife image datasets, each with the goal of narrowing down possible assault intentions while minimizing misidentifying false positives and risky false negatives. To alert an observer to the knife-wielding threat, we test and deploy classification built around MobileNet in a sparse and pruned neural network with a small memory requirement (< 2.2 megabytes) and 95% test accuracy. We secondly train a detection algorithm (MaskRCNN) to segment the hand from the knife in a single image and assign probable certainty to their relative location. This segmentation accomplishes both localization with bounding boxes but also relative positions to infer overhand threats. A final model built on the PoseNet architecture assigns anatomical waypoints or skeletal features to narrow the threat characteristics and reduce misunderstood intentions. We further identify and supplement existing data gaps that might blind a deployed knife threat detector such as collecting innocuous hand and fist images as important negative training sets. When automated on commodity hardware and software solutions one original research contribution is this systematic survey of timely and readily available image-based alerts to task and prioritize crime prevention countermeasures prior to a tragic outcome.



### Empirical Evaluation of PRNU Fingerprint Variation for Mismatched Imaging Pipelines
- **Arxiv ID**: http://arxiv.org/abs/2004.01929v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01929v2)
- **Published**: 2020-04-04 13:09:50+00:00
- **Updated**: 2020-10-09 18:29:16+00:00
- **Authors**: Sharad Joshi, Pawel Korus, Nitin Khanna, Nasir Memon
- **Comment**: 6 pages and 3 pages supplemental file
- **Journal**: None
- **Summary**: We assess the variability of PRNU-based camera fingerprints with mismatched imaging pipelines (e.g., different camera ISP or digital darkroom software). We show that camera fingerprints exhibit non-negligible variations in this setup, which may lead to unexpected degradation of detection statistics in real-world use-cases. We tested 13 different pipelines, including standard digital darkroom software and recent neural-networks. We observed that correlation between fingerprints from mismatched pipelines drops on average to 0.38 and the PCE detection statistic drops by over 40%. The degradation in error rates is the strongest for small patches commonly used in photo manipulation detection, and when neural networks are used for photo development. At a fixed 0.5% FPR setting, the TPR drops by 17 ppt (percentage points) for 128 px and 256 px patches.



### Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2004.01946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01946v1)
- **Published**: 2020-04-04 14:35:37+00:00
- **Updated**: 2020-04-04 14:35:37+00:00
- **Authors**: Dominik Kulon, Riza Alp GÃ¼ler, Iasonas Kokkinos, Michael Bronstein, Stefanos Zafeiriou
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2020). Additional resources: https://arielai.com/mesh_hands
- **Journal**: None
- **Summary**: We introduce a simple and effective network architecture for monocular 3D hand pose estimation consisting of an image encoder followed by a mesh convolutional decoder that is trained through a direct 3D hand mesh reconstruction loss. We train our network by gathering a large-scale dataset of hand action in YouTube videos and use it as a source of weak supervision. Our weakly-supervised mesh convolutions-based system largely outperforms state-of-the-art methods, even halving the errors on the in the wild benchmark. The dataset and additional resources are available at https://arielai.com/mesh_hands.



### Cross-domain Face Presentation Attack Detection via Multi-domain Disentangled Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.01959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01959v1)
- **Published**: 2020-04-04 15:45:14+00:00
- **Updated**: 2020-04-04 15:45:14+00:00
- **Authors**: Guoqing Wang, Hu Han, Shiguang Shan, Xilin Chen
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Face presentation attack detection (PAD) has been an urgent problem to be solved in the face recognition systems. Conventional approaches usually assume the testing and training are within the same domain; as a result, they may not generalize well into unseen scenarios because the representations learned for PAD may overfit to the subjects in the training set. In light of this, we propose an efficient disentangled representation learning for cross-domain face PAD. Our approach consists of disentangled representation learning (DR-Net) and multi-domain learning (MD-Net). DR-Net learns a pair of encoders via generative models that can disentangle PAD informative features from subject discriminative features. The disentangled features from different domains are fed to MD-Net which learns domain-independent features for the final cross-domain face PAD task. Extensive experiments on several public datasets validate the effectiveness of the proposed approach for cross-domain PAD.



### Neural Architecture Search for Lightweight Non-Local Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.01961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01961v1)
- **Published**: 2020-04-04 15:46:39+00:00
- **Updated**: 2020-04-04 15:46:39+00:00
- **Authors**: Yingwei Li, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie Yang, Cihang Xie, Qihang Yu, Yuyin Zhou, Song Bai, Alan Yuille
- **Comment**: CVPR 2020. Project page: https://github.com/LiYingwei/AutoNL
- **Journal**: None
- **Summary**: Non-Local (NL) blocks have been widely studied in various vision tasks. However, it has been rarely explored to embed the NL blocks in mobile neural networks, mainly due to the following challenges: 1) NL blocks generally have heavy computation cost which makes it difficult to be applied in applications where computational resources are limited, and 2) it is an open problem to discover an optimal configuration to embed NL blocks into mobile neural networks. We propose AutoNL to overcome the above two obstacles. Firstly, we propose a Lightweight Non-Local (LightNL) block by squeezing the transformation operations and incorporating compact features. With the novel design choices, the proposed LightNL block is 400x computationally cheaper} than its conventional counterpart without sacrificing the performance. Secondly, by relaxing the structure of the LightNL block to be differentiable during training, we propose an efficient neural architecture search algorithm to learn an optimal configuration of LightNL blocks in an end-to-end manner. Notably, using only 32 GPU hours, the searched AutoNL model achieves 77.7% top-1 accuracy on ImageNet under a typical mobile setting (350M FLOPs), significantly outperforming previous mobile models including MobileNetV2 (+5.7%), FBNet (+2.8%) and MnasNet (+2.1%). Code and models are available at https://github.com/LiYingwei/AutoNL.



### Fine grained classification for multi-source land cover mapping
- **Arxiv ID**: http://arxiv.org/abs/2004.01963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01963v1)
- **Published**: 2020-04-04 15:49:37+00:00
- **Updated**: 2020-04-04 15:49:37+00:00
- **Authors**: Yawogan Jean Eudes Gbodjo, Dino Ienco, Louise Leroux, Roberto Interdonato, Raffaelle Gaetano
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Nowadays, there is a general agreement on the need to better characterize agricultural monitoring systems in response to the global changes. Timely and accurate land use/land cover mapping can support this vision by providing useful information at fine scale. Here, a deep learning approach is proposed to deal with multi-source land cover mapping at object level. The approach is based on an extension of Recurrent Neural Network enriched via an attention mechanism dedicated to multi-temporal data context. Moreover, a new hierarchical pretraining strategy designed to exploit specific domain knowledge available under hierarchical relationships within land cover classes is introduced. Experiments carried out on the Reunion island - a french overseas department - demonstrate the significance of the proposal compared to remote sensing standard approaches for land cover mapping.



### Open Domain Dialogue Generation with Latent Images
- **Arxiv ID**: http://arxiv.org/abs/2004.01981v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01981v2)
- **Published**: 2020-04-04 17:32:46+00:00
- **Updated**: 2021-06-01 07:43:08+00:00
- **Authors**: Ze Yang, Wei Wu, Huang Hu, Can Xu, Wei Wang, Zhoujun Li
- **Comment**: AAAI2021
- **Journal**: None
- **Summary**: We consider grounding open domain dialogues with images. Existing work assumes that both an image and a textual context are available, but image-grounded dialogues by nature are more difficult to obtain than textual dialogues. Thus, we propose learning a response generation model with both image-grounded dialogues and textual dialogues by assuming that the visual scene information at the time of a conversation can be represented by an image, and trying to recover the latent images of the textual dialogues through text-to-image generation techniques. The likelihood of the two types of dialogues is then formulated by a response generator and an image reconstructor that are learned within a conditional variational auto-encoding framework. Empirical studies are conducted in both image-grounded conversation and text-based conversation. In the first scenario, image-grounded dialogues, especially under a low-resource setting, can be effectively augmented by textual dialogues with latent images; while in the second scenario, latent images can enrich the content of responses and at the same time keep them relevant to contexts.



### A Machine Learning Based Framework for the Smart Healthcare Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2004.03360v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03360v1)
- **Published**: 2020-04-04 17:41:28+00:00
- **Updated**: 2020-04-04 17:41:28+00:00
- **Authors**: Abrar Zahin, Le Thanh Tan, Rose Qingyang Hu
- **Comment**: None
- **Journal**: 2020 Intermountain Engineering, Technology and Computing (IETC)
- **Summary**: In this paper, we propose a novel framework for the smart healthcare system, where we employ the compressed sensing (CS) and the combination of the state-of-the-art machine learning based denoiser as well as the alternating direction of method of multipliers (ADMM) structure. This integration significantly simplifies the software implementation for the lowcomplexity encoder, thanks to the modular structure of ADMM. Furthermore, we focus on detecting fall down actions from image streams. Thus, teh primary purpose of thus study is to reconstruct the image as visibly clear as possible and hence it helps the detection step at the trained classifier. For this efficient smart health monitoring framework, we employ the trained binary convolutional neural network (CNN) classifier for the fall-action classifier, because this scheme is a part of surveillance scenario. In this scenario, we deal with the fallimages, thus, we compress, transmit and reconstruct the fallimages. Experimental results demonstrate the impacts of network parameters and the significant performance gain of the proposal compared to traditional methods.



### Volumetric Attention for 3D Medical Image Segmentation and Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.01997v1
- **DOI**: 10.1007/978-3-030-32226-7_20
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01997v1)
- **Published**: 2020-04-04 18:55:06+00:00
- **Updated**: 2020-04-04 18:55:06+00:00
- **Authors**: Xudong Wang, Shizhong Han, Yunqiang Chen, Dashan Gao, Nuno Vasconcelos
- **Comment**: Accepted by MICCAI 2019
- **Journal**: In International Conference on Medical Image Computing and
  Computer-Assisted Intervention, pp. 175-184. Springer, Cham, 2019
- **Summary**: A volumetric attention(VA) module for 3D medical image segmentation and detection is proposed. VA attention is inspired by recent advances in video processing, enables 2.5D networks to leverage context information along the z direction, and allows the use of pretrained 2D detection models when training data is limited, as is often the case for medical applications. Its integration in the Mask R-CNN is shown to enable state-of-the-art performance on the Liver Tumor Segmentation (LiTS) Challenge, outperforming the previous challenge winner by 3.9 points and achieving top performance on the LiTS leader board at the time of paper submission. Detection experiments on the DeepLesion dataset also show that the addition of VA to existing object detectors enables a 69.1 sensitivity at 0.5 false positive per image, outperforming the best published results by 6.6 points.



### Attention-Guided Version of 2D UNet for Automatic Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.02009v1
- **DOI**: 10.1109/ICCKE48569.2019.8964956
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02009v1)
- **Published**: 2020-04-04 20:09:06+00:00
- **Updated**: 2020-04-04 20:09:06+00:00
- **Authors**: Mehrdad Noori, Ali Bahri, Karim Mohammadi
- **Comment**: 7 pages, 5 figures, 4 tables, Accepted by ICCKE 2019
- **Journal**: 2019 9th International Conference on Computer and Knowledge
  Engineering (ICCKE), Mashhad, Iran, 2019, pp. 269-275
- **Summary**: Gliomas are the most common and aggressive among brain tumors, which cause a short life expectancy in their highest grade. Therefore, treatment assessment is a key stage to enhance the quality of the patients' lives. Recently, deep convolutional neural networks (DCNNs) have achieved a remarkable performance in brain tumor segmentation, but this task is still difficult owing to high varying intensity and appearance of gliomas. Most of the existing methods, especially UNet-based networks, integrate low-level and high-level features in a naive way, which may result in confusion for the model. Moreover, most approaches employ 3D architectures to benefit from 3D contextual information of input images. These architectures contain more parameters and computational complexity than 2D architectures. On the other hand, using 2D models causes not to benefit from 3D contextual information of input images. In order to address the mentioned issues, we design a low-parameter network based on 2D UNet in which we employ two techniques. The first technique is an attention mechanism, which is adopted after concatenation of low-level and high-level features. This technique prevents confusion for the model by weighting each of the channels adaptively. The second technique is the Multi-View Fusion. By adopting this technique, we can benefit from 3D contextual information of input images despite using a 2D model. Experimental results demonstrate that our method performs favorably against 2017 and 2018 state-of-the-art methods.



### Convolutional Neural Networks based automated segmentation and labelling of the lumbar spine X-ray
- **Arxiv ID**: http://arxiv.org/abs/2004.03364v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03364v1)
- **Published**: 2020-04-04 20:15:03+00:00
- **Updated**: 2020-04-04 20:15:03+00:00
- **Authors**: Sandor Konya, Sai Natarajan T R, Hassan Allouch, Kais Abu Nahleh, Omneya Yakout Dogheim, Heinrich Boehm
- **Comment**: Submitted to Medical & Biological Engineering & Computing
- **Journal**: None
- **Summary**: The aim of this study is to investigate the segmentation accuracies of different segmentation networks trained on 730 manually annotated lateral lumbar spine X-rays. Instance segmentation networks were compared to semantic segmentation networks. The study cohort comprised diseased spines and postoperative images with metallic implants. The average mean accuracy and mean intersection over union (IoU) was up to 3 percent better for the best performing instance segmentation model, the average pixel accuracy and weighted IoU were slightly better for the best performing semantic segmentation model. Moreover, the inferences of the instance segmentation models are easier to implement for further processing pipelines in clinical decision support.



### Deep learning approaches in food recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03357v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03357v2)
- **Published**: 2020-04-04 20:22:16+00:00
- **Updated**: 2020-04-08 08:44:48+00:00
- **Authors**: Chairi Kiourt, George Pavlidis, Stella Markantonatou
- **Comment**: 26 pages, 10 figures, book chapter for Machine Learning Paradigms -
  Advances in Theory and Applications of Deep Learning
- **Journal**: None
- **Summary**: Automatic image-based food recognition is a particularly challenging task. Traditional image analysis approaches have achieved low classification accuracy in the past, whereas deep learning approaches enabled the identification of food types and their ingredients. The contents of food dishes are typically deformable objects, usually including complex semantics, which makes the task of defining their structure very difficult. Deep learning methods have already shown very promising results in such challenges, so this chapter focuses on the presentation of some popular approaches and techniques applied in image-based food recognition. The three main lines of solutions, namely the design from scratch, the transfer learning and the platform-based approaches, are outlined, particularly for the task at hand, and are tested and compared to reveal the inherent strengths and weaknesses. The chapter is complemented with basic background material, a section devoted to the relevant datasets that are crucial in light of the empirical approaches adopted, and some concluding remarks that underline the future directions.



### Segmentation for Classification of Screening Pancreatic Neuroendocrine Tumors
- **Arxiv ID**: http://arxiv.org/abs/2004.02021v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02021v1)
- **Published**: 2020-04-04 21:21:44+00:00
- **Updated**: 2020-04-04 21:21:44+00:00
- **Authors**: Zhuotun Zhu, Yongyi Lu, Wei Shen, Elliot K. Fishman, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents comprehensive results to detect in the early stage the pancreatic neuroendocrine tumors (PNETs), a group of endocrine tumors arising in the pancreas, which are the second common type of pancreatic cancer, by checking the abdominal CT scans. To the best of our knowledge, this task has not been studied before as a computational task. To provide radiologists with tumor locations, we adopt a segmentation framework to classify CT volumes by checking if at least a sufficient number of voxels is segmented as tumors. To quantitatively analyze our method, we collect and voxelwisely label a new abdominal CT dataset containing $376$ cases with both arterial and venous phases available for each case, in which $228$ cases were diagnosed with PNETs while the remaining $148$ cases are normal, which is currently the largest dataset for PNETs to the best of our knowledge. In order to incorporate rich knowledge of radiologists to our framework, we annotate dilated pancreatic duct as well, which is regarded as the sign of high risk for pancreatic cancer. Quantitatively, our approach outperforms state-of-the-art segmentation networks and achieves a sensitivity of $89.47\%$ at a specificity of $81.08\%$, which indicates a potential direction to achieve a clinical impact related to cancer diagnosis by earlier tumor detection.



### SimAug: Learning Robust Representations from Simulation for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.02022v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02022v3)
- **Published**: 2020-04-04 21:22:01+00:00
- **Updated**: 2020-07-17 20:43:29+00:00
- **Authors**: Junwei Liang, Lu Jiang, Alexander Hauptmann
- **Comment**: Accepted by ECCV 2020. Project website:
  https://next.cs.cmu.edu/simaug
- **Journal**: None
- **Summary**: This paper studies the problem of predicting future trajectories of people in unseen cameras of novel scenarios and views. We approach this problem through the real-data-free setting in which the model is trained only on 3D simulation data and applied out-of-the-box to a wide variety of real cameras. We propose a novel approach to learn robust representation through augmenting the simulation training data such that the representation can better generalize to unseen real-world test data. The key idea is to mix the feature of the hardest camera view with the adversarial feature of the original view. We refer to our method as SimAug. We show that SimAug achieves promising results on three real-world benchmarks using zero real training data, and state-of-the-art performance in the Stanford Drone and the VIRAT/ActEV dataset when using in-domain training data.



### It Is Not the Journey but the Destination: Endpoint Conditioned Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.02025v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02025v3)
- **Published**: 2020-04-04 21:27:13+00:00
- **Updated**: 2020-07-18 21:33:55+00:00
- **Authors**: Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal, Kuan-Hui Lee, Ehsan Adeli, Jitendra Malik, Adrien Gaidon
- **Comment**: Accepted at ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: Human trajectory forecasting with multiple socially interacting agents is of critical importance for autonomous navigation in human environments, e.g., for self-driving cars and social robots. In this work, we present Predicted Endpoint Conditioned Network (PECNet) for flexible human trajectory prediction. PECNet infers distant trajectory endpoints to assist in long-range multi-modal trajectory prediction. A novel non-local social pooling layer enables PECNet to infer diverse yet socially compliant trajectories. Additionally, we present a simple "truncation-trick" for improving few-shot multi-modal trajectory prediction performance. We show that PECNet improves state-of-the-art performance on the Stanford Drone trajectory prediction benchmark by ~20.9% and on the ETH/UCY benchmark by ~40.8%. Project homepage: https://karttikeya.github.io/publication/htf/



### Generating Rationales in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2004.02032v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02032v1)
- **Published**: 2020-04-04 22:15:35+00:00
- **Updated**: 2020-04-04 22:15:35+00:00
- **Authors**: Hammad A. Ayyubi, Md. Mehrab Tanjim, Julian J. McAuley, Garrison W. Cottrell
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances in Visual QuestionAnswering (VQA), it remains a challenge todetermine how much success can be attributedto sound reasoning and comprehension ability.We seek to investigate this question by propos-ing a new task ofrationale generation. Es-sentially, we task a VQA model with generat-ing rationales for the answers it predicts. Weuse data from the Visual Commonsense Rea-soning (VCR) task, as it contains ground-truthrationales along with visual questions and an-swers. We first investigate commonsense un-derstanding in one of the leading VCR mod-els, ViLBERT, by generating rationales frompretrained weights using a state-of-the-art lan-guage model, GPT-2. Next, we seek to jointlytrain ViLBERT with GPT-2 in an end-to-endfashion with the dual task of predicting the an-swer in VQA and generating rationales. Weshow that this kind of training injects com-monsense understanding in the VQA modelthrough quantitative and qualitative evaluationmetrics



### Optimization of Image Embeddings for Few Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.02034v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.02034v1)
- **Published**: 2020-04-04 22:17:08+00:00
- **Updated**: 2020-04-04 22:17:08+00:00
- **Authors**: Arvind Srinivasan, Aprameya Bharadwaj, Manasa Sathyan, S Natarajan
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper we improve the image embeddings generated in the graph neural network solution for few shot learning. We propose alternate architectures for existing networks such as Inception-Net, U-Net, Attention U-Net, and Squeeze-Net to generate embeddings and increase the accuracy of the models. We improve the quality of embeddings created at the cost of the time taken to generate them. The proposed implementations outperform the existing state of the art methods for 1-shot and 5-shot learning on the Omniglot dataset. The experiments involved a testing set and training set which had no common classes between them. The results for 5-way and 10-way/20-way tests have been tabulated.



### FAIRS -- Soft Focus Generator and Attention for Robust Object Segmentation from Extreme Points
- **Arxiv ID**: http://arxiv.org/abs/2004.02038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.02038v1)
- **Published**: 2020-04-04 22:25:47+00:00
- **Updated**: 2020-04-04 22:25:47+00:00
- **Authors**: Ahmed H. Shahin, Prateek Munjal, Ling Shao, Shadab Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation from user inputs has been actively studied to facilitate interactive segmentation for data annotation and other applications. Recent studies have shown that extreme points can be effectively used to encode user inputs. A heat map generated from the extreme points can be appended to the RGB image and input to the model for training. In this study, we present FAIRS -- a new approach to generate object segmentation from user inputs in the form of extreme points and corrective clicks. We propose a novel approach for effectively encoding the user input from extreme points and corrective clicks, in a novel and scalable manner that allows the network to work with a variable number of clicks, including corrective clicks for output refinement. We also integrate a dual attention module with our approach to increase the efficacy of the model in preferentially attending to the objects. We demonstrate that these additions help achieve significant improvements over state-of-the-art in dense object segmentation from user inputs, on multiple large-scale datasets. Through experiments, we demonstrate our method's ability to generate high-quality training data as well as its scalability in incorporating extreme points, guiding clicks, and corrective clicks in a principled manner.



### ObjectNet Dataset: Reanalysis and Correction
- **Arxiv ID**: http://arxiv.org/abs/2004.02042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02042v1)
- **Published**: 2020-04-04 22:45:57+00:00
- **Updated**: 2020-04-04 22:45:57+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Barbu et al introduced a dataset called ObjectNet which includes objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding generalization ability of deep models, we take a second look at their findings. We highlight a major problem with their work which is applying object recognizers to the scenes containing multiple objects rather than isolated objects. The latter results in around 20-30% performance gain using our code. Compared with the results reported in the ObjectNet paper, we observe that around 10-15 % of the performance loss can be recovered, without any test time data augmentation. In accordance with Barbu et al.'s conclusions, however, we also conclude that deep models suffer drastically on this dataset. Thus, we believe that ObjectNet remains a challenging dataset for testing the generalization power of models beyond datasets on which they have been trained.




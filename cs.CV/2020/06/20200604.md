# Arxiv Papers in cs.CV on 2020-06-04
### Image Augmentations for GAN Training
- **Arxiv ID**: http://arxiv.org/abs/2006.02595v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02595v1)
- **Published**: 2020-06-04 00:16:02+00:00
- **Updated**: 2020-06-04 00:16:02+00:00
- **Authors**: Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, Han Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentations have been widely studied to improve the accuracy and robustness of classifiers. However, the potential of image augmentation in improving GAN models for image synthesis has not been thoroughly investigated in previous studies. In this work, we systematically study the effectiveness of various existing augmentation techniques for GAN training in a variety of settings. We provide insights and guidelines on how to augment images for both vanilla GANs and GANs with regularizations, improving the fidelity of the generated images substantially. Surprisingly, we find that vanilla GANs attain generation quality on par with recent state-of-the-art results if we use augmentations on both real and generated images. When this GAN training is combined with other augmentation-based regularization techniques, such as contrastive loss and consistency regularization, the augmentations further improve the quality of generated images. We provide new state-of-the-art results for conditional generation on CIFAR-10 with both consistency loss and contrastive loss as additional regularizations.



### COMET: Context-Aware IoU-Guided Network for Small Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.02597v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02597v3)
- **Published**: 2020-06-04 00:28:45+00:00
- **Updated**: 2020-09-18 15:04:26+00:00
- **Authors**: Seyed Mojtaba Marvasti-Zadeh, Javad Khaghani, Hossein Ghanei-Yakhdan, Shohreh Kasaei, Li Cheng
- **Comment**: Accepted manuscript in ACCV 2020
- **Journal**: None
- **Summary**: We consider the problem of tracking an unknown small target from aerial videos of medium to high altitudes. This is a challenging problem, which is even more pronounced in unavoidable scenarios of drastic camera motion and high density. To address this problem, we introduce a context-aware IoU-guided tracker (COMET) that exploits a multitask two-stream network and an offline reference proposal generation strategy. The proposed network fully exploits target-related information by multi-scale feature learning and attention modules. The proposed strategy introduces an efficient sampling strategy to generalize the network on the target and its parts without imposing extra computational complexity during online tracking. These strategies contribute considerably in handling significant occlusions and viewpoint changes. Empirically, COMET outperforms the state-of-the-arts in a range of aerial view datasets that focusing on tracking small objects. Specifically, COMET outperforms the celebrated ATOM tracker by an average margin of 6.2% (and 7%) in precision (and success) score on challenging benchmarks of UAVDT, VisDrone-2019, and Small-90.



### Info3D: Representation Learning on 3D Objects using Mutual Information Maximization and Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.02598v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02598v2)
- **Published**: 2020-06-04 00:30:26+00:00
- **Updated**: 2020-08-22 22:12:57+00:00
- **Authors**: Aditya Sanghi
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: A major endeavor of computer vision is to represent, understand and extract structure from 3D data. Towards this goal, unsupervised learning is a powerful and necessary tool. Most current unsupervised methods for 3D shape analysis use datasets that are aligned, require objects to be reconstructed and suffer from deteriorated performance on downstream tasks. To solve these issues, we propose to extend the InfoMax and contrastive learning principles on 3D shapes. We show that we can maximize the mutual information between 3D objects and their "chunks" to improve the representations in aligned datasets. Furthermore, we can achieve rotation invariance in SO${(3)}$ group by maximizing the mutual information between the 3D objects and their geometric transformed versions. Finally, we conduct several experiments such as clustering, transfer learning, shape retrieval, and achieve state of art results.



### Simple Unsupervised Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.02609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02609v1)
- **Published**: 2020-06-04 01:53:18+00:00
- **Updated**: 2020-06-04 01:53:18+00:00
- **Authors**: Shyamgopal Karthik, Ameya Prabhu, Vineet Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking has seen a lot of progress recently, albeit with substantial annotation costs for developing better and larger labeled datasets. In this work, we remove the need for annotated datasets by proposing an unsupervised re-identification network, thus sidestepping the labeling costs entirely, required for training. Given unlabeled videos, our proposed method (SimpleReID) first generates tracking labels using SORT and trains a ReID network to predict the generated labels using crossentropy loss. We demonstrate that SimpleReID performs substantially better than simpler alternatives, and we recover the full performance of its supervised counterpart consistently across diverse tracking frameworks. The observations are unusual because unsupervised ReID is not expected to excel in crowded scenarios with occlusions, and drastic viewpoint changes. By incorporating our unsupervised SimpleReID with CenterTrack trained on augmented still images, we establish a new state-of-the-art performance on popular datasets like MOT16/17 without using tracking supervision, beating current best (CenterTrack) by 0.2-0.3 MOTA and 4.4-4.8 IDF1 scores. We further provide evidence for limited scope for improvement in IDF1 scores beyond our unsupervised ReID in the studied settings. Our investigation suggests reconsideration towards more sophisticated, supervised, end-to-end trackers by showing promise in simpler unsupervised alternatives.



### Semi-supervised and Unsupervised Methods for Heart Sounds Classification in Restricted Data Environments
- **Arxiv ID**: http://arxiv.org/abs/2006.02610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02610v1)
- **Published**: 2020-06-04 02:07:35+00:00
- **Updated**: 2020-06-04 02:07:35+00:00
- **Authors**: Balagopal Unnikrishnan, Pranshu Ranjan Singh, Xulei Yang, Matthew Chin Heng Chua
- **Comment**: None
- **Journal**: None
- **Summary**: Automated heart sounds classification is a much-required diagnostic tool in the view of increasing incidences of heart related diseases worldwide. In this study, we conduct a comprehensive study of heart sounds classification by using various supervised, semi-supervised and unsupervised approaches on the PhysioNet/CinC 2016 Challenge dataset. Supervised approaches, including deep learning and machine learning methods, require large amounts of labelled data to train the models, which are challenging to obtain in most practical scenarios. In view of the need to reduce the labelling burden for clinical practices, where human labelling is both expensive and time-consuming, semi-supervised or even unsupervised approaches in restricted data setting are desirable. A GAN based semi-supervised method is therefore proposed, which allows the usage of unlabelled data samples to boost the learning of data distribution. It achieves a better performance in terms of AUROC over the supervised baseline when limited data samples exist. Furthermore, several unsupervised methods are explored as an alternative approach by considering the given problem as an anomaly detection scenario. In particular, the unsupervised feature extraction using 1D CNN Autoencoder coupled with one-class SVM obtains good performance without any data labelling. The potential of the proposed semi-supervised and unsupervised methods may lead to a workflow tool in the future for the creation of higher quality datasets.



### Image Completion and Extrapolation with Contextual Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/2006.02620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02620v1)
- **Published**: 2020-06-04 02:40:04+00:00
- **Updated**: 2020-06-04 02:40:04+00:00
- **Authors**: Sai Hemanth Kasaraneni, Abhishek Mishra
- **Comment**: This paper has been accepted to 2020 IEEE International Conference on
  Image Processing (ICIP 2020)
- **Journal**: None
- **Summary**: Image Completion refers to the task of filling in the missing regions of an image and Image Extrapolation refers to the task of extending an image at its boundaries while keeping it coherent. Many recent works based on GAN have shown progress in addressing these problem statements but lack adaptability for these two cases, i.e. the neural network trained for the completion of interior masked images does not generalize well for extrapolating over the boundaries and vice-versa. In this paper, we present a technique to train both completion and extrapolation networks concurrently while benefiting each other. We demonstrate our method's efficiency in completing large missing regions and we show the comparisons with the contemporary state of the art baseline.



### Robust Automatic Whole Brain Extraction on Magnetic Resonance Imaging of Brain Tumor Patients using Dense-Vnet
- **Arxiv ID**: http://arxiv.org/abs/2006.02627v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2006.02627v1)
- **Published**: 2020-06-04 03:18:43+00:00
- **Updated**: 2020-06-04 03:18:43+00:00
- **Authors**: Sara Ranjbar, Kyle W. Singleton, Lee Curtin, Cassandra R. Rickertsen, Lisa E. Paulson, Leland S. Hu, J. Ross Mitchell, Kristin R. Swanson
- **Comment**: None
- **Journal**: None
- **Summary**: Whole brain extraction, also known as skull stripping, is a process in neuroimaging in which non-brain tissue such as skull, eyeballs, skin, etc. are removed from neuroimages. Skull striping is a preliminary step in presurgical planning, cortical reconstruction, and automatic tumor segmentation. Despite a plethora of skull stripping approaches in the literature, few are sufficiently accurate for processing pathology-presenting MRIs, especially MRIs with brain tumors. In this work we propose a deep learning approach for skull striping common MRI sequences in oncology such as T1-weighted with gadolinium contrast (T1Gd) and T2-weighted fluid attenuated inversion recovery (FLAIR) in patients with brain tumors. We automatically created gray matter, white matter, and CSF probability masks using SPM12 software and merged the masks into one for a final whole-brain mask for model training. Dice agreement, sensitivity, and specificity of the model (referred herein as DeepBrain) was tested against manual brain masks. To assess data efficiency, we retrained our models using progressively fewer training data examples and calculated average dice scores on the test set for the models trained in each round. Further, we tested our model against MRI of healthy brains from the LBP40A dataset. Overall, DeepBrain yielded an average dice score of 94.5%, sensitivity of 96.4%, and specificity of 98.5% on brain tumor data. For healthy brains, model performance improved to a dice score of 96.2%, sensitivity of 96.6% and specificity of 99.2%. The data efficiency experiment showed that, for this specific task, comparable levels of accuracy could have been achieved with as few as 50 training samples. In conclusion, this study demonstrated that a deep learning model trained on minimally processed automatically-generated labels can generate more accurate brain masks on MRI of brain tumor patients within seconds.



### FastReID: A Pytorch Toolbox for General Instance Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2006.02631v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02631v4)
- **Published**: 2020-06-04 03:51:43+00:00
- **Updated**: 2020-07-15 03:33:02+00:00
- **Authors**: Lingxiao He, Xingyu Liao, Wu Liu, Xinchen Liu, Peng Cheng, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: General Instance Re-identification is a very important task in the computer vision, which can be widely used in many practical applications, such as person/vehicle re-identification, face recognition, wildlife protection, commodity tracing, and snapshop, etc.. To meet the increasing application demand for general instance re-identification, we present FastReID as a widely used software system in JD AI Research. In FastReID, highly modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art projects, including person re-id, partial re-id, cross-domain re-id and vehicle re-id, and plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most general and high-performance toolbox that supports single and multiple GPU servers, you can reproduce our project results very easily and are very welcome to use it, the code and models are available at https://github.com/JDAI-CV/fast-reid.



### M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2006.02635v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02635v4)
- **Published**: 2020-06-04 03:54:29+00:00
- **Updated**: 2021-04-01 03:43:53+00:00
- **Authors**: Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang, Jianfeng Gao, Dongdong Zhang, Nan Duan
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We present M3P, a Multitask Multilingual Multimodal Pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask pre-training. Our goal is to learn universal representations that can map objects occurred in different modalities or texts expressed in different languages into a common semantic space. In addition, to explicitly encourage fine-grained alignment between images and non-English languages, we also propose Multimodal Code-switched Training (MCT) to combine monolingual pre-training and multimodal pre-training via a code-switch strategy. Experiments are performed on the multilingual image retrieval task across two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve comparable results for English and new state-of-the-art results for non-English languages.



### The Importance of Prior Knowledge in Precise Multimodal Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.02636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02636v1)
- **Published**: 2020-06-04 03:56:11+00:00
- **Updated**: 2020-06-04 03:56:11+00:00
- **Authors**: Sergio Casas, Cole Gulino, Simon Suo, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: Roads have well defined geometries, topologies, and traffic rules. While this has been widely exploited in motion planning methods to produce maneuvers that obey the law, little work has been devoted to utilize these priors in perception and motion forecasting methods. In this paper we propose to incorporate these structured priors as a loss function. In contrast to imposing hard constraints, this approach allows the model to handle non-compliant maneuvers when those happen in the real world. Safe motion planning is the end goal, and thus a probabilistic characterization of the possible future developments of the scene is key to choose the plan with the lowest expected cost. Towards this goal, we design a framework that leverages REINFORCE to incorporate non-differentiable priors over sample trajectories from a probabilistic model, thus optimizing the whole distribution. We demonstrate the effectiveness of our approach on real-world self-driving datasets containing complex road topologies and multi-agent interactions. Our motion forecasts not only exhibit better precision and map understanding, but most importantly result in safer motion plans taken by our self-driving vehicle. We emphasize that despite the importance of this evaluation, it has been often overlooked by previous perception and motion forecasting works.



### MFPP: Morphological Fragmental Perturbation Pyramid for Black-Box Model Explanations
- **Arxiv ID**: http://arxiv.org/abs/2006.02659v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02659v3)
- **Published**: 2020-06-04 06:13:40+00:00
- **Updated**: 2020-10-14 06:59:37+00:00
- **Authors**: Qing Yang, Xia Zhu, Jong-Kae Fwu, Yun Ye, Ganmei You, Yuan Zhu
- **Comment**: Accepted by 25th International Conference on Pattern Recognition
  (ICPR2020)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have recently been applied and used in many advanced and diverse tasks, such as medical diagnosis, automatic driving, etc. Due to the lack of transparency of the deep models, DNNs are often criticized for their prediction that cannot be explainable by human. In this paper, we propose a novel Morphological Fragmental Perturbation Pyramid (MFPP) method to solve the Explainable AI problem. In particular, we focus on the black-box scheme, which can identify the input area that is responsible for the output of the DNN without having to understand the internal architecture of the DNN. In the MFPP method, we divide the input image into multi-scale fragments and randomly mask out fragments as perturbation to generate a saliency map, which indicates the significance of each pixel for the prediction result of the black box model. Compared with the existing input sampling perturbation method, the pyramid structure fragment has proved to be more effective. It can better explore the morphological information of the input image to match its semantic information, and does not need any value inside the DNN. We qualitatively and quantitatively prove that MFPP meets and exceeds the performance of state-of-the-art (SOTA) black-box interpretation method on multiple DNN models and datasets.



### Exploiting the Transferability of Deep Learning Systems Across Multi-modal Retinal Scans for Extracting Retinopathy Lesions
- **Arxiv ID**: http://arxiv.org/abs/2006.02662v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02662v2)
- **Published**: 2020-06-04 06:25:25+00:00
- **Updated**: 2020-08-14 15:48:51+00:00
- **Authors**: Taimur Hassan, Muhammad Usman Akram, Naoufel Werghi
- **Comment**: Accepted in the 20th IEEE International Conference on BioInformatics
  And BioEngineering (BIBE), 2020
- **Journal**: None
- **Summary**: Retinal lesions play a vital role in the accurate classification of retinal abnormalities. Many researchers have proposed deep lesion-aware screening systems that analyze and grade the progression of retinopathy. However, to the best of our knowledge, no literature exploits the tendency of these systems to generalize across multiple scanner specifications and multi-modal imagery. Towards this end, this paper presents a detailed evaluation of semantic segmentation, scene parsing and hybrid deep learning systems for extracting the retinal lesions such as intra-retinal fluid, sub-retinal fluid, hard exudates, drusen, and other chorioretinal anomalies from fused fundus and optical coherence tomography (OCT) imagery. Furthermore, we present a novel strategy exploiting the transferability of these models across multiple retinal scanner specifications. A total of 363 fundus and 173,915 OCT scans from seven publicly available datasets were used in this research (from which 297 fundus and 59,593 OCT scans were used for testing purposes). Overall, a hybrid retinal analysis and grading network (RAGNet), backboned through ResNet-50, stood first for extracting the retinal lesions, achieving a mean dice coefficient score of 0.822. Moreover, the complete source code and its documentation are released at: http://biomisa.org/index.php/downloads/.



### Deep Sequential Feature Learning in Clinical Image Classification of Infectious Keratitis
- **Arxiv ID**: http://arxiv.org/abs/2006.02666v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02666v1)
- **Published**: 2020-06-04 06:45:15+00:00
- **Updated**: 2020-06-04 06:45:15+00:00
- **Authors**: Yesheng Xu, Ming Kong, Wenjia Xie, Runping Duan, Zhengqing Fang, Yuxiao Lin, Qiang Zhu, Siliang Tang, Fei Wu, Yu-Feng Yao
- **Comment**: Accepted by Engineering
- **Journal**: None
- **Summary**: Infectious keratitis is the most common entities of corneal diseases, in which pathogen grows in the cornea leading to inflammation and destruction of the corneal tissues. Infectious keratitis is a medical emergency, for which a rapid and accurate diagnosis is needed for speedy initiation of prompt and precise treatment to halt the disease progress and to limit the extent of corneal damage; otherwise it may develop sight-threatening and even eye-globe-threatening condition. In this paper, we propose a sequential-level deep learning model to effectively discriminate the distinction and subtlety of infectious corneal disease via the classification of clinical images. In this approach, we devise an appropriate mechanism to preserve the spatial structures of clinical images and disentangle the informative features for clinical image classification of infectious keratitis. In competition with 421 ophthalmologists, the performance of the proposed sequential-level deep model achieved 80.00% diagnostic accuracy, far better than the 49.27% diagnostic accuracy achieved by ophthalmologists over 120 test images.



### Location, location, location: Satellite image-based real-estate appraisal
- **Arxiv ID**: http://arxiv.org/abs/2006.11406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11406v1)
- **Published**: 2020-06-04 07:25:02+00:00
- **Updated**: 2020-06-04 07:25:02+00:00
- **Authors**: Jan-Peter Kucklick, Oliver MÃ¼ller
- **Comment**: None
- **Journal**: None
- **Summary**: Buying a home is one of the most important buying decisions people have to make in their life. The latest research on real-estate appraisal focuses on incorporating image data in addition to structured data into the modeling process. This research measures the prediction performance of satellite images and structured data by using convolutional neural networks. The resulting CNN model trained performs 7% better in MAE than the advanced baseline of a neural network trained on structured data. Moreover, sliding-window heatmap provides visual interpretability of satellite images, revealing that neighborhood structures are essential in the price estimation.



### Uncertainty quantification in medical image segmentation with normalizing flows
- **Arxiv ID**: http://arxiv.org/abs/2006.02683v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.02683v2)
- **Published**: 2020-06-04 07:56:46+00:00
- **Updated**: 2020-08-04 10:40:10+00:00
- **Authors**: Raghavendra Selvan, Frederik Faye, Jon Middleton, Akshay Pai
- **Comment**: 12 pages. Accepted to be presented at 11th International Workshop on
  Machine Learning in Medical Imaging. Source code will be updated at
  https://github.com/raghavian/cFlow
- **Journal**: None
- **Summary**: Medical image segmentation is inherently an ambiguous task due to factors such as partial volumes and variations in anatomical definitions. While in most cases the segmentation uncertainty is around the border of structures of interest, there can also be considerable inter-rater differences. The class of conditional variational autoencoders (cVAE) offers a principled approach to inferring distributions over plausible segmentations that are conditioned on input images. Segmentation uncertainty estimated from samples of such distributions can be more informative than using pixel level probability scores. In this work, we propose a novel conditional generative model that is based on conditional Normalizing Flow (cFlow). The basic idea is to increase the expressivity of the cVAE by introducing a cFlow transformation step after the encoder. This yields improved approximations of the latent posterior distribution, allowing the model to capture richer segmentation variations. With this we show that the quality and diversity of samples obtained from our conditional generative model is enhanced. Performance of our model, which we call cFlow Net, is evaluated on two medical imaging datasets demonstrating substantial improvements in both qualitative and quantitative measures when compared to a recent cVAE based model.



### Problems of dataset creation for light source estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.02692v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02692v2)
- **Published**: 2020-06-04 08:20:30+00:00
- **Updated**: 2020-06-05 07:51:53+00:00
- **Authors**: E. I. Ershov, A. V. Belokopytov, A. V. Savchik
- **Comment**: None
- **Journal**: None
- **Summary**: The paper describes our experience collecting a new dataset for the light source estimation problem in a single image. The analysis of existing color targets is presented along with various technical and scientific aspects essential for data collection. The paper also contains an announcement of an upcoming 2-nd International Illumination Estimation Challenge (IEC 2020).



### Boundary-assisted Region Proposal Networks for Nucleus Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.02695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02695v1)
- **Published**: 2020-06-04 08:26:38+00:00
- **Updated**: 2020-06-04 08:26:38+00:00
- **Authors**: Shengcong Chen, Changxing Ding, Dacheng Tao
- **Comment**: Early Acception by MICCAI 2020
- **Journal**: None
- **Summary**: Nucleus segmentation is an important task in medical image analysis. However, machine learning models cannot perform well because there are large amount of clusters of crowded nuclei. To handle this problem, existing approaches typically resort to sophisticated hand-crafted post-processing strategies; therefore, they are vulnerable to the variation of post-processing hyper-parameters. Accordingly, in this paper, we devise a Boundary-assisted Region Proposal Network (BRP-Net) that achieves robust instance-level nucleus segmentation. First, we propose a novel Task-aware Feature Encoding (TAFE) network that efficiently extracts respective high-quality features for semantic segmentation and instance boundary detection tasks. This is achieved by carefully considering the correlation and differences between the two tasks. Second, coarse nucleus proposals are generated based on the predictions of the above two tasks. Third, these proposals are fed into instance segmentation networks for more accurate prediction. Experimental results demonstrate that the performance of BRP-Net is robust to the variation of post-processing hyper-parameters. Furthermore, BRP-Net achieves state-of-the-art performances on both the Kumar and CPM17 datasets. The code of BRP-Net will be released at https://github.com/csccsccsccsc/brpnet.



### LRNNet: A Light-Weighted Network with Efficient Reduced Non-Local Operation for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.02706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02706v1)
- **Published**: 2020-06-04 08:55:15+00:00
- **Updated**: 2020-06-04 08:55:15+00:00
- **Authors**: Weihao Jiang, Zhaozhi Xie, Yaoyi Li, Chang Liu, Hongtao Lu
- **Comment**: To appear in icme2020workshop(MMC)
- **Journal**: None
- **Summary**: The recent development of light-weighted neural networks has promoted the applications of deep learning under resource constraints and mobile applications. Many of these applications need to perform a real-time and efficient prediction for semantic segmentation with a light-weighted network. This paper introduces a light-weighted network with an efficient reduced non-local module (LRNNet) for efficient and realtime semantic segmentation. We proposed a factorized convolutional block in ResNet-Style encoder to achieve more lightweighted, efficient and powerful feature extraction. Meanwhile, our proposed reduced non-local module utilizes spatial regional dominant singular vectors to achieve reduced and more representative non-local feature integration with much lower computation and memory cost. Experiments demonstrate our superior trade-off among light-weight, speed, computation and accuracy. Without additional processing and pretraining, LRNNet achieves 72.2% mIoU on Cityscapes test dataset only using the fine annotation data for training with only 0.68M parameters and with 71 FPS on a GTX 1080Ti card.



### Auto-Rectify Network for Unsupervised Indoor Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.02708v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02708v2)
- **Published**: 2020-06-04 08:59:17+00:00
- **Updated**: 2021-12-14 06:17:08+00:00
- **Authors**: Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Tat-Jun Chin, Chunhua Shen, Ian Reid
- **Comment**: Accepted to TPAMI. Find code at https://github.com/JiawangBian
- **Journal**: None
- **Summary**: Single-View depth estimation using the CNNs trained from unlabelled videos has shown significant promise. However, excellent results have mostly been obtained in street-scene driving scenarios, and such methods often fail in other settings, particularly indoor videos taken by handheld devices. In this work, we establish that the complex ego-motions exhibited in handheld settings are a critical obstacle for learning depth. Our fundamental analysis suggests that the rotation behaves as noise during training, as opposed to the translation (baseline) which provides supervision signals. To address the challenge, we propose a data pre-processing method that rectifies training images by removing their relative rotations for effective learning. The significantly improved performance validates our motivation. Towards end-to-end learning without requiring pre-processing, we propose an Auto-Rectify Network with novel loss functions, which can automatically learn to rectify images during training. Consequently, our results outperform the previous unsupervised SOTA method by a large margin on the challenging NYUv2 dataset. We also demonstrate the generalization of our trained model in ScanNet and Make3D, and the universality of our proposed learning method on 7-Scenes and KITTI datasets.



### Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2006.02713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02713v2)
- **Published**: 2020-06-04 09:12:44+00:00
- **Updated**: 2020-10-13 13:10:31+00:00
- **Authors**: Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, Hongsheng Li
- **Comment**: Accepted in NeurIPS 2020. Code available:
  https://github.com/yxgeee/SpCL
- **Journal**: None
- **Summary**: Domain adaptive object re-ID aims to transfer the learned knowledge from the labeled source domain to the unlabeled target domain to tackle the open-class re-identification problems. Although state-of-the-art pseudo-label-based methods have achieved great success, they did not make full use of all valuable information because of the domain gap and unsatisfying clustering performance. To solve these problems, we propose a novel self-paced contrastive learning framework with hybrid memory. The hybrid memory dynamically generates source-domain class-level, target-domain cluster-level and un-clustered instance-level supervisory signals for learning feature representations. Different from the conventional contrastive learning strategy, the proposed framework jointly distinguishes source-domain classes, and target-domain clusters and un-clustered instances. Most importantly, the proposed self-paced method gradually creates more reliable clusters to refine the hybrid memory and learning targets, and is shown to be the key to our outstanding performance. Our method outperforms state-of-the-arts on multiple domain adaptation tasks of object re-ID and even boosts the performance on the source domain without any extra annotations. Our generalized version on unsupervised object re-ID surpasses state-of-the-art algorithms by considerable 16.7% and 7.9% on Market-1501 and MSMT17 benchmarks.



### GAN-Based Facial Attractiveness Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2006.02766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02766v1)
- **Published**: 2020-06-04 10:46:07+00:00
- **Updated**: 2020-06-04 10:46:07+00:00
- **Authors**: Yuhongze Zhou, Qinjie Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a generative framework based on generative adversarial network (GAN) to enhance facial attractiveness while preserving facial identity and high-fidelity. Given a portrait image as input, having applied gradient descent to recover a latent vector that this generative framework can use to synthesize an image resemble to the input image, beauty semantic editing manipulation on the corresponding recovered latent vector based on InterFaceGAN enables this framework to achieve facial image beautification. This paper compared our system with Beholder-GAN and our proposed result-enhanced version of Beholder-GAN. It turns out that our framework obtained state-of-art attractiveness enhancement results. The code is available at https://github.com/zoezhou1999/BeautifyBasedOnGAN.



### Overcoming Overfitting and Large Weight Update Problem in Linear Rectifiers: Thresholded Exponential Rectified Linear Units
- **Arxiv ID**: http://arxiv.org/abs/2006.02797v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02797v1)
- **Published**: 2020-06-04 11:55:47+00:00
- **Updated**: 2020-06-04 11:55:47+00:00
- **Authors**: Vijay Pandey
- **Comment**: None
- **Journal**: None
- **Summary**: In past few years, linear rectified unit activation functions have shown its significance in the neural networks, surpassing the performance of sigmoid activations. RELU (Nair & Hinton, 2010), ELU (Clevert et al., 2015), PRELU (He et al., 2015), LRELU (Maas et al., 2013), SRELU (Jin et al., 2016), ThresholdedRELU, all these linear rectified activation functions have its own significance over others in some aspect. Most of the time these activation functions suffer from bias shift problem due to non-zero output mean, and high weight update problem in deep complex networks due to unit gradient, which results in slower training, and high variance in model prediction respectively. In this paper, we propose, "Thresholded exponential rectified linear unit" (TERELU) activation function that works better in alleviating in overfitting: large weight update problem. Along with alleviating overfitting problem, this method also gives good amount of non-linearity as compared to other linear rectifiers. We will show better performance on the various datasets using neural networks, considering TERELU activation method compared to other activations.



### Height estimation from single aerial images using a deep ordinal regression network
- **Arxiv ID**: http://arxiv.org/abs/2006.02801v1
- **DOI**: 10.1109/LGRS.2020.3019252
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02801v1)
- **Published**: 2020-06-04 12:03:51+00:00
- **Updated**: 2020-06-04 12:03:51+00:00
- **Authors**: Xiang Li, Mingyang Wang, Yi Fang
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Understanding the 3D geometric structure of the Earth's surface has been an active research topic in photogrammetry and remote sensing community for decades, serving as an essential building block for various applications such as 3D digital city modeling, change detection, and city management. Previous researches have extensively studied the problem of height estimation from aerial images based on stereo or multi-view image matching. These methods require two or more images from different perspectives to reconstruct 3D coordinates with camera information provided. In this paper, we deal with the ambiguous and unsolved problem of height estimation from a single aerial image. Driven by the great success of deep learning, especially deep convolution neural networks (CNNs), some researches have proposed to estimate height information from a single aerial image by training a deep CNN model with large-scale annotated datasets. These methods treat height estimation as a regression problem and directly use an encoder-decoder network to regress the height values. In this paper, we proposed to divide height values into spacing-increasing intervals and transform the regression problem into an ordinal regression problem, using an ordinal loss for network training. To enable multi-scale feature extraction, we further incorporate an Atrous Spatial Pyramid Pooling (ASPP) module to extract features from multiple dilated convolution layers. After that, a post-processing technique is designed to transform the predicted height map of each patch into a seamless height map. Finally, we conduct extensive experiments on ISPRS Vaihingen and Potsdam datasets. Experimental results demonstrate significantly better performance of our method compared to the state-of-the-art methods.



### A Computational Model of Early Word Learning from the Infant's Point of View
- **Arxiv ID**: http://arxiv.org/abs/2006.02802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02802v1)
- **Published**: 2020-06-04 12:08:44+00:00
- **Updated**: 2020-06-04 12:08:44+00:00
- **Authors**: Satoshi Tsutsui, Arjun Chandrasekaran, Md Alimoor Reza, David Crandall, Chen Yu
- **Comment**: Accepted by Annual Conference of the Cognitive Science Society
  (CogSci) 2020. (Oral Acceptance Rate = 177/811 = 22%)
- **Journal**: None
- **Summary**: Human infants have the remarkable ability to learn the associations between object names and visual objects from inherently ambiguous experiences. Researchers in cognitive science and developmental psychology have built formal models that implement in-principle learning algorithms, and then used pre-selected and pre-cleaned datasets to test the abilities of the models to find statistical regularities in the input data. In contrast to previous modeling approaches, the present study used egocentric video and gaze data collected from infant learners during natural toy play with their parents. This allowed us to capture the learning environment from the perspective of the learner's own point of view. We then used a Convolutional Neural Network (CNN) model to process sensory data from the infant's point of view and learn name-object associations from scratch. As the first model that takes raw egocentric video to simulate infant word learning, the present study provides a proof of principle that the problem of early word learning can be solved, using actual visual data perceived by infant learners. Moreover, we conducted simulation experiments to systematically determine how visual, perceptual, and attentional properties of infants' sensory experiences may affect word learning.



### Pathological myopia classification with simultaneous lesion segmentation using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2006.02813v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02813v1)
- **Published**: 2020-06-04 12:21:06+00:00
- **Updated**: 2020-06-04 12:21:06+00:00
- **Authors**: Ruben Hemelings, Bart Elen, Matthew B. Blaschko, Julie Jacob, Ingeborg Stalmans, Patrick De Boever
- **Comment**: 18 pages, 2 figures, preprint to journal
- **Journal**: None
- **Summary**: This investigation reports on the results of convolutional neural networks developed for the recently introduced PathologicAL Myopia (PALM) dataset, which consists of 1200 fundus images. We propose a new Optic Nerve Head (ONH)-based prediction enhancement for the segmentation of atrophy and fovea. Models trained with 400 available training images achieved an AUC of 0.9867 for pathological myopia classification, and a Euclidean distance of 58.27 pixels on the fovea localization task, evaluated on a test set of 400 images. Dice and F1 metrics for semantic segmentation of lesions scored 0.9303 and 0.9869 on optic disc, 0.8001 and 0.9135 on retinal atrophy, and 0.8073 and 0.7059 on retinal detachment, respectively. Our work was acknowledged with an award in the context of the "PathologicAL Myopia detection from retinal images" challenge held during the IEEE International Symposium on Biomedical Imaging (April 2019). Considering that (pathological) myopia cases are often identified as false positives and negatives in classification systems for glaucoma, we envision that the current work could aid in future research to discriminate between glaucomatous and highly-myopic eyes, complemented by the localization and segmentation of landmarks such as fovea, optic disc and atrophy.



### Look Locally Infer Globally: A Generalizable Face Anti-Spoofing Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.02834v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02834v3)
- **Published**: 2020-06-04 13:11:17+00:00
- **Updated**: 2020-06-15 19:04:10+00:00
- **Authors**: Debayan Deb, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art spoof detection methods tend to overfit to the spoof types seen during training and fail to generalize to unknown spoof types. Given that face anti-spoofing is inherently a local task, we propose a face anti-spoofing framework, namely Self-Supervised Regional Fully Convolutional Network (SSR-FCN), that is trained to learn local discriminative cues from a face image in a self-supervised manner. The proposed framework improves generalizability while maintaining the computational efficiency of holistic face anti-spoofing approaches (< 4 ms on a Nvidia GTX 1080Ti GPU). The proposed method is interpretable since it localizes which parts of the face are labeled as spoofs. Experimental results show that SSR-FCN can achieve TDR = 65% @ 2.0% FDR when evaluated on a dataset comprising of 13 different spoof types under unknown attacks while achieving competitive performances under standard benchmark datasets (Oulu-NPU, CASIA-MFSD, and Replay-Attack).



### 2D Image Features Detector And Descriptor Selection Expert System
- **Arxiv ID**: http://arxiv.org/abs/2006.02933v1
- **DOI**: 10.5121/csit.2019.91206
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02933v1)
- **Published**: 2020-06-04 15:18:18+00:00
- **Updated**: 2020-06-04 15:18:18+00:00
- **Authors**: Ibon Merino, Jon Azpiazu, Anthony Remazeilles, Basilio Sierra
- **Comment**: 10 pages, 5 figures, 5 tables
- **Journal**: in 8th International Conference on Natural Language Processing
  (NLP 2019), Sep. 2019, pp. 51-61
- **Summary**: Detection and description of keypoints from an image is a well-studied problem in Computer Vision. Some methods like SIFT, SURF or ORB are computationally really efficient. This paper proposes a solution for a particular case study on object recognition of industrial parts based on hierarchical classification. Reducing the number of instances leads to better performance, indeed, that is what the use of the hierarchical classification is looking for. We demonstrate that this method performs better than using just one method like ORB, SIFT or FREAK, despite being fairly slower.



### RarePlanes: Synthetic Data Takes Flight
- **Arxiv ID**: http://arxiv.org/abs/2006.02963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2006.02963v2)
- **Published**: 2020-06-04 15:46:43+00:00
- **Updated**: 2020-11-10 17:17:01+00:00
- **Authors**: Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan Lewis, Daeil Kim
- **Comment**: To appear in WACV 2021 - 11 pages
- **Journal**: None
- **Summary**: RarePlanes is a unique open-source machine learning dataset that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700 hand-annotated aircraft. The accompanying synthetic dataset is generated via AI.Reverie's simulation platform and features 50,000 synthetic satellite images simulating a total area of 9331.2 km^2 with ~630,000 aircraft annotations. Both the real and synthetically generated aircraft feature 10 fine grain attributes including: aircraft length, wingspan, wing-shape, wing-position, wingspan class, propulsion, number of engines, number of vertical-stabilizers, presence of canards, and aircraft role. Finally, we conduct extensive experiments to evaluate the real and synthetic datasets and compare performances. By doing so, we show the value of synthetic data for the task of detecting and classifying aircraft from an overhead perspective.



### A Siamese Neural Network with Modified Distance Loss For Transfer Learning in Speech Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.03001v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03001v1)
- **Published**: 2020-06-04 16:44:33+00:00
- **Updated**: 2020-06-04 16:44:33+00:00
- **Authors**: Kexin Feng, Theodora Chaspari
- **Comment**: AffCon@AAAI-20; Presented at AAAI-20 W1: Affective Content Analysis
- **Journal**: None
- **Summary**: Automatic emotion recognition plays a significant role in the process of human computer interaction and the design of Internet of Things (IOT) technologies. Yet, a common problem in emotion recognition systems lies in the scarcity of reliable labels. By modeling pairwise differences between samples of interest, a Siamese network can help to mitigate this challenge since it requires fewer samples than traditional deep learning methods. In this paper, we propose a distance loss, which can be applied on the Siamese network fine-tuning, by optimizing the model based on the relevant distance between same and difference class pairs. Our system use samples from the source data to pre-train the weights of proposed Siamese neural network, which are fine-tuned based on the target data. We present an emotion recognition task that uses speech, since it is one of the most ubiquitous and frequently used bio-behavioral signals. Our target data comes from the RAVDESS dataset, while the CREMA-D and eNTERFACE'05 are used as source data, respectively. Our results indicate that the proposed distance loss is able to greatly benefit the fine-tuning process of Siamese network. Also, the selection of source data has more effect on the Siamese network performance compared to the number of frozen layers. These suggest the great potential of applying the Siamese network and modelling pairwise differences in the field of transfer learning for automatic emotion recognition.



### Visually Guided Sound Source Separation using Cascaded Opponent Filter Network
- **Arxiv ID**: http://arxiv.org/abs/2006.03028v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03028v2)
- **Published**: 2020-06-04 17:27:49+00:00
- **Updated**: 2020-07-14 15:38:36+00:00
- **Authors**: Lingyu Zhu, Esa Rahtu
- **Comment**: main paper 14 pages, ref 3 pages, and supp 7 pages. Revised argument
  in section 3 and 4
- **Journal**: None
- **Summary**: The objective of this paper is to recover the original component signals from a mixture audio with the aid of visual cues of the sound sources. Such task is usually referred as visually guided sound source separation. The proposed Cascaded Opponent Filter (COF) framework consists of multiple stages, which recursively refine the source separation. A key element in COF is a novel opponent filter module that identifies and relocates residual components between sources. The system is guided by the appearance and motion of the source, and, for this purpose, we study different representations based on video frames, optical flows, dynamic images, and their combinations. Finally, we propose a Sound Source Location Masking (SSLM) technique, which, together with COF, produces a pixel level mask of the source location. The entire system is trained end-to-end using a large set of unlabelled videos. We compare COF with recent baselines and obtain the state-of-the-art performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). Project page: https://ly-zhu.github.io/cof-net.



### Dendrite Net with Acceleration Module for Faster Nonlinear Mapping and System Identification
- **Arxiv ID**: http://arxiv.org/abs/2006.02901v2
- **DOI**: 10.3390/math10234477
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02901v2)
- **Published**: 2020-06-04 17:56:24+00:00
- **Updated**: 2022-12-01 17:51:04+00:00
- **Authors**: Gang Liu, Yajing Pang, Shuai Yin, Xiaoke Niu, Jing Wang, Hong Wan
- **Comment**: Published in Mathematics
- **Journal**: None
- **Summary**: Nonlinear mapping is an essential and common demand in online systems, such as sensor systems and mobile phones. Accelerating nonlinear mapping will directly speed up online systems. Previously the authors of this paper proposed a Dendrite Net (DD) with enormously lower time complexity than the existing nonlinear mapping algorithms; however, there still are redundant calculations in DD. This paper presents a DD with an acceleration module (AC) to accelerate nonlinear mapping further. We conduct three experiments to verify whether DD with AC has lower time complexity while retaining DD's nonlinear mapping properties and system identification properties: The first experiment is the precision and identification of unary nonlinear mapping, reflecting the calculation performance using DD with AC for basic functions in online systems. The second experiment is the mapping precision and identification of the multi-input nonlinear system, reflecting the performance for designing online systems via DD with AC. Finally, this paper compares the time complexity of DD and DD with AC and analyzes the theoretical reasons through repeated experiments. Results: DD with AC retains DD's excellent mapping and identification properties and has lower time complexity. Significance: DD with AC can be used for most engineering systems, such as sensor systems, and will speed up computation in these online systems. The code of DD with AC is available on https://github.com/liugang1234567/Gang-neuron



### SIDU: Similarity Difference and Uniqueness Method for Explainable AI
- **Arxiv ID**: http://arxiv.org/abs/2006.03122v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03122v1)
- **Published**: 2020-06-04 20:33:40+00:00
- **Updated**: 2020-06-04 20:33:40+00:00
- **Authors**: Satya M. Muddamsetty, Mohammad N. S. Jahromi, Thomas B. Moeslund
- **Comment**: Accepted manuscript in IEEE International Conference on Image
  Processing
- **Journal**: None
- **Summary**: A new brand of technical artificial intelligence ( Explainable AI ) research has focused on trying to open up the 'black box' and provide some explainability. This paper presents a novel visual explanation method for deep learning networks in the form of a saliency map that can effectively localize entire object regions. In contrast to the current state-of-the art methods, the proposed method shows quite promising visual explanations that can gain greater trust of human expert. Both quantitative and qualitative evaluations are carried out on both general and clinical data sets to confirm the effectiveness of the proposed method.



### Unsupervised clustering of Roman pottery profiles from their SSAE representation
- **Arxiv ID**: http://arxiv.org/abs/2006.03156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.LG, 68-04, 68-06, 68U10, 68T05, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2006.03156v1)
- **Published**: 2020-06-04 22:19:22+00:00
- **Updated**: 2020-06-04 22:19:22+00:00
- **Authors**: Simone Parisotto, Alessandro Launaro, Ninetta Leone, Carola-Bibiane SchÃ¶nlieb
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper we introduce the ROman COmmonware POTtery (ROCOPOT) database, which comprises of more than 2000 black and white imaging profiles of pottery shapes extracted from 11 Roman catalogues and related to different excavation sites. The partiality and the handcrafted variance of the shape fragments within this new database make their unsupervised clustering a very challenging problem: profile similarities are thus explored via the hierarchical clustering of non-linear features learned in the latent representation space of a stacked sparse autoencoder (SSAE) network, unveiling new profile matches. Results are commented both from a mathematical and archaeological perspective so as to unlock new research directions in the respective communities.



### Improving Train Track Safety using Drones, Computer Vision and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11379v1)
- **Published**: 2020-06-04 23:17:23+00:00
- **Updated**: 2020-06-04 23:17:23+00:00
- **Authors**: Kirthi Kumar, Anuraag Kaashyap
- **Comment**: 27 pages and 19 figures
- **Journal**: None
- **Summary**: Millions of human casualties resulting from train accidents globally are caused by the inefficient, manual track inspections. Government agencies are seriously concerned about the safe operations of the rail industry after series of accidents reported across e USA and around the globe, mainly attributed to track defects. Casualties resulting from track defects result in billions of dollars loss in public and private investments and loss of revenue due to downtime, ultimately resulting in loss of the public's confidence. The manual, mundane, and expensive monitoring of rail track safety can be transform through the use of drones, computer vision, and machine learning. The primary goal of this study is to develop multiple algorithms that implement supervised and semi-supervised learning that accurately analyze whether a track is safe or unsafe based on simulated training data of train tracks. This includes being able to develop a Convolutional Neural Network that can identify track defects using supervised learning without having to specify a particular algorithm for detecting those defects, and that the new model would both speed up and improve the quality of the track defect detection process, accompanied with a computer vision image-processing algorithm. Our other goals included designing and building a prototype representation of train tracks to simulate track defects, to precisely and consistently conduct the visual inspection using drones. Ultimately, the goal demonstrates that the state of good repairs in railway tracks can be attained through the use of drones, computer vision and machine learning.




# Arxiv Papers in cs.CV on 2020-06-01
### Automatic Building and Labeling of HD Maps with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.00644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00644v1)
- **Published**: 2020-06-01 00:02:45+00:00
- **Updated**: 2020-06-01 00:02:45+00:00
- **Authors**: Mahdi Elhousni, Yecheng Lyu, Ziming Zhang, Xinming Huang
- **Comment**: Accepted by IAAI2020
- **Journal**: None
- **Summary**: In a world where autonomous driving cars are becoming increasingly more common, creating an adequate infrastructure for this new technology is essential. This includes building and labeling high-definition (HD) maps accurately and efficiently. Today, the process of creating HD maps requires a lot of human input, which takes time and is prone to errors. In this paper, we propose a novel method capable of generating labelled HD maps from raw sensor data. We implemented and tested our methods on several urban scenarios using data collected from our test vehicle. The results show that the pro-posed deep learning based method can produce highly accurate HD maps. This approach speeds up the process of building and labeling HD maps, which can make meaningful contribution to the deployment of autonomous vehicle.



### A Survey on 3D LiDAR Localization for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2006.00648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00648v2)
- **Published**: 2020-06-01 00:19:35+00:00
- **Updated**: 2020-11-21 17:07:12+00:00
- **Authors**: Mahdi Elhousni, Xinming Huang
- **Comment**: Accepted by IV2020
- **Journal**: None
- **Summary**: LiDAR sensors are becoming one of the most essential sensors in achieving full autonomy for self driving cars. LiDARs are able to produce rich, dense and precise spatial data, which can tremendously help in localizing and tracking a moving vehicle. In this paper, we review the latest finding in 3D LiDAR localization for autonomous driving cars, and analyse the results obtained by each method, in an effort to guide the research community towards the path that seems to be the most promising.



### A multimodal approach for multi-label movie genre classification
- **Arxiv ID**: http://arxiv.org/abs/2006.00654v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.00654v1)
- **Published**: 2020-06-01 00:51:39+00:00
- **Updated**: 2020-06-01 00:51:39+00:00
- **Authors**: Rafael B. Mangolin, Rodolfo M. Pereira, Alceu S. Britto Jr., Carlos N. Silla Jr., Valéria D. Feltrim, Diego Bertolini, Yandre M. G. Costa
- **Comment**: 21 pages and 4 figures
- **Journal**: None
- **Summary**: Movie genre classification is a challenging task that has increasingly attracted the attention of researchers. In this paper, we addressed the multi-label classification of the movie genres in a multimodal way. For this purpose, we created a dataset composed of trailer video clips, subtitles, synopses, and movie posters taken from 152,622 movie titles from The Movie Database. The dataset was carefully curated and organized, and it was also made available as a contribution of this work. Each movie of the dataset was labeled according to a set of eighteen genre labels. We extracted features from these data using different kinds of descriptors, namely Mel Frequency Cepstral Coefficients, Statistical Spectrum Descriptor , Local Binary Pattern with spectrograms, Long-Short Term Memory, and Convolutional Neural Networks. The descriptors were evaluated using different classifiers, such as BinaryRelevance and ML-kNN. We have also investigated the performance of the combination of different classifiers/features using a late fusion strategy, which obtained encouraging results. Based on the F-Score metric, our best result, 0.628, was obtained by the fusion of a classifier created using LSTM on the synopses, and a classifier created using CNN on movie trailer frames. When considering the AUC-PR metric, the best result, 0.673, was also achieved by combining those representations, but in addition, a classifier based on LSTM created from the subtitles was used. These results corroborate the existence of complementarity among classifiers based on different sources of information in this field of application. As far as we know, this is the most comprehensive study developed in terms of the diversity of multimedia sources of information to perform movie genre classification.



### Symbol Spotting on Digital Architectural Floor Plans Using a Deep Learning-based Framework
- **Arxiv ID**: http://arxiv.org/abs/2006.00684v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00684v1)
- **Published**: 2020-06-01 03:16:05+00:00
- **Updated**: 2020-06-01 03:16:05+00:00
- **Authors**: Alireza Rezvanifar, Melissa Cote, Alexandra Branzan Albu
- **Comment**: Accepted to CVPR2020 Workshop on Text and Documents in the Deep
  Learning Era
- **Journal**: None
- **Summary**: This papers focuses on symbol spotting on real-world digital architectural floor plans with a deep learning (DL)-based framework. Traditional on-the-fly symbol spotting methods are unable to address the semantic challenge of graphical notation variability, i.e. low intra-class symbol similarity, an issue that is particularly important in architectural floor plan analysis. The presence of occlusion and clutter, characteristic of real-world plans, along with a varying graphical symbol complexity from almost trivial to highly complex, also pose challenges to existing spotting methods. In this paper, we address all of the above issues by leveraging recent advances in DL and adapting an object detection framework based on the You-Only-Look-Once (YOLO) architecture. We propose a training strategy based on tiles, avoiding many issues particular to DL-based object detection networks related to the relative small size of symbols compared to entire floor plans, aspect ratios, and data augmentation. Experiments on real-world floor plans demonstrate that our method successfully detects architectural symbols with low intra-class similarity and of variable graphical complexity, even in the presence of heavy occlusion and clutter. Additional experiments on the public SESYD dataset confirm that our proposed approach can deal with various degradation and noise levels and outperforms other symbol spotting methods.



### DeepMark++: Real-time Clothing Detection at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2006.00710v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00710v3)
- **Published**: 2020-06-01 04:36:57+00:00
- **Updated**: 2020-11-10 07:47:43+00:00
- **Authors**: Alexey Sidnev, Alexander Krapivin, Alexey Trushkov, Ekaterina Krasikova, Maxim Kazakov, Mikhail Viryasov
- **Comment**: Winter Conference on Applications of Computer Vision (WACV21)
- **Journal**: None
- **Summary**: Clothing recognition is the most fundamental AI application challenge within the fashion domain. While existing solutions offer decent recognition accuracy, they are generally slow and require significant computational resources. In this paper we propose a single-stage approach to overcome this obstacle and deliver rapid clothing detection and keypoint estimation. Our solution is based on a multi-target network CenterNet, and we introduce several powerful post-processing techniques to enhance performance. Our most accurate model achieves results comparable to state-of-the-art solutions on the DeepFashion2 dataset, and our light and fast model runs at 17 FPS on the Huawei P40 Pro smartphone. In addition, we achieved second place in the DeepFashion2 Landmark Estimation Challenge 2020 with 0.582 mAP on the test dataset.



### M2Net: Multi-modal Multi-channel Network for Overall Survival Time Prediction of Brain Tumor Patients
- **Arxiv ID**: http://arxiv.org/abs/2006.10135v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10135v2)
- **Published**: 2020-06-01 05:21:37+00:00
- **Updated**: 2020-07-14 18:47:11+00:00
- **Authors**: Tao Zhou, Huazhu Fu, Yu Zhang, Changqing Zhang, Xiankai Lu, Jianbing Shen, Ling Shao
- **Comment**: Accepted by MICCAI'20
- **Journal**: None
- **Summary**: Early and accurate prediction of overall survival (OS) time can help to obtain better treatment planning for brain tumor patients. Although many OS time prediction methods have been developed and obtain promising results, there are still several issues. First, conventional prediction methods rely on radiomic features at the local lesion area of a magnetic resonance (MR) volume, which may not represent the full image or model complex tumor patterns. Second, different types of scanners (i.e., multi-modal data) are sensitive to different brain regions, which makes it challenging to effectively exploit the complementary information across multiple modalities and also preserve the modality-specific properties. Third, existing methods focus on prediction models, ignoring complex data-to-label relationships. To address the above issues, we propose an end-to-end OS time prediction model; namely, Multi-modal Multi-channel Network (M2Net). Specifically, we first project the 3D MR volume onto 2D images in different directions, which reduces computational costs, while preserving important information and enabling pre-trained models to be transferred from other tasks. Then, we use a modality-specific network to extract implicit and high-level features from different MR scans. A multi-modal shared network is built to fuse these features using a bilinear pooling model, exploiting their correlations to provide complementary information. Finally, we integrate the outputs from each modality-specific network and the multi-modal shared network to generate the final prediction result. Experimental results demonstrate the superiority of our M2Net model over other methods.



### Using Generative Models for Pediatric wbMRI
- **Arxiv ID**: http://arxiv.org/abs/2006.00727v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00727v1)
- **Published**: 2020-06-01 05:29:18+00:00
- **Updated**: 2020-06-01 05:29:18+00:00
- **Authors**: Alex Chang, Vinith M. Suriyakumar, Abhishek Moturu, Nipaporn Tewattanarat, Andrea Doria, Anna Goldenberg
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection of cancer is key to a good prognosis and requires frequent testing, especially in pediatrics. Whole-body magnetic resonance imaging (wbMRI) is an essential part of several well-established screening protocols, with screening starting in early childhood. To date, machine learning (ML) has been used on wbMRI images to stage adult cancer patients. It is not possible to use such tools in pediatrics due to the changing bone signal throughout growth, the difficulty of obtaining these images in young children due to movement and limited compliance, and the rarity of positive cases. We evaluate the quality of wbMRI images generated using generative adversarial networks (GANs) trained on wbMRI data from The Hospital for Sick Children in Toronto. We use the Frchet Inception Distance (FID) metric, Domain Frchet Distance (DFD), and blind tests with a radiology fellow for evaluation. We demonstrate that StyleGAN2 provides the best performance in generating wbMRI images with respect to all three metrics.



### Automatic classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray image: combination of data augmentation methods
- **Arxiv ID**: http://arxiv.org/abs/2006.00730v2
- **DOI**: 10.1038/s41598-020-74539-2
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00730v2)
- **Published**: 2020-06-01 05:34:53+00:00
- **Updated**: 2020-06-12 03:47:25+00:00
- **Authors**: Mizuho Nishio, Shunjiro Noguchi, Hidetoshi Matsuo, Takamichi Murakami
- **Comment**: None
- **Journal**: Sci Rep 10, 17532 (2020)
- **Summary**: Purpose: This study aimed to develop and validate computer-aided diagnosis (CXDx) system for classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray (CXR) images.   Materials and Methods: From two public datasets, 1248 CXR images were obtained, which included 215, 533, and 500 CXR images of COVID-19 pneumonia patients, non-COVID-19 pneumonia patients, and the healthy samples. The proposed CADx system utilized VGG16 as a pre-trained model and combination of conventional method and mixup as data augmentation methods. Other types of pre-trained models were compared with the VGG16-based model. Single type or no data augmentation methods were also evaluated. Splitting of training/validation/test sets was used when building and evaluating the CADx system. Three-category accuracy was evaluated for test set with 125 CXR images.   Results: The three-category accuracy of the CAD system was 83.6% between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy. Sensitivity for COVID-19 pneumonia was more than 90%. The combination of conventional method and mixup was more useful than single type or no data augmentation method.   Conclusion: This study was able to create an accurate CADx system for the 3-category classification. Source code of our CADx system is available as open source for COVID-19 research.



### Global Distance-distributions Separation for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2006.00752v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00752v3)
- **Published**: 2020-06-01 07:05:39+00:00
- **Updated**: 2020-07-10 09:27:59+00:00
- **Authors**: Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Supervised person re-identification (ReID) often has poor scalability and usability in real-world deployments due to domain gaps and the lack of annotations for the target domain data. Unsupervised person ReID through domain adaptation is attractive yet challenging. Existing unsupervised ReID approaches often fail in correctly identifying the positive samples and negative samples through the distance-based matching/ranking. The two distributions of distances for positive sample pairs (Pos-distr) and negative sample pairs (Neg-distr) are often not well separated, having large overlap. To address this problem, we introduce a global distance-distributions separation (GDS) constraint over the two distributions to encourage the clear separation of positive and negative samples from a global view. We model the two global distance distributions as Gaussian distributions and push apart the two distributions while encouraging their sharpness in the unsupervised training process. Particularly, to model the distributions from a global view and facilitate the timely updating of the distributions and the GDS related losses, we leverage a momentum update mechanism for building and maintaining the distribution parameters (mean and variance) and calculate the loss on the fly during the training. Distribution-based hard mining is proposed to further promote the separation of the two distributions. We validate the effectiveness of the GDS constraint in unsupervised ReID networks. Extensive experiments on multiple ReID benchmark datasets show our method leads to significant improvement over the baselines and achieves the state-of-the-art performance.



### Structured Multimodal Attentions for TextVQA
- **Arxiv ID**: http://arxiv.org/abs/2006.00753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00753v2)
- **Published**: 2020-06-01 07:07:36+00:00
- **Updated**: 2021-11-26 03:00:58+00:00
- **Authors**: Chenyu Gao, Qi Zhu, Peng Wang, Hui Li, Yuliang Liu, Anton van den Hengel, Qi Wu
- **Comment**: winner of TextVQA Challenge 2020, Accepted by IEEE Transactions on
  Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end structured multimodal attention (SMA) neural network to mainly solve the first two issues above. SMA first uses a structural graph representation to encode the object-object, object-text and text-text relationships appearing in the image, and then designs a multimodal graph attention network to reason over it. Finally, the outputs from the above modules are processed by a global-local attentional answering module to produce an answer splicing together tokens from both OCR and general vocabulary iteratively by following M4C. Our proposed model outperforms the SoTA models on TextVQA dataset and two tasks of ST-VQA dataset among all models except pre-training based TAP. Demonstrating strong reasoning ability, it also won first place in TextVQA Challenge 2020. We extensively test different OCR methods on several reasoning models and investigate the impact of gradually increased OCR performance on TextVQA benchmark. With better OCR results, different models share dramatic improvement over the VQA accuracy, but our model benefits most blessed by strong textual-visual reasoning ability. To grant our method an upper bound and make a fair testing base available for further works, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset, which were not given in the original release. The code and ground-truth OCR annotations for the TextVQA dataset are available at https://github.com/ChenyuGAO-CS/SMA



### Residual Squeeze-and-Excitation Network for Fast Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2006.00757v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00757v1)
- **Published**: 2020-06-01 07:17:01+00:00
- **Updated**: 2020-06-01 07:17:01+00:00
- **Authors**: Jun Fu, Jianfeng Xu, Kazuyuki Tasaka, Zhibo Chen
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Image deraining is an important image processing task as rain streaks not only severely degrade the visual quality of images but also significantly affect the performance of high-level vision tasks. Traditional methods progressively remove rain streaks via different recurrent neural networks. However, these methods fail to yield plausible rain-free images in an efficient manner. In this paper, we propose a residual squeeze-and-excitation network called RSEN for fast image deraining as well as superior deraining performance compared with state-of-the-art approaches. Specifically, RSEN adopts a lightweight encoder-decoder architecture to conduct rain removal in one stage. Besides, both encoder and decoder adopt a novel residual squeeze-and-excitation block as the core of feature extraction, which contains a residual block for producing hierarchical features, followed by a squeeze-and-excitation block for channel-wisely enhancing the resulted hierarchical features. Experimental results demonstrate that our method can not only considerably reduce the computational complexity but also significantly improve the deraining performance compared with state-of-the-art methods.



### Reducing the X-ray radiation exposure frequency in cardio-angiography via deep-learning based video interpolation
- **Arxiv ID**: http://arxiv.org/abs/2006.00781v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00781v1)
- **Published**: 2020-06-01 08:14:10+00:00
- **Updated**: 2020-06-01 08:14:10+00:00
- **Authors**: Xiao-Lei Yin, Dong-Xue Liang, Lu Wang, Jing Qiu, Zhi-Yun Yang, Jun-Hui Xing, Jian-Zeng Dong, Zhao-Yuan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiac coronary angiography is a major technology to assist doctors during cardiac interventional surgeries. Under the exposure of X-ray radiation, doctors inject contrast agents through catheters to determine the position and status of coronary vessels in real time. To get a coronary angiography video with a high frame rate, the doctor needs to increase the exposure frequency and intensity of the X-ray. This will inevitably increase the X-ray harm to both patients and surgeons. In this work, we innovatively utilize a deep-learning based video interpolation algorithm to interpolate coronary angiography videos. Moreover, we establish a new coronary angiography image dataset ,which contains 95,039 triplets images to retrain the video interpolation network model. Using the retrained network we synthesize high frame rate coronary angiography video from the low frame rate coronary angiography video. The average peak signal to noise ratio(PSNR) of those synthesized video frames reaches 34dB. Extensive experiment results demonstrate the feasibility of using the video frame interpolation algorithm to synthesize continuous and clear high frame rate coronary angiography video. With the help of this technology, doctors can significantly reduce exposure frequency and intensity of the X-ray during coronary angiography.



### Transcription-Enriched Joint Embeddings for Spoken Descriptions of Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.00785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.00785v1)
- **Published**: 2020-06-01 08:18:15+00:00
- **Updated**: 2020-06-01 08:18:15+00:00
- **Authors**: Benet Oriol, Jordi Luque, Ferran Diego, Xavier Giro-i-Nieto
- **Comment**: Accepted for presentation at EPIC@CVPR2020 workshop
- **Journal**: None
- **Summary**: In this work, we propose an effective approach for training unique embedding representations by combining three simultaneous modalities: image and spoken and textual narratives. The proposed methodology departs from a baseline system that spawns a embedding space trained with only spoken narratives and image cues. Our experiments on the EPIC-Kitchen and Places Audio Caption datasets show that introducing the human-generated textual transcriptions of the spoken narratives helps to the training procedure yielding to get better embedding representations. The triad speech, image and words allows for a better estimate of the point embedding and show an improving of the performance within tasks like image and speech retrieval, even when text third modality, text, is not present in the task.



### Foreground-aware Semantic Representations for Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2006.00809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00809v1)
- **Published**: 2020-06-01 09:27:20+00:00
- **Updated**: 2020-06-01 09:27:20+00:00
- **Authors**: Konstantin Sofiiuk, Polina Popenova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Image harmonization is an important step in photo editing to achieve visual consistency in composite images by adjusting the appearances of foreground to make it compatible with background. Previous approaches to harmonize composites are based on training of encoder-decoder networks from scratch, which makes it challenging for a neural network to learn a high-level representation of objects. We propose a novel architecture to utilize the space of high-level features learned by a pre-trained classification network. We create our models as a combination of existing encoder-decoder architectures and a pre-trained foreground-aware deep high-resolution network. We extensively evaluate the proposed method on existing image harmonization benchmark and set up a new state-of-the-art in terms of MSE and PSNR metrics. The code and trained models are available at \url{https://github.com/saic-vul/image_harmonization}.



### Real-Time Face and Landmark Localization for Eyeblink Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.00816v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00816v2)
- **Published**: 2020-06-01 09:46:25+00:00
- **Updated**: 2020-07-15 16:16:11+00:00
- **Authors**: Paul Bakker, Henk-Jan Boele, Zaid Al-Ars, Christos Strydis
- **Comment**: Added public gitlab repo link with paper source code
- **Journal**: None
- **Summary**: Pavlovian eyeblink conditioning is a powerful experiment used in the field of neuroscience to measure multiple aspects of how we learn in our daily life. To track the movement of the eyelid during an experiment, researchers have traditionally made use of potentiometers or electromyography. More recently, the use of computer vision and image processing alleviated the need for these techniques but currently employed methods require human intervention and are not fast enough to enable real-time processing. In this work, a face- and landmark-detection algorithm have been carefully combined in order to provide fully automated eyelid tracking, and have further been accelerated to make the first crucial step towards online, closed-loop experiments. Such experiments have not been achieved so far and are expected to offer significant insights in the workings of neurological and psychiatric disorders. Based on an extensive literature search, various different algorithms for face detection and landmark detection have been analyzed and evaluated. Two algorithms were identified as most suitable for eyelid detection: the Histogram-of-Oriented-Gradients (HOG) algorithm for face detection and the Ensemble-of-Regression-Trees (ERT) algorithm for landmark detection. These two algorithms have been accelerated on GPU and CPU, achieving speedups of 1,753$\times$ and 11$\times$, respectively. To demonstrate the usefulness of our eyelid-detection algorithm, a research hypothesis was formed and a well-established neuroscientific experiment was employed: eyeblink detection. Our experimental evaluation reveals an overall application runtime of 0.533 ms per frame, which is 1,101$\times$ faster than the sequential implementation and well within the real-time requirements of eyeblink conditioning in humans, i.e. faster than 500 frames per second.



### Exploring Thermal Images for Object Detection in Underexposure Regions for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2006.00821v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.00821v2)
- **Published**: 2020-06-01 09:59:09+00:00
- **Updated**: 2021-05-03 09:24:14+00:00
- **Authors**: Farzeen Munir, Shoaib Azam, Muhammd Aasim Rafique, Ahmad Muqeem Sheri, Moongu Jeon, Witold Pedrycz
- **Comment**: None
- **Journal**: None
- **Summary**: Underexposure regions are vital to construct a complete perception of the surroundings for safe autonomous driving. The availability of thermal cameras has provided an essential alternate to explore regions where other optical sensors lack in capturing interpretable signals. A thermal camera captures an image using the heat difference emitted by objects in the infrared spectrum, and object detection in thermal images becomes effective for autonomous driving in challenging conditions. Although object detection in the visible spectrum domain imaging has matured, thermal object detection lacks effectiveness. A significant challenge is scarcity of labeled data for the thermal domain which is desiderata for SOTA artificial intelligence techniques. This work proposes a domain adaptation framework which employs a style transfer technique for transfer learning from visible spectrum images to thermal images. The framework uses a generative adversarial network (GAN) to transfer the low-level features from the visible spectrum domain to the thermal domain through style consistency. The efficacy of the proposed method of object detection in thermal images is evident from the improved results when used styled images from publicly available thermal image datasets (FLIR ADAS and KAIST Multi-Spectral).



### Deceiving computers in Reverse Turing Test through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11373v1)
- **Published**: 2020-06-01 10:11:42+00:00
- **Updated**: 2020-06-01 10:11:42+00:00
- **Authors**: Jimut Bahan Pal
- **Comment**: Masters thesis. All Text CAPTCHAs are broken with over 99% accuracy,
  hence they are proved to be unreliable
- **Journal**: None
- **Summary**: It is increasingly becoming difficult for human beings to work on their day to day life without going through the process of reverse Turing test, where the Computers tests the users to be humans or not. Almost every website and service providers today have the process of checking whether their website is being crawled or not by automated bots which could extract valuable information from their site. In the process the bots are getting more intelligent by the use of Deep Learning techniques to decipher those tests and gain unwanted automated access to data while create nuisance by posting spam. Humans spend a considerable amount of time almost every day when trying to decipher CAPTCHAs. The aim of this investigation is to check whether the use of a subset of commonly used CAPTCHAs, known as the text CAPTCHA is a reliable process for verifying their human customers. We mainly focused on the preprocessing step for every CAPTCHA which converts them in binary intensity and removes the confusion as much as possible and developed various models to correctly label as many CAPTCHAs as possible. We also suggested some ways to improve the process of verifying the humans which makes it easy for humans to solve the existing CAPTCHAs and difficult for bots to do the same.



### Temporal Aggregate Representations for Long-Range Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2006.00830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00830v2)
- **Published**: 2020-06-01 10:17:55+00:00
- **Updated**: 2020-07-30 23:33:43+00:00
- **Authors**: Fadime Sener, Dipika Singhania, Angela Yao
- **Comment**: ECCV 2020, European Conference on Computer Vision
- **Journal**: None
- **Summary**: Future prediction, especially in long-range videos, requires reasoning from current and past observations. In this work, we address questions of temporal extent, scaling, and level of semantic abstraction with a flexible multi-granular temporal aggregation framework. We show that it is possible to achieve state of the art in both next action and dense anticipation with simple techniques such as max-pooling and attention. To demonstrate the anticipation capabilities of our model, we conduct experiments on Breakfast, 50Salads, and EPIC-Kitchens datasets, where we achieve state-of-the-art results. With minimal modifications, our model can also be extended for video segmentation and action recognition.



### Multi-scale Cloud Detection in Remote Sensing Images using a Dual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2006.00836v1
- **DOI**: 10.1109/TGRS.2020.3015272
- **Categories**: **cs.CV**, cs.LG, cs.NE, I.2.6; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.00836v1)
- **Published**: 2020-06-01 10:27:42+00:00
- **Updated**: 2020-06-01 10:27:42+00:00
- **Authors**: Markku Luotamo, Sari Metsämäki, Arto Klami
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation by convolutional neural networks (CNN) has advanced the state of the art in pixel-level classification of remote sensing images. However, processing large images typically requires analyzing the image in small patches, and hence features that have large spatial extent still cause challenges in tasks such as cloud masking. To support a wider scale of spatial features while simultaneously reducing computational requirements for large satellite images, we propose an architecture of two cascaded CNN model components successively processing undersampled and full resolution images. The first component distinguishes between patches in the inner cloud area from patches at the cloud's boundary region. For the cloud-ambiguous edge patches requiring further segmentation, the framework then delegates computation to a fine-grained model component. We apply the architecture to a cloud detection dataset of complete Sentinel-2 multispectral images, approximately annotated for minimal false negatives in a land use application. On this specific task and data, we achieve a 16\% relative improvement in pixel accuracy over a CNN baseline based on patching.



### LFTag: A Scalable Visual Fiducial System with Low Spatial Frequency
- **Arxiv ID**: http://arxiv.org/abs/2006.00842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00842v1)
- **Published**: 2020-06-01 10:34:34+00:00
- **Updated**: 2020-06-01 10:34:34+00:00
- **Authors**: Ben Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual fiducial systems are a key component of many robotics and AR/VR applications for 6-DOF monocular relative pose estimation and target identification. This paper presents LFTag, a visual fiducial system based on topological detection and relative position data encoding which optimizes data density within spatial frequency constraints. The marker is constructed to resolve rotational ambiguity, which combined with the robust geometric and topological false positive rejection, allows all marker bits to be used for data.   When compared to existing state-of-the-art square binary markers (AprilTag) and topological markers (TopoTag) in simulation, the proposed fiducial system (LFTag) offers significant advances in dictionary size and range. LFTag 3x3 achieves 546 times the dictionary size of AprilTag 25h9 and LFTag 4x4 achieves 126 thousand times the dictionary size of AprilTag 41h12 while simultaneously achieving longer detection range. LFTag 3x3 also achieves more than twice the detection range of TopoTag 4x4 at the same dictionary size.



### 3D Lidar Mapping Relative Accuracy Automatic Evaluation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2006.00857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.00857v1)
- **Published**: 2020-06-01 11:30:31+00:00
- **Updated**: 2020-06-01 11:30:31+00:00
- **Authors**: Guibin Chen, Jiong Deng, Dongze Huang, Shuo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: HD (High Definition) map based on 3D lidar plays a vital role in autonomous vehicle localization, planning, decision-making, perception, etc. Many 3D lidar mapping technologies related to SLAM (Simultaneous Localization and Mapping) are used in HD map construction to ensure its high accuracy. To evaluate the accuracy of 3D lidar mapping, the most common methods use ground truth of poses to calculate the error between estimated poses and ground truth, however it's usually so difficult to get the ground truth of poses in the actual lidar mapping for autonomous vehicle. In this paper, we proposed a relative accuracy evaluation algorithm that can automatically evaluate the accuracy of HD map built by 3D lidar mapping without ground truth. A method for detecting the degree of ghosting in point cloud map quantitatively is designed to reflect the accuracy indirectly, which takes advantage of the principle of light traveling in a straight line and the fact that light can not penetrate opaque objects. Our experimental results confirm that the proposed evaluation algorithm can automatically and efficiently detect the bad poses whose accuracy are less than the set threshold such as 0.1m, then calculate the bad poses percentage P_bad in all estimated poses to obtain the final accuracy metric P_acc = 1 - P_bad.



### Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2006.00878v5
- **DOI**: 10.1109/TIP.2020.3045261
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00878v5)
- **Published**: 2020-06-01 12:26:08+00:00
- **Updated**: 2020-12-29 08:36:53+00:00
- **Authors**: Hanrong Ye, Hong Liu, Fanyang Meng, Xia Li
- **Comment**: First Submission: April 2019. The final revision accepted by the IEEE
  Transactions on Image Processing in December 2020
- **Journal**: None
- **Summary**: RGB-Infrared person re-identification (RGB-IR Re- ID) is a cross-modality matching problem, where the modality discrepancy is a big challenge. Most existing works use Euclidean metric based constraints to resolve the discrepancy between features of images from different modalities. However, these methods are incapable of learning angularly discriminative feature embedding because Euclidean distance cannot measure the included angle between embedding vectors effectively. As an angularly discriminative feature space is important for classifying the human images based on their embedding vectors, in this paper, we propose a novel ranking loss function, named Bi-directional Exponential Angular Triplet Loss, to help learn an angularly separable common feature space by explicitly constraining the included angles between embedding vectors. Moreover, to help stabilize and learn the magnitudes of embedding vectors, we adopt a common space batch normalization layer. The quantitative and qualitative experiments on the SYSU-MM01 and RegDB dataset support our analysis. On SYSU-MM01 dataset, the performance is improved from 7.40% / 11.46% to 38.57% / 38.61% for rank-1 accuracy / mAP compared with the baseline. The proposed method can be generalized to the task of single-modality Re-ID and improves the rank-1 accuracy / mAP from 92.0% / 81.7% to 94.7% / 86.6% on the Market-1501 dataset, from 82.6% / 70.6% to 87.6% / 77.1% on the DukeMTMC-reID dataset. Code: https://github.com/prismformore/expAT



### Implementing AI-powered semantic character recognition in motor racing sports
- **Arxiv ID**: http://arxiv.org/abs/2006.00904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00904v1)
- **Published**: 2020-06-01 12:59:56+00:00
- **Updated**: 2020-06-01 12:59:56+00:00
- **Authors**: Jose David Fernández Rodríguez, David Daniel Albarracín Molina, Jesús Hormigo Cebolla
- **Comment**: 8 pages, 7 figures, 2020 NAB Broadcast Engineering and Information
  Technology (BEIT) Conference
- **Journal**: None
- **Summary**: Oftentimes TV producers of motor-racing programs overlay visual and textual media to provide on-screen context about drivers, such as a driver's name, position or photo. Typically this is accomplished by a human producer who visually identifies the drivers on screen, manually toggling the contextual media associated to each one and coordinating with cameramen and other TV producers to keep the racer in the shot while the contextual media is on screen. This labor-intensive and highly dedicated process is mostly suited to static overlays and makes it difficult to overlay contextual information about many drivers at the same time in short shots. This paper presents a system that largely automates these tasks and enables dynamic overlays using deep learning to track the drivers as they move on screen, without human intervention. This system is not merely theoretical, but an implementation has already been deployed during live races by a TV production company at Formula E races. We present the challenges faced during the implementation and discuss the implications. Additionally, we cover future applications and roadmap of this new technological development.



### A Survey on Deep Learning Techniques for Stereo-based Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.02535v1
- **DOI**: 10.1109/TPAMI.2020.3032602
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.02535v1)
- **Published**: 2020-06-01 13:09:46+00:00
- **Updated**: 2020-06-01 13:09:46+00:00
- **Authors**: Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, Mohammed Bennamoun
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2020
- **Summary**: Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. Among the existing techniques, stereo matching remains one of the most widely used in the literature due to its strong connection to the human binocular system. Traditionally, stereo-based depth estimation has been addressed through matching hand-crafted features across multiple images. Despite the extensive amount of research, these traditional techniques still suffer in the presence of highly textured areas, large uniform regions, and occlusions. Motivated by their growing success in solving various 2D and 3D vision problems, deep learning for stereo-based depth estimation has attracted growing interest from the community, with more than 150 papers published in this area between 2014 and 2019. This new generation of methods has demonstrated a significant leap in performance, enabling applications such as autonomous driving and augmented reality. In this article, we provide a comprehensive survey of this new and continuously growing field of research, summarize the most commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we also conjecture what the future may hold for deep learning-based stereo for depth estimation research.



### Multimodal grid features and cell pointers for Scene Text Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2006.00923v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.00923v2)
- **Published**: 2020-06-01 13:17:44+00:00
- **Updated**: 2020-06-25 10:47:17+00:00
- **Authors**: Lluís Gómez, Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Marçal Rusiñol, Ernest Valveny, Dimosthenis Karatzas
- **Comment**: This paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: This paper presents a new model for the task of scene text visual question answering, in which questions about a given image can only be answered by reading and understanding scene text that is present in it. The proposed model is based on an attention mechanism that attends to multi-modal features conditioned to the question, allowing it to reason jointly about the textual and visual modalities in the scene. The output weights of this attention module over the grid of multi-modal spatial features are interpreted as the probability that a certain spatial location of the image contains the answer text the to the given question. Our experiments demonstrate competitive performance in two standard datasets. Furthermore, this paper provides a novel analysis of the ST-VQA dataset based on a human performance study.



### One Versus all for deep Neural Network Incertitude (OVNNI) quantification
- **Arxiv ID**: http://arxiv.org/abs/2006.00954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.00954v1)
- **Published**: 2020-06-01 14:06:12+00:00
- **Updated**: 2020-06-01 14:06:12+00:00
- **Authors**: Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, Isabelle Bloch
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are powerful learning models yet their results are not always reliable. This is due to the fact that modern DNNs are usually uncalibrated and we cannot characterize their epistemic uncertainty. In this work, we propose a new technique to quantify the epistemic uncertainty of data easily. This method consists in mixing the predictions of an ensemble of DNNs trained to classify One class vs All the other classes (OVA) with predictions from a standard DNN trained to perform All vs All (AVA) classification. On the one hand, the adjustment provided by the AVA DNN to the score of the base classifiers allows for a more fine-grained inter-class separation. On the other hand, the two types of classifiers enforce mutually their detection of out-of-distribution (OOD) samples, circumventing entirely the requirement of using such samples during training. Our method achieves state of the art performance in quantifying OOD data across multiple datasets and architectures while requiring little hyper-parameter tuning.



### A Comprehensive Study of Data Augmentation Strategies for Prostate Cancer Detection in Diffusion-weighted MRI using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.01693v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01693v1)
- **Published**: 2020-06-01 14:31:38+00:00
- **Updated**: 2020-06-01 14:31:38+00:00
- **Authors**: Ruqian Hao, Khashayar Namdar, Lin Liu, Masoom A. Haider, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation refers to a group of techniques whose goal is to battle limited amount of available data to improve model generalization and push sample distribution toward the true distribution. While different augmentation strategies and their combinations have been investigated for various computer vision tasks in the context of deep learning, a specific work in the domain of medical imaging is rare and to the best of our knowledge, there has been no dedicated work on exploring the effects of various augmentation methods on the performance of deep learning models in prostate cancer detection. In this work, we have statically applied five most frequently used augmentation techniques (random rotation, horizontal flip, vertical flip, random crop, and translation) to prostate Diffusion-weighted Magnetic Resonance Imaging training dataset of 217 patients separately and evaluated the effect of each method on the accuracy of prostate cancer detection. The augmentation algorithms were applied independently to each data channel and a shallow as well as a deep Convolutional Neural Network (CNN) were trained on the five augmented sets separately. We used Area Under Receiver Operating Characteristic (ROC) curve (AUC) to evaluate the performance of the trained CNNs on a separate test set of 95 patients, using a validation set of 102 patients for finetuning. The shallow network outperformed the deep network with the best 2D slice-based AUC of 0.85 obtained by the rotation method.



### An Online Platform for Automatic Skull Defect Restoration and Cranial Implant Design
- **Arxiv ID**: http://arxiv.org/abs/2006.00980v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.00980v1)
- **Published**: 2020-06-01 14:41:33+00:00
- **Updated**: 2020-06-01 14:41:33+00:00
- **Authors**: Jianning Li, Antonio Pepe, Christina Gsaxner, Jan Egger
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: We introduce a fully automatic system for cranial implant design, a common task in cranioplasty operations. The system is currently integrated in Studierfenster (http://studierfenster.tugraz.at/), an online, cloud-based medical image processing platform for medical imaging applications. Enhanced by deep learning algorithms, the system automatically restores the missing part of a skull (i.e., skull shape completion) and generates the desired implant by subtracting the defective skull from the completed skull. The generated implant can be downloaded in the STereoLithography (.stl) format directly via the browser interface of the system. The implant model can then be sent to a 3D printer for in loco implant manufacturing. Furthermore, thanks to the standard format, the user can thereafter load the model into another application for post-processing whenever necessary. Such an automatic cranial implant design system can be integrated into the clinical practice to improve the current routine for surgeries related to skull defect repair (e.g., cranioplasty). Our system, although currently intended for educational and research use only, can be seen as an application of additive manufacturing for fast, patient-specific implant design.



### PlenoptiSign: an optical design tool for plenoptic imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.01015v1
- **DOI**: 10.1016/j.softx.2019.100259
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01015v1)
- **Published**: 2020-06-01 15:21:44+00:00
- **Updated**: 2020-06-01 15:21:44+00:00
- **Authors**: Christopher Hahne, Amar Aggoun
- **Comment**: https://github.com/hahnec/plenoptisign/
- **Journal**: None
- **Summary**: Plenoptic imaging enables a light-field to be captured by a single monocular objective lens and an array of micro lenses attached to an image sensor. Metric distances of the light-field's depth planes remain unapparent prior to acquisition. Recent research showed that sampled depth locations rely on the parameters of the system's optical components. This paper presents PlenoptiSign, which implements these findings as a Python software package to help assist in an experimental or prototyping stage of a plenoptic system.



### GoodPoint: unsupervised learning of keypoint detection and description
- **Arxiv ID**: http://arxiv.org/abs/2006.01030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01030v1)
- **Published**: 2020-06-01 15:57:30+00:00
- **Updated**: 2020-06-01 15:57:30+00:00
- **Authors**: Anatoly Belikov, Alexey Potapov
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new algorithm for unsupervised learning of keypoint detectors and descriptors, which demonstrates fast convergence and good performance across different datasets. The training procedure uses homographic transformation of images. The proposed model learns to detect points and generate descriptors on pairs of transformed images, which are easy for it to distinguish and repeatedly detect. The trained model follows SuperPoint architecture for ease of comparison, and demonstrates similar performance on natural images from HPatches dataset, and better performance on retina images from Fundus Image Registration Dataset, which contain low number of corner-like features. For HPatches and other datasets, coverage was also computed to provide better estimation of model quality.



### A Smooth Representation of Belief over SO(3) for Deep Rotation Learning with Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2006.01031v4
- **DOI**: 10.15607/RSS.2020.XVI.007
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01031v4)
- **Published**: 2020-06-01 15:57:45+00:00
- **Updated**: 2021-01-17 19:47:56+00:00
- **Authors**: Valentin Peretroukhin, Matthew Giamou, David M. Rosen, W. Nicholas Greene, Nicholas Roy, Jonathan Kelly
- **Comment**: In Proceedings of Robotics: Science and Systems (RSS'20), Corvallis ,
  Oregon, USA, Jul. 12-16, 2020
- **Journal**: None
- **Summary**: Accurate rotation estimation is at the heart of robot perception tasks such as visual odometry and object pose estimation. Deep neural networks have provided a new way to perform these tasks, and the choice of rotation representation is an important part of network design. In this work, we present a novel symmetric matrix representation of the 3D rotation group, SO(3), with two important properties that make it particularly suitable for learned models: (1) it satisfies a smoothness property that improves convergence and generalization when regressing large rotation targets, and (2) it encodes a symmetric Bingham belief over the space of unit quaternions, permitting the training of uncertainty-aware models. We empirically validate the benefits of our formulation by training deep neural rotation regressors on two data modalities. First, we use synthetic point-cloud data to show that our representation leads to superior predictive accuracy over existing representations for arbitrary rotation targets. Second, we use image data collected onboard ground and aerial vehicles to demonstrate that our representation is amenable to an effective out-of-distribution (OOD) rejection technique that significantly improves the robustness of rotation estimates to unseen environmental effects and corrupted input images, without requiring the use of an explicit likelihood loss, stochastic sampling, or an auxiliary classifier. This capability is key for safety-critical applications where detecting novel inputs can prevent catastrophic failure of learned models.



### Deep Generation of Face Images from Sketches
- **Arxiv ID**: http://arxiv.org/abs/2006.01047v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01047v2)
- **Published**: 2020-06-01 16:20:23+00:00
- **Updated**: 2020-06-05 02:37:46+00:00
- **Authors**: Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, Hongbo Fu
- **Comment**: Accepted to Siggraph 2020
- **Journal**: None
- **Summary**: Recent deep image-to-image translation techniques allow fast generation of face images from freehand sketches. However, existing solutions tend to overfit to sketches, thus requiring professional sketches or even edge maps as input. To address this issue, our key idea is to implicitly model the shape space of plausible face images and synthesize a face image in this space to approximate an input sketch. We take a local-to-global approach. We first learn feature embeddings of key face components, and push corresponding parts of input sketches towards underlying component manifolds defined by the feature vectors of face component samples. We also propose another deep neural network to learn the mapping from the embedded component features to realistic images with multi-channel feature maps as intermediate results to improve the information flow. Our method essentially uses input sketches as soft constraints and is thus able to produce high-quality face images even from rough and/or incomplete sketches. Our tool is easy to use even for non-artists, while still supporting fine-grained control of shape details. Both qualitative and quantitative evaluations show the superior generation ability of our system to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.



### DPDnet: A Robust People Detector using Deep Learning with an Overhead Depth Camera
- **Arxiv ID**: http://arxiv.org/abs/2006.01053v1
- **DOI**: 10.1016/j.eswa.2019.113168
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01053v1)
- **Published**: 2020-06-01 16:28:25+00:00
- **Updated**: 2020-06-01 16:28:25+00:00
- **Authors**: David Fuentes-Jimenez, Roberto Martin-Lopez, Cristina Losada-Gutierrez, David Casillas-Perez, Javier Macias-Guarasa, Daniel Pizarro, Carlos A. Luna
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a method based on deep learning that detects multiple people from a single overhead depth image with high reliability. Our neural network, called DPDnet, is based on two fully-convolutional encoder-decoder neural blocks based on residual layers. The Main Block takes a depth image as input and generates a pixel-wise confidence map, where each detected person in the image is represented by a Gaussian-like distribution. The refinement block combines the depth image and the output from the main block, to refine the confidence map. Both blocks are simultaneously trained end-to-end using depth images and head position labels. The experimental work shows that DPDNet outperforms state-of-the-art methods, with accuracies greater than 99% in three different publicly available datasets, without retraining not fine-tuning. In addition, the computational complexity of our proposal is independent of the number of people in the scene and runs in real time using conventional GPUs.



### BIMCV COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients
- **Arxiv ID**: http://arxiv.org/abs/2006.01174v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 92B20, 92C50, 68T50, 92B10
- **Links**: [PDF](http://arxiv.org/pdf/2006.01174v3)
- **Published**: 2020-06-01 18:06:21+00:00
- **Updated**: 2020-06-05 12:53:43+00:00
- **Authors**: Maria de la Iglesia Vayá, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant, Xavier Barber, Domingo Orozco-Beltrán, Francisco García-García, Marisa Caparrós, Germán González, Jose María Salinas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes BIMCV COVID-19+, a large dataset from the Valencian Region Medical ImageBank (BIMCV) containing chest X-ray images CXR (CR, DX) and computed tomography (CT) imaging of COVID-19+ patients along with their radiological findings and locations, pathologies, radiological reports (in Spanish), DICOM metadata, Polymerase chain reaction (PCR), Immunoglobulin G (IgG) and Immunoglobulin M (IgM) diagnostic antibody tests. The findings have been mapped onto standard Unified Medical Language System (UMLS) terminology and cover a wide spectrum of thoracic entities, unlike the considerably more reduced number of entities annotated in previous datasets. Images are stored in high resolution and entities are localized with anatomical labels and stored in a Medical Imaging Data Structure (MIDS) format. In addition, 10 images were annotated by a team of radiologists to include semantic segmentation of radiological findings. This first iteration of the database includes 1,380 CX, 885 DX and 163 CT studies from 1,311 COVID-19+ patients. This is, to the best of our knowledge, the largest COVID-19+ dataset of images available in an open format. The dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/bimcv-covid19.



### High-quality Panorama Stitching based on Asymmetric Bidirectional Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2006.01201v3
- **DOI**: 10.1109/ICCIA49625.2020.00030
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01201v3)
- **Published**: 2020-06-01 18:54:11+00:00
- **Updated**: 2020-08-29 00:35:50+00:00
- **Authors**: Mingyuan Meng, Shaojun Liu
- **Comment**: Published at the 5th International Conference on Computational
  Intelligence and Applications (ICCIA 2020)
- **Journal**: 2020 5th International Conference on Computational Intelligence
  and Applications (ICCIA), Beijing, China, 2020, pp. 118-122
- **Summary**: In this paper, we propose a panorama stitching algorithm based on asymmetric bidirectional optical flow. This algorithm expects multiple photos captured by fisheye lens cameras as input, and then, through the proposed algorithm, these photos can be merged into a high-quality 360-degree spherical panoramic image. For photos taken from a distant perspective, the parallax among them is relatively small, and the obtained panoramic image can be nearly seamless and undistorted. For photos taken from a close perspective or with a relatively large parallax, a seamless though partially distorted panoramic image can also be obtained. Besides, with the help of Graphics Processing Unit (GPU), this algorithm can complete the whole stitching process at a very fast speed: typically, it only takes less than 30s to obtain a panoramic image of 9000-by-4000 pixels, which means our panorama stitching algorithm is of high value in many real-time applications. Our code is available at https://github.com/MungoMeng/Panorama-OpticalFlow.



### An embedded system for the automated generation of labeled plant images to enable machine learning applications in agriculture
- **Arxiv ID**: http://arxiv.org/abs/2006.01228v2
- **DOI**: 10.1371/journal.pone.0243923
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.01228v2)
- **Published**: 2020-06-01 20:01:20+00:00
- **Updated**: 2021-04-01 19:50:14+00:00
- **Authors**: Michael A. Beck, Chen-Yi Liu, Christopher P. Bidinosti, Christopher J. Henry, Cara M. Godee, Manisha Ajmani
- **Comment**: 35 pages, 8 figures, Preprint submitted to PLoS One
- **Journal**: None
- **Summary**: A lack of sufficient training data, both in terms of variety and quantity, is often the bottleneck in the development of machine learning (ML) applications in any domain. For agricultural applications, ML-based models designed to perform tasks such as autonomous plant classification will typically be coupled to just one or perhaps a few plant species. As a consequence, each crop-specific task is very likely to require its own specialized training data, and the question of how to serve this need for data now often overshadows the more routine exercise of actually training such models. To tackle this problem, we have developed an embedded robotic system to automatically generate and label large datasets of plant images for ML applications in agriculture. The system can image plants from virtually any angle, thereby ensuring a wide variety of data; and with an imaging rate of up to one image per second, it can produce lableled datasets on the scale of thousands to tens of thousands of images per day. As such, this system offers an important alternative to time- and cost-intensive methods of manual generation and labeling. Furthermore, the use of a uniform background made of blue keying fabric enables additional image processing techniques such as background replacement and plant segmentation. It also helps in the training process, essentially forcing the model to focus on the plant features and eliminating random correlations. To demonstrate the capabilities of our system, we generated a dataset of over 34,000 labeled images, with which we trained an ML-model to distinguish grasses from non-grasses in test data from a variety of sources. We now plan to generate much larger datasets of Canadian crop plants and weeds that will be made publicly available in the hope of further enabling ML applications in the agriculture sector.



### BWCNN: Blink to Word, a Real-Time Convolutional Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.01232v1
- **DOI**: 10.1007/978-3-030-59615-6_10
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01232v1)
- **Published**: 2020-06-01 20:07:44+00:00
- **Updated**: 2020-06-01 20:07:44+00:00
- **Authors**: Albara Ah Ramli, Rex Liu, Rahul Krishnamoorthy, Vishal I B, Xiaoxiao Wang, Ilias Tagkopoulos, Xin Liu
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative disease of the brain and the spinal cord, which leads to paralysis of motor functions. Patients retain their ability to blink, which can be used for communication. Here, We present an Artificial Intelligence (AI) system that uses eye-blinks to communicate with the outside world, running on real-time Internet-of-Things (IoT) devices. The system uses a Convolutional Neural Network (CNN) to find the blinking pattern, which is defined as a series of Open and Closed states. Each pattern is mapped to a collection of words that manifest the patient's intent. To investigate the best trade-off between accuracy and latency, we investigated several Convolutional Network architectures, such as ResNet, SqueezeNet, DenseNet, and InceptionV3, and evaluated their performance. We found that the InceptionV3 architecture, after hyper-parameter fine-tuning on the specific task led to the best performance with an accuracy of 99.20% and 94ms latency. This work demonstrates how the latest advances in deep learning architectures can be adapted for clinical systems that ameliorate the patient's quality of life regardless of the point-of-care.



### A comparative study of 2D image segmentation algorithms for traumatic brain lesions using CT data from the ProTECTIII multicenter clinical trial
- **Arxiv ID**: http://arxiv.org/abs/2006.01263v1
- **DOI**: 10.1117/12.2566332
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01263v1)
- **Published**: 2020-06-01 21:00:20+00:00
- **Updated**: 2020-06-01 21:00:20+00:00
- **Authors**: Shruti Jadon, Owen P. Leary, Ian Pan, Tyler J. Harder, David W. Wright, Lisa H. Merck, Derek L. Merck
- **Comment**: 9 pages, 3 figures, 3 tables
- **Journal**: SPIE MEDICAL IMAGING 2020
- **Summary**: Automated segmentation of medical imaging is of broad interest to clinicians and machine learning researchers alike. The goal of segmentation is to increase efficiency and simplicity of visualization and quantification of regions of interest within a medical image. Image segmentation is a difficult task because of multiparametric heterogeneity within the images, an obstacle that has proven especially challenging in efforts to automate the segmentation of brain lesions from non-contrast head computed tomography (CT). In this research, we have experimented with multiple available deep learning architectures to segment different phenotypes of hemorrhagic lesions found after moderate to severe traumatic brain injury (TBI). These include: intraparenchymal hemorrhage (IPH), subdural hematoma (SDH), epidural hematoma (EDH), and traumatic contusions. We were able to achieve an optimal Dice Coefficient1 score of 0.94 using UNet++ 2D Architecture with Focal Tversky Loss Function, an increase from 0.85 using UNet 2D with Binary Cross-Entropy Loss Function in intraparenchymal hemorrhage (IPH) cases. Furthermore, using the same setting, we were able to achieve the Dice Coefficient score of 0.90 and 0.86 in cases of Extra-Axial bleeds and Traumatic contusions, respectively.



### Fusion of Real Time Thermal Image and 1D/2D/3D Depth Laser Readings for Remote Thermal Sensing in Industrial Plants by Means of UAVs and/or Robots
- **Arxiv ID**: http://arxiv.org/abs/2006.01286v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2006.01286v3)
- **Published**: 2020-06-01 21:52:39+00:00
- **Updated**: 2020-06-04 10:22:23+00:00
- **Authors**: Corneliu Arsene
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents fast procedures for thermal infrared remote sensing in dark, GPS-denied environments, such as those found in industrial plants such as in High-Voltage Direct Current (HVDC) converter stations. These procedures are based on the combination of the depth estimation obtained from either a 1-Dimensional LIDAR laser or a 2-Dimensional Hokuyo laser or a 3D MultiSense SLB laser sensor and the visible and thermal cameras from a FLIR Duo R dual-sensor thermal camera. The combination of these sensors/cameras is suitable to be mounted on Unmanned Aerial Vehicles (UAVs) and/or robots in order to provide reliable information about the potential malfunctions, which can be found within the hazardous environment. For example, the capabilities of the developed software and hardware system corresponding to the combination of the 1-D LIDAR sensor and the FLIR Duo R dual-sensor thermal camera is assessed from the point of the accuracy of results and the required computational times: the obtained computational times are under 10 ms, with a maximum localization error of 8 mm and an average standard deviation for the measured temperatures of 1.11 degree Celsius, which results are obtained for a number of test cases. The paper is structured as follows: the description of the system used for identification and localization of hotspots in industrial plants is presented in section II. In section III, the method for faults identification and localization in plants by using a 1-Dimensional LIDAR laser sensor and thermal images is described together with results. In section IV the real time thermal image processing is presented. Fusion of the 2-Dimensional depth laser Hokuyo and the thermal images is described in section V. In section VI the combination of the 3D MultiSense SLB laser and thermal images is described. In section VII a discussion and several conclusions are drawn.



### Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019
- **Arxiv ID**: http://arxiv.org/abs/2006.01310v1
- **DOI**: 10.5120/ijca2020920243
- **Categories**: **cs.HC**, cs.CV, cs.DL, cs.LG, A.1; H.5; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2006.01310v1)
- **Published**: 2020-06-01 23:14:10+00:00
- **Updated**: 2020-06-01 23:14:10+00:00
- **Authors**: Antonio Ricardo Alexandre Brasil, Jefferson Oliveira Andrade, Karin Satie Komati
- **Comment**: 9 pages, 2 figures, journal
- **Journal**: International Journal of Computer Applications 176(24):1-9, May
  2020
- **Summary**: Person identification based on eye movements is getting more and more attention, as it is anti-spoofing resistant and can be useful for continuous authentication. Therefore, it is noteworthy for researchers to know who and what is relevant in the field, including authors, journals, conferences, and institutions. This paper presents a comprehensive quantitative overview of the field of eye movement biometrics using a bibliometric approach. All data and analyses are based on documents written in English published between 2004 and 2019. Scopus was used to perform information retrieval. This research focused on temporal evolution, leading authors, most cited papers, leading journals, competitions and collaboration networks.



### Multi-view Deep Features for Robust Facial Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2006.01315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01315v1)
- **Published**: 2020-06-01 23:33:18+00:00
- **Updated**: 2020-06-01 23:33:18+00:00
- **Authors**: Oualid Laiadi, Abdelmalik Ouamane, Abdelhamid Benakcha, Abdelmalik Taleb-Ahmed, Abdenour Hadid
- **Comment**: Will appear as part of RFIW2020 in the Proceedings of 2020
  International Conference on Automatic Face and Gesture Recognition (IEEE
  AMFG)
- **Journal**: None
- **Summary**: Automatic kinship verification from facial images is an emerging research topic in machine learning community. In this paper, we proposed an effective facial features extraction model based on multi-view deep features. Thus, we used four pre-trained deep learning models using eight features layers (FC6 and FC7 layers of each VGG-F, VGG-M, VGG-S and VGG-Face models) to train the proposed Multilinear Side-Information based Discriminant Analysis integrating Within Class Covariance Normalization (MSIDA+WCCN) method. Furthermore, we show that how can metric learning methods based on WCCN method integration improves the Simple Scoring Cosine similarity (SSC) method. We refer that we used the SSC method in RFIW'20 competition using the eight deep features concatenation. Thus, the integration of WCCN in the metric learning methods decreases the intra-class variations effect introduced by the deep features weights. We evaluate our proposed method on two kinship benchmarks namely KinFaceW-I and KinFaceW-II databases using four Parent-Child relations (Father-Son, Father-Daughter, Mother-Son and Mother-Daughter). Thus, the proposed MSIDA+WCCN method improves the SSC method with 12.80% and 14.65% on KinFaceW-I and KinFaceW-II databases, respectively. The results obtained are positively compared with some modern methods, including those that rely on deep learning.



### Two-hand Global 3D Pose Estimation Using Monocular RGB
- **Arxiv ID**: http://arxiv.org/abs/2006.01320v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01320v4)
- **Published**: 2020-06-01 23:53:52+00:00
- **Updated**: 2020-08-25 09:54:24+00:00
- **Authors**: Fanqing Lin, Connor Wilhelm, Tony Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the challenging task of estimating global 3D joint locations for both hands via only monocular RGB input images. We propose a novel multi-stage convolutional neural network based pipeline that accurately segments and locates the hands despite occlusion between two hands and complex background noise and estimates the 2D and 3D canonical joint locations without any depth information. Global joint locations with respect to the camera origin are computed using the hand pose estimations and the actual length of the key bone with a novel projection algorithm. To train the CNNs for this new task, we introduce a large-scale synthetic 3D hand pose dataset. We demonstrate that our system outperforms previous works on 3D canonical hand pose estimation benchmark datasets with RGB-only information. Additionally, we present the first work that achieves accurate global 3D hand tracking on both hands using RGB-only inputs and provide extensive quantitative and qualitative evaluation.




# Arxiv Papers in cs.CV on 2020-06-07
### Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples
- **Arxiv ID**: http://arxiv.org/abs/2006.04005v3
- **DOI**: 10.1109/TNNLS.2021.3112897
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04005v3)
- **Published**: 2020-06-07 00:34:57+00:00
- **Updated**: 2021-08-04 18:30:05+00:00
- **Authors**: David Macêdo, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I. Oliveira, Teresa Ludermir
- **Comment**: Accepted for publication in the IEEE Transactions on Neural Networks
  and Learning Systems: Special Issue on Deep Learning for Anomaly Detection
- **Journal**: None
- **Summary**: In this paper, we argue that the unsatisfactory out-of-distribution (OOD) detection performance of neural networks is mainly due to the SoftMax loss anisotropy and propensity to produce low entropy probability distributions in disagreement with the principle of maximum entropy. Current out-of-distribution (OOD) detection approaches usually do not directly fix the SoftMax loss drawbacks, but rather build techniques to circumvent it. Unfortunately, those methods usually produce undesired side effects (e.g., classification accuracy drop, additional hyperparameters, slower inferences, and collecting extra data). In the opposite direction, we propose replacing SoftMax loss with a novel loss function that does not suffer from the mentioned weaknesses. The proposed IsoMax loss is isotropic (exclusively distance-based) and provides high entropy posterior probability distributions. Replacing the SoftMax loss by IsoMax loss requires no model or training changes. Additionally, the models trained with IsoMax loss produce as fast and energy-efficient inferences as those trained using SoftMax loss. Moreover, no classification accuracy drop is observed. The proposed method does not rely on outlier/background data, hyperparameter tuning, temperature calibration, feature extraction, metric learning, adversarial training, ensemble procedures, or generative models. Our experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in replacement that significantly improves neural networks' OOD detection performance. Hence, it may be used as a baseline OOD detection approach to be combined with current or future OOD detection techniques to achieve even higher results.



### SharinGAN: Combining Synthetic and Real Data for Unsupervised Geometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.04026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04026v1)
- **Published**: 2020-06-07 02:45:33+00:00
- **Updated**: 2020-06-07 02:45:33+00:00
- **Authors**: Koutilya PNVR, Hao Zhou, David Jacobs
- **Comment**: Accepted to CVPR 2020. Supplementary material added towards the end
  instead of a separate file. A Github link to the code is also provided in
  this submission
- **Journal**: None
- **Summary**: We propose a novel method for combining synthetic and real images when training networks to determine geometric information from a single image. We suggest a method for mapping both image types into a single, shared domain. This is connected to a primary network for end-to-end training. Ideally, this results in images from two domains that present shared information to the primary network. Our experiments demonstrate significant improvements over the state-of-the-art in two important domains, surface normal estimation of human faces and monocular depth estimation for outdoor scenes, both in an unsupervised setting.



### A Comparative Analysis of E-Scooter and E-Bike Usage Patterns: Findings from the City of Austin, TX
- **Arxiv ID**: http://arxiv.org/abs/2006.04033v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04033v1)
- **Published**: 2020-06-07 03:27:44+00:00
- **Updated**: 2020-06-07 03:27:44+00:00
- **Authors**: Mohammed Hamad Almannaa, Huthaifa I. Ashqar, Mohammed Elhenawy, Mahmoud Masoud, Andry Rakotonirainy, Hesham Rakha
- **Comment**: Submitted to the International Journal of Sustainable Transportation
- **Journal**: None
- **Summary**: E-scooter-sharing and e-bike-sharing systems are accommodating and easing the increased traffic in dense cities and are expanding considerably. However, these new micro-mobility transportation modes raise numerous operational and safety concerns. This study analyzes e-scooter and dockless e-bike sharing system user behavior. We investigate how average trip speed change depending on the day of the week and the time of the day. We used a dataset from the city of Austin, TX from December 2018 to May 2019. Our results generally show that the trip average speed for e-bikes ranges between 3.01 and 3.44 m/s, which is higher than that for e-scooters (2.19 to 2.78 m/s). Results also show a similar usage pattern for the average speed of e-bikes and e-scooters throughout the days of the week and a different usage pattern for the average speed of e-bikes and e-scooters over the hours of the day. We found that users tend to ride e-bikes and e-scooters with a slower average speed for recreational purposes compared to when they are ridden for commuting purposes. This study is a building block in this field, which serves as a first of its kind, and sheds the light of significant new understanding of this emerging class of shared-road users.



### SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2006.04043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04043v2)
- **Published**: 2020-06-07 05:01:06+00:00
- **Updated**: 2021-12-23 13:17:48+00:00
- **Authors**: Qingdong He, Zhengning Wang, Hao Zeng, Yi Zeng, Yijun Liu
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Accurate 3D object detection from point clouds has become a crucial component in autonomous driving. However, the volumetric representations and the projection methods in previous works fail to establish the relationships between the local point sets. In this paper, we propose Sparse Voxel-Graph Attention Network (SVGA-Net), a novel end-to-end trainable network which mainly contains voxel-graph module and sparse-to-dense regression module to achieve comparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net constructs the local complete graph within each divided 3D spherical voxel and global KNN graph through all voxels. The local and global graphs serve as the attention mechanism to enhance the extracted features. In addition, the novel sparse-to-dense regression module enhances the 3D box estimation accuracy through feature maps aggregation at different levels. Experiments on KITTI detection benchmark demonstrate the efficiency of extending the graph representation to 3D object detection and the proposed SVGA-Net can achieve decent detection accuracy.



### A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton
- **Arxiv ID**: http://arxiv.org/abs/2006.04045v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.DS, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04045v2)
- **Published**: 2020-06-07 05:18:50+00:00
- **Updated**: 2020-07-02 10:26:42+00:00
- **Authors**: Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang
- **Comment**: Accepted at ICML 2020
- **Journal**: None
- **Summary**: In recent years, a variety of gradient-based first-order methods have been developed to solve bi-level optimization problems for learning applications. However, theoretical guarantees of these existing approaches heavily rely on the simplification that for each fixed upper-level variable, the lower-level solution must be a singleton (a.k.a., Lower-Level Singleton, LLS). In this work, we first design a counter-example to illustrate the invalidation of such LLS condition. Then by formulating BLPs from the view point of optimistic bi-level and aggregating hierarchical objective information, we establish Bi-level Descent Aggregation (BDA), a flexible and modularized algorithmic framework for generic bi-level optimization. Theoretically, we derive a new methodology to prove the convergence of BDA without the LLS condition. Our investigations also demonstrate that BDA is indeed compatible to a verify of particular first-order computation modules. Additionally, as an interesting byproduct, we also improve these conventional first-order bi-level schemes (under the LLS simplification). Particularly, we establish their convergences with weaker assumptions. Extensive experiments justify our theoretical results and demonstrate the superiority of the proposed BDA for different tasks, including hyper-parameter optimization and meta learning.



### DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.04047v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.04047v3)
- **Published**: 2020-06-07 05:22:29+00:00
- **Updated**: 2021-07-09 20:06:40+00:00
- **Authors**: Shing Yan Loo, Syamsiah Mashohor, Sai Hong Tang, Hong Zhang
- **Comment**: Accepted to be published in the Proceedings of the 2021 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: In this paper, we propose a dense monocular SLAM system, named DeepRelativeFusion, that is capable to recover a globally consistent 3D structure. To this end, we use a visual SLAM algorithm to reliably recover the camera poses and semi-dense depth maps of the keyframes, and then use relative depth prediction to densify the semi-dense depth maps and refine the keyframe pose-graph. To improve the semi-dense depth maps, we propose an adaptive filtering scheme, which is a structure-preserving weighted average smoothing filter that takes into account the pixel intensity and depth of the neighbouring pixels, yielding substantial reconstruction accuracy gain in densification. To perform densification, we introduce two incremental improvements upon the energy minimization framework proposed by DeepFusion: (1) an improved cost function, and (2) the use of single-image relative depth prediction. After densification, we update the keyframes with two-view consistent optimized semi-dense and dense depth maps to improve pose-graph optimization, providing a feedback loop to refine the keyframe poses for accurate scene reconstruction. Our system outperforms the state-of-the-art dense SLAM systems quantitatively in dense reconstruction accuracy by a large margin.



### Facial Expression Recognition using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.04057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04057v1)
- **Published**: 2020-06-07 06:32:05+00:00
- **Updated**: 2020-06-07 06:32:05+00:00
- **Authors**: Raghu Vamshi. N, Bharathi Raja S
- **Comment**: None
- **Journal**: None
- **Summary**: Throughout the various ages, facial expressions have become one of the universal ways of non-verbal communication. The ability to recognize facial expressions would pave the path for many novel applications. Despite the success of traditional approaches in a controlled environment, these approaches fail on challenging datasets consisting of partial faces. In this paper, I take one such dataset FER-2013 and will implement deep learning models that are able to achieve significant improvement over the previously used traditional approaches and even some of the deep learning models.



### NITS-VC System for VATEX Video Captioning Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2006.04058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04058v2)
- **Published**: 2020-06-07 06:39:56+00:00
- **Updated**: 2020-09-25 14:05:13+00:00
- **Authors**: Alok Singh, Thoudam Doren Singh, Sivaji Bandyopadhyay
- **Comment**: Workshop on Language & Vision with applications to Video
  Understanding (LVVU 2020) - In conjunction with CVPR 2020
- **Journal**: None
- **Summary**: Video captioning is process of summarising the content, event and action of the video into a short textual form which can be helpful in many research areas such as video guided machine translation, video sentiment analysis and providing aid to needy individual. In this paper, a system description of the framework used for VATEX-2020 video captioning challenge is presented. We employ an encoder-decoder based approach in which the visual features of the video are encoded using 3D convolutional neural network (C3D) and in the decoding phase two Long Short Term Memory (LSTM) recurrent networks are used in which visual features and input captions are fused separately and final output is generated by performing element-wise product between the output of both LSTMs. Our model is able to achieve BLEU scores of 0.20 and 0.22 on public and private test data sets respectively.



### Siamese Keypoint Prediction Network for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.04078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04078v1)
- **Published**: 2020-06-07 08:11:06+00:00
- **Updated**: 2020-06-07 08:11:06+00:00
- **Authors**: Qiang Li, Zekui Qin, Wenbo Zhang, Wen Zheng
- **Comment**: Code: https://github.com/ZekuiQin/SiamKPN
- **Journal**: None
- **Summary**: Visual object tracking aims to estimate the location of an arbitrary target in a video sequence given its initial bounding box. By utilizing offline feature learning, the siamese paradigm has recently been the leading framework for high performance tracking. However, current existing siamese trackers either heavily rely on complicated anchor-based detection networks or lack the ability to resist to distractors. In this paper, we propose the Siamese keypoint prediction network (SiamKPN) to address these challenges. Upon a Siamese backbone for feature embedding, SiamKPN benefits from a cascade heatmap strategy for coarse-to-fine prediction modeling. In particular, the strategy is implemented by sequentially shrinking the coverage of the label heatmap along the cascade to apply loose-to-strict intermediate supervisions. During inference, we find the predicted heatmaps of successive stages to be gradually concentrated to the target and reduced to the distractors. SiamKPN performs well against state-of-the-art trackers for visual object tracking on four benchmark datasets including OTB-100, VOT2018, LaSOT and GOT-10k, while running at real-time speed.



### CubifAE-3D: Monocular Camera Space Cubification for Auto-Encoder based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.04080v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04080v2)
- **Published**: 2020-06-07 08:17:00+00:00
- **Updated**: 2021-01-26 16:29:45+00:00
- **Authors**: Shubham Shrivastava, Punarjay Chakravarty
- **Comment**: 12 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: We introduce a method for 3D object detection using a single monocular image. Starting from a synthetic dataset, we pre-train an RGB-to-Depth Auto-Encoder (AE). The embedding learnt from this AE is then used to train a 3D Object Detector (3DOD) CNN which is used to regress the parameters of 3D object poses after the encoder from the AE generates a latent embedding from the RGB image. We show that we can pre-train the AE using paired RGB and depth images from simulation data once and subsequently only train the 3DOD network using real data, comprising of RGB images and 3D object pose labels (without the requirement of dense depth). Our 3DOD network utilizes a particular `cubification' of 3D space around the camera, where each cuboid is tasked with predicting N object poses, along with their class and confidence values. The AE pre-training and this method of dividing the 3D space around the camera into cuboids give our method its name - CubifAE-3D. We demonstrate results for monocular 3D object detection in the Autonomous Vehicle (AV) use-case with the Virtual KITTI 2 and the KITTI datasets.



### End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera
- **Arxiv ID**: http://arxiv.org/abs/2006.04082v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.04082v2)
- **Published**: 2020-06-07 08:18:31+00:00
- **Updated**: 2020-06-09 07:40:51+00:00
- **Authors**: Zhenbo Song, Jianfeng Lu, Tong Zhang, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.



### Multi-view Contrastive Learning for Online Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2006.04093v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04093v3)
- **Published**: 2020-06-07 09:11:28+00:00
- **Updated**: 2021-04-10 08:49:09+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Yongjun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Previous Online Knowledge Distillation (OKD) often carries out mutually exchanging probability distributions, but neglects the useful representational knowledge. We therefore propose Multi-view Contrastive Learning (MCL) for OKD to implicitly capture correlations of feature embeddings encoded by multiple peer networks, which provide various views for understanding the input data instances. Benefiting from MCL, we can learn a more discriminative representation space for classification than previous OKD methods. Experimental results on image classification demonstrate that our MCL-OKD outperforms other state-of-the-art OKD methods by large margins without sacrificing additional inference cost. Codes are available at https://github.com/winycg/MCL-OKD.



### Robust Learning Through Cross-Task Consistency
- **Arxiv ID**: http://arxiv.org/abs/2006.04096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04096v1)
- **Published**: 2020-06-07 09:24:33+00:00
- **Updated**: 2020-06-07 09:24:33+00:00
- **Authors**: Amir Zamir, Alexander Sax, Teresa Yeo, Oğuzhan Kar, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, Leonidas Guibas
- **Comment**: CVPR 2020 (Oral). Project website, models, live demo at
  http://consistency.epfl.ch/
- **Journal**: None
- **Summary**: Visual perception entails solving a wide set of tasks, e.g., object detection, depth estimation, etc. The predictions made for multiple tasks from the same image are not independent, and therefore, are expected to be consistent. We propose a broadly applicable and fully computational method for augmenting learning with Cross-Task Consistency. The proposed formulation is based on inference-path invariance over a graph of arbitrary tasks. We observe that learning with cross-task consistency leads to more accurate predictions and better generalization to out-of-distribution inputs. This framework also leads to an informative unsupervised quantity, called Consistency Energy, based on measuring the intrinsic consistency of the system. Consistency Energy correlates well with the supervised error (r=0.67), thus it can be employed as an unsupervised confidence metric as well as for detection of out-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on multiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and they benchmark cross-task consistency versus various baselines including conventional multi-task learning, cycle consistency, and analytical consistency.



### DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling
- **Arxiv ID**: http://arxiv.org/abs/2006.04115v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04115v2)
- **Published**: 2020-06-07 11:08:37+00:00
- **Updated**: 2020-10-22 15:36:52+00:00
- **Authors**: Moshe Eliasof, Eran Treister
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have shown to be effective in handling unordered data like point clouds and meshes. In this work we propose novel approaches for graph convolution, pooling and unpooling, inspired from finite differences and algebraic multigrid frameworks. We form a parameterized convolution kernel based on discretized differential operators, leveraging the graph mass, gradient and Laplacian. This way, the parameterization does not depend on the graph structure, only on the meaning of the network convolutions as differential operators. To allow hierarchical representations of the input, we propose pooling and unpooling operations that are based on algebraic multigrid methods, which are mainly used to solve partial differential equations on unstructured grids. To motivate and explain our method, we compare it to standard convolutional neural networks, and show their similarities and relations in the case of a regular grid. Our proposed method is demonstrated in various experiments like classification and part-segmentation, achieving on par or better than state of the art results. We also analyze the computational cost of our method compared to other GCNs.



### ADMP: An Adversarial Double Masks Based Pruning Framework For Unsupervised Cross-Domain Compression
- **Arxiv ID**: http://arxiv.org/abs/2006.04127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04127v1)
- **Published**: 2020-06-07 11:44:43+00:00
- **Updated**: 2020-06-07 11:44:43+00:00
- **Authors**: Xiaoyu Feng, Zhuqing Yuan, Guijin Wang, Yongpan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent progress of network pruning, directly applying it to the Internet of Things (IoT) applications still faces two challenges, i.e. the distribution divergence between end and cloud data and the missing of data label on end devices. One straightforward solution is to combine the unsupervised domain adaptation (UDA) technique and pruning. For example, the model is first pruned on the cloud and then transferred from cloud to end by UDA. However, such a naive combination faces high performance degradation. Hence this work proposes an Adversarial Double Masks based Pruning (ADMP) for such cross-domain compression. In ADMP, we construct a Knowledge Distillation framework not only to produce pseudo labels but also to provide a measurement of domain divergence as the output difference between the full-size teacher and the pruned student. Unlike existing mask-based pruning works, two adversarial masks, i.e. soft and hard masks, are adopted in ADMP. So ADMP can prune the model effectively while still allowing the model to extract strong domain-invariant features and robust classification boundaries. During training, the Alternating Direction Multiplier Method is used to overcome the binary constraint of {0,1}-masks. On Office31 and ImageCLEF-DA datasets, the proposed ADMP can prune 60% channels with only 0.2% and 0.3% average accuracy loss respectively. Compared with the state of art, we can achieve about 1.63x parameters reduction and 4.1% and 5.1% accuracy improvement.



### Isotropic multichannel total variation framework for joint reconstruction of multicontrast parallel MRI
- **Arxiv ID**: http://arxiv.org/abs/2006.04128v5
- **DOI**: 10.1117/1.JMI.9.1.013502
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04128v5)
- **Published**: 2020-06-07 11:44:54+00:00
- **Updated**: 2022-02-20 09:15:17+00:00
- **Authors**: Erfan Ebrahim Esfahani
- **Comment**: None
- **Journal**: J. Med. Imag. 9(1) 013502 (16 February 2022)
- **Summary**: Purpose: To develop a synergistic image reconstruction framework that exploits multicontrast (MC), multicoil, and compressed sensing (CS) redundancies in magnetic resonance imaging (MRI).   Approach: CS, MC acquisition, and parallel imaging (PI) have been individually well developed, but the combination of the three has not been equally well studied, much less the potential benefits of isotropy within such a setting. Inspired by total variation theory, we introduce an isotropic MC image regularizer and attain its full potential by integrating it into compressed MC multicoil MRI. A convex optimization problem is posed to model the new variational framework and a first-order algorithm is developed to solve the problem.   Results: It turns out that the proposed isotropic regularizer outperforms many of the state-of-the-art reconstruction methods not only in terms of rotation-invariance preservation of symmetrical features, but also in suppressing noise or streaking artifacts, which are normally encountered in PI methods at aggressive undersampling rates. Moreover, the new framework significantly prevents intercontrast leakage of contrast-specific details, which seems to be a difficult situation to handle for some variational and low-rank MC reconstruction approaches.   Conclusions: The new framework is a viable option for image reconstruction in fast protocols of MC parallel MRI, potentially reducing patient discomfort in otherwise long and time-consuming scans.



### Learning Texture Transformer Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2006.04139v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04139v2)
- **Published**: 2020-06-07 12:55:34+00:00
- **Updated**: 2020-06-22 12:19:51+00:00
- **Authors**: Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, Baining Guo
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a soft-attention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1x to 4x magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.



### Peer Collaborative Learning for Online Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2006.04147v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04147v2)
- **Published**: 2020-06-07 13:21:52+00:00
- **Updated**: 2021-03-03 15:00:39+00:00
- **Authors**: Guile Wu, Shaogang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a high-capacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-the-art methods.



### Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2006.04150v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04150v5)
- **Published**: 2020-06-07 13:32:33+00:00
- **Updated**: 2021-07-07 07:39:18+00:00
- **Authors**: Guile Wu, Shaogang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been successful for many computer vision tasks due to the availability of shared and centralised large-scale training data. However, increasing awareness of privacy concerns poses new challenges to deep learning, especially for human subject related recognition such as person re-identification (Re-ID). In this work, we solve the Re-ID problem by decentralised learning from non-shared private training data distributed at multiple user sites of independent multi-domain label spaces. We propose a novel paradigm called Federated Person Re-Identification (FedReID) to construct a generalisable global model (a central server) by simultaneously learning with multiple privacy-preserved local models (local clients). Specifically, each local client receives global model updates from the server and trains a local model using its local data independent from all the other clients. Then, the central server aggregates transferrable local model updates to construct a generalisable global feature embedding model without accessing local data so to preserve local privacy. This client-server collaborative learning process is iteratively performed under privacy control, enabling FedReID to realise decentralised learning without sharing distributed data nor collecting any centralised data. Extensive experiments on ten Re-ID benchmarks show that FedReID achieves compelling generalisation performance beyond any locally trained models without using shared training data, whilst inherently protects the privacy of each local client. This is uniquely advantageous over contemporary Re-ID methods.



### Realistic text replacement with non-uniform style conditioning
- **Arxiv ID**: http://arxiv.org/abs/2006.04170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04170v1)
- **Published**: 2020-06-07 15:05:42+00:00
- **Updated**: 2020-06-07 15:05:42+00:00
- **Authors**: Arseny Nerinovsky, Igor Buzhinsky, Andey Filchencov
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: In this work, we study the possibility of realistic text replacement, the goal of which is to replace text present in the image with user-supplied text. The replacement should be performed in a way that will not allow distinguishing the resulting image from the original one. We achieve this goal by developing a novel non-uniform style conditioning layer and apply it to an encoder-decoder ResNet based architecture. The resulting model is a single-stage model, with no post-processing. The proposed model achieves realistic text replacement and outperforms existing approaches on ICDAR MLT.



### Learning pose variations within shape population by constrained mixtures of factor analyzers
- **Arxiv ID**: http://arxiv.org/abs/2006.04171v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04171v1)
- **Published**: 2020-06-07 15:06:01+00:00
- **Updated**: 2020-06-07 15:06:01+00:00
- **Authors**: Xilu Wang
- **Comment**: 25 Pages, 15 Figures
- **Journal**: None
- **Summary**: Mining and learning the shape variability of underlying population has benefited the applications including parametric shape modeling, 3D animation, and image segmentation. The current statistical shape modeling method works well on learning unstructured shape variations without obvious pose changes (relative rotations of the body parts). Studying the pose variations within a shape population involves segmenting the shapes into different articulated parts and learning the transformations of the segmented parts. This paper formulates the pose learning problem as mixtures of factor analyzers. The segmentation is obtained by components posterior probabilities and the rotations in pose variations are learned by the factor loading matrices. To guarantee that the factor loading matrices are composed by rotation matrices, constraints are imposed and the corresponding closed form optimal solution is derived. Based on the proposed method, the pose variations are automatically learned from the given shape populations. The method is applied in motion animation where new poses are generated by interpolating the existing poses in the training set. The obtained results are smooth and realistic.



### Finger Texture Biometric Characteristic: a Survey
- **Arxiv ID**: http://arxiv.org/abs/2006.04193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04193v1)
- **Published**: 2020-06-07 16:33:59+00:00
- **Updated**: 2020-06-07 16:33:59+00:00
- **Authors**: Raid R. O. Al-Nima, Tingting Han, Taolue Chen, Satnam Dlay, Jonathon Chambers
- **Comment**: None
- **Journal**: None
- **Summary**: \begin{abstract}   In recent years, the Finger Texture (FT) has attracted considerable attention as a biometric characteristic. It can provide efficient human recognition performance, because it has different human-specific features of apparent lines, wrinkles and ridges distributed along the inner surface of all fingers. Also, such pattern structures are reliable, unique and remain stable throughout a human's life. Efficient biometric systems can be established based only on FTs. In this paper, a comprehensive survey of the relevant FT studies is presented. We also summarise the main drawbacks and obstacles of employing the FT as a biometric characteristic, and provide useful suggestions to further improve the work on FT. \end{abstract}



### Thoracic Disease Identification and Localization using Distance Learning and Region Verification
- **Arxiv ID**: http://arxiv.org/abs/2006.04203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04203v2)
- **Published**: 2020-06-07 16:56:50+00:00
- **Updated**: 2020-08-11 21:47:33+00:00
- **Authors**: Cheng Zhang, Francine Chen, Yan-Ying Chen
- **Comment**: British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: The identification and localization of diseases in medical images using deep learning models have recently attracted significant interest. Existing methods only consider training the networks with each image independently and most leverage an activation map for disease localization. In this paper, we propose an alternative approach that learns discriminative features among triplets of images and cyclically trains on region features to verify whether attentive regions contain information indicative of a disease. Concretely, we adapt a distance learning framework for multi-label disease classification to differentiate subtle disease features. Additionally, we feed back the features of the predicted class-specific regions to a separate classifier during training to better verify the localized diseases. Our model can achieve state-of-the-art classification performance on the challenging ChestX-ray14 dataset, and our ablation studies indicate that both distance learning and region verification contribute to overall classification performance. Moreover, the distance learning and region verification modules can capture essential information for better localization than baseline models without these modules.



### Efficient Poverty Mapping using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.04224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04224v2)
- **Published**: 2020-06-07 18:30:57+00:00
- **Updated**: 2021-01-05 11:30:00+00:00
- **Authors**: Kumar Ayush, Burak Uzkent, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: Accepted at AAAI 2021
- **Journal**: None
- **Summary**: The combination of high-resolution satellite imagery and machine learning have proven useful in many sustainability-related tasks, including poverty prediction, infrastructure measurement, and forest monitoring. However, the accuracy afforded by high-resolution imagery comes at a cost, as such imagery is extremely expensive to purchase at scale. This creates a substantial hurdle to the efficient scaling and widespread adoption of high-resolution-based approaches. To reduce acquisition costs while maintaining accuracy, we propose a reinforcement learning approach in which free low-resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on the high-resolution images. We apply this approach to the task of poverty prediction in Uganda, building on an earlier approach that used object detection to count objects and use these counts to predict poverty. Our approach exceeds previous performance benchmarks on this task while using 80% fewer high-resolution images. Our approach could have application in many sustainability domains that require high-resolution imagery.



### Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2006.04225v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04225v1)
- **Published**: 2020-06-07 18:36:56+00:00
- **Updated**: 2020-06-07 18:36:56+00:00
- **Authors**: Sina Sharif Mansouri, Farhad Pourkamali-Anaraki, Miguel Castano Arranz, Ali-akbar Agha-mohammadi, Joel Burdick, George Nikolakopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology.



### Self-Representation Based Unsupervised Exemplar Selection in a Union of Subspaces
- **Arxiv ID**: http://arxiv.org/abs/2006.04246v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04246v1)
- **Published**: 2020-06-07 19:43:33+00:00
- **Updated**: 2020-06-07 19:43:33+00:00
- **Authors**: Chong You, Chi Li, Daniel P. Robinson, Rene Vidal
- **Comment**: In submission; conference version at ECCV'2018
- **Journal**: None
- **Summary**: Finding a small set of representatives from an unlabeled dataset is a core problem in a broad range of applications such as dataset summarization and information extraction. Classical exemplar selection methods such as $k$-medoids work under the assumption that the data points are close to a few cluster centroids, and cannot handle the case where data lie close to a union of subspaces. This paper proposes a new exemplar selection model that searches for a subset that best reconstructs all data points as measured by the $\ell_1$ norm of the representation coefficients. Geometrically, this subset best covers all the data points as measured by the Minkowski functional of the subset. To solve our model efficiently, we introduce a farthest first search algorithm that iteratively selects the worst represented point as an exemplar. When the dataset is drawn from a union of independent subspaces, our method is able to select sufficiently many representatives from each subspace. We further develop an exemplar based subspace clustering method that is robust to imbalanced data and efficient for large scale data. Moreover, we show that a classifier trained on the selected exemplars (when they are labeled) can correctly classify the rest of the data points.



### AdaLAM: Revisiting Handcrafted Outlier Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.04250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04250v1)
- **Published**: 2020-06-07 20:16:36+00:00
- **Updated**: 2020-06-07 20:16:36+00:00
- **Authors**: Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: Local feature matching is a critical component of many computer vision pipelines, including among others Structure-from-Motion, SLAM, and Visual Localization. However, due to limitations in the descriptors, raw matches are often contaminated by a majority of outliers. As a result, outlier detection is a fundamental problem in computer vision, and a wide range of approaches have been proposed over the last decades. In this paper we revisit handcrafted approaches to outlier filtering. Based on best practices, we propose a hierarchical pipeline for effective outlier detection as well as integrate novel ideas which in sum lead to AdaLAM, an efficient and competitive approach to outlier rejection. AdaLAM is designed to effectively exploit modern parallel hardware, resulting in a very fast, yet very accurate, outlier filter. We validate AdaLAM on multiple large and diverse datasets, and we submit to the Image Matching Challenge (CVPR2020), obtaining competitive results with simple baseline descriptors. We show that AdaLAM is more than competitive to current state of the art, both in terms of efficiency and effectiveness.



### How useful is Active Learning for Image-based Plant Phenotyping?
- **Arxiv ID**: http://arxiv.org/abs/2006.04255v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04255v3)
- **Published**: 2020-06-07 20:32:42+00:00
- **Updated**: 2020-07-11 07:07:28+00:00
- **Authors**: Koushik Nagasubramanian, Talukder Z. Jubery, Fateme Fotouhi Ardakani, Seyed Vahid Mirnezami, Asheesh K. Singh, Arti Singh, Soumik Sarkar, Baskar Ganapathysubramanian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have been successfully deployed for a diverse array of image-based plant phenotyping applications including disease detection and classification. However, successful deployment of supervised deep learning models requires large amount of labeled data, which is a significant challenge in plant science (and most biological) domains due to the inherent complexity. Specifically, data annotation is costly, laborious, time consuming and needs domain expertise for phenotyping tasks, especially for diseases. To overcome this challenge, active learning algorithms have been proposed that reduce the amount of labeling needed by deep learning models to achieve good predictive performance. Active learning methods adaptively select samples to annotate using an acquisition function to achieve maximum (classification) performance under a fixed labeling budget. We report the performance of four different active learning methods, (1) Deep Bayesian Active Learning (DBAL), (2) Entropy, (3) Least Confidence, and (4) Coreset, with conventional random sampling-based annotation for two different image-based classification datasets. The first image dataset consists of soybean [Glycine max L. (Merr.)] leaves belonging to eight different soybean stresses and a healthy class, and the second consists of nine different weed species from the field. For a fixed labeling budget, we observed that the classification performance of deep learning models with active learning-based acquisition strategies is better than random sampling-based acquisition for both datasets. The integration of active learning strategies for data annotation can help mitigate labelling challenges in the plant sciences applications particularly where deep domain knowledge is required.



### Advance Warning Methodologies for COVID-19 using Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2006.05332v6
- **DOI**: 10.1109/ACCESS.2021.3064927
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05332v6)
- **Published**: 2020-06-07 20:42:25+00:00
- **Updated**: 2021-03-18 11:39:17+00:00
- **Authors**: Mete Ahishali, Aysen Degerli, Mehmet Yamac, Serkan Kiranyaz, Muhammad E. H. Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
- **Comment**: 12 pages
- **Journal**: in IEEE Access, vol. 9, pp. 41052-41065, 2021
- **Summary**: Coronavirus disease 2019 (COVID-19) has rapidly become a global health concern after its first known detection in December 2019. As a result, accurate and reliable advance warning system for the early diagnosis of COVID-19 has now become a priority. The detection of COVID-19 in early stages is not a straightforward task from chest X-ray images according to expert medical doctors because the traces of the infection are visible only when the disease has progressed to a moderate or severe stage. In this study, our first aim is to evaluate the ability of recent \textit{state-of-the-art} Machine Learning techniques for the early detection of COVID-19 from chest X-ray images. Both compact classifiers and deep learning approaches are considered in this study. Furthermore, we propose a recent compact classifier, Convolutional Support Estimator Network (CSEN) approach for this purpose since it is well-suited for a scarce-data classification task. Finally, this study introduces a new benchmark dataset called Early-QaTa-COV19, which consists of 1065 early-stage COVID-19 pneumonia samples (very limited or no infection signs) labelled by the medical doctors and 12 544 samples for control (normal) class. A detailed set of experiments shows that the CSEN achieves the top (over 97%) sensitivity with over 95.5% specificity. Moreover, DenseNet-121 network produces the leading performance among other deep networks with 95% sensitivity and 99.74% specificity.



### EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.04270v5
- **DOI**: 10.1109/TNNLS.2021.3069970
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.04270v5)
- **Published**: 2020-06-07 21:09:44+00:00
- **Updated**: 2022-03-07 15:33:11+00:00
- **Authors**: Hojjat Salehinejad, Shahrokh Valaee
- **Comment**: None
- **Journal**: None
- **Summary**: Dropout is a well-known regularization method by sampling a sub-network from a larger deep neural network and training different sub-networks on different subsets of the data. Inspired by the dropout concept, we propose EDropout as an energy-based framework for pruning neural networks in classification tasks. In this approach, a set of binary pruning state vectors (population) represents a set of corresponding sub-networks from an arbitrary provided original neural network. An energy loss function assigns a scalar energy loss value to each pruning state. The energy-based model stochastically evolves the population to find states with lower energy loss. The best pruning state is then selected and applied to the original network. Similar to dropout, the kept weights are updated using backpropagation in a probabilistic model. The energy-based model again searches for better pruning states and the cycle continuous. Indeed, this procedure is in fact switching between the energy model, which manages the pruning states, and the probabilistic model, which updates the temporarily unpruned weights, in each iteration. The population can dynamically converge to a pruning state. This can be interpreted as dropout leading to pruning the network. From an implementation perspective, EDropout can prune typical neural networks without modification of the network architecture. We evaluated the proposed method on different flavours of ResNets, AlexNet, and SqueezeNet on the Kuzushiji, Fashion, CIFAR-10, CIFAR-100, and Flowers datasets, and compared the pruning rate and classification performance of the models. On average the networks trained with EDropout achieved a pruning rate of more than $50\%$ of the trainable parameters with approximately $<5\%$ and $<1\%$ drop of Top-1 and Top-5 classification accuracy, respectively.




# Arxiv Papers in cs.CV on 2020-07-07
### Dual Mixup Regularized Learning for Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.03141v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03141v2)
- **Published**: 2020-07-07 00:24:14+00:00
- **Updated**: 2020-07-16 22:01:18+00:00
- **Authors**: Yuan Wu, Diana Inkpen, Ahmed El-Roby
- **Comment**: This paper has been accepted by ECCV 2020
- **Journal**: None
- **Summary**: Recent advances on unsupervised domain adaptation (UDA) rely on adversarial learning to disentangle the explanatory and transferable features for domain adaptation. However, there are two issues with the existing methods. First, the discriminability of the latent space cannot be fully guaranteed without considering the class-aware information in the target domain. Second, samples from the source and target domains alone are not sufficient for domain-invariant feature extracting in the latent space. In order to alleviate the above issues, we propose a dual mixup regularized learning (DMRL) method for UDA, which not only guides the classifier in enhancing consistent predictions in-between samples, but also enriches the intrinsic structures of the latent space. The DMRL jointly conducts category and domain mixup regularizations on pixel level to improve the effectiveness of models. A series of empirical studies on four domain adaptation benchmarks demonstrate that our approach can achieve the state-of-the-art.



### Discretization-Aware Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.03154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.03154v1)
- **Published**: 2020-07-07 01:18:58+00:00
- **Updated**: 2020-07-07 01:18:58+00:00
- **Authors**: Yunjie Tian, Chang Liu, Lingxi Xie, Jianbin Jiao, Qixiang Ye
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: The search cost of neural architecture search (NAS) has been largely reduced by weight-sharing methods. These methods optimize a super-network with all possible edges and operations, and determine the optimal sub-network by discretization, \textit{i.e.}, pruning off weak candidates. The discretization process, performed on either operations or edges, incurs significant inaccuracy and thus the quality of the final architecture is not guaranteed. This paper presents discretization-aware architecture search (DA\textsuperscript{2}S), with the core idea being adding a loss term to push the super-network towards the configuration of desired topology, so that the accuracy loss brought by discretization is largely alleviated. Experiments on standard image classification benchmarks demonstrate the superiority of our approach, in particular, under imbalanced target network configurations that were not studied before.



### Self domain adapted network
- **Arxiv ID**: http://arxiv.org/abs/2007.03162v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03162v1)
- **Published**: 2020-07-07 01:41:34+00:00
- **Updated**: 2020-07-07 01:41:34+00:00
- **Authors**: Yufan He, Aaron Carass, Lianrui Zuo, Blake E. Dewey, Jerry L. Prince
- **Comment**: early accept in miccai2020
- **Journal**: None
- **Summary**: Domain shift is a major problem for deploying deep networks in clinical practice. Network performance drops significantly with (target) images obtained differently than its (source) training data. Due to a lack of target label data, most work has focused on unsupervised domain adaptation (UDA). Current UDA methods need both source and target data to train models which perform image translation (harmonization) or learn domain-invariant features. However, training a model for each target domain is time consuming and computationally expensive, even infeasible when target domain data are scarce or source data are unavailable due to data privacy. In this paper, we propose a novel self domain adapted network (SDA-Net) that can rapidly adapt itself to a single test subject at the testing stage, without using extra data or training a UDA model. The SDA-Net consists of three parts: adaptors, task model, and auto-encoders. The latter two are pre-trained offline on labeled source images. The task model performs tasks like synthesis, segmentation, or classification, which may suffer from the domain shift problem. At the testing stage, the adaptors are trained to transform the input test image and features to reduce the domain shift as measured by the auto-encoders, and thus perform domain adaptation. We validated our method on retinal layer segmentation from different OCT scanners and T1 to T2 synthesis with T1 from different MRI scanners and with different imaging parameters. Results show that our SDA-Net, with a single test subject and a short amount of time for self adaptation at the testing stage, can achieve significant improvements.



### Spatial Semantic Embedding Network: Fast 3D Instance Segmentation with Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.03169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03169v1)
- **Published**: 2020-07-07 02:17:44+00:00
- **Updated**: 2020-07-07 02:17:44+00:00
- **Authors**: Dongsu Zhang, Junha Chun, Sang Kyun Cha, Young Min Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose spatial semantic embedding network (SSEN), a simple, yet efficient algorithm for 3D instance segmentation using deep metric learning. The raw 3D reconstruction of an indoor environment suffers from occlusions, noise, and is produced without any meaningful distinction between individual entities. For high-level intelligent tasks from a large scale scene, 3D instance segmentation recognizes individual instances of objects. We approach the instance segmentation by simply learning the correct embedding space that maps individual instances of objects into distinct clusters that reflect both spatial and semantic information. Unlike previous approaches that require complex pre-processing or post-processing, our implementation is compact and fast with competitive performance, maintaining scalability on large scenes with high resolution voxels. We demonstrate the state-of-the-art performance of our algorithm in the ScanNet 3D instance segmentation benchmark on AP score.



### Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search
- **Arxiv ID**: http://arxiv.org/abs/2007.07197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07197v2)
- **Published**: 2020-07-07 02:29:06+00:00
- **Updated**: 2020-08-05 08:56:56+00:00
- **Authors**: Yong Guo, Yaofo Chen, Yin Zheng, Peilin Zhao, Jian Chen, Junzhou Huang, Mingkui Tan
- **Comment**: Accepted by ICML 2020
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has become an important approach to automatically find effective architectures. To cover all possible good architectures, we need to search in an extremely large search space with billions of candidate architectures. More critically, given a large search space, we may face a very challenging issue of space explosion. However, due to the limitation of computational resources, we can only sample a very small proportion of the architectures, which provides insufficient information for the training. As a result, existing methods may often produce suboptimal architectures. To alleviate this issue, we propose a curriculum search method that starts from a small search space and gradually incorporates the learned knowledge to guide the search in a large space. With the proposed search strategy, our Curriculum Neural Architecture Search (CNAS) method significantly improves the search efficiency and finds better architectures than existing NAS methods. Extensive experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.



### Learning to Count in the Crowd from Limited Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/2007.03195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03195v2)
- **Published**: 2020-07-07 04:17:01+00:00
- **Updated**: 2020-07-08 17:01:17+00:00
- **Authors**: Vishwanath A. Sindagi, Rajeev Yasarla, Deepak Sam Babu, R. Venkatesh Babu, Vishal M. Patel
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Recent crowd counting approaches have achieved excellent performance. However, they are essentially based on fully supervised paradigm and require large number of annotated samples. Obtaining annotations is an expensive and labour-intensive process. In this work, we focus on reducing the annotation efforts by learning to count in the crowd from limited number of labeled samples while leveraging a large pool of unlabeled data. Specifically, we propose a Gaussian Process-based iterative learning mechanism that involves estimation of pseudo-ground truth for the unlabeled data, which is then used as supervision for training the network. The proposed method is shown to be effective under the reduced data (semi-supervised) settings for several datasets like ShanghaiTech, UCF-QNRF, WorldExpo, UCSD, etc. Furthermore, we demonstrate that the proposed method can be leveraged to enable the network in learning to count from synthetic dataset while being able to generalize better to real-world datasets (synthetic-to-real transfer).



### Regional Image Perturbation Reduces $L_p$ Norms of Adversarial Examples While Maintaining Model-to-model Transferability
- **Arxiv ID**: http://arxiv.org/abs/2007.03198v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03198v2)
- **Published**: 2020-07-07 04:33:16+00:00
- **Updated**: 2020-07-18 08:23:59+00:00
- **Authors**: Utku Ozbulak, Jonathan Peck, Wesley De Neve, Bart Goossens, Yvan Saeys, Arnout Van Messem
- **Comment**: Accepted for the ICML 2020, Workshop on Uncertainty and Robustness in
  Deep Learning (UDL)
- **Journal**: None
- **Summary**: Regional adversarial attacks often rely on complicated methods for generating adversarial perturbations, making it hard to compare their efficacy against well-known attacks. In this study, we show that effective regional perturbations can be generated without resorting to complex methods. We develop a very simple regional adversarial perturbation attack method using cross-entropy sign, one of the most commonly used losses in adversarial machine learning. Our experiments on ImageNet with multiple models reveal that, on average, $76\%$ of the generated adversarial examples maintain model-to-model transferability when the perturbation is applied to local image regions. Depending on the selected region, these localized adversarial examples require significantly less $L_p$ norm distortion (for $p \in \{0, 2, \infty\}$) compared to their non-local counterparts. These localized attacks therefore have the potential to undermine defenses that claim robustness under the aforementioned norms.



### Automatic lesion detection, segmentation and characterization via 3D multiscale morphological sifting in breast MRI
- **Arxiv ID**: http://arxiv.org/abs/2007.03199v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03199v1)
- **Published**: 2020-07-07 04:39:13+00:00
- **Updated**: 2020-07-07 04:39:13+00:00
- **Authors**: Hang Min, Darryl McClymont, Shekhar S. Chandra, Stuart Crozier, Andrew P. Bradley
- **Comment**: None
- **Journal**: None
- **Summary**: Previous studies on computer aided detection/diagnosis (CAD) in 4D breast magnetic resonance imaging (MRI) regard lesion detection, segmentation and characterization as separate tasks, and typically require users to manually select 2D MRI slices or regions of interest as the input. In this work, we present a breast MRI CAD system that can handle 4D multimodal breast MRI data, and integrate lesion detection, segmentation and characterization with no user intervention. The proposed CAD system consists of three major stages: region candidate generation, feature extraction and region candidate classification. Breast lesions are firstly extracted as region candidates using the novel 3D multiscale morphological sifting (MMS). The 3D MMS, which uses linear structuring elements to extract lesion-like patterns, can segment lesions from breast images accurately and efficiently. Analytical features are then extracted from all available 4D multimodal breast MRI sequences, including T1-, T2-weighted and DCE sequences, to represent the signal intensity, texture, morphological and enhancement kinetic characteristics of the region candidates. The region candidates are lastly classified as lesion or normal tissue by the random under-sampling boost (RUSboost), and as malignant or benign lesion by the random forest. Evaluated on a breast MRI dataset which contains a total of 117 cases with 95 malignant and 46 benign lesions, the proposed system achieves a true positive rate (TPR) of 0.90 at 3.19 false positives per patient (FPP) for lesion detection and a TPR of 0.91 at a FPP of 2.95 for identifying malignant lesions without any user intervention. The average dice similarity index (DSI) is 0.72 for lesion segmentation. Compared with previously proposed systems evaluated on the same breast MRI dataset, the proposed CAD system achieves a favourable performance in breast lesion detection and characterization.



### ReMOTS: Self-Supervised Refining Multi-Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.03200v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03200v3)
- **Published**: 2020-07-07 04:49:10+00:00
- **Updated**: 2021-01-13 12:38:59+00:00
- **Authors**: Fan Yang, Xin Chang, Chenyu Dang, Ziqiang Zheng, Sakriani Sakti, Satoshi Nakamura, Yang Wu
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: We aim to improve the performance of Multiple Object Tracking and Segmentation (MOTS) by refinement. However, it remains challenging for refining MOTS results, which could be attributed to that appearance features are not adapted to target videos and it is also difficult to find proper thresholds to discriminate them. To tackle this issue, we propose a self-supervised refining MOTS (i.e., ReMOTS) framework. ReMOTS mainly takes four steps to refine MOTS results from the data association perspective. (1) Training the appearance encoder using predicted masks. (2) Associating observations across adjacent frames to form short-term tracklets. (3) Training the appearance encoder using short-term tracklets as reliable pseudo labels. (4) Merging short-term tracklets to long-term tracklets utilizing adopted appearance features and thresholds that are automatically obtained from statistical information. Using ReMOTS, we reached the $1^{st}$ place on CVPR 2020 MOTS Challenge 1, with an sMOTSA score of $69.9$.



### Semi-Supervised Crowd Counting via Self-Training on Surrogate Tasks
- **Arxiv ID**: http://arxiv.org/abs/2007.03207v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03207v2)
- **Published**: 2020-07-07 05:30:53+00:00
- **Updated**: 2020-07-18 06:40:40+00:00
- **Authors**: Yan Liu, Lingqiao Liu, Peng Wang, Pingping Zhang, Yinjie Lei
- **Comment**: To be appeared in ECCV 2020
- **Journal**: None
- **Summary**: Most existing crowd counting systems rely on the availability of the object location annotation which can be expensive to obtain. To reduce the annotation cost, one attractive solution is to leverage a large number of unlabeled images to build a crowd counting model in semi-supervised fashion. This paper tackles the semi-supervised crowd counting problem from the perspective of feature learning. Our key idea is to leverage the unlabeled images to train a generic feature extractor rather than the entire network of a crowd counter. The rationale of this design is that learning the feature extractor can be more reliable and robust towards the inevitable noisy supervision generated from the unlabeled data. Also, on top of a good feature extractor, it is possible to build a density map regressor with much fewer density map annotations. Specifically, we proposed a novel semi-supervised crowd counting method which is built upon two innovative components: (1) a set of inter-related binary segmentation tasks are derived from the original density map regression task as the surrogate prediction target; (2) the surrogate target predictors are learned from both labeled and unlabeled data by utilizing a proposed self-training scheme which fully exploits the underlying constraints of these binary segmentation tasks. Through experiments, we show that the proposed method is superior over the existing semisupervised crowd counting method and other representative baselines.



### Classification with 2-D Convolutional Neural Networks for breast cancer diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2007.03218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03218v2)
- **Published**: 2020-07-07 06:08:06+00:00
- **Updated**: 2020-07-29 00:24:24+00:00
- **Authors**: Anuraganand Sharma, Dinesh Kumar
- **Comment**: 26 pages - preprint version
- **Journal**: None
- **Summary**: Breast cancer is the most common cancer in women. Classification of cancer/non-cancer patients with clinical records requires high sensitivity and specificity for an acceptable diagnosis test. The state-of-the-art classification model - Convolutional Neural Network (CNN), however, cannot be used with clinical data that are represented in 1-D format. CNN has been designed to work on a set of 2-D matrices whose elements show some correlation with neighboring elements such as in image data. Conversely, the data examples represented as a set of 1-D vectors -- apart from the time series data -- cannot be used with CNN, but with other classification models such as Artificial Neural Networks or RandomForest. We have proposed some novel preprocessing methods of data wrangling that transform a 1-D data vector, to a 2-D graphical image with appropriate correlations among the fields to be processed on CNN. We tested our methods on Wisconsin Original Breast Cancer (WBC) and Wisconsin Diagnostic Breast Cancer (WDBC) datasets. To our knowledge, this work is novel on non-image to image data transformation for the non-time series data. The transformed data processed with CNN using VGGnet-16 shows competitive results for the WBC dataset and outperforms other known methods for the WDBC dataset.



### AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses
- **Arxiv ID**: http://arxiv.org/abs/2007.03221v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03221v3)
- **Published**: 2020-07-07 06:17:08+00:00
- **Updated**: 2021-02-06 07:11:57+00:00
- **Authors**: Zixuan Xu, Banghuai Li, Miao Geng, Ye Yuan
- **Comment**: To appear in AAAI 2021
- **Journal**: AAAI 2021
- **Summary**: Facial landmark localization aims to detect the predefined points of human faces, and the topic has been rapidly improved with the recent development of neural network based methods. However, it remains a challenging task when dealing with faces in unconstrained scenarios, especially with large pose variations. In this paper, we target the problem of facial landmark localization across large poses and address this task based on a split-and-aggregate strategy. To split the search space, we propose a set of anchor templates as references for regression, which well addresses the large variations of face poses. Based on the prediction of each anchor template, we propose to aggregate the results, which can reduce the landmark uncertainty due to the large poses. Overall, our proposed approach, named AnchorFace, obtains state-of-the-art results with extremely efficient inference speed on four challenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be available at https://github.com/nothingelse92/AnchorFace.



### Extracting the fundamental diagram from aerial footage
- **Arxiv ID**: http://arxiv.org/abs/2007.03227v2
- **DOI**: 10.1109/VTC2020-Spring48590.2020.9128534
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03227v2)
- **Published**: 2020-07-07 06:34:50+00:00
- **Updated**: 2020-07-13 13:40:41+00:00
- **Authors**: Rafael Makrigiorgis, Panayiotis Kolios, Stelios Timotheou, Theocharis Theocharides, Christos G. Panayiotou
- **Comment**: 5 pages, 7 figures, 2020 IEEE 91st Vehicular Technology Conference
  (VTC2020-Spring)
- **Journal**: None
- **Summary**: Efficient traffic monitoring is playing a fundamental role in successfully tackling congestion in transportation networks. Congestion is strongly correlated with two measurable characteristics, the demand and the network density that impact the overall system behavior. At large, this system behavior is characterized through the fundamental diagram of a road segment, a region or the network. In this paper we devise an innovative way to obtain the fundamental diagram through aerial footage obtained from drone platforms. The derived methodology consists of 3 phases: vehicle detection, vehicle tracking and traffic state estimation. We elaborate on the algorithms developed for each of the 3 phases and demonstrate the applicability of the results in a real-world setting.



### Robust Processing-In-Memory Neural Networks via Noise-Aware Normalization
- **Arxiv ID**: http://arxiv.org/abs/2007.03230v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03230v2)
- **Published**: 2020-07-07 06:51:28+00:00
- **Updated**: 2020-11-24 05:35:46+00:00
- **Authors**: Li-Huang Tsai, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Cheng Juan
- **Comment**: 7 pages, 3 figure
- **Journal**: None
- **Summary**: Analog computing hardwares, such as Processing-in-memory (PIM) accelerators, have gradually received more attention for accelerating the neural network computations. However, PIM accelerators often suffer from intrinsic noise in the physical components, making it challenging for neural network models to achieve the same performance as on the digital hardware. Previous works in mitigating intrinsic noise assumed the knowledge of the noise model, and retraining the neural networks accordingly was required. In this paper, we propose a noise-agnostic method to achieve robust neural network performance against any noise setting. Our key observation is that the degradation of performance is due to the distribution shifts in network activations, which are caused by the noise. To properly track the shifts and calibrate the biased distributions, we propose a "noise-aware" batch normalization layer, which is able to align the distributions of the activations under variational noise inherent in the analog environments. Our method is simple, easy to implement, general to various noise settings, and does not need to retrain the models. We conduct experiments on several tasks in computer vision, including classification, object detection and semantic segmentation. The results demonstrate the effectiveness of our method, achieving robust performance under a wide range of noise settings, more reliable than existing methods. We believe that our simple yet general method can facilitate the adoption of analog computing devices for neural networks.



### Learning Model-Blind Temporal Denoisers without Ground Truths
- **Arxiv ID**: http://arxiv.org/abs/2007.03241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03241v2)
- **Published**: 2020-07-07 07:19:48+00:00
- **Updated**: 2021-03-31 13:51:22+00:00
- **Authors**: Yanghao Li, Bichuan Guo, Jiangtao Wen, Zhen Xia, Shan Liu, Yuxing Han
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: Denoisers trained with synthetic data often fail to cope with the diversity of unknown noises, giving way to methods that can adapt to existing noise without knowing its ground truth. Previous image-based method leads to noise overfitting if directly applied to video denoisers, and has inadequate temporal information management especially in terms of occlusion and lighting variation, which considerably hinders its denoising performance. In this paper, we propose a general framework for video denoising networks that successfully addresses these challenges. A novel twin sampler assembles training data by decoupling inputs from targets without altering semantics, which not only effectively solves the noise overfitting problem, but also generates better occlusion masks efficiently by checking optical flow consistency. An online denoising scheme and a warping loss regularizer are employed for better temporal alignment. Lighting variation is quantified based on the local similarity of aligned frames. Our method consistently outperforms the prior art by 0.6-3.2dB PSNR on multiple noises, datasets and network architectures. State-of-the-art results on reducing model-blind video noises are achieved. Extensive ablation studies are conducted to demonstrate the significance of each technical components.



### ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting
- **Arxiv ID**: http://arxiv.org/abs/2007.03260v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03260v4)
- **Published**: 2020-07-07 07:56:45+00:00
- **Updated**: 2021-08-14 19:36:54+00:00
- **Authors**: Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, Guiguang Ding
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter pruning), which slims down a CNN by reducing the width (number of output channels) of convolutional layers. Inspired by the neurobiology research about the independence of remembering and forgetting, we propose to re-parameterize a CNN into the remembering parts and forgetting parts, where the former learn to maintain the performance and the latter learn to prune. Via training with regular SGD on the former but a novel update rule with penalty gradients on the latter, we realize structured sparsity. Then we equivalently merge the remembering and forgetting parts into the original architecture with narrower layers. In this sense, ResRep can be viewed as a successful application of Structural Re-parameterization. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce sparsity, which may suppress the parameters essential for the remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is the first to achieve lossless pruning with such a high compression ratio. The code and models are at https://github.com/DingXiaoH/ResRep.



### RGBT Salient Object Detection: A Large-scale Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2007.03262v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03262v6)
- **Published**: 2020-07-07 07:58:14+00:00
- **Updated**: 2022-05-23 03:38:28+00:00
- **Authors**: Zhengzheng Tu, Yan Ma, Zhun Li, Chenglong Li, Jieming Xu, Yongtao Liu
- **Comment**: 12 pages, 10 figures
  https://github.com/lz118/RGBT-Salient-Object-Detection
- **Journal**: None
- **Summary**: Salient object detection in complex scenes and environments is a challenging research topic. Most works focus on RGB-based salient object detection, which limits its performance of real-life applications when confronted with adverse conditions such as dark environments and complex backgrounds. Taking advantage of RGB and thermal infrared images becomes a new research direction for detecting salient object in complex scenes recently, as thermal infrared spectrum imaging provides the complementary information and has been applied to many computer vision tasks. However, current research for RGBT salient object detection is limited by the lack of a large-scale dataset and comprehensive benchmark. This work contributes such a RGBT image dataset named VT5000, including 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms. With this dataset, we propose a powerful baseline approach, which extracts multi-level features within each modality and aggregates these features of all modalities with the attention mechanism, for accurate RGBT salient object detection. Extensive experiments show that the proposed baseline approach outperforms the state-of-the-art methods on VT5000 dataset and other two public datasets. In addition, we carry out a comprehensive analysis of different algorithms of RGBT salient object detection on VT5000 dataset, and then make several valuable conclusions and provide some potential research directions for RGBT salient object detection.



### Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.03263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03263v1)
- **Published**: 2020-07-07 07:58:56+00:00
- **Updated**: 2020-07-07 07:58:56+00:00
- **Authors**: Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic skeletal data, represented as the 2D/3D coordinates of human joints, has been widely studied for human action recognition due to its high-level semantic information and environmental robustness. However, previous methods heavily rely on designing hand-crafted traversal rules or graph topologies to draw dependencies between the joints, which are limited in performance and generalizability. In this work, we present a novel decoupled spatial-temporal attention network(DSTA-Net) for skeleton-based action recognition. It involves solely the attention blocks, allowing for modeling spatial-temporal dependencies between joints without the requirement of knowing their positions or mutual connections. Specifically, to meet the specific requirements of the skeletal data, three techniques are proposed for building attention blocks, namely, spatial-temporal attention decoupling, decoupled position encoding and spatial global regularization. Besides, from the data aspect, we introduce a skeletal data decoupling technique to emphasize the specific characteristics of space/time and different motion scales, resulting in a more comprehensive understanding of the human actions.To test the effectiveness of the proposed method, extensive experiments are conducted on four challenging datasets for skeleton-based gesture and action recognition, namely, SHREC, DHG, NTU-60 and NTU-120, where DSTA-Net achieves state-of-the-art performance on all of them.



### Single Storage Semi-Global Matching for Real Time Depth Processing
- **Arxiv ID**: http://arxiv.org/abs/2007.03269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.03269v1)
- **Published**: 2020-07-07 08:12:25+00:00
- **Updated**: 2020-07-07 08:12:25+00:00
- **Authors**: Prathmesh Sawant, Yashwant Temburu, Mandar Datar, Imran Ahmed, Vinayak Shriniwas, Sachin Patkar
- **Comment**: 10 pages, Published in National Conference on Computer Vision,
  Pattern Recognition, Image Processing and Graphics(NCVPRIPG) 2019
- **Journal**: None
- **Summary**: Depth-map is the key computation in computer vision and robotics. One of the most popular approach is via computation of disparity-map of images obtained from Stereo Camera. Semi Global Matching (SGM) method is a popular choice for good accuracy with reasonable computation time. To use such compute-intensive algorithms for real-time applications such as for autonomous aerial vehicles, blind Aid, etc. acceleration using GPU, FPGA is necessary. In this paper, we show the design and implementation of a stereo-vision system, which is based on FPGA-implementation of More Global Matching(MGM). MGM is a variant of SGM. We use 4 paths but store a single cumulative cost value for a corresponding pixel. Our stereo-vision prototype uses Zedboard containing an ARM-based Zynq-SoC, ZED-stereo-camera / ELP stereo-camera / Intel RealSense D435i, and VGA for visualization. The power consumption attributed to the custom FPGA-based acceleration of disparity map computation required for depth-map is just 0.72 watt. The update rate of the disparity map is realistic 10.5 fps.



### Spectral Graph-based Features for Recognition of Handwritten Characters: A Case Study on Handwritten Devanagari Numerals
- **Arxiv ID**: http://arxiv.org/abs/2007.03281v1
- **DOI**: 10.1515/jisys-2017-0448
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03281v1)
- **Published**: 2020-07-07 08:40:08+00:00
- **Updated**: 2020-07-07 08:40:08+00:00
- **Authors**: Mohammad Idrees Bhat, B. Sharada
- **Comment**: 16 pages, 8 figures
- **Journal**: Journal of Intelligent Systems,29,2018,799-813
- **Summary**: Interpretation of different writing styles, unconstrained cursiveness and relationship between different primitive parts is an essential and challenging task for recognition of handwritten characters. As feature representation is inadequate, appropriate interpretation/description of handwritten characters seems to be a challenging task. Although existing research in handwritten characters is extensive, it still remains a challenge to get the effective representation of characters in feature space. In this paper, we make an attempt to circumvent these problems by proposing an approach that exploits the robust graph representation and spectral graph embedding concept to characterise and effectively represent handwritten characters, taking into account writing styles, cursiveness and relationships. For corroboration of the efficacy of the proposed method, extensive experiments were carried out on the standard handwritten numeral Computer Vision Pattern Recognition, Unit of Indian Statistical Institute Kolkata dataset. The experimental results demonstrate promising findings, which can be used in future studies.



### LabelEnc: A New Intermediate Supervision Method for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.03282v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03282v3)
- **Published**: 2020-07-07 08:55:05+00:00
- **Updated**: 2020-09-01 02:37:24+00:00
- **Authors**: Miao Hao, Yitao Liu, Xiangyu Zhang, Jian Sun
- **Comment**: To appear in ECCV 2020. Code is available at
  https://github.com/megvii-model/LabelEnc
- **Journal**: None
- **Summary**: In this paper we propose a new intermediate supervision method, named LabelEnc, to boost the training of object detection systems. The key idea is to introduce a novel label encoding function, mapping the ground-truth labels into latent embedding, acting as an auxiliary intermediate supervision to the detection backbone during training. Our approach mainly involves a two-step training procedure. First, we optimize the label encoding function via an AutoEncoder defined in the label space, approximating the "desired" intermediate representations for the target object detector. Second, taking advantage of the learned label encoding function, we introduce a new auxiliary loss attached to the detection backbones, thus benefiting the performance of the derived detector. Experiments show our method improves a variety of detection systems by around 2% on COCO dataset, no matter one-stage or two-stage frameworks. Moreover, the auxiliary structures only exist during training, i.e. it is completely cost-free in inference time. Code is available at: https://github.com/megvii-model/LabelEnc



### Divide-and-Rule: Self-Supervised Learning for Survival Analysis in Colorectal Cancer
- **Arxiv ID**: http://arxiv.org/abs/2007.03292v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03292v1)
- **Published**: 2020-07-07 09:15:36+00:00
- **Updated**: 2020-07-07 09:15:36+00:00
- **Authors**: Christian Abbet, Inti Zlobec, Behzad Bozorgtabar, Jean-Philippe Thiran
- **Comment**: None
- **Journal**: None
- **Summary**: With the long-term rapid increase in incidences of colorectal cancer (CRC), there is an urgent clinical need to improve risk stratification. The conventional pathology report is usually limited to only a few histopathological features. However, most of the tumor microenvironments used to describe patterns of aggressive tumor behavior are ignored. In this work, we aim to learn histopathological patterns within cancerous tissue regions that can be used to improve prognostic stratification for colorectal cancer. To do so, we propose a self-supervised learning method that jointly learns a representation of tissue regions as well as a metric of the clustering to obtain their underlying patterns. These histopathological patterns are then used to represent the interaction between complex tissues and predict clinical outcomes directly. We furthermore show that the proposed approach can benefit from linear predictors to avoid overfitting in patient outcomes predictions. To this end, we introduce a new well-characterized clinicopathological dataset, including a retrospective collective of 374 patients, with their survival time and treatment information. Histomorphological clusters obtained by our method are evaluated by training survival models. The experimental results demonstrate statistically significant patient stratification, and our approach outperformed the state-of-the-art deep clustering methods.



### Automatic Ischemic Stroke Lesion Segmentation from Computed Tomography Perfusion Images by Image Synthesis and Attention-Based Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.03294v1
- **DOI**: 10.1016/j.media.2020.101787
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03294v1)
- **Published**: 2020-07-07 09:19:23+00:00
- **Updated**: 2020-07-07 09:19:23+00:00
- **Authors**: Guotai Wang, Tao Song, Qiang Dong, Mei Cui, Ning Huang, Shaoting Zhang
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: Ischemic stroke lesion segmentation from Computed Tomography Perfusion (CTP) images is important for accurate diagnosis of stroke in acute care units. However, it is challenged by low image contrast and resolution of the perfusion parameter maps, in addition to the complex appearance of the lesion. To deal with this problem, we propose a novel framework based on synthesized pseudo Diffusion-Weighted Imaging (DWI) from perfusion parameter maps to obtain better image quality for more accurate segmentation. Our framework consists of three components based on Convolutional Neural Networks (CNNs) and is trained end-to-end. First, a feature extractor is used to obtain both a low-level and high-level compact representation of the raw spatiotemporal Computed Tomography Angiography (CTA) images. Second, a pseudo DWI generator takes as input the concatenation of CTP perfusion parameter maps and our extracted features to obtain the synthesized pseudo DWI. To achieve better synthesis quality, we propose a hybrid loss function that pays more attention to lesion regions and encourages high-level contextual consistency. Finally, we segment the lesion region from the synthesized pseudo DWI, where the segmentation network is based on switchable normalization and channel calibration for better performance. Experimental results showed that our framework achieved the top performance on ISLES 2018 challenge and: 1) our method using synthesized pseudo DWI outperformed methods segmenting the lesion from perfusion parameter maps directly; 2) the feature extractor exploiting additional spatiotemporal CTA images led to better synthesized pseudo DWI quality and higher segmentation accuracy; and 3) the proposed loss functions and network structure improved the pseudo DWI synthesis and lesion segmentation performance.



### Learning to Generate Novel Domains for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2007.03304v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03304v3)
- **Published**: 2020-07-07 09:34:17+00:00
- **Updated**: 2021-03-09 11:50:54+00:00
- **Authors**: Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang
- **Comment**: ECCV'20
- **Journal**: None
- **Summary**: This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model's ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.



### DAM: Deliberation, Abandon and Memory Networks for Generating Detailed and Non-repetitive Responses in Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2007.03310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.03310v1)
- **Published**: 2020-07-07 09:49:47+00:00
- **Updated**: 2020-07-07 09:49:47+00:00
- **Authors**: Xiaoze Jiang, Jing Yu, Yajing Sun, Zengchang Qin, Zihao Zhu, Yue Hu, Qi Wu
- **Comment**: Accepted by IJCAI 2020. SOLE copyright holder is IJCAI (International
  Joint Conferences on Artificial Intelligence)
- **Journal**: None
- **Summary**: Visual Dialogue task requires an agent to be engaged in a conversation with human about an image. The ability of generating detailed and non-repetitive responses is crucial for the agent to achieve human-like conversation. In this paper, we propose a novel generative decoding architecture to generate high-quality responses, which moves away from decoding the whole encoded semantics towards the design that advocates both transparency and flexibility. In this architecture, word generation is decomposed into a series of attention-based information selection steps, performed by the novel recurrent Deliberation, Abandon and Memory (DAM) module. Each DAM module performs an adaptive combination of the response-level semantics captured from the encoder and the word-level semantics specifically selected for generating each word. Therefore, the responses contain more detailed and non-repetitive descriptions while maintaining the semantic accuracy. Furthermore, DAM is flexible to cooperate with existing visual dialogue encoders and adaptive to the encoder structures by constraining the information selection mode in DAM. We apply DAM to three typical encoders and verify the performance on the VisDial v1.0 dataset. Experimental results show that the proposed models achieve new state-of-the-art performance with high-quality responses. The code is available at https://github.com/JXZe/DAM.



### Structured (De)composable Representations Trained with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.03325v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03325v1)
- **Published**: 2020-07-07 10:20:31+00:00
- **Updated**: 2020-07-07 10:20:31+00:00
- **Authors**: Graham Spinks, Marie-Francine Moens
- **Comment**: None
- **Journal**: None
- **Summary**: The paper proposes a novel technique for representing templates and instances of concept classes. A template representation refers to the generic representation that captures the characteristics of an entire class. The proposed technique uses end-to-end deep learning to learn structured and composable representations from input images and discrete labels. The obtained representations are based on distance estimates between the distributions given by the class label and those given by contextual information, which are modeled as environments. We prove that the representations have a clear structure allowing to decompose the representation into factors that represent classes and environments. We evaluate our novel technique on classification and retrieval tasks involving different modalities (visual and language data).



### GOLD-NAS: Gradual, One-Level, Differentiable
- **Arxiv ID**: http://arxiv.org/abs/2007.03331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.03331v1)
- **Published**: 2020-07-07 10:37:49+00:00
- **Updated**: 2020-07-07 10:37:49+00:00
- **Authors**: Kaifeng Bi, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: There has been a large literature of neural architecture search, but most existing work made use of heuristic rules that largely constrained the search flexibility. In this paper, we first relax these manually designed constraints and enlarge the search space to contain more than $10^{160}$ candidates. In the new space, most existing differentiable search methods can fail dramatically. We then propose a novel algorithm named Gradual One-Level Differentiable Neural Architecture Search (GOLD-NAS) which introduces a variable resource constraint to one-level optimization so that the weak operators are gradually pruned out from the super-network. In standard image classification benchmarks, GOLD-NAS can find a series of Pareto-optimal architectures within a single search procedure. Most of the discovered architectures were never studied before, yet they achieve a nice tradeoff between recognition accuracy and model complexity. We believe the new space and search algorithm can advance the search of differentiable NAS.



### Diverse and Styled Image Captioning Using SVD-Based Mixture of Recurrent Experts
- **Arxiv ID**: http://arxiv.org/abs/2007.03338v1
- **DOI**: 10.1002/cpe.6866
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03338v1)
- **Published**: 2020-07-07 11:00:27+00:00
- **Updated**: 2020-07-07 11:00:27+00:00
- **Authors**: Marzieh Heidari, Mehdi Ghatee, Ahmad Nickabadi, Arash Pourhasan Nezhad
- **Comment**: 13 pages, 4 figures and 5 tables, extracted from an MSc thesis in the
  Amirkabir University of Technology, Tehran, Iran
- **Journal**: Concurrency and Computation: Practice and Experience, 2022
- **Summary**: With great advances in vision and natural language processing, the generation of image captions becomes a need. In a recent paper, Mathews, Xie and He [1], extended a new model to generate styled captions by separating semantics and style. In continuation of this work, here a new captioning model is developed including an image encoder to extract the features, a mixture of recurrent networks to embed the set of extracted features to a set of words, and a sentence generator that combines the obtained words as a stylized sentence. The resulted system that entitled as Mixture of Recurrent Experts (MoRE), uses a new training algorithm that derives singular value decomposition (SVD) from weighting matrices of Recurrent Neural Networks (RNNs) to increase the diversity of captions. Each decomposition step depends on a distinctive factor based on the number of RNNs in MoRE. Since the used sentence generator gives a stylized language corpus without paired images, our captioning model can do the same. Besides, the styled and diverse captions are extracted without training on a densely labeled or styled dataset. To validate this captioning model, we use Microsoft COCO which is a standard factual image caption dataset. We show that the proposed captioning model can generate a diverse and stylized image captions without the necessity of extra-labeling. The results also show better descriptions in terms of content accuracy.



### SpinalNet: Deep Neural Network with Gradual Input
- **Arxiv ID**: http://arxiv.org/abs/2007.03347v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03347v3)
- **Published**: 2020-07-07 11:27:00+00:00
- **Updated**: 2022-01-07 05:48:48+00:00
- **Authors**: H M Dipu Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, Amir F Atiya, Saeid Nahavandi, Dipti Srinivasan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved the state of the art performance in numerous fields. However, DNNs need high computation times, and people always expect better performance in a lower computation. Therefore, we study the human somatosensory system and design a neural network (SpinalNet) to achieve higher accuracy with fewer computations. Hidden layers in traditional NNs receive inputs in the previous layer, apply activation function, and then transfer the outcomes to the next layer. In the proposed SpinalNet, each layer is split into three splits: 1) input split, 2) intermediate split, and 3) output split. Input split of each layer receives a part of the inputs. The intermediate split of each layer receives outputs of the intermediate split of the previous layer and outputs of the input split of the current layer. The number of incoming weights becomes significantly lower than traditional DNNs. The SpinalNet can also be used as the fully connected or classification layer of DNN and supports both traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. Traditional learning on the VGG-5 network with SpinalNet classification layers provided the state-of-the-art (SOTA) performance on QMNIST, Kuzushiji-MNIST, EMNIST (Letters, Digits, and Balanced) datasets. Traditional learning with ImageNet pre-trained initial weights and SpinalNet classification layers provided the SOTA performance on STL-10, Fruits 360, Bird225, and Caltech-101 datasets. The scripts of the proposed SpinalNet are available at the following link: https://github.com/dipuk0506/SpinalNet



### RIFLE: Backpropagation in Depth for Deep Transfer Learning through Re-Initializing the Fully-connected LayEr
- **Arxiv ID**: http://arxiv.org/abs/2007.03349v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03349v1)
- **Published**: 2020-07-07 11:27:43+00:00
- **Updated**: 2020-07-07 11:27:43+00:00
- **Authors**: Xingjian Li, Haoyi Xiong, Haozhe An, Chengzhong Xu, Dejing Dou
- **Comment**: Accepted by ICML'2020
- **Journal**: None
- **Summary**: Fine-tuning the deep convolution neural network(CNN) using a pre-trained model helps transfer knowledge learned from larger datasets to the target task. While the accuracy could be largely improved even when the training dataset is small, the transfer learning outcome is usually constrained by the pre-trained model with close CNN weights (Liu et al., 2019), as the backpropagation here brings smaller updates to deeper CNN layers. In this work, we propose RIFLE - a simple yet effective strategy that deepens backpropagation in transfer learning settings, through periodically Re-Initializing the Fully-connected LayEr with random scratch during the fine-tuning procedure. RIFLE brings meaningful updates to the weights of deep CNN layers and improves low-level feature learning, while the effects of randomization can be easily converged throughout the overall learning procedure. The experiments show that the use of RIFLE significantly improves deep transfer learning accuracy on a wide range of datasets, out-performing known tricks for the similar purpose, such as Dropout, DropConnect, StochasticDepth, Disturb Label and Cyclic Learning Rate, under the same settings with 0.5% -2% higher testing accuracy. Empirical cases and ablation studies further indicate RIFLE brings meaningful updates to deep CNN layers with accuracy improved.



### Learning and Reasoning with the Graph Structure Representation in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2007.03357v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03357v3)
- **Published**: 2020-07-07 11:49:34+00:00
- **Updated**: 2020-09-10 21:50:45+00:00
- **Authors**: Mobarakol Islam, Lalithkumar Seenivasan, Lim Chwee Ming, Hongliang Ren
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Learning to infer graph representations and performing spatial reasoning in a complex surgical environment can play a vital role in surgical scene understanding in robotic surgery. For this purpose, we develop an approach to generate the scene graph and predict surgical interactions between instruments and surgical region of interest (ROI) during robot-assisted surgery. We design an attention link function and integrate with a graph parsing network to recognize the surgical interactions. To embed each node with corresponding neighbouring node features, we further incorporate SageConv into the network. The scene graph generation and active edge classification mostly depend on the embedding or feature extraction of node and edge features from complex image representation. Here, we empirically demonstrate the feature extraction methods by employing label smoothing weighted loss. Smoothing the hard label can avoid the over-confident prediction of the model and enhances the feature representation learned by the penultimate layer. To obtain the graph scene label, we annotate the bounding box and the instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery and employ it to evaluate our propositions.



### Hierarchical and Unsupervised Graph Representation Learning with Loukas's Coarsening
- **Arxiv ID**: http://arxiv.org/abs/2007.03373v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03373v2)
- **Published**: 2020-07-07 12:04:38+00:00
- **Updated**: 2020-08-17 15:15:18+00:00
- **Authors**: Louis Bthune, Yacouba Kaloga, Pierre Borgnat, Aurlien Garivier, Amaury Habrard
- **Comment**: 19 pages, 15 figures, submitted
- **Journal**: None
- **Summary**: We propose a novel algorithm for unsupervised graph representation learning with attributed graphs. It combines three advantages addressing some current limitations of the literature: i) The model is inductive: it can embed new graphs without re-training in the presence of new data; ii) The method takes into account both micro-structures and macro-structures by looking at the attributed graphs at different scales; iii) The model is end-to-end differentiable: it is a building block that can be plugged into deep learning pipelines and allows for back-propagation. We show that combining a coarsening method having strong theoretical guarantees with mutual information maximization suffices to produce high quality embeddings. We evaluate them on classification tasks with common benchmarks of the literature. We show that our algorithm is competitive with state of the art among unsupervised graph representation learning methods.



### Location Sensitive Image Retrieval and Tagging
- **Arxiv ID**: http://arxiv.org/abs/2007.03375v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2007.03375v1)
- **Published**: 2020-07-07 12:09:01+00:00
- **Updated**: 2020-07-07 12:09:01+00:00
- **Authors**: Raul Gomez, Jaume Gibert, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: People from different parts of the globe describe objects and concepts in distinct manners. Visual appearance can thus vary across different geographic locations, which makes location a relevant contextual information when analysing visual data. In this work, we address the task of image retrieval related to a given tag conditioned on a certain location on Earth. We present LocSens, a model that learns to rank triplets of images, tags and coordinates by plausibility, and two training strategies to balance the location influence in the final ranking. LocSens learns to fuse textual and location information of multimodal queries to retrieve related images at different levels of location granularity, and successfully utilizes location information to improve image tagging.



### C2G-Net: Exploiting Morphological Properties for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.03378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03378v1)
- **Published**: 2020-07-07 12:16:17+00:00
- **Updated**: 2020-07-07 12:16:17+00:00
- **Authors**: Laurin Herbsthofer, Barbara Prietl, Martina Tomberger, Thomas Pieber, Pablo Lpez-Garca
- **Comment**: 10 pages, 5 figures (Figure 3 with 4 sub-figures), Appendix A and
  Appendix B after the references. Originally submitted to ICML2020 but
  rejected
- **Journal**: None
- **Summary**: In this paper we propose C2G-Net, a pipeline for image classification that exploits the morphological properties of images containing a large number of similar objects like biological cells. C2G-Net consists of two components: (1) Cell2Grid, an image compression algorithm that identifies objects using segmentation and arranges them on a grid, and (2) DeepLNiNo, a CNN architecture with less than 10,000 trainable parameters aimed at facilitating model interpretability. To test the performance of C2G-Net we used multiplex immunohistochemistry images for predicting relapse risk in colon cancer. Compared to conventional CNN architectures trained on raw images, C2G-Net achieved similar prediction accuracy while training time was reduced by 85% and its model was is easier to interpret.



### Re-thinking Co-Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.03380v4
- **DOI**: 10.1109/TPAMI.2021.3060412
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03380v4)
- **Published**: 2020-07-07 12:20:51+00:00
- **Updated**: 2021-05-02 01:47:19+00:00
- **Authors**: Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng, Huazhu Fu, Jianbing Shen
- **Comment**: 22pages, 18 figures. CVPR2020-CoSOD3K extension. Code:
  https://github.com/DengPingFan/CoEGNet
- **Journal**: TPAMI 2021
- **Summary**: In this paper, we conduct a comprehensive study on the co-salient object detection (CoSOD) problem for images. CoSOD is an emerging and rapidly growing extension of salient object detection (SOD), which aims to detect the co-occurring salient objects in a group of images. However, existing CoSOD datasets often have a serious data bias, assuming that each group of images contains salient objects of similar visual appearances. This bias can lead to the ideal settings and effectiveness of models trained on existing datasets, being impaired in real-life situations, where similarities are usually semantic or conceptual. To tackle this issue, we first introduce a new benchmark, called CoSOD3k in the wild, which requires a large amount of semantic context, making it more challenging than existing CoSOD datasets. Our CoSOD3k consists of 3,316 high-quality, elaborately selected images divided into 160 groups with hierarchical annotations. The images span a wide range of categories, shapes, object sizes, and backgrounds. Second, we integrate the existing SOD techniques to build a unified, trainable CoSOD framework, which is long overdue in this field. Specifically, we propose a novel CoEG-Net that augments our prior model EGNet with a co-attention projection strategy to enable fast common information learning. CoEG-Net fully leverages previous large-scale SOD datasets and significantly improves the model scalability and stability. Third, we comprehensively summarize 40 cutting-edge algorithms, benchmarking 18 of them over three challenging CoSOD datasets (iCoSeg, CoSal2015, and our CoSOD3k), and reporting more detailed (i.e., group-level) performance analysis. Finally, we discuss the challenges and future works of CoSOD. We hope that our study will give a strong boost to growth in the CoSOD community. The benchmark toolbox and results are available on our project page at http://dpfan.net/CoSOD3K/.



### Are spoofs from latent fingerprints a real threat for the best state-of-art liveness detectors?
- **Arxiv ID**: http://arxiv.org/abs/2007.03397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03397v2)
- **Published**: 2020-07-07 13:04:40+00:00
- **Updated**: 2020-10-17 15:03:59+00:00
- **Authors**: Roberto Casula, Giulia Orr, Daniele Angioni, Xiaoyi Feng, Gian Luca Marcialis, Fabio Roli
- **Comment**: Accepted for the 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: We investigated the threat level of realistic attacks using latent fingerprints against sensors equipped with state-of-art liveness detectors and fingerprint verification systems which integrate such liveness algorithms. To the best of our knowledge, only a previous investigation was done with spoofs from latent prints. In this paper, we focus on using snapshot pictures of latent fingerprints. These pictures provide molds, that allows, after some digital processing, to fabricate high-quality spoofs. Taking a snapshot picture is much simpler than developing fingerprints left on a surface by magnetic powders and lifting the trace by a tape. What we are interested here is to evaluate preliminary at which extent attacks of the kind can be considered a real threat for state-of-art fingerprint liveness detectors and verification systems. To this aim, we collected a novel data set of live and spoof images fabricated with snapshot pictures of latent fingerprints. This data set provide a set of attacks at the most favourable conditions. We refer to this method and the related data set as "ScreenSpoof". Then, we tested with it the performances of the best liveness detection algorithms, namely, the three winners of the LivDet competition. Reported results point out that the ScreenSpoof method is a threat of the same level, in terms of detection and verification errors, than that of attacks using spoofs fabricated with the full consensus of the victim. We think that this is a notable result, never reported in previous work.



### Optical Navigation in Unstructured Dynamic Railroad Environments
- **Arxiv ID**: http://arxiv.org/abs/2007.03409v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03409v1)
- **Published**: 2020-07-07 13:21:47+00:00
- **Updated**: 2020-07-07 13:21:47+00:00
- **Authors**: Darius Burschka, Christian Robl, Sebastian Ohrendorf-Weiss
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for optical navigation in unstructured, dynamic railroad environments. We propose a way how to cope with the estimation of the train motion from sole observations of the planar track bed. The occasional significant occlusions during the operation of the train limit the available observation to this difficult to track, repetitive area. This approach is a step towards replacement of the expensive train management infrastructure with local intelligence on the train for SmartRail 4.0.   We derive our approach for robust estimation of translation and rotation in this difficult environments and provide experimental validation of the approach on real rail scenarios.



### Unsupervised CT Metal Artifact Learning using Attention-guided beta-CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2007.03480v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03480v1)
- **Published**: 2020-07-07 14:11:47+00:00
- **Updated**: 2020-07-07 14:11:47+00:00
- **Authors**: Junghyun Lee, Jawook Gu, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Metal artifact reduction (MAR) is one of the most important research topics in computed tomography (CT). With the advance of deep learning technology for image reconstruction,various deep learning methods have been also suggested for metal artifact removal, among which supervised learning methods are most popular. However, matched non-metal and metal image pairs are difficult to obtain in real CT acquisition. Recently, a promising unsupervised learning for MAR was proposed using feature disentanglement, but the resulting network architecture is complication and difficult to handle large size clinical images. To address this, here we propose a much simpler and much effective unsupervised MAR method for CT. The proposed method is based on a novel beta-cycleGAN architecture derived from the optimal transport theory for appropriate feature space disentanglement. Another important contribution is to show that attention mechanism is the key element to effectively remove the metal artifacts. Specifically, by adding the convolutional block attention module (CBAM) layers with a proper disentanglement parameter, experimental results confirm that we can get more improved MAR that preserves the detailed texture of the original image.



### AutoAssign: Differentiable Label Assignment for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.03496v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03496v3)
- **Published**: 2020-07-07 14:32:21+00:00
- **Updated**: 2020-11-25 15:57:47+00:00
- **Authors**: Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu, Zeming Li, Jian Sun
- **Comment**: Revised for submission
- **Journal**: None
- **Summary**: Determining positive/negative samples for object detection is known as label assignment. Here we present an anchor-free detector named AutoAssign. It requires little human knowledge and achieves appearance-aware through a fully differentiable weighting mechanism. During training, to both satisfy the prior distribution of data and adapt to category characteristics, we present Center Weighting to adjust the category-specific prior distributions. To adapt to object appearances, Confidence Weighting is proposed to adjust the specific assign strategy of each instance. The two weighting modules are then combined to generate positive and negative weights to adjust each location's confidence. Extensive experiments on the MS COCO show that our method steadily surpasses other best sampling strategies by large margins with various backbones. Moreover, our best model achieves 52.1% AP, outperforming all existing one-stage detectors. Besides, experiments on other datasets, e.g., PASCAL VOC, Objects365, and WiderFace, demonstrate the broad applicability of AutoAssign.



### Hierarchical nucleation in deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2007.03506v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03506v2)
- **Published**: 2020-07-07 14:42:18+00:00
- **Updated**: 2020-07-09 15:14:19+00:00
- **Authors**: Diego Doimo, Aldo Glielmo, Alessio Ansuini, Alessandro Laio
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks (DCNs) learn meaningful representations where data that share the same abstract characteristics are positioned closer and closer. Understanding these representations and how they are generated is of unquestioned practical and theoretical interest. In this work we study the evolution of the probability density of the ImageNet dataset across the hidden layers in some state-of-the-art DCNs. We find that the initial layers generate a unimodal probability density getting rid of any structure irrelevant for classification. In subsequent layers density peaks arise in a hierarchical fashion that mirrors the semantic hierarchy of the concepts. Density peaks corresponding to single categories appear only close to the output and via a very sharp transition which resembles the nucleation process of a heterogeneous liquid. This process leaves a footprint in the probability density of the output layer where the topography of the peaks allows reconstructing the semantic relationships of the categories.



### Imitation Learning Approach for AI Driving Olympics Trained on Real-world and Simulation Data Simultaneously
- **Arxiv ID**: http://arxiv.org/abs/2007.03514v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03514v1)
- **Published**: 2020-07-07 14:48:11+00:00
- **Updated**: 2020-07-07 14:48:11+00:00
- **Authors**: Mikita Sazanovich, Konstantin Chaika, Kirill Krinkin, Aleksei Shpilman
- **Comment**: Accepted to the Workshop on AI for Autonomous Driving (AIAD), the
  37th International Conference on Machine Learning (ICML2020)
- **Journal**: None
- **Summary**: In this paper, we describe our winning approach to solving the Lane Following Challenge at the AI Driving Olympics Competition through imitation learning on a mixed set of simulation and real-world data. AI Driving Olympics is a two-stage competition: at stage one, algorithms compete in a simulated environment with the best ones advancing to a real-world final. One of the main problems that participants encounter during the competition is that algorithms trained for the best performance in simulated environments do not hold up in a real-world environment and vice versa. Classic control algorithms also do not translate well between tasks since most of them have to be tuned to specific driving conditions such as lighting, road type, camera position, etc. To overcome this problem, we employed the imitation learning algorithm and trained it on a dataset collected from sources both from simulation and real-world, forcing our model to perform equally well in all environments.



### 3D Topology Transformation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.03532v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03532v1)
- **Published**: 2020-07-07 15:03:16+00:00
- **Updated**: 2020-07-07 15:03:16+00:00
- **Authors**: Luca Stornaiuolo, Nima Dehmamy, Albert-Lszl Barabsi, Mauro Martino
- **Comment**: None
- **Journal**: None
- **Summary**: Generation and transformation of images and videos using artificial intelligence have flourished over the past few years. Yet, there are only a few works aiming to produce creative 3D shapes, such as sculptures. Here we show a novel 3D-to-3D topology transformation method using Generative Adversarial Networks (GAN). We use a modified pix2pix GAN, which we call Vox2Vox, to transform the volumetric style of a 3D object while retaining the original object shape. In particular, we show how to transform 3D models into two new volumetric topologies - the 3D Network and the Ghirigoro. We describe how to use our approach to construct customized 3D representations. We believe that the generated 3D shapes are novel and inspirational. Finally, we compare the results between our approach and a baseline algorithm that directly convert the 3D shapes, without using our GAN.



### Light Field Image Super-Resolution Using Deformable Convolution
- **Arxiv ID**: http://arxiv.org/abs/2007.03535v4
- **DOI**: 10.1109/TIP.2020.3042059
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03535v4)
- **Published**: 2020-07-07 15:07:33+00:00
- **Updated**: 2020-11-25 12:01:05+00:00
- **Authors**: Yingqian Wang, Jungang Yang, Longguang Wang, Xinyi Ying, Tianhao Wu, Wei An, Yulan Guo
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Light field (LF) cameras can record scenes from multiple perspectives, and thus introduce beneficial angular information for image super-resolution (SR). However, it is challenging to incorporate angular information due to disparities among LF images. In this paper, we propose a deformable convolution network (i.e., LF-DFnet) to handle the disparity problem for LF image SR. Specifically, we design an angular deformable alignment module (ADAM) for feature-level alignment. Based on ADAM, we further propose a collect-and-distribute approach to perform bidirectional alignment between the center-view feature and each side-view feature. Using our approach, angular information can be well incorporated and encoded into features of each view, which benefits the SR reconstruction of all LF images. Moreover, we develop a baseline-adjustable LF dataset to evaluate SR performance under different disparity variations. Experiments on both public and our self-developed datasets have demonstrated the superiority of our method. Our LF-DFnet can generate high-resolution images with more faithful details and achieve state-of-the-art reconstruction accuracy. Besides, our LF-DFnet is more robust to disparity variations, which has not been well addressed in literature.



### Meta Corrupted Pixels Mining for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.03538v1
- **DOI**: None
- **Categories**: **cs.CV**, J.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.03538v1)
- **Published**: 2020-07-07 15:12:20+00:00
- **Updated**: 2020-07-07 15:12:20+00:00
- **Authors**: Jixin Wang, Sanping Zhou, Chaowei Fang, Le Wang, Jinjun Wang
- **Comment**: Accepted By MICCAI2020
- **Journal**: None
- **Summary**: Deep neural networks have achieved satisfactory performance in piles of medical image analysis tasks. However the training of deep neural network requires a large amount of samples with high-quality annotations. In medical image segmentation, it is very laborious and expensive to acquire precise pixel-level annotations. Aiming at training deep segmentation models on datasets with probably corrupted annotations, we propose a novel Meta Corrupted Pixels Mining (MCPM) method based on a simple meta mask network. Our method is targeted at automatically estimate a weighting map to evaluate the importance of every pixel in the learning of segmentation network. The meta mask network which regards the loss value map of the predicted segmentation results as input, is capable of identifying out corrupted layers and allocating small weights to them. An alternative algorithm is adopted to train the segmentation network and the meta mask network, simultaneously. Extensive experimental results on LIDC-IDRI and LiTS datasets show that our method outperforms state-of-the-art approaches which are devised for coping with corrupted annotations.



### On Learning Semantic Representations for Million-Scale Free-Hand Sketches
- **Arxiv ID**: http://arxiv.org/abs/2007.04101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04101v1)
- **Published**: 2020-07-07 15:23:22+00:00
- **Updated**: 2020-07-07 15:23:22+00:00
- **Authors**: Peng Xu, Yongye Huang, Tongtong Yuan, Tao Xiang, Timothy M. Hospedales, Yi-Zhe Song, Liang Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1804.01401
- **Journal**: None
- **Summary**: In this paper, we study learning semantic representations for million-scale free-hand sketches. This is highly challenging due to the domain-unique traits of sketches, e.g., diverse, sparse, abstract, noisy. We propose a dual-branch CNNRNN network architecture to represent sketches, which simultaneously encodes both the static and temporal patterns of sketch strokes. Based on this architecture, we further explore learning the sketch-oriented semantic representations in two challenging yet practical settings, i.e., hashing retrieval and zero-shot recognition on million-scale sketches. Specifically, we use our dual-branch architecture as a universal representation framework to design two sketch-specific deep models: (i) We propose a deep hashing model for sketch retrieval, where a novel hashing loss is specifically designed to accommodate both the abstract and messy traits of sketches. (ii) We propose a deep embedding model for sketch zero-shot recognition, via collecting a large-scale edge-map dataset and proposing to extract a set of semantic vectors from edge-maps as the semantic knowledge for sketch zero-shot domain alignment. Both deep models are evaluated by comprehensive experiments on million-scale sketches and outperform the state-of-the-art competitors.



### Single Shot Video Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2007.03560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03560v1)
- **Published**: 2020-07-07 15:36:26+00:00
- **Updated**: 2020-07-07 15:36:26+00:00
- **Authors**: Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, Tao Mei
- **Comment**: Accepted by IEEE Transactions on Multimedia; The code is available at
  \url{https://github.com/ddjiajun/SSVD}
- **Journal**: None
- **Summary**: Single shot detectors that are potentially faster and simpler than two-stage detectors tend to be more applicable to object detection in videos. Nevertheless, the extension of such object detectors from image to video is not trivial especially when appearance deterioration exists in videos, \emph{e.g.}, motion blur or occlusion. A valid question is how to explore temporal coherence across frames for boosting detection. In this paper, we propose to address the problem by enhancing per-frame features through aggregation of neighboring frames. Specifically, we present Single Shot Video Object Detector (SSVD) -- a new architecture that novelly integrates feature aggregation into a one-stage detector for object detection in videos. Technically, SSVD takes Feature Pyramid Network (FPN) as backbone network to produce multi-scale features. Unlike the existing feature aggregation methods, SSVD, on one hand, estimates the motion and aggregates the nearby features along the motion path, and on the other, hallucinates features by directly sampling features from the adjacent frames in a two-stream structure. Extensive experiments are conducted on ImageNet VID dataset, and competitive results are reported when comparing to state-of-the-art approaches. More remarkably, for $448 \times 448$ input, SSVD achieves 79.2% mAP on ImageNet VID, by processing one frame in 85 ms on an Nvidia Titan X Pascal GPU. The code is available at \url{https://github.com/ddjiajun/SSVD}.



### A Vision-based Social Distancing and Critical Density Detection System for COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2007.03578v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03578v2)
- **Published**: 2020-07-07 15:55:50+00:00
- **Updated**: 2020-07-08 22:53:16+00:00
- **Authors**: Dongfang Yang, Ekim Yurtsever, Vishnu Renganathan, Keith A. Redmill, mit zgner
- **Comment**: None
- **Journal**: None
- **Summary**: Social distancing has been proven as an effective measure against the spread of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are not used to tracking the required 6-feet (2-meters) distance between themselves and their surroundings. An active surveillance system capable of detecting distances between individuals and warning them can slow down the spread of the deadly disease. Furthermore, measuring social density in a region of interest (ROI) and modulating inflow can decrease social distancing violation occurrence chance.   On the other hand, recording data and labeling individuals who do not follow the measures will breach individuals' rights in free-societies. Here we propose an Artificial Intelligence (AI) based real-time social distancing detection and warning system considering four important ethical factors: (1) the system should never record/cache data, (2) the warnings should not target the individuals, (3) no human supervisor should be in the detection/warning loop, and (4) the code should be open-source and accessible to the public. Against this backdrop, we propose using a monocular camera and deep learning-based real-time object detectors to measure social distancing. If a violation is detected, a non-intrusive audio-visual warning signal is emitted without targeting the individual who breached the social distancing measure. Also, if the social density is over a critical value, the system sends a control signal to modulate inflow into the ROI. We tested the proposed method across real-world datasets to measure its generality and performance. The proposed method is ready for deployment, and our code is open-sourced.



### HKR For Handwritten Kazakh & Russian Database
- **Arxiv ID**: http://arxiv.org/abs/2007.03579v2
- **DOI**: 10.1007/s11042-021-11399-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03579v2)
- **Published**: 2020-07-07 15:57:41+00:00
- **Updated**: 2020-07-08 16:53:54+00:00
- **Authors**: Daniyar Nurseitov, Kairat Bostanbekov, Daniyar Kurmankhojayev, Anel Alimova, Abdelrahman Abdallah
- **Comment**: Multimed Tools Appl (2021)
- **Journal**: None
- **Summary**: In this paper, we present a new Russian and Kazakh database (with about 95% of Russian and 5% of Kazakh words/sentences respectively) for offline handwriting recognition. A few pre-processing and segmentation procedures have been developed together with the database. The database is written in Cyrillic and shares the same 33 characters. Besides these characters, the Kazakh alphabet also contains 9 additional specific characters. This dataset is a collection of forms. The sources of all the forms in the datasets were generated by \LaTeX which subsequently was filled out by persons with their handwriting. The database consists of more than 1400 filled forms. There are approximately 63000 sentences, more than 715699 symbols produced by approximately 200 different writers. It can serve researchers in the field of handwriting recognition tasks by using deep and machine learning.



### STADB: A Self-Thresholding Attention Guided ADB Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2007.03584v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03584v3)
- **Published**: 2020-07-07 16:06:22+00:00
- **Updated**: 2021-08-19 01:09:00+00:00
- **Authors**: Bo Jiang, Sheng Wang, Xiao Wang, Aihua Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on person image representation and re-identification task via feature erasing. However, BDB drops the features \textbf{randomly} which may lead to sub-optimal results. In this paper, we propose a novel Self-Thresholding attention guided Adaptive DropBlock network (STADB) for person re-ID which can \textbf{adaptively} erase the most discriminative regions. Specifically, STADB first obtains an attention map by channel-wise pooling and returns a drop mask by thresholding the attention map. Then, the input features and self-thresholding attention guided drop mask are multiplied to generate the dropped feature maps. In addition, STADB utilizes the spatial and channel attention to learn a better feature map and iteratively trains the feature dropping module for person re-ID. Experiments on several benchmark datasets demonstrate that the proposed STADB outperforms many other related methods for person re-ID. The source code of this paper is released at: \textcolor{red}{\url{https://github.com/wangxiao5791509/STADB_ReID}}.



### Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment
- **Arxiv ID**: http://arxiv.org/abs/2007.03593v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03593v1)
- **Published**: 2020-07-07 16:23:10+00:00
- **Updated**: 2020-07-07 16:23:10+00:00
- **Authors**: Aadarsh Jha, Haichun Yang, Ruining Deng, Meghan E. Kapp, Agnes B. Fogo, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic instance segmentation of glomeruli within kidney Whole Slide Imaging (WSI) is essential for clinical research in renal pathology. In computer vision, the end-to-end instance segmentation methods (e.g., Mask-RCNN) have shown their advantages relative to detect-then-segment approaches by performing complementary detection and segmentation tasks simultaneously. As a result, the end-to-end Mask-RCNN approach has been the de facto standard method in recent glomerular segmentation studies, where downsampling and patch-based techniques are used to properly evaluate the high resolution images from WSI (e.g., >10,000x10,000 pixels on 40x). However, in high resolution WSI, a single glomerulus itself can be more than 1,000x1,000 pixels in original resolution which yields significant information loss when the corresponding features maps are downsampled via the Mask-RCNN pipeline. In this paper, we assess if the end-to-end instance segmentation framework is optimal for high-resolution WSI objects by comparing Mask-RCNN with our proposed detect-then-segment framework. Beyond such a comparison, we also comprehensively evaluate the performance of our detect-then-segment pipeline through: 1) two of the most prevalent segmentation backbones (U-Net and DeepLab_v3); 2) six different image resolutions (from 512x512 to 28x28); and 3) two different color spaces (RGB and LAB). Our detect-then-segment pipeline, with the DeepLab_v3 segmentation framework operating on previously detected glomeruli of 512x512 resolution, achieved a 0.953 dice similarity coefficient (DSC), compared with a 0.902 DSC from the end-to-end Mask-RCNN pipeline. Further, we found that neither RGB nor LAB color spaces yield better performance when compared against each other in the context of a detect-then-segment framework. Detect-then-segment pipeline achieved better segmentation performance compared with End-to-end method.



### Monitoring Browsing Behavior of Customers in Retail Stores via RFID Imaging
- **Arxiv ID**: http://arxiv.org/abs/2007.03600v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03600v1)
- **Published**: 2020-07-07 16:36:24+00:00
- **Updated**: 2020-07-07 16:36:24+00:00
- **Authors**: Kamran Ali, Alex X. Liu, Eugene Chai, Karthik Sundaresan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose to use commercial off-the-shelf (COTS) monostatic RFID devices (i.e. which use a single antenna at a time for both transmitting and receiving RFID signals to and from the tags) to monitor browsing activity of customers in front of display items in places such as retail stores. To this end, we propose TagSee, a multi-person imaging system based on monostatic RFID imaging. TagSee is based on the insight that when customers are browsing the items on a shelf, they stand between the tags deployed along the boundaries of the shelf and the reader, which changes the multi-paths that the RFID signals travel along, and both the RSS and phase values of the RFID signals that the reader receives change. Based on these variations observed by the reader, TagSee constructs a coarse grained image of the customers. Afterwards, TagSee identifies the items that are being browsed by the customers by analyzing the constructed images. The key novelty of this paper is on achieving browsing behavior monitoring of multiple customers in front of display items by constructing coarse grained images via robust, analytical model-driven deep learning based, RFID imaging. To achieve this, we first mathematically formulate the problem of imaging humans using monostatic RFID devices and derive an approximate analytical imaging model that correlates the variations caused by human obstructions in the RFID signals. Based on this model, we then develop a deep learning framework to robustly image customers with high accuracy. We implement TagSee scheme using a Impinj Speedway R420 reader and SMARTRAC DogBone RFID tags. TagSee can achieve a TPR of more than ~90% and a FPR of less than ~10% in multi-person scenarios using training data from just 3-4 users.



### A Weakly Supervised Region-Based Active Learning Method for COVID-19 Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2007.07012v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07012v1)
- **Published**: 2020-07-07 16:38:04+00:00
- **Updated**: 2020-07-07 16:38:04+00:00
- **Authors**: Issam Laradji, Pau Rodriguez, Frederic Branchaud-Charron, Keegan Lensink, Parmida Atighehchian, William Parker, David Vazquez, Derek Nowrouzezahrai
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key challenges in the battle against the Coronavirus (COVID-19) pandemic is to detect and quantify the severity of the disease in a timely manner. Computed tomographies (CT) of the lungs are effective for assessing the state of the infection. Unfortunately, labeling CT scans can take a lot of time and effort, with up to 150 minutes per scan. We address this challenge introducing a scalable, fast, and accurate active learning system that accelerates the labeling of CT scan images. Conventionally, active learning methods require the labelers to annotate whole images with full supervision, but that can lead to wasted efforts as many of the annotations could be redundant. Thus, our system presents the annotator with unlabeled regions that promise high information content and low annotation cost. Further, the system allows annotators to label regions using point-level supervision, which is much cheaper to acquire than per-pixel annotations. Our experiments on open-source COVID-19 datasets show that using an entropy-based method to rank unlabeled regions yields to significantly better results than random labeling of these regions. Also, we show that labeling small regions of images is more efficient than labeling whole images. Finally, we show that with only 7\% of the labeling effort required to label the whole training set gives us around 90\% of the performance obtained by training the model on the fully annotated training set. Code is available at: \url{https://github.com/IssamLaradji/covid19_active_learning}.



### Can GAN Generated Morphs Threaten Face Recognition Systems Equally as Landmark Based Morphs? -- Vulnerability and Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.03621v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03621v1)
- **Published**: 2020-07-07 16:52:56+00:00
- **Updated**: 2020-07-07 16:52:56+00:00
- **Authors**: Sushma Venkatesh, Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Naser Damer, Christoph Busch
- **Comment**: Accepted in IWBF 2020
- **Journal**: None
- **Summary**: The primary objective of face morphing is to combine face images of different data subjects (e.g. a malicious actor and an accomplice) to generate a face image that can be equally verified for both contributing data subjects. In this paper, we propose a new framework for generating face morphs using a newer Generative Adversarial Network (GAN) - StyleGAN. In contrast to earlier works, we generate realistic morphs of both high-quality and high resolution of 1024$\times$1024 pixels. With the newly created morphing dataset of 2500 morphed face images, we pose a critical question in this work. \textit{(i) Can GAN generated morphs threaten Face Recognition Systems (FRS) equally as Landmark based morphs?} Seeking an answer, we benchmark the vulnerability of a Commercial-Off-The-Shelf FRS (COTS) and a deep learning-based FRS (ArcFace). This work also benchmarks the detection approaches for both GAN generated morphs against the landmark based morphs using established Morphing Attack Detection (MAD) schemes.



### What Gives the Answer Away? Question Answering Bias Analysis on Video QA Datasets
- **Arxiv ID**: http://arxiv.org/abs/2007.03626v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03626v1)
- **Published**: 2020-07-07 17:00:11+00:00
- **Updated**: 2020-07-07 17:00:11+00:00
- **Authors**: Jianing Yang, Yuying Zhu, Yongxin Wang, Ruitao Yi, Amir Zadeh, Louis-Philippe Morency
- **Comment**: None
- **Journal**: None
- **Summary**: Question answering biases in video QA datasets can mislead multimodal model to overfit to QA artifacts and jeopardize the model's ability to generalize. Understanding how strong these QA biases are and where they come from helps the community measure progress more accurately and provide researchers insights to debug their models. In this paper, we analyze QA biases in popular video question answering datasets and discover pretrained language models can answer 37-48% questions correctly without using any multimodal context information, far exceeding the 20% random guess baseline for 5-choose-1 multiple-choice questions. Our ablation study shows biases can come from annotators and type of questions. Specifically, annotators that have been seen during training are better predicted by the model and reasoning, abstract questions incur more biases than factual, direct questions. We also show empirically that using annotator-non-overlapping train-test splits can reduce QA biases for video QA datasets.



### Scribble-based Domain Adaptation via Co-segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.03632v2
- **DOI**: 10.1007/978-3-030-59710-8_47
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03632v2)
- **Published**: 2020-07-07 17:09:30+00:00
- **Updated**: 2020-08-06 15:53:03+00:00
- **Authors**: Reuben Dorent, Samuel Joutard, Jonathan Shapey, Sotirios Bisdas, Neil Kitchen, Robert Bradford, Shakeel Saeed, Marc Modat, Sebastien Ourselin, Tom Vercauteren
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Although deep convolutional networks have reached state-of-the-art performance in many medical image segmentation tasks, they have typically demonstrated poor generalisation capability. To be able to generalise from one domain (e.g. one imaging modality) to another, domain adaptation has to be performed. While supervised methods may lead to good performance, they require to fully annotate additional data which may not be an option in practice. In contrast, unsupervised methods don't need additional annotations but are usually unstable and hard to train. In this work, we propose a novel weakly-supervised method. Instead of requiring detailed but time-consuming annotations, scribbles on the target domain are used to perform domain adaptation. This paper introduces a new formulation of domain adaptation based on structured learning and co-segmentation. Our method is easy to train, thanks to the introduction of a regularised loss. The framework is validated on Vestibular Schwannoma segmentation (T1 to T2 scans). Our proposed method outperforms unsupervised approaches and achieves comparable performance to a fully-supervised approach.



### Human Trajectory Forecasting in Crowds: A Deep Learning Perspective
- **Arxiv ID**: http://arxiv.org/abs/2007.03639v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03639v3)
- **Published**: 2020-07-07 17:19:56+00:00
- **Updated**: 2021-01-11 11:02:34+00:00
- **Authors**: Parth Kothari, Sven Kreiss, Alexandre Alahi
- **Comment**: IEEE format, Layer-wise Relevance Propagation added
- **Journal**: None
- **Summary**: Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, deployment of intelligent transport systems, traffic operations, to name a few. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learned about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning-based methods for modelling social interactions. We propose two knowledge-based data-driven methods to effectively capture these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets.



### Segmentation of Pulmonary Opacification in Chest CT Scans of COVID-19 Patients
- **Arxiv ID**: http://arxiv.org/abs/2007.03643v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03643v2)
- **Published**: 2020-07-07 17:32:24+00:00
- **Updated**: 2020-07-08 21:26:06+00:00
- **Authors**: Keegan Lensink, Issam Laradji, Marco Law, Paolo Emilio Barbano, Savvas Nicolaou, William Parker, Eldad Haber
- **Comment**: 9 pages, 5 figures. Fix typo in delimiter between author names in
  arXiv metadata
- **Journal**: None
- **Summary**: The Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has rapidly spread into a global pandemic. A form of pneumonia, presenting as opacities with in a patient's lungs, is the most common presentation associated with this virus, and great attention has gone into how these changes relate to patient morbidity and mortality. In this work we provide open source models for the segmentation of patterns of pulmonary opacification on chest Computed Tomography (CT) scans which have been correlated with various stages and severities of infection. We have collected 663 chest CT scans of COVID-19 patients from healthcare centers around the world, and created pixel wise segmentation labels for nearly 25,000 slices that segment 6 different patterns of pulmonary opacification. We provide open source implementations and pre-trained weights for multiple segmentation models trained on our dataset. Our best model achieves an opacity Intersection-Over-Union score of 0.76 on our test set, demonstrates successful domain adaptation, and predicts the volume of opacification within 1.7\% of expert radiologists. Additionally, we present an analysis of the inter-observer variability inherent to this task, and propose methods for appropriate probabilistic approaches.



### See, Hear, Explore: Curiosity via Audio-Visual Association
- **Arxiv ID**: http://arxiv.org/abs/2007.03669v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03669v2)
- **Published**: 2020-07-07 17:56:35+00:00
- **Updated**: 2021-01-18 16:26:54+00:00
- **Authors**: Victoria Dean, Shubham Tulsiani, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Exploration is one of the core challenges in reinforcement learning. A common formulation of curiosity-driven exploration uses the difference between the real future and the future predicted by a learned model. However, predicting the future is an inherently difficult task which can be ill-posed in the face of stochasticity. In this paper, we introduce an alternative form of curiosity that rewards novel associations between different senses. Our approach exploits multiple modalities to provide a stronger signal for more efficient exploration. Our method is inspired by the fact that, for humans, both sight and sound play a critical role in exploration. We present results on several Atari environments and Habitat (a photorealistic navigation simulator), showing the benefits of using an audio-visual association model for intrinsically guiding learning agents in the absence of external rewards. For videos and code, see https://vdean.github.io/audio-curiosity.html.



### Long-term Human Motion Prediction with Scene Context
- **Arxiv ID**: http://arxiv.org/abs/2007.03672v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03672v3)
- **Published**: 2020-07-07 17:59:53+00:00
- **Updated**: 2020-07-31 17:23:11+00:00
- **Authors**: Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, Jitendra Malik
- **Comment**: ECCV 2020 Oral. Dataset & Code:
  https://github.com/ZheC/GTA-IM-Dataset Video:
  https://people.eecs.berkeley.edu/~zhecao/hmp/index.html
- **Journal**: None
- **Summary**: Human movement is goal-directed and influenced by the spatial layout of the objects in the scene. To plan future human motion, it is crucial to perceive the environment -- imagine how hard it is to navigate a new room with lights off. Existing works on predicting human motion do not pay attention to the scene context and thus struggle in long-term prediction. In this work, we propose a novel three-stage framework that exploits scene context to tackle this task. Given a single scene image and 2D pose histories, our method first samples multiple human motion goals, then plans 3D human paths towards each goal, and finally predicts 3D human pose sequences following each path. For stable training and rigorous evaluation, we contribute a diverse synthetic dataset with clean annotations. In both synthetic and real datasets, our method shows consistent quantitative and qualitative improvements over existing methods.



### Unsupervised object-centric video generation and decomposition in 3D
- **Arxiv ID**: http://arxiv.org/abs/2007.06705v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06705v2)
- **Published**: 2020-07-07 18:01:29+00:00
- **Updated**: 2021-03-24 19:11:43+00:00
- **Authors**: Paul Henderson, Christoph H. Lampert
- **Comment**: Appeared at NeurIPS 2020. Project page: http://pmh47.net/o3v/
- **Journal**: None
- **Summary**: A natural approach to generative modeling of videos is to represent them as a composition of moving objects. Recent works model a set of 2D sprites over a slowly-varying background, but without considering the underlying 3D scene that gives rise to them. We instead propose to model a video as the view seen while moving through a scene with multiple 3D objects and a 3D background. Our model is trained from monocular videos without any supervision, yet learns to generate coherent 3D scenes containing several moving objects. We conduct detailed experiments on two datasets, going beyond the visual complexity supported by state-of-the-art generative approaches. We evaluate our method on depth-prediction and 3D object detection -- tasks which cannot be addressed by those earlier works -- and show it out-performs them even on 2D instance segmentation and tracking.



### Detection as Regression: Certified Object Detection by Median Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2007.03730v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03730v4)
- **Published**: 2020-07-07 18:40:19+00:00
- **Updated**: 2022-02-25 14:23:54+00:00
- **Authors**: Ping-yeh Chiang, Michael J. Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the vulnerability of object detectors to adversarial attacks, very few defenses are known to date. While adversarial training can improve the empirical robustness of image classifiers, a direct extension to object detection is very expensive. This work is motivated by recent progress on certified classification by randomized smoothing. We start by presenting a reduction from object detection to a regression problem. Then, to enable certified regression, where standard mean smoothing fails, we propose median smoothing, which is of independent interest. We obtain the first model-agnostic, training-free, and certified defense for object detection against $\ell_2$-bounded attacks. The code for all experiments in the paper is available at http://github.com/Ping-C/CertifiedObjectDetection .



### Resonator networks for factoring distributed representations of data structures
- **Arxiv ID**: http://arxiv.org/abs/2007.03748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.03748v1)
- **Published**: 2020-07-07 19:24:27+00:00
- **Updated**: 2020-07-07 19:24:27+00:00
- **Authors**: E. Paxon Frady, Spencer Kent, Bruno A. Olshausen, Friedrich T. Sommer
- **Comment**: 20 pages, 5 figures, to appear in Neural Computation 2020 with
  companion paper: arXiv:1906.11684
- **Journal**: None
- **Summary**: The ability to encode and manipulate data structures with distributed neural representations could qualitatively enhance the capabilities of traditional neural networks by supporting rule-based symbolic reasoning, a central property of cognition. Here we show how this may be accomplished within the framework of Vector Symbolic Architectures (VSA) (Plate, 1991; Gayler, 1998; Kanerva, 1996), whereby data structures are encoded by combining high-dimensional vectors with operations that together form an algebra on the space of distributed representations. In particular, we propose an efficient solution to a hard combinatorial search problem that arises when decoding elements of a VSA data structure: the factorization of products of multiple code vectors. Our proposed algorithm, called a resonator network, is a new type of recurrent neural network that interleaves VSA multiplication operations and pattern completion. We show in two examples -- parsing of a tree-like data structure and parsing of a visual scene -- how the factorization problem arises and how the resonator network can solve it. More broadly, resonator networks open the possibility to apply VSAs to myriad artificial intelligence problems in real-world domains. A companion paper (Kent et al., 2020) presents a rigorous analysis and evaluation of the performance of resonator networks, showing it out-performs alternative approaches.



### README: REpresentation learning by fairness-Aware Disentangling MEthod
- **Arxiv ID**: http://arxiv.org/abs/2007.03775v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03775v1)
- **Published**: 2020-07-07 20:16:49+00:00
- **Updated**: 2020-07-07 20:16:49+00:00
- **Authors**: Sungho Park, Dohyung Kim, Sunhee Hwang, Hyeran Byun
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Fair representation learning aims to encode invariant representation with respect to the protected attribute, such as gender or age. In this paper, we design Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair representation learning. This network disentangles latent space into three subspaces with a decorrelation loss that encourages each subspace to contain independent information: 1) target attribute information, 2) protected attribute information, 3) mutual attribute information. After the representation learning, this disentangled representation is leveraged for fairer downstream classification by excluding the subspace with the protected attribute information. We demonstrate the effectiveness of our model through extensive experiments on CelebA and UTK Face datasets. Our method outperforms the previous state-of-the-art method by large margins in terms of equal opportunity and equalized odds.



### Placepedia: Comprehensive Place Understanding with Multi-Faceted Annotations
- **Arxiv ID**: http://arxiv.org/abs/2007.03777v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2007.03777v4)
- **Published**: 2020-07-07 20:17:01+00:00
- **Updated**: 2020-07-17 08:56:05+00:00
- **Authors**: Huaiyi Huang, Yuqi Zhang, Qingqiu Huang, Zhengkui Guo, Ziwei Liu, Dahua Lin
- **Comment**: To appear in ECCV 2020. Dataset is available at:
  https://hahehi.github.io/placepedia.html
- **Journal**: None
- **Summary**: Place is an important element in visual understanding. Given a photo of a building, people can often tell its functionality, e.g. a restaurant or a shop, its cultural style, e.g. Asian or European, as well as its economic type, e.g. industry oriented or tourism oriented. While place recognition has been widely studied in previous work, there remains a long way towards comprehensive place understanding, which is far beyond categorizing a place with an image and requires information of multiple aspects. In this work, we contribute Placepedia, a large-scale place dataset with more than 35M photos from 240K unique places. Besides the photos, each place also comes with massive multi-faceted information, e.g. GDP, population, etc., and labels at multiple levels, including function, city, country, etc.. This dataset, with its large amount of data and rich annotations, allows various studies to be conducted. Particularly, in our studies, we develop 1) PlaceNet, a unified framework for multi-level place recognition, and 2) a method for city embedding, which can produce a vector representation for a city that captures both visual and multi-faceted side information. Such studies not only reveal key challenges in place understanding, but also establish connections between visual observations and underlying socioeconomic/cultural implications.



### 3D Shape Reconstruction from Vision and Touch
- **Arxiv ID**: http://arxiv.org/abs/2007.03778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.03778v2)
- **Published**: 2020-07-07 20:20:33+00:00
- **Updated**: 2020-11-02 19:57:55+00:00
- **Authors**: Edward J. Smith, Roberto Calandra, Adriana Romero, Georgia Gkioxari, David Meger, Jitendra Malik, Michal Drozdzal
- **Comment**: Accepted at Neurips 2020
- **Journal**: None
- **Summary**: When a toddler is presented a new toy, their instinctual behaviour is to pick it upand inspect it with their hand and eyes in tandem, clearly searching over its surface to properly understand what they are playing with. At any instance here, touch provides high fidelity localized information while vision provides complementary global context. However, in 3D shape reconstruction, the complementary fusion of visual and haptic modalities remains largely unexplored. In this paper, we study this problem and present an effective chart-based approach to multi-modal shape understanding which encourages a similar fusion vision and touch information.To do so, we introduce a dataset of simulated touch and vision signals from the interaction between a robotic hand and a large array of 3D objects. Our results show that (1) leveraging both vision and touch signals consistently improves single-modality baselines; (2) our approach outperforms alternative modality fusion methods and strongly benefits from the proposed chart-based structure; (3) there construction quality increases with the number of grasps provided; and (4) the touch information not only enhances the reconstruction at the touch site but also extrapolates to its local neighborhood.



### SofGAN: A Portrait Image Generator with Dynamic Styling
- **Arxiv ID**: http://arxiv.org/abs/2007.03780v2
- **DOI**: 10.1145/3470848
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.03780v2)
- **Published**: 2020-07-07 20:28:47+00:00
- **Updated**: 2021-08-06 09:22:30+00:00
- **Authors**: Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, Jingyi Yu
- **Comment**: Project page: https://apchenstu.github.io/sofgan/ Code:
  https://github.com/apchenstu/sofgan
- **Journal**: None
- **Summary**: Recently, Generative Adversarial Networks (GANs)} have been widely used for portrait image generation. However, in the latent space learned by GANs, different attributes, such as pose, shape, and texture style, are generally entangled, making the explicit control of specific attributes difficult. To address this issue, we propose a SofGAN image generator to decouple the latent space of portraits into two subspaces: a geometry space and a texture space. The latent codes sampled from the two subspaces are fed to two network branches separately, one to generate the 3D geometry of portraits with canonical pose, and the other to generate textures. The aligned 3D geometries also come with semantic part segmentation, encoded as a semantic occupancy field (SOF). The SOF allows the rendering of consistent 2D semantic segmentation maps at arbitrary views, which are then fused with the generated texture maps and stylized to a portrait photo using our semantic instance-wise (SIW) module. Through extensive experiments, we show that our system can generate high quality portrait images with independently controllable geometry and texture attributes. The method also generalizes well in various applications such as appearance-consistent facial animation and dynamic styling.



### Real-time Semantic Segmentation with Fast Attention
- **Arxiv ID**: http://arxiv.org/abs/2007.03815v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.03815v2)
- **Published**: 2020-07-07 22:37:16+00:00
- **Updated**: 2020-07-09 22:44:34+00:00
- **Authors**: Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Kate Saenko, Stan Sclaroff
- **Comment**: project page: https://cs-people.bu.edu/pinghu/FANet.html
- **Journal**: None
- **Summary**: In deep CNN based models for semantic segmentation, high accuracy relies on rich spatial context (large receptive fields) and fine spatial details (high resolution), both of which incur high computational costs. In this paper, we propose a novel architecture that addresses both challenges and achieves state-of-the-art performance for semantic segmentation of high-resolution images and videos in real-time. The proposed architecture relies on our fast spatial attention, which is a simple yet efficient modification of the popular self-attention mechanism and captures the same rich spatial context at a small fraction of the computational cost, by changing the order of operations. Moreover, to efficiently process high-resolution input, we apply an additional spatial reduction to intermediate feature stages of the network with minimal loss in accuracy thanks to the use of the fast attention module to fuse features. We validate our method with a series of experiments, and show that results on multiple datasets demonstrate superior performance with better accuracy and speed compared to existing approaches for real-time semantic segmentation. On Cityscapes, our network achieves 74.4$\%$ mIoU at 72 FPS and 75.5$\%$ mIoU at 58 FPS on a single Titan X GPU, which is~$\sim$50$\%$ faster than the state-of-the-art while retaining the same accuracy.



### Self-supervised Skull Reconstruction in Brain CT Images with Decompressive Craniectomy
- **Arxiv ID**: http://arxiv.org/abs/2007.03817v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03817v2)
- **Published**: 2020-07-07 22:38:38+00:00
- **Updated**: 2020-07-11 00:33:14+00:00
- **Authors**: Franco Matzkin, Virginia Newcombe, Susan Stevenson, Aneesh Khetani, Tom Newman, Richard Digby, Andrew Stevens, Ben Glocker, Enzo Ferrante
- **Comment**: Accepted for publication in MICCAI 2020. Update: Figure 1 corrected
  to match description
- **Journal**: None
- **Summary**: Decompressive craniectomy (DC) is a common surgical procedure consisting of the removal of a portion of the skull that is performed after incidents such as stroke, traumatic brain injury (TBI) or other events that could result in acute subdural hemorrhage and/or increasing intracranial pressure. In these cases, CT scans are obtained to diagnose and assess injuries, or guide a certain therapy and intervention.   We propose a deep learning based method to reconstruct the skull defect removed during DC performed after TBI from post-operative CT images. This reconstruction is useful in multiple scenarios, e.g. to support the creation of cranioplasty plates, accurate measurements of bone flap volume and total intracranial volume, important for studies that aim to relate later atrophy to patient outcome. We propose and compare alternative self-supervised methods where an encoder-decoder convolutional neural network (CNN) estimates the missing bone flap on post-operative CTs. The self-supervised learning strategy only requires images with complete skulls and avoids the need for annotated DC images. For evaluation, we employ real and simulated images with DC, comparing the results with other state-of-the-art approaches. The experiments show that the proposed model outperforms current manual methods, enabling reconstruction even in highly challenging cases where big skull defects have been removed during surgery.




# Arxiv Papers in cs.CV on 2020-02-09
### GradMix: Multi-source Transfer across Domains and Tasks
- **Arxiv ID**: http://arxiv.org/abs/2002.03264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03264v1)
- **Published**: 2020-02-09 02:10:22+00:00
- **Updated**: 2020-02-09 02:10:22+00:00
- **Authors**: Junnan Li, Ziwei Xu, Yongkang Wong, Qi Zhao, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: The computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the deep convolutional networks' capability to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale annotated dataset, for supervised training of deep network. However, it is expensive and time-consuming to manually label sufficient amount of training data. Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task. While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting. We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule, to transfer knowledge via gradient descent by weighting and mixing the gradients from all sources during training. GradMix follows a meta-learning objective, which assigns layer-wise weights to the source gradients, such that the combined gradient follows the direction that minimize the loss for a small set of samples from the target dataset. In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain. We conduct MS-DTT experiments on two tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.



### Weakly-Supervised Multi-Person Action Recognition in 360$^{\circ}$ Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.03266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03266v1)
- **Published**: 2020-02-09 02:17:46+00:00
- **Updated**: 2020-02-09 02:17:46+00:00
- **Authors**: Junnan Li, Jianquan Liu, Yongkang Wong, Shoji Nishimura, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: The recent development of commodity 360$^{\circ}$ cameras have enabled a single video to capture an entire scene, which endows promising potentials in surveillance scenarios. However, research in omnidirectional video analysis has lagged behind the hardware advances. In this work, we address the important problem of action recognition in top-view 360$^{\circ}$ videos. Due to the wide filed-of-view, 360$^{\circ}$ videos usually capture multiple people performing actions at the same time. Furthermore, the appearance of people are deformed. The proposed framework first transforms omnidirectional videos into panoramic videos, then it extracts spatial-temporal features using region-based 3D CNNs for action recognition. We propose a weakly-supervised method based on multi-instance multi-label learning, which trains the model to recognize and localize multiple actions in a video using only video-level action labels as supervision. We perform experiments to quantitatively validate the efficacy of the proposed method and qualitatively demonstrate action localization results. To enable research in this direction, we introduce 360Action, the first omnidirectional video dataset for multi-person action recognition.



### Learning efficient structured dictionary for image classification
- **Arxiv ID**: http://arxiv.org/abs/2002.03271v2
- **DOI**: 10.1117/1.JEI.29.3.033019
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03271v2)
- **Published**: 2020-02-09 03:12:49+00:00
- **Updated**: 2020-05-08 01:58:51+00:00
- **Authors**: Zi-Qi Li, Jun Sun, Xiao-Jun Wu, He-Feng Yin
- **Comment**: Journal of Electronic Imaging (major revision)
- **Journal**: None
- **Summary**: Recent years have witnessed the success of dictionary learning (DL) based approaches in the domain of pattern classification. In this paper, we present an efficient structured dictionary learning (ESDL) method which takes both the diversity and label information of training samples into account. Specifically, ESDL introduces alternative training samples into the process of dictionary learning. To increase the discriminative capability of representation coefficients for classification, an ideal regularization term is incorporated into the objective function of ESDL. Moreover, in contrast with conventional DL approaches which impose computationally expensive L1-norm constraint on the coefficient matrix, ESDL employs L2-norm regularization term. Experimental results on benchmark databases (including four face databases and one scene dataset) demonstrate that ESDL outperforms previous DL approaches. More importantly, ESDL can be applied in a wide range of pattern classification tasks.



### Asymmetric Rejection Loss for Fairer Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03276v1)
- **Published**: 2020-02-09 04:01:03+00:00
- **Updated**: 2020-02-09 04:01:03+00:00
- **Authors**: Haoyu Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition performance has seen a tremendous gain in recent years, mostly due to the availability of large-scale face images dataset that can be exploited by deep neural networks to learn powerful face representations. However, recent research has shown differences in face recognition performance across different ethnic groups mostly due to the racial imbalance in the training datasets where Caucasian identities largely dominate other ethnicities. This is actually symptomatic of the under-representation of non-Caucasian ethnic groups in the celebdom from which face datasets are usually gathered, rendering the acquisition of labeled data of the under-represented groups challenging. In this paper, we propose an Asymmetric Rejection Loss, which aims at making full use of unlabeled images of those under-represented groups, to reduce the racial bias of face recognition models. We view each unlabeled image as a unique class, however as we cannot guarantee that two unlabeled samples are from a distinct class we exploit both labeled and unlabeled data in an asymmetric manner in our loss formalism. Extensive experiments show our method's strength in mitigating racial bias, outperforming state-of-the-art semi-supervision methods. Performance on the under-represented ethnicity groups increases while that on the well-represented group is nearly unchanged.



### PointHop++: A Lightweight Learning Model on Point Sets for 3D Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.03281v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03281v2)
- **Published**: 2020-02-09 04:49:32+00:00
- **Updated**: 2020-05-23 03:56:54+00:00
- **Authors**: Min Zhang, Yifan Wang, Pranav Kadam, Shan Liu, C. -C. Jay Kuo
- **Comment**: 4pages, 4 figures
- **Journal**: None
- **Summary**: The PointHop method was recently proposed by Zhang et al. for 3D point cloud classification with unsupervised feature extraction. It has an extremely low training complexity while achieving state-of-the-art classification performance. In this work, we improve the PointHop method furthermore in two aspects: 1) reducing its model complexity in terms of the model parameter number and 2) ordering discriminant features automatically based on the cross-entropy criterion. The resulting method is called PointHop++. The first improvement is essential for wearable and mobile computing while the second improvement bridges statistics-based and optimization-based machine learning methodologies. With experiments conducted on the ModelNet40 benchmark dataset, we show that the PointHop++ method performs on par with deep neural network (DNN) solutions and surpasses other unsupervised feature extraction methods.



### Convolutional Neural Network Pruning Using Filter Attenuation
- **Arxiv ID**: http://arxiv.org/abs/2002.03299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03299v1)
- **Published**: 2020-02-09 06:31:24+00:00
- **Updated**: 2020-02-09 06:31:24+00:00
- **Authors**: Morteza Mousa-Pasandi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi, Shahram Shirani
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Filters are the essential elements in convolutional neural networks (CNNs). Filters are corresponded to the feature maps and form the main part of the computational and memory requirement for the CNN processing. In filter pruning methods, a filter with all of its components, including channels and connections, are removed. The removal of a filter can cause a drastic change in the network's performance. Also, the removed filters cannot come back to the network structure. We want to address these problems in this paper. We propose a CNN pruning method based on filter attenuation in which weak filters are not directly removed. Instead, weak filters are attenuated and gradually removed. In the proposed attenuation approach, weak filters are not abruptly removed, and there is a chance for these filters to return to the network. The filter attenuation method is assessed using the VGG model for the Cifar10 image classification task. Simulation results show that the filter attenuation works with different pruning criteria, and better results are obtained in comparison with the conventional pruning methods.



### Splitting Convolutional Neural Network Structures for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2002.03302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03302v1)
- **Published**: 2020-02-09 06:53:18+00:00
- **Updated**: 2020-02-09 06:53:18+00:00
- **Authors**: Emad MalekHosseini, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi, Shahram Shirani
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: For convolutional neural networks (CNNs) that have a large volume of input data, memory management becomes a major concern. Memory cost reduction can be an effective way to deal with these problems that can be realized through different techniques such as feature map pruning, input data splitting, etc. Among various methods existing in this area of research, splitting the network structure is an interesting research field, and there are a few works done in this area. In this study, the problem of reducing memory utilization using network structure splitting is addressed. A new technique is proposed to split the network structure into small parts that consume lower memory than the original network. The split parts can be processed almost separately, which provides an essential role for better memory management. The split approach has been tested on two well-known network structures of VGG16 and ResNet18 for the classification of CIFAR10 images. Simulation results show that the splitting method reduces both the number of computational operations as well as the amount of memory consumption.



### Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2002.12749v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.12749v3)
- **Published**: 2020-02-09 07:10:58+00:00
- **Updated**: 2020-11-07 22:09:38+00:00
- **Authors**: Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz Koushanfar, Julian McAuley
- **Comment**: Published as a conference paper at WACV 2021
- **Journal**: None
- **Summary**: Recent advances in video manipulation techniques have made the generation of fake videos more accessible than ever before. Manipulated videos can fuel disinformation and reduce trust in media. Therefore detection of fake videos has garnered immense interest in academia and industry. Recently developed Deepfake detection methods rely on deep neural networks (DNNs) to distinguish AI-generated fake videos from real videos. In this work, we demonstrate that it is possible to bypass such detectors by adversarially modifying fake videos synthesized using existing Deepfake generation methods. We further demonstrate that our adversarial perturbations are robust to image and video compression codecs, making them a real-world threat. We present pipelines in both white-box and black-box attack scenarios that can fool DNN based Deepfake detectors into classifying fake videos as real.



### Face Hallucination with Finishing Touches
- **Arxiv ID**: http://arxiv.org/abs/2002.03308v2
- **DOI**: 10.1109/TIP.2020.3046918
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03308v2)
- **Published**: 2020-02-09 07:33:48+00:00
- **Updated**: 2020-10-28 03:10:44+00:00
- **Authors**: Yang Zhang, Ivor W. Tsang, Jun Li, Ping Liu, Xiaobo Lu, Xin Yu
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: Obtaining a high-quality frontal face image from a low-resolution (LR) non-frontal face image is primarily important for many facial analysis applications. However, mainstreams either focus on super-resolving near-frontal LR faces or frontalizing non-frontal high-resolution (HR) faces. It is desirable to perform both tasks seamlessly for daily-life unconstrained face images. In this paper, we present a novel Vivid Face Hallucination Generative Adversarial Network (VividGAN) for simultaneously super-resolving and frontalizing tiny non-frontal face images. VividGAN consists of coarse-level and fine-level Face Hallucination Networks (FHnet) and two discriminators, i.e., Coarse-D and Fine-D. The coarse-level FHnet generates a frontal coarse HR face and then the fine-level FHnet makes use of the facial component appearance prior, i.e., fine-grained facial components, to attain a frontal HR face image with authentic details. In the fine-level FHnet, we also design a facial component-aware module that adopts the facial geometry guidance as clues to accurately align and merge the frontal coarse HR face and prior information. Meanwhile, two-level discriminators are designed to capture both the global outline of a face image as well as detailed facial characteristics. The Coarse-D enforces the coarsely hallucinated faces to be upright and complete while the Fine-D focuses on the fine hallucinated ones for sharper details. Extensive experiments demonstrate that our VividGAN achieves photo-realistic frontal HR faces, reaching superior performance in downstream tasks, i.e., face recognition and expression classification, compared with other state-of-the-art methods.



### FSD-10: A Dataset for Competitive Sports Content Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.03312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03312v1)
- **Published**: 2020-02-09 08:04:26+00:00
- **Updated**: 2020-02-09 08:04:26+00:00
- **Authors**: Shenlan Liu, Xiang Liu, Gao Huang, Lin Feng, Lianyu Hu, Dong Jiang, Aibin Zhang, Yang Liu, Hong Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition is an important and challenging problem in video analysis. Although the past decade has witnessed progress in action recognition with the development of deep learning, such process has been slow in competitive sports content analysis. To promote the research on action recognition from competitive sports video clips, we introduce a Figure Skating Dataset (FSD-10) for finegrained sports content analysis. To this end, we collect 1484 clips from the worldwide figure skating championships in 2017-2018, which consist of 10 different actions in men/ladies programs. Each clip is at a rate of 30 frames per second with resolution 1080 $\times$ 720. These clips are then annotated by experts in type, grade of execution, skater info, .etc. To build a baseline for action recognition in figure skating, we evaluate state-of-the-art action recognition methods on FSD-10. Motivated by the idea that domain knowledge is of great concern in sports field, we propose a keyframe based temporal segment network (KTSN) for classification and achieve remarkable performance. Experimental results demonstrate that FSD-10 is an ideal dataset for benchmarking action recognition algorithms, as it requires to accurately extract action motions rather than action poses. We hope FSD-10, which is designed to have a large collection of finegrained actions, can serve as a new challenge to develop more robust and advanced action recognition models.



### Diversity-Achieving Slow-DropBlock Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2002.04414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04414v1)
- **Published**: 2020-02-09 08:05:39+00:00
- **Updated**: 2020-02-09 08:05:39+00:00
- **Authors**: Xiaofu Wu, Ben Xie, Shiliang Zhao, Suofei Zhang, Yong Xiao, Ming Li
- **Comment**: submitted to IEEE TMM for possible publication. arXiv admin note:
  substantial text overlap with arXiv:2001.07442
- **Journal**: None
- **Summary**: A big challenge of person re-identification (Re-ID) using a multi-branch network architecture is to learn diverse features from the ID-labeled dataset. The 2-branch Batch DropBlock (BDB) network was recently proposed for achieving diversity between the global branch and the feature-dropping branch. In this paper, we propose to move the dropping operation from the intermediate feature layer towards the input (image dropping). Since it may drop a large portion of input images, this makes the training hard to converge. Hence, we propose a novel double-batch-split co-training approach for remedying this problem. In particular, we show that the feature diversity can be well achieved with the use of multiple dropping branches by setting individual dropping ratio for each branch. Empirical evidence demonstrates that the proposed method performs superior to BDB on popular person Re-ID datasets, including Market-1501, DukeMTMC-reID and CUHK03 and the use of more dropping branches can further boost the performance.



### Patch-Based Holographic Image Sensing
- **Arxiv ID**: http://arxiv.org/abs/2002.03314v3
- **DOI**: 10.1137/20M1324041
- **Categories**: **cs.IT**, cs.CV, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2002.03314v3)
- **Published**: 2020-02-09 08:14:02+00:00
- **Updated**: 2020-12-19 09:32:31+00:00
- **Authors**: Alfred Marcel Bruckstein, Martianus Frederic Ezerman, Adamas Aqsa Fahreza, San Ling
- **Comment**: None
- **Journal**: SIAM J. Imaging Sci., no. 14 vol. , pp. 198--223, 2021
- **Summary**: Holographic representations of data enable distributed storage with progressive refinement when the stored packets of data are made available in any arbitrary order. In this paper, we propose and test patch-based transform coding holographic sensing of image data. Our proposal is optimized for progressive recovery under random order of retrieval of the stored data. The coding of the image patches relies on the design of distributed projections ensuring best image recovery, in terms of the $\ell_2$ norm, at each retrieval stage. The performance depends only on the number of data packets that has been retrieved thus far. Several possible options to enhance the quality of the recovery while changing the size and number of data packets are discussed and tested. This leads us to examine several interesting bit-allocation and rate-distortion trade offs, highlighted for a set of natural images with ensemble estimated statistical properties.



### Unlabeled Data Deployment for Classification of Diabetic Retinopathy Images Using Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2002.03321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03321v1)
- **Published**: 2020-02-09 09:01:11+00:00
- **Updated**: 2020-02-09 09:01:11+00:00
- **Authors**: Sajjad Abbasi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi, Shahram Shirani
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are extensively beneficial for medical image processing. Medical images are plentiful, but there is a lack of annotated data. Transfer learning is used to solve the problem of lack of labeled data and grants CNNs better training capability. Transfer learning can be used in many different medical applications; however, the model under transfer should have the same size as the original network. Knowledge distillation is recently proposed to transfer the knowledge of a model to another one and can be useful to cover the shortcomings of transfer learning. But some parts of the knowledge may not be distilled by knowledge distillation. In this paper, a novel knowledge distillation using transfer learning is proposed to transfer the whole knowledge of a model to another one. The proposed method can be beneficial and practical for medical image analysis in which a small number of labeled data are available. The proposed process is tested for diabetic retinopathy classification. Simulation results demonstrate that using the proposed method, knowledge of an extensive network can be transferred to a smaller model.



### VIFB: A Visible and Infrared Image Fusion Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2002.03322v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2002.03322v4)
- **Published**: 2020-02-09 09:10:00+00:00
- **Updated**: 2020-07-20 15:44:59+00:00
- **Authors**: Xingchen Zhang, Ping Ye, Gang Xiao
- **Comment**: 11 pages, 5 figures, 5 tables. Accepted to CVPRW2020. Compared to the
  CVPRW2020 version, this version corrects minor mistakes in Table 4 and the
  first paragraph of Section 4.2
- **Journal**: None
- **Summary**: Visible and infrared image fusion is one of the most important areas in image processing due to its numerous applications. While much progress has been made in recent years with efforts on developing fusion algorithms, there is a lack of code library and benchmark which can gauge the state-of-the-art. In this paper, after briefly reviewing recent advances of visible and infrared image fusion, we present a visible and infrared image fusion benchmark (VIFB) which consists of 21 image pairs, a code library of 20 fusion algorithms and 13 evaluation metrics. We also carry out large scale experiments within the benchmark to understand the performance of these algorithms. By analyzing qualitative and quantitative results, we identify effective algorithms for robust image fusion and give some observations on the status and future prospects of this field.



### Kullback-Leibler Divergence-Based Out-of-Distribution Detection with Flow-Based Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2002.03328v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03328v5)
- **Published**: 2020-02-09 09:54:12+00:00
- **Updated**: 2023-03-02 06:56:26+00:00
- **Authors**: Yufeng Zhang, Jialu Pan, Wanwei Liu, Zhenbang Chen, Ji Wang, Zhiming Liu, Kenli Li, Hongmei Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has revealed that deep generative models including flow-based models and Variational Autoencoders may assign higher likelihoods to out-of-distribution (OOD) data than in-distribution (ID) data. However, we cannot sample OOD data from the model. This counterintuitive phenomenon has not been satisfactorily explained and brings obstacles to OOD detection with flow-based models. In this paper, we prove theorems to investigate the Kullback-Leibler divergence in flow-based model and give two explanations for the above phenomenon. Based on our theoretical analysis, we propose a new method \PADmethod\ to leverage KL divergence and local pixel dependence of representations to perform anomaly detection. Experimental results on prevalent benchmarks demonstrate the effectiveness and robustness of our method. For group anomaly detection, our method achieves 98.1\% AUROC on average with a small batch size of 5. On the contrary, the baseline typicality test-based method only achieves 64.6\% AUROC on average due to its failure on challenging problems. Our method also outperforms the state-of-the-art method by 9.1\% AUROC. For point-wise anomaly detection, our method achieves 90.7\% AUROC on average and outperforms the baseline by 5.2\% AUROC. Besides, our method has the least notable failures and is the most robust one.



### Multi-Task Learning by a Top-Down Control Network
- **Arxiv ID**: http://arxiv.org/abs/2002.03335v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03335v3)
- **Published**: 2020-02-09 10:13:17+00:00
- **Updated**: 2020-10-29 19:53:34+00:00
- **Authors**: Hila Levi, Shimon Ullman
- **Comment**: None
- **Journal**: None
- **Summary**: As the range of tasks performed by a general vision system expands, executing multiple tasks accurately and efficiently in a single network has become an important and still open problem. Recent computer vision approaches address this problem by branching networks, or by a channel-wise modulation of the network feature-maps with task specific vectors. We present a novel architecture that uses a dedicated top-down control network to modify the activation of all the units in the main recognition network in a manner that depends on the selected task, image content, and spatial location. We show the effectiveness of our scheme by achieving significantly better results than alternative state-of-the-art approaches on four datasets. We further demonstrate our advantages in terms of task selectivity, scaling the number of tasks and interpretability.



### Dynamic Inference: A New Approach Toward Efficient Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03342v1)
- **Published**: 2020-02-09 11:09:56+00:00
- **Updated**: 2020-02-09 11:09:56+00:00
- **Authors**: Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, Shilei Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Though action recognition in videos has achieved great success recently, it remains a challenging task due to the massive computational cost. Designing lightweight networks is a possible solution, but it may degrade the recognition performance. In this paper, we innovatively propose a general dynamic inference idea to improve inference efficiency by leveraging the variation in the distinguishability of different videos. The dynamic inference approach can be achieved from aspects of the network depth and the number of input video frames, or even in a joint input-wise and network depth-wise manner. In a nutshell, we treat input frames and network depth of the computational graph as a 2-dimensional grid, and several checkpoints are placed on this grid in advance with a prediction module. The inference is carried out progressively on the grid by following some predefined route, whenever the inference process comes across a checkpoint, an early prediction can be made depending on whether the early stop criteria meets. For the proof-of-concept purpose, we instantiate three dynamic inference frameworks using two well-known backbone CNNs. In these instances, we overcome the drawback of limited temporal coverage resulted from an early prediction by a novel frame permutation scheme, and alleviate the conflict between progressive computation and video temporal relation modeling by introducing an online temporal shift module. Extensive experiments are conducted to thoroughly analyze the effectiveness of our ideas and to inspire future research efforts. Results on various datasets also evident the superiority of our approach.



### Weakly Supervised Attention Pyramid Convolutional Neural Network for Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.03353v1
- **DOI**: 10.1109/TIP.2021.3055617
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03353v1)
- **Published**: 2020-02-09 12:33:23+00:00
- **Updated**: 2020-02-09 12:33:23+00:00
- **Authors**: Yifeng Ding, Shaoguo Wen, Jiyang Xie, Dongliang Chang, Zhanyu Ma, Zhongwei Si, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying the sub-categories of an object from the same super-category (e.g. bird species, car and aircraft models) in fine-grained visual classification (FGVC) highly relies on discriminative feature representation and accurate region localization. Existing approaches mainly focus on distilling information from high-level features. In this paper, however, we show that by integrating low-level information (e.g. color, edge junctions, texture patterns), performance can be improved with enhanced feature representation and accurately located discriminative regions. Our solution, named Attention Pyramid Convolutional Neural Network (AP-CNN), consists of a) a pyramidal hierarchy structure with a top-down feature pathway and a bottom-up attention pathway, and hence learns both high-level semantic and low-level detailed feature representation, and b) an ROI guided refinement strategy with ROI guided dropblock and ROI guided zoom-in, which refines features with discriminative local regions enhanced and background noises eliminated. The proposed AP-CNN can be trained end-to-end, without the need of additional bounding box/part annotations. Extensive experiments on three commonly used FGVC datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that our approach can achieve state-of-the-art performance. Code available at \url{http://dwz1.cc/ci8so8a}



### MS-Net: Multi-Site Network for Improving Prostate Segmentation with Heterogeneous MRI Data
- **Arxiv ID**: http://arxiv.org/abs/2002.03366v2
- **DOI**: 10.1109/TMI.2020.2974574
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03366v2)
- **Published**: 2020-02-09 14:11:50+00:00
- **Updated**: 2020-02-19 06:21:01+00:00
- **Authors**: Quande Liu, Qi Dou, Lequan Yu, Pheng Ann Heng
- **Comment**: IEEE TMI, 2020
- **Journal**: None
- **Summary**: Automated prostate segmentation in MRI is highly demanded for computer-assisted diagnosis. Recently, a variety of deep learning methods have achieved remarkable progress in this task, usually relying on large amounts of training data. Due to the nature of scarcity for medical images, it is important to effectively aggregate data from multiple sites for robust model training, to alleviate the insufficiency of single-site samples. However, the prostate MRIs from different sites present heterogeneity due to the differences in scanners and imaging protocols, raising challenges for effective ways of aggregating multi-site data for network training. In this paper, we propose a novel multi-site network (MS-Net) for improving prostate segmentation by learning robust representations, leveraging multiple sources of data. To compensate for the inter-site heterogeneity of different MRI datasets, we develop Domain-Specific Batch Normalization layers in the network backbone, enabling the network to estimate statistics and perform feature normalization for each site separately. Considering the difficulty of capturing the shared knowledge from multiple datasets, a novel learning paradigm, i.e., Multi-site-guided Knowledge Transfer, is proposed to enhance the kernels to extract more generic representations from multi-site data. Extensive experiments on three heterogeneous prostate MRI datasets demonstrate that our MS-Net improves the performance across all datasets consistently, and outperforms state-of-the-art methods for multi-site learning.



### A Unified End-to-End Framework for Efficient Deep Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2002.03370v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03370v3)
- **Published**: 2020-02-09 14:21:08+00:00
- **Updated**: 2020-05-23 22:41:06+00:00
- **Authors**: Jiaheng Liu, Guo Lu, Zhihao Hu, Dong Xu
- **Comment**: We will released our code and training data
- **Journal**: None
- **Summary**: Image compression is a widely used technique to reduce the spatial redundancy in images. Recently, learning based image compression has achieved significant progress by using the powerful representation ability from neural networks. However, the current state-of-the-art learning based image compression methods suffer from the huge computational cost, which limits their capacity for practical applications. In this paper, we propose a unified framework called Efficient Deep Image Compression (EDIC) based on three new technologies, including a channel attention module, a Gaussian mixture model and a decoder-side enhancement module. Specifically, we design an auto-encoder style network for learning based image compression. To improve the coding efficiency, we exploit the channel relationship between latent representations by using the channel attention module. Besides, the Gaussian mixture model is introduced for the entropy model and improves the accuracy for bitrate estimation. Furthermore, we introduce the decoder-side enhancement module to further improve image compression performance. Our EDIC method can also be readily incorporated with the Deep Video Compression (DVC) framework to further improve the video compression performance. Simultaneously, our EDIC method boosts the coding performance significantly while bringing slightly increased computational cost. More importantly, experimental results demonstrate that the proposed approach outperforms the current state-of-the-art image compression methods and is up to more than 150 times faster in terms of decoding speed when compared with Minnen's method. The proposed framework also successfully improves the performance of the recent deep video compression system DVC. Our code will be released at https://github.com/liujiaheng/compression.



### Two-Stream Aural-Visual Affect Analysis in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2002.03399v2
- **DOI**: 10.1109/FG47880.2020.00056
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.5.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2002.03399v2)
- **Published**: 2020-02-09 16:59:56+00:00
- **Updated**: 2020-03-03 13:59:01+00:00
- **Authors**: Felix Kuhnke, Lars Rumberg, JÃ¶rn Ostermann
- **Comment**: 6 pages, 2 figures, Face and Gesture 2020 Workshop Paper (ABAW2020
  competition)
- **Journal**: None
- **Summary**: Human affect recognition is an essential part of natural human-computer interaction. However, current methods are still in their infancy, especially for in-the-wild data. In this work, we introduce our submission to the Affective Behavior Analysis in-the-wild (ABAW) 2020 competition. We propose a two-stream aural-visual analysis model to recognize affective behavior from videos. Audio and image streams are first processed separately and fed into a convolutional neural network. Instead of applying recurrent architectures for temporal analysis we only use temporal convolutions. Furthermore, the model is given access to additional features extracted during face-alignment. At training time, we exploit correlations between different emotion representations to improve performance. Our model achieves promising results on the challenging Aff-Wild2 database.



### Medical Image Registration Using Deep Neural Networks: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2002.03401v1
- **DOI**: 10.1016/j.compeleceng.2020.106767
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.03401v1)
- **Published**: 2020-02-09 17:22:05+00:00
- **Updated**: 2020-02-09 17:22:05+00:00
- **Authors**: Hamid Reza Boveiri, Raouf Khayami, Reza Javidan, Ali Reza MehdiZadeh
- **Comment**: 45 Pages, 39 Figures, 10 Tables, 2 Appendixes
- **Journal**: None
- **Summary**: Image-guided interventions are saving the lives of a large number of patients where the image registration problem should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, the recently huge progress in the field of machine learning made by the possibility of implementing deep neural networks on the contemporary many-core GPUs opened up a promising window to challenge with many medical applications, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confiding challenges, novelties and main contributions, key-enabling techniques, future directions and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature.



### TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2002.04461v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2002.04461v2)
- **Published**: 2020-02-09 21:00:38+00:00
- **Updated**: 2020-07-26 13:42:19+00:00
- **Authors**: Alexander Tong, Jessie Huang, Guy Wolf, David van Dijk, Smita Krishnaswamy
- **Comment**: Presented at ICML 2020
- **Journal**: None
- **Summary**: It is increasingly common to encounter data from dynamic processes captured by static cross-sectional measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model continuous dynamics and non-linear paths that entities can take in these systems. To address this issue, we establish a link between continuous normalizing flows and dynamic optimal transport, that allows us to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present TrajectoryNet, which controls the continuous paths taken between distributions to produce dynamic optimal transport. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.



### A Deep Learning Approach to Automate High-Resolution Blood Vessel Reconstruction on Computerized Tomography Images With or Without the Use of Contrast Agent
- **Arxiv ID**: http://arxiv.org/abs/2002.03463v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2002.03463v1)
- **Published**: 2020-02-09 22:32:37+00:00
- **Updated**: 2020-02-09 22:32:37+00:00
- **Authors**: Anirudh Chandrashekar, Ashok Handa, Natesh Shivakumar, Pierfrancesco Lapolla, Vicente Grau, Regent Lee
- **Comment**: 18 pages, 10 figures, 7 tables
- **Journal**: None
- **Summary**: Existing methods to reconstruct vascular structures from a computed tomography (CT) angiogram rely on injection of intravenous contrast to enhance the radio-density within the vessel lumen. However, pathological changes can be present in the blood lumen, vessel wall or a combination of both that prevent accurate reconstruction. In the example of aortic aneurysmal disease, a blood clot or thrombus adherent to the aortic wall within the expanding aneurysmal sac is present in 70-80% of cases. These deformations prevent the automatic extraction of vital clinically relevant information by current methods. In this study, we implemented a modified U-Net architecture with attention-gating to establish a high-throughput and automated segmentation pipeline of pathological blood vessels in CT images acquired with or without the use of a contrast agent. Twenty-six patients with paired non-contrast and contrast-enhanced CT images within the ongoing Oxford Abdominal Aortic Aneurysm (OxAAA) study were randomly selected, manually annotated and used for model training and evaluation (13/13). Data augmentation methods were implemented to diversify the training data set in a ratio of 10:1. The performance of our Attention-based U-Net in extracting both the inner lumen and the outer wall of the aortic aneurysm from CT angiograms (CTA) was compared against a generic 3-D U-Net and displayed superior results. Subsequent implementation of this network architecture within the aortic segmentation pipeline from both contrast-enhanced CTA and non-contrast CT images has allowed for accurate and efficient extraction of the entire aortic volume. This extracted volume can be used to standardize current methods of aneurysmal disease management and sets the foundation for subsequent complex geometric and morphological analysis. Furthermore, the proposed pipeline can be extended to other vascular pathologies.




# Arxiv Papers in cs.CV on 2020-02-23
### Real-Time Detectors for Digital and Physical Adversarial Inputs to Perception Systems
- **Arxiv ID**: http://arxiv.org/abs/2002.09792v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09792v2)
- **Published**: 2020-02-23 00:03:57+00:00
- **Updated**: 2022-04-21 22:20:39+00:00
- **Authors**: Yiannis Kantaros, Taylor Carpenter, Kaustubh Sridhar, Yahan Yang, Insup Lee, James Weimer
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network (DNN) models have proven to be vulnerable to adversarial digital and physical attacks. In this paper, we propose a novel attack- and dataset-agnostic and real-time detector for both types of adversarial inputs to DNN-based perception systems. In particular, the proposed detector relies on the observation that adversarial images are sensitive to certain label-invariant transformations. Specifically, to determine if an image has been adversarially manipulated, the proposed detector checks if the output of the target classifier on a given input image changes significantly after feeding it a transformed version of the image under investigation. Moreover, we show that the proposed detector is computationally-light both at runtime and design-time which makes it suitable for real-time applications that may also involve large-scale image domains. To highlight this, we demonstrate the efficiency of the proposed detector on ImageNet, a task that is computationally challenging for the majority of relevant defenses, and on physically attacked traffic signs that may be encountered in real-time autonomy applications. Finally, we propose the first adversarial dataset, called AdvNet that includes both clean and physical traffic sign images. Our extensive comparative experiments on the MNIST, CIFAR10, ImageNet, and AdvNet datasets show that VisionGuard outperforms existing defenses in terms of scalability and detection performance. We have also evaluated the proposed detector on field test data obtained on a moving vehicle equipped with a perception-based DNN being under attack.



### Reliable Fidelity and Diversity Metrics for Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2002.09797v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09797v2)
- **Published**: 2020-02-23 00:50:01+00:00
- **Updated**: 2020-06-28 20:37:50+00:00
- **Authors**: Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, Jaejun Yoo
- **Comment**: First two authors have contributed equally; ICML 2020 accepted
- **Journal**: None
- **Summary**: Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Fr\'echet Inception Distance (FID) score. Because it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics. Code: https://github.com/clovaai/generative-evaluation-prdc.



### Random Bundle: Brain Metastases Segmentation Ensembling through Annotation Randomization
- **Arxiv ID**: http://arxiv.org/abs/2002.09809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09809v2)
- **Published**: 2020-02-23 02:07:01+00:00
- **Updated**: 2020-04-28 15:56:03+00:00
- **Authors**: Darvin Yi, Endre Grøvik, Michael Iv, Elizabeth Tong, Greg Zaharchuk, Daniel Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel ensembling method, Random Bundle (RB), that improves performance for brain metastases segmentation. We create our ensemble by training each network on our dataset with 50% of our annotated lesions censored out. We also apply a lopsided bootstrap loss to recover performance after inducing an in silico 50% false negative rate and make our networks more sensitive. We improve our network detection of lesions's mAP value by 39% and more than triple the sensitivity at 80% precision. We also show slight improvements in segmentation quality through DICE score. Further, RB ensembling improves performance over baseline by a larger margin than a variety of popular ensembling strategies. Finally, we show that RB ensembling is computationally efficient by comparing its performance to a single network when both systems are constrained to have the same compute.



### Neuron Shapley: Discovering the Responsible Neurons
- **Arxiv ID**: http://arxiv.org/abs/2002.09815v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.09815v3)
- **Published**: 2020-02-23 03:29:58+00:00
- **Updated**: 2020-11-13 22:06:48+00:00
- **Authors**: Amirata Ghorbani, James Zou
- **Comment**: None
- **Journal**: None
- **Summary**: We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.



### Assembling Semantically-Disentangled Representations for Predictive-Generative Models via Adaptation from Synthetic Domain
- **Arxiv ID**: http://arxiv.org/abs/2002.09818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2002.09818v1)
- **Published**: 2020-02-23 03:35:45+00:00
- **Updated**: 2020-02-23 03:35:45+00:00
- **Authors**: Burkay Donderici, Caleb New, Chenliang Xu
- **Comment**: 8 pages, 18 figures
- **Journal**: None
- **Summary**: Deep neural networks can form high-level hierarchical representations of input data. Various researchers have demonstrated that these representations can be used to enable a variety of useful applications. However, such representations are typically based on the statistics within the data, and may not conform with the semantic representation that may be necessitated by the application. Conditional models are typically used to overcome this challenge, but they require large annotated datasets which are difficult to come by and costly to create. In this paper, we show that semantically-aligned representations can be generated instead with the help of a physics based engine. This is accomplished by creating a synthetic dataset with decoupled attributes, learning an encoder for the synthetic dataset, and augmenting prescribed attributes from the synthetic domain with attributes from the real domain. It is shown that the proposed (SYNTH-VAE-GAN) method can construct a conditional predictive-generative model of human face attributes without relying on real data labels.



### Predicting TUG score from gait characteristics with video analysis and machine learning
- **Arxiv ID**: http://arxiv.org/abs/2003.00875v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00875v2)
- **Published**: 2020-02-23 05:27:37+00:00
- **Updated**: 2020-04-28 11:34:58+00:00
- **Authors**: Jian Ma
- **Comment**: Experimental results and discussion are revised. The code for
  estimating copula entropy is available at https://github.com/majianthu/copent
- **Journal**: None
- **Summary**: Fall is a leading cause of death which suffers the elderly and society. Timed Up and Go (TUG) test is a common tool for fall risk assessment. In this paper, we propose a method for predicting TUG score from gait characteristics extracted from video with computer vision and machine learning technologies. First, 3D pose is estimated from video captured with 2D and 3D cameras during human motion and then a group of gait characteristics are computed from 3D pose series. After that, copula entropy is used to select those characteristics which are mostly associated with TUG score. Finally, the selected characteristics are fed into the predictive models to predict TUG score. Experiments on real world data demonstrated the effectiveness of the proposed method. As a byproduct, the associations between TUG score and several gait characteristics are discovered, which laid the scientific foundation of the proposed method and make the predictive models such built interpretable to clinical users.



### An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.09843v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09843v5)
- **Published**: 2020-02-23 06:50:20+00:00
- **Updated**: 2021-08-15 16:48:08+00:00
- **Authors**: Xue Yang, Yan Feng, Weijun Fang, Jun Shao, Xiaohu Tang, Shu-Tao Xia, Rongxing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Although federated learning improves privacy of training data by exchanging local gradients or parameters rather than raw data, the adversary still can leverage local gradients and parameters to obtain local training data by launching reconstruction and membership inference attacks. To defend such privacy attacks, many noises perturbation methods (like differential privacy or CountSketch matrix) have been widely designed. However, the strong defence ability and high learning accuracy of these schemes cannot be ensured at the same time, which will impede the wide application of FL in practice (especially for medical or financial institutions that require both high accuracy and strong privacy guarantee). To overcome this issue, in this paper, we propose \emph{an efficient model perturbation method for federated learning} to defend reconstruction and membership inference attacks launched by curious clients. On the one hand, similar to the differential privacy, our method also selects random numbers as perturbed noises added to the global model parameters, and thus it is very efficient and easy to be integrated in practice. Meanwhile, the random selected noises are positive real numbers and the corresponding value can be arbitrarily large, and thus the strong defence ability can be ensured. On the other hand, unlike differential privacy or other perturbation methods that cannot eliminate the added noises, our method allows the server to recover the true gradients by eliminating the added noises. Therefore, our method does not hinder learning accuracy at all.



### Unsupervised Denoising for Satellite Imagery using Wavelet Subband CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2002.09847v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09847v1)
- **Published**: 2020-02-23 07:11:05+00:00
- **Updated**: 2020-02-23 07:11:05+00:00
- **Authors**: Joonyoung Song, Jae-Heon Jeong, Dae-Soon Park, Hyun-Ho Kim, Doo-Chun Seo, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-spectral satellite imaging sensors acquire various spectral band images such as red (R), green (G), blue (B), near-infrared (N), etc. Thanks to the unique spectroscopic property of each spectral band with respective to the objects on the ground, multi-spectral satellite imagery can be used for various geological survey applications. Unfortunately, image artifacts from imaging sensor noises often affect the quality of scenes and have negative impacts on the applications of satellite imagery. Recently, deep learning approaches have been extensively explored for the removal of noises in satellite imagery. Most deep learning denoising methods, however, follow a supervised learning scheme, which requires matched noisy image and clean image pairs that are difficult to collect in real situations. In this paper, we propose a novel unsupervised multispectral denoising method for satellite imagery using wavelet subband cycle-consistent adversarial network (WavCycleGAN). The proposed method is based on unsupervised learning scheme using adversarial loss and cycle-consistency loss to overcome the lack of paired data. Moreover, in contrast to the standard image domain cycleGAN, we introduce a wavelet subband domain learning scheme for effective denoising without sacrificing high frequency components such as edges and detail information. Experimental results for the removal of vertical stripe and wave noises in satellite imaging sensors demonstrate that the proposed method effectively removes noises and preserves important high frequency features of satellite images.



### DotFAN: A Domain-transferred Face Augmentation Network for Pose and Illumination Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.09859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09859v1)
- **Published**: 2020-02-23 08:16:34+00:00
- **Updated**: 2020-02-23 08:16:34+00:00
- **Authors**: Hao-Chiang Shao, Kang-Yu Liu, Chia-Wen Lin, Jiwen Lu
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: The performance of a convolutional neural network (CNN) based face recognition model largely relies on the richness of labelled training data. Collecting a training set with large variations of a face identity under different poses and illumination changes, however, is very expensive, making the diversity of within-class face images a critical issue in practice. In this paper, we propose a 3D model-assisted domain-transferred face augmentation network (DotFAN) that can generate a series of variants of an input face based on the knowledge distilled from existing rich face datasets collected from other domains. DotFAN is structurally a conditional CycleGAN but has two additional subnetworks, namely face expert network (FEM) and face shape regressor (FSR), for latent code control. While FSR aims to extract face attributes, FEM is designed to capture a face identity. With their aid, DotFAN can learn a disentangled face representation and effectively generate face images of various facial attributes while preserving the identity of augmented faces. Experiments show that DotFAN is beneficial for augmenting small face datasets to improve their within-class diversity so that a better face recognition model can be learned from the augmented dataset.



### Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2002.09905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09905v2)
- **Published**: 2020-02-23 13:46:29+00:00
- **Updated**: 2020-05-22 14:46:22+00:00
- **Authors**: Beibei Jin, Yu Hu, Qiankun Tang, Jingyu Niu, Zhiping Shi, Yinhe Han, Xiaowei Li
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Video prediction is a pixel-wise dense prediction task to infer future frames based on past frames. Missing appearance details and motion blur are still two major problems for current predictive models, which lead to image distortion and temporal inconsistency. In this paper, we point out the necessity of exploring multi-frequency analysis to deal with the two problems. Inspired by the frequency band decomposition characteristic of Human Vision System (HVS), we propose a video prediction network based on multi-level wavelet analysis to deal with spatial and temporal information in a unified manner. Specifically, the multi-level spatial discrete wavelet transform decomposes each video frame into anisotropic sub-bands with multiple frequencies, helping to enrich structural information and reserve fine details. On the other hand, multi-level temporal discrete wavelet transform which operates on time axis decomposes the frame sequence into sub-band groups of different frequencies to accurately capture multi-frequency motions under a fixed frame rate. Extensive experiments on diverse datasets demonstrate that our model shows significant improvements on fidelity and temporal consistency over state-of-the-art works.



### Beyond Dropout: Feature Map Distortion to Regularize Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.11022v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11022v1)
- **Published**: 2020-02-23 13:59:13+00:00
- **Updated**: 2020-02-23 13:59:13+00:00
- **Authors**: Yehui Tang, Yunhe Wang, Yixing Xu, Boxin Shi, Chao Xu, Chunjing Xu, Chang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks often consist of a great number of trainable parameters for extracting powerful features from given datasets. On one hand, massive trainable parameters significantly enhance the performance of these deep networks. On the other hand, they bring the problem of over-fitting. To this end, dropout based methods disable some elements in the output feature maps during the training phase for reducing the co-adaptation of neurons. Although the generalization ability of the resulting models can be enhanced by these approaches, the conventional binary dropout is not the optimal solution. Therefore, we investigate the empirical Rademacher complexity related to intermediate layers of deep neural networks and propose a feature distortion method (Disout) for addressing the aforementioned problem. In the training period, randomly selected elements in the feature maps will be replaced with specific values by exploiting the generalization error bound. The superiority of the proposed feature map distortion for producing deep neural network with higher testing performance is analyzed and demonstrated on several benchmark image datasets.



### Monocular Direct Sparse Localization in a Prior 3D Surfel Map
- **Arxiv ID**: http://arxiv.org/abs/2002.09923v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09923v1)
- **Published**: 2020-02-23 15:29:38+00:00
- **Updated**: 2020-02-23 15:29:38+00:00
- **Authors**: Haoyang Ye, Huaiyang Huang, Ming Liu
- **Comment**: 7 pages, 6 figures, to appear in ICRA 2020
- **Journal**: None
- **Summary**: In this paper, we introduce an approach to tracking the pose of a monocular camera in a prior surfel map. By rendering vertex and normal maps from the prior surfel map, the global planar information for the sparse tracked points in the image frame is obtained. The tracked points with and without the global planar information involve both global and local constraints of frames to the system. Our approach formulates all constraints in the form of direct photometric errors within a local window of the frames. The final optimization utilizes these constraints to provide the accurate estimation of global 6-DoF camera poses with the absolute scale. The extensive simulation and real-world experiments demonstrate that our monocular method can provide accurate camera localization results under various conditions.



### PolyGen: An Autoregressive Generative Model of 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/2002.10880v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10880v1)
- **Published**: 2020-02-23 17:16:34+00:00
- **Updated**: 2020-02-23 17:16:34+00:00
- **Authors**: Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter W. Battaglia
- **Comment**: None
- **Journal**: None
- **Summary**: Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.



### Multi-Stream Networks and Ground-Truth Generation for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2002.09951v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09951v3)
- **Published**: 2020-02-23 17:31:48+00:00
- **Updated**: 2020-03-11 20:47:00+00:00
- **Authors**: Rodolfo Quispe, Darwin Ttito, Adín Ramírez Rivera, Helio Pedrini
- **Comment**: https://github.com/RQuispeC/multi-stream-crowd-counting-extended ,
  The International Journal of Electrical and Computer Engineering Systems 2020
- **Journal**: None
- **Summary**: Crowd scene analysis has received a lot of attention recently due to the wide variety of applications, for instance, forensic science, urban planning, surveillance and security. In this context, a challenging task is known as crowd counting, whose main purpose is to estimate the number of people present in a single image. A Multi-Stream Convolutional Neural Network is developed and evaluated in this work, which receives an image as input and produces a density map that represents the spatial distribution of people in an end-to-end fashion. In order to address complex crowd counting issues, such as extremely unconstrained scale and perspective changes, the network architecture utilizes receptive fields with different size filters for each stream. In addition, we investigate the influence of the two most common fashions on the generation of ground truths and propose a hybrid method based on tiny face detection and scale interpolation. Experiments conducted on two challenging datasets, UCF-CC-50 and ShanghaiTech, demonstrate that using our ground truth generation methods achieves superior results.



### Gradual Channel Pruning while Training using Feature Relevance Scores for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.09958v2
- **DOI**: 10.1109/ACCESS.2020.3024992
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09958v2)
- **Published**: 2020-02-23 17:56:18+00:00
- **Updated**: 2020-04-29 15:01:47+00:00
- **Authors**: Sai Aparna Aketi, Sourjya Roy, Anand Raghunathan, Kaushik Roy
- **Comment**: 15 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: The enormous inference cost of deep neural networks can be scaled down by network compression. Pruning is one of the predominant approaches used for deep network compression. However, existing pruning techniques have one or more of the following limitations: 1) Additional energy cost on top of the compute heavy training stage due to pruning and fine-tuning stages, 2) Layer-wise pruning based on the statistics of a particular, ignoring the effect of error propagation in the network, 3) Lack of an efficient estimate for determining the important channels globally, 4) Unstructured pruning requires specialized hardware for effective use. To address all the above issues, we present a simple-yet-effective gradual channel pruning while training methodology using a novel data-driven metric referred to as feature relevance score. The proposed technique gets rid of the additional retraining cycles by pruning the least important channels in a structured fashion at fixed intervals during the actual training phase. Feature relevance scores help in efficiently evaluating the contribution of each channel towards the discriminative power of the network. We demonstrate the effectiveness of the proposed methodology on architectures such as VGG and ResNet using datasets such as CIFAR-10, CIFAR-100 and ImageNet, and successfully achieve significant model compression while trading off less than $1\%$ accuracy. Notably on CIFAR-10 dataset trained on ResNet-110, our approach achieves $2.4\times$ compression and a $56\%$ reduction in FLOPs with an accuracy drop of $0.01\%$ compared to the unpruned network.



### NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through Aggregated Convolutional Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2002.10003v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.10003v1)
- **Published**: 2020-02-23 22:35:59+00:00
- **Updated**: 2020-02-23 22:35:59+00:00
- **Authors**: Maximilian Seitzer
- **Comment**: Disentanglement Challenge - 33rd Conference on Neural Information
  Processing Systems (NeurIPS) - NeurIPS 2019
- **Journal**: None
- **Summary**: This report to our stage 1 submission to the NeurIPS 2019 disentanglement challenge presents a simple image preprocessing method for training VAEs leading to improved disentanglement compared to directly using the images. In particular, we propose to use regionally aggregated feature maps extracted from CNNs pretrained on ImageNet. Our method achieved the 2nd place in stage 1 of the challenge. Code is available at https://github.com/mseitzer/neurips2019-disentanglement-challenge.



### Deep Multimodal Image-Text Embeddings for Automatic Cross-Media Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2002.10016v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV, cs.LG, E.0; H.3.3; I.2.0; I.2.6; I.2.7; I.2.10; I.5.0; I.4.0; I.4.10; I.7.0
- **Links**: [PDF](http://arxiv.org/pdf/2002.10016v1)
- **Published**: 2020-02-23 23:58:04+00:00
- **Updated**: 2020-02-23 23:58:04+00:00
- **Authors**: Hadi Abdi Khojasteh, Ebrahim Ansari, Parvin Razzaghi, Akbar Karimi
- **Comment**: 6 pages and 2 figures, Learn more about this project at
  https://iasbs.ac.ir/~ansari/deeptwitter
- **Journal**: None
- **Summary**: This paper considers the task of matching images and sentences by learning a visual-textual embedding space for cross-modal retrieval. Finding such a space is a challenging task since the features and representations of text and image are not comparable. In this work, we introduce an end-to-end deep multimodal convolutional-recurrent network for learning both vision and language representations simultaneously to infer image-text similarity. The model learns which pairs are a match (positive) and which ones are a mismatch (negative) using a hinge-based triplet ranking. To learn about the joint representations, we leverage our newly extracted collection of tweets from Twitter. The main characteristic of our dataset is that the images and tweets are not standardized the same as the benchmarks. Furthermore, there can be a higher semantic correlation between the pictures and tweets contrary to benchmarks in which the descriptions are well-organized. Experimental results on MS-COCO benchmark dataset show that our model outperforms certain methods presented previously and has competitive performance compared to the state-of-the-art. The code and dataset have been made available publicly.




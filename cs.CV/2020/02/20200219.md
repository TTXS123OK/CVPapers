# Arxiv Papers in cs.CV on 2020-02-19
### Algorithm-hardware Co-design for Deformable Convolution
- **Arxiv ID**: http://arxiv.org/abs/2002.08357v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08357v1)
- **Published**: 2020-02-19 01:08:11+00:00
- **Updated**: 2020-02-19 01:08:11+00:00
- **Authors**: Qijing Huang, Dequan Wang, Yizhao Gao, Yaohui Cai, Zhen Dong, Bichen Wu, Kurt Keutzer, John Wawrzynek
- **Comment**: None
- **Journal**: NeurIPS EMC2 2019
- **Summary**: FPGAs provide a flexible and efficient platform to accelerate rapidly-changing algorithms for computer vision. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, including object detection and instance segmentation, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this, recent work proposes dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolutions may access arbitrary pixels in the image and the access pattern is input-dependent and varies per spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware. In this work, we first investigate the overhead of the deformable convolution on embedded FPGA SoCs, and then show the accuracy-latency tradeoffs for a set of algorithm modifications including full versus depthwise, fixed-shape, and limited-range. These modifications benefit the energy efficiency for embedded devices in general as they reduce the compute complexity. We then build an efficient object detection network with modified deformable convolutions and quantize the network using state-of-the-art quantization methods. We implement a unified hardware engine on FPGA to support all the operations in the network. Preliminary experiments show that little accuracy is compromised and speedup can be achieved with our co-design optimization for the deformable convolution.



### Universal Domain Adaptation through Self Supervision
- **Arxiv ID**: http://arxiv.org/abs/2002.07953v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.07953v3)
- **Published**: 2020-02-19 01:26:11+00:00
- **Updated**: 2020-10-06 03:30:01+00:00
- **Authors**: Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Kate Saenko
- **Comment**: Accepted to NeurIPS2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation methods traditionally assume that all source categories are present in the target domain. In practice, little may be known about the category overlap between the two domains. While some methods address target settings with either partial or open-set categories, they assume that the particular setting is known a priori. We propose a more universally applicable domain adaptation framework that can handle arbitrary category shift, called Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE). DANCE combines two novel ideas: First, as we cannot fully rely on source categories to learn features discriminative for the target, we propose a novel neighborhood clustering technique to learn the structure of the target domain in a self-supervised way. Second, we use entropy-based feature alignment and rejection to align target features with the source, or reject them as unknown categories based on their entropy. We show through extensive experiments that DANCE outperforms baselines across open-set, open-partial and partial domain adaptation settings. Implementation is available at https://github.com/VisionLearningGroup/DANCE.



### SAFE: Similarity-Aware Multi-Modal Fake News Detection
- **Arxiv ID**: http://arxiv.org/abs/2003.04981v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.04981v1)
- **Published**: 2020-02-19 02:51:04+00:00
- **Updated**: 2020-02-19 02:51:04+00:00
- **Authors**: Xinyi Zhou, Jindi Wu, Reza Zafarani
- **Comment**: To be published in The 24th Pacific-Asia Conference on Knowledge
  Discovery and Data Mining (PAKDD 2020)
- **Journal**: None
- **Summary**: Effective detection of fake news has recently attracted significant attention. Current studies have made significant contributions to predicting fake news with less focus on exploiting the relationship (similarity) between the textual and visual information in news articles. Attaching importance to such similarity helps identify fake news stories that, for example, attempt to use irrelevant images to attract readers' attention. In this work, we propose a $\mathsf{S}$imilarity-$\mathsf{A}$ware $\mathsf{F}$ak$\mathsf{E}$ news detection method ($\mathsf{SAFE}$) which investigates multi-modal (textual and visual) information of news articles. First, neural networks are adopted to separately extract textual and visual features for news representation. We further investigate the relationship between the extracted features across modalities. Such representations of news textual and visual information along with their relationship are jointly learned and used to predict fake news. The proposed method facilitates recognizing the falsity of news articles based on their text, images, or their "mismatches." We conduct extensive experiments on large-scale real-world data, which demonstrate the effectiveness of the proposed method.



### Globally optimal point set registration by joint symmetry plane fitting
- **Arxiv ID**: http://arxiv.org/abs/2002.07988v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.07988v1)
- **Published**: 2020-02-19 03:40:04+00:00
- **Updated**: 2020-02-19 03:40:04+00:00
- **Authors**: Lan Hu, Haomin Shi, Laurent Kneip
- **Comment**: None
- **Journal**: None
- **Summary**: The present work proposes a solution to the challenging problem of registering two partial point sets of the same object with very limited overlap. We leverage the fact that most objects found in man-made environments contain a plane of symmetry. By reflecting the points of each set with respect to the plane of symmetry, we can largely increase the overlap between the sets and therefore boost the registration process. However, prior knowledge about the plane of symmetry is generally unavailable or at least very hard to find, especially with limited partial views, and finding this plane could strongly benefit from a prior alignment of the partial point sets. We solve this chicken-and-egg problem by jointly optimizing the relative pose and symmetry plane parameters, and notably do so under global optimality by employing the branch-and-bound (BnB) paradigm. Our results demonstrate a great improvement over the current state-of-the-art in globally optimal point set registration for common objects. We furthermore show an interesting application of our method to dense 3D reconstruction of scenes with repetitive objects.



### On-line non-overlapping camera calibration net
- **Arxiv ID**: http://arxiv.org/abs/2002.08005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08005v1)
- **Published**: 2020-02-19 04:59:11+00:00
- **Updated**: 2020-02-19 04:59:11+00:00
- **Authors**: Zhao Fangda, Toru Tamaki, Takio Kurita, Bisser Raytchev, Kazufumi Kaneda
- **Comment**: 7 pages
- **Journal**: in Proc. of MIRU2018
- **Summary**: We propose an easy-to-use non-overlapping camera calibration method. First, successive images are fed to a PoseNet-based network to obtain ego-motion of cameras between frames. Next, the pose between cameras are estimated. Instead of using a batch method, we propose an on-line method of the inter-camera pose estimation. Furthermore, we implement the entire procedure on a computation graph. Experiments with simulations and the KITTI dataset show the proposed method to be effective in simulation.



### Cross-Resolution Adversarial Dual Network for Person Re-Identification and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2002.09274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09274v2)
- **Published**: 2020-02-19 07:21:38+00:00
- **Updated**: 2020-10-22 18:01:01+00:00
- **Authors**: Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Yu-Chiang Frank Wang
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). 17 pages. arXiv admin note: substantial text overlap
  with arXiv:1908.06052
- **Journal**: None
- **Summary**: Person re-identification (re-ID) aims at matching images of the same person across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Extensive experimental results on five standard person re-ID benchmarks confirm the effectiveness of our method and the superiority over the state-of-the-art approaches, especially when the input resolutions are not seen during training. Furthermore, the experimental results on two vehicle re-ID benchmarks also confirm the generalization of our model on cross-resolution visual tasks. The extensions of semi-supervised settings further support the use of our proposed approach to real-world scenarios and applications.



### Feasibility of Video-based Sub-meter Localization on Resource-constrained Platforms
- **Arxiv ID**: http://arxiv.org/abs/2002.08039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08039v1)
- **Published**: 2020-02-19 07:35:12+00:00
- **Updated**: 2020-02-19 07:35:12+00:00
- **Authors**: Abm Musa, Jakob Eriksson
- **Comment**: None
- **Journal**: None
- **Summary**: While the satellite-based Global Positioning System (GPS) is adequate for some outdoor applications, many other applications are held back by its multi-meter positioning errors and poor indoor coverage. In this paper, we study the feasibility of real-time video-based localization on resource-constrained platforms. Before commencing a localization task, a video-based localization system downloads an offline model of a restricted target environment, such as a set of city streets, or an indoor shopping mall. The system is then able to localize the user within the model, using only video as input.   To enable such a system to run on resource-constrained embedded systems or smartphones, we (a) propose techniques for efficiently building a 3D model of a surveyed path, through frame selection and efficient feature matching, (b) substantially reduce model size by multiple compression techniques, without sacrificing localization accuracy, (c) propose efficient and concurrent techniques for feature extraction and matching to enable online localization, (d) propose a method with interleaved feature matching and optical flow based tracking to reduce the feature extraction and matching time in online localization.   Based on an extensive set of both indoor and outdoor videos, manually annotated with location ground truth, we demonstrate that sub-meter accuracy, at real-time rates, is achievable on smart-phone type platforms, despite challenging video conditions.



### Enlarging Discriminative Power by Adding an Extra Class in Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.08041v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08041v1)
- **Published**: 2020-02-19 07:58:24+00:00
- **Updated**: 2020-02-19 07:58:24+00:00
- **Authors**: Hai H. Tran, Sumyeong Ahn, Taeyoung Lee, Yung Yi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of unsupervised domain adaptation that aims at obtaining a prediction model for the target domain using labeled data from the source domain and unlabeled data from the target domain. There exists an array of recent research based on the idea of extracting features that are not only invariant for both domains but also provide high discriminative power for the target domain. In this paper, we propose an idea of empowering the discriminativeness: Adding a new, artificial class and training the model on the data together with the GAN-generated samples of the new class. The trained model based on the new class samples is capable of extracting the features that are more discriminative by repositioning data of current classes in the target domain and therefore drawing the decision boundaries more effectively. Our idea is highly generic so that it is compatible with many existing methods such as DANN, VADA, and DIRT-T. We conduct various experiments for the standard data commonly used for the evaluation of unsupervised domain adaptations and demonstrate that our algorithm achieves the SOTA performance for many scenarios.



### Meta Segmentation Network for Ultra-Resolution Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2002.08043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08043v1)
- **Published**: 2020-02-19 08:05:47+00:00
- **Updated**: 2020-02-19 08:05:47+00:00
- **Authors**: Tong Wu, Yuan Xie, Yanyun Qu, Bicheng Dai, Shuxin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent progress on semantic segmentation, there still exist huge challenges in medical ultra-resolution image segmentation. The methods based on multi-branch structure can make a good balance between computational burdens and segmentation accuracy. However, the fusion structure in these methods require to be designed elaborately to achieve desirable result, which leads to model redundancy. In this paper, we propose Meta Segmentation Network (MSN) to solve this challenging problem. With the help of meta-learning, the fusion module of MSN is quite simple but effective. MSN can fast generate the weights of fusion layers through a simple meta-learner, requiring only a few training samples and epochs to converge. In addition, to avoid learning all branches from scratch, we further introduce a particular weight sharing mechanism to realize a fast knowledge adaptation and share the weights among multiple branches, resulting in the performance improvement and significant parameters reduction. The experimental results on two challenging ultra-resolution medical datasets BACH and ISIC show that MSN achieves the best performance compared with the state-of-the-art methods.



### Unsupervised Temporal Feature Aggregation for Event Detection in Unstructured Sports Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.08097v1
- **DOI**: 10.1109/ISM46123.2019.00011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08097v1)
- **Published**: 2020-02-19 10:24:22+00:00
- **Updated**: 2020-02-19 10:24:22+00:00
- **Authors**: Subhajit Chaudhury, Daiki Kimura, Phongtharin Vinayavekhin, Asim Munawar, Ryuki Tachibana, Koji Ito, Yuki Inaba, Minoru Matsumoto, Shuji Kidokoro, Hiroki Ozaki
- **Comment**: Accepted to IEEE International Symposium on Multimedia, 2019
- **Journal**: None
- **Summary**: Image-based sports analytics enable automatic retrieval of key events in a game to speed up the analytics process for human experts. However, most existing methods focus on structured television broadcast video datasets with a straight and fixed camera having minimum variability in the capturing pose. In this paper, we study the case of event detection in sports videos for unstructured environments with arbitrary camera angles. The transition from structured to unstructured video analysis produces multiple challenges that we address in our paper. Specifically, we identify and solve two major problems: unsupervised identification of players in an unstructured setting and generalization of the trained models to pose variations due to arbitrary shooting angles. For the first problem, we propose a temporal feature aggregation algorithm using person re-identification features to obtain high player retrieval precision by boosting a weak heuristic scoring method. Additionally, we propose a data augmentation technique, based on multi-modal image translation model, to reduce bias in the appearance of training samples. Experimental evaluations show that our proposed method improves precision for player retrieval from 0.78 to 0.86 for obliquely angled videos. Additionally, we obtain an improvement in F1 score for rally detection in table tennis videos from 0.79 in case of global frame-level features to 0.89 using our proposed player-level features. Please see the supplementary video submission at https://ibm.biz/BdzeZA.



### Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.08098v1
- **DOI**: 10.1007/s11263-020-01293-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08098v1)
- **Published**: 2020-02-19 10:32:03+00:00
- **Updated**: 2020-02-19 10:32:03+00:00
- **Authors**: Xiang Wang, Sifei Liu, Huimin Ma, Ming-Hsuan Yang
- **Comment**: IJCV 2020
- **Journal**: None
- **Summary**: Weakly-supervised semantic segmentation is a challenging task as no pixel-wise label information is provided for training. Recent methods have exploited classification networks to localize objects by selecting regions with strong response. While such response map provides sparse information, however, there exist strong pairwise relations between pixels in natural images, which can be utilized to propagate the sparse map to a much denser one. In this paper, we propose an iterative algorithm to learn such pairwise relations, which consists of two branches, a unary segmentation network which learns the label probabilities for each pixel, and a pairwise affinity network which learns affinity matrix and refines the probability map generated from the unary network. The refined results by the pairwise network are then used as supervision to train the unary network, and the procedures are conducted iteratively to obtain better segmentation progressively. To learn reliable pixel affinity without accurate annotation, we also propose to mine confident regions. We show that iteratively training this framework is equivalent to optimizing an energy function with convergence to a local minimum. Experimental results on the PASCAL VOC 2012 and COCO datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.



### Analyzing Neural Networks Based on Random Graphs
- **Arxiv ID**: http://arxiv.org/abs/2002.08104v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08104v3)
- **Published**: 2020-02-19 11:04:49+00:00
- **Updated**: 2020-12-02 11:29:36+00:00
- **Authors**: Romuald A. Janik, Aleksandra Nowak
- **Comment**: Added new results and discussion
- **Journal**: None
- **Summary**: We perform a massive evaluation of neural networks with architectures corresponding to random graphs of various types. We investigate various structural and numerical properties of the graphs in relation to neural network test accuracy. We find that none of the classical numerical graph invariants by itself allows to single out the best networks. Consequently, we introduce a new numerical graph characteristic that selects a set of quasi-1-dimensional graphs, which are a majority among the best performing networks. We also find that networks with primarily short-range connections perform better than networks which allow for many long-range connections. Moreover, many resolution reducing pathways are beneficial. We provide a dataset of 1020 graphs and the test accuracies of their corresponding neural networks at https://github.com/rmldj/random-graph-nn-paper



### Hierarchical Quantized Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2002.08111v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08111v3)
- **Published**: 2020-02-19 11:26:34+00:00
- **Updated**: 2020-10-16 11:10:26+00:00
- **Authors**: Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty
- **Comment**: None
- **Journal**: None
- **Summary**: Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.



### Randomized Smoothing of All Shapes and Sizes
- **Arxiv ID**: http://arxiv.org/abs/2002.08118v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08118v5)
- **Published**: 2020-02-19 11:41:09+00:00
- **Updated**: 2020-07-23 21:20:51+00:00
- **Authors**: Greg Yang, Tony Duan, J. Edward Hu, Hadi Salman, Ilya Razenshteyn, Jerry Li
- **Comment**: 9 pages main text, 49 pages total
- **Journal**: None
- **Summary**: Randomized smoothing is the current state-of-the-art defense with provable robustness against $\ell_2$ adversarial attacks. Many works have devised new randomized smoothing schemes for other metrics, such as $\ell_1$ or $\ell_\infty$; however, substantial effort was needed to derive such new guarantees. This begs the question: can we find a general theory for randomized smoothing?   We propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are: (1) we show that for an appropriate notion of "optimal", the optimal smoothing distributions for any "nice" norms have level sets given by the norm's *Wulff Crystal*; (2) we propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution; and, (3) we show fundamental limits to current randomized smoothing techniques via the theory of *Banach space cotypes*. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in $\ell_1$ on standard datasets. Meanwhile, we show using (3) that with only label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of $\ell_p$-norm $\Omega(\min(1, d^{\frac{1}{p} - \frac{1}{2}}))$, when the input dimension $d$ is large. We provide code in github.com/tonyduan/rs4a.



### Structured Sparsification with Joint Optimization of Group Convolution and Channel Shuffle
- **Arxiv ID**: http://arxiv.org/abs/2002.08127v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08127v2)
- **Published**: 2020-02-19 12:03:10+00:00
- **Updated**: 2021-05-14 05:50:42+00:00
- **Authors**: Xin-Yu Zhang, Kai Zhao, Taihong Xiao, Ming-Ming Cheng, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in convolutional neural networks(CNNs) usually come with the expense of excessive computational overhead and memory footprint. Network compression aims to alleviate this issue by training compact models with comparable performance. However, existing compression techniques either entail dedicated expert design or compromise with a moderate performance drop. In this paper, we propose a novel structured sparsification method for efficient network compression. The proposed method automatically induces structured sparsity on the convolutional weights, thereby facilitating the implementation of the compressed model with the highly-optimized group convolution. We further address the problem of inter-group communication with a learnable channel shuffle mechanism. The proposed approach can be easily applied to compress many network architectures with a negligible performance drop. Extensive experimental results and analysis demonstrate that our approach gives a competitive performance against the recent network compression counterparts with a sound accuracy-complexity trade-off.



### Fast Implementation of Morphological Filtering Using ARM NEON Extension
- **Arxiv ID**: http://arxiv.org/abs/2002.09474v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, eess.IV, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2002.09474v1)
- **Published**: 2020-02-19 12:55:34+00:00
- **Updated**: 2020-02-19 12:55:34+00:00
- **Authors**: Elena Limonova, Arseny Terekhin, Dmitry Nikolaev, Vladimir Arlazarov
- **Comment**: 6 pages, 4 figures
- **Journal**: International Journal of Applied Engineering Research (ISSN
  0973-4562), Volume 11, Number 24 (2016), pp. 11675-11680
- **Summary**: In this paper we consider speedup potential of morphological image filtering on ARM processors. Morphological operations are widely used in image analysis and recognition and their speedup in some cases can significantly reduce overall execution time of recognition. More specifically, we propose fast implementation of erosion and dilation using ARM SIMD extension NEON. These operations with the rectangular structuring element are separable. They were implemented using the advantages of separability as sequential horizontal and vertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm for large windows and low-constant linear complexity algorithm for small windows. Final implementation was improved with SIMD and used a combination of these methods. We also considered fast transpose implementation of 8x8 and 16x16 matrices using ARM NEON to get additional computational gain for morphological operations. Experiments showed 3 times efficiency increase for final implementation of erosion and dilation compared to van Herk/Gil-Werman algorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times speedup for 16x16 matrix transpose compared to transpose without SIMD.



### SYMOG: learning symmetric mixture of Gaussian modes for improved fixed-point quantization
- **Arxiv ID**: http://arxiv.org/abs/2002.08204v1
- **DOI**: 10.1016/j.neucom.2019.11.114
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08204v1)
- **Published**: 2020-02-19 14:17:32+00:00
- **Updated**: 2020-02-19 14:17:32+00:00
- **Authors**: Lukas Enderich, Fabian Timm, Wolfram Burgard
- **Comment**: Preprint submitted to Neurocomputing
- **Journal**: Neurocomputing 2020
- **Summary**: Deep neural networks (DNNs) have been proven to outperform classical methods on several machine learning benchmarks. However, they have high computational complexity and require powerful processing units. Especially when deployed on embedded systems, model size and inference time must be significantly reduced. We propose SYMOG (symmetric mixture of Gaussian modes), which significantly decreases the complexity of DNNs through low-bit fixed-point quantization. SYMOG is a novel soft quantization method such that the learning task and the quantization are solved simultaneously. During training the weight distribution changes from an unimodal Gaussian distribution to a symmetric mixture of Gaussians, where each mean value belongs to a particular fixed-point mode. We evaluate our approach with different architectures (LeNet5, VGG7, VGG11, DenseNet) on common benchmark data sets (MNIST, CIFAR-10, CIFAR-100) and we compare with state-of-the-art quantization approaches. We achieve excellent results and outperform 2-bit state-of-the-art performance with an error rate of only 5.71% on CIFAR-10 and 27.65% on CIFAR-100.



### DeFraudNet:End2End Fingerprint Spoof Detection using Patch Level Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.08214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08214v1)
- **Published**: 2020-02-19 14:41:06+00:00
- **Updated**: 2020-02-19 14:41:06+00:00
- **Authors**: B. V. S Anusha, Sayan Banerjee, Subhasis Chaudhuri
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: In recent years, fingerprint recognition systems have made remarkable advancements in the field of biometric security as it plays an important role in personal, national and global security. In spite of all these notable advancements, the fingerprint recognition technology is still susceptible to spoof attacks which can significantly jeopardize the user security. The cross sensor and cross material spoof detection still pose a challenge with a myriad of spoof materials emerging every day, compromising sensor interoperability and robustness. This paper proposes a novel method for fingerprint spoof detection using both global and local fingerprint feature descriptors. These descriptors are extracted using DenseNet which significantly improves cross-sensor, cross-material and cross-dataset performance. A novel patch attention network is used for finding the most discriminative patches and also for network fusion. We evaluate our method on four publicly available datasets:LivDet 2011, 2013, 2015 and 2017. A set of comprehensive experiments are carried out to evaluate cross-sensor, cross-material and cross-dataset performance over these datasets. The proposed approach achieves an average accuracy of 99.52%, 99.16% and 99.72% on LivDet 2017,2015 and 2011 respectively outperforming the current state-of-the-art results by 3% and 4% for LivDet 2015 and 2011 respectively.



### Three-Stream Fusion Network for First-Person Interaction Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.08219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08219v1)
- **Published**: 2020-02-19 14:47:05+00:00
- **Updated**: 2020-02-19 14:47:05+00:00
- **Authors**: Ye-Ji Kim, Dong-Gyu Lee, Seong-Whan Lee
- **Comment**: 30 pages, 9 figures
- **Journal**: None
- **Summary**: First-person interaction recognition is a challenging task because of unstable video conditions resulting from the camera wearer's movement. For human interaction recognition from a first-person viewpoint, this paper proposes a three-stream fusion network with two main parts: three-stream architecture and three-stream correlation fusion. Thre three-stream architecture captures the characteristics of the target appearance, target motion, and camera ego-motion. Meanwhile the three-stream correlation fusion combines the feature map of each of the three streams to consider the correlations among the target appearance, target motion and camera ego-motion. The fused feature vector is robust to the camera movement and compensates for the noise of the camera ego-motion. Short-term intervals are modeled using the fused feature vector, and a long short-term memory(LSTM) model considers the temporal dynamics of the video. We evaluated the proposed method on two-public benchmark datasets to validate the effectiveness of our approach. The experimental results show that the proposed fusion method successfully generated a discriminative feature vector, and our network outperformed all competing activity recognition methods in first-person videos where considerable camera ego-motion occurs.



### siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.08239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08239v1)
- **Published**: 2020-02-19 15:32:38+00:00
- **Updated**: 2020-02-19 15:32:38+00:00
- **Authors**: Irene Cortes, Jorge Beltran, Arturo de la Escalera, Fernando Garcia
- **Comment**: Submitted to IEEE Intelligent Vehicles Symposium 2020 (IV2020)
- **Journal**: None
- **Summary**: The rapid development of embedded hardware in autonomous vehicles broadens their computational capabilities, thus bringing the possibility to mount more complete sensor setups able to handle driving scenarios of higher complexity. As a result, new challenges such as multiple detections of the same object have to be addressed. In this work, a siamese network is integrated into the pipeline of a well-known 3D object detector approach to suppress duplicate proposals coming from different cameras via re-identification. Additionally, associations are exploited to enhance the 3D box regression of the object by aggregating their corresponding LiDAR frustums. The experimental evaluation on the nuScenes dataset shows that the proposed method outperforms traditional NMS approaches.



### Weakly Supervised Semantic Segmentation of Satellite Images for Land Cover Mapping -- Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/2002.08254v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08254v2)
- **Published**: 2020-02-19 16:01:25+00:00
- **Updated**: 2020-04-28 13:24:16+00:00
- **Authors**: Michael Schmitt, Jonathan Prexl, Patrick Ebel, Lukas Liebel, Xiao Xiang Zhu
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Fully automatic large-scale land cover mapping belongs to the core challenges addressed by the remote sensing community. Usually, the basis of this task is formed by (supervised) machine learning models. However, in spite of recent growth in the availability of satellite observations, accurate training data remains comparably scarce. On the other hand, numerous global land cover products exist and can be accessed often free-of-charge. Unfortunately, these maps are typically of a much lower resolution than modern day satellite imagery. Besides, they always come with a significant amount of noise, as they cannot be considered ground truth, but are products of previous (semi-)automatic prediction tasks. Therefore, this paper seeks to make a case for the application of weakly supervised learning strategies to get the most out of available data sources and achieve progress in high-resolution large-scale land cover mapping. Challenges and opportunities are discussed based on the SEN12MS dataset, for which also some baseline results are shown. These baselines indicate that there is still a lot of potential for dedicated approaches designed to deal with remote sensing-specific forms of weak supervision.



### When Radiology Report Generation Meets Knowledge Graph
- **Arxiv ID**: http://arxiv.org/abs/2002.08277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08277v1)
- **Published**: 2020-02-19 16:39:42+00:00
- **Updated**: 2020-02-19 16:39:42+00:00
- **Authors**: Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Yuille, Daguang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic radiology report generation has been an attracting research problem towards computer-aided diagnosis to alleviate the workload of doctors in recent years. Deep learning techniques for natural image captioning are successfully adapted to generating radiology reports. However, radiology image reporting is different from the natural image captioning task in two aspects: 1) the accuracy of positive disease keyword mentions is critical in radiology image reporting in comparison to the equivalent importance of every single word in a natural image caption; 2) the evaluation of reporting quality should focus more on matching the disease keywords and their associated attributes instead of counting the occurrence of N-gram. Based on these concerns, we propose to utilize a pre-constructed graph embedding module (modeled with a graph convolutional neural network) on multiple disease findings to assist the generation of reports in this work. The incorporation of knowledge graph allows for dedicated feature learning for each disease finding and the relationship modeling between them. In addition, we proposed a new evaluation metric for radiology image reporting with the assistance of the same composed graph. Experimental results demonstrate the superior performance of the methods integrated with the proposed graph embedding module on a publicly accessible dataset (IU-RR) of chest radiographs compared with previous approaches using both the conventional evaluation metrics commonly adopted for image captioning and our proposed ones.



### Variational Encoder-based Reliable Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.08289v2
- **DOI**: 10.1109/ICIP40778.2020.9190836
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08289v2)
- **Published**: 2020-02-19 17:05:32+00:00
- **Updated**: 2020-10-17 13:51:37+00:00
- **Authors**: Chitresh Bhushan, Zhaoyuan Yang, Nurali Virani, Naresh Iyer
- **Comment**: Published in ICIP 2020. Typos fixed in revision
- **Journal**: IEEE International Conference on Image Processing (2020) 1941-1945
- **Summary**: Machine learning models provide statistically impressive results which might be individually unreliable. To provide reliability, we propose an Epistemic Classifier (EC) that can provide justification of its belief using support from the training dataset as well as quality of reconstruction. Our approach is based on modified variational auto-encoders that can identify a semantically meaningful low-dimensional space where perceptually similar instances are close in $\ell_2$-distance too. Our results demonstrate improved reliability of predictions and robust identification of samples with adversarial attacks as compared to baseline of softmax-based thresholding.



### Robust Pruning at Initialization
- **Arxiv ID**: http://arxiv.org/abs/2002.08797v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.08797v5)
- **Published**: 2020-02-19 17:09:50+00:00
- **Updated**: 2021-05-19 22:43:36+00:00
- **Authors**: Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, Yee Whye Teh
- **Comment**: 37 pages, 12 figures
- **Journal**: None
- **Summary**: Overparameterized Neural Networks (NN) display state-of-the-art performance. However, there is a growing need for smaller, energy-efficient, neural networks tobe able to use machine learning applications on devices with limited computational resources. A popular approach consists of using pruning techniques. While these techniques have traditionally focused on pruning pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et al. (2018) has shown promising results when pruning at initialization. However, for Deep NNs, such procedures remain unsatisfactory as the resulting pruned networks can be difficult to train and, for instance, they do not prevent one layer from being fully pruned. In this paper, we provide a comprehensive theoretical analysis of Magnitude and Gradient based pruning at initialization and training of sparse architectures. This allows us to propose novel principled approaches which we validate experimentally on a variety of NN architectures.



### Human Action Recognition using Local Two-Stream Convolution Neural Network Features and Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/2002.09423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09423v1)
- **Published**: 2020-02-19 17:26:32+00:00
- **Updated**: 2020-02-19 17:26:32+00:00
- **Authors**: David Torpey, Turgay Celik
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a simple yet effective method for human action recognition in video. The proposed method separately extracts local appearance and motion features using state-of-the-art three-dimensional convolutional neural networks from sampled snippets of a video. These local features are then concatenated to form global representations which are then used to train a linear SVM to perform the action classification using full context of the video, as partial context as used in previous works. The videos undergo two simple proposed preprocessing techniques, optical flow scaling and crop filling. We perform an extensive evaluation on three common benchmark dataset to empirically show the benefit of the SVM, and the two preprocessing steps.



### VQA-LOL: Visual Question Answering under the Lens of Logic
- **Arxiv ID**: http://arxiv.org/abs/2002.08325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2002.08325v2)
- **Published**: 2020-02-19 17:57:46+00:00
- **Updated**: 2020-07-15 22:39:12+00:00
- **Authors**: Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this \textit{Lens of Logic}, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our {Lens of Logic (LOL)} model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Fr\'echet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding.



### Fawkes: Protecting Privacy against Unauthorized Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2002.08327v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08327v2)
- **Published**: 2020-02-19 18:00:22+00:00
- **Updated**: 2020-06-23 03:54:20+00:00
- **Authors**: Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng, Ben Y. Zhao
- **Comment**: None
- **Journal**: USENIX Security Symposium 2020
- **Summary**: Today's proliferation of powerful facial recognition systems poses a real threat to personal privacy. As Clearview.ai demonstrated, anyone can canvas the Internet for data and train highly accurate facial recognition models of individuals without their knowledge. We need tools to protect ourselves from potential misuses of unauthorized facial recognition systems. Unfortunately, no practical or effective solutions exist.   In this paper, we propose Fawkes, a system that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes (we call them "cloaks") to their own photos before releasing them. When used to train facial recognition models, these "cloaked" images produce functional models that consistently cause normal images of the user to be misidentified. We experimentally demonstrate that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. Even when clean, uncloaked images are "leaked" to the tracker and used for training, Fawkes can still maintain an 80+% protection success rate. We achieve 100% success in experiments against today's state-of-the-art facial recognition services. Finally, we show that Fawkes is robust against a variety of countermeasures that try to detect or disrupt image cloaks.



### Towards a Complete Pipeline for Segmenting Nuclei in Feulgen-Stained Images
- **Arxiv ID**: http://arxiv.org/abs/2002.08331v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08331v1)
- **Published**: 2020-02-19 18:14:57+00:00
- **Updated**: 2020-02-19 18:14:57+00:00
- **Authors**: Luiz Antonio Buschetto Macarini, Aldo von Wangenheim, Felipe Perozzo Dalto√©, Alexandre Sherlley Casimiro Onofre, Fabiana Botelho de Miranda Onofre, Marcelo Ricardo Stemmer
- **Comment**: 7 pages, 8 figures (Figure 2 with "a" and "b"), conference paper
  accepted for presentation in XI Computer on the Beach
  (https://www.computeronthebeach.com.br/)
- **Journal**: None
- **Summary**: Cervical cancer is the second most common cancer type in women around the world. In some countries, due to non-existent or inadequate screening, it is often detected at late stages, making standard treatment options often absent or unaffordable. It is a deadly disease that could benefit from early detection approaches. It is usually done by cytological exams which consist of visually inspecting the nuclei searching for morphological alteration. Since it is done by humans, naturally, some subjectivity is introduced. Computational methods could be used to reduce this, where the first stage of the process would be the nuclei segmentation. In this context, we present a complete pipeline for the segmentation of nuclei in Feulgen-stained images using Convolutional Neural Networks. Here we show the entire process of segmentation, since the collection of the samples, passing through pre-processing, training the network, post-processing and results evaluation. We achieved an overall IoU of 0.78, showing the affordability of the approach of nuclei segmentation on Feulgen-stained images. The code is available in: https://github.com/luizbuschetto/feulgen_nuclei_segmentation.



### SummaryNet: A Multi-Stage Deep Learning Model for Automatic Video Summarisation
- **Arxiv ID**: http://arxiv.org/abs/2002.09424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09424v1)
- **Published**: 2020-02-19 18:24:35+00:00
- **Updated**: 2020-02-19 18:24:35+00:00
- **Authors**: Ziyad Jappie, David Torpey, Turgay Celik
- **Comment**: None
- **Journal**: None
- **Summary**: Video summarisation can be posed as the task of extracting important parts of a video in order to create an informative summary of what occurred in the video. In this paper we introduce SummaryNet as a supervised learning framework for automated video summarisation. SummaryNet employs a two-stream convolutional network to learn spatial (appearance) and temporal (motion) representations. It utilizes an encoder-decoder model to extract the most salient features from the learned video representations. Lastly, it uses a sigmoid regression network with bidirectional long short-term memory cells to predict the probability of a frame being a summary frame. Experimental results on benchmark datasets show that the proposed method achieves comparable or significantly better results than the state-of-the-art video summarisation methods.



### Extracting Semantic Indoor Maps from Occupancy Grids
- **Arxiv ID**: http://arxiv.org/abs/2002.08348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.08348v1)
- **Published**: 2020-02-19 18:52:27+00:00
- **Updated**: 2020-02-19 18:52:27+00:00
- **Authors**: Ziyuan Liu, Georg von Wichert
- **Comment**: None
- **Journal**: None
- **Summary**: The primary challenge for any autonomous system operating in realistic, rather unconstrained scenarios is to manage the complexity and uncertainty of the real world. While it is unclear how exactly humans and other higher animals master these problems, it seems evident, that abstraction plays an important role. The use of abstract concepts allows to define the system behavior on higher levels. In this paper we focus on the semantic mapping of indoor environments. We propose a method to extract an abstracted floor plan from typical grid maps using Bayesian reasoning. The result of this procedure is a probabilistic generative model of the environment defined over abstract concepts. It is well suited for higher-level reasoning and communication purposes. We demonstrate the effectiveness of the approach using real-world data.



### Rethinking the Hyperparameters for Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/2002.11770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.11770v1)
- **Published**: 2020-02-19 18:59:52+00:00
- **Updated**: 2020-02-19 18:59:52+00:00
- **Authors**: Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous theoretical findings. (2) Optimal hyperparameters for fine-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for "dissimilar" datasets. Our findings challenge common practices of fine-tuning and encourages deep learning practitioners to rethink the hyperparameters for fine-tuning.



### MonoLayout: Amodal scene layout from a single image
- **Arxiv ID**: http://arxiv.org/abs/2002.08394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.08394v1)
- **Published**: 2020-02-19 19:16:34+00:00
- **Updated**: 2020-02-19 19:16:34+00:00
- **Authors**: Kaustubh Mani, Swapnil Daga, Shubhika Garg, N. Sai Shankar, Krishna Murthy Jatavallabhula, K. Madhava Krishna
- **Comment**: To be presented at WACV 2020 Video:
  https://www.youtube.com/watch?v=HcroGyo6yRQ Project page:
  https://hbutsuak95.github.io/monolayout
- **Journal**: None
- **Summary**: In this paper, we address the novel, highly challenging problem of estimating the layout of a complex urban driving scenario. Given a single color image captured from a driving platform, we aim to predict the bird's-eye view layout of the road and other traffic participants. The estimated layout should reason beyond what is visible in the image, and compensate for the loss of 3D information due to projection. We dub this problem amodal scene layout estimation, which involves "hallucinating" scene layout for even parts of the world that are occluded in the image. To this end, we present MonoLayout, a deep neural network for real-time amodal scene layout estimation from a single image. We represent scene layout as a multi-channel semantic occupancy grid, and leverage adversarial feature learning to hallucinate plausible completions for occluded image parts. Due to the lack of fair baseline methods, we extend several state-of-the-art approaches for road-layout estimation and vehicle occupancy estimation in bird's-eye view to the amodal setup for rigorous evaluation. By leveraging temporal sensor fusion to generate training labels, we significantly outperform current art over a number of datasets. On the KITTI and Argoverse datasets, we outperform all baselines by a significant margin. We also make all our annotations, and code publicly available. A video abstract of this paper is available https://www.youtube.com/watch?v=HcroGyo6yRQ .



### JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset
- **Arxiv ID**: http://arxiv.org/abs/2002.08397v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.08397v4)
- **Published**: 2020-02-19 19:21:33+00:00
- **Updated**: 2020-07-22 07:29:39+00:00
- **Authors**: Abhijeet Shenoi, Mihir Patel, JunYoung Gwak, Patrick Goebel, Amir Sadeghian, Hamid Rezatofighi, Roberto Mart√≠n-Mart√≠n, Silvio Savarese
- **Comment**: 8 pages, 5 figures, 2 tables; Accepted at IROS 2020
- **Journal**: None
- **Summary**: Robots navigating autonomously need to perceive and track the motion of objects and other agents in its surroundings. This information enables planning and executing robust and safe trajectories. To facilitate these processes, the motion should be perceived in 3D Cartesian space. However, most recent multi-object tracking (MOT) research has focused on tracking people and moving objects in 2D RGB video sequences. In this work we present JRMOT, a novel 3D MOT system that integrates information from RGB images and 3D point clouds to achieve real-time, state-of-the-art tracking performance. Our system is built with recent neural networks for re-identification, 2D and 3D detection and track description, combined into a joint probabilistic data-association framework within a multi-modal recursive Kalman architecture. As part of our work, we release the JRDB dataset, a novel large scale 2D+3D dataset and benchmark, annotated with over 2 million boxes and 3500 time consistent 2D+3D trajectories across 54 indoor and outdoor scenes. JRDB contains over 60 minutes of data including 360 degree cylindrical RGB video and 3D pointclouds in social settings that we use to develop, train and evaluate JRMOT. The presented 3D MOT system demonstrates state-of-the-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as first 3D tracking solution for our benchmark. Real-robot tests on our social robot JackRabbot indicate that the system is capable of tracking multiple pedestrians fast and reliably. We provide the ROS code of our tracker at https://sites.google.com/view/jrmot.



### A Generalizable Knowledge Framework for Semantic Indoor Mapping Based on Markov Logic Networks and Data Driven MCMC
- **Arxiv ID**: http://arxiv.org/abs/2002.08402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.08402v1)
- **Published**: 2020-02-19 19:30:10+00:00
- **Updated**: 2020-02-19 19:30:10+00:00
- **Authors**: Ziyuan Liu, Georg von Wichert
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a generalizable knowledge framework for data abstraction, i.e. finding compact abstract model for input data using predefined abstract terms. Based on these abstract terms, intelligent autonomous systems, such as a robot, should be able to make inference according to specific knowledge base, so that they can better handle the complexity and uncertainty of the real world. We propose to realize this framework by combining Markov logic networks (MLNs) and data driven MCMC sampling, because the former are a powerful tool for modelling uncertain knowledge and the latter provides an efficient way to draw samples from unknown complex distributions. Furthermore, we show in detail how to adapt this framework to a certain task, in particular, semantic robot mapping. Based on MLNs, we formulate task-specific context knowledge as descriptive soft rules. Experiments on real world data and simulated data confirm the usefulness of our framework.



### T-Net: Learning Feature Representation with Task-specific Supervision for Biomedical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.08406v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08406v2)
- **Published**: 2020-02-19 19:38:28+00:00
- **Updated**: 2021-01-09 08:28:27+00:00
- **Authors**: Weinan Song, Yuan Liang, Jiawei Yang, Kun Wang, Lei He
- **Comment**: None
- **Journal**: None
- **Summary**: The encoder-decoder network is widely used to learn deep feature representations from pixel-wise annotations in biomedical image analysis. Under this structure, the performance profoundly relies on the effectiveness of feature extraction achieved by the encoding network. However, few models have considered adapting the attention of the feature extractor even in different kinds of tasks. In this paper, we propose a novel training strategy by adapting the attention of the feature extractor according to different tasks for effective representation learning. Specifically, the framework, named T-Net, consists of an encoding network supervised by task-specific attention maps and a posterior network that takes in the learned features to predict the corresponding results. The attention map is obtained by the transformation from pixel-wise annotations according to the specific task, which is used as the supervision to regularize the feature extractor to focus on different locations of the recognition object. To show the effectiveness of our method, we evaluate T-Net on two different tasks, i.e. , segmentation and localization. Extensive results on three public datasets (BraTS-17, MoNuSeg and IDRiD) have indicated the effectiveness and efficiency of our proposed supervision method, especially over the conventional encoding-decoding network.



### Table-Top Scene Analysis Using Knowledge-Supervised MCMC
- **Arxiv ID**: http://arxiv.org/abs/2002.08417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.08417v1)
- **Published**: 2020-02-19 20:10:38+00:00
- **Updated**: 2020-02-19 20:10:38+00:00
- **Authors**: Ziyuan Liu, Dong Chen, Kai M. Wurm, Georg von Wichert
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a probabilistic method to generate abstract scene graphs for table-top scenes from 6D object pose estimates. We explicitly make use of task-specfic context knowledge by encoding this knowledge as descriptive rules in Markov logic networks. Our approach to generate scene graphs is probabilistic: Uncertainty in the object poses is addressed by a probabilistic sensor model that is embedded in a data driven MCMC process. We apply Markov logic inference to reason about hidden objects and to detect false estimates of object poses. The effectiveness of our approach is demonstrated and evaluated in real world experiments.



### Interactive Natural Language-based Person Search
- **Arxiv ID**: http://arxiv.org/abs/2002.08434v1
- **DOI**: 10.1109/LRA.2020.2969921
- **Categories**: **cs.RO**, cs.CL, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2002.08434v1)
- **Published**: 2020-02-19 20:42:19+00:00
- **Updated**: 2020-02-19 20:42:19+00:00
- **Authors**: Vikram Shree, Wei-Lun Chao, Mark Campbell
- **Comment**: 8 pages, 12 figures, Published in IEEE Robotics and Automation
  Letters (RA-L), "Dataset at:
  https://github.com/vikshree/QA_PersonSearchLanguageData" , Video attachment
  at: https://www.youtube.com/watch?v=Yyxu8uVUREE&feature=youtu.be
- **Journal**: in IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.
  1851-1858, April 2020
- **Summary**: In this work, we consider the problem of searching people in an unconstrained environment, with natural language descriptions. Specifically, we study how to systematically design an algorithm to effectively acquire descriptions from humans. An algorithm is proposed by adapting models, used for visual and language understanding, to search a person of interest (POI) in a principled way, achieving promising results without the need to re-design another complicated model. We then investigate an iterative question-answering (QA) strategy that enable robots to request additional information about the POI's appearance from the user. To this end, we introduce a greedy algorithm to rank questions in terms of their significance, and equip the algorithm with the capability to dynamically adjust the length of human-robot interaction according to model's uncertainty. Our approach is validated not only on benchmark datasets but on a mobile robot, moving in a dynamic and crowded environment.



### Fine tuning U-Net for ultrasound image segmentation: which layers?
- **Arxiv ID**: http://arxiv.org/abs/2002.08438v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.08438v1)
- **Published**: 2020-02-19 20:45:40+00:00
- **Updated**: 2020-02-19 20:45:40+00:00
- **Authors**: Mina Amiri, Rupert Brooks, Hassan Rivaz
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-tuning a network which has been trained on a large dataset is an alternative to full training in order to overcome the problem of scarce and expensive data in medical applications. While the shallow layers of the network are usually kept unchanged, deeper layers are modified according to the new dataset. This approach may not work for ultrasound images due to their drastically different appearance. In this study, we investigated the effect of fine-tuning different layers of a U-Net which was trained on segmentation of natural images in breast ultrasound image segmentation. Tuning the contracting part and fixing the expanding part resulted in substantially better results compared to fixing the contracting part and tuning the expanding part. Furthermore, we showed that starting to fine-tune the U-Net from the shallow layers and gradually including more layers will lead to a better performance compared to fine-tuning the network from the deep layers moving back to shallow layers. We did not observe the same results on segmentation of X-ray images, which have different salient features compared to ultrasound, it may therefore be more appropriate to fine-tune the shallow layers rather than deep layers. Shallow layers learn lower level features (including speckle pattern, and probably the noise and artifact properties) which are critical in automatic segmentation in this modality.



### AdvMS: A Multi-source Multi-cost Defense Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2002.08439v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08439v1)
- **Published**: 2020-02-19 20:46:54+00:00
- **Updated**: 2020-02-19 20:46:54+00:00
- **Authors**: Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, Peter Chin
- **Comment**: Accepted by 45th International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2020)
- **Journal**: None
- **Summary**: Designing effective defense against adversarial attacks is a crucial topic as deep neural networks have been proliferated rapidly in many security-critical domains such as malware detection and self-driving cars. Conventional defense methods, although shown to be promising, are largely limited by their single-source single-cost nature: The robustness promotion tends to plateau when the defenses are made increasingly stronger while the cost tends to amplify. In this paper, we study principles of designing multi-source and multi-cost schemes where defense performance is boosted from multiple defending components. Based on this motivation, we propose a multi-source and multi-cost defense scheme, Adversarially Trained Model Switching (AdvMS), that inherits advantages from two leading schemes: adversarial training and random model switching. We show that the multi-source nature of AdvMS mitigates the performance plateauing issue and the multi-cost nature enables improving robustness at a flexible and adjustable combination of costs over different factors which can better suit specific restrictions and needs in practice.



### Cooperative LIDAR Object Detection via Feature Sharing in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.08440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08440v1)
- **Published**: 2020-02-19 20:47:09+00:00
- **Updated**: 2020-02-19 20:47:09+00:00
- **Authors**: Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser P. Fallah, Rui Guo, HongSheng Lu
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: The recent advancements in communication and computational systems has led to significant improvement of situational awareness in connected and autonomous vehicles. Computationally efficient neural networks and high speed wireless vehicular networks have been some of the main contributors to this improvement. However, scalability and reliability issues caused by inherent limitations of sensory and communication systems are still challenging problems. In this paper, we aim to mitigate the effects of these limitations by introducing the concept of feature sharing for cooperative object detection (FS-COD). In our proposed approach, a better understanding of the environment is achieved by sharing partially processed data between cooperative vehicles while maintaining a balance between computation and communication load. This approach is different from current methods of map sharing, or sharing of raw data which are not scalable. The performance of the proposed approach is verified through experiments on Volony dataset. It is shown that the proposed approach has significant performance superiority over the conventional single-vehicle object detection approaches.



### SD-GAN: Structural and Denoising GAN reveals facial parts under occlusion
- **Arxiv ID**: http://arxiv.org/abs/2002.08448v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08448v1)
- **Published**: 2020-02-19 21:12:49+00:00
- **Updated**: 2020-02-19 21:12:49+00:00
- **Authors**: Samik Banerjee, Sukhendu Das
- **Comment**: Recommended for revision in Neurocomputing, Elsevier
- **Journal**: None
- **Summary**: Certain facial parts are salient (unique) in appearance, which substantially contribute to the holistic recognition of a subject. Occlusion of these salient parts deteriorates the performance of face recognition algorithms. In this paper, we propose a generative model to reconstruct the missing parts of the face which are under occlusion. The proposed generative model (SD-GAN) reconstructs a face preserving the illumination variation and identity of the face. A novel adversarial training algorithm has been designed for a bimodal mutually exclusive Generative Adversarial Network (GAN) model, for faster convergence. A novel adversarial "structural" loss function is also proposed, comprising of two components: a holistic and a local loss, characterized by SSIM and patch-wise MSE. Ablation studies on real and synthetically occluded face datasets reveal that our proposed technique outperforms the competing methods by a considerable margin, even for boosting the performance of Face Recognition.



### MADAN: Multi-source Adversarial Domain Aggregation Network for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2003.00820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2003.00820v1)
- **Published**: 2020-02-19 21:22:00+00:00
- **Updated**: 2020-02-19 21:22:00+00:00
- **Authors**: Sicheng Zhao, Bo Li, Xiangyu Yue, Pengfei Xu, Kurt Keutzer
- **Comment**: Extension of our previous NeurIPS 2019 paper arXiv:1910.12181 on
  multi-source domain adaptation
- **Journal**: None
- **Summary**: Domain adaptation aims to learn a transferable model to bridge the domain shift between one labeled source domain and another sparsely labeled or unlabeled target domain. Since the labeled data may be collected from multiple sources, multi-source domain adaptation (MDA) has attracted increasing attention. Recent MDA methods do not consider the pixel-level alignment between sources and target or the misalignment across different sources. In this paper, we propose a novel MDA framework to address these challenges. Specifically, we design an end-to-end Multi-source Adversarial Domain Aggregation Network (MADAN). First, an adapted domain is generated for each source with dynamic semantic consistency while aligning towards the target at the pixel-level cycle-consistently. Second, sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and the target domain while training the task network. For the segmentation adaptation, we further enforce category-level alignment and incorporate context-aware generation, which constitutes MADAN+. We conduct extensive MDA experiments on digit recognition, object classification, and simulation-to-real semantic segmentation. The results demonstrate that the proposed MADAN and MANDA+ models outperform state-of-the-art approaches by a large margin.



### Revisiting Training Strategies and Generalization Performance in Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.08473v9
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08473v9)
- **Published**: 2020-02-19 22:16:12+00:00
- **Updated**: 2020-08-01 16:14:33+00:00
- **Authors**: Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bj√∂rn Ommer, Joseph Paul Cohen
- **Comment**: ICML 2020. Main paper 8.25 pages, 26 pages total
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets. Code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch.



### Modelling response to trypophobia trigger using intermediate layers of ImageNet networks
- **Arxiv ID**: http://arxiv.org/abs/2002.08490v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08490v2)
- **Published**: 2020-02-19 22:58:58+00:00
- **Updated**: 2020-02-26 01:05:23+00:00
- **Authors**: Piotr Wo≈∫nicki, Micha≈Ç Ku≈∫ba, Piotr Migda≈Ç
- **Comment**: extended abstract submitted to Eastern European Machine Learning
  2019, 3 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: In this paper, we approach the problem of detecting trypophobia triggers using Convolutional neural networks. We show that standard architectures such as VGG or ResNet are capable of recognizing trypophobia patterns. We also conduct experiments to analyze the nature of this phenomenon. To do that, we dissect the network decreasing the number of its layers and parameters. We prove, that even significantly reduced networks have accuracy above 91% and focus their attention on the trypophobia patterns as presented on the visual explanations.




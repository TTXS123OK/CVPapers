# Arxiv Papers in cs.CV on 2020-02-20
### Expressing Objects just like Words: Recurrent Visual Embedding for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2002.08510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08510v1)
- **Published**: 2020-02-20 00:51:01+00:00
- **Updated**: 2020-02-20 00:51:01+00:00
- **Authors**: Tianlang Chen, Jiebo Luo
- **Comment**: Accepted by AAAI-20
- **Journal**: None
- **Summary**: Existing image-text matching approaches typically infer the similarity of an image-text pair by capturing and aggregating the affinities between the text and each independent object of the image. However, they ignore the connections between the objects that are semantically related. These objects may collectively determine whether the image corresponds to a text or not. To address this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN) which processes images and sentences symmetrically by recurrent neural networks (RNN). In particular, given an input image-text pair, our model reorders the image objects based on the positions of their most related words in the text. In the same way as extracting the hidden features from word embeddings, the model leverages RNN to extract high-level object features from the reordered object inputs. We validate that the high-level object features contain useful joint information of semantically related objects, which benefit the retrieval task. To compute the image-text similarity, we incorporate a Multi-attention Cross Matching Model into DP-RNN. It aggregates the affinity between objects and words with cross-modality guided attention and self-attention. Our model achieves the state-of-the-art performance on Flickr30K dataset and competitive performance on MS-COCO dataset. Extensive experiments demonstrate the effectiveness of our model.



### Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2002.08546v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.08546v6)
- **Published**: 2020-02-20 03:13:58+00:00
- **Updated**: 2021-06-01 09:06:00+00:00
- **Authors**: Jian Liang, Dapeng Hu, Jiashi Feng
- **Comment**: ICML2020. Fix the typos for Digits. Code is available at
  https://github.com/tim-learn/SHOT
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.



### Deep Fusion of Local and Non-Local Features for Precision Landslide Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.08547v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08547v1)
- **Published**: 2020-02-20 03:18:59+00:00
- **Updated**: 2020-02-20 03:18:59+00:00
- **Authors**: Qing Zhu, Lin Chen, Han Hu, Binzhi Xu, Yeting Zhang, Haifeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Precision mapping of landslide inventory is crucial for hazard mitigation. Most landslides generally co-exist with other confusing geological features, and the presence of such areas can only be inferred unambiguously at a large scale. In addition, local information is also important for the preservation of object boundaries. Aiming to solve this problem, this paper proposes an effective approach to fuse both local and non-local features to surmount the contextual problem. Built upon the U-Net architecture that is widely adopted in the remote sensing community, we utilize two additional modules. The first one uses dilated convolution and the corresponding atrous spatial pyramid pooling, which enlarged the receptive field without sacrificing spatial resolution or increasing memory usage. The second uses a scale attention mechanism to guide the up-sampling of features from the coarse level by a learned weight map. In implementation, the computational overhead against the original U-Net was only a few convolutional layers. Experimental evaluations revealed that the proposed method outperformed state-of-the-art general-purpose semantic segmentation approaches. Furthermore, ablation studies have shown that the two models afforded extensive enhancements in landslide-recognition performance.



### Fast and Regularized Reconstruction of Building Façades from Street-View Images using Binary Integer Programming
- **Arxiv ID**: http://arxiv.org/abs/2002.08549v3
- **DOI**: 10.5194/isprs-annals-V-2-2020-365-2020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08549v3)
- **Published**: 2020-02-20 03:29:52+00:00
- **Updated**: 2020-04-22 10:21:39+00:00
- **Authors**: Han Hu, Libin Wang, Mier Zhang, Yulin Ding, Qing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Regularized arrangement of primitives on building fa\c{c}ades to aligned locations and consistent sizes is important towards structured reconstruction of urban environment. Mixed integer linear programing was used to solve the problem, however, it is extreamly time consuming even for state-of-the-art commercial solvers. Aiming to alleviate this issue, we cast the problem into binary integer programming, which omits the requirements for real value parameters and is more efficient to be solved . Firstly, the bounding boxes of the primitives are detected using the YOLOv3 architecture in real-time. Secondly, the coordinates of the upper left corners and the sizes of the bounding boxes are automatically clustered in a binary integer programming optimization, which jointly considers the geometric fitness, regularity and additional constraints; this step does not require \emph{a priori} knowledge, such as the number of clusters or pre-defined grammars. Finally, the regularized bounding boxes can be directly used to guide the fa\c{c}ade reconstruction in an interactive envinronment. Experimental evaluations have revealed that the accuracies for the extraction of primitives are above 0.82, which is sufficient for the following 3D reconstruction. The proposed approach only takes about $ 10\% $ to $ 20\% $ of the runtime than previous approach and reduces the diversity of the bounding boxes to about $20\%$ to $50\%$.



### Learning Object Scale With Click Supervision for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2002.08555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08555v1)
- **Published**: 2020-02-20 03:59:46+00:00
- **Updated**: 2020-02-20 03:59:46+00:00
- **Authors**: Liao Zhang, Yan Yan, Lin Cheng, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly-supervised object detection has recently attracted increasing attention since it only requires image-levelannotations. However, the performance obtained by existingmethods is still far from being satisfactory compared with fully-supervised object detection methods. To achieve a good trade-off between annotation cost and object detection performance,we propose a simple yet effective method which incorporatesCNN visualization with click supervision to generate the pseudoground-truths (i.e., bounding boxes). These pseudo ground-truthscan be used to train a fully-supervised detector. To estimatethe object scale, we firstly adopt a proposal selection algorithmto preserve high-quality proposals, and then generate ClassActivation Maps (CAMs) for these preserved proposals by theproposed CNN visualization algorithm called Spatial AttentionCAM. Finally, we fuse these CAMs together to generate pseudoground-truths and train a fully-supervised object detector withthese ground-truths. Experimental results on the PASCAL VOC2007 and VOC 2012 datasets show that the proposed methodcan obtain much higher accuracy for estimating the object scale,compared with the state-of-the-art image-level based methodsand the center-click based method



### Captioning Images Taken by People Who Are Blind
- **Arxiv ID**: http://arxiv.org/abs/2002.08565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08565v2)
- **Published**: 2020-02-20 04:36:39+00:00
- **Updated**: 2020-07-15 15:48:35+00:00
- **Authors**: Danna Gurari, Yinan Zhao, Meng Zhang, Nilavra Bhattacharya
- **Comment**: None
- **Journal**: None
- **Summary**: While an important problem in the vision community is to design algorithms that can automatically caption images, few publicly-available datasets for algorithm development directly address the interests of real users. Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of over 39,000 images originating from people who are blind that are each paired with five captions. We analyze this dataset to (1) characterize the typical captions, (2) characterize the diversity of content found in the images, and (3) compare its content to that found in eight popular vision datasets. We also analyze modern image captioning algorithms to identify what makes this new dataset challenging for the vision community. We publicly-share the dataset with captioning challenge instructions at https://vizwiz.org



### Cross-stained Segmentation from Renal Biopsy Images Using Multi-level Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.08587v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08587v1)
- **Published**: 2020-02-20 06:49:48+00:00
- **Updated**: 2020-02-20 06:49:48+00:00
- **Authors**: Ke Mei, Chuang Zhu, Lei Jiang, Jun Liu, Yuanyuan Qiao
- **Comment**: Accepted by ICASSP2020
- **Journal**: None
- **Summary**: Segmentation from renal pathological images is a key step in automatic analyzing the renal histological characteristics. However, the performance of models varies significantly in different types of stained datasets due to the appearance variations. In this paper, we design a robust and flexible model for cross-stained segmentation. It is a novel multi-level deep adversarial network architecture that consists of three sub-networks: (i) a segmentation network; (ii) a pair of multi-level mirrored discriminators for guiding the segmentation network to extract domain-invariant features; (iii) a shape discriminator that is utilized to further identify the output of the segmentation network and the ground truth. Experimental results on glomeruli segmentation from renal biopsy images indicate that our network is able to improve segmentation performance on target type of stained images and use unlabeled data to achieve similar accuracy to labeled data. In addition, this method can be easily applied to other tasks.



### KaoKore: A Pre-modern Japanese Art Facial Expression Dataset
- **Arxiv ID**: http://arxiv.org/abs/2002.08595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08595v1)
- **Published**: 2020-02-20 07:22:13+00:00
- **Updated**: 2020-02-20 07:22:13+00:00
- **Authors**: Yingtao Tian, Chikahiko Suzuki, Tarin Clanuwat, Mikel Bober-Irizar, Alex Lamb, Asanobu Kitamoto
- **Comment**: None
- **Journal**: None
- **Summary**: From classifying handwritten digits to generating strings of text, the datasets which have received long-time focus from the machine learning community vary greatly in their subject matter. This has motivated a renewed interest in building datasets which are socially and culturally relevant, so that algorithmic research may have a more direct and immediate impact on society. One such area is in history and the humanities, where better and relevant machine learning models can accelerate research across various fields. To this end, newly released benchmarks and models have been proposed for transcribing historical Japanese cursive writing, yet for the field as a whole using machine learning for historical Japanese artworks still remains largely uncharted. To bridge this gap, in this work we propose a new dataset KaoKore which consists of faces extracted from pre-modern Japanese artwork. We demonstrate its value as both a dataset for image classification as well as a creative and artistic dataset, which we explore using generative models. Dataset available at https://github.com/rois-codh/kaokore



### Domain Adaptive Adversarial Learning Based on Physics Model Feedback for Underwater Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2002.09315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09315v1)
- **Published**: 2020-02-20 07:50:00+00:00
- **Updated**: 2020-02-20 07:50:00+00:00
- **Authors**: Yuan Zhou, Kangming Yan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1808.00605,
  arXiv:1807.03528 by other authors
- **Journal**: None
- **Summary**: Owing to refraction, absorption, and scattering of light by suspended particles in water, raw underwater images suffer from low contrast, blurred details, and color distortion. These characteristics can significantly interfere with the visibility of underwater images and the result of visual tasks, such as segmentation and tracking. To address this problem, we propose a new robust adversarial learning framework via physics model based feedback control and domain adaptation mechanism for enhancing underwater images to get realistic results. A new method for simulating underwater-like training dataset from RGB-D data by underwater image formation model is proposed. Upon the synthetic dataset, a novel enhancement framework, which introduces a domain adaptive mechanism as well as a physics model constraint feedback control, is trained to enhance the underwater scenes. Final enhanced results on synthetic and real underwater images demonstrate the superiority of the proposed method, which outperforms nondeep and deep learning methods in both qualitative and quantitative evaluations. Furthermore, we perform an ablation study to show the contributions of each component we proposed.



### Boosting Adversarial Training with Hypersphere Embedding
- **Arxiv ID**: http://arxiv.org/abs/2002.08619v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08619v3)
- **Published**: 2020-02-20 08:42:29+00:00
- **Updated**: 2020-11-25 16:18:38+00:00
- **Authors**: Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, Hang Su
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Adversarial training (AT) is one of the most effective defenses against adversarial attacks for deep learning models. In this work, we advocate incorporating the hypersphere embedding (HE) mechanism into the AT procedure by regularizing the features onto compact manifolds, which constitutes a lightweight yet effective module to blend in the strength of representation learning. Our extensive analyses reveal that AT and HE are well coupled to benefit the robustness of the adversarially trained models from several aspects. We validate the effectiveness and adaptability of HE by embedding it into the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as the FreeAT and FastAT strategies. In the experiments, we evaluate our methods under a wide range of adversarial attacks on the CIFAR-10 and ImageNet datasets, which verifies that integrating HE can consistently enhance the model robustness for each AT framework with little extra computation.



### Focus on Semantic Consistency for Cross-domain Crowd Understanding
- **Arxiv ID**: http://arxiv.org/abs/2002.08623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08623v1)
- **Published**: 2020-02-20 08:51:05+00:00
- **Updated**: 2020-02-20 08:51:05+00:00
- **Authors**: Tao Han, Junyu Gao, Yuan Yuan, Qi Wang
- **Comment**: Accpeted by ICASSP2020
- **Journal**: None
- **Summary**: For pixel-level crowd understanding, it is time-consuming and laborious in data collection and annotation. Some domain adaptation algorithms try to liberate it by training models with synthetic data, and the results in some recent works have proved the feasibility. However, we found that a mass of estimation errors in the background areas impede the performance of the existing methods. In this paper, we propose a domain adaptation method to eliminate it. According to the semantic consistency, a similar distribution in deep layer's features of the synthetic and real-world crowd area, we first introduce a semantic extractor to effectively distinguish crowd and background in high-level semantic information. Besides, to further enhance the adapted model, we adopt adversarial learning to align features in the semantic space. Experiments on three representative real datasets show that the proposed domain adaptation scheme achieves the state-of-the-art for cross-domain counting problems.



### A Novel Framework for Selection of GANs for an Application
- **Arxiv ID**: http://arxiv.org/abs/2002.08641v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08641v2)
- **Published**: 2020-02-20 09:51:48+00:00
- **Updated**: 2021-05-17 09:48:42+00:00
- **Authors**: Tanya Motwani, Manojkumar Parmar
- **Comment**: 11 pages, 1 figure, 8 tables
- **Journal**: None
- **Summary**: Generative Adversarial Network (GAN) is a current focal point of research. The body of knowledge is fragmented, leading to a trial-error method while selecting an appropriate GAN for a given scenario. We provide a comprehensive summary of the evolution of GANs starting from its inception addressing issues like mode collapse, vanishing gradient, unstable training and non-convergence. We also provide a comparison of various GANs from the application point of view, its behaviour and implementation details. We propose a novel framework to identify candidate GANs for a specific use case based on architecture, loss, regularization and divergence. We also discuss application of the framework using an example, and we demonstrate a significant reduction in search space. This efficient way to determine potential GANs lowers unit economics of AI development for organizations.



### A Convolutional Baseline for Person Re-Identification Using Vision and Language Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2003.00808v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2003.00808v1)
- **Published**: 2020-02-20 10:12:02+00:00
- **Updated**: 2020-02-20 10:12:02+00:00
- **Authors**: Ammarah Farooq, Muhammad Awais, Fei Yan, Josef Kittler, Ali Akbari, Syed Safwan Khalid
- **Comment**: 12 pages including references, currently under review in IEEE
  transactions on Image Processing
- **Journal**: None
- **Summary**: Classical person re-identification approaches assume that a person of interest has appeared across different cameras and can be queried by one of the existing images. However, in real-world surveillance scenarios, frequently no visual information will be available about the queried person. In such scenarios, a natural language description of the person by a witness will provide the only source of information for retrieval. In this work, person re-identification using both vision and language information is addressed under all possible gallery and query scenarios. A two stream deep convolutional neural network framework supervised by cross entropy loss is presented. The weights connecting the second last layer to the last layer with class probabilities, i.e., logits of softmax layer are shared in both networks. Canonical Correlation Analysis is performed to enhance the correlation between the two modalities in a joint latent embedding space. To investigate the benefits of the proposed approach, a new testing protocol under a multi modal ReID setting is proposed for the test split of the CUHK-PEDES and CUHK-SYSU benchmarks. The experimental results verify the merits of the proposed system. The learnt visual representations are more robust and perform 22\% better during retrieval as compared to a single modality system. The retrieval with a multi modal query greatly enhances the re-identification capability of the system quantitatively as well as qualitatively.



### Stroke Constrained Attention Network for Online Handwritten Mathematical Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.08670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08670v1)
- **Published**: 2020-02-20 11:01:12+00:00
- **Updated**: 2020-02-20 11:01:12+00:00
- **Authors**: Jiaming Wang, Jun Du, Jianshu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel stroke constrained attention network (SCAN) which treats stroke as the basic unit for encoder-decoder based online handwritten mathematical expression recognition (HMER). Unlike previous methods which use trace points or image pixels as basic units, SCAN makes full use of stroke-level information for better alignment and representation. The proposed SCAN can be adopted in both single-modal (online or offline) and multi-modal HMER. For single-modal HMER, SCAN first employs a CNN-GRU encoder to extract point-level features from input traces in online mode and employs a CNN encoder to extract pixel-level features from input images in offline mode, then use stroke constrained information to convert them into online and offline stroke-level features. Using stroke-level features can explicitly group points or pixels belonging to the same stroke, therefore reduces the difficulty of symbol segmentation and recognition via the decoder with attention mechanism. For multi-modal HMER, other than fusing multi-modal information in decoder, SCAN can also fuse multi-modal information in encoder by utilizing the stroke based alignments between online and offline modalities. The encoder fusion is a better way for combining multi-modal information as it implements the information interaction one step before the decoder fusion so that the advantages of multiple modalities can be exploited earlier and more adequately when training the encoder-decoder model. Evaluated on a benchmark published by CROHME competition, the proposed SCAN achieves the state-of-the-art performance.



### Unsupervised Domain Adaptation via Discriminative Manifold Embedding and Alignment
- **Arxiv ID**: http://arxiv.org/abs/2002.08675v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08675v2)
- **Published**: 2020-02-20 11:06:41+00:00
- **Updated**: 2020-02-28 16:36:53+00:00
- **Authors**: You-Wei Luo, Chuan-Xian Ren, Pengfei Ge, Ke-Kun Huang, Yu-Feng Yu
- **Comment**: Accepted to AAAI 2020. Code available:
  \<https://github.com/LavieLuo/DRMEA>
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is effective in leveraging the rich information from the source domain to the unsupervised target domain. Though deep learning and adversarial strategy make an important breakthrough in the adaptability of features, there are two issues to be further explored. First, the hard-assigned pseudo labels on the target domain are risky to the intrinsic data structure. Second, the batch-wise training manner in deep learning limits the description of the global structure. In this paper, a Riemannian manifold learning framework is proposed to achieve transferability and discriminability consistently. As to the first problem, this method establishes a probabilistic discriminant criterion on the target domain via soft labels. Further, this criterion is extended to a global approximation scheme for the second issue; such approximation is also memory-saving. The manifold metric alignment is exploited to be compatible with the embedding space. A theoretical error bound is derived to facilitate the alignment. Extensive experiments have been conducted to investigate the proposal and results of the comparison study manifest the superiority of consistent manifold learning framework.



### Neural Network Compression Framework for fast model inference
- **Arxiv ID**: http://arxiv.org/abs/2002.08679v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T01, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2002.08679v4)
- **Published**: 2020-02-20 11:24:01+00:00
- **Updated**: 2020-12-30 08:17:23+00:00
- **Authors**: Alexander Kozlov, Ivan Lazarevich, Vasily Shamporov, Nikolay Lyalyushkin, Yury Gorbachev
- **Comment**: 13 pages, 1 figure
- **Journal**: None
- **Summary**: In this work we present a new framework for neural networks compression with fine-tuning, which we called Neural Network Compression Framework (NNCF). It leverages recent advances of various network compression methods and implements some of them, such as sparsity, quantization, and binarization. These methods allow getting more hardware-friendly models which can be efficiently run on general-purpose hardware computation units (CPU, GPU) or special Deep Learning accelerators. We show that the developed methods can be successfully applied to a wide range of models to accelerate the inference time while keeping the original accuracy. The framework can be used within the training samples, which are supplied with it, or as a standalone package that can be seamlessly integrated into the existing training code with minimal adaptations. Currently, a PyTorch version of NNCF is available as a part of OpenVINO Training Extensions at https://github.com/openvinotoolkit/nncf.



### Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice
- **Arxiv ID**: http://arxiv.org/abs/2002.08681v2
- **DOI**: 10.1109/TPAMI.2020.3036956
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08681v2)
- **Published**: 2020-02-20 11:26:45+00:00
- **Updated**: 2020-11-22 09:36:34+00:00
- **Authors**: Yabin Zhang, Bin Deng, Hui Tang, Lei Zhang, Kui Jia
- **Comment**: CVPR extension; TPAMI camera ready version:
  https://ieeexplore.ieee.org/document/9253700; IEEE copyright; Codes are
  available at: https://github.com/YBZh/MultiClassDA
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI),10 November 2020
- **Summary**: In this paper, we study the formalism of unsupervised multi-class domain adaptation (multi-class UDA), which underlies a few recent algorithms whose learning objectives are only motivated empirically. Multi-Class Scoring Disagreement (MCSD) divergence is presented by aggregating the absolute margin violations in multi-class classification, and this proposed MCSD is able to fully characterize the relations between any pair of multi-class scoring hypotheses. By using MCSD as a measure of domain distance, we develop a new domain adaptation bound for multi-class UDA; its data-dependent, probably approximately correct bound is also developed that naturally suggests adversarial learning objectives to align conditional feature distributions across source and target domains. Consequently, an algorithmic framework of Multi-class Domain-adversarial learning Networks (McDalNets) is developed, and its different instantiations via surrogate learning objectives either coincide with or resemble a few recently popular methods, thus (partially) underscoring their practical effectiveness. Based on our identical theory for multi-class UDA, we also introduce a new algorithm of Domain-Symmetric Networks (SymmNets), which is featured by a novel adversarial strategy of domain confusion and discrimination. SymmNets affords simple extensions that work equally well under the problem settings of either closed set, partial, or open set UDA. We conduct careful empirical studies to compare different algorithms of McDalNets and our newly introduced SymmNets. Experiments verify our theoretical analysis and show the efficacy of our proposed SymmNets. In addition, we have made our implementation code publicly available.



### An empirical study of Conv-TasNet
- **Arxiv ID**: http://arxiv.org/abs/2002.08688v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2002.08688v2)
- **Published**: 2020-02-20 11:51:43+00:00
- **Updated**: 2020-02-24 15:00:22+00:00
- **Authors**: Berkan Kadioglu, Michael Horgan, Xiaoyu Liu, Jordi Pons, Dan Darcy, Vivek Kumar
- **Comment**: In proceedings of ICASSP2020
- **Journal**: None
- **Summary**: Conv-TasNet is a recently proposed waveform-based deep neural network that achieves state-of-the-art performance in speech source separation. Its architecture consists of a learnable encoder/decoder and a separator that operates on top of this learned space. Various improvements have been proposed to Conv-TasNet. However, they mostly focus on the separator, leaving its encoder/decoder as a (shallow) linear operator. In this paper, we conduct an empirical study of Conv-TasNet and propose an enhancement to the encoder/decoder that is based on a (deep) non-linear variant of it. In addition, we experiment with the larger and more diverse LibriTTS dataset and investigate the generalization capabilities of the studied models when trained on a much larger dataset. We propose cross-dataset evaluation that includes assessing separations from the WSJ0-2mix, LibriTTS and VCTK databases. Our results show that enhancements to the encoder/decoder can improve average SI-SNR performance by more than 1 dB. Furthermore, we offer insights into the generalization capabilities of Conv-TasNet and the potential value of improvements to the encoder/decoder.



### Bi-directional Dermoscopic Feature Learning and Multi-scale Consistent Decision Fusion for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.08694v1
- **DOI**: 10.1109/TIP.2019.2955297
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08694v1)
- **Published**: 2020-02-20 12:00:24+00:00
- **Updated**: 2020-02-20 12:00:24+00:00
- **Authors**: Xiaohong Wang, Xudong Jiang, Henghui Ding, Jun Liu
- **Comment**: Accepted to TIP
- **Journal**: None
- **Summary**: Accurate segmentation of skin lesion from dermoscopic images is a crucial part of computer-aided diagnosis of melanoma. It is challenging due to the fact that dermoscopic images from different patients have non-negligible lesion variation, which causes difficulties in anatomical structure learning and consistent skin lesion delineation. In this paper, we propose a novel bi-directional dermoscopic feature learning (biDFL) framework to model the complex correlation between skin lesions and their informative context. By controlling feature information passing through two complementary directions, a substantially rich and discriminative feature representation is achieved. Specifically, we place biDFL module on the top of a CNN network to enhance high-level parsing performance. Furthermore, we propose a multi-scale consistent decision fusion (mCDF) that is capable of selectively focusing on the informative decisions generated from multiple classification layers. By analysis of the consistency of the decision at each position, mCDF automatically adjusts the reliability of decisions and thus allows a more insightful skin lesion delineation. The comprehensive experimental results show the effectiveness of the proposed method on skin lesion segmentation, achieving state-of-the-art performance consistently on two publicly available dermoscopic image databases.



### A Neural Lip-Sync Framework for Synthesizing Photorealistic Virtual News Anchors
- **Arxiv ID**: http://arxiv.org/abs/2002.08700v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2002.08700v2)
- **Published**: 2020-02-20 12:26:20+00:00
- **Updated**: 2021-05-05 10:01:18+00:00
- **Authors**: Ruobing Zheng, Zhou Zhu, Bo Song, Changjiang Ji
- **Comment**: Accepted by ICPR2020
- **Journal**: None
- **Summary**: Lip sync has emerged as a promising technique for generating mouth movements from audio signals. However, synthesizing a high-resolution and photorealistic virtual news anchor is still challenging. Lack of natural appearance, visual consistency, and processing efficiency are the main problems with existing methods. This paper presents a novel lip-sync framework specially designed for producing high-fidelity virtual news anchors. A pair of Temporal Convolutional Networks are used to learn the cross-modal sequential mapping from audio signals to mouth movements, followed by a neural rendering network that translates the synthetic facial map into a high-resolution and photorealistic appearance. This fully trainable framework provides end-to-end processing that outperforms traditional graphics-based methods in many low-delay applications. Experiments also show the framework has advantages over modern neural-based methods in both visual appearance and efficiency.



### A survey on Semi-, Self- and Unsupervised Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2002.08721v5
- **DOI**: 10.1109/ACCESS.2021.3084358
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.08721v5)
- **Published**: 2020-02-20 13:29:27+00:00
- **Updated**: 2021-05-25 12:29:55+00:00
- **Authors**: Lars Schmarje, Monty Santarossa, Simon-Martin Schröder, Reinhard Koch
- **Comment**: Accepted to IEEE Access 2021
- **Journal**: IEEE Access 2021
- **Summary**: While deep learning strategies achieve outstanding results in computer vision tasks, one issue remains: The current strategies rely heavily on a huge amount of labeled data. In many real-world problems, it is not feasible to create such an amount of labeled training data. Therefore, it is common to incorporate unlabeled data into the training process to reach equal results with fewer labels. Due to a lot of concurrent research, it is difficult to keep track of recent developments. In this survey, we provide an overview of often used ideas and methods in image classification with fewer labels. We compare 34 methods in detail based on their performance and their commonly used ideas rather than a fine-grained taxonomy. In our analysis, we identify three major trends that lead to future research opportunities. 1. State-of-the-art methods are scaleable to real-world applications in theory but issues like class imbalance, robustness, or fuzzy labels are not considered. 2. The degree of supervision which is needed to achieve comparable results to the usage of all labels is decreasing and therefore methods need to be extended to settings with a variable number of classes. 3. All methods share some common ideas but we identify clusters of methods that do not share many ideas. We show that combining ideas from different clusters can lead to better performance.



### Roto-Translation Equivariant Convolutional Networks: Application to Histopathology Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2002.08725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08725v1)
- **Published**: 2020-02-20 13:44:29+00:00
- **Updated**: 2020-02-20 13:44:29+00:00
- **Authors**: Maxime W. Lafarge, Erik J. Bekkers, Josien P. W. Pluim, Remco Duits, Mitko Veta
- **Comment**: None
- **Journal**: None
- **Summary**: Rotation-invariance is a desired property of machine-learning models for medical image analysis and in particular for computational pathology applications. We propose a framework to encode the geometric structure of the special Euclidean motion group SE(2) in convolutional networks to yield translation and rotation equivariance via the introduction of SE(2)-group convolution layers. This structure enables models to learn feature representations with a discretized orientation dimension that guarantees that their outputs are invariant under a discrete set of rotations. Conventional approaches for rotation invariance rely mostly on data augmentation, but this does not guarantee the robustness of the output when the input is rotated. At that, trained conventional CNNs may require test-time rotation augmentation to reach their full capability. This study is focused on histopathology image analysis applications for which it is desirable that the arbitrary global orientation information of the imaged tissues is not captured by the machine learning models. The proposed framework is evaluated on three different histopathology image analysis tasks (mitosis detection, nuclei segmentation and tumor classification). We present a comparative analysis for each problem and show that consistent increase of performances can be achieved when using the proposed framework.



### Bimodal Distribution Removal and Genetic Algorithm in Neural Network for Breast Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2002.08729v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08729v1)
- **Published**: 2020-02-20 13:51:40+00:00
- **Updated**: 2020-02-20 13:51:40+00:00
- **Authors**: Ke Quan
- **Comment**: None
- **Journal**: None
- **Summary**: Diagnosis of breast cancer has been well studied in the past. Multiple linear programming models have been devised to approximate the relationship between cell features and tumour malignancy. However, these models are less capable in handling non-linear correlations. Neural networks instead are powerful in processing complex non-linear correlations. It is thus certainly beneficial to approach this cancer diagnosis problem with a model based on neural network. Particularly, introducing bias to neural network training process is deemed as an important means to increase training efficiency. Out of a number of popular proposed methods for introducing artificial bias, Bimodal Distribution Removal (BDR) presents ideal efficiency improvement results and fair simplicity in implementation. However, this paper examines the effectiveness of BDR against the target cancer diagnosis classification problem and shows that BDR process in fact negatively impacts classification performance. In addition, this paper also explores genetic algorithm as an efficient tool for feature selection and produced significantly better results comparing to baseline model that without any feature selection in place



### Disentangled Speech Embeddings using Cross-modal Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2002.08742v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2002.08742v2)
- **Published**: 2020-02-20 14:13:12+00:00
- **Updated**: 2020-05-04 15:01:49+00:00
- **Authors**: Arsha Nagrani, Joon Son Chung, Samuel Albanie, Andrew Zisserman
- **Comment**: ICASSP 2020. The first three authors contributed equally to this work
- **Journal**: None
- **Summary**: The objective of this paper is to learn representations of speaker identity without access to manually annotated data. To do so, we develop a self-supervised learning objective that exploits the natural cross-modal synchrony between faces and audio in video. The key idea behind our approach is to tease apart--without annotation--the representations of linguistic content and speaker identity. We construct a two-stream architecture which: (1) shares low-level features common to both representations; and (2) provides a natural mechanism for explicitly disentangling these factors, offering the potential for greater generalisation to novel combinations of content and identity and ultimately producing speaker identity representations that are more robust. We train our method on a large-scale audio-visual dataset of talking heads `in the wild', and demonstrate its efficacy by evaluating the learned speaker representations for standard speaker recognition performance.



### Object 6D Pose Estimation with Non-local Attention
- **Arxiv ID**: http://arxiv.org/abs/2002.08749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08749v1)
- **Published**: 2020-02-20 14:23:32+00:00
- **Updated**: 2020-02-20 14:23:32+00:00
- **Authors**: Jianhan Mei, Henghui Ding, Xudong Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the challenging task of estimating 6D object pose from a single RGB image. Motivated by the deep learning based object detection methods, we propose a concise and efficient network that integrate 6D object pose parameter estimation into the object detection framework. Furthermore, for more robust estimation to occlusion, a non-local self-attention module is introduced. The experimental results show that the proposed method reaches the state-of-the-art performance on the YCB-video and the Linemod datasets.



### A Convolutional Neural Network into graph space
- **Arxiv ID**: http://arxiv.org/abs/2002.09285v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2002.09285v2)
- **Published**: 2020-02-20 15:14:21+00:00
- **Updated**: 2020-02-25 12:59:14+00:00
- **Authors**: Maxime Martineau, Romain Raveaux, Donatello Conte, Gilles Venturini
- **Comment**: arXiv admin note: text overlap with arXiv:1611.08402 by other authors
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs), in a few decades, have outperformed the existing state of the art methods in classification context. However, in the way they were formalised, CNNs are bound to operate on euclidean spaces. Indeed, convolution is a signal operation that are defined on euclidean spaces. This has restricted deep learning main use to euclidean-defined data such as sound or image. And yet, numerous computer application fields (among which network analysis, computational social science, chemo-informatics or computer graphics) induce non-euclideanly defined data such as graphs, networks or manifolds. In this paper we propose a new convolution neural network architecture, defined directly into graph space. Convolution and pooling operators are defined in graph domain. We show its usability in a back-propagation context. Experimental results show that our model performance is at state of the art level on simple tasks. It shows robustness with respect to graph domain changes and improvement with respect to other euclidean and non-euclidean convolutional architectures.



### Deep Learning Estimation of Multi-Tissue Constrained Spherical Deconvolution with Limited Single Shell DW-MRI
- **Arxiv ID**: http://arxiv.org/abs/2002.08820v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2002.08820v1)
- **Published**: 2020-02-20 15:59:03+00:00
- **Updated**: 2020-02-20 15:59:03+00:00
- **Authors**: Vishwesh Nath, Sudhir K. Pathak, Kurt G. Schilling, Walt Schneider, Bennett A. Landman
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Diffusion-weighted magnetic resonance imaging (DW-MRI) is the only non-invasive approach for estimation of intra-voxel tissue microarchitecture and reconstruction of in vivo neural pathways for the human brain. With improvement in accelerated MRI acquisition technologies, DW-MRI protocols that make use of multiple levels of diffusion sensitization have gained popularity. A well-known advanced method for reconstruction of white matter microstructure that uses multi-shell data is multi-tissue constrained spherical deconvolution (MT-CSD). MT-CSD substantially improves the resolution of intra-voxel structure over the traditional single shell version, constrained spherical deconvolution (CSD). Herein, we explore the possibility of using deep learning on single shell data (using the b=1000 s/mm2 from the Human Connectome Project (HCP)) to estimate the information content captured by 8th order MT-CSD using the full three shell data (b=1000, 2000, and 3000 s/mm2 from HCP). Briefly, we examine two network architectures: 1.) Sequential network of fully connected dense layers with a residual block in the middle (ResDNN), 2.) Patch based convolutional neural network with a residual block (ResCNN). For both networks an additional output block for estimation of voxel fraction was used with a modified loss function. Each approach was compared against the baseline of using MT-CSD on all data on 15 subjects from the HCP divided into 5 training, 2 validation, and 8 testing subjects with a total of 6.7 million voxels. The fiber orientation distribution function (fODF) can be recovered with high correlation (0.77 vs 0.74 and 0.65) as compared to the ground truth of MT-CST, which was derived from the multi-shell DW-MRI acquisitions. Source code and models have been made publicly available.



### Automatic Shortcut Removal for Self-Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.08822v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08822v3)
- **Published**: 2020-02-20 16:00:18+00:00
- **Updated**: 2020-06-30 11:15:48+00:00
- **Authors**: Matthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen
- **Comment**: None
- **Journal**: None
- **Summary**: In self-supervised visual representation learning, a feature extractor is trained on a "pretext task" for which labels can be generated cheaply, without human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such "shortcut" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a "lens" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.



### The DIDI dataset: Digital Ink Diagram data
- **Arxiv ID**: http://arxiv.org/abs/2002.09303v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.09303v2)
- **Published**: 2020-02-20 16:16:28+00:00
- **Updated**: 2020-02-24 11:56:21+00:00
- **Authors**: Philippe Gervais, Thomas Deselaers, Emre Aksan, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: We are releasing a dataset of diagram drawings with dynamic drawing information. The dataset aims to foster research in interactive graphical symbolic understanding. The dataset was obtained using a prompted data collection effort.



### A Bayes-Optimal View on Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2002.08859v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08859v2)
- **Published**: 2020-02-20 16:43:47+00:00
- **Updated**: 2021-03-17 09:47:10+00:00
- **Authors**: Eitan Richardson, Yair Weiss
- **Comment**: Minor revision per journal review, 28 pages
- **Journal**: None
- **Summary**: Since the discovery of adversarial examples - the ability to fool modern CNN classifiers with tiny perturbations of the input, there has been much discussion whether they are a "bug" that is specific to current neural architectures and training methods or an inevitable "feature" of high dimensional geometry. In this paper, we argue for examining adversarial examples from the perspective of Bayes-Optimal classification. We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and derive analytic conditions on the distributions under which these classifiers are provably robust against any adversarial attack even in high dimensions. Our results show that even when these "gold standard" optimal classifiers are robust, CNNs trained on the same datasets consistently learn a vulnerable classifier, indicating that adversarial examples are often an avoidable "bug". We further show that RBF SVMs trained on the same data consistently learn a robust classifier. The same trend is observed in experiments with real images in different datasets.



### Are Gabor Kernels Optimal for Iris Recognition?
- **Arxiv ID**: http://arxiv.org/abs/2002.08959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08959v1)
- **Published**: 2020-02-20 17:51:11+00:00
- **Updated**: 2020-02-20 17:51:11+00:00
- **Authors**: Aidan Boyd, Adam Czajka, Kevin Bowyer
- **Comment**: To appear at IJCB 2020
- **Journal**: None
- **Summary**: Gabor kernels are widely accepted as dominant filters for iris recognition. In this work we investigate, given the current interest in neural networks, if Gabor kernels are the only family of functions performing best in iris recognition, or if better filters can be learned directly from iris data. We use (on purpose) a single-layer convolutional neural network as it mimics an iris code-based algorithm. We learn two sets of data-driven kernels; one starting from randomly initialized weights and the other from open-source set of Gabor kernels. Through experimentation, we show that the network does not converge on Gabor kernels, instead converging on a mix of edge detectors, blob detectors and simple waves. In our experiments carried out with three subject-disjoint datasets we found that the performance of these learned kernels is comparable to the open-source Gabor kernels. These lead us to two conclusions: (a) a family of functions offering optimal performance in iris recognition is wider than Gabor kernels, and (b) we probably hit the maximum performance for an iris coding algorithm that uses a single convolutional layer, yet with multiple filters. Released with this work is a framework to learn data-driven kernels that can be easily transplanted into open-source iris recognition software (for instance, OSIRIS -- Open Source IRIS).



### Deep Multi-Facial Patches Aggregation Network For Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.09298v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.09298v1)
- **Published**: 2020-02-20 17:57:06+00:00
- **Updated**: 2020-02-20 17:57:06+00:00
- **Authors**: Ahmed Rachid Hazourli, Amine Djeghri, Hanan Salam, Alice Othmani
- **Comment**: This article arXiv:2002.09298 is an updated version of
  arXiv:1909.10305
- **Journal**: None
- **Summary**: In this paper, we propose an approach for Facial Expressions Recognition (FER) based on a deep multi-facial patches aggregation network. Deep features are learned from facial patches using deep sub-networks and aggregated within one deep architecture for expression classification . Several problems may affect the performance of deep-learning based FER approaches, in particular, the small size of existing FER datasets which might not be sufficient to train large deep learning networks. Moreover, it is extremely time-consuming to collect and annotate a large number of facial images. To account for this, we propose two data augmentation techniques for facial expression generation to expand FER labeled training datasets. We evaluate the proposed framework on three FER datasets. Results show that the proposed approach achieves state-of-art FER deep learning approaches performance when the model is trained and tested on images from the same dataset. Moreover, the proposed data augmentation techniques improve the expression recognition rate, and thus can be a solution for training deep learning FER models using small datasets. The accuracy degrades significantly when testing for dataset bias.



### Deep Learning-Based Feature Extraction in Iris Recognition: Use Existing Models, Fine-tune or Train From Scratch?
- **Arxiv ID**: http://arxiv.org/abs/2002.08916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08916v1)
- **Published**: 2020-02-20 18:00:33+00:00
- **Updated**: 2020-02-20 18:00:33+00:00
- **Authors**: Aidan Boyd, Adam Czajka, Kevin Bowyer
- **Comment**: Presented at BTAS 2019
- **Journal**: None
- **Summary**: Modern deep learning techniques can be employed to generate effective feature extractors for the task of iris recognition. The question arises: should we train such structures from scratch on a relatively large iris image dataset, or it is better to fine-tune the existing models to adapt them to a new domain? In this work we explore five different sets of weights for the popular ResNet-50 architecture to find out whether iris-specific feature extractors perform better than models trained for non-iris tasks. Features are extracted from each convolutional layer and the classification accuracy achieved by a Support Vector Machine is measured on a dataset that is disjoint from the samples used in training of the ResNet-50 model. We show that the optimal training strategy is to fine-tune an off-the-shelf set of weights to the iris recognition domain. This approach results in greater accuracy than both off-the-shelf weights and a model trained from scratch. The winning, fine-tuned approach also shows an increase in performance when compared to previous work, in which only off-the-shelf (not fine-tuned) models were used in iris feature extraction. We make the best-performing ResNet-50 model, fine-tuned with more than 360,000 iris images, publicly available along with this paper.



### Strategy to Increase the Safety of a DNN-based Perception for HAD Systems
- **Arxiv ID**: http://arxiv.org/abs/2002.08935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08935v1)
- **Published**: 2020-02-20 18:32:53+00:00
- **Updated**: 2020-02-20 18:32:53+00:00
- **Authors**: Timo Sämann, Peter Schlicht, Fabian Hüger
- **Comment**: None
- **Journal**: None
- **Summary**: Safety is one of the most important development goals for highly automated driving (HAD) systems. This applies in particular to the perception function driven by Deep Neural Networks (DNNs). For these, large parts of the traditional safety processes and requirements are not fully applicable or sufficient. The aim of this paper is to present a framework for the description and mitigation of DNN insufficiencies and the derivation of relevant safety mechanisms to increase the safety of DNNs. To assess the effectiveness of these safety mechanisms, we present a categorization scheme for evaluation metrics.



### Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction
- **Arxiv ID**: http://arxiv.org/abs/2002.08945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08945v1)
- **Published**: 2020-02-20 18:50:44+00:00
- **Updated**: 2020-02-20 18:50:44+00:00
- **Authors**: Bingbin Liu, Ehsan Adeli, Zhangjie Cao, Kuan-Hui Lee, Abhijeet Shenoi, Adrien Gaidon, Juan Carlos Niebles
- **Comment**: Accepted at ICRA 2020 and IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Reasoning over visual data is a desirable capability for robotics and vision-based applications. Such reasoning enables forecasting of the next events or actions in videos. In recent years, various models have been developed based on convolution operations for prediction or forecasting, but they lack the ability to reason over spatiotemporal data and infer the relationships of different objects in the scene. In this paper, we present a framework based on graph convolution to uncover the spatiotemporal relationships in the scene for reasoning about pedestrian intent. A scene graph is built on top of segmented object instances within and across video frames. Pedestrian intent, defined as the future action of crossing or not-crossing the street, is a very crucial piece of information for autonomous vehicles to navigate safely and more smoothly. We approach the problem of intent prediction from two different perspectives and anticipate the intention-to-cross within both pedestrian-centric and location-centric scenarios. In addition, we introduce a new dataset designed specifically for autonomous-driving scenarios in areas with dense pedestrian populations: the Stanford-TRI Intent Prediction (STIP) dataset. Our experiments on STIP and another benchmark dataset show that our graph modeling framework is able to predict the intention-to-cross of the pedestrians with an accuracy of 79.10% on STIP and 79.28% on \rev{Joint Attention for Autonomous Driving (JAAD) dataset up to one second earlier than when the actual crossing happens. These results outperform the baseline and previous work. Please refer to http://stip.stanford.edu/ for the dataset and code.



### Affinity and Diversity: Quantifying Mechanisms of Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2002.08973v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08973v2)
- **Published**: 2020-02-20 19:02:02+00:00
- **Updated**: 2020-06-04 19:04:48+00:00
- **Authors**: Raphael Gontijo-Lopes, Sylvia J. Smullin, Ekin D. Cubuk, Ethan Dyer
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of either distribution shift or augmentation diversity. Inspired by these, we seek to quantify how data augmentation improves model generalization. To this end, we introduce interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.



### Learning Intermediate Features of Object Affordances with a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2002.08975v1
- **DOI**: 10.32470/CCN.2018.1134-0
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08975v1)
- **Published**: 2020-02-20 19:04:40+00:00
- **Updated**: 2020-02-20 19:04:40+00:00
- **Authors**: Aria Yuan Wang, Michael J. Tarr
- **Comment**: Published on 2018 Conference on Cognitive Computational Neuroscience.
  See <https://ccneuro.org/2018/Papers/ViewPapers.asp?PaperNum=1134>
- **Journal**: None
- **Summary**: Our ability to interact with the world around us relies on being able to infer what actions objects afford -- often referred to as affordances. The neural mechanisms of object-action associations are realized in the visuomotor pathway where information about both visual properties and actions is integrated into common representations. However, explicating these mechanisms is particularly challenging in the case of affordances because there is hardly any one-to-one mapping between visual features and inferred actions. To better understand the nature of affordances, we trained a deep convolutional neural network (CNN) to recognize affordances from images and to learn the underlying features or the dimensionality of affordances. Such features form an underlying compositional structure for the general representation of affordances which can then be tested against human neural data. We view this representational analysis as the first step towards a more formal account of how humans perceive and interact with the environment.



### Comparing recurrent and convolutional neural networks for predicting wave propagation
- **Arxiv ID**: http://arxiv.org/abs/2002.08981v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.08981v3)
- **Published**: 2020-02-20 19:15:04+00:00
- **Updated**: 2020-04-20 14:28:56+00:00
- **Authors**: Stathi Fotiadis, Eduardo Pignatelli, Mario Lino Valencia, Chris Cantwell, Amos Storkey, Anil A. Bharath
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamical systems can be modelled by partial differential equations and numerical computations are used everywhere in science and engineering. In this work, we investigate the performance of recurrent and convolutional deep neural network architectures to predict the surface waves. The system is governed by the Saint-Venant equations. We improve on the long-term prediction over previous methods while keeping the inference time at a fraction of numerical simulations. We also show that convolutional networks perform at least as well as recurrent networks in this task. Finally, we assess the generalisation capability of each network by extrapolating in longer time-frames and in different physical settings.



### BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images
- **Arxiv ID**: http://arxiv.org/abs/2002.08988v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.08988v4)
- **Published**: 2020-02-20 19:41:06+00:00
- **Updated**: 2020-12-02 11:57:32+00:00
- **Authors**: Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, Niloy Mitra
- **Comment**: For project page, see https://www.monkeyoverflow.com/#/blockgan/
  Accepted to Conference on Neural Information Processing Systemsm, NeurIPS
  2020
- **Journal**: None
- **Summary**: We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the wholes cene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects' appearance, such as shadow and lighting, and provides control over each object's 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity).



### Comparing Different Deep Learning Architectures for Classification of Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2002.08991v1
- **DOI**: 10.1038/s41598-020-70479-z
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.08991v1)
- **Published**: 2020-02-20 19:47:16+00:00
- **Updated**: 2020-02-20 19:47:16+00:00
- **Authors**: Keno K. Bressem, Lisa Adams, Christoph Erxleben, Bernd Hamm, Stefan Niehues, Janis Vahldiek
- **Comment**: 15 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Chest radiographs are among the most frequently acquired images in radiology and are often the subject of computer vision research. However, most of the models used to classify chest radiographs are derived from openly available deep neural networks, trained on large image-datasets. These datasets routinely differ from chest radiographs in that they are mostly color images and contain several possible image classes, while radiographs are greyscale images and often only contain fewer image classes. Therefore, very deep neural networks, which can represent more complex relationships in image-features, might not be required for the comparatively simpler task of classifying grayscale chest radiographs. We compared fifteen different architectures of artificial neural networks regarding training-time and performance on the openly available CheXpert dataset to identify the most suitable models for deep learning tasks on chest radiographs. We could show, that smaller networks such as ResNet-34, AlexNet or VGG-16 have the potential to classify chest radiographs as precisely as deeper neural networks such as DenseNet-201 or ResNet-151, while being less computationally demanding.



### Complete Endomorphisms in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2002.09003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.09003v1)
- **Published**: 2020-02-20 20:28:08+00:00
- **Updated**: 2020-02-20 20:28:08+00:00
- **Authors**: Javier Finat, Francisco Delgado-del-Hoyo
- **Comment**: 22 pages, 2 figures
- **Journal**: None
- **Summary**: Correspondences between k-tuples of points are key in multiple view geometry and motion analysis. Regular transformations are posed by homographies between two projective planes that serves as structural models for images. Such transformations can not include degenerate situations. Fundamental or essential matrices expand homographies with structural information by using degenerate bilinear maps. The projectivization of the endomorphisms of a three-dimensional vector space includes all of them. Hence, they are able to explain a wider range of eventually degenerate transformations between arbitrary pairs of views. To include these degenerate situations, this paper introduces a completion of bilinear maps between spaces given by an equivariant compactification of regular transformations. This completion is extensible to the varieties of fundamental and essential matrices, where most methods based on regular transformations fail. The construction of complete endomorphisms manages degenerate projection maps using a simultaneous action on source and target spaces. In such way, this mathematical construction provides a robust framework to relate corresponding views in multiple view geometry.



### Audio-video Emotion Recognition in the Wild using Deep Hybrid Networks
- **Arxiv ID**: http://arxiv.org/abs/2002.09023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09023v1)
- **Published**: 2020-02-20 21:18:17+00:00
- **Updated**: 2020-02-20 21:18:17+00:00
- **Authors**: Xin Guo, Luisa F. Polanía, Kenneth E. Barner
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an audiovisual-based emotion recognition hybrid network. While most of the previous work focuses either on using deep models or hand-engineered features extracted from images, we explore multiple deep models built on both images and audio signals. Specifically, in addition to convolutional neural networks (CNN) and recurrent neutral networks (RNN) trained on facial images, the hybrid network also contains one SVM classifier trained on holistic acoustic feature vectors, one long short-term memory network (LSTM) trained on short-term feature sequences extracted from segmented audio clips, and one Inception(v2)-LSTM network trained on image-like maps, which are built based on short-term acoustic feature sequences. Experimental results show that the proposed hybrid network outperforms the baseline method by a large margin.



### Brain Age Estimation Using LSTM on Children's Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2002.09045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09045v1)
- **Published**: 2020-02-20 22:27:52+00:00
- **Updated**: 2020-02-20 22:27:52+00:00
- **Authors**: Sheng He, Randy L. Gollub, Shawn N. Murphy, Juan David Perez, Sanjay Prabhu, Rudolph Pienaar, Richard L. Robertson, P. Ellen Grant, Yangming Ou
- **Comment**: ISBI 2020
- **Journal**: None
- **Summary**: Brain age prediction based on children's brain MRI is an important biomarker for brain health and brain development analysis. In this paper, we consider the 3D brain MRI volume as a sequence of 2D images and propose a new framework using the recurrent neural network for brain age estimation. The proposed method is named as 2D-ResNet18+Long short-term memory (LSTM), which consists of four parts: 2D ResNet18 for feature extraction on 2D images, a pooling layer for feature reduction over the sequences, an LSTM layer, and a final regression layer. We apply the proposed method on a public multisite NIH-PD dataset and evaluate generalization on a second multisite dataset, which shows that the proposed 2D-ResNet18+LSTM method provides better results than traditional 3D based neural network for brain age estimation.



### Unsupervised Pre-trained, Texture Aware And Lightweight Model for Deep Learning-Based Iris Recognition Under Limited Annotated Data
- **Arxiv ID**: http://arxiv.org/abs/2002.09048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09048v1)
- **Published**: 2020-02-20 22:30:38+00:00
- **Updated**: 2020-02-20 22:30:38+00:00
- **Authors**: Manashi Chakraborty, Mayukh Roy, Prabir Kumar Biswas, Pabitra Mitra
- **Comment**: Under review at ICIP2020
- **Journal**: None
- **Summary**: In this paper, we present a texture aware lightweight deep learning framework for iris recognition. Our contributions are primarily three fold. Firstly, to address the dearth of labelled iris data, we propose a reconstruction loss guided unsupervised pre-training stage followed by supervised refinement. This drives the network weights to focus on discriminative iris texture patterns. Next, we propose several texture aware improvisations inside a Convolution Neural Net to better leverage iris textures. Finally, we show that our systematic training and architectural choices enable us to design an efficient framework with upto 100X fewer parameters than contemporary deep learning baselines yet achieve better recognition performance for within and cross dataset evaluations.



### Post-training Quantization with Multiple Points: Mixed Precision without Mixed Precision
- **Arxiv ID**: http://arxiv.org/abs/2002.09049v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.09049v3)
- **Published**: 2020-02-20 22:37:45+00:00
- **Updated**: 2021-01-14 15:25:38+00:00
- **Authors**: Xingchao Liu, Mao Ye, Dengyong Zhou, Qiang Liu
- **Comment**: Accepted by AAAI2021
- **Journal**: None
- **Summary**: We consider the post-training quantization problem, which discretizes the weights of pre-trained deep neural networks without re-training the model. We propose multipoint quantization, a quantization method that approximates a full-precision weight vector using a linear combination of multiple vectors of low-bit numbers; this is in contrast to typical quantization methods that approximate each weight using a single low precision number. Computationally, we construct the multipoint quantization with an efficient greedy selection procedure, and adaptively decides the number of low precision points on each quantized weight vector based on the error of its output. This allows us to achieve higher precision levels for important weights that greatly influence the outputs, yielding an 'effect of mixed precision' but without physical mixed precision implementations (which requires specialized hardware accelerators). Empirically, our method can be implemented by common operands, bringing almost no memory and computation overhead. We show that our method outperforms a range of state-of-the-art methods on ImageNet classification and it can be generalized to more challenging tasks like PASCAL VOC object detection.



### Adapted Center and Scale Prediction: More Stable and More Accurate
- **Arxiv ID**: http://arxiv.org/abs/2002.09053v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.09053v2)
- **Published**: 2020-02-20 22:49:50+00:00
- **Updated**: 2020-03-01 02:32:54+00:00
- **Authors**: Wenhao Wang
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Pedestrian detection benefits from deep learning technology and gains rapid development in recent years. Most of detectors follow general object detection frame, i.e. default boxes and two-stage process. Recently, anchor-free and one-stage detectors have been introduced into this area. However, their accuracies are unsatisfactory. Therefore, in order to enjoy the simplicity of anchor-free detectors and the accuracy of two-stage ones simultaneously, we propose some adaptations based on a detector, Center and Scale Prediction(CSP). The main contributions of our paper are: (1) We improve the robustness of CSP and make it easier to train. (2) We propose a novel method to predict width, namely compressing width. (3) We achieve the second best performance on CityPersons benchmark, i.e. 9.3% log-average miss rate(MR) on reasonable set, 8.7% MR on partial set and 5.6% MR on bare set, which shows an anchor-free and one-stage detector can still have high accuracy. (4) We explore some capabilities of Switchable Normalization which are not mentioned in its original paper.




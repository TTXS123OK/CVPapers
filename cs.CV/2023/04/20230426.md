# Arxiv Papers in cs.CV on 2023-04-26
### Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2304.13201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13201v1)
- **Published**: 2023-04-26 00:04:50+00:00
- **Updated**: 2023-04-26 00:04:50+00:00
- **Authors**: Negar Nejatishahidin, Will Hutchcroft, Manjunath Narayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khosravan, Jana Kosecka, Sing Bing Kang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of wide-baseline camera pose estimation from a group of 360$^\circ$ panoramas under upright-camera assumption. Recent work has demonstrated the merit of deep-learning for end-to-end direct relative pose regression in 360$^\circ$ panorama pairs [11]. To exploit the benefits of multi-view logic in a learning-based framework, we introduce Graph-CoVis, which non-trivially extends CoVisPose [11] from relative two-view to global multi-view spherical camera pose estimation. Graph-CoVis is a novel Graph Neural Network based architecture that jointly learns the co-visible structure and global motion in an end-to-end and fully-supervised approach. Using the ZInD [4] dataset, which features real homes presenting wide-baselines, occlusion, and limited visual overlap, we show that our model performs competitively to state-of-the-art approaches.



### EverLight: Indoor-Outdoor Editable HDR Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/2304.13207v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13207v2)
- **Published**: 2023-04-26 00:20:59+00:00
- **Updated**: 2023-08-21 18:53:15+00:00
- **Authors**: Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, Jean-François Lalonde
- **Comment**: ICCV 2023, https://lvsn.github.io/everlight/
- **Journal**: None
- **Summary**: Because of the diversity in lighting environments, existing illumination estimation techniques have been designed explicitly on indoor or outdoor environments. Methods have focused specifically on capturing accurate energy (e.g., through parametric lighting models), which emphasizes shading and strong cast shadows; or producing plausible texture (e.g., with GANs), which prioritizes plausible reflections. Approaches which provide editable lighting capabilities have been proposed, but these tend to be with simplified lighting models, offering limited realism. In this work, we propose to bridge the gap between these recent trends in the literature, and propose a method which combines a parametric light model with 360{\deg} panoramas, ready to use as HDRI in rendering engines. We leverage recent advances in GAN-based LDR panorama extrapolation from a regular image, which we extend to HDR using parametric spherical gaussians. To achieve this, we introduce a novel lighting co-modulation method that injects lighting-related features throughout the generator, tightly coupling the original or edited scene illumination within the panorama generation process. In our representation, users can easily edit light direction, intensity, number, etc. to impact shading while providing rich, complex reflections while seamlessly blending with the edits. Furthermore, our method encompasses indoor and outdoor environments, demonstrating state-of-the-art results even when compared to domain-specific methods.



### Single-View Height Estimation with Conditional Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2304.13214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13214v1)
- **Published**: 2023-04-26 00:37:05+00:00
- **Updated**: 2023-04-26 00:37:05+00:00
- **Authors**: Isaac Corley, Peyman Najafirad
- **Comment**: None
- **Journal**: None
- **Summary**: Digital Surface Models (DSM) offer a wealth of height information for understanding the Earth's surface as well as monitoring the existence or change in natural and man-made structures. Classical height estimation requires multi-view geospatial imagery or LiDAR point clouds which can be expensive to acquire. Single-view height estimation using neural network based models shows promise however it can struggle with reconstructing high resolution features. The latest advancements in diffusion models for high resolution image synthesis and editing have yet to be utilized for remote sensing imagery, particularly height estimation. Our approach involves training a generative diffusion model to learn the joint distribution of optical and DSM images across both domains as a Markov chain. This is accomplished by minimizing a denoising score matching objective while being conditioned on the source image to generate realistic high resolution 3D surfaces. In this paper we experiment with conditional denoising diffusion probabilistic models (DDPM) for height estimation from a single remotely sensed image and show promising results on the Vaihingen benchmark dataset.



### Exploiting CNNs for Semantic Segmentation with Pascal VOC
- **Arxiv ID**: http://arxiv.org/abs/2304.13216v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.13216v2)
- **Published**: 2023-04-26 00:40:27+00:00
- **Updated**: 2023-05-05 05:27:24+00:00
- **Authors**: Sourabh Prakash, Priyanshi Shah, Ashrya Agrawal
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements: a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in training pipeline, we also explore three different architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that the improvements help in greatly improving the performance, as reflected both, in metrics and segmentation maps. Interestingly, we observe that among the improvements, dataset augmentation has the greatest contribution. Also, note that transfer learning model performs the best on the pascal dataset. We analyse the performance of these using loss, accuracy and IoU plots along with segmentation maps, which help us draw valuable insights about the working of the models.



### ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.13219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13219v1)
- **Published**: 2023-04-26 00:52:42+00:00
- **Updated**: 2023-04-26 00:52:42+00:00
- **Authors**: Isaac Corley, Jonathan Lwowski, Peyman Najafirad
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset. ZRG contains thousands of samples of high resolution orthomosaics of aerial imagery of residential rooftops with corresponding digital surface models (DSM), 3D rooftop wireframes, and multiview imagery generated point clouds for the purpose of residential rooftop geometry and scene understanding. We perform thorough benchmarks to illustrate the numerous applications unlocked by this dataset and provide baselines for the tasks of roof outline extraction, monocular height estimation, and planar roof structure extraction.



### Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2304.14204v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.14204v1)
- **Published**: 2023-04-26 01:26:19+00:00
- **Updated**: 2023-04-26 01:26:19+00:00
- **Authors**: Bingqian Lin, Zicong Chen, Mingjie Li, Haokun Lin, Hang Xu, Yi Zhu, Jianzhuang Liu, Wenjia Cai, Lei Yang, Shen Zhao, Chenfei Wu, Ling Chen, Xiaojun Chang, Yi Yang, Lei Xing, Xiaodan Liang
- **Comment**: Project page: https://github.com/chenzcv7/MOTOR
- **Journal**: None
- **Summary**: Medical artificial general intelligence (MAGI) enables one foundation model to solve different medical tasks, which is very practical in the medical domain. It can significantly reduce the requirement of large amounts of task-specific data by sufficiently sharing medical knowledge among different tasks. However, due to the challenges of designing strongly generalizable models with limited and complex medical data, most existing approaches tend to develop task-specific models. To take a step towards MAGI, we propose a new paradigm called Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR). In MOTOR, we combine two kinds of basic medical knowledge, i.e., general and specific knowledge, in a complementary manner to boost the general pretraining process. As a result, the foundation model with comprehensive basic knowledge can learn compact representations from pretraining radiographic data for better cross-modal alignment. MOTOR unifies the understanding and generation, which are two kinds of core intelligence of an AI system, into a single medical foundation model, to flexibly handle more diverse medical tasks. To enable a comprehensive evaluation and facilitate further research, we construct a medical multimodal benchmark including a wide range of downstream tasks, such as chest x-ray report generation and medical visual question answering. Extensive experiments on our benchmark show that MOTOR obtains promising results through simple task-oriented adaptation. The visualization shows that the injected knowledge successfully highlights key information in the medical data, demonstrating the excellent interpretability of MOTOR. Our MOTOR successfully mimics the human practice of fulfilling a "medical student" to accelerate the process of becoming a "specialist". We believe that our work makes a significant stride in realizing MAGI.



### Generating Adversarial Examples with Task Oriented Multi-Objective Optimization
- **Arxiv ID**: http://arxiv.org/abs/2304.13229v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13229v2)
- **Published**: 2023-04-26 01:30:02+00:00
- **Updated**: 2023-06-01 18:01:31+00:00
- **Authors**: Anh Bui, Trung Le, He Zhao, Quan Tran, Paul Montague, Dinh Phung
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to address this issue, in the context where we can explicitly define the goal achievement for a task. Our principle is to only maintain the goal-achieved tasks, while letting the optimizer spend more effort on improving the goal-unachieved tasks. We conduct comprehensive experiments for our Task Oriented MOO on various adversarial example generation schemes. The experimental results firmly demonstrate the merit of our proposed approach. Our code is available at \url{https://github.com/tuananhbui89/TAMOO}.



### Structure Diagram Recognition in Financial Announcements
- **Arxiv ID**: http://arxiv.org/abs/2304.13240v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13240v2)
- **Published**: 2023-04-26 02:04:19+00:00
- **Updated**: 2023-05-01 11:44:49+00:00
- **Authors**: Meixuan Qiao, Jun Wang, Junfu Xiang, Qiyu Hou, Ruixuan Li
- **Comment**: ICDAR2023
- **Journal**: None
- **Summary**: Accurately extracting structured data from structure diagrams in financial announcements is of great practical importance for building financial knowledge graphs and further improving the efficiency of various financial applications. First, we proposed a new method for recognizing structure diagrams in financial announcements, which can better detect and extract different types of connecting lines, including straight lines, curves, and polylines of different orientations and angles. Second, we developed a two-stage method to efficiently generate the industry's first benchmark of structure diagrams from Chinese financial announcements, where a large number of diagrams were synthesized and annotated using an automated tool to train a preliminary recognition model with fairly good performance, and then a high-quality benchmark can be obtained by automatically annotating the real-world structure diagrams using the preliminary model and then making few manual corrections. Finally, we experimentally verified the significant performance advantage of our structure diagram recognition method over previous methods.



### Learning to Predict Navigational Patterns from Partial Observations
- **Arxiv ID**: http://arxiv.org/abs/2304.13242v2
- **DOI**: 10.1109/LRA.2023.3291924
- **Categories**: **cs.CV**, cs.LG, cs.RO, I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2304.13242v2)
- **Published**: 2023-04-26 02:08:46+00:00
- **Updated**: 2023-07-06 00:46:50+00:00
- **Authors**: Robin Karlsson, Alexander Carballo, Francisco Lepe-Salazar, Keisuke Fujii, Kento Ohtani, Kazuya Takeda
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RA-L) 2023
- **Journal**: None
- **Summary**: Human beings cooperatively navigate rule-constrained environments by adhering to mutually known navigational patterns, which may be represented as directional pathways or road lanes. Inferring these navigational patterns from incompletely observed environments is required for intelligent mobile robots operating in unmapped locations. However, algorithmically defining these navigational patterns is nontrivial. This paper presents the first self-supervised learning (SSL) method for learning to infer navigational patterns in real-world environments from partial observations only. We explain how geometric data augmentation, predictive world modeling, and an information-theoretic regularizer enables our model to predict an unbiased local directional soft lane probability (DSLP) field in the limit of infinite data. We demonstrate how to infer global navigational patterns by fitting a maximum likelihood graph to the DSLP field. Experiments show that our SSL model outperforms two SOTA supervised lane graph prediction models on the nuScenes dataset. We propose our SSL method as a scalable and interpretable continual learning paradigm for navigation by perception. Code is available at https://github.com/robin-karlsson0/dslp.



### StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2304.13265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13265v1)
- **Published**: 2023-04-26 03:37:28+00:00
- **Updated**: 2023-04-26 03:37:28+00:00
- **Authors**: Nikita Dvornik, Isma Hadji, Ran Zhang, Konstantinos G. Derpanis, Animesh Garg, Richard P. Wildes, Allan D. Jepson
- **Comment**: CVPR'23
- **Journal**: None
- **Summary**: Instructional videos are an important resource to learn procedural tasks from human demonstrations. However, the instruction steps in such videos are typically short and sparse, with most of the video being irrelevant to the procedure. This motivates the need to temporally localize the instruction steps in such videos, i.e. the task called key-step localization. Traditional methods for key-step localization require video-level human annotations and thus do not scale to large datasets. In this work, we tackle the problem with no human supervision and introduce StepFormer, a self-supervised model that discovers and localizes instruction steps in a video. StepFormer is a transformer decoder that attends to the video with learnable queries, and produces a sequence of slots capturing the key-steps in the video. We train our system on a large dataset of instructional videos, using their automatically-generated subtitles as the only source of supervision. In particular, we supervise our system with a sequence of text narrations using an order-aware loss function that filters out irrelevant phrases. We show that our model outperforms all previous unsupervised and weakly-supervised approaches on step detection and localization by a large margin on three challenging benchmarks. Moreover, our model demonstrates an emergent property to solve zero-shot multi-step localization and outperforms all relevant baselines at this task.



### From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping
- **Arxiv ID**: http://arxiv.org/abs/2304.13273v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13273v3)
- **Published**: 2023-04-26 04:06:20+00:00
- **Updated**: 2023-05-08 02:29:28+00:00
- **Authors**: Junyang Wang, Ming Yan, Yi Zhang, Jitao Sang
- **Comment**: 11 pages, 15 figures, has been accepted by IJCAI 2023
- **Journal**: None
- **Summary**: With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsupervised training, Knight achieves State-of-the-Art performance in zero-shot methods for image captioning and video captioning. Our code is available at https://github.com/junyangwang0410/Knight.



### ESPT: A Self-Supervised Episodic Spatial Pretext Task for Improving Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.13287v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.13287v1)
- **Published**: 2023-04-26 04:52:08+00:00
- **Updated**: 2023-04-26 04:52:08+00:00
- **Authors**: Yi Rong, Xiongbo Lu, Zhaoyang Sun, Yaxiong Chen, Shengwu Xiong
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) techniques have recently been integrated into the few-shot learning (FSL) framework and have shown promising results in improving the few-shot image classification performance. However, existing SSL approaches used in FSL typically seek the supervision signals from the global embedding of every single image. Therefore, during the episodic training of FSL, these methods cannot capture and fully utilize the local visual information in image samples and the data structure information of the whole episode, which are beneficial to FSL. To this end, we propose to augment the few-shot learning objective with a novel self-supervised Episodic Spatial Pretext Task (ESPT). Specifically, for each few-shot episode, we generate its corresponding transformed episode by applying a random geometric transformation to all the images in it. Based on these, our ESPT objective is defined as maximizing the local spatial relationship consistency between the original episode and the transformed one. With this definition, the ESPT-augmented FSL objective promotes learning more transferable feature representations that capture the local spatial features of different images and their inter-relational structural information in each input episode, thus enabling the model to generalize better to new categories with only a few samples. Extensive experiments indicate that our ESPT method achieves new state-of-the-art performance for few-shot image classification on three mainstay benchmark datasets. The source code will be available at: https://github.com/Whut-YiRong/ESPT.



### Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs
- **Arxiv ID**: http://arxiv.org/abs/2304.13312v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13312v1)
- **Published**: 2023-04-26 06:33:31+00:00
- **Updated**: 2023-04-26 06:33:31+00:00
- **Authors**: Mingjie Li, Quanshi Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:2111.06206
- **Journal**: None
- **Summary**: In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.



### Concept-Monitor: Understanding DNN training through individual neurons
- **Arxiv ID**: http://arxiv.org/abs/2304.13346v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13346v1)
- **Published**: 2023-04-26 07:36:03+00:00
- **Updated**: 2023-04-26 07:36:03+00:00
- **Authors**: Mohammad Ali Khan, Tuomas Oikarinen, Tsui-Wei Weng
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a general framework called Concept-Monitor to help demystify the black-box DNN training processes automatically using a novel unified embedding space and concept diversity metric. Concept-Monitor enables human-interpretable visualization and indicators of the DNN training processes and facilitates transparency as well as deeper understanding on how DNNs develop along the during training. Inspired by these findings, we also propose a new training regularizer that incentivizes hidden neurons to learn diverse concepts, which we show to improve training performance. Finally, we apply Concept-Monitor to conduct several case studies on different training paradigms including adversarial training, fine-tuning and network pruning via the Lottery Ticket Hypothesis



### TextDeformer: Geometry Manipulation using Text Guidance
- **Arxiv ID**: http://arxiv.org/abs/2304.13348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13348v1)
- **Published**: 2023-04-26 07:38:41+00:00
- **Updated**: 2023-04-26 07:38:41+00:00
- **Authors**: William Gao, Noam Aigerman, Thibault Groueix, Vladimir G. Kim, Rana Hanocka
- **Comment**: None
- **Journal**: None
- **Summary**: We present a technique for automatically producing a deformation of an input triangle mesh, guided solely by a text prompt. Our framework is capable of deformations that produce both large, low-frequency shape changes, and small high-frequency details. Our framework relies on differentiable rendering to connect geometry to powerful pre-trained image encoders, such as CLIP and DINO. Notably, updating mesh geometry by taking gradient steps through differentiable rendering is notoriously challenging, commonly resulting in deformed meshes with significant artifacts. These difficulties are amplified by noisy and inconsistent gradients from CLIP. To overcome this limitation, we opt to represent our mesh deformation through Jacobians, which updates deformations in a global, smooth manner (rather than locally-sub-optimal steps). Our key observation is that Jacobians are a representation that favors smoother, large deformations, leading to a global relation between vertices and pixels, and avoiding localized noisy gradients. Additionally, to ensure the resulting shape is coherent from all 3D viewpoints, we encourage the deep features computed on the 2D encoding of the rendering to be consistent for a given vertex from all viewpoints. We demonstrate that our method is capable of smoothly-deforming a wide variety of source mesh and target text prompts, achieving both large modifications to, e.g., body proportions of animals, as well as adding fine semantic details, such as shoe laces on an army boot and fine details of a face.



### Discrepancy-Guided Reconstruction Learning for Image Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.13349v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.13349v2)
- **Published**: 2023-04-26 07:40:43+00:00
- **Updated**: 2023-05-03 12:50:10+00:00
- **Authors**: Zenan Shi, Haipeng Chen, Long Chen, Dong Zhang
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel image forgery detection paradigm for boosting the model learning capacity on both forgery-sensitive and genuine compact visual patterns. Compared to the existing methods that only focus on the discrepant-specific patterns (\eg, noises, textures, and frequencies), our method has a greater generalization. Specifically, we first propose a Discrepancy-Guided Encoder (DisGE) to extract forgery-sensitive visual patterns. DisGE consists of two branches, where the mainstream backbone branch is used to extract general semantic features, and the accessorial discrepant external attention branch is used to extract explicit forgery cues. Besides, a Double-Head Reconstruction (DouHR) module is proposed to enhance genuine compact visual patterns in different granular spaces. Under DouHR, we further introduce a Discrepancy-Aggregation Detector (DisAD) to aggregate these genuine compact visual patterns, such that the forgery detection capability on unknown patterns can be improved. Extensive experimental results on four challenging datasets validate the effectiveness of our proposed method against state-of-the-art competitors.



### Deep Lifelong Cross-modal Hashing
- **Arxiv ID**: http://arxiv.org/abs/2304.13357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2304.13357v1)
- **Published**: 2023-04-26 07:56:22+00:00
- **Updated**: 2023-04-26 07:56:22+00:00
- **Authors**: Liming Xu, Hanqi Li, Bochuan Zheng, Weisheng Li, Jiancheng Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing methods have made significant progress in cross-modal retrieval tasks with fast query speed and low storage cost. Among them, deep learning-based hashing achieves better performance on large-scale data due to its excellent extraction and representation ability for nonlinear heterogeneous features. However, there are still two main challenges in catastrophic forgetting when data with new categories arrive continuously, and time-consuming for non-continuous hashing retrieval to retrain for updating. To this end, we, in this paper, propose a novel deep lifelong cross-modal hashing to achieve lifelong hashing retrieval instead of re-training hash function repeatedly when new data arrive. Specifically, we design lifelong learning strategy to update hash functions by directly training the incremental data instead of retraining new hash functions using all the accumulated data, which significantly reduce training time. Then, we propose lifelong hashing loss to enable original hash codes participate in lifelong learning but remain invariant, and further preserve the similarity and dis-similarity among original and incremental hash codes to maintain performance. Additionally, considering distribution heterogeneity when new data arriving continuously, we introduce multi-label semantic similarity to supervise hash learning, and it has been proven that the similarity improves performance with detailed analysis. Experimental results on benchmark datasets show that the proposed methods achieves comparative performance comparing with recent state-of-the-art cross-modal hashing methods, and it yields substantial average increments over 20\% in retrieval accuracy and almost reduces over 80\% training time when new data arrives continuously.



### SEAL: Simultaneous Label Hierarchy Exploration And Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.13374v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13374v1)
- **Published**: 2023-04-26 08:31:59+00:00
- **Updated**: 2023-04-26 08:31:59+00:00
- **Authors**: Zhiquan Tan, Zihao Wang, Yifan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Label hierarchy is an important source of external knowledge that can enhance classification performance. However, most existing methods rely on predefined label hierarchies that may not match the data distribution. To address this issue, we propose Simultaneous label hierarchy Exploration And Learning (SEAL), a new framework that explores the label hierarchy by augmenting the observed labels with latent labels that follow a prior hierarchical structure. Our approach uses a 1-Wasserstein metric over the tree metric space as an objective function, which enables us to simultaneously learn a data-driven label hierarchy and perform (semi-)supervised learning. We evaluate our method on several datasets and show that it achieves superior results in both supervised and semi-supervised scenarios and reveals insightful label structures. Our implementation is available at https://github.com/tzq1999/SEAL.



### Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2304.13375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13375v1)
- **Published**: 2023-04-26 08:34:00+00:00
- **Updated**: 2023-04-26 08:34:00+00:00
- **Authors**: Bilel Benjdira, Anas M. Ali, Anis Koubaa
- **Comment**: Accepted in CVPR 2023 Workshops
- **Journal**: None
- **Summary**: Image Dehazing aims to remove atmospheric fog or haze from an image. Although the Dehazing models have evolved a lot in recent years, few have precisely tackled the problem of High-Resolution hazy images. For this kind of image, the model needs to work on a downscaled version of the image or on cropped patches from it. In both cases, the accuracy will drop. This is primarily due to the inherent failure to combine global and local features when the image size increases. The Dehazing model requires global features to understand the general scene peculiarities and the local features to work better with fine and pixel details. In this study, we propose the Streamlined Global and Local Features Combinator (SGLC) to solve these issues and to optimize the application of any Dehazing model to High-Resolution images. The SGLC contains two successive blocks. The first is the Global Features Generator (GFG) which generates the first version of the Dehazed image containing strong global features. The second block is the Local Features Enhancer (LFE) which improves the local feature details inside the previously generated image. When tested on the Uformer architecture for Dehazing, SGLC increased the PSNR metric by a significant margin. Any other model can be incorporated inside the SGLC process to improve its efficiency on High-Resolution input data.



### Low-field magnetic resonance image enhancement via stochastic image quality transfer
- **Arxiv ID**: http://arxiv.org/abs/2304.13385v1
- **DOI**: 10.1016/j.media.2023.102807
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13385v1)
- **Published**: 2023-04-26 08:52:29+00:00
- **Updated**: 2023-04-26 08:52:29+00:00
- **Authors**: Hongxiang Lin, Matteo Figini, Felice D'Arco, Godwin Ogbole, Ryutaro Tanno, Stefano B. Blumberg, Lisa Ronan, Biobele J. Brown, David W. Carmichael, Ikeoluwa Lagunju, Judith Helen Cross, Delmiro Fernandez-Reyes, Daniel C. Alexander
- **Comment**: Accepted in Medical Image Analysis
- **Journal**: None
- **Summary**: Low-field (<1T) magnetic resonance imaging (MRI) scanners remain in widespread use in low- and middle-income countries (LMICs) and are commonly used for some applications in higher income countries e.g. for small child patients with obesity, claustrophobia, implants, or tattoos. However, low-field MR images commonly have lower resolution and poorer contrast than images from high field (1.5T, 3T, and above). Here, we present Image Quality Transfer (IQT) to enhance low-field structural MRI by estimating from a low-field image the image we would have obtained from the same subject at high field. Our approach uses (i) a stochastic low-field image simulator as the forward model to capture uncertainty and variation in the contrast of low-field images corresponding to a particular high-field image, and (ii) an anisotropic U-Net variant specifically designed for the IQT inverse problem. We evaluate the proposed algorithm both in simulation and using multi-contrast (T1-weighted, T2-weighted, and fluid attenuated inversion recovery (FLAIR)) clinical low-field MRI data from an LMIC hospital. We show the efficacy of IQT in improving contrast and resolution of low-field MR images. We demonstrate that IQT-enhanced images have potential for enhancing visualisation of anatomical structures and pathological lesions of clinical relevance from the perspective of radiologists. IQT is proved to have capability of boosting the diagnostic value of low-field MRI, especially in low-resource settings.



### VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs
- **Arxiv ID**: http://arxiv.org/abs/2304.13386v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2304.13386v2)
- **Published**: 2023-04-26 08:52:55+00:00
- **Updated**: 2023-06-02 05:39:27+00:00
- **Authors**: Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li, Boyan Ji, Lei Zhao, Wei Xing, Huaizhong Lin
- **Comment**: IJCAI 2023 Accepted (Main Track)
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) has shown great success in novel view synthesis due to its state-of-the-art quality and flexibility. However, NeRF requires dense input views (tens to hundreds) and a long training time (hours to days) for a single scene to generate high-fidelity images. Although using the voxel grids to represent the radiance field can significantly accelerate the optimization process, we observe that for sparse inputs, the voxel grids are more prone to overfitting to the training views and will have holes and floaters, which leads to artifacts. In this paper, we propose VGOS, an approach for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10 views) to address these issues. To improve the performance of voxel-based radiance field in sparse input scenarios, we propose two methods: (a) We introduce an incremental voxel training strategy, which prevents overfitting by suppressing the optimization of peripheral voxels in the early stage of reconstruction. (b) We use several regularization techniques to smooth the voxels, which avoids degenerate solutions. Experiments demonstrate that VGOS achieves state-of-the-art performance for sparse inputs with super-fast convergence. Code will be available at https://github.com/SJoJoK/VGOS.



### Group Equivariant BEV for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.13390v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2304.13390v2)
- **Published**: 2023-04-26 09:00:31+00:00
- **Updated**: 2023-06-29 03:22:08+00:00
- **Authors**: Hongwei Liu, Jian Yang, Jianfeng Zhang, Dongheng Shao, Jielong Guo, Shaobo Li, Xuan Tang, Xian Wei
- **Comment**: 8 pages,3 figures
- **Journal**: None
- **Summary**: Recently, 3D object detection has attracted significant attention and achieved continuous improvement in real road scenarios. The environmental information is collected from a single sensor or multi-sensor fusion to detect interested objects. However, most of the current 3D object detection approaches focus on developing advanced network architectures to improve the detection precision of the object rather than considering the dynamic driving scenes, where data collected from sensors equipped in the vehicle contain various perturbation features. As a result, existing work cannot still tackle the perturbation issue. In order to solve this problem, we propose a group equivariant bird's eye view network (GeqBevNet) based on the group equivariant theory, which introduces the concept of group equivariant into the BEV fusion object detection network. The group equivariant network is embedded into the fused BEV feature map to facilitate the BEV-level rotational equivariant feature extraction, thus leading to lower average orientation error. In order to demonstrate the effectiveness of the GeqBevNet, the network is verified on the nuScenes validation dataset in which mAOE can be decreased to 0.325. Experimental results demonstrate that GeqBevNet can extract more rotational equivariant features in the 3D object detection of the actual road scene and improve the performance of object orientation prediction.



### STIR: Siamese Transformer for Image Retrieval Postprocessing
- **Arxiv ID**: http://arxiv.org/abs/2304.13393v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2304.13393v2)
- **Published**: 2023-04-26 09:10:15+00:00
- **Updated**: 2023-04-27 05:35:46+00:00
- **Authors**: Aleksei Shabanov, Aleksei Tarasov, Sergey Nikolenko
- **Comment**: 14 pages, 3 figures
- **Journal**: None
- **Summary**: Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the art on standard image retrieval datasets: Stanford Online Products and DeepFashion In-shop. We also release the source code at https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/postprocessing/ and an interactive demo of our approach at https://dapladoc-oml-postprocessing-demo-srcappmain-pfh2g0.streamlit.app/



### Filter Pruning via Filters Similarity in Consecutive Layers
- **Arxiv ID**: http://arxiv.org/abs/2304.13397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2304.13397v1)
- **Published**: 2023-04-26 09:18:38+00:00
- **Updated**: 2023-04-26 09:18:38+00:00
- **Authors**: Xiaorui Wang, Jun Wang, Xin Tang, Peng Gao, Rui Fang, Guotong Xie
- **Comment**: Accepted by ICASSP 2023 (oral)
- **Journal**: None
- **Summary**: Filter pruning is widely adopted to compress and accelerate the Convolutional Neural Networks (CNNs), but most previous works ignore the relationship between filters and channels in different layers. Processing each layer independently fails to utilize the collaborative relationship across layers. In this paper, we intuitively propose a novel pruning method by explicitly leveraging the Filters Similarity in Consecutive Layers (FSCL). FSCL compresses models by pruning filters whose corresponding features are more worthless in the model. The extensive experiments demonstrate the effectiveness of FSCL, and it yields remarkable improvement over state-of-the-art on accuracy, FLOPs and parameter reduction on several benchmark models and datasets.



### Development of a Realistic Crowd Simulation Environment for Fine-grained Validation of People Tracking Methods
- **Arxiv ID**: http://arxiv.org/abs/2304.13403v1
- **DOI**: 10.5220/0011691500003417
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13403v1)
- **Published**: 2023-04-26 09:29:58+00:00
- **Updated**: 2023-04-26 09:29:58+00:00
- **Authors**: Paweł Foszner, Agnieszka Szczęsna, Luca Ciampi, Nicola Messina, Adam Cygan, Bartosz Bizoń, Michał Cogiel, Dominik Golba, Elżbieta Macioszek, Michał Staniszewski
- **Comment**: None
- **Journal**: Proceedings of the 18th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications, 2023
- **Summary**: Generally, crowd datasets can be collected or generated from real or synthetic sources. Real data is generated by using infrastructure-based sensors (such as static cameras or other sensors). The use of simulation tools can significantly reduce the time required to generate scenario-specific crowd datasets, facilitate data-driven research, and next build functional machine learning models. The main goal of this work was to develop an extension of crowd simulation (named CrowdSim2) and prove its usability in the application of people-tracking algorithms. The simulator is developed using the very popular Unity 3D engine with particular emphasis on the aspects of realism in the environment, weather conditions, traffic, and the movement and models of individual agents. Finally, three methods of tracking were used to validate generated dataset: IOU-Tracker, Deep-Sort, and Deep-TAMA.



### Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/2304.13409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13409v1)
- **Published**: 2023-04-26 09:48:48+00:00
- **Updated**: 2023-04-26 09:48:48+00:00
- **Authors**: Marco Huber, Anh Thi Luu, Philipp Terhörst, Naser Damer
- **Comment**: None
- **Journal**: None
- **Summary**: Explainable Face Recognition is gaining growing attention as the use of the technology is gaining ground in security-critical applications. Understanding why two faces images are matched or not matched by a given face recognition system is important to operators, users, anddevelopers to increase trust, accountability, develop better systems, and highlight unfair behavior. In this work, we propose xSSAB, an approach to back-propagate similarity score-based arguments that support or oppose the face matching decision to visualize spatial maps that indicate similar and dissimilar areas as interpreted by the underlying FR model. Furthermore, we present Patch-LFW, a new explainable face verification benchmark that enables along with a novel evaluation protocol, the first quantitative evaluation of the validity of similarity and dissimilarity maps in explainable face recognition approaches. We compare our efficient approach to state-of-the-art approaches demonstrating a superior trade-off between efficiency and performance. The code as well as the proposed Patch-LFW is publicly available at: https://github.com/marcohuber/xSSAB.



### Improving Adversarial Transferability via Intermediate-level Perturbation Decay
- **Arxiv ID**: http://arxiv.org/abs/2304.13410v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13410v2)
- **Published**: 2023-04-26 09:49:55+00:00
- **Updated**: 2023-05-09 12:39:35+00:00
- **Authors**: Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen
- **Comment**: Revision of ICML '23 submission for better clarity
- **Journal**: None
- **Summary**: Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneously. In-depth discussion verifies the effectiveness of our method. Experimental results show that it outperforms state-of-the-arts by large margins in attacking various victim models on ImageNet (+10.07% on average) and CIFAR-10 (+3.88% on average). Our code is at https://github.com/qizhangli/ILPD-attack.



### DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models
- **Arxiv ID**: http://arxiv.org/abs/2304.13416v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13416v2)
- **Published**: 2023-04-26 09:55:12+00:00
- **Updated**: 2023-06-06 09:44:19+00:00
- **Authors**: Shitong Shao, Xiaohan Yuan, Zhen Huang, Ziming Qiu, Shuai Wang, Kevin Zhou
- **Comment**: Accepted by IJCAI workshop (1st International Workshop on
  Generalizing from Limited Resources in the Open World). pre-version was
  rejected by MICCAI
- **Journal**: None
- **Summary**: Dataset expansion can effectively alleviate the problem of data scarcity for medical image segmentation, due to privacy concerns and labeling difficulties. However, existing expansion algorithms still face great challenges due to their inability of guaranteeing the diversity of synthesized images with paired segmentation masks. In recent years, Diffusion Probabilistic Models (DPMs) have shown powerful image synthesis performance, even better than Generative Adversarial Networks. Based on this insight, we propose an approach called DiffuseExpand for expanding datasets for 2D medical image segmentation using DPM, which first samples a variety of masks from Gaussian noise to ensure the diversity, and then synthesizes images to ensure the alignment of images and masks. After that, DiffuseExpand chooses high-quality samples to further enhance the effectiveness of data expansion. Our comparison and ablation experiments on COVID-19 and CGMH Pelvis datasets demonstrate the effectiveness of DiffuseExpand. Our code is released at https://github.com/shaoshitong/DiffuseExpand.



### Are Explainability Tools Gender Biased? A Case Study on Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.13419v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13419v2)
- **Published**: 2023-04-26 09:59:49+00:00
- **Updated**: 2023-06-13 20:33:41+00:00
- **Authors**: Marco Huber, Meiling Fang, Fadi Boutros, Naser Damer
- **Comment**: Accepted at EUSIPCO 2023
- **Journal**: None
- **Summary**: Face recognition (FR) systems continue to spread in our daily lives with an increasing demand for higher explainability and interpretability of FR systems that are mainly based on deep learning. While bias across demographic groups in FR systems has already been studied, the bias of explainability tools has not yet been investigated. As such tools aim at steering further development and enabling a better understanding of computer vision problems, the possible existence of bias in their outcome can lead to a chain of biased decisions. In this paper, we explore the existence of bias in the outcome of explainability tools by investigating the use case of face presentation attack detection. By utilizing two different explainability tools on models with different levels of bias, we investigate the bias in the outcome of such tools. Our study shows that these tools show clear signs of gender bias in the quality of their explanations.



### Learnable Ophthalmology SAM
- **Arxiv ID**: http://arxiv.org/abs/2304.13425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13425v1)
- **Published**: 2023-04-26 10:14:03+00:00
- **Updated**: 2023-04-26 10:14:03+00:00
- **Authors**: Zhongxi Qiu, Yan Hu, Heng Li, Jiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation is vital for ophthalmology image analysis. But its various modal images hinder most of the existing segmentation algorithms applications, as they rely on training based on a large number of labels or hold weak generalization ability. Based on Segment Anything (SAM), we propose a simple but effective learnable prompt layer suitable for multiple target segmentation in ophthalmology multi-modal images, named Learnable Ophthalmology Segment Anything (SAM). The learnable prompt layer learns medical prior knowledge from each transformer layer. During training, we only train the prompt layer and task head based on a one-shot mechanism. We demonstrate the effectiveness of our thought based on four medical segmentation tasks based on nine publicly available datasets. Moreover, we only provide a new improvement thought for applying the existing fundamental CV models in the medical field. Our codes are available at \href{https://github.com/Qsingle/LearnablePromptSAM}{website}.



### Training-Free Location-Aware Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2304.13427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13427v1)
- **Published**: 2023-04-26 10:25:15+00:00
- **Updated**: 2023-04-26 10:25:15+00:00
- **Authors**: Jiafeng Mao, Xueting Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Current large-scale generative models have impressive efficiency in generating high-quality images based on text prompts. However, they lack the ability to precisely control the size and position of objects in the generated image. In this study, we analyze the generative mechanism of the stable diffusion model and propose a new interactive generation paradigm that allows users to specify the position of generated objects without additional training. Moreover, we propose an object detection-based evaluation metric to assess the control capability of location aware generation task. Our experimental results show that our method outperforms state-of-the-art methods on both control capacity and image quality.



### Compensation Learning in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.13428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13428v1)
- **Published**: 2023-04-26 10:26:11+00:00
- **Updated**: 2023-04-26 10:26:11+00:00
- **Authors**: Timo Kaiser, Christoph Reinders, Bodo Rosenhahn
- **Comment**: 8 pages, 6 figures, 4 tables, Vision Datasets Understanding Workshop
  on CVPR23
- **Journal**: None
- **Summary**: Label noise and ambiguities between similar classes are challenging problems in developing new models and annotating new data for semantic segmentation. In this paper, we propose Compensation Learning in Semantic Segmentation, a framework to identify and compensate ambiguities as well as label noise. More specifically, we add a ground truth depending and globally learned bias to the classification logits and introduce a novel uncertainty branch for neural networks to induce the compensation bias only to relevant regions. Our method is employed into state-of-the-art segmentation frameworks and several experiments demonstrate that our proposed compensation learns inter-class relations that allow global identification of challenging ambiguities as well as the exact localization of subsequent label noise. Additionally, it enlarges robustness against label noise during training and allows target-oriented manipulation during inference. We evaluate the proposed method on %the widely used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.



### Neural-PBIR Reconstruction of Shape, Material, and Illumination
- **Arxiv ID**: http://arxiv.org/abs/2304.13445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13445v3)
- **Published**: 2023-04-26 11:02:04+00:00
- **Updated**: 2023-08-17 04:16:21+00:00
- **Authors**: Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng Zhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, Zhao Dong
- **Comment**: ICCV 2023. Project page at https://neural-pbir.github.io/
- **Journal**: None
- **Summary**: Reconstructing the shape and spatially varying surface appearances of a physical-world object as well as its surrounding illumination based on 2D images (e.g., photographs) of the object has been a long-standing problem in computer vision and graphics. In this paper, we introduce an accurate and highly efficient object reconstruction pipeline combining neural based object reconstruction and physics-based inverse rendering (PBIR). Our pipeline firstly leverages a neural SDF based shape reconstruction to produce high-quality but potentially imperfect object shape. Then, we introduce a neural material and lighting distillation stage to achieve high-quality predictions for material and illumination. In the last stage, initialized by the neural predictions, we perform PBIR to refine the initial results and obtain the final high-quality reconstruction of object shape, material, and illumination. Experimental results demonstrate our pipeline significantly outperforms existing methods quality-wise and performance-wise.



### From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection
- **Arxiv ID**: http://arxiv.org/abs/2304.13455v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13455v4)
- **Published**: 2023-04-26 11:27:34+00:00
- **Updated**: 2023-08-30 19:44:41+00:00
- **Authors**: Nikola Zubić, Daniel Gehrig, Mathias Gehrig, Davide Scaramuzza
- **Comment**: 15 pages, 11 figures, 2 tables, ICCV 2023 Camera Ready paper
- **Journal**: None
- **Summary**: Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. This work eliminates this bottleneck by selecting representations based on the Gromov-Wasserstein Discrepancy (GWD) between raw events and their representation. It is about 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, datasets, and tasks. Thus finding representations with high task scores is equivalent to finding representations with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family of event representations, revealing new and powerful representations that exceed the state-of-the-art. Our optimized representations outperform existing representations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1 dataset, two established object detection benchmarks, and reach a 3.8% higher classification score on the mini N-ImageNet benchmark. Moreover, we outperform state-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methods by 6.0 mAP on the 1 Mpx datasets. This work opens a new unexplored field of explicit representation optimization for event-based learning.



### OPDN: Omnidirectional Position-aware Deformable Network for Omnidirectional Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2304.13471v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13471v1)
- **Published**: 2023-04-26 11:47:40+00:00
- **Updated**: 2023-04-26 11:47:40+00:00
- **Authors**: Xiaopeng Sun, Weiqi Li, Zhenyu Zhang, Qiufang Ma, Xuhan Sheng, Ming Cheng, Haoyu Ma, Shijie Zhao, Jian Zhang, Junlin Li, Li Zhang
- **Comment**: Accepted to CVPRW 2023
- **Journal**: None
- **Summary**: 360{\deg} omnidirectional images have gained research attention due to their immersive and interactive experience, particularly in AR/VR applications. However, they suffer from lower angular resolution due to being captured by fisheye lenses with the same sensor size for capturing planar images. To solve the above issues, we propose a two-stage framework for 360{\deg} omnidirectional image superresolution. The first stage employs two branches: model A, which incorporates omnidirectional position-aware deformable blocks (OPDB) and Fourier upsampling, and model B, which adds a spatial frequency fusion module (SFF) to model A. Model A aims to enhance the feature extraction ability of 360{\deg} image positional information, while Model B further focuses on the high-frequency information of 360{\deg} images. The second stage performs same-resolution enhancement based on the structure of model A with a pixel unshuffle operation. In addition, we collected data from YouTube to improve the fitting ability of the transformer, and created pseudo low-resolution images using a degradation network. Our proposed method achieves superior performance and wins the NTIRE 2023 challenge of 360{\deg} omnidirectional image super-resolution.



### Effect of latent space distribution on the segmentation of images with multiple annotations
- **Arxiv ID**: http://arxiv.org/abs/2304.13476v1
- **DOI**: 10.59275/j.melba.2023-18ae
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13476v1)
- **Published**: 2023-04-26 12:00:00+00:00
- **Updated**: 2023-04-26 12:00:00+00:00
- **Authors**: Ishaan Bhat, Josien P. W. Pluim, Max A. Viergever, Hugo J. Kuijf
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:005. arXiv admin
  note: text overlap with arXiv:2207.12872
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 2 (2023)
- **Summary**: We propose the Generalized Probabilistic U-Net, which extends the Probabilistic U-Net by allowing more general forms of the Gaussian distribution as the latent space distribution that can better approximate the uncertainty in the reference segmentations. We study the effect the choice of latent space distribution has on capturing the variation in the reference segmentations for lung tumors and white matter hyperintensities in the brain. We show that the choice of distribution affects the sample diversity of the predictions and their overlap with respect to the reference segmentations. We have made our implementation available at https://github.com/ishaanb92/GeneralizedProbabilisticUNet



### Mixing Data Augmentation with Preserving Foreground Regions in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.13490v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13490v1)
- **Published**: 2023-04-26 12:29:54+00:00
- **Updated**: 2023-04-26 12:29:54+00:00
- **Authors**: Xiaoqing Liu, Kenji Ono, Ryoma Bise
- **Comment**: Accepted by IEEE ISBI'23
- **Journal**: None
- **Summary**: The development of medical image segmentation using deep learning can significantly support doctors' diagnoses. Deep learning needs large amounts of data for training, which also requires data augmentation to extend diversity for preventing overfitting. However, the existing methods for data augmentation of medical image segmentation are mainly based on models which need to update parameters and cost extra computing resources. We proposed data augmentation methods designed to train a high accuracy deep learning network for medical image segmentation. The proposed data augmentation approaches are called KeepMask and KeepMix, which can create medical images by better identifying the boundary of the organ with no more parameters. Our methods achieved better performance and obtained more precise boundaries for medical image segmentation on datasets. The dice coefficient of our methods achieved 94.15% (3.04% higher than baseline) on CHAOS and 74.70% (5.25% higher than baseline) on MSD spleen with Unet.



### Automated Whole Slide Imaging for Label-Free Histology using Photon Absorption Remote Sensing Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2304.13736v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.SY, eess.SY, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2304.13736v2)
- **Published**: 2023-04-26 12:36:19+00:00
- **Updated**: 2023-05-16 20:39:22+00:00
- **Authors**: James E. D. Tweel, Benjamin R. Ecclestone, Marian Boktor, Deepak Dinakaran, John R. Mackey, Parsin Haji Reza
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: The field of histology relies heavily on antiquated tissue processing and staining techniques that limit the efficiency of pathologic diagnoses of cancer and other diseases. Current staining and advanced labeling methods are often destructive and mutually incompatible, requiring new tissue sections for each stain. This prolongs the diagnostic process and depletes valuable biopsy samples. In this study, we present an alternative label-free histology platform using the first transmission-mode Photon Absorption Remote Sensing microscope. Optimized for automated whole slide scanning of unstained tissue samples, the system provides slide images at magnifications up to 40x that are fully compatible with existing digital pathology tools. The scans capture high quality and high-resolution images with subcellular diagnostic detail. After imaging, samples remain suitable for histochemical, immunohistochemical, and other staining techniques. Scattering and absorption (radiative and non-radiative) contrasts are shown in whole slide images of malignant human breast and skin tissues samples. Clinically relevant features are highlighted, and close correspondence and analogous contrast is demonstrated with one-to-one gold standard H&E stained images. Our previously reported pix2pix virtual staining model is applied to an entire whole slide image, showcasing the potential of this approach in whole slide label-free H&E emulation. This work is a critical advance for integrating label-free optical methods into standard histopathology workflows, both enhancing diagnostic efficiency, and broadening the number of stains that can be applied while preserving valuable tissue samples.



### EasyPortrait -- Face Parsing and Portrait Segmentation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2304.13509v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13509v2)
- **Published**: 2023-04-26 12:51:34+00:00
- **Updated**: 2023-05-02 05:32:50+00:00
- **Authors**: Alexander Kapitanov, Karina Kvanchiani, Sofia Kirillova
- **Comment**: portrait segmentation, face parsing, image segmentation dataset
- **Journal**: None
- **Summary**: Recently, due to COVID-19 and the growing demand for remote work, video conferencing apps have become especially widespread. The most valuable features of video chats are real-time background removal and face beautification. While solving these tasks, computer vision researchers face the problem of having relevant data for the training stage. There is no large dataset with high-quality labeled and diverse images of people in front of a laptop or smartphone camera to train a lightweight model without additional approaches. To boost the progress in this area, we provide a new image dataset, EasyPortrait, for portrait segmentation and face parsing tasks. It contains 20,000 primarily indoor photos of 8,377 unique users, and fine-grained segmentation masks separated into 9 classes. Images are collected and labeled from crowdsourcing platforms. Unlike most face parsing datasets, in EasyPortrait, the beard is not considered part of the skin mask, and the inside area of the mouth is separated from the teeth. These features allow using EasyPortrait for skin enhancement and teeth whitening tasks. This paper describes the pipeline for creating a large-scale and clean image segmentation dataset using crowdsourcing platforms without additional synthetic data. Moreover, we trained several models on EasyPortrait and showed experimental results. Proposed dataset and trained models are publicly available.



### Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.13513v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13513v1)
- **Published**: 2023-04-26 12:53:41+00:00
- **Updated**: 2023-04-26 12:53:41+00:00
- **Authors**: Xiaoqing Liu, Kengo Araki, Shota Harada, Akihiko Yoshizawa, Kazuhiro Terada, Mariyo Kurata, Naoki Nakajima, Hiroyuki Abe, Tetsuo Ushiku, Ryoma Bise
- **Comment**: Accepted by IEEE ISBI'23
- **Journal**: None
- **Summary**: The domain shift in pathological segmentation is an important problem, where a network trained by a source domain (collected at a specific hospital) does not work well in the target domain (from different hospitals) due to the different image features. Due to the problems of class imbalance and different class prior of pathology, typical unsupervised domain adaptation methods do not work well by aligning the distribution of source domain and target domain. In this paper, we propose a cluster entropy for selecting an effective whole slide image (WSI) that is used for semi-supervised domain adaptation. This approach can measure how the image features of the WSI cover the entire distribution of the target domain by calculating the entropy of each cluster and can significantly improve the performance of domain adaptation. Our approach achieved competitive results against the prior arts on datasets collected from two hospitals.



### Super-NeRF: View-consistent Detail Generation for NeRF super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2304.13518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13518v1)
- **Published**: 2023-04-26 12:54:40+00:00
- **Updated**: 2023-04-26 12:54:40+00:00
- **Authors**: Yuqi Han, Tao Yu, Xiaohang Yu, Yuwang Wang, Qionghai Dai
- **Comment**: None
- **Journal**: None
- **Summary**: The neural radiance field (NeRF) achieved remarkable success in modeling 3D scenes and synthesizing high-fidelity novel views. However, existing NeRF-based methods focus more on the make full use of the image resolution to generate novel views, but less considering the generation of details under the limited input resolution. In analogy to the extensive usage of image super-resolution, NeRF super-resolution is an effective way to generate the high-resolution implicit representation of 3D scenes and holds great potential applications. Up to now, such an important topic is still under-explored. In this paper, we propose a NeRF super-resolution method, named Super-NeRF, to generate high-resolution NeRF from only low-resolution inputs. Given multi-view low-resolution images, Super-NeRF constructs a consistency-controlling super-resolution module to generate view-consistent high-resolution details for NeRF. Specifically, an optimizable latent code is introduced for each low-resolution input image to control the 2D super-resolution images to converge to the view-consistent output. The latent codes of each low-resolution image are optimized synergistically with the target Super-NeRF representation to fully utilize the view consistency constraint inherent in NeRF construction. We verify the effectiveness of Super-NeRF on synthetic, real-world, and AI-generated NeRF datasets. Super-NeRF achieves state-of-the-art NeRF super-resolution performance on high-resolution detail generation and cross-view consistency.



### Key-value information extraction from full handwritten pages
- **Arxiv ID**: http://arxiv.org/abs/2304.13530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2304.13530v1)
- **Published**: 2023-04-26 13:06:55+00:00
- **Updated**: 2023-04-26 13:06:55+00:00
- **Authors**: Solène Tarride, Mélodie Boillet, Christopher Kermorvant
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.



### Tissue Classification During Needle Insertion Using Self-Supervised Contrastive Learning and Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2304.13574v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13574v1)
- **Published**: 2023-04-26 14:11:04+00:00
- **Updated**: 2023-04-26 14:11:04+00:00
- **Authors**: Debayan Bhattacharya, Sarah Latus, Finn Behrendt, Florin Thimm, Dennis Eggert, Christian Betz, Alexander Schlaefer
- **Comment**: None
- **Journal**: None
- **Summary**: Needle positioning is essential for various medical applications such as epidural anaesthesia. Physicians rely on their instincts while navigating the needle in epidural spaces. Thereby, identifying the tissue structures may be helpful to the physician as they can provide additional feedback in the needle insertion process. To this end, we propose a deep neural network that classifies the tissues from the phase and intensity data of complex OCT signals acquired at the needle tip. We investigate the performance of the deep neural network in a limited labelled dataset scenario and propose a novel contrastive pretraining strategy that learns invariant representation for phase and intensity data. We show that with 10% of the training set, our proposed pretraining strategy helps the model achieve an F1 score of 0.84 whereas the model achieves an F1 score of 0.60 without it. Further, we analyse the importance of phase and intensity individually towards tissue classification.



### Multi-Modality Deep Network for Extreme Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2304.13583v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13583v1)
- **Published**: 2023-04-26 14:22:59+00:00
- **Updated**: 2023-04-26 14:22:59+00:00
- **Authors**: Xuhao Jiang, Weimin Tan, Tian Tan, Bo Yan, Liquan Shen
- **Comment**: 13 pages, 14 figures, accepted by AAAI 2023
- **Journal**: None
- **Summary**: Image-based single-modality compression learning approaches have demonstrated exceptionally powerful encoding and decoding capabilities in the past few years , but suffer from blur and severe semantics loss at extremely low bitrates. To address this issue, we propose a multimodal machine learning method for text-guided image compression, in which the semantic information of text is used as prior information to guide image compression for better compression performance. We fully study the role of text description in different components of the codec, and demonstrate its effectiveness. In addition, we adopt the image-text attention module and image-request complement module to better fuse image and text features, and propose an improved multimodal semantic-consistent loss to produce semantically complete reconstructions. Extensive experiments, including a user study, prove that our method can obtain visually pleasing results at extremely low bitrates, and achieves a comparable or even better performance than state-of-the-art methods, even though these methods are at 2x to 4x bitrates of ours.



### Energy-Based Sliced Wasserstein Distance
- **Arxiv ID**: http://arxiv.org/abs/2304.13586v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13586v1)
- **Published**: 2023-04-26 14:28:45+00:00
- **Updated**: 2023-04-26 14:28:45+00:00
- **Authors**: Khai Nguyen, Nhat Ho
- **Comment**: 36 pages, 7 figures, 6 tables
- **Journal**: None
- **Summary**: The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter-free and has the density proportional to an energy function of the projected one-dimensional Wasserstein distance. We then derive a novel sliced Wasserstein metric, energy-based sliced Waserstein (EBSW) distance, and investigate its topological, statistical, and computational properties via importance sampling, sampling importance resampling, and Markov Chain methods. Finally, we conduct experiments on point-cloud gradient flow, color transfer, and point-cloud reconstruction to show the favorable performance of the EBSW.



### Synthetic Aperture Anomaly Imaging
- **Arxiv ID**: http://arxiv.org/abs/2304.13590v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13590v1)
- **Published**: 2023-04-26 14:34:43+00:00
- **Updated**: 2023-04-26 14:34:43+00:00
- **Authors**: Rakesh John Amala Arokia Nathan, Oliver Bimber
- **Comment**: None
- **Journal**: None
- **Summary**: Previous research has shown that in the presence of foliage occlusion, anomaly detection performs significantly better in integral images resulting from synthetic aperture imaging compared to applying it to conventional aerial images. In this article, we hypothesize and demonstrate that integrating detected anomalies is even more effective than detecting anomalies in integrals. This results in enhanced occlusion removal, outlier suppression, and higher chances of visually as well as computationally detecting targets that are otherwise occluded. Our hypothesis was validated through both: simulations and field experiments. We also present a real-time application that makes our findings practically available for blue-light organizations and others using commercial drone platforms. It is designed to address use-cases that suffer from strong occlusion caused by vegetation, such as search and rescue, wildlife observation, early wildfire detection, and sur-veillance.



### Video Frame Interpolation with Densely Queried Bilateral Correlation
- **Arxiv ID**: http://arxiv.org/abs/2304.13596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13596v1)
- **Published**: 2023-04-26 14:45:09+00:00
- **Updated**: 2023-04-26 14:45:09+00:00
- **Authors**: Chang Zhou, Jie Liu, Jie Tang, Gangshan Wu
- **Comment**: Accepted by IJCAI 2023
- **Journal**: None
- **Summary**: Video Frame Interpolation (VFI) aims to synthesize non-existent intermediate frames between existent frames. Flow-based VFI algorithms estimate intermediate motion fields to warp the existent frames. Real-world motions' complexity and the reference frame's absence make motion estimation challenging. Many state-of-the-art approaches explicitly model the correlations between two neighboring frames for more accurate motion estimation. In common approaches, the receptive field of correlation modeling at higher resolution depends on the motion fields estimated beforehand. Such receptive field dependency makes common motion estimation approaches poor at coping with small and fast-moving objects. To better model correlations and to produce more accurate motion fields, we propose the Densely Queried Bilateral Correlation (DQBC) that gets rid of the receptive field dependency problem and thus is more friendly to small and fast-moving objects. The motion fields generated with the help of DQBC are further refined and up-sampled with context features. After the motion fields are fixed, a CNN-based SynthNet synthesizes the final interpolated frame. Experiments show that our approach enjoys higher accuracy and less inference time than the state-of-the-art. Source code is available at https://github.com/kinoud/DQBC.



### SIMARA: a database for key-value information extraction from full pages
- **Arxiv ID**: http://arxiv.org/abs/2304.13606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2304.13606v1)
- **Published**: 2023-04-26 15:00:04+00:00
- **Updated**: 2023-04-26 15:00:04+00:00
- **Authors**: Solène Tarride, Mélodie Boillet, Jean-François Moufflet, Christopher Kermorvant
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new database for information extraction from historical handwritten documents. The corpus includes 5,393 finding aids from six different series, dating from the 18th-20th centuries. Finding aids are handwritten documents that contain metadata describing older archives. They are stored in the National Archives of France and are used by archivists to identify and find archival documents. Each document is annotated at page-level, and contains seven fields to retrieve. The localization of each field is not available in such a way that this dataset encourages research on segmentation-free systems for information extraction. We propose a model based on the Transformer architecture trained for end-to-end information extraction and provide three sets for training, validation and testing, to ensure fair comparison with future works. The database is freely accessible at https://zenodo.org/record/7868059.



### Multi-View Stereo Representation Revisit: Region-Aware MVSNet
- **Arxiv ID**: http://arxiv.org/abs/2304.13614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13614v2)
- **Published**: 2023-04-26 15:17:51+00:00
- **Updated**: 2023-04-27 07:08:37+00:00
- **Authors**: Yisu Zhang, Jianke Zhu, Lixiang Lin
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Deep learning-based multi-view stereo has emerged as a powerful paradigm for reconstructing the complete geometrically-detailed objects from multi-views. Most of the existing approaches only estimate the pixel-wise depth value by minimizing the gap between the predicted point and the intersection of ray and surface, which usually ignore the surface topology. It is essential to the textureless regions and surface boundary that cannot be properly reconstructed. To address this issue, we suggest to take advantage of point-to-surface distance so that the model is able to perceive a wider range of surfaces. To this end, we predict the distance volume from cost volume to estimate the signed distance of points around the surface. Our proposed RA-MVSNet is patch-awared, since the perception range is enhanced by associating hypothetical planes with a patch of surface. Therefore, it could increase the completion of textureless regions and reduce the outliers at the boundary. Moreover, the mesh topologies with fine details can be generated by the introduced distance volume. Comparing to the conventional deep learning-based multi-view stereo methods, our proposed RA-MVSNet approach obtains more complete reconstruction results by taking advantage of signed distance supervision. The experiments on both the DTU and Tanks \& Temples datasets demonstrate that our proposed approach achieves the state-of-the-art results.



### Domain Adaptive and Generalizable Network Architectures and Training Strategies for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.13615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13615v1)
- **Published**: 2023-04-26 15:18:45+00:00
- **Updated**: 2023-04-26 15:18:45+00:00
- **Authors**: Lukas Hoyer, Dengxin Dai, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) and domain generalization (DG) enable machine learning models trained on a source domain to perform well on unlabeled or even unseen target domains. As previous UDA&DG semantic segmentation methods are mostly based on outdated networks, we benchmark more recent architectures, reveal the potential of Transformers, and design the DAFormer network tailored for UDA&DG. It is enabled by three training strategies to avoid overfitting to the source domain: While (1) Rare Class Sampling mitigates the bias toward common source domain classes, (2) a Thing-Class ImageNet Feature Distance and (3) a learning rate warmup promote feature transfer from ImageNet pretraining. As UDA&DG are usually GPU memory intensive, most previous methods downscale or crop images. However, low-resolution predictions often fail to preserve fine details while models trained with cropped images fall short in capturing long-range, domain-robust context information. Therefore, we propose HRDA, a multi-resolution framework for UDA&DG, that combines the strengths of small high-resolution crops to preserve fine segmentation details and large low-resolution crops to capture long-range context dependencies with a learned scale attention. DAFormer and HRDA significantly improve the state-of-the-art UDA&DG by more than 10 mIoU on 5 different benchmarks. The implementation is available at https://github.com/lhoyer/HRDA.



### Non-rigid Point Cloud Registration for Middle Ear Diagnostics with Endoscopic Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2304.13618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13618v1)
- **Published**: 2023-04-26 15:20:48+00:00
- **Updated**: 2023-04-26 15:20:48+00:00
- **Authors**: Peng Liu, Jonas Golde, Joseph Morgenstern, Sebastian Bodenstedt, Chenpan Li, Yujia Hu, Zhaoyu Chen, Edmund Koch, Marcus Neudert, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Middle ear infection is the most prevalent inflammatory disease, especially among the pediatric population. Current diagnostic methods are subjective and depend on visual cues from an otoscope, which is limited for otologists to identify pathology. To address this shortcoming, endoscopic optical coherence tomography (OCT) provides both morphological and functional in-vivo measurements of the middle ear. However, due to the shadow of prior structures, interpretation of OCT images is challenging and time-consuming. To facilitate fast diagnosis and measurement, improvement in the readability of OCT data is achieved by merging morphological knowledge from ex-vivo middle ear models with OCT volumetric data, so that OCT applications can be further promoted in daily clinical settings. Methods: We propose C2P-Net: a two-staged non-rigid registration pipeline for complete to partial point clouds, which are sampled from ex-vivo and in-vivo OCT models, respectively. To overcome the lack of labeled training data, a fast and effective generation pipeline in Blender3D is designed to simulate middle ear shapes and extract in-vivo noisy and partial point clouds. Results: We evaluate the performance of C2P-Net through experiments on both synthetic and real OCT datasets. The results demonstrate that C2P-Net is generalized to unseen middle ear point clouds and capable of handling realistic noise and incompleteness in synthetic and real OCT data. Conclusion: In this work, we aim to enable diagnosis of middle ear structures with the assistance of OCT images. We propose C2P-Net: a two-staged non-rigid registration pipeline for point clouds to support the interpretation of in-vivo noisy and partial OCT images for the first time. Code is available at: https://gitlab.com/nct\_tso\_public/c2p-net.



### An Edge Assisted Robust Smart Traffic Management and Signalling System for Guiding Emergency Vehicles During Peak Hours
- **Arxiv ID**: http://arxiv.org/abs/2304.14924v2
- **DOI**: 10.1007/978-981-99-3478-2_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.14924v2)
- **Published**: 2023-04-26 15:31:38+00:00
- **Updated**: 2023-05-02 11:32:15+00:00
- **Authors**: Shuvadeep Masanta, Ramyashree Pramanik, Sourav Ghosh, Tanmay Bhattacharya
- **Comment**: Accepted at the Doctoral Symposium on Human Centered Computing (HUMAN
  2023), February 25, 2023. To be published in Springer Tracts in
  Human-Centered Computing, Book Title: Intelligent Human Centered Computing;
  see https://link.springer.com/book/9789819934775
- **Journal**: Intelligent Human Centered Computing. Human 2023. Springer Tracts
  in Human-Centered Computing. Springer, Singapore. 2023. pp. 337-346
- **Summary**: Congestion in traffic is an unavoidable circumstance in many cities in India and other countries. It is an issue of major concern. The steep rise in the number of automobiles on the roads followed by old infrastructure, accidents, pedestrian traffic, and traffic rule violations all add to challenging traffic conditions. Given these poor conditions of traffic, there is a critical need for automatically detecting and signaling systems. There are already various technologies that are used for traffic management and signaling systems like video analysis, infrared sensors, and wireless sensors. The main issue with these methods is they are very costly and high maintenance is required. In this paper, we have proposed a three-phase system that can guide emergency vehicles and manage traffic based on the degree of congestion. In the first phase, the system processes the captured images and calculates the Index value which is used to discover the degree of congestion. The Index value of a particular road depends on its width and the length up to which the camera captures images of that road. We have to take input for the parameters (length and width) while setting up the system. In the second phase, the system checks whether there are any emergency vehicles present or not in any lane. In the third phase, the whole processing and decision-making part is performed at the edge server. The proposed model is robust and it takes into consideration adverse weather conditions such as hazy, foggy, and windy. It works very efficiently in low light conditions also. The edge server is a strategically placed server that provides us with low latency and better connectivity. Using Edge technology in this traffic management system reduces the strain on cloud servers and the system becomes more reliable in real-time because the latency and bandwidth get reduced due to processing at the intermediate edge server.



### HDR-VDP-3: A multi-metric for predicting image differences, quality and contrast distortions in high dynamic range and regular content
- **Arxiv ID**: http://arxiv.org/abs/2304.13625v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2304.13625v1)
- **Published**: 2023-04-26 15:32:04+00:00
- **Updated**: 2023-04-26 15:32:04+00:00
- **Authors**: Rafal K. Mantiuk, Dounia Hammou, Param Hanji
- **Comment**: None
- **Journal**: None
- **Summary**: High-Dynamic-Range Visual-Difference-Predictor version 3, or HDR-VDP-3, is a visual metric that can fulfill several tasks, such as full-reference image/video quality assessment, prediction of visual differences between a pair of images, or prediction of contrast distortions. Here we present a high-level overview of the metric, position it with respect to related work, explain the main differences compared to version 2.2, and describe how the metric was adapted for the HDR Video Quality Measurement Grand Challenge 2023.



### PVP: Pre-trained Visual Parameter-Efficient Tuning
- **Arxiv ID**: http://arxiv.org/abs/2304.13639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13639v1)
- **Published**: 2023-04-26 15:55:29+00:00
- **Updated**: 2023-04-26 15:55:29+00:00
- **Authors**: Zhao Song, Ke Yang, Naiyang Guan, Junjie Zhu, Peng Qiao, Qingyong Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappropriate way of initializing prompt modules, which has also been verified in the pre-trained language models. Next, we propose a Pre-trained Visual Parameter-efficient (PVP) Tuning framework, which pre-trains the parameter-efficient tuning modules first and then leverages the pre-trained modules along with the pre-trained transformer backbone to perform parameter-efficient tuning on downstream tasks. Experiment results on five Fine-Grained Visual Classification (FGVC) and VTAB-1k datasets demonstrate that our proposed method significantly outperforms state-of-the-art PETuning methods.



### A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2304.13649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2304.13649v1)
- **Published**: 2023-04-26 16:14:39+00:00
- **Updated**: 2023-04-26 16:14:39+00:00
- **Authors**: Alireza Salemi, Juan Altmayer Pizzorno, Hamed Zamani
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retrieved passage separately and uses all passages jointly in its decoder. Compared to competitive baselines in the literature, this approach leads to 5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA and FVQA, respectively.



### What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging
- **Arxiv ID**: http://arxiv.org/abs/2304.13651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13651v1)
- **Published**: 2023-04-26 16:23:10+00:00
- **Updated**: 2023-04-26 16:23:10+00:00
- **Authors**: Zitian Tang, Wenjie Ye, Wei-Chiu Ma, Hang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring past human motion from RGB images is challenging due to the inherent uncertainty of the prediction problem. Thermal images, on the other hand, encode traces of past human-object interactions left in the environment via thermal radiation measurement. Based on this observation, we collect the first RGB-Thermal dataset for human motion analysis, dubbed Thermal-IM. Then we develop a three-stage neural network model for accurate past human pose estimation. Comprehensive experiments show that thermal cues significantly reduce the ambiguities of this task, and the proposed model achieves remarkable performance. The dataset is available at https://github.com/ZitianTang/Thermal-IM.



### Noise-Tolerance GPU-based Age Estimation Using ResNet-50
- **Arxiv ID**: http://arxiv.org/abs/2305.00848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.00848v1)
- **Published**: 2023-04-26 16:38:41+00:00
- **Updated**: 2023-04-26 16:38:41+00:00
- **Authors**: Mahtab Taheri, Mahdi Taheri, Amirhossein Hadjahmadi
- **Comment**: 4 pages, 8 Figs, 1 table. 7th International Conference on Reliability
  and Safety Engineering, 2023
- **Journal**: None
- **Summary**: The human face contains important and understandable information such as personal identity, gender, age, and ethnicity. In recent years, a person's age has been studied as one of the important features of the face. The age estimation system consists of a combination of two modules, the presentation of the face image and the extraction of age characteristics, and then the detection of the exact age or age group based on these characteristics. So far, various algorithms have been presented for age estimation, each of which has advantages and disadvantages. In this work, we implemented a deep residual neural network on the UTKFace data set. We validated our implementation by comparing it with the state-of-the-art implementations of different age estimation algorithms and the results show 28.3% improvement in MAE as one of the critical error validation metrics compared to the recent works and also 71.39% MAE improvements compared to the implemented AlexNet. In the end, we show that the performance degradation of our implemented network is lower than 1.5% when injecting 15 dB noise to the input data (5 times more than the normal environmental noise) which justifies the noise tolerance of our proposed method.



### FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.13672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13672v1)
- **Published**: 2023-04-26 16:42:45+00:00
- **Updated**: 2023-04-26 16:42:45+00:00
- **Authors**: Yan Wang, Jian Cheng, Yixin Chen, Shuai Shao, Lanyun Zhu, Zhenzhou Wu, Tao Liu, Haogang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation methods normally perform poorly when there is a domain shift between training and testing data. Unsupervised Domain Adaptation (UDA) addresses the domain shift problem by training the model using both labeled data from the source domain and unlabeled data from the target domain. Source-Free UDA (SFUDA) was recently proposed for UDA without requiring the source data during the adaptation, due to data privacy or data transmission issues, which normally adapts the pre-trained deep model in the testing stage. However, in real clinical scenarios of medical image segmentation, the trained model is normally frozen in the testing stage. In this paper, we propose Fourier Visual Prompting (FVP) for SFUDA of medical image segmentation. Inspired by prompting learning in natural language processing, FVP steers the frozen pre-trained model to perform well in the target domain by adding a visual prompt to the input target data. In FVP, the visual prompt is parameterized using only a small amount of low-frequency learnable parameters in the input frequency space, and is learned by minimizing the segmentation loss between the predicted segmentation of the prompted target image and reliable pseudo segmentation label of the target image under the frozen model. To our knowledge, FVP is the first work to apply visual prompts to SFUDA for medical image segmentation. The proposed FVP is validated using three public datasets, and experiments demonstrate that FVP yields better segmentation results, compared with various existing methods.



### A marker-less human motion analysis system for motion-based biomarker discovery in knee disorders
- **Arxiv ID**: http://arxiv.org/abs/2304.13678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13678v1)
- **Published**: 2023-04-26 16:47:42+00:00
- **Updated**: 2023-04-26 16:47:42+00:00
- **Authors**: Kai Armstrong, Lei Zhang, Yan Wen, Alexander P. Willmott, Paul Lee, Xujioing Ye
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: In recent years the NHS has been having increased difficulty seeing all low-risk patients, this includes but not limited to suspected osteoarthritis (OA) patients. To help address the increased waiting lists and shortages of staff, we propose a novel method of automated biomarker identification for diagnosis of knee disorders and the monitoring of treatment progression. The proposed method allows for the measurement and analysis of biomechanics and analyse their clinical significance, in both a cheap and sensitive alternative to the currently available commercial alternatives. These methods and results validate the capabilities of standard RGB cameras in clinical environments to capture motion and show that when compared to alternatives such as depth cameras there is a comparable accuracy in the clinical environment. Biomarker identification using Principal Component Analysis (PCA) allows the reduction of the dimensionality to produce the most representative features from motion data, these new biomarkers can then be used to assess the success of treatment and track the progress of rehabilitation. This was validated by applying these techniques on a case study utilising the exploratory use of local anaesthetic applied on knee pain, this allows these new representative biomarkers to be validated as statistically significant (p-value < 0.05).



### Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2304.13681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13681v1)
- **Published**: 2023-04-26 16:54:10+00:00
- **Updated**: 2023-04-26 16:54:10+00:00
- **Authors**: Eric Ming Chen, Sidhanth Holalkere, Ruyu Yan, Kai Zhang, Abe Davis
- **Comment**: Project page at https://ray-cond.github.io/
- **Journal**: None
- **Summary**: Multi-view image generation attracts particular attention these days due to its promising 3D-related applications, e.g., image viewpoint editing. Most existing methods follow a paradigm where a 3D representation is first synthesized, and then rendered into 2D images to ensure photo-consistency across viewpoints. However, such explicit bias for photo-consistency sacrifices photo-realism, causing geometry artifacts and loss of fine-scale details when these methods are applied to edit real images. To address this issue, we propose ray conditioning, a geometry-free alternative that relaxes the photo-consistency constraint. Our method generates multi-view images by conditioning a 2D GAN on a light field prior. With explicit viewpoint control, state-of-the-art photo-realism and identity consistency, our method is particularly suited for the viewpoint editing task.



### UniNeXt: Exploring A Unified Architecture for Vision Recognition
- **Arxiv ID**: http://arxiv.org/abs/2304.13700v3
- **DOI**: 10.1145/3581783.3612260
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13700v3)
- **Published**: 2023-04-26 17:28:09+00:00
- **Updated**: 2023-08-17 06:37:52+00:00
- **Authors**: Fangjian Lin, Jianlong Yuan, Sitong Wu, Fan Wang, Zhibin Wang
- **Comment**: Accep by ACM MM 2023
- **Journal**: None
- **Summary**: Vision Transformers have shown great potential in computer vision tasks. Most recent works have focused on elaborating the spatial token mixer for performance gains. However, we observe that a well-designed general architecture can significantly improve the performance of the entire backbone, regardless of which spatial token mixer is equipped. In this paper, we propose UniNeXt, an improved general architecture for the vision backbone. To verify its effectiveness, we instantiate the spatial token mixer with various typical and modern designs, including both convolution and attention modules. Compared with the architecture in which they are first proposed, our UniNeXt architecture can steadily boost the performance of all the spatial token mixers, and narrows the performance gap among them. Surprisingly, our UniNeXt equipped with naive local window attention even outperforms the previous state-of-the-art. Interestingly, the ranking of these spatial token mixers also changes under our UniNeXt, suggesting that an excellent spatial token mixer may be stifled due to a suboptimal general architecture, which further shows the importance of the study on the general architecture of vision backbone.



### Controllable Image Generation via Collage Representations
- **Arxiv ID**: http://arxiv.org/abs/2304.13722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13722v1)
- **Published**: 2023-04-26 17:58:39+00:00
- **Updated**: 2023-04-26 17:58:39+00:00
- **Authors**: Arantxa Casanova, Marlène Careil, Adriana Romero-Soriano, Christopher J. Pal, Jakob Verbeek, Michal Drozdzal
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in conditional generative image models have enabled impressive results. On the one hand, text-based conditional models have achieved remarkable generation quality, by leveraging large-scale datasets of image-text pairs. To enable fine-grained controllability, however, text-based models require long prompts, whose details may be ignored by the model. On the other hand, layout-based conditional models have also witnessed significant advances. These models rely on bounding boxes or segmentation maps for precise spatial conditioning in combination with coarse semantic labels. The semantic labels, however, cannot be used to express detailed appearance characteristics. In this paper, we approach fine-grained scene controllability through image collages which allow a rich visual description of the desired scene as well as the appearance and location of the objects therein, without the need of class nor attribute labels. We introduce "mixing and matching scenes" (M&Ms), an approach that consists of an adversarially trained generative image model which is conditioned on appearance features and spatial positions of the different elements in a collage, and integrates these into a coherent image. We train our model on the OpenImages (OI) dataset and evaluate it on collages derived from OI and MS-COCO datasets. Our experiments on the OI dataset show that M&Ms outperforms baselines in terms of fine-grained scene controllability while being very competitive in terms of image quality and sample diversity. On the MS-COCO dataset, we highlight the generalization ability of our model by outperforming DALL-E in terms of the zero-shot FID metric, despite using two magnitudes fewer parameters and data. Collage based generative models have the potential to advance content creation in an efficient and effective way as they are intuitive to use and yield high quality generations.



### A Control-Centric Benchmark for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2304.13723v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2304.13723v1)
- **Published**: 2023-04-26 17:59:45+00:00
- **Updated**: 2023-04-26 17:59:45+00:00
- **Authors**: Stephen Tian, Chelsea Finn, Jiajun Wu
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction trajectories for each task category. A central design goal of our benchmark is to expose a simple interface -- a single forward prediction call -- so it is straightforward to evaluate almost any action-conditioned video prediction model. We then leverage our benchmark to study the effects of scaling model size, quantity of training data, and model ensembling by analyzing five highly-performant video prediction models, finding that while scale can improve perceptual quality when modeling visually diverse settings, other attributes such as uncertainty awareness can also aid planning performance.



### Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis
- **Arxiv ID**: http://arxiv.org/abs/2304.13764v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13764v1)
- **Published**: 2023-04-26 18:10:35+00:00
- **Updated**: 2023-04-26 18:10:35+00:00
- **Authors**: Mehdi Ounissi, Morwena Latouche, Daniel Racoceanu
- **Comment**: None
- **Journal**: None
- **Summary**: Quantifying the phagocytosis of dynamic, unstained cells is essential for evaluating neurodegenerative diseases. However, measuring rapid cell interactions and distinguishing cells from backgrounds make this task challenging when processing time-lapse phase-contrast video microscopy. In this study, we introduce a fully automated, scalable, and versatile realtime framework for quantifying and analyzing phagocytic activity. Our proposed pipeline can process large data-sets and includes a data quality verification module to counteract potential perturbations such as microscope movements and frame blurring. We also propose an explainable cell segmentation module to improve the interpretability of deep learning methods compared to black-box algorithms. This includes two interpretable deep learning capabilities: visual explanation and model simplification. We demonstrate that interpretability in deep learning is not the opposite of high performance, but rather provides essential deep learning algorithm optimization insights and solutions. Incorporating interpretable modules results in an efficient architecture design and optimized execution time. We apply this pipeline to quantify and analyze microglial cell phagocytosis in frontotemporal dementia (FTD) and obtain statistically reliable results showing that FTD mutant cells are larger and more aggressive than control cells. To stimulate translational approaches and future research, we release an open-source pipeline and a unique microglial cells phagocytosis dataset for immune system characterization in neurodegenerative diseases research. This pipeline and dataset will consistently crystallize future advances in this field, promoting the development of efficient and effective interpretable algorithms dedicated to this critical domain. https://github.com/ounissimehdi/PhagoStat



### Automated Classification of Stroke Blood Clot Origin using Whole-Slide Digital Pathology Images
- **Arxiv ID**: http://arxiv.org/abs/2304.13775v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13775v1)
- **Published**: 2023-04-26 18:46:26+00:00
- **Updated**: 2023-04-26 18:46:26+00:00
- **Authors**: Koushik Sivarama Krishnan, P. J. Joe Nikesh, M. Logeshwaran, G. Senthilkumar, D. Elangovan
- **Comment**: None
- **Journal**: None
- **Summary**: The classification of the origin of blood clots is a crucial step in diagnosing and treating ischemic stroke. Various imaging techniques such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound have been employed to detect and locate blood clots within the body. However, identifying the origin of a blood clot remains challenging due to the complexity of the blood flow dynamics and the limitations of the imaging techniques. The study suggests a novel methodology for classifying the source of a blood clot through the integration of data from whole-slide digital pathology images, which are utilized to fine-tune several cutting-edge computer vision models. Upon comparison, the SwinTransformerV2 model outperforms all the other models and achieves an accuracy score of 94.24%, precision score of 94.41%, recall score of 94.09%, and, f1-score of 94.06%. Our approach shows promising results in detecting the origin of blood clots in different vascular regions and can potentially improve the diagnosis and management of ischemic stroke.



### Customized Segment Anything Model for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2304.13785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13785v1)
- **Published**: 2023-04-26 19:05:34+00:00
- **Updated**: 2023-04-26 19:05:34+00:00
- **Authors**: Kaidong Zhang, Dong Liu
- **Comment**: Technical report, 14 pages
- **Journal**: None
- **Summary**: We propose SAMed, a general solution for medical image segmentation. Different from the previous methods, SAMed is built upon the large-scale image segmentation model, Segment Anything Model (SAM), to explore the new research paradigm of customizing large-scale models for medical image segmentation. SAMed applies the low-rank-based (LoRA) finetuning strategy to the SAM image encoder and finetunes it together with the prompt encoder and the mask decoder on labeled medical image segmentation datasets. We also observe the warmup finetuning strategy and the AdamW optimizer lead SAMed to successful convergence and lower loss. Different from SAM, SAMed could perform semantic segmentation on medical images. Our trained SAMed model achieves 81.88 DSC and 20.64 HD on the Synapse multi-organ segmentation dataset, which is on par with the state-of-the-art methods. We conduct extensive experiments to validate the effectiveness of our design. Since SAMed only updates a small fraction of the SAM parameters, its deployment cost and storage cost are quite marginal in practical usage. The code of SAMed is available at https://github.com/hitachinsk/SAMed.



### Latent Fingerprint Recognition: Fusion of Local and Global Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2304.13800v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13800v1)
- **Published**: 2023-04-26 19:42:57+00:00
- **Updated**: 2023-04-26 19:42:57+00:00
- **Authors**: Steven A. Grosz, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most challenging problems in fingerprint recognition continues to be establishing the identity of a suspect associated with partial and smudgy fingerprints left at a crime scene (i.e., latent prints or fingermarks). Despite the success of fixed-length embeddings for rolled and slap fingerprint recognition, the features learned for latent fingerprint matching have mostly been limited to local minutiae-based embeddings and have not directly leveraged global representations for matching. In this paper, we combine global embeddings with local embeddings for state-of-the-art latent to rolled matching accuracy with high throughput. The combination of both local and global representations leads to improved recognition accuracy across NIST SD 27, NIST SD 302, MSP, MOLF DB1/DB4, and MOLF DB2/DB4 latent fingerprint datasets for both closed-set (84.11%, 54.36%, 84.35%, 70.43%, 62.86% rank-1 retrieval rate, respectively) and open-set (0.50, 0.74, 0.44, 0.60, 0.68 FNIR at FPIR=0.02, respectively) identification scenarios on a gallery of 100K rolled fingerprints. Not only do we fuse the complimentary representations, we also use the local features to guide the global representations to focus on discriminatory regions in two fingerprint images to be compared. This leads to a multi-stage matching paradigm in which subsets of the retrieved candidate lists for each probe image are passed to subsequent stages for further processing, resulting in a considerable reduction in latency (requiring just 0.068 ms per latent to rolled comparison on a AMD EPYC 7543 32-Core Processor, roughly 15K comparisons per second). Finally, we show the generalizability of the fused representations for improving authentication accuracy across several rolled, plain, and contactless fingerprint datasets.



### MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.13819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13819v1)
- **Published**: 2023-04-26 20:42:40+00:00
- **Updated**: 2023-04-26 20:42:40+00:00
- **Authors**: Jiaze Sun, Zhixiang Chen, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: 3D pose transfer is a challenging generation task that aims to transfer the pose of a source geometry onto a target geometry with the target identity preserved. Many prior methods require keypoint annotations to find correspondence between the source and target. Current pose transfer methods allow end-to-end correspondence learning but require the desired final output as ground truth for supervision. Unsupervised methods have been proposed for graph convolutional models but they require ground truth correspondence between the source and target inputs. We present a novel self-supervised framework for 3D pose transfer which can be trained in unsupervised, semi-supervised, or fully supervised settings without any correspondence labels. We introduce two contrastive learning constraints in the latent space: a mesh-level loss for disentangling global patterns including pose and identity, and a point-level loss for discriminating local semantics. We demonstrate quantitatively and qualitatively that our method achieves state-of-the-art results in supervised 3D pose transfer, with comparable results in unsupervised and semi-supervised settings. Our method is also generalisable to unseen human and animal data with complex topologies.



### Programmatically Grounded, Compositionally Generalizable Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2304.13826v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2304.13826v1)
- **Published**: 2023-04-26 20:56:40+00:00
- **Updated**: 2023-04-26 20:56:40+00:00
- **Authors**: Renhao Wang, Jiayuan Mao, Joy Hsu, Hang Zhao, Jiajun Wu, Yang Gao
- **Comment**: ICLR 2023 camera-ready
- **Journal**: None
- **Summary**: Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional pretraining-finetuning pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose ProgramPort, a modular approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities. Each functional module is realized as a combination of deterministic computation and learnable neural networks. Program execution produces parameters to general manipulation primitives for a robotic end-effector. The entire modular network can be trained with end-to-end imitation learning objectives. Experiments show that our model successfully disentangles action and perception, translating to improved zero-shot and compositional generalization in a variety of manipulation behaviors. Project webpage at: \url{https://progport.github.io}.



### On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective
- **Arxiv ID**: http://arxiv.org/abs/2304.13836v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2304.13836v3)
- **Published**: 2023-04-26 21:43:42+00:00
- **Updated**: 2023-05-11 03:27:12+00:00
- **Authors**: Junhwa Song, Keumgang Cha, Junghoon Seo
- **Comment**: Code: https://github.com/SIAnalytics/roar
- **Journal**: None
- **Summary**: Approaches for appraising feature importance approximations, alternatively referred to as attribution methods, have been established across an extensive array of contexts. The development of resilient techniques for performance benchmarking constitutes a critical concern in the sphere of explainable deep learning. This study scrutinizes the dependability of the RemOve-And-Retrain (ROAR) procedure, which is prevalently employed for gauging the performance of feature importance estimates. The insights gleaned from our theoretical foundation and empirical investigations reveal that attributions containing lesser information about the decision function may yield superior results in ROAR benchmarks, contradicting the original intent of ROAR. This occurrence is similarly observed in the recently introduced variant RemOve-And-Debias (ROAD), and we posit a persistent pattern of blurriness bias in ROAR attribution metrics. Our findings serve as a warning against indiscriminate use on ROAR metrics. The code is available as open source.



### GazeSAM: What You See is What You Segment
- **Arxiv ID**: http://arxiv.org/abs/2304.13844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2304.13844v1)
- **Published**: 2023-04-26 22:18:29+00:00
- **Updated**: 2023-04-26 22:18:29+00:00
- **Authors**: Bin Wang, Armstrong Aboah, Zheyuan Zhang, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: This study investigates the potential of eye-tracking technology and the Segment Anything Model (SAM) to design a collaborative human-computer interaction system that automates medical image segmentation. We present the \textbf{GazeSAM} system to enable radiologists to collect segmentation masks by simply looking at the region of interest during image diagnosis. The proposed system tracks radiologists' eye movement and utilizes the eye-gaze data as the input prompt for SAM, which automatically generates the segmentation mask in real time. This study is the first work to leverage the power of eye-tracking technology and SAM to enhance the efficiency of daily clinical practice. Moreover, eye-gaze data coupled with image and corresponding segmentation labels can be easily recorded for further advanced eye-tracking research. The code is available in \url{https://github.com/ukaukaaaa/GazeSAM}.



### Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2304.13850v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13850v2)
- **Published**: 2023-04-26 22:29:49+00:00
- **Updated**: 2023-04-28 15:14:30+00:00
- **Authors**: Casey Meehan, Florian Bordes, Pascal Vincent, Kamalika Chaudhuri, Chuan Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that d\'ej\`a vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of d\'ej\`a vu memorization reveals previously unknown privacy risks in SSL models, as well as suggests potential practical mitigation strategies. Code is available at https://github.com/facebookresearch/DejaVu.



### Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models
- **Arxiv ID**: http://arxiv.org/abs/2304.13855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.13855v1)
- **Published**: 2023-04-26 22:53:31+00:00
- **Updated**: 2023-04-26 22:53:31+00:00
- **Authors**: Abhishek Mandal, Susan Leavy, Suzanne Little
- **Comment**: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution has
  been accepted at the Fourth International Workshop on Algorithmic Bias in
  Search and Recommendation held as a part of the 45th European Conference on
  Information Retrieval (ECIR 2023) and will be published soon
- **Journal**: None
- **Summary**: Generative multimodal models based on diffusion models have seen tremendous growth and advances in recent years. Models such as DALL-E and Stable Diffusion have become increasingly popular and successful at creating images from texts, often combining abstract ideas. However, like other deep learning models, they also reflect social biases they inherit from their training data, which is often crawled from the internet. Manually auditing models for biases can be very time and resource consuming and is further complicated by the unbounded and unconstrained nature of inputs these models can take. Research into bias measurement and quantification has generally focused on small single-stage models working on a single modality. Thus the emergence of multistage multimodal models requires a different approach. In this paper, we propose Multimodal Composite Association Score (MCAS) as a new method of measuring gender bias in multimodal generative models. Evaluating both DALL-E 2 and Stable Diffusion using this approach uncovered the presence of gendered associations of concepts embedded within the models. We propose MCAS as an accessible and scalable method of quantifying potential bias for models with different modalities and a range of potential biases.



### SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2304.14418v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2304.14418v1)
- **Published**: 2023-04-26 23:39:40+00:00
- **Updated**: 2023-04-26 23:39:40+00:00
- **Authors**: Fisseha Admasu Ferede, Madhusudhanan Balasubramanian
- **Comment**: 5 tables, 7 figures, MS thesis
- **Journal**: None
- **Summary**: Inaccurate optical flow estimates in and near occluded regions, and out-of-boundary regions are two of the current significant limitations of optical flow estimation algorithms. Recent state-of-the-art optical flow estimation algorithms are two-frame based methods where optical flow is estimated sequentially for each consecutive image pair in a sequence. While this approach gives good flow estimates, it fails to generalize optical flows in occluded regions mainly due to limited local evidence regarding moving elements in a scene. In this work, we propose a learning-based multi-frame optical flow estimation method that estimates two or more consecutive optical flows in parallel from multi-frame image sequences. Our underlying hypothesis is that by understanding temporal scene dynamics from longer sequences with more than two frames, we can characterize pixel-wise dependencies in a larger spatiotemporal domain, generalize complex motion patterns and thereby improve the accuracy of optical flow estimates in occluded regions. We present learning-based spatiotemporal recurrent transformers for multi-frame based optical flow estimation (SSTMs). Our method utilizes 3D Convolutional Gated Recurrent Units (3D-ConvGRUs) and spatiotemporal transformers to learn recurrent space-time motion dynamics and global dependencies in the scene and provide a generalized optical flow estimation. When compared with recent state-of-the-art two-frame and multi-frame methods on real world and synthetic datasets, performance of the SSTMs were significantly higher in occluded and out-of-boundary regions. Among all published state-of-the-art multi-frame methods, SSTM achieved state-of the-art results on the Sintel Final and KITTI2015 benchmark datasets.



### Deep Learning Techniques for Hyperspectral Image Analysis in Agriculture: A Review
- **Arxiv ID**: http://arxiv.org/abs/2304.13880v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2304.13880v1)
- **Published**: 2023-04-26 23:58:18+00:00
- **Updated**: 2023-04-26 23:58:18+00:00
- **Authors**: Mohamed Fadhlallah Guerri, Cosimo Distante, Paolo Spagnolo, Fares Bougourzi, Abdelmalik Taleb-Ahmed
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent years, hyperspectral imaging (HSI) has gained considerably popularity among computer vision researchers for its potential in solving remote sensing problems, especially in agriculture field. However, HSI classification is a complex task due to the high redundancy of spectral bands, limited training samples, and non-linear relationship between spatial position and spectral bands. Fortunately, deep learning techniques have shown promising results in HSI analysis. This literature review explores recent applications of deep learning approaches such as Autoencoders, Convolutional Neural Networks (1D, 2D, and 3D), Recurrent Neural Networks, Deep Belief Networks, and Generative Adversarial Networks in agriculture. The performance of these approaches has been evaluated and discussed on well-known land cover datasets including Indian Pines, Salinas Valley, and Pavia University.




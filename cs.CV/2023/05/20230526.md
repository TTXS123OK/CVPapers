# Arxiv Papers in cs.CV on 2023-05-26
### CVB: A Video Dataset of Cattle Visual Behaviors
- **Arxiv ID**: http://arxiv.org/abs/2305.16555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16555v2)
- **Published**: 2023-05-26 00:44:11+00:00
- **Updated**: 2023-07-03 07:11:17+00:00
- **Authors**: Ali Zia, Renuka Sharma, Reza Arablouei, Greg Bishop-Hurley, Jody McNally, Neil Bagnall, Vivien Rolland, Brano Kusy, Lars Petersson, Aaron Ingham
- **Comment**: None
- **Journal**: None
- **Summary**: Existing image/video datasets for cattle behavior recognition are mostly small, lack well-defined labels, or are collected in unrealistic controlled environments. This limits the utility of machine learning (ML) models learned from them. Therefore, we introduce a new dataset, called Cattle Visual Behaviors (CVB), that consists of 502 video clips, each fifteen seconds long, captured in natural lighting conditions, and annotated with eleven visually perceptible behaviors of grazing cattle. We use the Computer Vision Annotation Tool (CVAT) to collect our annotations. To make the procedure more efficient, we perform an initial detection and tracking of cattle in the videos using appropriate pre-trained models. The results are corrected by domain experts along with cattle behavior labeling in CVAT. The pre-hoc detection and tracking step significantly reduces the manual annotation time and effort. Moreover, we convert CVB to the atomic visual action (AVA) format and train and evaluate the popular SlowFast action recognition model on it. The associated preliminary results confirm that we can localize the cattle and recognize their frequently occurring behaviors with confidence. By creating and sharing CVB, our aim is to develop improved models capable of recognizing all important behaviors accurately and to assist other researchers and practitioners in developing and evaluating new ML models for cattle behavior classification using video data.



### Integrating Listwise Ranking into Pairwise-based Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2305.16566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16566v1)
- **Published**: 2023-05-26 01:18:52+00:00
- **Updated**: 2023-05-26 01:18:52+00:00
- **Authors**: Zheng Li, Caili Guo, Xin Wang, Zerun Feng, Yanjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-Text Retrieval (ITR) is essentially a ranking problem. Given a query caption, the goal is to rank candidate images by relevance, from large to small. The current ITR datasets are constructed in a pairwise manner. Image-text pairs are annotated as positive or negative. Correspondingly, ITR models mainly use pairwise losses, such as triplet loss, to learn to rank. Pairwise-based ITR increases positive pair similarity while decreasing negative pair similarity indiscriminately. However, the relevance between dissimilar negative pairs is different. Pairwise annotations cannot reflect this difference in relevance. In the current datasets, pairwise annotations miss many correlations. There are many potential positive pairs among the pairs labeled as negative. Pairwise-based ITR can only rank positive samples before negative samples, but cannot rank negative samples by relevance. In this paper, we integrate listwise ranking into conventional pairwise-based ITR. Listwise ranking optimizes the entire ranking list based on relevance scores. Specifically, we first propose a Relevance Score Calculation (RSC) module to calculate the relevance score of the entire ranked list. Then we choose the ranking metric, Normalized Discounted Cumulative Gain (NDCG), as the optimization objective. We transform the non-differentiable NDCG into a differentiable listwise loss, named Smooth-NDCG (S-NDCG). Our listwise ranking approach can be plug-and-play integrated into current pairwise-based ITR models. Experiments on ITR benchmarks show that integrating listwise ranking can improve the performance of current ITR models and provide more user-friendly retrieval results. The code is available at https://github.com/AAA-Zheng/Listwise_ITR.



### Structured Latent Variable Models for Articulated Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2305.16567v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.16567v1)
- **Published**: 2023-05-26 01:22:35+00:00
- **Updated**: 2023-05-26 01:22:35+00:00
- **Authors**: Emily Liu, Michael Noseworthy, Nicholas Roy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate a scenario in which a robot learns a low-dimensional representation of a door given a video of the door opening or closing. This representation can be used to infer door-related parameters and predict the outcomes of interacting with the door. Current machine learning based approaches in the doors domain are based primarily on labelled datasets. However, the large quantity of available door data suggests the feasibility of a semisupervised approach based on pretraining. To exploit the hierarchical structure of the dataset where each door has multiple associated images, we pretrain with a structured latent variable model known as a neural statistician. The neural satsitician enforces separation between shared context-level variables (common across all images associated with the same door) and instance-level variables (unique to each individual image). We first demonstrate that the neural statistician is able to learn an embedding that enables reconstruction and sampling of realistic door images. Then, we evaluate the correspondence of the learned embeddings to human-interpretable parameters in a series of supervised inference tasks. It was found that a pretrained neural statistician encoder outperformed analogous context-free baselines when predicting door handedness, size, angle location, and configuration from door images. Finally, in a visual bandit door-opening task with a variety of door configuration, we found that neural statistician embeddings achieve lower regret than context-free baselines.



### TFDet: Target-aware Fusion for RGB-T Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.16580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16580v1)
- **Published**: 2023-05-26 02:09:48+00:00
- **Updated**: 2023-05-26 02:09:48+00:00
- **Authors**: Xue Zhang, Xiaohan Zhang, Zehua Sheng, Hui-Liang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection is a critical task in computer vision because of its role in ensuring traffic safety. However, existing methods that rely solely on RGB images suffer from performance degradation under low-light conditions due to the lack of useful information. To address this issue, recent multispectral detection approaches combine thermal images to provide complementary information. Nevertheless, these approaches have limitations such as the noisy fused feature maps and the loss of informative features. In this paper, we propose a novel target-aware fusion strategy for multispectral pedestrian detection, named TFDet. Unlike existing methods, TFDet enhances features by supervising the fusion process with a correlation-maximum loss function. Our fusion strategy highlights the pedestrian-related features while suppressing the unrelated ones. TFDet achieves state-of-the-art performances on both KAIST and LLVIP benchmarks, with a speed comparable to the previous state-of-the-art counterpart. Importantly, TFDet performs remarkably well under low-light conditions, which is a significant advancement for road safety.



### Discovering Novel Actions in an Open World with Object-Grounded Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2305.16602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16602v1)
- **Published**: 2023-05-26 03:21:30+00:00
- **Updated**: 2023-05-26 03:21:30+00:00
- **Authors**: Sathyanarayanan N. Aakur, Sanjoy Kundu, Shubham Trehan
- **Comment**: 13 Pages, 2 figures, 3 tables
- **Journal**: None
- **Summary**: Learning to infer labels in an open world, i.e., in an environment where the target ``labels'' are unknown, is an important characteristic for achieving autonomy. Foundation models pre-trained on enormous amounts of data have shown remarkable generalization skills through prompting, particularly in zero-shot inference. However, their performance is restricted to the correctness of the target label's search space. In an open world where these labels are unknown, the search space can be exceptionally large. It can require reasoning over several combinations of elementary concepts to arrive at an inference, which severely restricts the performance of such models. To tackle this challenging problem, we propose a neuro-symbolic framework called ALGO - novel Action Learning with Grounded Object recognition that can use symbolic knowledge stored in large-scale knowledge bases to infer activities (verb-noun combinations) in egocentric videos with limited supervision using two steps. First, we propose a novel neuro-symbolic prompting approach that uses object-centric vision-language foundation models as a noisy oracle to ground objects in the video through evidence-based reasoning. Second, driven by prior commonsense knowledge, we discover plausible activities through an energy-based symbolic pattern theory framework and learn to ground knowledge-based action (verb) concepts in the video. Extensive experiments on two publicly available datasets (GTEA Gaze and GTEA Gaze Plus) demonstrate its performance on open-world activity inference and its generalization to unseen actions in an unknown search space. We show that ALGO can be extended to zero-shot settings and demonstrate its competitive performance to multimodal foundation models.



### Thailand Asset Value Estimation Using Aerial or Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2307.08650v2
- **DOI**: None
- **Categories**: **q-fin.ST**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2307.08650v2)
- **Published**: 2023-05-26 04:57:43+00:00
- **Updated**: 2023-08-04 18:03:39+00:00
- **Authors**: Supawich Puengdang, Worawate Ausawalaithong, Phiratath Nopratanawong, Narongdech Keeratipranon, Chayut Wongkamthong
- **Comment**: None
- **Journal**: None
- **Summary**: Real estate is a critical sector in Thailand's economy, which has led to a growing demand for a more accurate land price prediction approach. Traditional methods of land price prediction, such as the weighted quality score (WQS), are limited due to their reliance on subjective criteria and their lack of consideration for spatial variables. In this study, we utilize aerial or satellite imageries from Google Map API to enhance land price prediction models from the dataset provided by Kasikorn Business Technology Group (KBTG). We propose a similarity-based asset valuation model that uses a Siamese-inspired Neural Network with pretrained EfficientNet architecture to assess the similarity between pairs of lands. By ensembling deep learning and tree-based models, we achieve an area under the ROC curve (AUC) of approximately 0.81, outperforming the baseline model that used only tabular data. The appraisal prices of nearby lands with similarity scores higher than a predefined threshold were used for weighted averaging to predict the reasonable price of the land in question. At 20\% mean absolute percentage error (MAPE), we improve the recall from 59.26\% to 69.55\%, indicating a more accurate and reliable approach to predicting land prices. Our model, which is empowered by a more comprehensive view of land use and environmental factors from aerial or satellite imageries, provides a more precise, data-driven, and adaptive approach for land valuation in Thailand.



### Improving Position Encoding of Transformers for Multivariate Time Series Classification
- **Arxiv ID**: http://arxiv.org/abs/2305.16642v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.16642v1)
- **Published**: 2023-05-26 05:30:04+00:00
- **Updated**: 2023-05-26 05:30:04+00:00
- **Authors**: Navid Mohammadi Foumani, Chang Wei Tan, Geoffrey I. Webb, Mahsa Salehi
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose a novel multivariate time series classification (MTSC) model combining tAPE/eRPE and convolution-based input encoding named ConvTran to improve the position and data embedding of time series data. The proposed absolute and relative position encoding methods are simple and efficient. They can be easily integrated into transformer blocks and used for downstream tasks such as forecasting, extrinsic regression, and anomaly detection. Extensive experiments on 32 multivariate time-series datasets show that our model is significantly more accurate than state-of-the-art convolution and transformer-based models. Code and models are open-sourced at \url{https://github.com/Navidfoumani/ConvTran}.



### Summarizing Stream Data for Memory-Restricted Online Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.16645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16645v1)
- **Published**: 2023-05-26 05:31:51+00:00
- **Updated**: 2023-05-26 05:31:51+00:00
- **Authors**: Jianyang Gu, Kai Wang, Wei Jiang, Yang You
- **Comment**: None
- **Journal**: None
- **Summary**: Replay-based methods have proved their effectiveness on online continual learning by rehearsing past samples from an auxiliary memory. With many efforts made on improving training schemes based on the memory, however, the information carried by each sample in the memory remains under-investigated. Under circumstances with restricted storage space, the informativeness of the memory becomes critical for effective replay. Although some works design specific strategies to select representative samples, by only employing original images, the storage space is still not well utilized. To this end, we propose to Summarize the knowledge from the Stream Data (SSD) into more informative samples by distilling the training characteristics of real images. Through maintaining the consistency of training gradients and relationship to the past tasks, the summarized samples are more representative for the stream data compared to the original images. Extensive experiments are conducted on multiple online continual learning benchmarks to support that the proposed SSD method significantly enhances the replay effects. We demonstrate that with limited extra computational overhead, SSD provides more than 3% accuracy boost for sequential CIFAR-100 under extremely restricted memory buffer. The code is available in https://github.com/vimar-gu/SSD.



### FSD: Fully-Specialized Detector via Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2305.16649v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16649v4)
- **Published**: 2023-05-26 05:41:20+00:00
- **Updated**: 2023-07-21 05:46:30+00:00
- **Authors**: Zhe Huang, Yudian Li
- **Comment**: None
- **Journal**: None
- **Summary**: Most generic object detectors are mainly built for standard object detection tasks such as COCO and PASCAL VOC. They might not work well and/or efficiently on tasks of other domains consisting of images that are visually different from standard datasets. To this end, many advances have been focused on adapting a general-purposed object detector with limited domain-specific designs. However, designing a successful task-specific detector requires extraneous manual experiments and parameter tuning through trial and error. In this paper, we first propose and examine a fully-automatic pipeline to design a fully-specialized detector (FSD) which mainly incorporates a neural-architectural-searched model by exploring ideal network structures over the backbone and task-specific head. On the DeepLesion dataset, extensive results show that FSD can achieve 3.1 mAP gain while using approximately 40% fewer parameters on binary lesion detection task and improved the mAP by around 10% on multi-type lesion detection task via our region-aware graph modeling compared with existing general-purposed medical lesion detection networks.



### Clustering Method for Time-Series Images Using Quantum-Inspired Computing Technology
- **Arxiv ID**: http://arxiv.org/abs/2305.16656v3
- **DOI**: 10.21203/rs.3.rs-3141099/v1
- **Categories**: **eess.SP**, cs.CV, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2305.16656v3)
- **Published**: 2023-05-26 05:58:14+00:00
- **Updated**: 2023-07-18 04:23:26+00:00
- **Authors**: Tomoki Inoue, Koyo Kubota, Tsubasa Ikami, Yasuhiro Egami, Hiroki Nagai, Takahiro Kashikawa, Koichi Kimura, Yu Matsuda
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Time-series clustering serves as a powerful data mining technique for time-series data in the absence of prior knowledge about clusters. A large amount of time-series data with large size has been acquired and used in various research fields. Hence, clustering method with low computational cost is required. Given that a quantum-inspired computing technology, such as a simulated annealing machine, surpasses conventional computers in terms of fast and accurately solving combinatorial optimization problems, it holds promise for accomplishing clustering tasks that are challenging to achieve using existing methods. This study proposes a novel time-series clustering method that leverages an annealing machine. The proposed method facilitates an even classification of time-series data into clusters close to each other while maintaining robustness against outliers. Moreover, its applicability extends to time-series images. We compared the proposed method with a standard existing method for clustering an online distributed dataset. In the existing method, the distances between each data are calculated based on the Euclidean distance metric, and the clustering is performed using the k-means++ method. We found that both methods yielded comparable results. Furthermore, the proposed method was applied to a flow measurement image dataset containing noticeable noise with a signal-to-noise ratio of approximately 1. Despite a small signal variation of approximately 2%, the proposed method effectively classified the data without any overlap among the clusters. In contrast, the clustering results by the standard existing method and the conditional image sampling (CIS) method, a specialized technique for flow measurement data, displayed overlapping clusters. Consequently, the proposed method provides better results than the other two methods, demonstrating its potential as a superior clustering method.



### Higher Order Gauge Equivariant CNNs on Riemannian Manifolds and Applications
- **Arxiv ID**: http://arxiv.org/abs/2305.16657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.16657v1)
- **Published**: 2023-05-26 06:02:31+00:00
- **Updated**: 2023-05-26 06:02:31+00:00
- **Authors**: Gianfranco Cortes, Yue Yu, Robin Chen, Melissa Armstrong, David Vaillancourt, Baba C. Vemuri
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of group equivariant convolutions in deep networks literature, spherical CNNs with $\mathsf{SO}(3)$-equivariant layers have been developed to cope with data that are samples of signals on the sphere $S^2$. One can implicitly obtain $\mathsf{SO}(3)$-equivariant convolutions on $S^2$ with significant efficiency gains by explicitly requiring gauge equivariance w.r.t. $\mathsf{SO}(2)$. In this paper, we build on this fact by introducing a higher order generalization of the gauge equivariant convolution, whose implementation is dubbed a gauge equivariant Volterra network (GEVNet). This allows us to model spatially extended nonlinear interactions within a given receptive field while still maintaining equivariance to global isometries. We prove theoretical results regarding the equivariance and construction of higher order gauge equivariant convolutions. Then, we empirically demonstrate the parameter efficiency of our model, first on computer vision benchmark data (e.g. spherical MNIST), and then in combination with a convolutional kernel network (CKN) on neuroimaging data. In the neuroimaging data experiments, the resulting two-part architecture (CKN + GEVNet) is used to automatically discriminate between patients with Lewy Body Disease (DLB), Alzheimer's Disease (AD) and Parkinson's Disease (PD) from diffusion magnetic resonance images (dMRI). The GEVNet extracts micro-architectural features within each voxel, while the CKN extracts macro-architectural features across voxels. This compound architecture is uniquely poised to exploit the intra- and inter-voxel information contained in the dMRI data, leading to improved performance over the classification results obtained from either of the individual components.



### Gender, Smoking History and Age Prediction from Laryngeal Images
- **Arxiv ID**: http://arxiv.org/abs/2305.16661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.16661v1)
- **Published**: 2023-05-26 06:16:47+00:00
- **Updated**: 2023-05-26 06:16:47+00:00
- **Authors**: Tianxiao Zhang, Andr√©s M. Bur, Shannon Kraft, Hannah Kavookjian, Bryan Renslo, Xiangyu Chen, Bo Luo, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Flexible laryngoscopy is commonly performed by otolaryngologists to detect laryngeal diseases and to recognize potentially malignant lesions. Recently, researchers have introduced machine learning techniques to facilitate automated diagnosis using laryngeal images and achieved promising results. Diagnostic performance can be improved when patients' demographic information is incorporated into models. However, manual entry of patient data is time consuming for clinicians. In this study, we made the first endeavor to employ deep learning models to predict patient demographic information to improve detector model performance. The overall accuracy for gender, smoking history, and age was 85.5%, 65.2%, and 75.9%, respectively. We also created a new laryngoscopic image set for machine learning study and benchmarked the performance of 8 classical deep learning models based on CNNs and Transformers. The results can be integrated into current learning models to improve their performance by incorporating the patient's demographic information.



### CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.16681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16681v1)
- **Published**: 2023-05-26 07:02:57+00:00
- **Updated**: 2023-05-26 07:02:57+00:00
- **Authors**: Zhaoheng Zheng, Haidong Zhu, Ram Nevatia
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Compositionality, the ability to combine existing concepts and generalize towards novel compositions, is a key functionality for intelligent entities. Here, we study the problem of Compositional Zero-Shot Learning (CZSL), which aims at recognizing novel attribute-object compositions. Recent approaches build their systems on top of large-scale Vision-Language Pre-trained (VLP) models, e.g. CLIP, and observe significant improvements. However, these methods treat CLIP as a black box and focus on pre- and post-CLIP operations. Here, we propose to dive deep into the architecture and insert adapters, a parameter-efficient technique proven to be effective among large language models, to each CLIP encoder layer. We further equip adapters with concept awareness so that concept-specific features of "object", "attribute" and "composition" can be extracted. We name our method CAILA, Concept-Aware Intra-Layer Adapters. Quantitative evaluations performed on three popular CZSL datasets, MIT-States, C-GQA, and UT-Zappos, reveal that CAILA achieves double-digit relative improvements against the current state-of-the-art on all benchmarks.



### Sharpend Cosine Similarity based Neural Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2305.16682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16682v1)
- **Published**: 2023-05-26 07:04:00+00:00
- **Updated**: 2023-05-26 07:04:00+00:00
- **Authors**: Muhammad Ahmad
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral Image Classification (HSIC) is a difficult task due to high inter and intra-class similarity and variability, nested regions, and overlapping. 2D Convolutional Neural Networks (CNN) emerged as a viable network whereas, 3D CNNs are a better alternative due to accurate classification. However, 3D CNNs are highly computationally complex due to their volume and spectral dimensions. Moreover, down-sampling and hierarchical filtering (high frequency) i.e., texture features need to be smoothed during the forward pass which is crucial for accurate HSIC. Furthermore, CNN requires tons of tuning parameters which increases the training time. Therefore, to overcome the aforesaid issues, Sharpened Cosine Similarity (SCS) concept as an alternative to convolutions in a Neural Network for HSIC is introduced. SCS is exceptionally parameter efficient due to skipping the non-linear activation layers, normalization, and dropout after the SCS layer. Use of MaxAbsPool instead of MaxPool which selects the element with the highest magnitude of activity, even if it's negative. Experimental results on publicly available HSI datasets proved the performance of SCS as compared to the convolutions in Neural Networks.



### S4M: Generating Radiology Reports by A Single Model for Multiple Body Parts
- **Arxiv ID**: http://arxiv.org/abs/2305.16685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16685v1)
- **Published**: 2023-05-26 07:12:35+00:00
- **Updated**: 2023-05-26 07:12:35+00:00
- **Authors**: Qi Chen, Yutong Xie, Biao Wu, Minh-Son To, James Ang, Qi Wu
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: In this paper, we seek to design a report generation model that is able to generate reasonable reports even given different images of various body parts. We start by directly merging multiple datasets and training a single report generation model on this one. We, however, observe that the reports generated in such a simple way only obtain comparable performance compared with that trained separately on each specific dataset. We suspect that this is caused by the dilemma between the diversity of body parts and the limited availability of medical data. To develop robust and generalizable models, it is important to consider a diverse range of body parts and medical conditions. However, collecting a sufficiently large dataset for each specific body part can be difficult due to various factors, such as data availability and privacy concerns. Thus, rather than striving for more data, we propose a single-for-multiple (S4M) framework, which seeks to facilitate the learning of the report generation model with two auxiliary priors: an explicit prior (\ie, feeding radiology-informed knowledge) and an implicit prior (\ie, guided by cross-modal features). Specifically, based on the conventional encoder-decoder report generation framework, we incorporate two extra branches: a Radiology-informed Knowledge Aggregation (RadKA) branch and an Implicit Prior Guidance (IPG) branch. We conduct the experiments on our merged dataset which consists of a public dataset (\ie, IU-Xray) and five private datasets, covering six body parts: chest, abdomen, knee, hip, wrist and shoulder. Our S4M model outperforms all the baselines, regardless of whether they are trained on separate or merged datasets. Code is available at: \url{https://github.com/YtongXie/S4M}.



### Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.16687v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.16687v1)
- **Published**: 2023-05-26 07:17:24+00:00
- **Updated**: 2023-05-26 07:17:24+00:00
- **Authors**: In-Ug Yoon, Tae-Min Choi, Young-Min Kim, Jong-Hwan Kim
- **Comment**: 14 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) presents the primary challenge of balancing underfitting to a new session's task and forgetting the tasks from previous sessions. To address this challenge, we develop a simple yet powerful learning scheme that integrates effective methods for each core component of the FSCIL network, including the feature extractor, base session classifiers, and incremental session classifiers. In feature extractor training, our goal is to obtain balanced generic representations that benefit both current viewable and unseen or past classes. To achieve this, we propose a balanced supervised contrastive loss that effectively balances these two objectives. In terms of classifiers, we analyze and emphasize the importance of unifying initialization methods for both the base and incremental session classifiers. Our method demonstrates outstanding ability for new task learning and preventing forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significant improvements over previous state-of-the-art methods across diverse metrics. We conduct experiments to analyze the significance and rationale behind our approach and visualize the effectiveness of our representations on new tasks. Furthermore, we conduct diverse ablation studies to analyze the effects of each module.



### Learning from Multi-Perception Features for Real-Word Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2305.18547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.18547v1)
- **Published**: 2023-05-26 07:35:49+00:00
- **Updated**: 2023-05-26 07:35:49+00:00
- **Authors**: Axi Niu, Kang Zhang, Trung X. Pham, Pei Wang, Jinqiu Sun, In So Kweon, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, there are two popular approaches for addressing real-world image super-resolution problems: degradation-estimation-based and blind-based methods. However, degradation-estimation-based methods may be inaccurate in estimating the degradation, making them less applicable to real-world LR images. On the other hand, blind-based methods are often limited by their fixed single perception information, which hinders their ability to handle diverse perceptual characteristics. To overcome this limitation, we propose a novel SR method called MPF-Net that leverages multiple perceptual features of input images. Our method incorporates a Multi-Perception Feature Extraction (MPFE) module to extract diverse perceptual information and a series of newly-designed Cross-Perception Blocks (CPB) to combine this information for effective super-resolution reconstruction. Additionally, we introduce a contrastive regularization term (CR) that improves the model's learning capability by using newly generated HR and LR images as positive and negative samples for ground truth HR. Experimental results on challenging real-world SR datasets demonstrate that our approach significantly outperforms existing state-of-the-art methods in both qualitative and quantitative measures.



### Detect Any Shadow: Segment Anything for Video Shadow Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.16698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16698v1)
- **Published**: 2023-05-26 07:39:10+00:00
- **Updated**: 2023-05-26 07:39:10+00:00
- **Authors**: Yonghui Wang, Wengang Zhou, Yunyao Mao, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Segment anything model (SAM) has achieved great success in the field of natural image segmentation. Nevertheless, SAM tends to classify shadows as background, resulting in poor segmentation performance for shadow detection task. In this paper, we propose an simple but effective approach for fine tuning SAM to detect shadows. Additionally, we also combine it with long short-term attention mechanism to extend its capabilities to video shadow detection. Specifically, we first fine tune SAM by utilizing shadow data combined with sparse prompts and apply the fine-tuned model to detect a specific frame (e.g., first frame) in the video with a little user assistance. Subsequently, using the detected frame as a reference, we employ a long short-term network to learn spatial correlations between distant frames and temporal consistency between contiguous frames, thereby achieving shadow information propagation across frames. Extensive experimental results demonstrate that our method outperforms the state-of-the-art techniques, with improvements of 17.2% and 3.3% in terms of MAE and IoU, respectively, validating the effectiveness of our method.



### ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.16713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16713v1)
- **Published**: 2023-05-26 07:59:36+00:00
- **Updated**: 2023-05-26 07:59:36+00:00
- **Authors**: Jeeho Hyun, Sangyun Kim, Giyoung Jeon, Seung Hwan Kim, Kyunghoon Bae, Byung Jun Kang
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Anomaly detection is crucial to the advanced identification of product defects such as incorrect parts, misaligned components, and damages in industrial manufacturing. Due to the rare observations and unknown types of defects, anomaly detection is considered to be challenging in machine learning. To overcome this difficulty, recent approaches utilize the common visual representations from natural image datasets and distill the relevant features. However, existing approaches still have the discrepancy between the pre-trained feature and the target data, or require the input augmentation which should be carefully designed particularly for the industrial dataset. In this paper, we introduce ReConPatch, which constructs discriminative features for anomaly detection by training a linear modulation attached to a pre-trained model. ReConPatch employs contrastive representation learning to collect and distribute features in a way that produces a target-oriented and easily separable representation. To address the absence of labeled pairs for the contrastive learning, we utilize two similarity measures, pairwise and contextual similarities, between data representations as a pseudo-label. Unlike previous work, ReConPatch achieves robust anomaly detection performance without extensive input augmentation. Our method achieves the state-of-the-art anomaly detection performance (99.72%) for the widely used and challenging MVTec AD dataset.



### Shape-based pose estimation for automatic standard views of the knee
- **Arxiv ID**: http://arxiv.org/abs/2305.16717v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.16717v1)
- **Published**: 2023-05-26 08:03:47+00:00
- **Updated**: 2023-05-26 08:03:47+00:00
- **Authors**: Lisa Kausch, Sarina Thomas, Holger Kunze, Jan Siad El Barbari, Klaus Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical treatment of complicated knee fractures is guided by real-time imaging using a mobile C-arm. Immediate and continuous control is achieved via 2D anatomy-specific standard views that correspond to a specific C-arm pose relative to the patient positioning, which is currently determined manually, following a trial-and-error approach at the cost of time and radiation dose. The characteristics of the standard views of the knee suggests that the shape information of individual bones could guide an automatic positioning procedure, reducing time and the amount of unnecessary radiation during C-arm positioning. To fully automate the C-arm positioning task during knee surgeries, we propose a complete framework that enables (1) automatic laterality and standard view classification and (2) automatic shape-based pose regression toward the desired standard view based on a single initial X-ray. A suitable shape representation is proposed to incorporate semantic information into the pose regression pipeline. The pipeline is designed to handle two distinct standard views simultaneously. Experiments were conducted to assess the performance of the proposed system on 3528 synthetic and 1386 real X-rays for the a.-p. and lateral standard. The view/laterality classificator resulted in an accuracy of 100\%/98\% on the simulated and 99\%/98\% on the real X-rays. The pose regression performance was $d\theta_{a.-p}=5.8\pm3.3\degree,\,d\theta_{lateral}=3.7\pm2.0\degree$ on the simulated data and $d\theta_{a.-p}=7.4\pm5.0\degree,\,d\theta_{lateral}=8.4\pm5.4\degree$ on the real data outperforming intensity-based pose regression.



### A Novel Application for Real-time Arrhythmia Detection using YOLOv8
- **Arxiv ID**: http://arxiv.org/abs/2305.16727v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.16727v2)
- **Published**: 2023-05-26 08:27:09+00:00
- **Updated**: 2023-08-17 02:01:22+00:00
- **Authors**: Guang Jun Nicholas Ang, Aritejh Kr Goil, Henryk Chan, Jieyi Jeric Lew, Xin Chun Lee, Raihan Bin Ahmad Mustaffa, Timotius Jason, Ze Ting Woon, Bingquan Shen
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there has been an increasing need to reduce healthcare costs in remote monitoring of cardiovascular health. Detecting and classifying cardiac arrhythmia is critical to diagnosing patients with cardiac abnormalities. This paper shows that complex systems such as electrocardiograms (ECG) can be applicable for at-home monitoring. This paper proposes a novel application for arrhythmia detection using the state-of-the-art You-Only-Look-Once (YOLO)v8 algorithm to classify single-lead ECG signals. We proposed a loss-modified YOLOv8 model that was fine-tuned on the MIT-BIH arrhythmia dataset to detect to allow real-time continuous monitoring. Results show that our model can detect arrhythmia with an average accuracy of 99.5% and 0.992 mAP@50 with a detection time of 0.002s on an NVIDIA Tesla V100. Our study demonstrated the potential of real-time arrhythmia detection, where the model output can be visually interpreted for at-home users. Furthermore, this study could be extended into a real-time XAI model, deployed in the healthcare industry, and significantly advancing healthcare needs.



### Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2305.18212v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.18212v1)
- **Published**: 2023-05-26 08:43:46+00:00
- **Updated**: 2023-05-26 08:43:46+00:00
- **Authors**: Yuxing Long, Binyuan Hui, Caixia Yuan1, Fei Huang, Yongbin Li, Xiaojie Wang
- **Comment**: ACL 2023
- **Journal**: None
- **Summary**: Existing multimodal task-oriented dialog data fails to demonstrate the diverse expressions of user subjective preferences and recommendation acts in the real-life shopping scenario. This paper introduces a new dataset SURE (Multimodal Recommendation Dialog with SUbjective PREference), which contains 12K shopping dialogs in complex store scenes. The data is built in two phases with human annotations to ensure quality and diversity. SURE is well-annotated with subjective preferences and recommendation acts proposed by sales experts. A comprehensive analysis is given to reveal the distinguishing features of SURE. Three benchmark tasks are then proposed on the data to evaluate the capability of multimodal recommendation agents. Based on the SURE, we propose a baseline model, powered by a state-of-the-art multimodal model, for these tasks.



### CNN Feature Map Augmentation for Single-Source Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2305.16746v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16746v2)
- **Published**: 2023-05-26 08:48:17+00:00
- **Updated**: 2023-06-15 09:26:46+00:00
- **Authors**: Aristotelis Ballas, Christos Diou
- **Comment**: In proceedings of IEEE BigDataService2023
  (https://ieeebigdataservice.com/)
- **Journal**: None
- **Summary**: In search of robust and generalizable machine learning models, Domain Generalization (DG) has gained significant traction during the past few years. The goal in DG is to produce models which continue to perform well when presented with data distributions different from the ones available during training. While deep convolutional neural networks (CNN) have been able to achieve outstanding performance on downstream computer vision tasks, they still often fail to generalize on previously unseen data Domains. Therefore, in this work we focus on producing a model which is able to remain robust under data distribution shift and propose an alternative regularization technique for convolutional neural network architectures in the single-source DG image classification setting. To mitigate the problem caused by domain shift between source and target data, we propose augmenting intermediate feature maps of CNNs. Specifically, we pass them through a novel Augmentation Layer} to prevent models from overfitting on the training set and improve their cross-domain generalization. To the best of our knowledge, this is the first paper proposing such a setup for the DG image classification setting. Experiments on the DG benchmark datasets of PACS, VLCS, Office-Home and TerraIncognita validate the effectiveness of our method, in which our model surpasses state-of-the-art algorithms in most cases.



### StyleHumanCLIP: Text-guided Garment Manipulation for StyleGAN-Human
- **Arxiv ID**: http://arxiv.org/abs/2305.16759v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2305.16759v2)
- **Published**: 2023-05-26 09:21:56+00:00
- **Updated**: 2023-07-25 08:39:31+00:00
- **Authors**: Takato Yoshikawa, Yuki Endo, Yoshihiro Kanamori
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles text-guided control of StyleGAN for editing garments in full-body human images. Existing StyleGAN-based methods suffer from handling the rich diversity of garments and body shapes and poses. We propose a framework for text-guided full-body human image synthesis via an attention-based latent code mapper, which enables more disentangled control of StyleGAN than existing mappers. Our latent code mapper adopts an attention mechanism that adaptively manipulates individual latent codes on different StyleGAN layers under text guidance. In addition, we introduce feature-space masking at inference time to avoid unwanted changes caused by text inputs. Our quantitative and qualitative evaluations reveal that our method can control generated images more faithfully to given texts than existing methods.



### Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings
- **Arxiv ID**: http://arxiv.org/abs/2306.06110v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CE, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.06110v1)
- **Published**: 2023-05-26 09:33:12+00:00
- **Updated**: 2023-05-26 09:33:12+00:00
- **Authors**: Binyang Song, Chenyang Yuan, Frank Permenter, Nikos Arechiga, Faez Ahmed
- **Comment**: None
- **Journal**: None
- **Summary**: Generative AI models have made significant progress in automating the creation of 3D shapes, which has the potential to transform car design. In engineering design and optimization, evaluating engineering metrics is crucial. To make generative models performance-aware and enable them to create high-performing designs, surrogate modeling of these metrics is necessary. However, the currently used representations of three-dimensional (3D) shapes either require extensive computational resources to learn or suffer from significant information loss, which impairs their effectiveness in surrogate modeling. To address this issue, we propose a new two-dimensional (2D) representation of 3D shapes. We develop a surrogate drag model based on this representation to verify its effectiveness in predicting 3D car drag. We construct a diverse dataset of 9,070 high-quality 3D car meshes labeled by drag coefficients computed from computational fluid dynamics (CFD) simulations to train our model. Our experiments demonstrate that our model can accurately and efficiently evaluate drag coefficients with an $R^2$ value above 0.84 for various car categories. Moreover, the proposed representation method can be generalized to many other product categories beyond cars. Our model is implemented using deep neural networks, making it compatible with recent AI image generation tools (such as Stable Diffusion) and a significant step towards the automatic generation of drag-optimized car designs. We have made the dataset and code publicly available at https://decode.mit.edu/projects/dragprediction/.



### Modulate Your Spectrum in Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.16789v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2305.16789v1)
- **Published**: 2023-05-26 09:59:48+00:00
- **Updated**: 2023-05-26 09:59:48+00:00
- **Authors**: Xi Weng, Yunhao Ni, Tengwei Song, Jie Luo, Rao Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan, Lei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures. One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output. In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass. We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation. Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL). We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization. Moreover, INTL achieves 76.6% top-1 accuracy in linear evaluation on ImageNet using ResNet-50, which exceeds the performance of the supervised baseline, and this result is obtained by using a batch size of only 256. Comprehensive experiments show that INTL is a promising SSL method in practice. The code is available at https://github.com/winci-ai/intl.



### Motion-Based Sign Language Video Summarization using Curvature and Torsion
- **Arxiv ID**: http://arxiv.org/abs/2305.16801v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, 68T45, 68U10, I.4.9; I.5.4; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2305.16801v2)
- **Published**: 2023-05-26 10:30:23+00:00
- **Updated**: 2023-06-02 12:02:09+00:00
- **Authors**: Evangelos G. Sartinas, Emmanouil Z. Psarakis, Dimitrios I. Kosmopoulos
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: An interesting problem in many video-based applications is the generation of short synopses by selecting the most informative frames, a procedure which is known as video summarization. For sign language videos the benefits of using the $t$-parameterized counterpart of the curvature of the 2-D signer's wrist trajectory to identify keyframes, have been recently reported in the literature. In this paper we extend these ideas by modeling the 3-D hand motion that is extracted from each frame of the video. To this end we propose a new informative function based on the $t$-parameterized curvature and torsion of the 3-D trajectory. The method to characterize video frames as keyframes depends on whether the motion occurs in 2-D or 3-D space. Specifically, in the case of 3-D motion we look for the maxima of the harmonic mean of the curvature and torsion of the target's trajectory; in the planar motion case we seek for the maxima of the trajectory's curvature. The proposed 3-D feature is experimentally evaluated in applications of sign language videos on (1) objective measures using ground-truth keyframe annotations, (2) human-based evaluation of understanding, and (3) gloss classification and the results obtained are promising.



### Towards Open-World Segmentation of Parts
- **Arxiv ID**: http://arxiv.org/abs/2305.16804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16804v1)
- **Published**: 2023-05-26 10:34:58+00:00
- **Updated**: 2023-05-26 10:34:58+00:00
- **Authors**: Tai-Yu Pan, Qing Liu, Wei-Lun Chao, Brian Price
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Segmenting object parts such as cup handles and animal bodies is important in many real-world applications but requires more annotation effort. The largest dataset nowadays contains merely two hundred object categories, implying the difficulty to scale up part segmentation to an unconstrained setting. To address this, we propose to explore a seemingly simplified but empirically useful and scalable task, class-agnostic part segmentation. In this problem, we disregard the part class labels in training and instead treat all of them as a single part class. We argue and demonstrate that models trained without part classes can better localize parts and segment them on objects unseen in training. We then present two further improvements. First, we propose to make the model object-aware, leveraging the fact that parts are "compositions", whose extents are bounded by the corresponding objects and whose appearances are by nature not independent but bundled. Second, we introduce a novel approach to improve part segmentation on unseen objects, inspired by an interesting finding -- for unseen objects, the pixel-wise features extracted by the model often reveal high-quality part segments. To this end, we propose a novel self-supervised procedure that iterates between pixel clustering and supervised contrastive learning that pulls pixels closer or pushes them away. Via extensive experiments on PartImageNet and Pascal-Part, we show notable and consistent gains by our approach, essentially a critical step towards open-world part segmentation.



### Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2305.16807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16807v1)
- **Published**: 2023-05-26 10:41:08+00:00
- **Updated**: 2023-05-26 10:41:08+00:00
- **Authors**: Daiki Miyake, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka
- **Comment**: 22 pages, 11 figures
- **Journal**: None
- **Summary**: In image editing employing diffusion models, it is crucial to preserve the reconstruction quality of the original image while changing its style. Although existing methods ensure reconstruction quality through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling much faster editing processes. We experimentally demonstrate that the reconstruction quality of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction quality with a moderate increase in computation time.



### Improved Visual Story Generation with Adaptive Context Modeling
- **Arxiv ID**: http://arxiv.org/abs/2305.16811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.16811v1)
- **Published**: 2023-05-26 10:43:42+00:00
- **Updated**: 2023-05-26 10:43:42+00:00
- **Authors**: Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng Feng, Duyu Tang, Shuming Shi, Bing Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models developed on top of powerful text-to-image generation models like Stable Diffusion achieve remarkable success in visual story generation. However, the best-performing approach considers historically generated results as flattened memory cells, ignoring the fact that not all preceding images contribute equally to the generation of the characters and scenes at the current stage. To address this, we present a simple method that improves the leading system with adaptive context modeling, which is not only incorporated in the encoder but also adopted as additional guidance in the sampling stage to boost the global consistency of the generated story. We evaluate our model on PororoSV and FlintstonesSV datasets and show that our approach achieves state-of-the-art FID scores on both story visualization and continuation scenarios. We conduct detailed model analysis and show that our model excels at generating semantically consistent images for stories.



### BEV-IO: Enhancing Bird's-Eye-View 3D Detection with Instance Occupancy
- **Arxiv ID**: http://arxiv.org/abs/2305.16829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16829v1)
- **Published**: 2023-05-26 11:16:12+00:00
- **Updated**: 2023-05-26 11:16:12+00:00
- **Authors**: Zaibin Zhang, Lijun Wang, Yifan Wang, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: A popular approach for constructing bird's-eye-view (BEV) representation in 3D detection is to lift 2D image features onto the viewing frustum space based on explicitly predicted depth distribution. However, depth distribution can only characterize the 3D geometry of visible object surfaces but fails to capture their internal space and overall geometric structure, leading to sparse and unsatisfactory 3D representations. To mitigate this issue, we present BEV-IO, a new 3D detection paradigm to enhance BEV representation with instance occupancy information. At the core of our method is the newly-designed instance occupancy prediction (IOP) module, which aims to infer point-level occupancy status for each instance in the frustum space. To ensure training efficiency while maintaining representational flexibility, it is trained using the combination of both explicit and implicit supervision. With the predicted occupancy, we further design a geometry-aware feature propagation mechanism (GFP), which performs self-attention based on occupancy distribution along each ray in frustum and is able to enforce instance-level feature consistency. By integrating the IOP module with GFP mechanism, our BEV-IO detector is able to render highly informative 3D scene structures with more comprehensive BEV representations. Experimental results demonstrate that BEV-IO can outperform state-of-the-art methods while only adding a negligible increase in parameters (0.2%) and computational overhead (0.24%in GFLOPs).



### OpenVIS: Open-vocabulary Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.16835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16835v1)
- **Published**: 2023-05-26 11:25:59+00:00
- **Updated**: 2023-05-26 11:25:59+00:00
- **Authors**: Pinxue Guo, Tony Huang, Peiyang He, Xuefeng Liu, Tianjun Xiao, Zhaoyu Chen, Wenqiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and study a new computer vision task named open-vocabulary video instance segmentation (OpenVIS), which aims to simultaneously segment, detect, and track arbitrary objects in a video according to corresponding text descriptions. Compared to the original video instance segmentation, OpenVIS enables users to identify objects of desired categories, regardless of whether those categories were included in the training dataset. To achieve this goal, we propose a two-stage pipeline for proposing high-quality class-agnostic object masks and predicting their corresponding categories via pre-trained VLM. Specifically, we first employ a query-based mask proposal network to generate masks of all potential objects, where we replace the original class head with an instance head trained with a binary object loss, thereby enhancing the class-agnostic mask proposal ability. Then, we introduce a proposal post-processing approach to adapt the proposals better to the pre-trained VLMs, avoiding distortion and unnatural proposal inputs. Meanwhile, to facilitate research on this new task, we also propose an evaluation benchmark that utilizes off-the-shelf datasets to comprehensively assess its performance. Experimentally, the proposed OpenVIS exhibits a remarkable 148\% improvement compared to the full-supervised baselines on BURST, which have been trained on all categories.



### Pruning Distorted Images in MNIST Handwritten Digits
- **Arxiv ID**: http://arxiv.org/abs/2307.14343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2307.14343v1)
- **Published**: 2023-05-26 11:44:35+00:00
- **Updated**: 2023-05-26 11:44:35+00:00
- **Authors**: Amarnath R, Vinay Kumar V
- **Comment**: 26 pages, 10 figures, 14 tables, 54 references
- **Journal**: None
- **Summary**: Recognizing handwritten digits is a challenging task primarily due to the diversity of writing styles and the presence of noisy images. The widely used MNIST dataset, which is commonly employed as a benchmark for this task, includes distorted digits with irregular shapes, incomplete strokes, and varying skew in both the training and testing datasets. Consequently, these factors contribute to reduced accuracy in digit recognition. To overcome this challenge, we propose a two-stage deep learning approach. In the first stage, we create a simple neural network to identify distorted digits within the training set. This model serves to detect and filter out such distorted and ambiguous images. In the second stage, we exclude these identified images from the training dataset and proceed to retrain the model using the filtered dataset. This process aims to improve the classification accuracy and confidence levels while mitigating issues of underfitting and overfitting. Our experimental results demonstrate the effectiveness of the proposed approach, achieving an accuracy rate of over 99.5% on the testing dataset. This significant improvement showcases the potential of our method in enhancing digit classification accuracy. In our future work, we intend to explore the scalability of this approach and investigate techniques to further enhance accuracy by reducing the size of the training data.



### Single-Model Attribution of Generative Models Through Final-Layer Inversion
- **Arxiv ID**: http://arxiv.org/abs/2306.06210v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06210v2)
- **Published**: 2023-05-26 13:06:38+00:00
- **Updated**: 2023-07-11 08:37:56+00:00
- **Authors**: Mike Laszkiewicz, Jonas Ricker, Johannes Lederer, Asja Fischer
- **Comment**: None
- **Journal**: None
- **Summary**: Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.



### Image background assessment as a novel technique for insect microhabitat identification
- **Arxiv ID**: http://arxiv.org/abs/2305.18207v1
- **DOI**: None
- **Categories**: **q-bio.PE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.18207v1)
- **Published**: 2023-05-26 13:14:26+00:00
- **Updated**: 2023-05-26 13:14:26+00:00
- **Authors**: Sesa Singha Roy, Reid Tingley, Alan Dorin
- **Comment**: Submitted in Ecological Informatics journal, first review completed,
  19 pages, 10 figures
- **Journal**: None
- **Summary**: The effects of climate change, urbanisation and agriculture are changing the way insects occupy habitats. Some species may utilise anthropogenic microhabitat features for their existence, either because they prefer them to natural features, or because of no choice. Other species are dependent on natural microhabitats. Identifying and analysing these insects' use of natural and anthropogenic microhabitats is important to assess their responses to a changing environment, for improving pollination and managing invasive pests. Traditional studies of insect microhabitat use can now be supplemented by machine learning-based insect image analysis. Typically, research has focused on automatic insect classification, but valuable data in image backgrounds has been ignored. In this research, we analysed the image backgrounds available on the ALA database to determine their microhabitats. We analysed the microhabitats of three insect species common across Australia: Drone flies, European honeybees and European wasps. Image backgrounds were classified as natural or anthropogenic microhabitats using computer vision and machine learning tools benchmarked against a manual classification algorithm. We found flies and honeybees in natural microhabitats, confirming their need for natural havens within cities. Wasps were commonly seen in anthropogenic microhabitats. Results show these insects are well adapted to survive in cities. Management of this invasive pest requires a thoughtful reduction of their access to human-provided resources. The assessment of insect image backgrounds is instructive to document the use of microhabitats by insects. The method offers insight that is increasingly vital for biodiversity management as urbanisation continues to encroach on natural ecosystems and we must consciously provide resources within built environments to maintain insect biodiversity and manage invasive pests.



### PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2305.16914v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.5, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2305.16914v3)
- **Published**: 2023-05-26 13:26:46+00:00
- **Updated**: 2023-06-06 10:01:48+00:00
- **Authors**: Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Rold√£o, Dzmitry Tsishkou
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstruction for large-scale outdoor scenes and achieves SoTA rendering quality on the KITTI-360 NVS benchmark.



### Fast refacing of MR images with a generative neural network lowers re-identification risk and preserves volumetric consistency
- **Arxiv ID**: http://arxiv.org/abs/2305.16922v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.16922v1)
- **Published**: 2023-05-26 13:34:14+00:00
- **Updated**: 2023-05-26 13:34:14+00:00
- **Authors**: Nataliia Molchanova, B√©n√©dicte Mar√©chal, Jean-Philippe Thiran, Tobias Kober, Till Huelnhagen, Jonas Richiardi
- **Comment**: preprint
- **Journal**: None
- **Summary**: With the rise of open data, identifiability of individuals based on 3D renderings obtained from routine structural magnetic resonance imaging (MRI) scans of the head has become a growing privacy concern. To protect subject privacy, several algorithms have been developed to de-identify imaging data using blurring, defacing or refacing. Completely removing facial structures provides the best re-identification protection but can significantly impact post-processing steps, like brain morphometry. As an alternative, refacing methods that replace individual facial structures with generic templates have a lower effect on the geometry and intensity distribution of original scans, and are able to provide more consistent post-processing results by the price of higher re-identification risk and computational complexity. In the current study, we propose a novel method for anonymised face generation for defaced 3D T1-weighted scans based on a 3D conditional generative adversarial network. To evaluate the performance of the proposed de-identification tool, a comparative study was conducted between several existing defacing and refacing tools, with two different segmentation algorithms (FAST and Morphobox). The aim was to evaluate (i) impact on brain morphometry reproducibility, (ii) re-identification risk, (iii) balance between (i) and (ii), and (iv) the processing time. The proposed method takes 9 seconds for face generation and is suitable for recovering consistent post-processing results after defacing.



### How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers
- **Arxiv ID**: http://arxiv.org/abs/2305.16925v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.16925v1)
- **Published**: 2023-05-26 13:38:33+00:00
- **Updated**: 2023-05-26 13:38:33+00:00
- **Authors**: Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, Fisher Yu
- **Comment**: Accepted by/To be published in Robotics: Science and Systems (RSS)
  2023; 11 pages, 5 figures
- **Journal**: None
- **Summary**: Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment -- typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, \eg, end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph.   Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.



### DeepSeaNet: Improving Underwater Object Detection using EfficientDet
- **Arxiv ID**: http://arxiv.org/abs/2306.06075v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.06075v1)
- **Published**: 2023-05-26 13:41:35+00:00
- **Updated**: 2023-05-26 13:41:35+00:00
- **Authors**: Sanyam Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Marine animals and deep underwater objects are difficult to recognize and monitor for safety of aquatic life. There is an increasing challenge when the water is saline with granular particles and impurities. In such natural adversarial environment, traditional approaches like CNN start to fail and are expensive to compute. This project involves implementing and evaluating various object detection models, including EfficientDet, YOLOv5, YOLOv8, and Detectron2, on an existing annotated underwater dataset, called the Brackish-Dataset. The dataset comprises annotated image sequences of fish, crabs, starfish, and other aquatic animals captured in Limfjorden water with limited visibility. The aim of this research project is to study the efficiency of newer models on the same dataset and contrast them with the previous results based on accuracy and inference time. Firstly, I compare the results of YOLOv3 (31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%), YOLOv8 (98.20%), EfficientDet (98.56% mAP) and Detectron2 (95.20% mAP) on the same dataset. Secondly, I provide a modified BiSkFPN mechanism (BiFPN neck with skip connections) to perform complex feature fusion in adversarial noise which makes modified EfficientDet robust to perturbations. Third, analyzed the effect on accuracy of EfficientDet (98.63% mAP) and YOLOv5 by adversarial learning (98.04% mAP). Last, I provide class activation map based explanations (CAM) for the two models to promote Explainability in black box models. Overall, the results indicate that modified EfficientDet achieved higher accuracy with five-fold cross validation than the other models with 88.54% IoU of feature maps.



### On Evaluating Adversarial Robustness of Large Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2305.16934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.CR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.16934v1)
- **Published**: 2023-05-26 13:49:44+00:00
- **Updated**: 2023-05-26 13:49:44+00:00
- **Authors**: Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.



### CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography
- **Arxiv ID**: http://arxiv.org/abs/2305.16936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16936v1)
- **Published**: 2023-05-26 13:52:57+00:00
- **Updated**: 2023-05-26 13:52:57+00:00
- **Authors**: Jiwen Yu, Xuanyu Zhang, Youmin Xu, Jian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Current image steganography techniques are mainly focused on cover-based methods, which commonly have the risk of leaking secret images and poor robustness against degraded container images. Inspired by recent developments in diffusion models, we discovered that two properties of diffusion models, the ability to achieve translation between two images without training, and robustness to noisy data, can be used to improve security and natural robustness in image steganography tasks. For the choice of diffusion model, we selected Stable Diffusion, a type of conditional diffusion model, and fully utilized the latest tools from open-source communities, such as LoRAs and ControlNets, to improve the controllability and diversity of container images. In summary, we propose a novel image steganography framework, named Controllable, Robust and Secure Image Steganography (CRoSS), which has significant advantages in controllability, robustness, and security compared to cover-based image steganography methods. These benefits are obtained without additional training. To our knowledge, this is the first work to introduce diffusion models to the field of image steganography. In the experimental section, we conducted detailed experiments to demonstrate the advantages of our proposed CRoSS framework in controllability, robustness, and security.



### Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination
- **Arxiv ID**: http://arxiv.org/abs/2305.16963v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.16963v1)
- **Published**: 2023-05-26 14:19:17+00:00
- **Updated**: 2023-05-26 14:19:17+00:00
- **Authors**: Yuchen Bai, Jean-Baptiste Durand, Florence Forbes, Gr√©goire Vincent
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR (Light Detection and Ranging) has become an essential part of the remote sensing toolbox used for biosphere monitoring. In particular, LiDAR provides the opportunity to map forest leaf area with unprecedented accuracy, while leaf area has remained an important source of uncertainty affecting models of gas exchanges between the vegetation and the atmosphere. Unmanned Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent revisits to track the response of vegetation to climate change. However, miniature sensors embarked on UAVs usually provide point clouds of limited density, which are further affected by a strong decrease in density from top to bottom of the canopy due to progressively stronger occlusion. In such a context, discriminating leaf points from wood points presents a significant challenge due in particular to strong class imbalance and spatially irregular sampling intensity. Here we introduce a neural network model based on the Pointnet ++ architecture which makes use of point geometry only (excluding any spectral information). To cope with local data sparsity, we propose an innovative sampling scheme which strives to preserve local important geometric information. We also propose a loss function adapted to the severe class imbalance. We show that our model outperforms state-of-the-art alternatives on UAV point clouds. We discuss future possible improvements, particularly regarding much denser point clouds acquired from below the canopy.



### Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling
- **Arxiv ID**: http://arxiv.org/abs/2305.16965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.16965v1)
- **Published**: 2023-05-26 14:20:36+00:00
- **Updated**: 2023-05-26 14:20:36+00:00
- **Authors**: Gongye Liu, Haoze Sun, Jiayi Li, Fei Yin, Yujiu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, diffusion models have demonstrated a remarkable ability to solve inverse problems in an unsupervised manner. Existing methods mainly focus on modifying the posterior sampling process while neglecting the potential of the forward process. In this work, we propose Shortcut Sampling for Diffusion (SSD), a novel pipeline for solving inverse problems. Instead of initiating from random noise, the key concept of SSD is to find the "Embryo", a transitional state that bridges the measurement image y and the restored image x. By utilizing the "shortcut" path of "input-Embryo-output", SSD can achieve precise and fast restoration. To obtain the Embryo in the forward process, We propose Distortion Adaptive Inversion (DA Inversion). Moreover, we apply back projection and attention injection as additional consistency constraints during the generation process. Experimentally, we demonstrate the effectiveness of SSD on several representative tasks, including super-resolution, deblurring, and colorization. Compared to state-of-the-art zero-shot methods, our method achieves competitive results with only 30 NFEs. Moreover, SSD with 100 NFEs can outperform state-of-the-art zero-shot methods in certain tasks.



### Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.16966v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16966v3)
- **Published**: 2023-05-26 14:21:39+00:00
- **Updated**: 2023-06-01 13:31:45+00:00
- **Authors**: Marc Lafon, Elias Ramzi, Cl√©ment Rambour, Nicolas Thome
- **Comment**: None
- **Journal**: International Conference on Machine Learning, ICML 2023, 23-29
  July 2023, Honolulu, Hawaii, USA
- **Summary**: Out-of-distribution (OOD) detection is a critical requirement for the deployment of deep neural networks. This paper introduces the HEAT model, a new post-hoc OOD detection method estimating the density of in-distribution (ID) samples using hybrid energy-based models (EBM) in the feature space of a pre-trained backbone. HEAT complements prior density estimators of the ID density, e.g. parametric models like the Gaussian Mixture Model (GMM), to provide an accurate yet robust density estimation. A second contribution is to leverage the EBM framework to provide a unified density estimation and to compose several energy terms. Extensive experiments demonstrate the significance of the two contributions. HEAT sets new state-of-the-art OOD detection results on the CIFAR-10 / CIFAR-100 benchmark as well as on the large-scale Imagenet benchmark. The code is available at: https://github.com/MarcLafon/heatood.



### Linear Object Detection in Document Images using Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2305.16968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16968v1)
- **Published**: 2023-05-26 14:22:03+00:00
- **Updated**: 2023-05-26 14:22:03+00:00
- **Authors**: Philippe Bernet, Joseph Chazalon, Edwin Carlinet, Alexandre Bourquelot, Elodie Puybareau
- **Comment**: Accepted to ICDAR 2023
- **Journal**: None
- **Summary**: Linear objects convey substantial information about document structure, but are challenging to detect accurately because of degradation (curved, erased) or decoration (doubled, dashed). Many approaches can recover some vector representation, but only one closed-source technique introduced in 1994, based on Kalman filters (a particular case of Multiple Object Tracking algorithm), can perform a pixel-accurate instance segmentation of linear objects and enable to selectively remove them from the original image. We aim at re-popularizing this approach and propose: 1. a framework for accurate instance segmentation of linear objects in document images using Multiple Object Tracking (MOT); 2. document image datasets and metrics which enable both vector- and pixel-based evaluation of linear object detection; 3. performance measures of MOT approaches against modern segment detectors; 4. performance measures of various tracking strategies, exhibiting alternatives to the original Kalman filters approach; and 5. an open-source implementation of a detector which can discriminate instances of curved, erased, dashed, intersecting and/or overlapping linear objects.



### Maskomaly:Zero-Shot Mask Anomaly Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.16972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.16972v2)
- **Published**: 2023-05-26 14:28:09+00:00
- **Updated**: 2023-08-25 23:28:45+00:00
- **Authors**: Jan Ackermann, Christos Sakaridis, Fisher Yu
- **Comment**: BMVC 2023
- **Journal**: None
- **Summary**: We present a simple and practical framework for anomaly segmentation called Maskomaly. It builds upon mask-based standard semantic segmentation networks by adding a simple inference-time post-processing step which leverages the raw mask outputs of such networks. Maskomaly does not require additional training and only adds a small computational overhead to inference. Most importantly, it does not require anomalous data at training. We show top results for our method on SMIYC, RoadAnomaly, and StreetHazards. On the most central benchmark, SMIYC, Maskomaly outperforms all directly comparable approaches. Further, we introduce a novel metric that benefits the development of robust anomaly segmentation methods and demonstrate its informativeness on RoadAnomaly.



### NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models
- **Arxiv ID**: http://arxiv.org/abs/2305.16986v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.16986v2)
- **Published**: 2023-05-26 14:41:06+00:00
- **Updated**: 2023-05-29 04:49:00+00:00
- **Authors**: Gengze Zhou, Yicong Hong, Qi Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models.



### Three Towers: Flexible Contrastive Learning with Pretrained Image Models
- **Arxiv ID**: http://arxiv.org/abs/2305.16999v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.16999v2)
- **Published**: 2023-05-26 14:59:16+00:00
- **Updated**: 2023-05-29 08:48:00+00:00
- **Authors**: Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, Efi Kokiopoulou
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperforms relative to LiT for JFT-pretrained models, it outperforms LiT for ImageNet-21k and Places365 pretraining.



### Zero-shot Visual Question Answering with Language Model Feedback
- **Arxiv ID**: http://arxiv.org/abs/2305.17006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2305.17006v1)
- **Published**: 2023-05-26 15:04:20+00:00
- **Updated**: 2023-05-26 15:04:20+00:00
- **Authors**: Yifan Du, Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen
- **Comment**: Accepted by ACL2023 findings
- **Journal**: None
- **Summary**: In this paper, we propose a novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-trained Language model (PLM). As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task goal and information need from the PLM. To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the captioning model according to the task goal (learning from feedback of the PLM). Extensive experiments demonstrate the effectiveness of the proposed approach on the knowledge-based VQA task. Specifically, on the challenging A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Our code is publicly available at https://github.com/RUCAIBox/LAMOC.



### Improving Knowledge Distillation via Regularizing Feature Norm and Direction
- **Arxiv ID**: http://arxiv.org/abs/2305.17007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17007v1)
- **Published**: 2023-05-26 15:05:19+00:00
- **Updated**: 2023-05-26 15:05:19+00:00
- **Authors**: Yuzhu Wang, Lechao Cheng, Manni Duan, Yongheng Wang, Zunlei Feng, Shu Kong
- **Comment**: 16 pages, 8 figures, 6 tables
- **Journal**: None
- **Summary**: Knowledge distillation (KD) exploits a large well-trained model (i.e., teacher) to train a small student model on the same dataset for the same task. Treating teacher features as knowledge, prevailing methods of knowledge distillation train student by aligning its features with the teacher's, e.g., by minimizing the KL-divergence between their logits or L2 distance between their intermediate features. While it is natural to believe that better alignment of student features to the teacher better distills teacher knowledge, simply forcing this alignment does not directly contribute to the student's performance, e.g., classification accuracy. In this work, we propose to align student features with class-mean of teacher features, where class-mean naturally serves as a strong classifier. To this end, we explore baseline techniques such as adopting the cosine distance based loss to encourage the similarity between student features and their corresponding class-means of the teacher. Moreover, we train the student to produce large-norm features, inspired by other lines of work (e.g., model pruning and domain adaptation), which find the large-norm features to be more significant. Finally, we propose a rather simple loss term (dubbed ND loss) to simultaneously (1) encourage student to produce large-\emph{norm} features, and (2) align the \emph{direction} of student features and teacher class-means. Experiments on standard benchmarks demonstrate that our explored techniques help existing KD methods achieve better performance, i.e., higher classification accuracy on ImageNet and CIFAR100 datasets, and higher detection precision on COCO dataset. Importantly, our proposed ND loss helps the most, leading to the state-of-the-art performance on these benchmarks. The source code is available at \url{https://github.com/WangYZ1608/Knowledge-Distillation-via-ND}.



### SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.17011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17011v1)
- **Published**: 2023-05-26 15:13:44+00:00
- **Updated**: 2023-05-26 15:13:44+00:00
- **Authors**: Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies referring video object segmentation (RVOS) by boosting video-level visual-linguistic alignment. Recent approaches model the RVOS task as a sequence prediction problem and perform multi-modal interaction as well as segmentation for each frame separately. However, the lack of a global view of video content leads to difficulties in effectively utilizing inter-frame relationships and understanding textual descriptions of object temporal variations. To address this issue, we propose Semantic-assisted Object Cluster (SOC), which aggregates video content and textual guidance for unified temporal modeling and cross-modal alignment. By associating a group of frame-level object embeddings with language tokens, SOC facilitates joint space learning across modalities and time steps. Moreover, we present multi-modal contrastive supervision to help construct well-aligned joint space at the video level. We conduct extensive experiments on popular RVOS benchmarks, and our method outperforms state-of-the-art competitors on all benchmarks by a remarkable margin. Besides, the emphasis on temporal coherence enhances the segmentation stability and adaptability of our method in processing text expressions with temporal variations. Code will be available.



### Are Deep Neural Networks Adequate Behavioural Models of Human Visual Perception?
- **Arxiv ID**: http://arxiv.org/abs/2305.17023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2305.17023v1)
- **Published**: 2023-05-26 15:31:06+00:00
- **Updated**: 2023-05-26 15:31:06+00:00
- **Authors**: Felix A. Wichmann, Robert Geirhos
- **Comment**: Preprint version of article accepted by Annual Review of Vision
  Science
  (https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-120522-031739).
  Posted with permission from the Annual Review of Vision Science, Volume 9 by
  Annual Reviews, http://www.annualreviews.org
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are machine learning algorithms that have revolutionised computer vision due to their remarkable successes in tasks like object classification and segmentation. The success of DNNs as computer vision algorithms has led to the suggestion that DNNs may also be good models of human visual perception. We here review evidence regarding current DNNs as adequate behavioural models of human core object recognition. To this end, we argue that it is important to distinguish between statistical tools and computational models, and to understand model quality as a multidimensional concept where clarity about modelling goals is key. Reviewing a large number of psychophysical and computational explorations of core object recognition performance in humans and DNNs, we argue that DNNs are highly valuable scientific tools but that as of today DNNs should only be regarded as promising -- but not yet adequate -- computational models of human core object recognition behaviour. On the way we dispel a number of myths surrounding DNNs in vision science.



### Contouring by Unit Vector Field Regression
- **Arxiv ID**: http://arxiv.org/abs/2305.17024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17024v1)
- **Published**: 2023-05-26 15:32:22+00:00
- **Updated**: 2023-05-26 15:32:22+00:00
- **Authors**: Amir Jamaludin, Sarim Ather, Timor Kadir, Rhydian Windsor
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI) 2023
- **Journal**: None
- **Summary**: This work introduces a simple deep-learning based method to delineate contours by `walking' along learnt unit vector fields. We demonstrate the effectiveness of our pipeline on the unique case of open contours on the task of delineating the sacroiliac joints (SIJs) in spinal MRIs. We show that: (i) 95% of the time the average root mean square error of the predicted contour against the original ground truth is below 4.5 pixels (2.5mm for a standard T1-weighted SIJ MRI), and (ii) the proposed method is better than the baseline of regressing vertices or landmarks of contours.



### The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)
- **Arxiv ID**: http://arxiv.org/abs/2305.17033v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2305.17033v2)
- **Published**: 2023-05-26 15:40:11+00:00
- **Updated**: 2023-07-07 20:59:43+00:00
- **Authors**: Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
- **Comment**: None
- **Journal**: None
- **Summary**: Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain glioma through standardized quantitative performance evaluation metrics utilized across the BraTS 2023 cluster of challenges. Models gaining knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training data will be evaluated on separate validation and unseen test mpMRI dataof high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge brings together clinicians and AI/imaging scientists to lead to faster development of automated segmentation techniques that could benefit clinical trials, and ultimately the care of children with brain tumors.



### SelfClean: A Self-Supervised Data Cleaning Strategy
- **Arxiv ID**: http://arxiv.org/abs/2305.17048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17048v1)
- **Published**: 2023-05-26 15:57:04+00:00
- **Updated**: 2023-05-26 15:57:04+00:00
- **Authors**: Fabian Gr√∂ger, Simone Lionetti, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Ludovic Amruthalingam, Labelling Consortium, Matthew Groh, Alexander A. Navarini, Marc Pouly
- **Comment**: None
- **Journal**: None
- **Summary**: Most commonly used benchmark datasets for computer vision contain irrelevant images, near duplicates, and label errors. Consequently, model performance on these benchmarks may not be an accurate estimate of generalization ability. This is a particularly acute concern in computer vision for medicine where datasets are typically small, stakes are high, and annotation processes are expensive and error-prone. In this paper, we propose SelfClean, a general procedure to clean up image datasets exploiting a latent space learned with self-supervision. By relying on self-supervised learning, our approach focuses on intrinsic properties of the data and avoids annotation biases. We formulate dataset cleaning as either a set of ranking problems, where human experts can make decisions with significantly reduced effort, or a set of scoring problems, where decisions can be fully automated based on score distributions. We compare SelfClean against other algorithms on common computer vision benchmarks enhanced with synthetic noise and demonstrate state-of-the-art performance on detecting irrelevant images, near duplicates, and label errors. In addition, we apply our method to multiple image datasets and confirm an improvement in evaluation reliability.



### Extremely weakly-supervised blood vessel segmentation with physiologically based synthesis and domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2305.17054v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.17054v1)
- **Published**: 2023-05-26 16:01:49+00:00
- **Updated**: 2023-05-26 16:01:49+00:00
- **Authors**: Peidi Xu, Olga Sosnovtseva, Charlotte Mehlin S√∏rensen, Kenny Erleben, Sune Darkner
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate analysis and modeling of renal functions require a precise segmentation of the renal blood vessels. Micro-CT scans provide image data at higher resolutions, making more small vessels near the renal cortex visible. Although deep-learning-based methods have shown state-of-the-art performance in automatic blood vessel segmentations, they require a large amount of labeled training data. However, voxel-wise labeling in micro-CT scans is extremely time-consuming given the huge volume sizes. To mitigate the problem, we simulate synthetic renal vascular trees physiologically while generating corresponding scans of the simulated trees by training a generative model on unlabeled scans. This enables the generative model to learn the mapping implicitly without the need for explicit functions to emulate the image acquisition process. We further propose an additional segmentation branch over the generative model trained on the generated scans. We demonstrate that the model can directly segment blood vessels on real scans and validate our method on both 3D micro-CT scans of rat kidneys and a proof-of-concept experiment on 2D retinal images. Code and 3D results are available at https://github.com/miccai2023anony/RenalVesselSeg



### Mindstorms in Natural Language-Based Societies of Mind
- **Arxiv ID**: http://arxiv.org/abs/2305.17066v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG, cs.MA, 68T07, I.2.6; I.2.11
- **Links**: [PDF](http://arxiv.org/pdf/2305.17066v1)
- **Published**: 2023-05-26 16:21:25+00:00
- **Updated**: 2023-05-26 16:21:25+00:00
- **Authors**: Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R. Ashley, R√≥bert Csord√°s, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li, Shuming Liu, Jinjie Mai, Piotr Piƒôkos, Aditya Ramesh, Imanol Schlag, Weimin Shi, Aleksandar Staniƒá, Wenyi Wang, Yuhui Wang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, J√ºrgen Schmidhuber
- **Comment**: 9 pages in main text + 7 pages of references + 38 pages of
  appendices, 14 figures in main text + 13 in appendices, 7 tables in
  appendices
- **Journal**: None
- **Summary**: Both Minsky's "society of mind" and Schmidhuber's "learning to think" inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a "mindstorm." Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs), new agents -- all communicating through the same universal symbolic language -- are easily added in a modular fashion. To demonstrate the power of NLSOMs, we assemble and experiment with several of them (having up to 129 members), leveraging mindstorms in them to solve some practical AI tasks: visual question answering, image captioning, text-to-image synthesis, 3D generation, egocentric retrieval, embodied AI, and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents-some of which may be humans. And with this emergence of great societies of heterogeneous minds, many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM? What would be the (dis)advantages of having a monarchical rather than a democratic structure? How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM? In this work, we identify, discuss, and try to answer some of these questions.



### SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox Based on PyTorch
- **Arxiv ID**: http://arxiv.org/abs/2305.17091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17091v1)
- **Published**: 2023-05-26 17:02:42+00:00
- **Updated**: 2023-05-26 17:02:42+00:00
- **Authors**: Zhenchao Jin
- **Comment**: tech report
- **Journal**: None
- **Summary**: This paper presents SSSegmenation, which is an open source supervised semantic image segmentation toolbox based on PyTorch. The design of this toolbox is motivated by MMSegmentation while it is easier to use because of fewer dependencies and achieves superior segmentation performance under a comparable training and testing setup. Moreover, the toolbox also provides plenty of trained weights for popular and contemporary semantic segmentation methods, including Deeplab, PSPNet, OCRNet, MaskFormer, \emph{etc}. We expect that this toolbox can contribute to the future development of semantic segmentation. Codes and model zoos are available at \href{https://github.com/SegmentationBLWX/sssegmentation/}{SSSegmenation}.



### GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.17096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17096v1)
- **Published**: 2023-05-26 17:10:24+00:00
- **Updated**: 2023-05-26 17:10:24+00:00
- **Authors**: Tanveer Hannan, Rajat Koner, Maximilian Bernhard, Suprosanna Shit, Bjoern Menze, Volker Tresp, Matthias Schubert, Thomas Seidl
- **Comment**: 14 pages, 5 tables, 9 figures
- **Journal**: None
- **Summary**: Recent trends in Video Instance Segmentation (VIS) have seen a growing reliance on online methods to model complex and lengthy video sequences. However, the degradation of representation and noise accumulation of the online methods, especially during occlusion and abrupt changes, pose substantial challenges. Transformer-based query propagation provides promising directions at the cost of quadratic memory attention. However, they are susceptible to the degradation of instance features due to the above-mentioned challenges and suffer from cascading effects. The detection and rectification of such errors remain largely underexplored. To this end, we introduce \textbf{GRAtt-VIS}, \textbf{G}ated \textbf{R}esidual \textbf{Att}ention for \textbf{V}ideo \textbf{I}nstance \textbf{S}egmentation. Firstly, we leverage a Gumbel-Softmax-based gate to detect possible errors in the current frame. Next, based on the gate activation, we rectify degraded features from its past representation. Such a residual configuration alleviates the need for dedicated memory and provides a continuous stream of relevant instance features. Secondly, we propose a novel inter-instance interaction using gate activation as a mask for self-attention. This masking strategy dynamically restricts the unrepresentative instance queries in the self-attention and preserves vital information for long-term tracking. We refer to this novel combination of Gated Residual Connection and Masked Self-Attention as \textbf{GRAtt} block, which can easily be integrated into the existing propagation-based framework. Further, GRAtt blocks significantly reduce the attention overhead and simplify dynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance on YouTube-VIS and the highly challenging OVIS dataset, significantly improving over previous methods. Code is available at \url{https://github.com/Tanveer81/GRAttVIS}.



### ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing
- **Arxiv ID**: http://arxiv.org/abs/2305.17098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17098v1)
- **Published**: 2023-05-26 17:13:55+00:00
- **Updated**: 2023-05-26 17:13:55+00:00
- **Authors**: Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present ControlVideo, a novel method for text-driven video editing. Leveraging the capabilities of text-to-image diffusion models and ControlNet, ControlVideo aims to enhance the fidelity and temporal consistency of videos that align with a given text while preserving the structure of the source video. This is achieved by incorporating additional conditions such as edge maps, fine-tuning the key-frame and temporal attention on the source video-text pair with carefully designed strategies. An in-depth exploration of ControlVideo's design is conducted to inform future research on one-shot tuning video diffusion models. Quantitatively, ControlVideo outperforms a range of competitive baselines in terms of faithfulness and consistency while still aligning with the textual prompt. Additionally, it delivers videos with high visual realism and fidelity w.r.t. the source content, demonstrating flexibility in utilizing controls containing varying degrees of source video information, and the potential for multiple control combinations. The project page is available at \href{https://ml.cs.tsinghua.edu.cn/controlvideo/}{https://ml.cs.tsinghua.edu.cn/controlvideo/}.



### GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2305.17102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17102v1)
- **Published**: 2023-05-26 17:15:22+00:00
- **Updated**: 2023-05-26 17:15:22+00:00
- **Authors**: Jingyang Huo, Qiang Sun, Boyan Jiang, Haitao Lin, Yanwei Fu
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Most existing works solving Room-to-Room VLN problem only utilize RGB images and do not consider local context around candidate views, which lack sufficient visual cues about surrounding environment. Moreover, natural language contains complex semantic information thus its correlations with visual inputs are hard to model merely with cross attention. In this paper, we propose GeoVLN, which learns Geometry-enhanced visual representation based on slot attention for robust Visual-and-Language Navigation. The RGB images are compensated with the corresponding depth maps and normal maps predicted by Omnidata as visual inputs. Technically, we introduce a two-stage module that combine local slot attention and CLIP model to produce geometry-enhanced representation from such input. We employ V&L BERT to learn a cross-modal representation that incorporate both language and vision informations. Additionally, a novel multiway attention module is designed, encouraging different phrases of input instruction to exploit the most related features from visual input. Extensive experiments demonstrate the effectiveness of our newly designed modules and show the compelling performance of the proposed method.



### High-Fidelity Image Compression with Score-based Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2305.18231v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2305.18231v1)
- **Published**: 2023-05-26 17:16:16+00:00
- **Updated**: 2023-05-26 17:16:16+00:00
- **Authors**: Emiel Hoogeboom, Eirikur Agustsson, Fabian Mentzer, Luca Versari, George Toderici, Lucas Theis
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.



### Random-Access Neural Compression of Material Textures
- **Arxiv ID**: http://arxiv.org/abs/2305.17105v1
- **DOI**: 10.1145/3592407
- **Categories**: **cs.GR**, cs.CV, I.3
- **Links**: [PDF](http://arxiv.org/pdf/2305.17105v1)
- **Published**: 2023-05-26 17:16:22+00:00
- **Updated**: 2023-05-26 17:16:22+00:00
- **Authors**: Karthik Vaidyanathan, Marco Salvi, Bartlomiej Wronski, Tomas Akenine-M√∂ller, Pontus Ebelin, Aaron Lefohn
- **Comment**: 22 pages, accepted to ACM SIGGRAPH 2023 Transactions on Graphics
- **Journal**: ACM Transactions on Graphics; Volume 42; Issue 4 (2023); Article
  No.: 88; pp 1-25
- **Summary**: The continuous advancement of photorealism in rendering is accompanied by a growth in texture data and, consequently, increasing storage and memory demands. To address this issue, we propose a novel neural compression technique specifically designed for material textures. We unlock two more levels of detail, i.e., 16x more texels, using low bitrate compression, with image quality that is better than advanced image compression techniques, such as AVIF and JPEG XL. At the same time, our method allows on-demand, real-time decompression with random access similar to block texture compression on GPUs, enabling compression on disk and memory. The key idea behind our approach is compressing multiple material textures and their mipmap chains together, and using a small neural network, that is optimized for each material, to decompress them. Finally, we use a custom training implementation to achieve practical compression speeds, whose performance surpasses that of general frameworks, like PyTorch, by an order of magnitude.



### Manifold Regularization for Memory-Efficient Training of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.17119v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2305.17119v1)
- **Published**: 2023-05-26 17:40:15+00:00
- **Updated**: 2023-05-26 17:40:15+00:00
- **Authors**: Shadi Sartipi, Edgar A. Bernal
- **Comment**: None
- **Journal**: None
- **Summary**: One of the prevailing trends in the machine- and deep-learning community is to gravitate towards the use of increasingly larger models in order to keep pushing the state-of-the-art performance envelope. This tendency makes access to the associated technologies more difficult for the average practitioner and runs contrary to the desire to democratize knowledge production in the field. In this paper, we propose a framework for achieving improved memory efficiency in the process of learning traditional neural networks by leveraging inductive-bias-driven network design principles and layer-wise manifold-oriented regularization objectives. Use of the framework results in improved absolute performance and empirical generalization error relative to traditional learning techniques. We provide empirical validation of the framework, including qualitative and quantitative evidence of its effectiveness on two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed framework can be seamlessly combined with existing network compression methods for further memory savings.



### NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support
- **Arxiv ID**: http://arxiv.org/abs/2305.17134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17134v1)
- **Published**: 2023-05-26 17:59:21+00:00
- **Updated**: 2023-05-26 17:59:21+00:00
- **Authors**: Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, Hao Su
- **Comment**: Project page: https://sarahweiii.github.io/neumanifold/
- **Journal**: None
- **Summary**: We present a method for generating high-quality watertight manifold meshes from multi-view input images. Existing volumetric rendering methods are robust in optimization but tend to generate noisy meshes with poor topology. Differentiable rasterization-based methods can generate high-quality meshes but are sensitive to initialization. Our method combines the benefits of both worlds; we take the geometry initialization obtained from neural volumetric fields, and further optimize the geometry as well as a compact neural texture representation with differentiable rasterizers. Through extensive experiments, we demonstrate that our method can generate accurate mesh reconstructions with faithful appearance that are comparable to previous volume rendering methods while being an order of magnitude faster in rendering. We also show that our generated mesh and neural texture reconstruction is compatible with existing graphics pipelines and enables downstream 3D applications such as simulation. Project page: https://sarahweiii.github.io/neumanifold/



### Selective Communication for Cooperative Perception in End-to-End Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2305.17181v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.17181v1)
- **Published**: 2023-05-26 18:13:17+00:00
- **Updated**: 2023-05-26 18:13:17+00:00
- **Authors**: Hsu-kuang Chiu, Stephen F. Smith
- **Comment**: Scalable Autonomous Driving Workshop of IEEE International Conference
  on Robotics and Automation (ICRA Workshop), 2023
- **Journal**: None
- **Summary**: The reliability of current autonomous driving systems is often jeopardized in situations when the vehicle's field-of-view is limited by nearby occluding objects. To mitigate this problem, vehicle-to-vehicle communication to share sensor information among multiple autonomous driving vehicles has been proposed. However, to enable timely processing and use of shared sensor data, it is necessary to constrain communication bandwidth, and prior work has done so by restricting the number of other cooperative vehicles and randomly selecting the subset of vehicles to exchange information with from all those that are within communication range. Although simple and cost effective from a communication perspective, this selection approach suffers from its susceptibility to missing those vehicles that possess the perception information most critical to navigation planning. Inspired by recent multi-agent path finding research, we propose a novel selective communication algorithm for cooperative perception to address this shortcoming. Implemented with a lightweight perception network and a previously developed control network, our algorithm is shown to produce higher success rates than a random selection approach on previously studied safety-critical driving scenario simulations, with minimal additional communication overhead.



### Image Quality Is Not All You Want: Task-Driven Lens Design for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2305.17185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2305.17185v1)
- **Published**: 2023-05-26 18:20:56+00:00
- **Updated**: 2023-05-26 18:20:56+00:00
- **Authors**: Xinge Yang, Qiang Fu, Yunfeng Nie, Wolfgang Heidrich
- **Comment**: Use an image classification network to supervise the lens design from
  scratch. The final designs can achieve higher accuracy with fewer optical
  elements
- **Journal**: None
- **Summary**: In computer vision, it has long been taken for granted that high-quality images obtained through well-designed camera lenses would lead to superior results. However, we find that this common perception is not a "one-size-fits-all" solution for diverse computer vision tasks. We demonstrate that task-driven and deep-learned simple optics can actually deliver better visual task performance. The Task-Driven lens design approach, which relies solely on a well-trained network model for supervision, is proven to be capable of designing lenses from scratch. Experimental results demonstrate the designed image classification lens (``TaskLens'') exhibits higher accuracy compared to conventional imaging-driven lenses, even with fewer lens elements. Furthermore, we show that our TaskLens is compatible with various network models while maintaining enhanced classification accuracy. We propose that TaskLens holds significant potential, particularly when physical dimensions and cost are severely constrained.



### Live American Sign Language Letter Classification with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2305.17192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2305.17192v1)
- **Published**: 2023-05-26 18:29:33+00:00
- **Updated**: 2023-05-26 18:29:33+00:00
- **Authors**: Kyle Boone, Ben Wurster, Seth Thao, Yu Hen Hu
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: This project is centered around building a neural network that is able to recognize ASL letters in images, particularly within the scope of a live video feed. Initial testing results came up short of expectations when both the convolutional network and VGG16 transfer learning approaches failed to generalize in settings of different backgrounds. The use of a pre-trained hand joint detection model was then adopted with the produced joint locations being fed into a fully-connected neural network. The results of this approach exceeded those of prior methods and generalized well to a live video feed application.



### AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth
- **Arxiv ID**: http://arxiv.org/abs/2305.17193v1
- **DOI**: None
- **Categories**: **q-bio.SC**, cs.AI, cs.CV, cs.LG, physics.bio-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2305.17193v1)
- **Published**: 2023-05-26 18:31:53+00:00
- **Updated**: 2023-05-26 18:31:53+00:00
- **Authors**: Ivan R. Nabi, Ben Cardoen, Ismail M. Khater, Guang Gao, Timothy H. Wong, Ghassan Hamarneh
- **Comment**: 14 pages, 3 figures
- **Journal**: None
- **Summary**: The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.



### Large language models improve Alzheimer's disease diagnosis using multi-modality data
- **Arxiv ID**: http://arxiv.org/abs/2305.19280v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.19280v1)
- **Published**: 2023-05-26 18:42:19+00:00
- **Updated**: 2023-05-26 18:42:19+00:00
- **Authors**: Yingjie Feng, Jun Wang, Xianfeng Gu, Xiaoyin Xu, Min Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.



### Building One-class Detector for Anything: Open-vocabulary Zero-shot OOD Detection Using Text-image Models
- **Arxiv ID**: http://arxiv.org/abs/2305.17207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17207v1)
- **Published**: 2023-05-26 18:58:56+00:00
- **Updated**: 2023-05-26 18:58:56+00:00
- **Authors**: Yunhao Ge, Jie Ren, Jiaping Zhao, Kaifeng Chen, Andrew Gallagher, Laurent Itti, Balaji Lakshminarayanan
- **Comment**: 16 pages (including appendix and references), 3 figures
- **Journal**: None
- **Summary**: We focus on the challenge of out-of-distribution (OOD) detection in deep learning models, a crucial aspect in ensuring reliability. Despite considerable effort, the problem remains significantly challenging in deep learning models due to their propensity to output over-confident predictions for OOD inputs. We propose a novel one-class open-set OOD detector that leverages text-image pre-trained models in a zero-shot fashion and incorporates various descriptions of in-domain and OOD. Our approach is designed to detect anything not in-domain and offers the flexibility to detect a wide variety of OOD, defined via fine- or coarse-grained labels, or even in natural language. We evaluate our approach on challenging benchmarks including large-scale datasets containing fine-grained, semantically similar classes, distributionally shifted images, and multi-object images containing a mixture of in-domain and OOD objects. Our method shows superior performance over previous methods on all benchmarks. Code is available at https://github.com/gyhandy/One-Class-Anything



### Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities
- **Arxiv ID**: http://arxiv.org/abs/2305.17214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17214v1)
- **Published**: 2023-05-26 19:16:23+00:00
- **Updated**: 2023-05-26 19:16:23+00:00
- **Authors**: Jingyuan Sun, Mingxiao Li, Zijiao Chen, Yunhao Zhang, Shaonan Wang, Marie-Francine Moens
- **Comment**: 17 pages, 6 figures, conference
- **Journal**: None
- **Summary**: Decoding visual stimuli from neural responses recorded by functional Magnetic Resonance Imaging (fMRI) presents an intriguing intersection between cognitive neuroscience and machine learning, promising advancements in understanding human visual perception and building non-invasive brain-machine interfaces. However, the task is challenging due to the noisy nature of fMRI signals and the intricate pattern of brain visual representations. To mitigate these challenges, we introduce a two-phase fMRI representation learning framework. The first phase pre-trains an fMRI feature learner with a proposed Double-contrastive Mask Auto-encoder to learn denoised representations. The second phase tunes the feature learner to attend to neural activation patterns most informative for visual reconstruction with guidance from an image auto-encoder. The optimized fMRI feature learner then conditions a latent diffusion model to reconstruct image stimuli from brain activities. Experimental results demonstrate our model's superiority in generating high-resolution and semantically accurate images, substantially exceeding previous state-of-the-art methods by 39.34% in the 50-way-top-1 semantic classification accuracy. Our research invites further exploration of the decoding task's potential and contributes to the development of non-invasive brain-machine interfaces.



### Generating Images with Multimodal Language Models
- **Arxiv ID**: http://arxiv.org/abs/2305.17216v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.17216v2)
- **Published**: 2023-05-26 19:22:03+00:00
- **Updated**: 2023-06-13 22:13:51+00:00
- **Authors**: Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov
- **Comment**: Project page: http://jykoh.com/gill
- **Journal**: None
- **Summary**: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.



### GVdoc: Graph-based Visual Document Classification
- **Arxiv ID**: http://arxiv.org/abs/2305.17219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.17219v1)
- **Published**: 2023-05-26 19:23:20+00:00
- **Updated**: 2023-05-26 19:23:20+00:00
- **Authors**: Fnu Mohbat, Mohammed J. Zaki, Catherine Finegan-Dollak, Ashish Verma
- **Comment**: None
- **Journal**: None
- **Summary**: The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms state-of-the-art models on out-of-distribution data while retaining comparable performance on the in-distribution test set.



### VoxDet: Voxel Learning for Novel Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/2305.17220v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17220v3)
- **Published**: 2023-05-26 19:25:13+00:00
- **Updated**: 2023-06-04 14:22:17+00:00
- **Authors**: Bowen Li, Jiashun Wang, Yaoyu Hu, Chen Wang, Sebastian Scherer
- **Comment**: 17 pages, 10 figures
- **Journal**: None
- **Summary**: Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world nature. Traditional methodologies, which primarily rely on 2D representations and matching techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA) module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated camera poses, these features are aggregated into a compact 3D template voxel. In novel instance detection, this voxel representation demonstrates heightened resilience to occlusion and pose variations. We also discover that a 3D reconstruction objective helps to pre-train the 2D-3D mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel Matching (QVM) module. The 2D queries are first converted into their voxel representation with the learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and efficiency. Exhaustive experiments are conducted on the demanding LineMod-Occlusion, YCB-video, and the newly built RoboTools benchmarks, where VoxDet outperforms various 2D baselines remarkably with 20% higher recall and faster speed. To the best of our knowledge, VoxDet is the first to incorporate implicit 3D knowledge for 2D detection tasks.



### Do We Really Need a Large Number of Visual Prompts?
- **Arxiv ID**: http://arxiv.org/abs/2305.17223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.17223v1)
- **Published**: 2023-05-26 19:31:57+00:00
- **Updated**: 2023-05-26 19:31:57+00:00
- **Authors**: Youngeun Kim, Yuhang Li, Abhishek Moitra, Priyadarshini Panda
- **Comment**: None
- **Journal**: None
- **Summary**: Due to increasing interest in adapting models on resource-constrained edges, parameter-efficient transfer learning has been widely explored. Among various methods, Visual Prompt Tuning (VPT), prepending learnable prompts to input space, shows competitive fine-tuning performance compared to training of full network parameters. However, VPT increases the number of input tokens, resulting in additional computational overhead. In this paper, we analyze the impact of the number of prompts on fine-tuning performance and self-attention operation in a vision transformer architecture. Through theoretical and empirical analysis we show that adding more prompts does not lead to linear performance improvement. Further, we propose a Prompt Condensation (PC) technique that aims to prevent performance degradation from using a small number of prompts. We validate our methods on FGVC and VTAB-1k tasks and show that our approach reduces the number of prompts by ~70% while maintaining accuracy.



### COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2305.17235v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.17235v2)
- **Published**: 2023-05-26 19:50:00+00:00
- **Updated**: 2023-06-09 16:11:21+00:00
- **Authors**: Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan
- **Comment**: ICML 2023 Poster
- **Journal**: None
- **Summary**: Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\times$ speedup) and lower extra storage cost (up to $1927.5\times$ reduction) than the existing works.



### Error Estimation for Single-Image Human Body Mesh Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2305.17245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17245v2)
- **Published**: 2023-05-26 20:18:42+00:00
- **Updated**: 2023-05-31 00:02:39+00:00
- **Authors**: Hamoon Jafarian, Faisal Z. Qureshi
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose and shape estimation methods continue to suffer in situations where one or more parts of the body are occluded. More importantly, these methods cannot express when their predicted pose is incorrect. This has serious consequences when these methods are used in human-robot interaction scenarios, where we need methods that can evaluate their predictions and flag situations where they might be wrong. This work studies this problem. We propose a method that combines information from OpenPose and SPIN -- two popular human pose and shape estimation methods -- to highlight regions on the predicted mesh that are least reliable. We have evaluated the proposed approach on 3DPW, 3DOH, and Human3.6M datasets, and the results demonstrate our model's effectiveness in identifying inaccurate regions of the human body mesh. Our code is available at https://github.com/Hamoon1987/meshConfidence.



### Generalizable Pose Estimation Using Implicit Scene Representations
- **Arxiv ID**: http://arxiv.org/abs/2305.17252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17252v1)
- **Published**: 2023-05-26 20:42:52+00:00
- **Updated**: 2023-05-26 20:42:52+00:00
- **Authors**: Vaibhav Saxena, Kamal Rahimi Malekshan, Linh Tran, Yotto Koga
- **Comment**: None
- **Journal**: None
- **Summary**: 6-DoF pose estimation is an essential component of robotic manipulation pipelines. However, it usually suffers from a lack of generalization to new instances and object types. Most widely used methods learn to infer the object pose in a discriminative setup where the model filters useful information to infer the exact pose of the object. While such methods offer accurate poses, the model does not store enough information to generalize to new objects. In this work, we address the generalization capability of pose estimation using models that contain enough information about the object to render it in different poses. We follow the line of work that inverts neural renderers to infer the pose. We propose i-$\sigma$SRN to maximize the information flowing from the input pose to the rendered scene and invert them to infer the pose given an input image. Specifically, we extend Scene Representation Networks (SRNs) by incorporating a separate network for density estimation and introduce a new way of obtaining a weighted scene representation. We investigate several ways of initial pose estimates and losses for the neural renderer. Our final evaluation shows a significant improvement in inference performance and speed compared to existing approaches.



### Study of Subjective and Objective Quality Assessment of Mobile Cloud Gaming Videos
- **Arxiv ID**: http://arxiv.org/abs/2305.17260v1
- **DOI**: 10.1109/TIP.2023.3281170
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.17260v1)
- **Published**: 2023-05-26 21:08:17+00:00
- **Updated**: 2023-05-26 21:08:17+00:00
- **Authors**: Avinab Saha, Yu-Chih Chen, Chase Davis, Bo Qiu, Xiaoming Wang, Rahul Gowda, Ioannis Katsavounidis, Alan C. Bovik
- **Comment**: Accepted to IEEE Transactions on Image Processing, 2023. The database
  will be publicly available by 1st week of July 2023
- **Journal**: None
- **Summary**: We present the outcomes of a recent large-scale subjective study of Mobile Cloud Gaming Video Quality Assessment (MCG-VQA) on a diverse set of gaming videos. Rapid advancements in cloud services, faster video encoding technologies, and increased access to high-speed, low-latency wireless internet have all contributed to the exponential growth of the Mobile Cloud Gaming industry. Consequently, the development of methods to assess the quality of real-time video feeds to end-users of cloud gaming platforms has become increasingly important. However, due to the lack of a large-scale public Mobile Cloud Gaming Video dataset containing a diverse set of distorted videos with corresponding subjective scores, there has been limited work on the development of MCG-VQA models. Towards accelerating progress towards these goals, we created a new dataset, named the LIVE-Meta Mobile Cloud Gaming (LIVE-Meta-MCG) video quality database, composed of 600 landscape and portrait gaming videos, on which we collected 14,400 subjective quality ratings from an in-lab subjective study. Additionally, to demonstrate the usefulness of the new resource, we benchmarked multiple state-of-the-art VQA algorithms on the database. The new database will be made publicly available on our website: \url{https://live.ece.utexas.edu/research/LIVE-Meta-Mobile-Cloud-Gaming/index.html}



### Im-Promptu: In-Context Composition from Image Prompts
- **Arxiv ID**: http://arxiv.org/abs/2305.17262v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.17262v2)
- **Published**: 2023-05-26 21:10:11+00:00
- **Updated**: 2023-06-17 00:06:34+00:00
- **Authors**: Bhishma Dedhia, Michael Chang, Jake C. Snell, Thomas L. Griffiths, Niraj K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Large language models are few-shot learners that can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that the attention mechanisms over word tokens may play a role in analogical reasoning. In this work, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. First, we introduce a suite of three benchmarks to test the generalization properties of a visual in-context learner. We formalize the notion of an analogy-based in-context learner and use it to design a meta-learning framework called Im-Promptu. Whereas the requisite token granularity for language is well established, the appropriate compositional granularity for enabling in-context generalization in visual stimuli is usually unspecified. To this end, we use Im-Promptu to train multiple agents with different levels of compositionality, including vector representations, patch representations, and object slots. Our experiments reveal tradeoffs between extrapolation abilities and the degree of compositionality, with non-compositional representations extending learned composition rules to unseen domains but performing poorly on combinatorial tasks. Patch-based representations require patches to contain entire objects for robust extrapolation. At the same time, object-centric tokenizers coupled with a cross-attention module generate consistent and high-fidelity solutions, with these inductive biases being particularly crucial for compositional generalization. Lastly, we demonstrate a use case of Im-Promptu as an intuitive programming interface for image generation.



### Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss
- **Arxiv ID**: http://arxiv.org/abs/2305.17271v2
- **DOI**: 10.1109/TITS.2023.3305015
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.17271v2)
- **Published**: 2023-05-26 21:36:08+00:00
- **Updated**: 2023-08-11 08:35:06+00:00
- **Authors**: Ruohan Li, Yongqi Dong
- **Comment**: 12 pages, 8 figures, accepted by journal of IEEE Transactions on
  Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs, and the pre-trained model weights are transferred and further updated using the backpropagation mechanism with customized PolyLoss calculating the weighted errors between the output lane detection results and the labeled ground truth. Extensive experiment results demonstrate that, with the proposed pipeline, the lane detection model performance on both normal and challenging scenes can be advanced beyond the state-of-the-art, delivering the best testing accuracy (98.38%), precision (0.937), and F1-measure (0.924) on the normal scene testing set, together with the best overall accuracy (98.36%) and precision (0.844) in the challenging scene test set, while the training time can be substantially shortened.



### Distilling BlackBox to Interpretable models for Efficient Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.17303v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.17303v7)
- **Published**: 2023-05-26 23:23:48+00:00
- **Updated**: 2023-07-07 21:30:01+00:00
- **Authors**: Shantanu Ghosh, Ke Yu, Kayhan Batmanghelich
- **Comment**: 26th International Conference on Medical Image Computing and Computer
  Assisted Intervention, MICCAI 2023, Early accept
- **Journal**: None
- **Summary**: Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (e.g., scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Further, we use the pseudo-labeling technique from semi-supervised learning (SSL) to learn the concept classifier in the target domain, followed by fine-tuning the interpretable models in the target domain. We evaluate our model using a real-life large-scale chest-X-ray (CXR) classification dataset. The code is available at: \url{https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs}.



### DynaShare: Task and Instance Conditioned Parameter Sharing for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.17305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.17305v1)
- **Published**: 2023-05-26 23:43:21+00:00
- **Updated**: 2023-05-26 23:43:21+00:00
- **Authors**: Elahe Rahimian, Golara Javadi, Frederick Tung, Gabriel Oliveira
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task networks rely on effective parameter sharing to achieve robust generalization across tasks. In this paper, we present a novel parameter sharing method for multi-task learning that conditions parameter sharing on both the task and the intermediate feature representations at inference time. In contrast to traditional parameter sharing approaches, which fix or learn a deterministic sharing pattern during training and apply the same pattern to all examples during inference, we propose to dynamically decide which parts of the network to activate based on both the task and the input instance. Our approach learns a hierarchical gating policy consisting of a task-specific policy for coarse layer selection and gating units for individual input instances, which work together to determine the execution path at inference time. Experiments on the NYU v2, Cityscapes and MIMIC-III datasets demonstrate the potential of the proposed approach and its applicability across problem domains.




# Arxiv Papers in cs.CV on 2023-05-03
### AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.01836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2305.01836v1)
- **Published**: 2023-05-03 00:33:52+00:00
- **Updated**: 2023-05-03 00:33:52+00:00
- **Authors**: Shentong Mo, Yapeng Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Segment Anything Model (SAM) has recently shown its powerful effectiveness in visual segmentation tasks. However, there is less exploration concerning how SAM works on audio-visual tasks, such as visual sound localization and segmentation. In this work, we propose a simple yet effective audio-visual localization and segmentation framework based on the Segment Anything Model, namely AV-SAM, that can generate sounding object masks corresponding to the audio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion across audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Then, the aggregated cross-modal features are fed into the prompt encoder and mask decoder to generate the final audio-visual segmentation masks. We conduct extensive experiments on Flickr-SoundNet and AVSBench datasets. The results demonstrate that the proposed AV-SAM can achieve competitive performance on sounding object localization and segmentation.



### LineFormer: Rethinking Line Chart Data Extraction as Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.01837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.01837v1)
- **Published**: 2023-05-03 00:38:24+00:00
- **Updated**: 2023-05-03 00:38:24+00:00
- **Authors**: Jay Lal, Aditya Mitkari, Mahesh Bhosale, David Doermann
- **Comment**: Accepted to ICDAR 2023
- **Journal**: None
- **Summary**: Data extraction from line-chart images is an essential component of the automated document understanding process, as line charts are a ubiquitous data visualization format. However, the amount of visual and structural variations in multi-line graphs makes them particularly challenging for automated parsing. Existing works, however, are not robust to all these variations, either taking an all-chart unified approach or relying on auxiliary information such as legends for line data extraction. In this work, we propose LineFormer, a robust approach to line data extraction using instance segmentation. We achieve state-of-the-art performance on several benchmark synthetic and real chart datasets. Our implementation is available at https://github.com/TheJaeLal/LineFormer .



### Bio-Inspired Simple Neural Network for Low-Light Image Restoration: A Minimalist Approach
- **Arxiv ID**: http://arxiv.org/abs/2305.01844v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.01844v1)
- **Published**: 2023-05-03 01:16:45+00:00
- **Updated**: 2023-05-03 01:16:45+00:00
- **Authors**: Junjie Ye, Jilin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we explore the potential of using a straightforward neural network inspired by the retina model to efficiently restore low-light images. The retina model imitates the neurophysiological principles and dynamics of various optical neurons. Our proposed neural network model reduces the computational overhead compared to traditional signal-processing models while achieving results similar to complex deep learning models from a subjective perceptual perspective. By directly simulating retinal neuron functionalities with neural networks, we not only avoid manual parameter optimization but also lay the groundwork for constructing artificial versions of specific neurobiological organizations.



### Multimodal Data Augmentation for Image Captioning using Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2305.01855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.01855v1)
- **Published**: 2023-05-03 01:57:33+00:00
- **Updated**: 2023-05-03 01:57:33+00:00
- **Authors**: Changrong Xiao, Sean Xin Xu, Kunpeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning, an important vision-language task, often requires a tremendous number of finely labeled image-caption pairs for learning the underlying alignment between images and texts. In this paper, we proposed a multimodal data augmentation method, leveraging a recent text-to-image model called Stable Diffusion, to expand the training set via high-quality generation of image-caption pairs. Extensive experiments on the MS COCO dataset demonstrate the advantages of our approach over several benchmark methods, and particularly a significant boost when having fewer training instances. In addition, models trained on our augmented datasets also outperform prior unpaired image captioning methods by a large margin. Finally, further improvement regarding the training efficiency and effectiveness can be obtained after intentionally filtering the generated data based on quality assessment.



### Morphological Classification of Galaxies Using SpinalNet
- **Arxiv ID**: http://arxiv.org/abs/2305.01873v1
- **DOI**: 10.1109/ICECCO53203.2021.9663784
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.01873v1)
- **Published**: 2023-05-03 03:20:18+00:00
- **Updated**: 2023-05-03 03:20:18+00:00
- **Authors**: Dim Shaiakhmetov, Remudin Reshid Mekuria, Ruslan Isaev, Fatma Unsal
- **Comment**: 5 pages, 4 figures, ICECCO conference
- **Journal**: D. Shaiakhmetov, R. R. Mekuria, R. Isaev and F. Unsal,
  "Morphological Classification of Galaxies Using SpinalNet," 2021 16th
  International Conference on Electronics Computer and Computation (ICECCO),
  Kaskelen, Kazakhstan, 2021, pp. 1-5
- **Summary**: Deep neural networks (DNNs) with a step-by-step introduction of inputs, which is constructed by imitating the somatosensory system in human body, known as SpinalNet have been implemented in this work on a Galaxy Zoo dataset. The input segmentation in SpinalNet has enabled the intermediate layers to take some of the inputs as well as output of preceding layers thereby reducing the amount of the collected weights in the intermediate layers. As a result of these, the authors of SpinalNet reported to have achieved in most of the DNNs they tested, not only a remarkable cut in the error but also in the large reduction of the computational costs. Having applied it to the Galaxy Zoo dataset, we are able to classify the different classes and/or sub-classes of the galaxies. Thus, we have obtained higher classification accuracies of 98.2, 95 and 82 percents between elliptical and spirals, between these two and irregulars, and between 10 sub-classes of galaxies, respectively.



### Class adaptive threshold and negative class guided noisy annotation robust Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.01884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01884v1)
- **Published**: 2023-05-03 04:28:49+00:00
- **Updated**: 2023-05-03 04:28:49+00:00
- **Authors**: Darshan Gera, Badveeti Naveen Siva Kumar, Bobbili Veerendra Raj Kumar, S Balasubramanian
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: The hindering problem in facial expression recognition (FER) is the presence of inaccurate annotations referred to as noisy annotations in the datasets. These noisy annotations are present in the datasets inherently because the labeling is subjective to the annotator, clarity of the image, etc. Recent works use sample selection methods to solve this noisy annotation problem in FER. In our work, we use a dynamic adaptive threshold to separate confident samples from non-confident ones so that our learning won't be hampered due to non-confident samples. Instead of discarding the non-confident samples, we impose consistency in the negative classes of those non-confident samples to guide the model to learn better in the positive class. Since FER datasets usually come with 7 or 8 classes, we can correctly guess a negative class by 85% probability even by choosing randomly. By learning "which class a sample doesn't belong to", the model can learn "which class it belongs to" in a better manner. We demonstrate proposed framework's effectiveness using quantitative as well as qualitative results. Our method performs better than the baseline by a margin of 4% to 28% on RAFDB and 3.3% to 31.4% on FERPlus for various levels of synthetic noisy labels in the aforementioned datasets.



### Evolving Dictionary Representation for Few-shot Class-incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.01885v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.01885v1)
- **Published**: 2023-05-03 04:30:34+00:00
- **Updated**: 2023-05-03 04:30:34+00:00
- **Authors**: Xuejun Han, Yuhong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: New objects are continuously emerging in the dynamically changing world and a real-world artificial intelligence system should be capable of continual and effectual adaptation to new emerging classes without forgetting old ones. In view of this, in this paper we tackle a challenging and practical continual learning scenario named few-shot class-incremental learning (FSCIL), in which labeled data are given for classes in a base session but very limited labeled instances are available for new incremental classes. To address this problem, we propose a novel and succinct approach by introducing deep dictionary learning which is a hybrid learning architecture that combines dictionary learning and visual representation learning to provide a better space for characterizing different classes. We simultaneously optimize the dictionary and the feature extraction backbone in the base session, while only finetune the dictionary in the incremental session for adaptation to novel classes, which can alleviate the forgetting on base classes compared to finetuning the entire model. To further facilitate future adaptation, we also incorporate multiple pseudo classes into the base session training so that certain space projected by dictionary can be reserved for future new concepts. The extensive experimental results on CIFAR100, miniImageNet and CUB200 validate the effectiveness of our approach compared to other SOTA methods.



### Fairness in AI Systems: Mitigating gender bias from language-vision models
- **Arxiv ID**: http://arxiv.org/abs/2305.01888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01888v1)
- **Published**: 2023-05-03 04:33:44+00:00
- **Updated**: 2023-05-03 04:33:44+00:00
- **Authors**: Lavisha Aggarwal, Shruti Bhargava
- **Comment**: None
- **Journal**: None
- **Summary**: Our society is plagued by several biases, including racial biases, caste biases, and gender bias. As a matter of fact, several years ago, most of these notions were unheard of. These biases passed through generations along with amplification have lead to scenarios where these have taken the role of expected norms by certain groups in the society. One notable example is of gender bias. Whether we talk about the political world, lifestyle or corporate world, some generic differences are observed regarding the involvement of both the groups. This differential distribution, being a part of the society at large, exhibits its presence in the recorded data as well. Machine learning is almost entirely dependent on the availability of data; and the idea of learning from data and making predictions assumes that data defines the expected behavior at large. Hence, with biased data the resulting models are corrupted with those inherent biases too; and with the current popularity of ML in products, this can result in a huge obstacle in the path of equality and justice. This work studies and attempts to alleviate gender bias issues from language vision models particularly the task of image captioning. We study the extent of the impact of gender bias in existing datasets and propose a methodology to mitigate its impact in caption based language vision models.



### Localization using Multi-Focal Spatial Attention for Masked Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.01905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01905v1)
- **Published**: 2023-05-03 05:39:12+00:00
- **Updated**: 2023-05-03 05:39:12+00:00
- **Authors**: Yooshin Cho, Hanbyel Cho, Hyeong Gwon Hong, Jaesung Ahn, Dongmin Cho, JungWoo Chang, Junmo Kim
- **Comment**: Accepted at FG 2023 - InterID Workshop
- **Journal**: None
- **Summary**: Since the beginning of world-wide COVID-19 pandemic, facial masks have been recommended to limit the spread of the disease. However, these masks hide certain facial attributes. Hence, it has become difficult for existing face recognition systems to perform identity verification on masked faces. In this context, it is necessary to develop masked Face Recognition (MFR) for contactless biometric recognition systems. Thus, in this paper, we propose Complementary Attention Learning and Multi-Focal Spatial Attention that precisely removes masked region by training complementary spatial attention to focus on two distinct regions: masked regions and backgrounds. In our method, standard spatial attention and networks focus on unmasked regions, and extract mask-invariant features while minimizing the loss of the conventional Face Recognition (FR) performance. For conventional FR, we evaluate the performance on the IJB-C, Age-DB, CALFW, and CPLFW datasets. We evaluate the MFR performance on the ICCV2021-MFR/Insightface track, and demonstrate the improved performance on the both MFR and FR datasets. Additionally, we empirically verify that spatial attention of proposed method is more precisely activated in unmasked regions.



### Distributional Instance Segmentation: Modeling Uncertainty and High Confidence Predictions with Latent-MaskRCNN
- **Arxiv ID**: http://arxiv.org/abs/2305.01910v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.01910v1)
- **Published**: 2023-05-03 05:57:29+00:00
- **Updated**: 2023-05-03 05:57:29+00:00
- **Authors**: YuXuan Liu, Nikhil Mishra, Pieter Abbeel, Xi Chen
- **Comment**: In ICRA 2023. Code and dataset are available at
  https://segm.yuxuanliu.com/
- **Journal**: None
- **Summary**: Object recognition and instance segmentation are fundamental skills in any robotic or autonomous system. Existing state-of-the-art methods are often unable to capture meaningful uncertainty in challenging or ambiguous scenes, and as such can cause critical errors in high-performance applications. In this paper, we explore a class of distributional instance segmentation models using latent codes that can model uncertainty over plausible hypotheses of object masks. For robotic picking applications, we propose a confidence mask method to achieve the high precision necessary in industrial use cases. We show that our method can significantly reduce critical errors in robotic systems, including our newly released dataset of ambiguous scenes in a robotic application. On a real-world apparel-picking robot, our method significantly reduces double pick errors while maintaining high performance.



### DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2305.01921v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01921v3)
- **Published**: 2023-05-03 06:38:35+00:00
- **Updated**: 2023-08-20 22:57:46+00:00
- **Authors**: Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, Leonidas J Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: While the community of 3D point cloud generation has witnessed a big growth in recent years, there still lacks an effective way to enable intuitive user control in the generation process, hence limiting the general utility of such methods. Since an intuitive way of decomposing a shape is through its parts, we propose to tackle the task of controllable part-based point cloud generation. We introduce DiffFacto, a novel probabilistic generative model that learns the distribution of shapes with part-level control. We propose a factorization that models independent part style and part configuration distributions and presents a novel cross-diffusion network that enables us to generate coherent and plausible shapes under our proposed factorization. Experiments show that our method is able to generate novel shapes with multiple axes of control. It achieves state-of-the-art part-level generation quality and generates plausible and coherent shapes while enabling various downstream editing applications such as shape interpolation, mixing, and transformation editing. Project website: https://difffacto.github.io/



### Visual Transformation Telling
- **Arxiv ID**: http://arxiv.org/abs/2305.01928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01928v1)
- **Published**: 2023-05-03 07:02:57+00:00
- **Updated**: 2023-05-03 07:02:57+00:00
- **Authors**: Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new visual reasoning task, called Visual Transformation Telling (VTT). This task requires a machine to describe the transformation that occurred between every two adjacent states (i.e. images) in a series. Unlike most existing visual reasoning tasks that focus on state reasoning, VTT emphasizes transformation reasoning. We collected 13,547 samples from two instructional video datasets, CrossTask and COIN, and extracted desired states and transformation descriptions to create a suitable VTT benchmark dataset. Humans can naturally reason from superficial states differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience but how to model this process to bridge this semantic gap is challenging. We designed TTNet on top of existing visual storytelling models by enhancing the model's state-difference sensitivity and transformation-context awareness. TTNet significantly outperforms other baseline models adapted from similar tasks, such as visual storytelling and dense video captioning, demonstrating the effectiveness of our modeling on transformations. Through comprehensive diagnostic analyses, we found TTNet has strong context utilization abilities, but even with some state-of-the-art techniques such as CLIP, there remain challenges in generalization that need to be further explored.



### Illicit item detection in X-ray images for security applications
- **Arxiv ID**: http://arxiv.org/abs/2305.01936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01936v1)
- **Published**: 2023-05-03 07:28:05+00:00
- **Updated**: 2023-05-03 07:28:05+00:00
- **Authors**: Georgios Batsis, Ioannis Mademlis, Georgios Th. Papadopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: Automated detection of contraband items in X-ray images can significantly increase public safety, by enhancing the productivity and alleviating the mental load of security officers in airports, subways, customs/post offices, etc. The large volume and high throughput of passengers, mailed parcels, etc., during rush hours make it a Big Data analysis task. Modern computer vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of undertaking this task even under resource-constrained and embedded execution scenarios, e.g., as is the case with fast, single-stage, anchor-based object detectors. This paper proposes a two-fold improvement of such algorithms for the X-ray analysis domain, introducing two complementary novelties. Firstly, more efficient anchors are obtained by hierarchical clustering the sizes of the ground-truth training set bounding boxes; thus, the resulting anchors follow a natural hierarchy aligned with the semantic structure of the data. Secondly, the default Non-Maximum Suppression (NMS) algorithm at the end of the object detection pipeline is modified to better handle occluded object detection and to reduce the number of false predictions, by inserting the Efficient Intersection over Union (E-IoU) metric into the Weighted Cluster NMS method. E-IoU provides more discriminative geometrical correlations between the candidate bounding boxes/Regions-of-Interest (RoIs). The proposed method is implemented on a common single-stage object detector (YOLOv5) and its experimental evaluation on a relevant public dataset indicates significant accuracy gains over both the baseline and competing approaches. This highlights the potential of Big Data analysis in enhancing public safety.



### Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models
- **Arxiv ID**: http://arxiv.org/abs/2305.01939v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.01939v1)
- **Published**: 2023-05-03 07:32:28+00:00
- **Updated**: 2023-05-03 07:32:28+00:00
- **Authors**: Qihan Ren, Jiayang Gao, Wen Shen, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to prove the emergence of symbolic concepts in well-trained AI models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the AI model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the AI model does not significantly degrade on occluded samples, then the AI model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most AI models, we can usually use a small number of interactive concepts to mimic the model outputs on any arbitrarily masked samples.



### DPSeq: A Novel and Efficient Digital Pathology Classifier for Predicting Cancer Biomarkers using Sequencer Architecture
- **Arxiv ID**: http://arxiv.org/abs/2305.01968v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.01968v1)
- **Published**: 2023-05-03 08:31:44+00:00
- **Updated**: 2023-05-03 08:31:44+00:00
- **Authors**: Min Cen, Xingyu Li, Bangwei Guo, Jitendra Jonnagaddala, Hong Zhang, Xu Steven Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In digital pathology tasks, transformers have achieved state-of-the-art results, surpassing convolutional neural networks (CNNs). However, transformers are usually complex and resource intensive. In this study, we developed a novel and efficient digital pathology classifier called DPSeq, to predict cancer biomarkers through fine-tuning a sequencer architecture integrating horizon and vertical bidirectional long short-term memory (BiLSTM) networks. Using hematoxylin and eosin (H&E)-stained histopathological images of colorectal cancer (CRC) from two international datasets: The Cancer Genome Atlas (TCGA) and Molecular and Cellular Oncology (MCO), the predictive performance of DPSeq was evaluated in series of experiments. DPSeq demonstrated exceptional performance for predicting key biomarkers in CRC (MSI status, Hypermutation, CIMP status, BRAF mutation, TP53 mutation and chromosomal instability [CING]), outperforming most published state-of-the-art classifiers in a within-cohort internal validation and a cross-cohort external validation. Additionally, under the same experimental conditions using the same set of training and testing datasets, DPSeq surpassed 4 CNN (ResNet18, ResNet50, MobileNetV2, and EfficientNet) and 2 transformer (ViT and Swin-T) models, achieving the highest AUROC and AUPRC values in predicting MSI status, BRAF mutation, and CIMP status. Furthermore, DPSeq required less time for both training and prediction due to its simple architecture. Therefore, DPSeq appears to be the preferred choice over transformer and CNN models for predicting cancer biomarkers.



### District-scale surface temperatures generated from high-resolution longitudinal thermal infrared images
- **Arxiv ID**: http://arxiv.org/abs/2305.01971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01971v1)
- **Published**: 2023-05-03 08:36:06+00:00
- **Updated**: 2023-05-03 08:36:06+00:00
- **Authors**: Subin Lin, Vasantha Ramani, Miguel Martin, Pandarasamy Arjunan, Adrian Chong, Filip Biljecki, Marcel Ignatius, Kameshwar Poolla, Clayton Miller
- **Comment**: None
- **Journal**: None
- **Summary**: The paper describes a dataset that was collected by infrared thermography, which is a non-contact, non-intrusive technique to collect data and analyze the built environment in various aspects. While most studies focus on the city and building scales, the rooftop observatory provides high temporal and spatial resolution observations with dynamic interactions on the district scale. The rooftop infrared thermography observatory with a multi-modal platform that is capable of assessing a wide range of dynamic processes in urban systems was deployed in Singapore. It was placed on the top of two buildings that overlook the outdoor context of the campus of the National University of Singapore. The platform collects remote sensing data from tropical areas on a temporal scale, allowing users to determine the temperature trend of individual features such as buildings, roads, and vegetation. The dataset includes 1,365,921 thermal images collected on average at approximately 10 seconds intervals from two locations during ten months.



### Glitch in the Matrix: A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2305.01979v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.01979v3)
- **Published**: 2023-05-03 08:48:45+00:00
- **Updated**: 2023-07-16 07:03:45+00:00
- **Authors**: Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, Munawar Hayat
- **Comment**: The paper is under consideration/review at Computer Vision and Image
  Understanding Journal
- **Journal**: None
- **Summary**: Most deepfake detection methods focus on detecting spatial and/or spatio-temporal changes in facial attributes and are centered around the binary classification task of detecting whether a video is real or fake. This is because available benchmark datasets contain mostly visual-only modifications present in the entirety of the video. However, a sophisticated deepfake may include small segments of audio or audio-visual manipulations that can completely change the meaning of the video content. To addresses this gap, we propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual and audio-visual manipulations. The proposed baseline method, Boundary Aware Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture which effectively captures multimodal manipulations. We further improve (i.e. BA-TFD+) the baseline method by replacing the backbone with a Multiscale Vision Transformer and guide the training process with contrastive, frame classification, boundary matching and multimodal boundary matching loss functions. The quantitative analysis demonstrates the superiority of BA-TFD+ on temporal forgery localization and deepfake detection tasks using several benchmark datasets including our newly proposed dataset. The dataset, models and code are available at https://github.com/ControlNet/LAV-DF.



### The Beauty or the Beast: Which Aspect of Synthetic Medical Images Deserves Our Focus?
- **Arxiv ID**: http://arxiv.org/abs/2305.09789v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09789v2)
- **Published**: 2023-05-03 09:09:54+00:00
- **Updated**: 2023-06-14 14:39:17+00:00
- **Authors**: Xiaodan Xing, Yang Nan, Federico Felder, Simon Walsh, Guang Yang
- **Comment**: CBMS 2023
- **Journal**: None
- **Summary**: Training medical AI algorithms requires large volumes of accurately labeled datasets, which are difficult to obtain in the real world. Synthetic images generated from deep generative models can help alleviate the data scarcity problem, but their effectiveness relies on their fidelity to real-world images. Typically, researchers select synthesis models based on image quality measurements, prioritizing synthetic images that appear realistic. However, our empirical analysis shows that high-fidelity and visually appealing synthetic images are not necessarily superior. In fact, we present a case where low-fidelity synthetic images outperformed their high-fidelity counterparts in downstream tasks. Our findings highlight the importance of comprehensive analysis before incorporating synthetic data into real-world applications. We hope our results will raise awareness among the research community of the value of low-fidelity synthetic images in medical AI algorithm training.



### Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?
- **Arxiv ID**: http://arxiv.org/abs/2305.01997v2
- **DOI**: 10.1007/978-3-031-35302-4_25
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.01997v2)
- **Published**: 2023-05-03 09:38:52+00:00
- **Updated**: 2023-05-08 11:05:52+00:00
- **Authors**: Hang Jung Ling, Nathan Painchaud, Pierre-Yves Courand, Pierre-Marc Jodoin, Damien Garcia, Olivier Bernard
- **Comment**: 10 pages, accepted for FIMH 2023; camera ready corrections, corrected
  acknowledgments
- **Journal**: None
- **Summary**: Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when tested on CAMUS without any fine-tuning, still manage to perform competitively with respect to prior methods. Overall, the experimental results suggest that with sufficient training data, 3D nnU-Net could become the first automated tool to finally meet the standards of an everyday clinical device.



### Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2305.02008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.02008v1)
- **Published**: 2023-05-03 09:59:18+00:00
- **Updated**: 2023-05-03 09:59:18+00:00
- **Authors**: Mina Alibeigi, William Ljungbergh, Adam Tonderski, Georg Hess, Adam Lilja, Carl Lindstrom, Daria Motorniuk, Junsheng Fu, Jenny Widahl, Christoffer Petersson
- **Comment**: None
- **Journal**: None
- **Summary**: Existing datasets for autonomous driving (AD) often lack diversity and long-range capabilities, focusing instead on 360{\deg} perception and temporal reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a large-scale and diverse multimodal dataset collected over two years in various European countries, covering an area 9x that of existing datasets. ZOD boasts the highest range and resolution sensors among comparable datasets, coupled with detailed keyframe annotations for 2D and 3D objects (up to 245m), road instance/semantic segmentation, traffic sign recognition, and road classification. We believe that this unique combination will facilitate breakthroughs in long-range perception and multi-task learning. The dataset is composed of Frames, Sequences, and Drives, designed to encompass both data diversity and support for spatio-temporal learning, sensor fusion, localization, and mapping. Frames consist of 100k curated camera images with two seconds of other supporting sensor data, while the 1473 Sequences and 29 Drives include the entire sensor suite for 20 seconds and a few minutes, respectively. ZOD is the only large-scale AD dataset released under a permissive license, allowing for both research and commercial use. The dataset is accompanied by an extensive development kit. Data and more information are available online (https://zod.zenseact.com).



### Deep Learning-Based Multiband Signal Fusion for 3-D SAR Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2305.02017v1
- **DOI**: 10.1109/TAES.2023.3270111
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2305.02017v1)
- **Published**: 2023-05-03 10:14:58+00:00
- **Updated**: 2023-05-03 10:14:58+00:00
- **Authors**: Josiah Smith, Murat Torlak
- **Comment**: Accepted to IEEE Transactions on Aerospace and Electronic Systems
- **Journal**: None
- **Summary**: Three-dimensional (3-D) synthetic aperture radar (SAR) is widely used in many security and industrial applications requiring high-resolution imaging of concealed or occluded objects. The ability to resolve intricate 3-D targets is essential to the performance of such applications and depends directly on system bandwidth. However, because high-bandwidth systems face several prohibitive hurdles, an alternative solution is to operate multiple radars at distinct frequency bands and fuse the multiband signals. Current multiband signal fusion methods assume a simple target model and a small number of point reflectors, which is invalid for realistic security screening and industrial imaging scenarios wherein the target model effectively consists of a large number of reflectors. To the best of our knowledge, this study presents the first use of deep learning for multiband signal fusion. The proposed network, called kR-Net, employs a hybrid, dual-domain complex-valued convolutional neural network (CV-CNN) to fuse multiband signals and impute the missing samples in the frequency gaps between subbands. By exploiting the relationships in both the wavenumber domain and wavenumber spectral domain, the proposed framework overcomes the drawbacks of existing multiband imaging techniques for realistic scenarios at a fraction of the computation time of existing multiband fusion algorithms. Our method achieves high-resolution imaging of intricate targets previously impossible using conventional techniques and enables finer resolution capacity for concealed weapon detection and occluded object classification using multiband signaling without requiring more advanced hardware. Furthermore, a fully integrated multiband imaging system is developed using commercially available millimeter-wave (mmWave) radars for efficient multiband imaging.



### Neural Network Training and Non-Differentiable Objective Functions
- **Arxiv ID**: http://arxiv.org/abs/2305.02024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02024v1)
- **Published**: 2023-05-03 10:28:23+00:00
- **Updated**: 2023-05-03 10:28:23+00:00
- **Authors**: Yash Patel
- **Comment**: Ph.D. Dissertation (under review). Supervisor: Prof. Jiri Matas
- **Journal**: None
- **Summary**: Many important computer vision tasks are naturally formulated to have a non-differentiable objective. Therefore, the standard, dominant training procedure of a neural network is not applicable since back-propagation requires the gradients of the objective with respect to the output of the model. Most deep learning methods side-step the problem sub-optimally by using a proxy loss for training, which was originally designed for another task and is not tailored to the specifics of the objective. The proxy loss functions may or may not align well with the original non-differentiable objective. An appropriate proxy has to be designed for a novel task, which may not be feasible for a non-specialist. This thesis makes four main contributions toward bridging the gap between the non-differentiable objective and the training loss function. Throughout the thesis, we refer to a loss function as a surrogate loss if it is a differentiable approximation of the non-differentiable objective. Note that we use the terms objective and evaluation metric interchangeably.   The contributions of this thesis make the training of neural networks more scalable -- to new tasks in a nearly labor-free manner when the evaluation metric is decomposable, which will help researchers with novel tasks. For non-decomposable evaluation metrics, the differentiable components developed for the recall@k surrogate, such as sorting and counting, can also be used for creating new surrogates.



### Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.02324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02324v1)
- **Published**: 2023-05-03 10:31:35+00:00
- **Updated**: 2023-05-03 10:31:35+00:00
- **Authors**: Ding Li, Yongqiang Tang, Zhizhong Zhang, Wensheng Zhang
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Self-supervised skeleton-based action recognition enjoys a rapid growth along with the development of contrastive learning. The existing methods rely on imposing invariance to augmentations of 3D skeleton within a single data stream, which merely leverages the easy positive pairs and limits the ability to explore the complicated movement patterns. In this paper, we advocate that the defect of single-stream contrast and the lack of necessary feature transformation are responsible for easy positives, and therefore propose a Cross-Stream Contrastive Learning framework for skeleton-based action Representation learning (CSCLR). Specifically, the proposed CSCLR not only utilizes intra-stream contrast pairs, but introduces inter-stream contrast pairs as hard samples to formulate a better representation learning. Besides, to further exploit the potential of positive pairs and increase the robustness of self-supervised representation learning, we propose a Positive Feature Transformation (PFT) strategy which adopts feature-level manipulation to increase the variance of positive pairs. To validate the effectiveness of our method, we conduct extensive experiments on three benchmark datasets NTU-RGB+D 60, NTU-RGB+D 120 and PKU-MMD. Experimental results show that our proposed CSCLR exceeds the state-of-the-art methods on a diverse range of evaluation protocols.



### Near-Field MIMO-ISAR Millimeter-Wave Imaging
- **Arxiv ID**: http://arxiv.org/abs/2305.02030v1
- **DOI**: 10.1109/RadarConf2043947.2020.9266412
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02030v1)
- **Published**: 2023-05-03 10:46:48+00:00
- **Updated**: 2023-05-03 10:46:48+00:00
- **Authors**: Josiah W. Smith, Muhammet Emin Yanik, Murat Torlak
- **Comment**: Accepted to IEEE Radar Conference 2020
- **Journal**: None
- **Summary**: Multiple-input-multiple-output (MIMO) millimeter-wave (mmWave) sensors for synthetic aperture radar (SAR) and inverse SAR (ISAR) address the fundamental challenges of cost-effectiveness and scalability inherent to near-field imaging. In this paper, near-field MIMO-ISAR mmWave imaging systems are discussed and developed. The rotational ISAR (R-ISAR) regime investigated in this paper requires rotating the target at a constant radial distance from the transceiver and scanning the transceiver along a vertical track. Using a 77GHz mmWave radar, a high resolution three-dimensional (3-D) image can be reconstructed from this two-dimensional scanning taking into account the spherical near-field wavefront. While prior work in literature consists of single-input-single-output circular synthetic aperture radar (SISO-CSAR) algorithms or computationally sluggish MIMO-CSAR image reconstruction algorithms, this paper proposes a novel algorithm for efficient MIMO 3-D holographic imaging and details the design of a MIMO R-ISAR imaging system. The proposed algorithm applies a multistatic-to-monostatic phase compensation to the R-ISAR regime allowing for use of highly efficient monostatic algorithms. We demonstrate the algorithm's performance in real-world imaging scenarios on a prototyped MIMO R-ISAR platform. Our fully integrated system, consisting of a mechanical scanner and efficient imaging algorithm, is capable of pairing the scanning efficiency of the MIMO regime with the computational efficiency of single pixel image reconstruction algorithms.



### Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2305.02032v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.02032v1)
- **Published**: 2023-05-03 10:54:18+00:00
- **Updated**: 2023-05-03 10:54:18+00:00
- **Authors**: Sajid Javed, Arif Mahmood, Talha Qaiser, Naoufel Werghi, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Classification of gigapixel Whole Slide Images (WSIs) is an important prediction task in the emerging area of computational pathology. There has been a surge of research in deep learning models for WSI classification with clinical applications such as cancer detection or prediction of molecular mutations from WSIs. Most methods require expensive and labor-intensive manual annotations by expert pathologists. Weakly supervised Multiple Instance Learning (MIL) methods have recently demonstrated excellent performance; however, they still require large slide-level labeled training datasets that need a careful inspection of each slide by an expert pathologist. In this work, we propose a fully unsupervised WSI classification algorithm based on mutual transformer learning. Instances from gigapixel WSI (i.e., image patches) are transformed into a latent space and then inverse-transformed to the original space. Using the transformation loss, pseudo-labels are generated and cleaned using a transformer label-cleaner. The proposed transformer-based pseudo-label generation and cleaning modules mutually train each other iteratively in an unsupervised manner. A discriminative learning mechanism is introduced to improve normal versus cancerous instance labeling. In addition to unsupervised classification, we demonstrate the effectiveness of the proposed framework for weak supervision for cancer subtype classification as downstream analysis. Extensive experiments on four publicly available datasets show excellent performance compared to the state-of-the-art methods. We intend to make the source code of our algorithm publicly available soon.



### Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model
- **Arxiv ID**: http://arxiv.org/abs/2305.02034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02034v1)
- **Published**: 2023-05-03 10:58:07+00:00
- **Updated**: 2023-05-03 10:58:07+00:00
- **Authors**: Di Wang, Jing Zhang, Bo Du, Dacheng Tao, Liangpei Zhang
- **Comment**: The code and dataset will be available at
  https://github.com/ViTAE-Transformer/SAMRS
- **Journal**: None
- **Summary**: The success of the Segment Anything Model (SAM) demonstrates the significance of data-centric machine learning. However, due to the difficulties and high costs associated with annotating Remote Sensing (RS) images, a large amount of valuable RS data remains unlabeled, particularly at the pixel level. In this study, we leverage SAM and existing RS object detection datasets to develop an efficient pipeline for generating a large-scale RS segmentation dataset, dubbed SAMRS. SAMRS surpasses existing high-resolution RS segmentation datasets in size by several orders of magnitude, and provides object category, location, and instance information that can be used for semantic segmentation, instance segmentation, and object detection, either individually or in combination. We also provide a comprehensive analysis of SAMRS from various aspects. We hope it could facilitate research in RS segmentation, particularly in large model pre-training.



### Improved Static Hand Gesture Classification on Deep Convolutional Neural Networks using Novel Sterile Training Technique
- **Arxiv ID**: http://arxiv.org/abs/2305.02039v1
- **DOI**: 10.1109/ACCESS.2021.3051454
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2305.02039v1)
- **Published**: 2023-05-03 11:10:50+00:00
- **Updated**: 2023-05-03 11:10:50+00:00
- **Authors**: Josiah Smith, Shiva Thiagarajan, Richard Willis, Yiorgos Makris, Murat Torlak
- **Comment**: Accepted to IEEE Access
- **Journal**: IEEE Access, vol. 9, pp. 10893-10902, 2021
- **Summary**: In this paper, we investigate novel data collection and training techniques towards improving classification accuracy of non-moving (static) hand gestures using a convolutional neural network (CNN) and frequency-modulated-continuous-wave (FMCW) millimeter-wave (mmWave) radars. Recently, non-contact hand pose and static gesture recognition have received considerable attention in many applications ranging from human-computer interaction (HCI), augmented/virtual reality (AR/VR), and even therapeutic range of motion for medical applications. While most current solutions rely on optical or depth cameras, these methods require ideal lighting and temperature conditions. mmWave radar devices have recently emerged as a promising alternative offering low-cost system-on-chip sensors whose output signals contain precise spatial information even in non-ideal imaging conditions. Additionally, deep convolutional neural networks have been employed extensively in image recognition by learning both feature extraction and classification simultaneously. However, little work has been done towards static gesture recognition using mmWave radars and CNNs due to the difficulty involved in extracting meaningful features from the radar return signal, and the results are inferior compared with dynamic gesture classification. This article presents an efficient data collection approach and a novel technique for deep CNN training by introducing ``sterile'' images which aid in distinguishing distinct features among the static gestures and subsequently improve the classification accuracy. Applying the proposed data collection and training methods yields an increase in classification rate of static hand gestures from $85\%$ to $93\%$ and $90\%$ to $95\%$ for range and range-angle profiles, respectively.



### Efficient 3-D Near-Field MIMO-SAR Imaging for Irregular Scanning Geometries
- **Arxiv ID**: http://arxiv.org/abs/2305.02064v1
- **DOI**: 10.1109/ACCESS.2022.3145370
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02064v1)
- **Published**: 2023-05-03 12:07:21+00:00
- **Updated**: 2023-05-03 12:07:21+00:00
- **Authors**: Josiah Smith, Murat Torlak
- **Comment**: Accepted to IEEE Access
- **Journal**: IEEE Access, vol. 10, pp. 10283-10294, 2022
- **Summary**: In this article, we introduce a novel algorithm for efficient near-field synthetic aperture radar (SAR) imaging for irregular scanning geometries. With the emergence of fifth-generation (5G) millimeter-wave (mmWave) devices, near-field SAR imaging is no longer confined to laboratory environments. Recent advances in positioning technology have attracted significant interest for a diverse set of new applications in mmWave imaging. However, many use cases, such as automotive-mounted SAR imaging, unmanned aerial vehicle (UAV) imaging, and freehand imaging with smartphones, are constrained to irregular scanning geometries. Whereas traditional near-field SAR imaging systems and quick personnel security (QPS) scanners employ highly precise motion controllers to create ideal synthetic arrays, emerging applications, mentioned previously, inherently cannot achieve such ideal positioning. In addition, many Internet of Things (IoT) and 5G applications impose strict size and computational complexity limitations that must be considered for edge mmWave imaging technology. In this study, we propose a novel algorithm to leverage the advantages of non-cooperative SAR scanning patterns, small form-factor multiple-input multiple-output (MIMO) radars, and efficient monostatic planar image reconstruction algorithms. We propose a framework to mathematically decompose arbitrary and irregular sampling geometries and a joint solution to mitigate multistatic array imaging artifacts. The proposed algorithm is validated through simulations and an empirical study of arbitrary scanning scenarios. Our algorithm achieves high-resolution and high-efficiency near-field MIMO-SAR imaging, and is an elegant solution to computationally constrained irregularly sampled imaging problems.



### Codesign of Edge Intelligence and Automated Guided Vehicle Control
- **Arxiv ID**: http://arxiv.org/abs/2305.09788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.09788v1)
- **Published**: 2023-05-03 12:15:35+00:00
- **Updated**: 2023-05-03 12:15:35+00:00
- **Authors**: Malith Gallage, Rafaela Scaciota, Sumudu Samarakoon, Mehdi Bennis
- **Comment**: 3 pages, 3 figures, 2023 IEEE International Conference on Pervasive
  Computing and Communications Workshops and other Affiliated Events (PerCom
  Workshops): Demos
- **Journal**: None
- **Summary**: This work presents a harmonic design of autonomous guided vehicle (AGV) control, edge intelligence, and human input to enable autonomous transportation in industrial environments. The AGV has the capability to navigate between a source and destinations and pick/place objects. The human input implicitly provides preferences of the destination and exact drop point, which are derived from an artificial intelligence (AI) module at the network edge and shared with the AGV over a wireless network. The demonstration indicates that the proposed integrated design of hardware, software, and AI design achieve a technology readiness level (TRL) of range 4-5



### A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2305.02074v2
- **DOI**: 10.1109/WMCS55582.2022.9866326
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2305.02074v2)
- **Published**: 2023-05-03 12:25:01+00:00
- **Updated**: 2023-06-27 06:27:49+00:00
- **Authors**: Josiah Smith, Yusef Alimam, Geetika Vedula, Murat Torlak
- **Comment**: Accepted to Proc. IEEE WMCS
- **Journal**: None
- **Summary**: In this paper, we develop a novel super-resolution algorithm for near-field synthetic-aperture radar (SAR) under irregular scanning geometries. As fifth-generation (5G) millimeter-wave (mmWave) devices are becoming increasingly affordable and available, high-resolution SAR imaging is feasible for end-user applications and non-laboratory environments. Emerging applications such freehand imaging, wherein a handheld radar is scanned throughout space by a user, unmanned aerial vehicle (UAV) imaging, and automotive SAR face several unique challenges for high-resolution imaging. First, recovering a SAR image requires knowledge of the array positions throughout the scan. While recent work has introduced camera-based positioning systems capable of adequately estimating the position, recovering the algorithm efficiently is a requirement to enable edge and Internet of Things (IoT) technologies. Efficient algorithms for non-cooperative near-field SAR sampling have been explored in recent work, but suffer image defocusing under position estimation error and can only produce medium-fidelity images. In this paper, we introduce a mobile-friend vision transformer (ViT) architecture to address position estimation error and perform SAR image super-resolution (SR) under irregular sampling geometries. The proposed algorithm, Mobile-SRViT, is the first to employ a ViT approach for SAR image enhancement and is validated in simulation and via empirical studies.



### A Systematic Study on Object Recognition Using Millimeter-wave Radar
- **Arxiv ID**: http://arxiv.org/abs/2305.02085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2305.02085v1)
- **Published**: 2023-05-03 12:42:44+00:00
- **Updated**: 2023-05-03 12:42:44+00:00
- **Authors**: Maloy Kumar Devnath, Avijoy Chakma, Mohammad Saeid Anwar, Emon Dey, Zahid Hasan, Marc Conn, Biplab Pal, Nirmalya Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Due to its light and weather-independent sensing, millimeter-wave (MMW) radar is essential in smart environments. Intelligent vehicle systems and industry-grade MMW radars have integrated such capabilities. Industry-grade MMW radars are expensive and hard to get for community-purpose smart environment applications. However, commercially available MMW radars have hidden underpinning challenges that need to be investigated for tasks like recognizing objects and activities, real-time person tracking, object localization, etc. Image and video data are straightforward to gather, understand, and annotate for such jobs. Image and video data are light and weather-dependent, susceptible to the occlusion effect, and present privacy problems. To eliminate dependence and ensure privacy, commercial MMW radars should be tested. MMW radar's practicality and performance in varied operating settings must be addressed before promoting it. To address the problems, we collected a dataset using Texas Instruments' Automotive mmWave Radar (AWR2944) and reported the best experimental settings for object recognition performance using different deep learning algorithms. Our extensive data gathering technique allows us to systematically explore and identify object identification task problems under cross-ambience conditions. We investigated several solutions and published detailed experimental data.



### Rethinking the Encoding of Satellite Image Time Series
- **Arxiv ID**: http://arxiv.org/abs/2305.02086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02086v1)
- **Published**: 2023-05-03 12:44:20+00:00
- **Updated**: 2023-05-03 12:44:20+00:00
- **Authors**: Xin Cai, Yaxin Bi, Peter Nicholl, Roy Sterritt
- **Comment**: None
- **Journal**: None
- **Summary**: Representation learning of Satellite Image Time Series (SITS) presents its unique challenges, such as prohibitive computation burden caused by high spatiotemporal resolutions, irregular acquisition times, and complex spatiotemporal interactions, leading to highly-specialized neural network architectures for SITS analysis. Despite the promising results achieved by some pioneering work, we argue that satisfactory representation learning paradigms have not yet been established for SITS analysis, causing an isolated island where transferring successful paradigms or the latest advances from Computer Vision (CV) to SITS is arduous. In this paper, we develop a unique perspective of SITS processing as a direct set prediction problem, inspired by the recent trend in adopting query-based transformer decoders to streamline the object detection or image segmentation pipeline, and further propose to decompose the representation learning process of SITS into three explicit steps: collect--update--distribute, which is computationally efficient and suits for irregularly-sampled and asynchronous temporal observations. Facilitated by the unique reformulation and effective feature extraction framework proposed, our models pre-trained on pixel-set format input and then fine-tuned on downstream dense prediction tasks by simply appending a commonly-used segmentation network have attained new state-of-the-art (SoTA) results on PASTIS dataset compared to bespoke neural architectures such as U-TAE. Furthermore, the clear separation, conceptually and practically, between temporal and spatial components in the panoptic segmentation pipeline of SITS allows us to leverage the recent advances in CV, such as Mask2Former, a universal segmentation architecture, resulting in a noticeable 8.8 points increase in PQ compared to the best score reported so far.



### Sex Detection in the Early Stage of Fertilized Chicken Eggs via Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2305.02325v1
- **DOI**: 10.5121/ijcsit.2023.15202
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02325v1)
- **Published**: 2023-05-03 12:46:36+00:00
- **Updated**: 2023-05-03 12:46:36+00:00
- **Authors**: Ufuk Asil, Efendi Nasibov
- **Comment**: 8 pages, 4 figures, 1 table
- **Journal**: International Journal of Computer Science & Information Technology
  (IJCSIT) Vol 15, No 2, April 2023, pp.19-26
- **Summary**: Culling newly hatched male chicks in industrial hatcheries poses a serious ethical problem. Both laying and broiler breeders need males, but it is a problem because they are produced more than needed. Being able to determine the sex of chicks in the egg at the beginning or early stage of incubation can eliminate ethical problems as well as many additional costs. When we look at the literature, the methods used are very costly, low in applicability, invasive, inadequate in accuracy, or too late to eliminate ethical problems. Considering the embryo's development, the earliest observed candidate feature for sex determination is blood vessels. Detection from blood vessels can eliminate ethical issues, and these vessels can be seen when light is shined into the egg until the first seven days. In this study, sex determination was made by morphological analysis from embryonic vascular images obtained in the first week when the light was shined into the egg using a standard camera without any invasive procedure to the egg.



### Efficient CNN-based Super Resolution Algorithms for mmWave Mobile Radar Imaging
- **Arxiv ID**: http://arxiv.org/abs/2305.02092v1
- **DOI**: 10.1109/ICIP46576.2022.9897190
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2305.02092v1)
- **Published**: 2023-05-03 12:54:28+00:00
- **Updated**: 2023-05-03 12:54:28+00:00
- **Authors**: Christos Vasileiou, Josiah W. Smith, Shiva Thiagarajan, Matthew Nigh, Yiorgos Makris, Murat Torlak
- **Comment**: Accepted to IEEE ICIP
- **Journal**: None
- **Summary**: In this paper, we introduce an innovative super resolution approach to emerging modes of near-field synthetic aperture radar (SAR) imaging. Recent research extends convolutional neural network (CNN) architectures from the optical to the electromagnetic domain to achieve super resolution on images generated from radar signaling. Specifically, near-field synthetic aperture radar (SAR) imaging, a method for generating high-resolution images by scanning a radar across space to create a synthetic aperture, is of interest due to its high-fidelity spatial sensing capability, low cost devices, and large application space. Since SAR imaging requires large aperture sizes to achieve high resolution, super-resolution algorithms are valuable for many applications. Freehand smartphone SAR, an emerging sensing modality, requires irregular SAR apertures in the near-field and computation on mobile devices. Achieving efficient high-resolution SAR images from irregularly sampled data collected by freehand motion of a smartphone is a challenging task. In this paper, we propose a novel CNN architecture to achieve SAR image super-resolution for mobile applications by employing state-of-the-art SAR processing and deep learning techniques. The proposed algorithm is verified via simulation and an empirical study. Our algorithm demonstrates high-efficiency and high-resolution radar imaging for near-field scenarios with irregular scanning geometries.



### Removing Human Bottlenecks in Bird Classification Using Camera Trap Images and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.02097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.02097v1)
- **Published**: 2023-05-03 13:04:39+00:00
- **Updated**: 2023-05-03 13:04:39+00:00
- **Authors**: Carl Chalmers, Paul Fergus, Serge Wich, Steven N Longmore, Naomi Davies Walsh, Philip Stephens, Chris Sutherland, Naomi Matthews, Jens Mudde, Amira Nuseibeh
- **Comment**: None
- **Journal**: None
- **Summary**: Birds are important indicators for monitoring both biodiversity and habitat health; they also play a crucial role in ecosystem management. Decline in bird populations can result in reduced eco-system services, including seed dispersal, pollination and pest control. Accurate and long-term monitoring of birds to identify species of concern while measuring the success of conservation interventions is essential for ecologists. However, monitoring is time consuming, costly and often difficult to manage over long durations and at meaningfully large spatial scales. Technology such as camera traps, acoustic monitors and drones provide methods for non-invasive monitoring. There are two main problems with using camera traps for monitoring: a) cameras generate many images, making it difficult to process and analyse the data in a timely manner; and b) the high proportion of false positives hinders the processing and analysis for reporting. In this paper, we outline an approach for overcoming these issues by utilising deep learning for real-time classi-fication of bird species and automated removal of false positives in camera trap data. Images are classified in real-time using a Faster-RCNN architecture. Images are transmitted over 3/4G cam-eras and processed using Graphical Processing Units (GPUs) to provide conservationists with key detection metrics therefore removing the requirement for manual observations. Our models achieved an average sensitivity of 88.79%, a specificity of 98.16% and accuracy of 96.71%. This demonstrates the effectiveness of using deep learning for automatic bird monitoring.



### Joint A-SNN: Joint Training of Artificial and Spiking Neural Networks via Self-Distillation and Weight Factorization
- **Arxiv ID**: http://arxiv.org/abs/2305.02099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02099v1)
- **Published**: 2023-05-03 13:12:17+00:00
- **Updated**: 2023-05-03 13:12:17+00:00
- **Authors**: Yufei Guo, Weihang Peng, Yuanpei Chen, Liwen Zhang, Xiaode Liu, Xuhui Huang, Zhe Ma
- **Comment**: Accepted by Pattern Recognition
- **Journal**: None
- **Summary**: Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent. In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN's optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task, the spiking ResNet-18 model trained by our method can reach to 77.39% top-1 accuracy with only 4 time steps.



### Single Image Deraining via Feature-based Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2305.02100v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, Machine vision and scene understanding
- **Links**: [PDF](http://arxiv.org/pdf/2305.02100v1)
- **Published**: 2023-05-03 13:12:51+00:00
- **Updated**: 2023-05-03 13:12:51+00:00
- **Authors**: Chaobing Zheng, Jun Jiang, Wenjian Ying, Shiqian Wu
- **Comment**: 6 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:2209.07808
- **Journal**: None
- **Summary**: It is challenging to remove rain-steaks from a single rainy image because the rain steaks are spatially varying in the rainy image. Although the CNN based methods have reported promising performance recently, there are still some defects, such as data dependency and insufficient interpretation. A single image deraining algorithm based on the combination of data-driven and model-based approaches is proposed. Firstly, an improved weighted guided image filter (iWGIF) is used to extract high-frequency information and learn the rain steaks to avoid interference from other information through the input image. Then, transfering the input image and rain steaks from the image domain to the feature domain adaptively to learn useful features for high-quality image deraining. Finally, networks with attention mechanisms is used to restore high-quality images from the latent features. Experiments show that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both qualitative and quantitative measures.



### ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2305.02103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02103v1)
- **Published**: 2023-05-03 13:24:06+00:00
- **Updated**: 2023-05-03 13:24:06+00:00
- **Authors**: Andrea Ramazzina, Mario Bijelic, Stefanie Walz, Alessandro Sanvito, Dominik Scheuble, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: Vision in adverse weather conditions, whether it be snow, rain, or fog is challenging. In these scenarios, scattering and attenuation severly degrades image quality. Handling such inclement weather conditions, however, is essential to operate autonomous vehicles, drones and robotic applications where human performance is impeded the most. A large body of work explores removing weather-induced image degradations with dehazing methods. Most methods rely on single images as input and struggle to generalize from synthetic fully-supervised training approaches or to generate high fidelity results from unpaired real-world datasets. With data as bottleneck and most of today's training data relying on good weather conditions with inclement weather as outlier, we rely on an inverse rendering approach to reconstruct the scene content. We introduce ScatterNeRF, a neural rendering method which adequately renders foggy scenes and decomposes the fog-free background from the participating media-exploiting the multiple views from a short automotive sequence without the need for a large training data corpus. Instead, the rendering approach is optimized on the multi-view scene itself, which can be typically captured by an autonomous vehicle, robot or drone during operation. Specifically, we propose a disentangled representation for the scattering volume and the scene objects, and learn the scene reconstruction with physics-inspired losses. We validate our method by capturing multi-view In-the-Wild data and controlled captures in a large-scale fog chamber.



### Automatic Parameterization for Aerodynamic Shape Optimization via Deep Geometric Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.02116v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2305.02116v1)
- **Published**: 2023-05-03 13:45:40+00:00
- **Updated**: 2023-05-03 13:45:40+00:00
- **Authors**: Zhen Wei, Pascal Fua, Michal Bauerheim
- **Comment**: 15 pages, to be appeared at AIAA Aviation Forum 2023
- **Journal**: None
- **Summary**: We propose two deep learning models that fully automate shape parameterization for aerodynamic shape optimization. Both models are optimized to parameterize via deep geometric learning to embed human prior knowledge into learned geometric patterns, eliminating the need for further handcrafting. The Latent Space Model (LSM) learns a low-dimensional latent representation of an object from a dataset of various geometries, while the Direct Mapping Model (DMM) builds parameterization on the fly using only one geometry of interest. We also devise a novel regularization loss that efficiently integrates volumetric mesh deformation into the parameterization model. The models directly manipulate the high-dimensional mesh data by moving vertices. LSM and DMM are fully differentiable, enabling gradient-based, end-to-end pipeline design and plug-and-play deployment of surrogate models or adjoint solvers. We perform shape optimization experiments on 2D airfoils and discuss the applicable scenarios for the two models.



### Bicubic++: Slim, Slimmer, Slimmest -- Designing an Industry-Grade Super-Resolution Network
- **Arxiv ID**: http://arxiv.org/abs/2305.02126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02126v1)
- **Published**: 2023-05-03 13:57:00+00:00
- **Updated**: 2023-05-03 13:57:00+00:00
- **Authors**: Bahri Batuhan Bilecen, Mustafa Ayazoglu
- **Comment**: Winner of the New Trends in Image Restoration and Enhancement (NTIRE)
  @ CVPR 2023, Real Time Super Resolution (RTSR) Challange Track 2 (x3
  super-resolution). Code available at:
  https://github.com/aselsan-research-imaging-team/bicubic-plusplus
- **Journal**: None
- **Summary**: We propose a real-time and lightweight single-image super-resolution (SR) network named Bicubic++. Despite using spatial dimensions of the input image across the whole network, Bicubic++ first learns quick reversible downgraded and lower resolution features of the image in order to decrease the number of computations. We also construct a training pipeline, where we apply an end-to-end global structured pruning of convolutional layers without using metrics like magnitude and gradient norms, and focus on optimizing the pruned network's PSNR on the validation set. Furthermore, we have experimentally shown that the bias terms take considerable amount of the runtime while increasing PSNR marginally, hence we have also applied bias removal to the convolutional layers. Our method adds ~1dB on Bicubic upscaling PSNR for all tested SR datasets and runs with ~1.17ms on RTX3090 and ~2.9ms on RTX3070, for 720p inputs and 4K outputs, both in FP16 precision. Bicubic++ won NTIRE 2023 RTSR Track 2 x3 SR competition and is the fastest among all competitive methods. Being almost as fast as the standard Bicubic upsampling method, we believe that Bicubic++ can set a new industry standard.



### GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions
- **Arxiv ID**: http://arxiv.org/abs/2305.02143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2305.02143v1)
- **Published**: 2023-05-03 14:22:48+00:00
- **Updated**: 2023-05-03 14:22:48+00:00
- **Authors**: Fabio Hellmann, Silvan Mertes, Mohamed Benouis, Alexander Hustinx, Tzung-Chien Hsieh, Cristina Conati, Peter Krawitz, Elisabeth Andr
- **Comment**: 24 pages, 10 figures, 5 tables, ACM Transactions on Multimedia
  Computing, Communications, and Applications
- **Journal**: None
- **Summary**: In recent years, the increasing availability of personal data has raised concerns regarding privacy and security. One of the critical processes to address these concerns is data anonymization, which aims to protect individual privacy and prevent the release of sensitive information. This research focuses on the importance of face anonymization. Therefore, we introduce GANonymization, a novel face anonymization framework with facial expression-preserving abilities. Our approach is based on a high-level representation of a face which is synthesized into an anonymized version based on a generative adversarial network (GAN). The effectiveness of the approach was assessed by evaluating its performance in removing identifiable facial attributes to increase the anonymity of the given individual face. Additionally, the performance of preserving facial expressions was evaluated on several affect recognition datasets and outperformed the state-of-the-art method in most categories. Finally, our approach was analyzed for its ability to remove various facial traits, such as jewelry, hair color, and multiple others. Here, it demonstrated reliable performance in removing these attributes. Our results suggest that GANonymization is a promising approach for anonymizing faces while preserving facial expressions.



### ProgDTD: Progressive Learned Image Compression with Double-Tail-Drop Training
- **Arxiv ID**: http://arxiv.org/abs/2305.02145v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02145v1)
- **Published**: 2023-05-03 14:23:37+00:00
- **Updated**: 2023-05-03 14:23:37+00:00
- **Authors**: Ali Hojjat, Janek Haberer, Olaf Landsiedel
- **Comment**: None
- **Journal**: None
- **Summary**: Progressive compression allows images to start loading as low-resolution versions, becoming clearer as more data is received. This increases user experience when, for example, network connections are slow. Today, most approaches for image compression, both classical and learned ones, are designed to be non-progressive. This paper introduces ProgDTD, a training method that transforms learned, non-progressive image compression approaches into progressive ones. The design of ProgDTD is based on the observation that the information stored within the bottleneck of a compression model commonly varies in importance. To create a progressive compression model, ProgDTD modifies the training steps to enforce the model to store the data in the bottleneck sorted by priority. We achieve progressive compression by transmitting the data in order of its sorted index. ProgDTD is designed for CNN-based learned image compression models, does not need additional parameters, and has a customizable range of progressiveness. For evaluation, we apply ProgDTDto the hyperprior model, one of the most common structures in learned image compression. Our experimental results show that ProgDTD performs comparably to its non-progressive counterparts and other state-of-the-art progressive models in terms of MS-SSIM and accuracy.



### Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level
- **Arxiv ID**: http://arxiv.org/abs/2305.02148v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.02148v1)
- **Published**: 2023-05-03 14:29:09+00:00
- **Updated**: 2023-05-03 14:29:09+00:00
- **Authors**: Volodymyr Sydorskyi, Igor Krashenyi, Denis Savka, Oleksandr Zarichkovyi
- **Comment**: None
- **Journal**: IT&I-WS 2022
- **Summary**: We present a new method for functional tissue unit segmentation at the cellular level, which utilizes the latest deep learning semantic segmentation approaches together with domain adaptation and semi-supervised learning techniques. This approach allows for minimizing the domain gap, class imbalance, and captures settings influence between HPA and HubMAP datasets. The presented approach achieves comparable with state-of-the-art-result in functional tissue unit segmentation at the cellular level. The source code is available at https://github.com/VSydorskyy/hubmap_2022_htt_solution



### New Adversarial Image Detection Based on Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2305.03173v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.03173v1)
- **Published**: 2023-05-03 14:32:21+00:00
- **Updated**: 2023-05-03 14:32:21+00:00
- **Authors**: Yulong Wang, Tianxiang Li, Shenghong Li, Xin Yuan, Wei Ni
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to adversarial examples, while adversarial attack models, e.g., DeepFool, are on the rise and outrunning adversarial example detection techniques. This paper presents a new adversarial example detector that outperforms state-of-the-art detectors in identifying the latest adversarial attacks on image datasets. Specifically, we propose to use sentiment analysis for adversarial example detection, qualified by the progressively manifesting impact of an adversarial perturbation on the hidden-layer feature maps of a DNN under attack. Accordingly, we design a modularized embedding layer with the minimum learnable parameters to embed the hidden-layer feature maps into word vectors and assemble sentences ready for sentiment analysis. Extensive experiments demonstrate that the new detector consistently surpasses the state-of-the-art detection algorithms in detecting the latest attacks launched against ResNet and Inception neutral networks on the CIFAR-10, CIFAR-100 and SVHN datasets. The detector only has about 2 million parameters, and takes shorter than 4.6 milliseconds to detect an adversarial example generated by the latest attack models using a Tesla K80 GPU card.



### Transforming Visual Scene Graphs to Image Captions
- **Arxiv ID**: http://arxiv.org/abs/2305.02177v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02177v3)
- **Published**: 2023-05-03 15:18:37+00:00
- **Updated**: 2023-05-05 05:27:30+00:00
- **Authors**: Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Ming Yan, Fei Huang, Zhangzikang Li, Yu Zhang
- **Comment**: 12 pages, 4 figures, has been accepted by ACL 2023 main conference
- **Journal**: None
- **Summary**: We propose to Transform Scene Graphs (TSG) into more descriptive captions. In TSG, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. Since both the encoder and decoder are built based on the MHA, as a result, we construct a homogeneous encoder-decoder unlike the previous heterogeneous ones which usually apply Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TSG. The code is in: https://anonymous.4open.science/r/ACL23_TSG.



### CLUSTSEG: Clustering for Universal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2305.02187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02187v2)
- **Published**: 2023-05-03 15:31:16+00:00
- **Updated**: 2023-05-18 15:34:53+00:00
- **Authors**: James Liang, Tianfei Zhou, Dongfang Liu, Wenguan Wang
- **Comment**: Accepted to ICML 2023; Code:
  https://github.com/JamesLiang819/ClustSeg
- **Journal**: None
- **Summary**: We present CLUSTSEG, a general, transformer-based framework that tackles different image segmentation tasks (i.e., superpixel, semantic, instance, and panoptic) through a unified neural clustering scheme. Regarding queries as cluster centers, CLUSTSEG is innovative in two aspects:1) cluster centers are initialized in heterogeneous ways so as to pointedly address task-specific demands (e.g., instance- or category-level distinctiveness), yet without modifying the architecture; and 2) pixel-cluster assignment, formalized in a cross-attention fashion, is alternated with cluster center update, yet without learning additional parameters. These innovations closely link CLUSTSEG to EM clustering and make it a transparent and powerful framework that yields superior results across the above segmentation tasks.



### Inverse Global Illumination using a Neural Radiometric Prior
- **Arxiv ID**: http://arxiv.org/abs/2305.02192v2
- **DOI**: 10.1145/3588432.3591553
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2305.02192v2)
- **Published**: 2023-05-03 15:36:39+00:00
- **Updated**: 2023-05-18 02:13:18+00:00
- **Authors**: Saeed Hadadan, Geng Lin, Jan Novk, Fabrice Rousselle, Matthias Zwicker
- **Comment**: Homepage: https://inverse-neural-radiosity.github.io
- **Journal**: None
- **Summary**: Inverse rendering methods that account for global illumination are becoming more popular, but current methods require evaluating and automatically differentiating millions of path integrals by tracing multiple light bounces, which remains expensive and prone to noise. Instead, this paper proposes a radiometric prior as a simple alternative to building complete path integrals in a traditional differentiable path tracer, while still correctly accounting for global illumination. Inspired by the Neural Radiosity technique, we use a neural network as a radiance function, and we introduce a prior consisting of the norm of the residual of the rendering equation in the inverse rendering loss. We train our radiance network and optimize scene parameters simultaneously using a loss consisting of both a photometric term between renderings and the multi-view input images, and our radiometric prior (the residual term). This residual term enforces a physical constraint on the optimization that ensures that the radiance field accounts for global illumination. We compare our method to a vanilla differentiable path tracer, and more advanced techniques such as Path Replay Backpropagation. Despite the simplicity of our approach, we can recover scene parameters with comparable and in some cases better quality, at considerably lower computation times.



### DocLangID: Improving Few-Shot Training to Identify the Language of Historical Documents
- **Arxiv ID**: http://arxiv.org/abs/2305.02208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02208v1)
- **Published**: 2023-05-03 15:45:30+00:00
- **Updated**: 2023-05-03 15:45:30+00:00
- **Authors**: Furkan Simsek, Brian Pfitzmann, Hendrik Raetz, Jona Otholt, Haojin Yang, Christoph Meinel
- **Comment**: 6 pages (including references and excluding appendix)
- **Journal**: None
- **Summary**: Language identification describes the task of recognizing the language of written text in documents. This information is crucial because it can be used to support the analysis of a document's vocabulary and context. Supervised learning methods in recent years have advanced the task of language identification. However, these methods usually require large labeled datasets, which often need to be included for various domains of images, such as documents or scene images. In this work, we propose DocLangID, a transfer learning approach to identify the language of unlabeled historical documents. We achieve this by first leveraging labeled data from a different but related domain of historical documents. Secondly, we implement a distance-based few-shot learning approach to adapt a convolutional neural network to new languages of the unlabeled dataset. By introducing small amounts of manually labeled examples from the set of unlabeled images, our feature extractor develops a better adaptability towards new and different data distributions of historical documents. We show that such a model can be effectively fine-tuned for the unlabeled set of images by only reusing the same few-shot examples. We showcase our work across 10 languages that mostly use the Latin script. Our experiments on historical documents demonstrate that our combined approach improves the language identification performance, achieving 74% recognition accuracy on the four unseen languages of the unlabeled dataset.



### Robot Goes Fishing: Rapid, High-Resolution Biological Hotspot Mapping in Coral Reefs with Vision-Guided Autonomous Underwater Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2305.02330v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02330v2)
- **Published**: 2023-05-03 16:12:47+00:00
- **Updated**: 2023-05-22 01:49:29+00:00
- **Authors**: Daniel Yang, Levi Cai, Stewart Jamieson, Yogesh Girdhar
- **Comment**: None
- **Journal**: None
- **Summary**: Coral reefs are fast-changing and complex ecosystems that are crucial to monitor and study. Biological hotspot detection can help coral reef managers prioritize limited resources for monitoring and intervention tasks. Here, we explore the use of autonomous underwater vehicles (AUVs) with cameras, coupled with visual detectors and photogrammetry, to map and identify these hotspots. This approach can provide high spatial resolution information in fast feedback cycles. To the best of our knowledge, we present one of the first attempts at using an AUV to gather visually-observed, fine-grain biological hotspot maps in concert with topography of a coral reefs. Our hotspot maps correlate with rugosity, an established proxy metric for coral reef biodiversity and abundance, as well as with our visual inspections of the 3D reconstruction. We also investigate issues of scaling this approach when applied to new reefs by using these visual detectors pre-trained on large public datasets.



### Multi-dimensional Signal Recovery using Low-rank Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/2305.02264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02264v1)
- **Published**: 2023-05-03 16:51:43+00:00
- **Updated**: 2023-05-03 16:51:43+00:00
- **Authors**: David Reixach
- **Comment**: 5 pages, 2 figures, 1 table, 2 algorithms. To be published in ICASSP
  2023-2023 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), IEEE, 2023, To Appear
- **Journal**: None
- **Summary**: In this work we present Low-rank Deconvolution, a powerful framework for low-level feature-map learning for efficient signal representation with application to signal recovery. Its formulation in multi-linear algebra inherits properties from convolutional sparse coding and low-rank approximation methods as in this setting signals are decomposed in a set of filters convolved with a set of low-rank tensors. We show its advantages by learning compressed video representations and solving image in-painting problems.



### Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models
- **Arxiv ID**: http://arxiv.org/abs/2305.02279v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02279v3)
- **Published**: 2023-05-03 17:15:58+00:00
- **Updated**: 2023-06-29 14:04:26+00:00
- **Authors**: Qiufeng Wang, Xu Yang, Shuxia Lin, Jing Wang, Xin Geng
- **Comment**: None
- **Journal**: None
- **Summary**: During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm Learngene to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an ancestry model. (ii) Condensing: the extensive accumulated knowledge is condensed into a much more compact information piece, i.e., learngene. (iii) Inheriting: the condensed learngene is inherited to make it easier for descendant models to adapt to new environments. Since accumulating has been studied in well-established paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these issues in this paper: (i) Learngene Form: the learngene is set to a few integral layers that can preserve significance. (ii) Learngene Condensing: we identify which layers among the ancestry model have the most similarity as one pseudo descendant model. (iii) Learngene Inheriting: to construct distinct descendant models for the specific downstream tasks, we stack some randomly initialized layers to the learngene layers. Extensive experiments across various settings, including using different network architectures like Vision Transformer (ViT) and Convolutional Neural Networks (CNNs) on different datasets, are carried out to confirm four advantages of Learngene: it makes the descendant models 1) converge more quickly, 2) exhibit less sensitivity to hyperparameters, 3) perform better, and 4) require fewer training samples to converge.



### Iranian License Plate Recognition Using a Reliable Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2305.02292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.02292v1)
- **Published**: 2023-05-03 17:34:10+00:00
- **Updated**: 2023-05-03 17:34:10+00:00
- **Authors**: Soheila Hatami, Majid Sadedel, Farideh Jamali
- **Comment**: Under Review in Scientia Iranica Journal
- **Journal**: None
- **Summary**: The issue of Automatic License Plate Recognition (ALPR) has been one of the most challenging issues in recent years. Weather conditions, camera angle of view, lighting conditions, different characters written on license plates, and many other factors are among the challenges for the issue of ALPR. Given the advances that have been made in recent years in the field of deep neural networks, some types of neural networks and models based on them can be used to perform the task of Iranian license plate recognition. In the proposed method presented in this paper, the license plate recognition is done in two steps. The first step is to detect the rectangles of the license plates from the input image. In the second step, these license plates are cropped from the image and their characters are recognized. For the first step, 3065 images including license plates and for the second step, 3364 images including characters of license plates have been prepared and considered as the desired datasets. In the first step, license plates are detected using the YOLOv4-tiny model, which is based on Convolutional Neural Network (CNN). In the next step, the characters of these license plates are recognized using Convolutional Recurrent Neural Network (CRNN), and Connectionist Temporal Classification (CTC). In the second step, there is no need to segment and label the characters separately, only one string of numbers and letters is enough for the labels.



### DynamicStereo: Consistent Dynamic Depth from Stereo Videos
- **Arxiv ID**: http://arxiv.org/abs/2305.02296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.02296v1)
- **Published**: 2023-05-03 17:40:49+00:00
- **Updated**: 2023-05-03 17:40:49+00:00
- **Authors**: Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, Christian Rupprecht
- **Comment**: CVPR 2023; project page available at
  https://dynamic-stereo.github.io/
- **Journal**: None
- **Summary**: We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing methods for depth from stereo treat different stereo frames independently, leading to temporally inconsistent depth predictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly diminishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate disparity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new benchmark dataset containing synthetic videos of people and animals in scanned environments, which provides complementary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves the quality of predictions of our proposed DynamicStereo as well as prior methods. Finally, it acts as a benchmark for consistent stereo methods.



### Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime
- **Arxiv ID**: http://arxiv.org/abs/2305.02297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02297v1)
- **Published**: 2023-05-03 17:42:54+00:00
- **Updated**: 2023-05-03 17:42:54+00:00
- **Authors**: Chuhan Zhang, Antoine Miech, Jiajun Shen, Jean-Baptiste Alayrac, Pauline Luc
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Large-scale visual language models are widely used as pre-trained models and then adapted for various downstream tasks. While humans are known to efficiently learn new tasks from a few examples, deep learning models struggle with adaptation from few examples. In this work, we look into task adaptation in the low-data regime, and provide a thorough study of the existing adaptation methods for generative Visual Language Models. And we show important benefits of self-labelling, i.e. using the model's own predictions to self-improve when having access to a larger number of unlabelled images of the same distribution. Our study demonstrates significant gains using our proposed task adaptation pipeline across a wide range of visual language tasks such as visual classification (ImageNet), visual captioning (COCO), detailed visual captioning (Localised Narratives) and visual question answering (VQAv2).



### Dynamic Sparse Training with Structured Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2305.02299v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02299v2)
- **Published**: 2023-05-03 17:48:55+00:00
- **Updated**: 2023-05-19 16:11:49+00:00
- **Authors**: Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, Yani Ioannou
- **Comment**: 20 pages, 12 figures
- **Journal**: None
- **Summary**: Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method to learn a variant of structured N:M sparsity by imposing a constant fan-in constraint. We demonstrate with both a theoretical analysis and empirical results: state-of-the-art spare-to-sparse structured DST performance on a variety of network architectures, a condensed representation with a reduced parameter and memory footprint, and reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation. Our source code is available at https://github.com/calgaryml/condensed-sparsity



### Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste
- **Arxiv ID**: http://arxiv.org/abs/2305.02307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2305.02307v1)
- **Published**: 2023-05-03 17:54:50+00:00
- **Updated**: 2023-05-03 17:54:50+00:00
- **Authors**: Mengyun Shi, Serge Belongie, Claire Cardie
- **Comment**: None
- **Journal**: None
- **Summary**: Existing fashion datasets do not consider the multi-facts that cause a consumer to like or dislike a fashion image. Even two consumers like a same fashion image, they could like this image for total different reasons. In this paper, we study the reason why a consumer like a certain fashion image. Towards this goal, we introduce an interpretability dataset, Fashionpedia-taste, consist of rich annotation to explain why a subject like or dislike a fashion image from the following 3 perspectives: 1) localized attributes; 2) human attention; 3) caption. Furthermore, subjects are asked to provide their personal attributes and preference on fashion, such as personality and preferred fashion brands. Our dataset makes it possible for researchers to build computational models to fully understand and interpret human fashion taste from different humanistic perspectives and modalities.



### Real-Time Radiance Fields for Single-Image Portrait View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2305.02310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.02310v1)
- **Published**: 2023-05-03 17:56:01+00:00
- **Updated**: 2023-05-03 17:56:01+00:00
- **Authors**: Alex Trevithick, Matthew Chan, Michael Stengel, Eric R. Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano
- **Comment**: Project page: https://research.nvidia.com/labs/nxp/lp3d/
- **Journal**: None
- **Summary**: We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.



### AG3D: Learning to Generate 3D Avatars from 2D Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2305.02312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02312v1)
- **Published**: 2023-05-03 17:56:24+00:00
- **Updated**: 2023-05-03 17:56:24+00:00
- **Authors**: Zijian Dong, Xu Chen, Jinlong Yang, Michael J. Black, Otmar Hilliges, Andreas Geiger
- **Comment**: Project Page: https://zj-dong.github.io/AG3D/
- **Journal**: None
- **Summary**: While progress in 2D generative models of human appearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire. The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image collections. However, learning realistic and complete 3D appearance and geometry in this under-constrained setting remains challenging, especially in the presence of loose clothing such as dresses. In this paper, we propose a new adversarial generative model of realistic 3D people from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D generator and integrating an efficient and flexible articulation module. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps. We experimentally find that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.



### Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings
- **Arxiv ID**: http://arxiv.org/abs/2305.02317v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2305.02317v1)
- **Published**: 2023-05-03 17:58:29+00:00
- **Updated**: 2023-05-03 17:58:29+00:00
- **Authors**: Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in large language models elicit reasoning in a chain of thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain of thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain of thought baselines, which can be used to enhance downstream performance.



### Wavelet Coherence Of Total Solar Irradiance and Atlantic Climate
- **Arxiv ID**: http://arxiv.org/abs/2305.02319v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.SE, eess.SP, hep-ex, 65T60, 42C40, 37M10, 62M10, 26A15, 43A50, 68-04, 68W30, 85-04,
  85-06, 85-10,, D.2; G.1; G.4; J.2; J.7; H.4; I.4; I.6
- **Links**: [PDF](http://arxiv.org/pdf/2305.02319v1)
- **Published**: 2023-05-03 17:59:05+00:00
- **Updated**: 2023-05-03 17:59:05+00:00
- **Authors**: Vasil Kolev, Yavor Chapanov
- **Comment**: pages 12, Proceedings of the XIII Bulgarian-Serbian Astronomical
  Conference (XIII BSAC), Velingrad, Bulgaria, 2022
- **Journal**: Proceedings of the XIII Bulgarian-Serbian Astronomical Conference
  (XIII BSAC) Velingrad, Bulgaria, October 3-7, no.25, pp.97-107, 2022
- **Summary**: The oscillations of climatic parameters of North Atlantic Ocean play important role in various events in North America and Europe. Several climatic indices are associated with these oscillations. The long term Atlantic temperature anomalies are described by the Atlantic Multidecadal Oscillation (AMO). The Atlantic Multidecadal Oscillation also known as Atlantic Multidecadal Variability (AMV), is the variability of the sea surface temperature (SST) of the North Atlantic Ocean at the timescale of several decades. The AMO is correlated to air temperatures and rainfall over much of the Northern Hemisphere, in particular in the summer climate in North America and Europe. The long-term variations of surface temperature are driven mainly by the cycles of solar activity, represented by the variations of the Total Solar Irradiance (TSI). The frequency and amplitude dependences between the TSI and AMO are analyzed by wavelet coherence of millennial time series since 800 AD till now. The results of wavelet coherence are compared with the detected common solar and climate cycles in narrow frequency bands by the method of Partial Fourier Approximation. The long-term coherence between TSI and AMO can help to understand better the recent climate change and can improve the long term forecast.



### Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?
- **Arxiv ID**: http://arxiv.org/abs/2305.02360v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2305.02360v1)
- **Published**: 2023-05-03 18:00:42+00:00
- **Updated**: 2023-05-03 18:00:42+00:00
- **Authors**: Mengyun Shi, Claire Cardie, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: Consumers are exposed to advertisements across many different domains on the internet, such as fashion, beauty, car, food, and others. On the other hand, fashion represents second highest e-commerce shopping category. Does consumer digital record behavior on various fashion ad images reveal their fashion taste? Does ads from other domains infer their fashion taste as well? In this paper, we study the correlation between advertisements and fashion taste. Towards this goal, we introduce a new dataset, Fashionpedia-Ads, which asks subjects to provide their preferences on both ad (fashion, beauty, car, and dessert) and fashion product (social network and e-commerce style) images. Furthermore, we exhaustively collect and annotate the emotional, visual and textual information on the ad images from multi-perspectives (abstractive level, physical level, captions, and brands). We open-source Fashionpedia-Ads to enable future studies and encourage more approaches to interpretability research between advertisements and fashion taste.



### SimSC: A Simple Framework for Semantic Correspondence with Temperature Learning
- **Arxiv ID**: http://arxiv.org/abs/2305.02385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2305.02385v1)
- **Published**: 2023-05-03 18:52:38+00:00
- **Updated**: 2023-05-03 18:52:38+00:00
- **Authors**: Xinghui Li, Kai Han, Xingchen Wan, Victor Adrian Prisacariu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose SimSC, a remarkably simple framework, to address the problem of semantic matching only based on the feature backbone. We discover that when fine-tuning ImageNet pre-trained backbone on the semantic matching task, L2 normalization of the feature map, a standard procedure in feature matching, produces an overly smooth matching distribution and significantly hinders the fine-tuning process. By setting an appropriate temperature to the softmax, this over-smoothness can be alleviated and the quality of features can be substantially improved. We employ a learning module to predict the optimal temperature for fine-tuning feature backbones. This module is trained together with the backbone and the temperature is updated online. We evaluate our method on three public datasets and demonstrate that we can achieve accuracy on par with state-of-the-art methods under the same backbone without using a learned matching head. Our method is versatile and works on various types of backbones. We show that the accuracy of our framework can be easily improved by coupling it with more powerful backbones.



### Learning-based Relational Object Matching Across Views
- **Arxiv ID**: http://arxiv.org/abs/2305.02398v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2305.02398v1)
- **Published**: 2023-05-03 19:36:51+00:00
- **Updated**: 2023-05-03 19:36:51+00:00
- **Authors**: Cathrin Elich, Iro Armeni, Martin R. Oswald, Marc Pollefeys, Joerg Stueckler
- **Comment**: Accepted for publication in IEEE International Conference on Robotics
  and Automation (ICRA), 2023
- **Journal**: None
- **Summary**: Intelligent robots require object-level scene understanding to reason about possible tasks and interactions with the environment. Moreover, many perception tasks such as scene reconstruction, image retrieval, or place recognition can benefit from reasoning on the level of objects. While keypoint-based matching can yield strong results for finding correspondences for images with small to medium view point changes, for large view point changes, matching semantically on the object-level becomes advantageous. In this paper, we propose a learning-based approach which combines local keypoints with novel object-level features for matching object detections between RGB images. We train our object-level matching features based on appearance and inter-frame and cross-frame spatial relations between objects in an associative graph neural network. We demonstrate our approach in a large variety of views on realistically rendered synthetic images. Our approach compares favorably to previous state-of-the-art object-level matching approaches and achieves improved performance over a pure keypoint-based approach for large view-point changes.



### Synthetic DOmain-Targeted Augmentation (S-DOTA) Improves Model Generalization in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2305.02401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.02401v1)
- **Published**: 2023-05-03 19:53:30+00:00
- **Updated**: 2023-05-03 19:53:30+00:00
- **Authors**: Sai Chowdary Gullapally, Yibo Zhang, Nitin Kumar Mittal, Deeksha Kartik, Sandhya Srinivasan, Kevin Rose, Daniel Shenker, Dinkar Juyal, Harshith Padigela, Raymond Biju, Victor Minden, Chirag Maheshwari, Marc Thibault, Zvi Goldstein, Luke Novak, Nidhi Chandra, Justin Lee, Aaditya Prakash, Chintan Shah, John Abel, Darren Fahy, Amaro Taylor-Weiner, Anand Sampat
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning algorithms have the potential to improve patient outcomes in digital pathology. However, generalization of these tools is currently limited by sensitivity to variations in tissue preparation, staining procedures and scanning equipment that lead to domain shift in digitized slides. To overcome this limitation and improve model generalization, we studied the effectiveness of two Synthetic DOmain-Targeted Augmentation (S-DOTA) methods, namely CycleGAN-enabled Scanner Transform (ST) and targeted Stain Vector Augmentation (SVA), and compared them against the International Color Consortium (ICC) profile-based color calibration (ICC Cal) method and a baseline method using traditional brightness, color and noise augmentations. We evaluated the ability of these techniques to improve model generalization to various tasks and settings: four models, two model types (tissue segmentation and cell classification), two loss functions, six labs, six scanners, and three indications (hepatocellular carcinoma (HCC), nonalcoholic steatohepatitis (NASH), prostate adenocarcinoma). We compared these methods based on the macro-averaged F1 scores on in-distribution (ID) and out-of-distribution (OOD) test sets across multiple domains, and found that S-DOTA methods (i.e., ST and SVA) led to significant improvements over ICC Cal and baseline on OOD data while maintaining comparable performance on ID data. Thus, we demonstrate that S-DOTA may help address generalization due to domain shift in real world applications.



### GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content
- **Arxiv ID**: http://arxiv.org/abs/2305.02422v3
- **DOI**: 10.1109/LSP.2023.3255011
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2305.02422v3)
- **Published**: 2023-05-03 20:29:04+00:00
- **Updated**: 2023-08-29 22:12:04+00:00
- **Authors**: Yu-Chih Chen, Avinab Saha, Chase Davis, Bo Qiu, Xiaoming Wang, Rahul Gowda, Ioannis Katsavounidis, Alan C. Bovik
- **Comment**: Accepted to IEEE SPL 2023. The implementation of GAMIVAL has been
  made available online: https://github.com/lskdream/GAMIVAL
- **Journal**: IEEE Signal Processing Letters, vol. 30, pp. 324-328, 2023
- **Summary**: The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, and deep semantic features. Using a support vector regression (SVR) as a regressor, GAMIVAL achieves superior performance on the new LIVE-Meta Mobile Cloud Gaming (LIVE-Meta MCG) video quality database.



### Shap-E: Generating Conditional 3D Implicit Functions
- **Arxiv ID**: http://arxiv.org/abs/2305.02463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2305.02463v1)
- **Published**: 2023-05-03 23:59:13+00:00
- **Updated**: 2023-05-03 23:59:13+00:00
- **Authors**: Heewoo Jun, Alex Nichol
- **Comment**: 23 pages, 13 figures
- **Journal**: None
- **Summary**: We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.




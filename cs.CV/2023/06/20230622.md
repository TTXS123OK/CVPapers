# Arxiv Papers in cs.CV on 2023-06-22
### Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches
- **Arxiv ID**: http://arxiv.org/abs/2306.12610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12610v1)
- **Published**: 2023-06-22 00:13:44+00:00
- **Updated**: 2023-06-22 00:13:44+00:00
- **Authors**: Aniruddha Saha, Shuhua Yu, Arash Norouzzadeh, Wan-Yi Lin, Chaithanya Kumar Mummadi
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser arXiv:2108.09135 [cs.CV], the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout arXiv:1708.04552v2 [cs.CV] augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1\% to 62.3\% against a 3\% square patch applied anywhere on the image.



### RXFOOD: Plug-in RGB-X Fusion for Object of Interest Detection
- **Arxiv ID**: http://arxiv.org/abs/2306.12621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12621v1)
- **Published**: 2023-06-22 01:27:00+00:00
- **Updated**: 2023-06-22 01:27:00+00:00
- **Authors**: Jin Ma, Jinlong Li, Qing Guo, Tianyun Zhang, Yuewei Lin, Hongkai Yu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The emergence of different sensors (Near-Infrared, Depth, etc.) is a remedy for the limited application scenarios of traditional RGB camera. The RGB-X tasks, which rely on RGB input and another type of data input to resolve specific problems, have become a popular research topic in multimedia. A crucial part in two-branch RGB-X deep neural networks is how to fuse information across modalities. Given the tremendous information inside RGB-X networks, previous works typically apply naive fusion (e.g., average or max fusion) or only focus on the feature fusion at the same scale(s). While in this paper, we propose a novel method called RXFOOD for the fusion of features across different scales within the same modality branch and from different modality branches simultaneously in a unified attention mechanism. An Energy Exchange Module is designed for the interaction of each feature map's energy matrix, who reflects the inter-relationship of different positions and different channels inside a feature map. The RXFOOD method can be easily incorporated to any dual-branch encoder-decoder network as a plug-in module, and help the original backbone network better focus on important positions and channels for object of interest detection. Experimental results on RGB-NIR salient object detection, RGB-D salient object detection, and RGBFrequency image manipulation detection demonstrate the clear effectiveness of the proposed RXFOOD.



### DreamEdit: Subject-driven Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2306.12624v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12624v2)
- **Published**: 2023-06-22 01:29:06+00:00
- **Updated**: 2023-08-16 18:30:35+00:00
- **Authors**: Tianle Li, Max Ku, Cong Wei, Wenhu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Subject-driven image generation aims at generating images containing customized subjects, which has recently drawn enormous attention from the research community. However, the previous works cannot precisely control the background and position of the target subject. In this work, we aspire to fill the void and propose two novel subject-driven sub-tasks, i.e., Subject Replacement and Subject Addition. The new tasks are challenging in multiple aspects: replacing a subject with a customized one can change its shape, texture, and color, while adding a target subject to a designated position in a provided scene necessitates a context-aware posture. To conquer these two novel tasks, we first manually curate a new dataset DreamEditBench containing 22 different types of subjects, and 440 source images with different difficulty levels. We plan to host DreamEditBench as a platform and hire trained evaluators for standard human evaluation. We also devise an innovative method DreamEditor to resolve these tasks by performing iterative generation, which enables a smooth adaptation to the customized subject. In this project, we conduct automatic and human evaluations to understand the performance of DreamEditor and baselines on DreamEditBench. For Subject Replacement, we found that the existing models are sensitive to the shape and color of the original subject. The model failure rate will dramatically increase when the source and target subjects are highly different. For Subject Addition, we found that the existing models cannot easily blend the customized subjects into the background smoothly, leading to noticeable artifacts in the generated image. We hope DreamEditBench can become a standard platform to enable future investigations toward building more controllable subject-driven image editing. Our project homepage is https://dreameditbenchteam.github.io/.



### 1st Place Solution to MultiEarth 2023 Challenge on Multimodal SAR-to-EO Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2306.12626v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12626v1)
- **Published**: 2023-06-22 01:32:30+00:00
- **Updated**: 2023-06-22 01:32:30+00:00
- **Authors**: Jingi Ju, Hyeoncheol Noh, Minwoo Kim, Dong-Geol Choi
- **Comment**: None
- **Journal**: None
- **Summary**: The Multimodal Learning for Earth and Environment Workshop (MultiEarth 2023) aims to harness the substantial amount of remote sensing data gathered over extensive periods for the monitoring and analysis of Earth's ecosystems'health. The subtask, Multimodal SAR-to-EO Image Translation, involves the use of robust SAR data, even under adverse weather and lighting conditions, transforming it into high-quality, clear, and visually appealing EO data. In the context of the SAR2EO task, the presence of clouds or obstructions in EO data can potentially pose a challenge. To address this issue, we propose the Clean Collector Algorithm (CCA), designed to take full advantage of this cloudless SAR data and eliminate factors that may hinder the data learning process. Subsequently, we applied pix2pixHD for the SAR-to-EO translation and Restormer for image enhancement. In the final evaluation, the team 'CDRL' achieved an MAE of 0.07313, securing the top rank on the leaderboard.



### Targeted collapse regularized autoencoder for anomaly detection: black hole at the center
- **Arxiv ID**: http://arxiv.org/abs/2306.12627v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2306.12627v1)
- **Published**: 2023-06-22 01:33:47+00:00
- **Updated**: 2023-06-22 01:33:47+00:00
- **Authors**: Amin Ghafourian, Huanyi Shui, Devesh Upadhyay, Rajesh Gupta, Dimitar Filev, Iman Soltani Bozchalooi
- **Comment**: 16 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes the requirement for hyperparameter tuning and customization for new applications which, paired with its permissive data modality constraint, enhances the potential for successful adoption across a broad range of applications. We test the method on various visual and tabular benchmarks and demonstrate that the technique matches and frequently outperforms alternatives. We also provide a theoretical analysis and numerical simulations that help demonstrate the underlying process that unfolds during training and how it can help with anomaly detection. This mitigates the black-box nature of autoencoder-based anomaly detection algorithms and offers an avenue for further investigation of advantages, fail cases, and potential new directions.



### Efficient quantum image representation and compression circuit using zero-discarded state preparation approach
- **Arxiv ID**: http://arxiv.org/abs/2306.12634v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12634v1)
- **Published**: 2023-06-22 02:18:56+00:00
- **Updated**: 2023-06-22 02:18:56+00:00
- **Authors**: Md Ershadul Haque, Manoranjan Paul, Anwaar Ulhaq, Tanmoy Debnath
- **Comment**: 7 figures
- **Journal**: None
- **Summary**: Quantum image computing draws a lot of attention due to storing and processing image data faster than classical. With increasing the image size, the number of connections also increases, leading to the circuit complex. Therefore, efficient quantum image representation and compression issues are still challenging. The encoding of images for representation and compression in quantum systems is different from classical ones. In quantum, encoding of position is more concerned which is the major difference from the classical. In this paper, a novel zero-discarded state connection novel enhance quantum representation (ZSCNEQR) approach is introduced to reduce complexity further by discarding '0' in the location representation information. In the control operational gate, only input '1' contribute to its output thus, discarding zero makes the proposed ZSCNEQR circuit more efficient. The proposed ZSCNEQR approach significantly reduced the required bit for both representation and compression. The proposed method requires 11.76\% less qubits compared to the recent existing method. The results show that the proposed approach is highly effective for representing and compressing images compared to the two relevant existing methods in terms of rate-distortion performance.



### TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter
- **Arxiv ID**: http://arxiv.org/abs/2306.12642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12642v1)
- **Published**: 2023-06-22 03:00:24+00:00
- **Updated**: 2023-06-22 03:00:24+00:00
- **Authors**: Binjie Zhang, Yixiao Ge, Xuyuan Xu, Ying Shan, Mike Zheng Shou
- **Comment**: None
- **Journal**: None
- **Summary**: Visual foundation models like CLIP excel in learning feature representations from extensive datasets through self-supervised methods, demonstrating remarkable transfer learning and generalization capabilities. A growing number of applications based on visual foundation models are emerging, including innovative solutions such as BLIP-2. These applications employ pre-trained CLIP models as upstream feature extractors and train various downstream modules to accomplish diverse tasks. In situations involving system upgrades that require updating the upstream foundation model, it becomes essential to re-train all downstream modules to adapt to the new foundation model, which is inflexible and inefficient. In this paper, we introduce a parameter-efficient and task-agnostic adapter, dubbed TaCA, that facilitates compatibility across distinct foundation models while ensuring enhanced performance for the new models. TaCA allows downstream applications to seamlessly integrate better-performing foundation models without necessitating retraining. We conduct extensive experimental validation of TaCA using different scales of models with up to one billion parameters on various tasks such as video-text retrieval, video recognition, and visual question answering. The results consistently demonstrate the emergent ability of TaCA on hot-plugging upgrades for visual foundation models. Codes and models will be available at https://github.com/TencentARC/TaCA.



### Learnability and Algorithm for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.12646v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12646v1)
- **Published**: 2023-06-22 03:08:42+00:00
- **Updated**: 2023-06-22 03:08:42+00:00
- **Authors**: Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Bing Liu
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: This paper studies the challenging continual learning (CL) setting of Class Incremental Learning (CIL). CIL learns a sequence of tasks consisting of disjoint sets of concepts or classes. At any time, a single model is built that can be applied to predict/classify test instances of any classes learned thus far without providing any task related information for each test instance. Although many techniques have been proposed for CIL, they are mostly empirical. It has been shown recently that a strong CIL system needs a strong within-task prediction (WP) and a strong out-of-distribution (OOD) detection for each task. However, it is still not known whether CIL is actually learnable. This paper shows that CIL is learnable. Based on the theory, a new CIL algorithm is also proposed. Experimental results demonstrate its effectiveness.



### Curriculum Knowledge Switching for Pancreas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.12651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12651v1)
- **Published**: 2023-06-22 03:35:09+00:00
- **Updated**: 2023-06-22 03:35:09+00:00
- **Authors**: Yumou Tang, Kun Zhan, Zhibo Tian, Mingxuan Zhang, Saisai Wang, Xueming Wen
- **Comment**: ICIP 2023
- **Journal**: ICIP 2023
- **Summary**: Pancreas segmentation is challenging due to the small proportion and highly changeable anatomical structure. It motivates us to propose a novel segmentation framework, namely Curriculum Knowledge Switching (CKS) framework, which decomposes detecting pancreas into three phases with different difficulty extent: straightforward, difficult, and challenging. The framework switches from straightforward to challenging phases and thereby gradually learns to detect pancreas. In addition, we adopt the momentum update parameter updating mechanism during switching, ensuring the loss converges gradually when the input dataset changes. Experimental results show that different neural network backbones with the CKS framework achieved state-of-the-art performance on the NIH dataset as measured by the DSC metric.



### Hand Pose Estimation with Mems-Ultrasonic Sensors
- **Arxiv ID**: http://arxiv.org/abs/2306.12652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2306.12652v1)
- **Published**: 2023-06-22 03:41:47+00:00
- **Updated**: 2023-06-22 03:41:47+00:00
- **Authors**: Qiang Zhang, Yuanqiao Lin, Yubin Lin, Szymon Rusinkiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: Hand tracking is an important aspect of human-computer interaction and has a wide range of applications in extended reality devices. However, current hand motion capture methods suffer from various limitations. For instance, visual-based hand pose estimation is susceptible to self-occlusion and changes in lighting conditions, while IMU-based tracking gloves experience significant drift and are not resistant to external magnetic field interference. To address these issues, we propose a novel and low-cost hand-tracking glove that utilizes several MEMS-ultrasonic sensors attached to the fingers, to measure the distance matrix among the sensors. Our lightweight deep network then reconstructs the hand pose from the distance matrix. Our experimental results demonstrate that this approach is both accurate, size-agnostic, and robust to external interference. We also show the design logic for the sensor selection, sensor configurations, circuit diagram, as well as model architecture.



### Identifying and Disentangling Spurious Features in Pretrained Image Representations
- **Arxiv ID**: http://arxiv.org/abs/2306.12673v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12673v1)
- **Published**: 2023-06-22 05:16:58+00:00
- **Updated**: 2023-06-22 05:16:58+00:00
- **Authors**: Rafayel Darbinyan, Hrayr Harutyunyan, Aram H. Markosyan, Hrant Khachatrian
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks employ spurious correlations in their predictions, resulting in decreased performance when these correlations do not hold. Recent works suggest fixing pretrained representations and training a classification head that does not use spurious features. We investigate how spurious features are represented in pretrained representations and explore strategies for removing information about spurious features. Considering the Waterbirds dataset and a few pretrained representations, we find that even with full knowledge of spurious features, their removal is not straightforward due to entangled representation. To address this, we propose a linear autoencoder training method to separate the representation into core, spurious, and other features. We propose two effective spurious feature removal approaches that are applied to the encoding and significantly improve classification performance measured by worst group accuracy.



### One at A Time: Multi-step Volumetric Probability Distribution Diffusion for Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2306.12681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12681v2)
- **Published**: 2023-06-22 05:55:53+00:00
- **Updated**: 2023-07-07 08:43:39+00:00
- **Authors**: Bohan Li, Jingxin Dong, Yunnan Wang, Jinming Liu, Lianying Yin, Wei Zhao, Zheng Zhu, Xin Jin, Wenjun Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have explored the fundamental role of depth estimation in multi-view stereo (MVS) and semantic scene completion (SSC). They generally construct 3D cost volumes to explore geometric correspondence in depth, and estimate such volumes in a single step relying directly on the ground truth approximation. However, such problem cannot be thoroughly handled in one step due to complex empirical distributions, especially in challenging regions like occlusions, reflections, etc. In this paper, we formulate the depth estimation task as a multi-step distribution approximation process, and introduce a new paradigm of modeling the Volumetric Probability Distribution progressively (step-by-step) following a Markov chain with Diffusion models (VPDD). Specifically, to constrain the multi-step generation of volume in VPDD, we construct a meta volume guidance and a confidence-aware contextual guidance as conditional geometry priors to facilitate the distribution approximation. For the sampling process, we further investigate an online filtering strategy to maintain consistency in volume representations for stable training. Experiments demonstrate that our plug-and-play VPDD outperforms the state-of-the-arts for tasks of MVS and SSC, and can also be easily extended to different baselines to get improvement. It is worth mentioning that we are the first camera-based work that surpasses LiDAR-based methods on the SemanticKITTI dataset.



### Rethinking the Backward Propagation for Adversarial Transferability
- **Arxiv ID**: http://arxiv.org/abs/2306.12685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12685v1)
- **Published**: 2023-06-22 06:12:23+00:00
- **Updated**: 2023-06-22 06:12:23+00:00
- **Authors**: Xiaosen Wang, Kangheng Tong, Kun He
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Transfer-based attacks generate adversarial examples on the surrogate model, which can mislead other black-box models without any access, making it promising to attack real-world applications. Recently, several works have been proposed to boost adversarial transferability, in which the surrogate model is usually overlooked. In this work, we identify that non-linear layers (e.g., ReLU, max-pooling, etc.) truncate the gradient during backward propagation, making the gradient w.r.t.input image imprecise to the loss function. We hypothesize and empirically validate that such truncation undermines the transferability of adversarial examples. Based on these findings, we propose a novel method called Backward Propagation Attack (BPA) to increase the relevance between the gradient w.r.t. input image and loss function so as to generate adversarial examples with higher transferability. Specifically, BPA adopts a non-monotonic function as the derivative of ReLU and incorporates softmax with temperature to smooth the derivative of max-pooling, thereby mitigating the information loss during the backward propagation of gradients. Empirical results on the ImageNet dataset demonstrate that not only does our method substantially boost the adversarial transferability, but it also is general to existing transfer-based attacks.



### FlowFace++: Explicit Semantic Flow-supervised End-to-End Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2306.12686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12686v2)
- **Published**: 2023-06-22 06:18:29+00:00
- **Updated**: 2023-06-26 05:11:17+00:00
- **Authors**: Yu Zhang, Hao Zeng, Bowen Ma, Wei Zhang, Zhimeng Zhang, Yu Ding, Tangjie Lv, Changjie Fan
- **Comment**: arXiv admin note: text overlap with arXiv:2212.02797
- **Journal**: None
- **Summary**: This work proposes a novel face-swapping framework FlowFace++, utilizing explicit semantic flow supervision and end-to-end architecture to facilitate shape-aware face-swapping. Specifically, our work pretrains a facial shape discriminator to supervise the face swapping network. The discriminator is shape-aware and relies on a semantic flow-guided operation to explicitly calculate the shape discrepancies between the target and source faces, thus optimizing the face swapping network to generate highly realistic results. The face swapping network is a stack of a pre-trained face-masked autoencoder (MAE), a cross-attention fusion module, and a convolutional decoder. The MAE provides a fine-grained facial image representation space, which is unified for the target and source faces and thus facilitates final realistic results. The cross-attention fusion module carries out the source-to-target face swapping in a fine-grained latent space while preserving other attributes of the target image (e.g. expression, head pose, hair, background, illumination, etc). Lastly, the convolutional decoder further synthesizes the swapping results according to the face-swapping latent embedding from the cross-attention fusion module. Extensive quantitative and qualitative experiments on in-the-wild faces demonstrate that our FlowFace++ outperforms the state-of-the-art significantly, particularly while the source face is obstructed by uneven lighting or angle offset.



### Set-Membership Inference Attacks using Data Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2307.15067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2307.15067v1)
- **Published**: 2023-06-22 06:47:56+00:00
- **Updated**: 2023-06-22 06:47:56+00:00
- **Authors**: Mike Laszkiewicz, Denis Lukovnikov, Johannes Lederer, Asja Fischer
- **Comment**: Preliminary work
- **Journal**: None
- **Summary**: In this work, we propose a set-membership inference attack for generative models using deep image watermarking techniques. In particular, we demonstrate how conditional sampling from a generative model can reveal the watermark that was injected into parts of the training data. Our empirical results demonstrate that the proposed watermarking technique is a principled approach for detecting the non-consensual use of image data in training generative models.



### Ladder Fine-tuning approach for SAM integrating complementary network
- **Arxiv ID**: http://arxiv.org/abs/2306.12737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12737v1)
- **Published**: 2023-06-22 08:36:17+00:00
- **Updated**: 2023-06-22 08:36:17+00:00
- **Authors**: Shurong Chai, Rahul Kumar Jain, Shiyu Teng, Jiaqing Liu, Yinhao Li, Tomoko Tateyama, Yen-wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, foundation models have been introduced demonstrating various tasks in the field of computer vision. These models such as Segment Anything Model (SAM) are generalized models trained using huge datasets. Currently, ongoing research focuses on exploring the effective utilization of these generalized models for specific domains, such as medical imaging. However, in medical imaging, the lack of training samples due to privacy concerns and other factors presents a major challenge for applying these generalized models to medical image segmentation task. To address this issue, the effective fine tuning of these models is crucial to ensure their optimal utilization. In this study, we propose to combine a complementary Convolutional Neural Network (CNN) along with the standard SAM network for medical image segmentation. To reduce the burden of fine tuning large foundation model and implement cost-efficient trainnig scheme, we focus only on fine-tuning the additional CNN network and SAM decoder part. This strategy significantly reduces trainnig time and achieves competitive results on publicly available dataset. The code is available at https://github.com/11yxk/SAM-LST.



### Restoration of the JPEG Maximum Lossy Compressed Face Images with Hourglass Block based on Early Stopping Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2306.12757v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12757v1)
- **Published**: 2023-06-22 09:21:48+00:00
- **Updated**: 2023-06-22 09:21:48+00:00
- **Authors**: Jongwook Si, Sungyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: When a JPEG image is compressed using the loss compression method with a high compression rate, a blocking phenomenon can occur in the image, making it necessary to restore the image to its original quality. In particular, restoring compressed images that are unrecognizable presents an innovative challenge. Therefore, this paper aims to address the restoration of JPEG images that have suffered significant loss due to maximum compression using a GAN-based net-work method. The generator in this network is based on the U-Net architecture and features a newly presented hourglass structure that can preserve the charac-teristics of deep layers. Additionally, the network incorporates two loss functions, LF Loss and HF Loss, to generate natural and high-performance images. HF Loss uses a pretrained VGG-16 network and is configured using a specific layer that best represents features, which can enhance performance for the high-frequency region. LF Loss, on the other hand, is used to handle the low-frequency region. These two loss functions facilitate the generation of images by the generator that can deceive the discriminator while accurately generating both high and low-frequency regions. The results show that the blocking phe-nomenon in lost compressed images was removed, and recognizable identities were generated. This study represents a significant improvement over previous research in terms of image restoration performance.



### Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2306.12760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.12760v1)
- **Published**: 2023-06-22 09:34:55+00:00
- **Updated**: 2023-06-22 09:34:55+00:00
- **Authors**: Ori Gordon, Omri Avrahami, Dani Lischinski
- **Comment**: 14 pages, 12 figures. Project page:
  https://www.vision.huji.ac.il/blended-nerf/
- **Journal**: None
- **Summary**: Editing a local region or a specific object in a 3D scene represented by a NeRF is challenging, mainly due to the implicit nature of the scene representation. Consistently blending a new realistic object into the scene adds an additional level of difficulty. We present Blended-NeRF, a robust and flexible framework for editing a specific region of interest in an existing NeRF scene, based on text prompts or image patches, along with a 3D ROI box. Our method leverages a pretrained language-image model to steer the synthesis towards a user-provided text prompt or image patch, along with a 3D MLP model initialized on an existing NeRF scene to generate the object and blend it into a specified region in the original scene. We allow local editing by localizing a 3D ROI box in the input scene, and seamlessly blend the content synthesized inside the ROI with the existing scene using a novel volumetric blending technique. To obtain natural looking and view-consistent results, we leverage existing and new geometric priors and 3D augmentations for improving the visual fidelity of the final result.   We test our framework both qualitatively and quantitatively on a variety of real 3D scenes and text prompts, demonstrating realistic multi-view consistent results with much flexibility and diversity compared to the baselines. Finally, we show the applicability of our framework for several 3D editing applications, including adding new objects to a scene, removing/replacing/altering existing objects, and texture conversion.



### 3D Reconstruction of Spherical Images based on Incremental Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2306.12770v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12770v2)
- **Published**: 2023-06-22 09:49:28+00:00
- **Updated**: 2023-06-24 11:00:15+00:00
- **Authors**: San Jiang, Kan You, Yaxin Li, Duojie Weng, Wu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D reconstruction plays an increasingly important role in modern photogrammetric systems. Conventional satellite or aerial-based remote sensing (RS) platforms can provide the necessary data sources for the 3D reconstruction of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned Aerial Vehicles), 3D reconstruction in complicated situations, such as urban canyons and indoor scenes, is challenging due to the frequent tracking failures between camera frames and high data collection costs. Recently, spherical images have been extensively exploited due to the capability of recording surrounding environments from one camera exposure. Classical 3D reconstruction pipelines, however, cannot be used for spherical images. Besides, there exist few software packages for 3D reconstruction of spherical images. Based on the imaging geometry of spherical cameras, this study investigates the algorithms for the relative orientation using spherical correspondences, absolute orientation using 3D correspondences between scene and spherical points, and the cost functions for BA (bundle adjustment) optimization. In addition, an incremental SfM (Structure from Motion) workflow has been proposed for spherical images using the above-mentioned algorithms. The proposed solution is finally verified by using three spherical datasets captured by both consumer-grade and professional spherical cameras. The results demonstrate that the proposed SfM workflow can achieve the successful 3D reconstruction of complex scenes and provide useful clues for the implementation in open-source software packages. The source code of the designed SfM workflow would be made publicly available.



### DiffWA: Diffusion Models for Watermark Attack
- **Arxiv ID**: http://arxiv.org/abs/2306.12790v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12790v1)
- **Published**: 2023-06-22 10:45:49+00:00
- **Updated**: 2023-06-22 10:45:49+00:00
- **Authors**: Xinyu Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of deep neural networks(DNNs), many robust blind watermarking algorithms and frameworks have been proposed and achieved good results. At present, the watermark attack algorithm can not compete with the watermark addition algorithm. And many watermark attack algorithms only care about interfering with the normal extraction of the watermark, and the watermark attack will cause great visual loss to the image. To this end, we propose DiffWA, a conditional diffusion model with distance guidance for watermark attack, which can restore the image while removing the embedded watermark. The core of our method is training an image-to-image conditional diffusion model on unwatermarked images and guiding the conditional model using a distance guidance when sampling so that the model will generate unwatermarked images which is similar to original images. We conducted experiments on CIFAR-10 using our proposed models. The results shows that the model can remove the watermark with good effect and make the bit error rate of watermark extraction higher than 0.4. At the same time, the attacked image will maintain good visual effect with PSNR more than 31 and SSIM more than 0.97 compared with the original image.



### Learning Unseen Modality Interaction
- **Arxiv ID**: http://arxiv.org/abs/2306.12795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12795v1)
- **Published**: 2023-06-22 10:53:10+00:00
- **Updated**: 2023-06-22 10:53:10+00:00
- **Authors**: Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek
- **Comment**: Under review
- **Journal**: None
- **Summary**: Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences. In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a feature projection module to project the multidimensional features of different modalities into a common space with rich information reserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to unreliable modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, and multimedia retrieval.



### Super-Resolution of BVOC Emission Maps Via Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2306.12796v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12796v1)
- **Published**: 2023-06-22 10:59:15+00:00
- **Updated**: 2023-06-22 10:59:15+00:00
- **Authors**: Antonio Giganti, Sara Mandelli, Paolo Bestagini, Marco Marcon, Stefano Tubaro
- **Comment**: 4 pages, 4 figures, 1 table, accepted at IEEE-IGARSS 2023
- **Journal**: None
- **Summary**: Enhancing the resolution of Biogenic Volatile Organic Compound (BVOC) emission maps is a critical task in remote sensing. Recently, some Super-Resolution (SR) methods based on Deep Learning (DL) have been proposed, leveraging data from numerical simulations for their training process. However, when dealing with data derived from satellite observations, the reconstruction is particularly challenging due to the scarcity of measurements to train SR algorithms with. In our work, we aim at super-resolving low resolution emission maps derived from satellite observations by leveraging the information of emission maps obtained through numerical simulations. To do this, we combine a SR method based on DL with Domain Adaptation (DA) techniques, harmonizing the different aggregation strategies and spatial information used in simulated and observed domains to ensure compatibility. We investigate the effectiveness of DA strategies at different stages by systematically varying the number of simulated and observed emissions used, exploring the implications of data scarcity on the adaptation strategies. To the best of our knowledge, there are no prior investigations of DA in satellite-derived BVOC maps enhancement. Our work represents a first step toward the development of robust strategies for the reconstruction of observed BVOC emissions.



### XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance
- **Arxiv ID**: http://arxiv.org/abs/2306.12816v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12816v1)
- **Published**: 2023-06-22 11:31:11+00:00
- **Updated**: 2023-06-22 11:31:11+00:00
- **Authors**: Benedict Clark, Rick Wilming, Stefan Haufe
- **Comment**: Under review
- **Journal**: None
- **Summary**: The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI methods are often unable to significantly outperform random performance baselines and edge detection methods. Moreover, we demonstrate that explanations derived from different model architectures can be vastly different; thus, prone to misinterpretation even under controlled conditions.



### Learning from Visual Observation via Offline Pretrained State-to-Go Transformer
- **Arxiv ID**: http://arxiv.org/abs/2306.12860v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12860v1)
- **Published**: 2023-06-22 13:14:59+00:00
- **Updated**: 2023-06-22 13:14:59+00:00
- **Authors**: Bohan Zhou, Ke Li, Jiechuan Jiang, Zongqing Lu
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potential of utilizing video-only data to solve difficult visual reinforcement learning tasks rather than relying on complete offline datasets containing states, actions, and rewards. The project's website and code can be found at https://sites.google.com/view/stgtransformer.



### Data-Free Backbone Fine-Tuning for Pruned Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2306.12881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12881v1)
- **Published**: 2023-06-22 13:44:40+00:00
- **Updated**: 2023-06-22 13:44:40+00:00
- **Authors**: Adrian Holzbock, Achyut Hegde, Klaus Dietmayer, Vasileios Belagiannis
- **Comment**: Accpeted for presentation at the 31st European Signal Processing
  Conference (EUSIPCO) 2023, September 4-8, 2023, Helsinki, Finland
- **Journal**: None
- **Summary**: Model compression techniques reduce the computational load and memory consumption of deep neural networks. After the compression operation, e.g. parameter pruning, the model is normally fine-tuned on the original training dataset to recover from the performance drop caused by compression. However, the training data is not always available due to privacy issues or other factors. In this work, we present a data-free fine-tuning approach for pruning the backbone of deep neural networks. In particular, the pruned network backbone is trained with synthetically generated images, and our proposed intermediate supervision to mimic the unpruned backbone's output feature map. Afterwards, the pruned backbone can be combined with the original network head to make predictions. We generate synthetic images by back-propagating gradients to noise images while relying on L1-pruning for the backbone pruning. In our experiments, we show that our approach is task-independent due to pruning only the backbone. By evaluating our approach on 2D human pose estimation, object detection, and image classification, we demonstrate promising performance compared to the unpruned model. Our code is available at https://github.com/holzbock/dfbf.



### Multi-Objective Hull Form Optimization with CAD Engine-based Deep Learning Physics for 3D Flow Prediction
- **Arxiv ID**: http://arxiv.org/abs/2306.12915v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.flu-dyn, J.2
- **Links**: [PDF](http://arxiv.org/pdf/2306.12915v1)
- **Published**: 2023-06-22 14:30:41+00:00
- **Updated**: 2023-06-22 14:30:41+00:00
- **Authors**: Jocelyn Ahmed Mazari, Antoine Reverberi, Pierre Yser, Sebastian Sigmund
- **Comment**: X International Conference on Computational Methods in Marine
  Engineering, MARINE 2023, Madrid, Spain
- **Journal**: None
- **Summary**: In this work, we propose a built-in Deep Learning Physics Optimization (DLPO) framework to set up a shape optimization study of the Duisburg Test Case (DTC) container vessel. We present two different applications: (1) sensitivity analysis to detect the most promising generic basis hull shapes, and (2) multi-objective optimization to quantify the trade-off between optimal hull forms. DLPO framework allows for the evaluation of design iterations automatically in an end-to-end manner. We achieved these results by coupling Extrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer. Our proposed DLP model is trained on full 3D volume data coming from RANS simulations, and it can provide accurate and high-quality 3D flow predictions in real-time, which makes it a good evaluator to perform optimization of new container vessel designs w.r.t the hydrodynamic efficiency. In particular, it is able to recover the forces acting on the vessel by integration on the hull surface with a mean relative error of 3.84\% \pm 2.179\% on the total resistance. Each iteration takes only 20 seconds, thus leading to a drastic saving of time and engineering efforts, while delivering valuable insight into the performance of the vessel, including RANS-like detailed flow information. We conclude that DLPO framework is a promising tool to accelerate the ship design process and lead to more efficient ships with better hydrodynamic performance.



### Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing
- **Arxiv ID**: http://arxiv.org/abs/2306.12929v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12929v1)
- **Published**: 2023-06-22 14:39:04+00:00
- **Updated**: 2023-06-22 14:39:04+00:00
- **Authors**: Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention matrix for a no-update, the input to the softmax is pushed to be larger and larger during training, causing outliers in other parts of the network. Based on these observations, we propose two simple (independent) modifications to the attention mechanism - clipped softmax and gated attention. We empirically show that models pre-trained using our methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance. This enables us to quantize transformers to full INT8 quantization of the activations without any additional effort. We demonstrate the effectiveness of our methods on both language models (BERT, OPT) and vision transformers.



### Feature Mixing for Writer Retrieval and Identification on Papyri Fragments
- **Arxiv ID**: http://arxiv.org/abs/2306.12939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12939v1)
- **Published**: 2023-06-22 14:55:01+00:00
- **Updated**: 2023-06-22 14:55:01+00:00
- **Authors**: Marco Peer, Robert Sablatnig
- **Comment**: accepted for HIP@ICDAR2023
- **Journal**: None
- **Summary**: This paper proposes a deep-learning-based approach to writer retrieval and identification for papyri, with a focus on identifying fragments associated with a specific writer and those corresponding to the same image. We present a novel neural network architecture that combines a residual backbone with a feature mixing stage to improve retrieval performance, and the final descriptor is derived from a projection layer. The methodology is evaluated on two benchmarks: PapyRow, where we achieve a mAP of 26.6 % and 24.9 % on writer and page retrieval, and HisFragIR20, showing state-of-the-art performance (44.0 % and 29.3 % mAP). Furthermore, our network has an accuracy of 28.7 % for writer identification. Additionally, we conduct experiments on the influence of two binarization techniques on fragments and show that binarizing does not enhance performance. Our code and models are available to the community.



### Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models
- **Arxiv ID**: http://arxiv.org/abs/2306.12941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.12941v1)
- **Published**: 2023-06-22 14:56:06+00:00
- **Updated**: 2023-06-22 14:56:06+00:00
- **Authors**: Francesco Croce, Naman D Singh, Matthias Hein
- **Comment**: None
- **Journal**: None
- **Summary**: While a large amount of work has focused on designing adversarial attacks against image classifiers, only a few methods exist to attack semantic segmentation models. We show that attacking segmentation models presents task-specific challenges, for which we propose novel solutions. Our final evaluation protocol outperforms existing methods, and shows that those can overestimate the robustness of the models. Additionally, so far adversarial training, the most successful way for obtaining robust image classifiers, could not be successfully applied to semantic segmentation. We argue that this is because the task to be learned is more challenging, and requires significantly higher computational effort than for image classification. As a remedy, we show that by taking advantage of recent advances in robust ImageNet classifiers, one can train adversarially robust segmentation models at limited computational cost by fine-tuning robust backbones.



### An overview on the evaluated video retrieval tasks at TRECVID 2022
- **Arxiv ID**: http://arxiv.org/abs/2306.13118v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2306.13118v1)
- **Published**: 2023-06-22 15:15:13+00:00
- **Updated**: 2023-06-22 15:15:13+00:00
- **Authors**: George Awad, Keith Curtis, Asad Butt, Jonathan Fiscus, Afzal Godil, Yooyoung Lee, Andrew Delgado, Eliot Godard, Lukas Diduch, Jeffrey Liu, Yvette Graham, Georges Quenot
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2104.13473,
  arXiv:2009.09984
- **Journal**: None
- **Summary**: The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, tasks-based evaluation supported by metrology. Over the last twenty-one years this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals worldwide contribute significant time and effort. TRECVID 2022 planned for the following six tasks: Ad-hoc video search, Video to text captioning, Disaster scene description and indexing, Activity in extended videos, deep video understanding, and movie summarization. In total, 35 teams from various research organizations worldwide signed up to join the evaluation campaign this year. This paper introduces the tasks, datasets used, evaluation frameworks and metrics, as well as a high-level results overview.



### Towards More Realistic Membership Inference Attacks on Large Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2306.12983v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.12983v1)
- **Published**: 2023-06-22 15:41:15+00:00
- **Updated**: 2023-06-22 15:41:15+00:00
- **Authors**: Jan Dubiski, Antoni Kowalczuk, Stanisaw Pawlak, Przemysaw Rokita, Tomasz Trzciski, Pawe Morawiecki
- **Comment**: None
- **Journal**: None
- **Summary**: Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred to as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a methodology to establish a fair evaluation setup and apply it to Stable Diffusion, enabling potential extensions to other generative models. Utilizing this evaluation setup, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.



### Can a single image processing algorithm work equally well across all phases of DCE-MRI?
- **Arxiv ID**: http://arxiv.org/abs/2306.12988v1
- **DOI**: 10.1117/12.2654228
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.12988v1)
- **Published**: 2023-06-22 15:44:39+00:00
- **Updated**: 2023-06-22 15:44:39+00:00
- **Authors**: Adam G. Tattersall, Keith A. Goatman, Lucy E. Kershaw, Scott I. K. Semple, Sonia Dahdouh
- **Comment**: None
- **Journal**: Proc. SPIE 12464, Medical Imaging 2023: Image Processing, 124642X
  (3 April 2023)
- **Summary**: Image segmentation and registration are said to be challenging when applied to dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causes rapid changes in intensity in the region of interest and elsewhere, which can lead to false positive predictions for segmentation tasks and confound the image registration similarity metric. While it is widely assumed that contrast changes increase the difficulty of these tasks, to our knowledge no work has quantified these effects. In this paper we examine the effect of training with different ratios of contrast enhanced (CE) data on two popular tasks: segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph and VTN. We experimented further by strategically using the available datasets through pretraining and fine tuning with different splits of data. We found that to create a generalisable model, pretraining with CE data and fine tuning with non-CE data gave the best result. This interesting find could be expanded to other deep learning based image processing tasks with DCE-MRI and provide significant improvements to the models performance.



### Minimalist and High-Quality Panoramic Imaging with PSF-aware Transformers
- **Arxiv ID**: http://arxiv.org/abs/2306.12992v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2306.12992v1)
- **Published**: 2023-06-22 15:47:58+00:00
- **Updated**: 2023-06-22 15:47:58+00:00
- **Authors**: Qi Jiang, Shaohua Gao, Yao Gao, Kailun Yang, Zhonghua Yi, Hao Shi, Lei Sun, Kaiwei Wang
- **Comment**: The dataset and code will be available at
  https://github.com/zju-jiangqi/PCIE-PART
- **Journal**: None
- **Summary**: High-quality panoramic images with a Field of View (FoV) of 360-degree are essential for contemporary panoramic computer vision tasks. However, conventional imaging systems come with sophisticated lens designs and heavy optical components. This disqualifies their usage in many mobile and wearable applications where thin and portable, minimalist imaging systems are desired. In this paper, we propose a Panoramic Computational Imaging Engine (PCIE) to address minimalist and high-quality panoramic imaging. With less than three spherical lenses, a Minimalist Panoramic Imaging Prototype (MPIP) is constructed based on the design of the Panoramic Annular Lens (PAL), but with low-quality imaging results due to aberrations and small image plane size. We propose two pipelines, i.e. Aberration Correction (AC) and Super-Resolution and Aberration Correction (SR&AC), to solve the image quality problems of MPIP, with imaging sensors of small and large pixel size, respectively. To provide a universal network for the two pipelines, we leverage the information from the Point Spread Function (PSF) of the optical system and design a PSF-aware Aberration-image Recovery Transformer (PART), in which the self-attention calculation and feature extraction are guided via PSF-aware mechanisms. We train PART on synthetic image pairs from simulation and put forward the PALHQ dataset to fill the gap of real-world high-quality PAL images for low-level vision. A comprehensive variety of experiments on synthetic and real-world benchmarks demonstrates the impressive imaging results of PCIE and the effectiveness of plug-and-play PSF-aware mechanisms. We further deliver heuristic experimental findings for minimalist and high-quality panoramic imaging. Our dataset and code will be available at https://github.com/zju-jiangqi/PCIE-PART.



### Affine Correspondences between Multi-Camera Systems for Relative Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2306.12996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.12996v1)
- **Published**: 2023-06-22 15:52:48+00:00
- **Updated**: 2023-06-22 15:52:48+00:00
- **Authors**: Banglei Guan, Ji Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method to compute the relative pose of multi-camera systems using two affine correspondences (ACs). Existing solutions to the multi-camera relative pose estimation are either restricted to special cases of motion, have too high computational complexity, or require too many point correspondences (PCs). Thus, these solvers impede an efficient or accurate relative pose estimation when applying RANSAC as a robust estimator. This paper shows that the 6DOF relative pose estimation problem using ACs permits a feasible minimal solution, when exploiting the geometric constraints between ACs and multi-camera systems using a special parameterization. We present a problem formulation based on two ACs that encompass two common types of ACs across two views, i.e., inter-camera and intra-camera. Moreover, the framework for generating the minimal solvers can be extended to solve various relative pose estimation problems, e.g., 5DOF relative pose estimation with known rotation angle prior. Experiments on both virtual and real multi-camera systems prove that the proposed solvers are more efficient than the state-of-the-art algorithms, while resulting in a better relative pose accuracy. Source code is available at https://github.com/jizhaox/relpose-mcs-depth.



### Toward Automated Detection of Microbleeds with Anatomical Scale Localization: A Complete Clinical Diagnosis Support Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.13020v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13020v1)
- **Published**: 2023-06-22 16:29:46+00:00
- **Updated**: 2023-06-22 16:29:46+00:00
- **Authors**: Jun-Ho Kim, Young Noh, Haejoon Lee, Seul Lee, Woo-Ram Kim, Koung Mi Kang, Eung Yeop Kim, Mohammed A. Al-masni, Dong-Hyun Kim
- **Comment**: 16 pages, 10 figures,3 tables
- **Journal**: None
- **Summary**: Cerebral Microbleeds (CMBs) are chronic deposits of small blood products in the brain tissues, which have explicit relation to various cerebrovascular diseases depending on their anatomical location, including cognitive decline, intracerebral hemorrhage, and cerebral infarction. However, manual detection of CMBs is a time-consuming and error-prone process because of their sparse and tiny structural properties. The detection of CMBs is commonly affected by the presence of many CMB mimics that cause a high false-positive rate (FPR), such as calcification and pial vessels. This paper proposes a novel 3D deep learning framework that does not only detect CMBs but also inform their anatomical location in the brain (i.e., lobar, deep, and infratentorial regions). For the CMB detection task, we propose a single end-to-end model by leveraging the U-Net as a backbone with Region Proposal Network (RPN). To significantly reduce the FPs within the same single model, we develop a new scheme, containing Feature Fusion Module (FFM) that detects small candidates utilizing contextual information and Hard Sample Prototype Learning (HSPL) that mines CMB mimics and generates additional loss term called concentration loss using Convolutional Prototype Learning (CPL). The anatomical localization task does not only tell to which region the CMBs belong but also eliminate some FPs from the detection task by utilizing anatomical information. The results show that the proposed RPN that utilizes the FFM and HSPL outperforms the vanilla RPN and achieves a sensitivity of 94.66% vs. 93.33% and an average number of false positives per subject (FPavg) of 0.86 vs. 14.73. Also, the anatomical localization task further improves the detection performance by reducing the FPavg to 0.56 while maintaining the sensitivity of 94.66%.



### AugDMC: Data Augmentation Guided Deep Multiple Clustering
- **Arxiv ID**: http://arxiv.org/abs/2306.13023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13023v1)
- **Published**: 2023-06-22 16:31:46+00:00
- **Updated**: 2023-06-22 16:31:46+00:00
- **Authors**: Jiawei Yao, Enbei Liu, Maham Rashid, Juhua Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering aims to group similar objects together while separating dissimilar ones apart. Thereafter, structures hidden in data can be identified to help understand data in an unsupervised manner. Traditional clustering methods such as k-means provide only a single clustering for one data set. Deep clustering methods such as auto-encoder based clustering methods have shown a better performance, but still provide a single clustering. However, a given dataset might have multiple clustering structures and each represents a unique perspective of the data. Therefore, some multiple clustering methods have been developed to discover multiple independent structures hidden in data. Although deep multiple clustering methods provide better performance, how to efficiently capture the alternative perspectives in data is still a problem. In this paper, we propose AugDMC, a novel data Augmentation guided Deep Multiple Clustering method, to tackle the challenge. Specifically, AugDMC leverages data augmentations to automatically extract features related to a certain aspect of the data using a self-supervised prototype-based representation learning, where different aspects of the data can be preserved under different data augmentations. Moreover, a stable optimization strategy is proposed to alleviate the unstable problem from different augmentations. Thereafter, multiple clusterings based on different aspects of the data can be obtained. Experimental results on three real-world datasets compared with state-of-the-art methods validate the effectiveness of the proposed method.



### Deep Metric Learning with Soft Orthogonal Proxies
- **Arxiv ID**: http://arxiv.org/abs/2306.13055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13055v1)
- **Published**: 2023-06-22 17:22:15+00:00
- **Updated**: 2023-06-22 17:22:15+00:00
- **Authors**: Farshad Saberi-Movahed, Mohammad K. Ebrahimpour, Farid Saberi-Movahed, Monireh Moshavash, Dorsa Rahmatian, Mahvash Mohazzebi, Mahdi Shariatzadeh, Mahdi Eftekhari
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) models rely on strong representations and similarity-based measures with specific loss functions. Proxy-based losses have shown great performance compared to pair-based losses in terms of convergence speed. However, proxies that are assigned to different classes may end up being closely located in the embedding space and hence having a hard time to distinguish between positive and negative items. Alternatively, they may become highly correlated and hence provide redundant information with the model. To address these issues, we propose a novel approach that introduces Soft Orthogonality (SO) constraint on proxies. The constraint ensures the proxies to be as orthogonal as possible and hence control their positions in the embedding space. Our approach leverages Data-Efficient Image Transformer (DeiT) as an encoder to extract contextual features from images along with a DML objective. The objective is made of the Proxy Anchor loss along with the SO regularization. We evaluate our method on four public benchmarks for category-level image retrieval and demonstrate its effectiveness with comprehensive experimental results and ablation studies. Our evaluations demonstrate the superiority of our proposed approach over state-of-the-art methods by a significant margin.



### The Role of Schwartz Measures in Human Tri-Color Vision
- **Arxiv ID**: http://arxiv.org/abs/2307.05377v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, math.PR
- **Links**: [PDF](http://arxiv.org/pdf/2307.05377v1)
- **Published**: 2023-06-22 17:31:14+00:00
- **Updated**: 2023-06-22 17:31:14+00:00
- **Authors**: M. L. Sloan
- **Comment**: None
- **Journal**: None
- **Summary**: The human tri-color vision process may be characterized as follows:   1. A requirement of three scalar quantities to fully define a color (for example, intensity, hue, and purity), with   2. These scalar measures linear in the intensity of the incident light, allowing in general any specific color to be duplicated by an additive mixture of light from three standardized (basis) colors,   3. The exception being that the spectral colors are unique, in that they cannot be duplicated by any positive mixture of other colors.   These characteristics strongly suggest that human color vision makes use of Schwartz measures in processing color data. This hypothesis is subject to test. In this brief paper, the results of this hypothesis are shown to be in good agreement with measured data.



### Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports
- **Arxiv ID**: http://arxiv.org/abs/2306.13074v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13074v3)
- **Published**: 2023-06-22 17:47:08+00:00
- **Updated**: 2023-07-19 03:46:37+00:00
- **Authors**: Hsiang-Wei Huang, Cheng-Yen Yang, Jiacheng Sun, Jenq-Neng Hwang, Chung-I Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking algorithms have made significant advancements due to the recent developments in object detection. However, most existing methods primarily focus on tracking pedestrians or vehicles, which exhibit relatively simple and regular motion patterns. Consequently, there is a scarcity of algorithms that address the tracking of targets with irregular or non-linear motion, such as multi-athlete tracking. Furthermore, popular tracking algorithms often rely on the Kalman filter for object motion modeling, which fails to track objects when their motion contradicts the linear motion assumption of the Kalman filter. Due to this reason, we proposed a novel online and robust multi-object tracking approach, named Iterative Scale-Up ExpansionIoU and Deep Features for multi-object tracking. Unlike conventional methods, we abandon the use of the Kalman filter and propose utilizing the iterative scale-up expansion IoU. This approach achieves superior tracking performance without requiring additional training data or adopting a more robust detector, all while maintaining a lower computational cost compared to other appearance-based methods. Our proposed method demonstrates remarkable effectiveness in tracking irregular motion objects, achieving a score of 76.9% in HOTA. It outperforms all state-of-the-art tracking algorithms on the SportsMOT dataset, covering various kinds of sport scenarios.



### Semi-automated extraction of research topics and trends from NCI funding in radiological sciences from 2000-2020
- **Arxiv ID**: http://arxiv.org/abs/2306.13075v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, 68T50 (Primary), 68T10 (Secondary), I.2.7; I.5.3; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2306.13075v1)
- **Published**: 2023-06-22 17:47:42+00:00
- **Updated**: 2023-06-22 17:47:42+00:00
- **Authors**: Mark Nguyen, Peter Beidler, Joseph Tsai, August Anderson, Daniel Chen, Paul Kinahan, John Kang
- **Comment**: Presented at the American Society of Radiation Oncology annual
  meeting in 2021 ((doi: 10.1016/j.ijrobp.2021.07.263) and the Practical Big
  Data Workshop 2022
- **Journal**: None
- **Summary**: Investigators, funders, and the public desire knowledge on topics and trends in publicly funded research but current efforts in manual categorization are limited in scale and understanding. We developed a semi-automated approach to extract and name research topics, and applied this to \$1.9B of NCI funding over 21 years in the radiological sciences to determine micro- and macro-scale research topics and funding trends. Our method relies on sequential clustering of existing biomedical-based word embeddings, naming using subject matter experts, and visualization to discover trends at a macroscopic scale above individual topics. We present results using 15 and 60 cluster topics, where we found that 2D projection of grant embeddings reveals two dominant axes: physics-biology and therapeutic-diagnostic. For our dataset, we found that funding for therapeutics- and physics-based research have outpaced diagnostics- and biology-based research, respectively. We hope these results may (1) give insight to funders on the appropriateness of their funding allocation, (2) assist investigators in contextualizing their work and explore neighboring research domains, and (3) allow the public to review where their tax dollars are being allocated.



### Continuous Layout Editing of Single Images with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2306.13078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13078v1)
- **Published**: 2023-06-22 17:51:05+00:00
- **Updated**: 2023-06-22 17:51:05+00:00
- **Authors**: Zhiyuan Zhang, Zhitong Huang, Jing Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in large-scale text-to-image diffusion models have enabled many applications in image editing. However, none of these methods have been able to edit the layout of single existing images. To address this gap, we propose the first framework for layout editing of a single image while preserving its visual properties, thus allowing for continuous editing on a single image. Our approach is achieved through two key modules. First, to preserve the characteristics of multiple objects within an image, we disentangle the concepts of different objects and embed them into separate textual tokens using a novel method called masked textual inversion. Next, we propose a training-free optimization method to perform layout control for a pre-trained diffusion model, which allows us to regenerate images with learned concepts and align them with user-specified layouts. As the first framework to edit the layout of existing images, we demonstrate that our method is effective and outperforms other baselines that were modified to support this task. Our code will be freely available for public use upon acceptance.



### PromptIR: Prompting for All-in-One Blind Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2306.13090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13090v1)
- **Published**: 2023-06-22 17:59:52+00:00
- **Updated**: 2023-06-22 17:59:52+00:00
- **Authors**: Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration involves recovering a high-quality clean image from its degraded version. Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels. This restricts their real-world application since it requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. We present a prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation. In particular, our method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. This allows our method to generalize to different degradation types and levels, while still achieving state-of-the-art results on image denoising, deraining, and dehazing. Overall, PromptIR offers a generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image. Our code and pretrained models are available here: https://github.com/va1shn9v/PromptIR



### Evading Forensic Classifiers with Attribute-Conditioned Adversarial Faces
- **Arxiv ID**: http://arxiv.org/abs/2306.13091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.13091v1)
- **Published**: 2023-06-22 17:59:55+00:00
- **Updated**: 2023-06-22 17:59:55+00:00
- **Authors**: Fahad Shamshad, Koushik Srivatsan, Karthik Nandakumar
- **Comment**: Accepted in CVPR 2023. Project page:
  https://koushiksrivats.github.io/face_attribute_attack/
- **Journal**: None
- **Summary**: The ability of generative models to produce highly realistic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been developed. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are detectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly perturb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to successfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gender, etc.). To achieve this goal, we leverage the state-of-the-art generative model StyleGAN with disentangled representations, which enables a range of modifications without leaving the manifold of natural images. We propose a framework to search for adversarial latent codes within the feature space of StyleGAN, where the search can be guided either by a text prompt or a reference image. We also propose a meta-learning based optimization strategy to achieve transferable performance on unknown target models. Extensive experiments demonstrate that the proposed approach can produce semantically manipulated adversarial fake faces, which are true to the specified attribute set and can successfully fool forensic face classifiers, while remaining undetectable by humans. Code: https://github.com/koushiksrivats/face_attribute_attack.



### Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
- **Arxiv ID**: http://arxiv.org/abs/2306.13092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.13092v1)
- **Published**: 2023-06-22 17:59:58+00:00
- **Updated**: 2023-06-22 17:59:58+00:00
- **Authors**: Zeyuan Yin, Eric Xing, Zhiqiang Shen
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (ResNet-18) faster in speed with less memory consumption of 11.6$\times$ and 6.4$\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://zeyuanyin.github.io/projects/SRe2L/.



### A Sparse Graph Formulation for Efficient Spectral Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.13166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13166v1)
- **Published**: 2023-06-22 19:06:27+00:00
- **Updated**: 2023-06-22 19:06:27+00:00
- **Authors**: Rahul Palnitkar, Jeova Farias Sales Rocha Neto
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral Clustering is one of the most traditional methods to solve segmentation problems. Based on Normalized Cuts, it aims at partitioning an image using an objective function defined by a graph. Despite their mathematical attractiveness, spectral approaches are traditionally neglected by the scientific community due to their practical issues and underperformance. In this paper, we adopt a sparse graph formulation based on the inclusion of extra nodes to a simple grid graph. While the grid encodes the pixel spatial disposition, the extra nodes account for the pixel color data. Applying the original Normalized Cuts algorithm to this graph leads to a simple and scalable method for spectral image segmentation, with an interpretable solution. Our experiments also demonstrate that our proposed methodology over performs traditional spectral algorithms for segmentation.



### Targeted Background Removal Creates Interpretable Feature Visualizations
- **Arxiv ID**: http://arxiv.org/abs/2306.13178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2306.13178v1)
- **Published**: 2023-06-22 19:39:06+00:00
- **Updated**: 2023-06-22 19:39:06+00:00
- **Authors**: Ian E. Nielsen, Erik Grundeland, Joseph Snedeker, Ghulam Rasool, Ravi P. Ramachandran
- **Comment**: None
- **Journal**: None
- **Summary**: Feature visualization is used to visualize learned features for black box machine learning models. Our approach explores an altered training process to improve interpretability of the visualizations. We argue that by using background removal techniques as a form of robust training, a network is forced to learn more human recognizable features, namely, by focusing on the main object of interest without any distractions from the background. Four different training methods were used to verify this hypothesis. The first used unmodified pictures. The second used a black background. The third utilized Gaussian noise as the background. The fourth approach employed a mix of background removed images and unmodified images. The feature visualization results show that the background removed images reveal a significant improvement over the baseline model. These new results displayed easily recognizable features from their respective classes, unlike the model trained on unmodified data.



### DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability
- **Arxiv ID**: http://arxiv.org/abs/2306.13196v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.13196v1)
- **Published**: 2023-06-22 20:40:24+00:00
- **Updated**: 2023-06-22 20:40:24+00:00
- **Authors**: Xiaolin Fang, Caelan Reed Garrett, Clemens Eppner, Toms Lozano-Prez, Leslie Pack Kaelbling, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constraint-based reasoning.



### Improving Log-Cumulant Based Estimation of Roughness Information in SAR imagery
- **Arxiv ID**: http://arxiv.org/abs/2306.13200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.13200v1)
- **Published**: 2023-06-22 20:53:39+00:00
- **Updated**: 2023-06-22 20:53:39+00:00
- **Authors**: Jeova Farias Sales Rocha Neto, Francisco Alixandre Avila Rodrigues
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) image understanding is crucial in remote sensing applications, but it is hindered by its intrinsic noise contamination, called speckle. Sophisticated statistical models, such as the $\mathcal{G}^0$ family of distributions, have been employed to SAR data and many of the current advancements in processing this imagery have been accomplished through extracting information from these models. In this paper, we propose improvements to parameter estimation in $\mathcal{G}^0$ distributions using the Method of Log-Cumulants. First, using Bayesian modeling, we construct that regularly produce reliable roughness estimates under both $\mathcal{G}^0_A$ and $\mathcal{G}^0_I$ models. Second, we make use of an approximation of the Trigamma function to compute the estimated roughness in constant time, making it considerably faster than the existing method for this task. Finally, we show how we can use this method to achieve fast and reliable SAR image understanding based on roughness information.



### Neural Network Pruning for Real-time Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.13203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13203v1)
- **Published**: 2023-06-22 21:03:50+00:00
- **Updated**: 2023-06-22 21:03:50+00:00
- **Authors**: Suman Sapkota, Pranav Poudel, Sudarshan Regmi, Bibek Panthi, Binod Bhattarai
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-assisted treatment has emerged as a viable application of medical imaging, owing to the efficacy of deep learning models. Real-time inference speed remains a key requirement for such applications to help medical personnel. Even though there generally exists a trade-off between performance and model size, impressive efforts have been made to retain near-original performance by compromising model size. Neural network pruning has emerged as an exciting area that aims to eliminate redundant parameters to make the inference faster. In this study, we show an application of neural network pruning in polyp segmentation. We compute the importance score of convolutional filters and remove the filters having the least scores, which to some value of pruning does not degrade the performance. For computing the importance score, we use the Taylor First Order (TaylorFO) approximation of the change in network output for the removal of certain filters. Specifically, we employ a gradient-normalized backpropagation for the computation of the importance score. Through experiments in the polyp datasets, we validate that our approach can significantly reduce the parameter count and FLOPs retaining similar performance.



### Directional diffusion models for graph representation learning
- **Arxiv ID**: http://arxiv.org/abs/2306.13210v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13210v1)
- **Published**: 2023-06-22 21:27:48+00:00
- **Updated**: 2023-06-22 21:27:48+00:00
- **Authors**: Run Yang, Yuling Yang, Fan Zhou, Qiang Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, diffusion models have achieved remarkable success in various domains of artificial intelligence, such as image synthesis, super-resolution, and 3D molecule generation. However, the application of diffusion models in graph learning has received relatively little attention. In this paper, we address this gap by investigating the use of diffusion models for unsupervised graph representation learning. We begin by identifying the anisotropic structures of graphs and a crucial limitation of the vanilla forward diffusion process in learning anisotropic structures. This process relies on continuously adding an isotropic Gaussian noise to the data, which may convert the anisotropic signals to noise too quickly. This rapid conversion hampers the training of denoising neural networks and impedes the acquisition of semantically meaningful representations in the reverse process. To address this challenge, we propose a new class of models called {\it directional diffusion models}. These models incorporate data-dependent, anisotropic, and directional noises in the forward diffusion process. To assess the efficacy of our proposed models, we conduct extensive experiments on 12 publicly available datasets, focusing on two distinct graph representation learning tasks. The experimental results demonstrate the superiority of our models over state-of-the-art baselines, indicating their effectiveness in capturing meaningful graph representations. Our studies not only provide valuable insights into the forward process of diffusion models but also highlight the wide-ranging potential of these models for various graph-related tasks.



### Document Image Cleaning using Budget-Aware Black-Box Approximation
- **Arxiv ID**: http://arxiv.org/abs/2306.13236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.13236v1)
- **Published**: 2023-06-22 23:07:31+00:00
- **Updated**: 2023-06-22 23:07:31+00:00
- **Authors**: Ganesh Tata, Katyani Singh, Eric Van Oeveren, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown that by approximating the behaviour of a non-differentiable black-box function using a neural network, the black-box can be integrated into a differentiable training pipeline for end-to-end training. This methodology is termed "differentiable bypass,'' and a successful application of this method involves training a document preprocessor to improve the performance of a black-box OCR engine. However, a good approximation of an OCR engine requires querying it for all samples throughout the training process, which can be computationally and financially expensive. Several zeroth-order optimization (ZO) algorithms have been proposed in black-box attack literature to find adversarial examples for a black-box model by computing its gradient in a query-efficient manner. However, the query complexity and convergence rate of such algorithms makes them infeasible for our problem. In this work, we propose two sample selection algorithms to train an OCR preprocessor with less than 10% of the original system's OCR engine queries, resulting in more than 60% reduction of the total training time without significant loss of accuracy. We also show an improvement of 4% in the word-level accuracy of a commercial OCR engine with only 2.5% of the total queries and a 32x reduction in monetary cost. Further, we propose a simple ranking technique to prune 30% of the document images from the training dataset without affecting the system's performance.



### Pruning for Better Domain Generalizability
- **Arxiv ID**: http://arxiv.org/abs/2306.13237v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.13237v1)
- **Published**: 2023-06-22 23:08:16+00:00
- **Updated**: 2023-06-22 23:08:16+00:00
- **Authors**: Xinglong Sun
- **Comment**: Accepted at ICML 2023 SCIS Workshop
- **Journal**: None
- **Summary**: In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generalizability



### Continuous Online Extrinsic Calibration of Fisheye Camera and LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2306.13240v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U99, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2306.13240v1)
- **Published**: 2023-06-22 23:16:31+00:00
- **Updated**: 2023-06-22 23:16:31+00:00
- **Authors**: Jack Borer, Jeremy Tschirner, Florian lsner, Stefan Milz
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Automated driving systems use multi-modal sensor suites to ensure the reliable, redundant and robust perception of the operating domain, for example camera and LiDAR. An accurate extrinsic calibration is required to fuse the camera and LiDAR data into a common spatial reference frame required by high-level perception functions. Over the life of the vehicle the value of the extrinsic calibration can change due physical disturbances, introducing an error into the high-level perception functions. Therefore there is a need for continuous online extrinsic calibration algorithms which can automatically update the value of the camera-LiDAR calibration during the life of the vehicle using only sensor data.   We propose using mutual information between the camera image's depth estimate, provided by commonly available monocular depth estimation networks, and the LiDAR pointcloud's geometric distance as a optimization metric for extrinsic calibration. Our method requires no calibration target, no ground truth training data and no expensive offline optimization. We demonstrate our algorithm's accuracy, precision, speed and self-diagnosis capability on the KITTI-360 data set.




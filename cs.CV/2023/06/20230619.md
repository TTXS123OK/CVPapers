# Arxiv Papers in cs.CV on 2023-06-19
### Object Topological Character Acquisition by Inductive Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.10664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10664v1)
- **Published**: 2023-06-19 01:19:37+00:00
- **Updated**: 2023-06-19 01:19:37+00:00
- **Authors**: Wei Hui, Liping Yu, Yiran Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the shape and structure of objects is undoubtedly extremely important for object recognition, but the most common pattern recognition method currently used is machine learning, which often requires a large number of training data. The problem is that this kind of object-oriented learning lacks a priori knowledge. The amount of training data and the complexity of computations are very large, and it is hard to extract explicit knowledge after learning. This is typically called "knowing how without knowing why". We adopted a method of inductive learning, hoping to derive conceptual knowledge of the shape of an object and its formal representation based on a small number of positive examples. It is clear that implementing object recognition is not based on simple physical features such as colors, edges, textures, etc., but on their common geometry, such as topologies, which are stable, persistent, and essential to recognition. In this paper, a formal representation of topological structure based on object's skeleton (RTS) was proposed and the induction process of "seeking common ground" is realized. This research helps promote the method of object recognition from empiricism to rationalism.



### Dual-view Correlation Hybrid Attention Network for Robust Holistic Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/2306.10676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.10676v1)
- **Published**: 2023-06-19 02:34:42+00:00
- **Updated**: 2023-06-19 02:34:42+00:00
- **Authors**: Zhiwei Wang, Junlin Xian, Kangyi Liu, Xin Li, Qiang Li, Xin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Mammogram image is important for breast cancer screening, and typically obtained in a dual-view form, i.e., cranio-caudal (CC) and mediolateral oblique (MLO), to provide complementary information. However, previous methods mostly learn features from the two views independently, which violates the clinical knowledge and ignores the importance of dual-view correlation. In this paper, we propose a dual-view correlation hybrid attention network (DCHA-Net) for robust holistic mammogram classification. Specifically, DCHA-Net is carefully designed to extract and reinvent deep features for the two views, and meanwhile to maximize the underlying correlations between them. A hybrid attention module, consisting of local relation and non-local attention blocks, is proposed to alleviate the spatial misalignment of the paired views in the correlation maximization. A dual-view correlation loss is introduced to maximize the feature similarity between corresponding strip-like regions with equal distance to the chest wall, motivated by the fact that their features represent the same breast tissues, and thus should be highly-correlated. Experimental results on two public datasets, i.e., INbreast and CBIS-DDSM, demonstrate that DCHA-Net can well preserve and maximize feature correlations across views, and thus outperforms the state-of-the-arts for classifying a whole mammogram as malignant or not.



### LVVC: A Learned Versatile Video Coding Framework for Efficient Human-Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/2306.10681v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10681v1)
- **Published**: 2023-06-19 03:04:57+00:00
- **Updated**: 2023-06-19 03:04:57+00:00
- **Authors**: Xihua Sheng, Li Li, Dong Liu, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Almost all digital videos are coded into compact representations before being transmitted. Such compact representations need to be decoded back to pixels before being displayed to human and - as usual - before being processed/analyzed by machine vision algorithms. For machine vision, it is more efficient at least conceptually, to process/analyze the coded representations directly without decoding them into pixels. Motivated by this concept, we propose a learned versatile video coding (LVVC) framework, which targets on learning compact representations to support both decoding and direct processing/analysis, thereby being versatile for both human and machine vision. Our LVVC framework has a feature-based compression loop, where one frame is encoded (resp. decoded) to intermediate features, and the intermediate features are referenced for encoding (resp. decoding) the following frames. Our proposed feature-based compression loop has two key technologies, one is feature-based temporal context mining, and the other is cross-domain motion encoder/decoder. With the LVVC framework, the intermediate features may be used to reconstruct videos, or be fed into different task networks. The LVVC framework is implemented and evaluated with video reconstruction, video processing, and video analysis tasks on the well-established benchmark datasets. The evaluation results demonstrate the compression efficiency of the proposed LVVC framework.



### Visually-Guided Sound Source Separation with Audio-Visual Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/2306.10684v1
- **DOI**: 10.1109/TNNLS.2023.3288022
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2306.10684v1)
- **Published**: 2023-06-19 03:10:57+00:00
- **Updated**: 2023-06-19 03:10:57+00:00
- **Authors**: Zengjie Song, Zhaoxiang Zhang
- **Comment**: Accepted to IEEE Transactions on Neural Networks and Learning Systems
  (T-NNLS)
- **Journal**: None
- **Summary**: The framework of visually-guided sound source separation generally consists of three parts: visual feature extraction, multimodal feature fusion, and sound signal processing. An ongoing trend in this field has been to tailor involved visual feature extractor for informative visual guidance and separately devise module for feature fusion, while utilizing U-Net by default for sound analysis. However, such divide-and-conquer paradigm is parameter inefficient and, meanwhile, may obtain suboptimal performance as jointly optimizing and harmonizing various model components is challengeable. By contrast, this paper presents a novel approach, dubbed audio-visual predictive coding (AVPC), to tackle this task in a parameter efficient and more effective manner. The network of AVPC features a simple ResNet-based video analysis network for deriving semantic visual features, and a predictive coding-based sound separation network that can extract audio features, fuse multimodal information, and predict sound separation masks in the same architecture. By iteratively minimizing the prediction error between features, AVPC integrates audio and visual information recursively, leading to progressively improved performance. In addition, we develop a valid self-supervised learning strategy for AVPC via co-predicting two audio-visual representations of the same sound source. Extensive evaluations demonstrate that AVPC outperforms several baselines in separating musical instrument sounds, while reducing the model size significantly. Code is available at: https://github.com/zjsong/Audio-Visual-Predictive-Coding.



### Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2306.10687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10687v1)
- **Published**: 2023-06-19 03:42:44+00:00
- **Updated**: 2023-06-19 03:42:44+00:00
- **Authors**: Chuanguang Yang, Xinqiang Yu, Zhulin An, Yongjun Xu
- **Comment**: Published at Springer book "Advancements in Knowledge Distillation:
  Towards New Horizons of Intelligent Systems"
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable performance for artificial intelligence tasks. The success behind intelligent systems often relies on large-scale models with high computational complexity and storage costs. The over-parameterized networks are often easy to optimize and can achieve better performance. However, it is challenging to deploy them over resource-limited edge-devices. Knowledge Distillation (KD) aims to optimize a lightweight network from the perspective of over-parameterized training. The traditional offline KD transfers knowledge from a cumbersome teacher to a small and fast student network. When a sizeable pre-trained teacher network is unavailable, online KD can improve a group of models by collaborative or mutual learning. Without needing extra models, Self-KD boosts the network itself using attached auxiliary architectures. KD mainly involves knowledge extraction and distillation strategies these two aspects. Beyond KD schemes, various KD algorithms are widely used in practical applications, such as multi-teacher KD, cross-modal KD, attention-based KD, data-free KD and adversarial KD. This paper provides a comprehensive KD survey, including knowledge categories, distillation schemes and algorithms, as well as some empirical studies on performance comparison. Finally, we discuss the open challenges of existing KD works and prospect the future directions.



### Realistic Restorer: artifact-free flow restorer(AF2R) for MRI motion artifact removal
- **Arxiv ID**: http://arxiv.org/abs/2306.10689v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10689v1)
- **Published**: 2023-06-19 04:02:01+00:00
- **Updated**: 2023-06-19 04:02:01+00:00
- **Authors**: Jiandong Su, Kun Shang, Dong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Motion artifact is a major challenge in magnetic resonance imaging (MRI) that severely degrades image quality, reduces examination efficiency, and makes accurate diagnosis difficult. However, previous methods often relied on implicit models for artifact correction, resulting in biases in modeling the artifact formation mechanism and characterizing the relationship between artifact information and anatomical details. These limitations have hindered the ability to obtain high-quality MR images. In this work, we incorporate the artifact generation mechanism to reestablish the relationship between artifacts and anatomical content in the image domain, highlighting the superiority of explicit models over implicit models in medical problems. Based on this, we propose a novel end-to-end image domain model called AF2R, which addresses this problem using conditional normalization flow. Specifically, we first design a feature encoder to extract anatomical features from images with motion artifacts. Then, through a series of reversible transformations using the feature-to-image flow module, we progressively obtain MR images unaffected by motion artifacts. Experimental results on simulated and real datasets demonstrate that our method achieves better performance in both quantitative and qualitative results, preserving better anatomical details.



### SeMAIL: Eliminating Distractors in Visual Imitation via Separated Models
- **Arxiv ID**: http://arxiv.org/abs/2306.10695v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10695v1)
- **Published**: 2023-06-19 04:33:44+00:00
- **Updated**: 2023-06-19 04:33:44+00:00
- **Authors**: Shenghua Wan, Yucen Wang, Minghao Shao, Ruying Chen, De-Chuan Zhan
- **Comment**: 18 pages, 7 figures
- **Journal**: None
- **Summary**: Model-based imitation learning (MBIL) is a popular reinforcement learning method that improves sample efficiency on high-dimension input sources, such as images and videos. Following the convention of MBIL research, existing algorithms are highly deceptive by task-irrelevant information, especially moving distractors in videos. To tackle this problem, we propose a new algorithm - named Separated Model-based Adversarial Imitation Learning (SeMAIL) - decoupling the environment dynamics into two parts by task-relevant dependency, which is determined by agent actions, and training separately. In this way, the agent can imagine its trajectories and imitate the expert behavior efficiently in task-relevant state space. Our method achieves near-expert performance on various visual control tasks with complex observations and the more challenging tasks with different backgrounds from expert observations.



### Frame Fusion with Vehicle Motion Prediction for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2306.10699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10699v1)
- **Published**: 2023-06-19 04:57:53+00:00
- **Updated**: 2023-06-19 04:57:53+00:00
- **Authors**: Xirui Li, Feng Wang, Naiyan Wang, Chao Ma
- **Comment**: None
- **Journal**: None
- **Summary**: In LiDAR-based 3D detection, history point clouds contain rich temporal information helpful for future prediction. In the same way, history detections should contribute to future detections. In this paper, we propose a detection enhancement method, namely FrameFusion, which improves 3D object detection results by fusing history frames. In FrameFusion, we ''forward'' history frames to the current frame and apply weighted Non-Maximum-Suppression on dense bounding boxes to obtain a fused frame with merged boxes. To ''forward'' frames, we use vehicle motion models to estimate the future pose of the bounding boxes. However, the commonly used constant velocity model fails naturally on turning vehicles, so we explore two vehicle motion models to address this issue. On Waymo Open Dataset, our FrameFusion method consistently improves the performance of various 3D detectors by about $2$ vehicle level 2 APH with negligible latency and slightly enhances the performance of the temporal fusion method MPPNet. We also conduct extensive experiments on motion model selection.



### Optical Coherence Tomography Image Enhancement via Block Hankelization and Low Rank Tensor Network Approximation
- **Arxiv ID**: http://arxiv.org/abs/2306.11750v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11750v1)
- **Published**: 2023-06-19 06:23:26+00:00
- **Updated**: 2023-06-19 06:23:26+00:00
- **Authors**: Farnaz Sedighin, Andrzej Cichocki, Hossein Rabbani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, the problem of image super-resolution for Optical Coherence Tomography (OCT) has been addressed. Due to the motion artifacts, OCT imaging is usually done with a low sampling rate and the resulting images are often noisy and have low resolution. Therefore, reconstruction of high resolution OCT images from the low resolution versions is an essential step for better OCT based diagnosis. In this paper, we propose a novel OCT super-resolution technique using Tensor Ring decomposition in the embedded space. A new tensorization method based on a block Hankelization approach with overlapped patches, called overlapped patch Hankelization, has been proposed which allows us to employ Tensor Ring decomposition. The Hankelization method enables us to better exploit the inter connection of pixels and consequently achieve better super-resolution of images. The low resolution image was first patch Hankelized and then its Tensor Ring decomposition with rank incremental has been computed. Simulation results confirm that the proposed approach is effective in OCT super-resolution.



### Exploring the Relationship between Samples and Masks for Robust Defect Localization
- **Arxiv ID**: http://arxiv.org/abs/2306.10720v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10720v3)
- **Published**: 2023-06-19 06:41:19+00:00
- **Updated**: 2023-08-30 02:47:27+00:00
- **Authors**: Jiang Lin, Yaping Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Defect detection aims to detect and localize regions out of the normal distribution.Previous approaches model normality and compare it with the input to identify defective regions, potentially limiting their generalizability.This paper proposes a one-stage framework that detects defective patterns directly without the modeling process.This ability is adopted through the joint efforts of three parties: a generative adversarial network (GAN), a newly proposed scaled pattern loss, and a dynamic masked cycle-consistent auxiliary network. Explicit information that could indicate the position of defects is intentionally excluded to avoid learning any direct mapping.Experimental results on the texture class of the challenging MVTec AD dataset show that the proposed method is 2.9\% higher than the SOTA methods in F1-Score, while substantially outperforming SOTA methods in generalizability.



### Renderers are Good Zero-Shot Representation Learners: Exploring Diffusion Latents for Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.10721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10721v1)
- **Published**: 2023-06-19 06:41:44+00:00
- **Updated**: 2023-06-19 06:41:44+00:00
- **Authors**: Michael Tang, David Shustin
- **Comment**: None
- **Journal**: None
- **Summary**: Can the latent spaces of modern generative neural rendering models serve as representations for 3D-aware discriminative visual understanding tasks? We use retrieval as a proxy for measuring the metric learning properties of the latent spaces of Shap-E, including capturing view-independence and enabling the aggregation of scene representations from the representations of individual image views, and find that Shap-E representations outperform those of the classical EfficientNet baseline representations zero-shot, and is still competitive when both methods are trained using a contrative loss. These findings give preliminary indication that 3D-based rendering and generative models can yield useful representations for discriminative tasks in our innately 3D-native world. Our code is available at \url{https://github.com/michaelwilliamtang/golden-retriever}.



### UniG3D: A Unified 3D Object Generation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2306.10730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10730v1)
- **Published**: 2023-06-19 07:03:45+00:00
- **Updated**: 2023-06-19 07:03:45+00:00
- **Authors**: Qinghong Sun, Yangguang Li, ZeXiang Liu, Xiaoshui Huang, Fenggang Liu, Xihui Liu, Wanli Ouyang, Jing Shao
- **Comment**: None
- **Journal**: None
- **Summary**: The field of generative AI has a transformative impact on various areas, including virtual reality, autonomous driving, the metaverse, gaming, and robotics. Among these applications, 3D object generation techniques are of utmost importance. This technique has unlocked fresh avenues in the realm of creating, customizing, and exploring 3D objects. However, the quality and diversity of existing 3D object generation methods are constrained by the inadequacies of existing 3D object datasets, including issues related to text quality, the incompleteness of multi-modal data representation encompassing 2D rendered images and 3D assets, as well as the size of the dataset. In order to resolve these issues, we present UniG3D, a unified 3D object generation dataset constructed by employing a universal data transformation pipeline on Objaverse and ShapeNet datasets. This pipeline converts each raw 3D model into comprehensive multi-modal data representation <text, image, point cloud, mesh> by employing rendering engines and multi-modal models. These modules ensure the richness of textual information and the comprehensiveness of data representation. Remarkably, the universality of our pipeline refers to its ability to be applied to any 3D dataset, as it only requires raw 3D data. The selection of data sources for our dataset is based on their scale and quality. Subsequently, we assess the effectiveness of our dataset by employing Point-E and SDFusion, two widely recognized methods for object generation, tailored to the prevalent 3D representations of point clouds and signed distance functions. Our dataset is available at: https://unig3d.github.io.



### WiCo: Win-win Cooperation of Bottom-up and Top-down Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.10750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2306.10750v1)
- **Published**: 2023-06-19 07:49:29+00:00
- **Updated**: 2023-06-19 07:49:29+00:00
- **Authors**: Zesen Cheng, Peng Jin, Hao Li, Kehan Li, Siheng Li, Xiangyang Ji, Chang Liu, Jie Chen
- **Comment**: Accepted to IJCAI2023
- **Journal**: None
- **Summary**: The top-down and bottom-up methods are two mainstreams of referring segmentation, while both methods have their own intrinsic weaknesses. Top-down methods are chiefly disturbed by Polar Negative (PN) errors owing to the lack of fine-grained cross-modal alignment. Bottom-up methods are mainly perturbed by Inferior Positive (IP) errors due to the lack of prior object information. Nevertheless, we discover that two types of methods are highly complementary for restraining respective weaknesses but the direct average combination leads to harmful interference. In this context, we build Win-win Cooperation (WiCo) to exploit complementary nature of two types of methods on both interaction and integration aspects for achieving a win-win improvement. For the interaction aspect, Complementary Feature Interaction (CFI) provides fine-grained information to top-down branch and introduces prior object information to bottom-up branch for complementary feature enhancement. For the integration aspect, Gaussian Scoring Integration (GSI) models the gaussian performance distributions of two branches and weightedly integrates results by sampling confident scores from the distributions. With our WiCo, several prominent top-down and bottom-up combinations achieve remarkable improvements on three common datasets with reasonable extra costs, which justifies effectiveness and generality of our method.



### A HRNet-based Rehabilitation Monitoring System
- **Arxiv ID**: http://arxiv.org/abs/2306.10756v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.10756v4)
- **Published**: 2023-06-19 08:00:28+00:00
- **Updated**: 2023-07-14 08:06:00+00:00
- **Authors**: Yi-Ching Hung, Yu-Qing Jiang, Fong-Syuan Liou, Yu-Hsuan Tsao, Zi-Cing Chiang, MIn-Te Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The rehabilitation treatment helps to heal minor sports and occupational injuries. In a traditional rehabilitation process, a therapist will assign certain actions to a patient to perform in between hospital visits, and it will rely on the patient to remember actions correctly and the schedule to perform them. Unfortunately, many patients forget to perform actions or fail to recall actions in detail. As a consequence, the rehabilitation treatment is hampered or, in the worst case, the patient may suffer from additional injury caused by performing incorrect actions. To resolve these issues, we propose a HRNet-based rehabilitation monitoring system, which can remind a patient when to perform the actions and display the actions for the patient to follow via the patient's smartphone. In addition, it helps the therapist to monitor the progress of the rehabilitation for the patient. Our system consists of an iOS app and several components at the server side. The app is in charge of displaying and collecting action videos. The server computes the similarity score between the therapist's actions and the patient's in the videos to keep track of the number of repetitions of each action. Theses stats will be shown to both of the patient and therapist. The extensive experiments show that the F1-Score of the similarity calculation is as high as 0.9 and the soft accuracy of the number of repetitions is higher than 90%.



### PowerBEV: A Powerful Yet Lightweight Framework for Instance Prediction in Bird's-Eye View
- **Arxiv ID**: http://arxiv.org/abs/2306.10761v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2306.10761v1)
- **Published**: 2023-06-19 08:11:05+00:00
- **Updated**: 2023-06-19 08:11:05+00:00
- **Authors**: Peizheng Li, Shuxiao Ding, Xieyuanli Chen, Niklas Hanselmann, Marius Cordts, Juergen Gall
- **Comment**: 12 pages, 8 figures. This paper is accepted by IJCAI2023. Peizheng Li
  and Shuxiao Ding contributed equally to this work
- **Journal**: None
- **Summary**: Accurately perceiving instances and predicting their future motion are key tasks for autonomous vehicles, enabling them to navigate safely in complex urban traffic. While bird's-eye view (BEV) representations are commonplace in perception for autonomous driving, their potential in a motion prediction setting is less explored. Existing approaches for BEV instance prediction from surround cameras rely on a multi-task auto-regressive setup coupled with complex post-processing to predict future instances in a spatio-temporally consistent manner. In this paper, we depart from this paradigm and propose an efficient novel end-to-end framework named POWERBEV, which differs in several design choices aimed at reducing the inherent redundancy in previous methods. First, rather than predicting the future in an auto-regressive fashion, POWERBEV uses a parallel, multi-scale module built from lightweight 2D convolutional networks. Second, we show that segmentation and centripetal backward flow are sufficient for prediction, simplifying previous multi-task objectives by eliminating redundant output modalities. Building on this output representation, we propose a simple, flow warping-based post-processing approach which produces more stable instance associations across time. Through this lightweight yet powerful design, POWERBEV outperforms state-of-the-art baselines on the NuScenes Dataset and poses an alternative paradigm for BEV instance prediction. We made our code publicly available at: https://github.com/EdwardLeeLPZ/PowerBEV.



### Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost
- **Arxiv ID**: http://arxiv.org/abs/2306.10765v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10765v1)
- **Published**: 2023-06-19 08:15:14+00:00
- **Updated**: 2023-06-19 08:15:14+00:00
- **Authors**: Juexiao Zhou, Xiuying Chen, Xin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Medical artificial general intelligence (AGI) is an emerging field that aims to develop systems specifically designed for medical applications that possess the ability to understand, learn, and apply knowledge across a wide range of tasks and domains. Large language models (LLMs) represent a significant step towards AGI. However, training cross-domain LLMs in the medical field poses significant challenges primarily attributed to the requirement of collecting data from diverse domains. This task becomes particularly difficult due to privacy restrictions and the scarcity of publicly available medical datasets. Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific medical LLMs with the lowest cost, and suggest a possible path to achieve medical AGI. With an increasing number of domain-specific professional multimodal LLMs in the medical field being developed, MedAGI is designed to automatically select appropriate medical models by analyzing users' questions with our novel adaptive expert selection algorithm. It offers a unified approach to existing LLMs in the medical field, eliminating the need for retraining regardless of the introduction of new models. This characteristic renders it a future-proof solution in the dynamically advancing medical domain. To showcase the resilience of MedAGI, we conducted an evaluation across three distinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis of pathology pictures. The results demonstrated that MedAGI exhibited remarkable versatility and scalability, delivering exceptional performance across diverse domains. Our code is publicly available to facilitate further research at https://github.com/JoshuaChou2018/MedAGI.



### SegT: A Novel Separated Edge-guidance Transformer Network for Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.10773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10773v1)
- **Published**: 2023-06-19 08:32:05+00:00
- **Updated**: 2023-06-19 08:32:05+00:00
- **Authors**: Feiyu Chen, Haiping Ma, Weijia Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of colonoscopic polyps is considered a fundamental step in medical image analysis and surgical interventions. Many recent studies have made improvements based on the encoder-decoder framework, which can effectively segment diverse polyps. Such improvements mainly aim to enhance local features by using global features and applying attention methods. However, relying only on the global information of the final encoder block can result in losing local regional features in the intermediate layer. In addition, determining the edges between benign regions and polyps could be a challenging task. To address the aforementioned issues, we propose a novel separated edge-guidance transformer (SegT) network that aims to build an effective polyp segmentation model. A transformer encoder that learns a more robust representation than existing CNN-based approaches was specifically applied. To determine the precise segmentation of polyps, we utilize a separated edge-guidance module consisting of separator and edge-guidance blocks. The separator block is a two-stream operator to highlight edges between the background and foreground, whereas the edge-guidance block lies behind both streams to strengthen the understanding of the edge. Lastly, an innovative cascade fusion module was used and fused the refined multi-level features. To evaluate the effectiveness of SegT, we conducted experiments with five challenging public datasets, and the proposed model achieved state-of-the-art performance.



### PartSLAM: Unsupervised Part-based Scene Modeling for Fast Succinct Map Matching
- **Arxiv ID**: http://arxiv.org/abs/2306.10782v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2306.10782v1)
- **Published**: 2023-06-19 08:54:21+00:00
- **Updated**: 2023-06-19 08:54:21+00:00
- **Authors**: Shogo Hanada, Kanji Tanaka
- **Comment**: Offprint of IROS2013 paper
- **Journal**: None
- **Summary**: In this paper, we explore the challenging 1-to-N map matching problem, which exploits a compact description of map data, to improve the scalability of map matching techniques used by various robot vision tasks. We propose a first method explicitly aimed at fast succinct map matching, which consists only of map-matching subtasks. These tasks include offline map matching attempts to find a compact part-based scene model that effectively explains each map using fewer larger parts. The tasks also include an online map matching attempt to efficiently find correspondence between the part-based maps. Our part-based scene modeling approach is unsupervised and uses common pattern discovery (CPD) between the input and known reference maps. This enables a robot to learn a compact map model without human intervention. We also present a practical implementation that uses the state-of-the-art CPD technique of randomized visual phrases (RVP) with a compact bounding box (BB) based part descriptor, which consists of keypoint and descriptor BBs. The results of our challenging map-matching experiments, which use a publicly available radish dataset, show that the proposed approach achieves successful map matching with significant speedup and a compact description of map data that is tens of times more compact. Although this paper focuses on the standard 2D point-set map and the BB-based part representation, we believe our approach is sufficiently general to be applicable to a broad range of map formats, such as the 3D point cloud map, as well as to general bounding volumes and other compact part representations.



### NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.10792v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10792v1)
- **Published**: 2023-06-19 09:11:04+00:00
- **Updated**: 2023-06-19 09:11:04+00:00
- **Authors**: Yun Yi, Haokui Zhang, Rong Xiao, Nannan Wang, Xiaoyu Wang
- **Comment**: 9 pages, 2 figures, 6 tables
- **Journal**: None
- **Summary**: As more deep learning models are being applied in real-world applications, there is a growing need for modeling and learning the representations of neural networks themselves. An efficient representation can be used to predict target attributes of networks without the need for actual training and deployment procedures, facilitating efficient network deployment and design. Recently, inspired by the success of Transformer, some Transformer-based representation learning frameworks have been proposed and achieved promising performance in handling cell-structured models. However, graph neural network (GNN) based approaches still dominate the field of learning representation for the entire network. In this paper, we revisit Transformer and compare it with GNN to analyse their different architecture characteristics. We then propose a modified Transformer-based universal neural network representation learning model NAR-Former V2. It can learn efficient representations from both cell-structured networks and entire networks. Specifically, we first take the network as a graph and design a straightforward tokenizer to encode the network into a sequence. Then, we incorporate the inductive representation learning capability of GNN into Transformer, enabling Transformer to generalize better when encountering unseen architecture. Additionally, we introduce a series of simple yet effective modifications to enhance the ability of the Transformer in learning representation from graph structures. Our proposed method surpasses the GNN-based method NNLP by a significant margin in latency estimation on the NNLQP dataset. Furthermore, regarding accuracy prediction on the NASBench101 and NASBench201 datasets, our method achieves highly comparable performance to other state-of-the-art methods.



### ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers
- **Arxiv ID**: http://arxiv.org/abs/2306.10798v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.10798v2)
- **Published**: 2023-06-19 09:38:21+00:00
- **Updated**: 2023-06-23 17:09:10+00:00
- **Authors**: Ioannis Romanelis, Vlassis Fotis, Konstantinos Moustakas, Adrian Munteanu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.



### SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces
- **Arxiv ID**: http://arxiv.org/abs/2306.10799v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10799v2)
- **Published**: 2023-06-19 09:39:10+00:00
- **Updated**: 2023-08-30 05:01:31+00:00
- **Authors**: Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, Zhaoxin Fan
- **Comment**: Accepted by ACM MM 2023
- **Journal**: None
- **Summary**: Speech-driven 3D face animation technique, extending its applications to various multimedia fields. Previous research has generated promising realistic lip movements and facial expressions from audio signals. However, traditional regression models solely driven by data face several essential problems, such as difficulties in accessing precise labels and domain gaps between different modalities, leading to unsatisfactory results lacking precision and coherence. To enhance the visual accuracy of generated lip movement while reducing the dependence on labeled data, we propose a novel framework SelfTalk, by involving self-supervision in a cross-modals network system to learn 3D talking faces. The framework constructs a network system consisting of three modules: facial animator, speech recognizer, and lip-reading interpreter. The core of SelfTalk is a commutative training diagram that facilitates compatible features exchange among audio, text, and lip shape, enabling our models to learn the intricate connection between these factors. The proposed framework leverages the knowledge learned from the lip-reading interpreter to generate more plausible lip shapes. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. We recommend watching the supplementary video.



### Conditional Text Image Generation with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2306.10804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10804v1)
- **Published**: 2023-06-19 09:44:43+00:00
- **Updated**: 2023-06-19 09:44:43+00:00
- **Authors**: Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, Cong Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Current text recognition systems, including those for handwritten scripts and scene text, have relied heavily on image synthesis and augmentation, since it is difficult to realize real-world complexity and diversity through collecting and annotating enough real text images. In this paper, we explore the problem of text image generation, by taking advantage of the powerful abilities of Diffusion Models in generating photo-realistic and diverse image samples with given conditions, and propose a method called Conditional Text Image Generation with Diffusion Models (CTIG-DM for short). To conform to the characteristics of text images, we devise three conditions: image condition, text condition, and style condition, which can be used to control the attributes, contents, and styles of the samples in the image generation process. Specifically, four text image generation modes, namely: (1) synthesis mode, (2) augmentation mode, (3) recovery mode, and (4) imitation mode, can be derived by combining and configuring these three conditions. Extensive experiments on both handwritten and scene text demonstrate that the proposed CTIG-DM is able to produce image samples that simulate real-world complexity and diversity, and thus can boost the performance of existing text recognizers. Besides, CTIG-DM shows its appealing potential in domain adaptation and generating images containing Out-Of-Vocabulary (OOV) words.



### MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities
- **Arxiv ID**: http://arxiv.org/abs/2306.13674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2306.13674v1)
- **Published**: 2023-06-19 09:47:33+00:00
- **Updated**: 2023-06-19 09:47:33+00:00
- **Authors**: Hymalai Bello, Sungho Suh, Bo Zhou, Paul Lukowicz
- **Comment**: Submitted to the International Symposium on Wearable Computers (ISWC)
  2023
- **Journal**: None
- **Summary**: We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.



### Experts' cognition-driven ensemble deep learning for external validation of predicting pathological complete response to neoadjuvant chemotherapy from histological images in breast cancer
- **Arxiv ID**: http://arxiv.org/abs/2306.10805v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10805v1)
- **Published**: 2023-06-19 09:48:24+00:00
- **Updated**: 2023-06-19 09:48:24+00:00
- **Authors**: Yongquan Yang, Fengling Li, Yani Wei, Yuanyuan Zhao, Jing Fu, Xiuli Xiao, Hong Bu
- **Comment**: None
- **Journal**: None
- **Summary**: In breast cancer imaging, there has been a trend to directly predict pathological complete response (pCR) to neoadjuvant chemotherapy (NAC) from histological images based on deep learning (DL). However, it has been a commonly known problem that the constructed DL-based models numerically have better performances in internal validation than in external validation. The primary reason for this situation lies in that the distribution of the external data for validation is different from the distribution of the training data for the construction of the predictive model. In this paper, we aim to alleviate this situation with a more intrinsic approach. We propose an experts' cognition-driven ensemble deep learning (ECDEDL) approach for external validation of predicting pCR to NAC from histological images in breast cancer. The proposed ECDEDL, which takes the cognition of both pathology and artificial intelligence experts into consideration to improve the generalization of the predictive model to the external validation, more intrinsically approximates the working paradigm of a human being which will refer to his various working experiences to make decisions. The proposed ECDEDL approach was validated with 695 WSIs collected from the same center as the primary dataset to develop the predictive model and perform the internal validation, and 340 WSIs collected from other three centers as the external dataset to perform the external validation. In external validation, the proposed ECDEDL approach improves the AUCs of pCR prediction from 61.52(59.80-63.26) to 67.75(66.74-68.80) and the Accuracies of pCR prediction from 56.09(49.39-62.79) to 71.01(69.44-72.58). The proposed ECDEDL was quite effective for external validation, numerically more approximating the internal validation.



### Shape Guided Gradient Voting for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2306.10809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10809v1)
- **Published**: 2023-06-19 09:54:37+00:00
- **Updated**: 2023-06-19 09:54:37+00:00
- **Authors**: Jiaqi Xu, Yuwang Wang, Xuejin Chen
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Domain generalization aims to address the domain shift between training and testing data. To learn the domain invariant representations, the model is usually trained on multiple domains. It has been found that the gradients of network weight relative to a specific task loss can characterize the task itself. In this work, with the assumption that the gradients of a specific domain samples under the classification task could also reflect the property of the domain, we propose a Shape Guided Gradient Voting (SGGV) method for domain generalization. Firstly, we introduce shape prior via extra inputs of the network to guide gradient descending towards a shape-biased direction for better generalization. Secondly, we propose a new gradient voting strategy to remove the outliers for robust optimization in the presence of shape guidance. To provide shape guidance, we add edge/sketch extracted from the training data as an explicit way, and also use texture augmented images as an implicit way. We conduct experiments on several popular domain generalization datasets in image classification task, and show that our shape guided gradient updating strategy brings significant improvement of the generalization.



### Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions
- **Arxiv ID**: http://arxiv.org/abs/2306.10813v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.10813v2)
- **Published**: 2023-06-19 10:03:11+00:00
- **Updated**: 2023-08-16 08:02:02+00:00
- **Authors**: Yuqi Sun, Ruian He, Weimin Tan, Bo Yan
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Recent neural talking radiance field methods have shown great success in photorealistic audio-driven talking face synthesis. In this paper, we propose a novel interactive framework that utilizes human instructions to edit such implicit neural representations to achieve real-time personalized talking face generation. Given a short speech video, we first build an efficient talking radiance field, and then apply the latest conditional diffusion model for image editing based on the given instructions and guiding implicit representation optimization towards the editing target. To ensure audio-lip synchronization during the editing process, we propose an iterative dataset updating strategy and utilize a lip-edge loss to constrain changes in the lip region. We also introduce a lightweight refinement network for complementing image details and achieving controllable detail generation in the final rendered image. Our method also enables real-time rendering at up to 30FPS on consumer hardware. Multiple metrics and user verification show that our approach provides a significant improvement in rendering quality compared to state-of-the-art methods.



### Replace and Report: NLP Assisted Radiology Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2306.17180v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.17180v1)
- **Published**: 2023-06-19 10:04:42+00:00
- **Updated**: 2023-06-19 10:04:42+00:00
- **Authors**: Kaveri Kale, pushpak Bhattacharyya, Kshitij Jadhav
- **Comment**: The 61st Annual Meeting of the Association for Computational
  Linguistics
- **Journal**: None
- **Summary**: Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional image captioning approaches to generate the whole report proves to be insufficient, as these are designed to briefly describe images with short sentences. We propose a template-based approach to generate radiology reports from radiographs. Our approach involves the following: i) using a multilabel image classifier, produce the tags for the input radiograph; ii) using a transformer-based model, generate pathological descriptions (a description of abnormal findings seen on radiographs) from the tags generated in step (i); iii) using a BERT-based multi-label text classifier, find the spans in the normal report template to replace with the generated pathological descriptions; and iv) using a rule-based system, replace the identified span with the generated pathological description. We performed experiments with the two most popular radiology report datasets, IU Chest X-ray and MIMIC-CXR and demonstrated that the BLEU-1, ROUGE-L, METEOR, and CIDEr scores are better than the State-of-the-Art models by 25%, 36%, 44% and 48% respectively, on the IU X-RAY dataset. To the best of our knowledge, this is the first attempt to generate chest X-ray radiology reports by first creating small sentences for abnormal findings and then replacing them in the normal report template.



### 3D VR Sketch Guided 3D Shape Prototyping and Exploration
- **Arxiv ID**: http://arxiv.org/abs/2306.10830v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10830v4)
- **Published**: 2023-06-19 10:27:24+00:00
- **Updated**: 2023-08-01 03:22:22+00:00
- **Authors**: Ling Luo, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, Yulia Gryaditskaya
- **Comment**: Accepted by ICCV 2023
- **Journal**: None
- **Summary**: 3D shape modeling is labor-intensive, time-consuming, and requires years of expertise. To facilitate 3D shape modeling, we propose a 3D shape generation network that takes a 3D VR sketch as a condition. We assume that sketches are created by novices without art training and aim to reconstruct geometrically realistic 3D shapes of a given category. To handle potential sketch ambiguity, our method creates multiple 3D shapes that align with the original sketch's structure. We carefully design our method, training the model step-by-step and leveraging multi-modal 3D shape representation to support training with limited training data. To guarantee the realism of generated 3D shapes we leverage the normalizing flow that models the distribution of the latent space of 3D shapes. To encourage the fidelity of the generated 3D shapes to an input sketch, we propose a dedicated loss that we deploy at different stages of the training process. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.



### Road Barlow Twins: Redundancy Reduction for Road Environment Descriptors and Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2306.10840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2306.10840v1)
- **Published**: 2023-06-19 10:40:09+00:00
- **Updated**: 2023-06-19 10:40:09+00:00
- **Authors**: Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez Lopez
- **Comment**: Technical report, 14 pages, 7 figures
- **Journal**: None
- **Summary**: Anticipating the future motion of traffic agents is vital for self-driving vehicles to ensure their safe operation. We introduce a novel self-supervised pre-training method as well as a transformer model for motion prediction. Our method is based on Barlow Twins and applies the redundancy reduction principle to embeddings generated from HD maps. Additionally, we introduce a novel approach for redundancy reduction, where a potentially large and variable set of road environment tokens is transformed into a fixed-size set of road environment descriptors (RED). Our experiments reveal that the proposed pre-training method can improve minADE and minFDE by 12% and 15% and outperform contrastive learning with PreTraM and SimCLR in a semi-supervised setting. Our REDMotion model achieves results that are competitive with those of recent related methods such as MultiPath++ or Scene Transformer. Code is available at: https://github.com/kit-mrt/road-barlow-twins



### FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes
- **Arxiv ID**: http://arxiv.org/abs/2306.10858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10858v1)
- **Published**: 2023-06-19 11:21:59+00:00
- **Updated**: 2023-06-19 11:21:59+00:00
- **Authors**: Ting Zhe, Yongqian Li, Jing Zhang, Yong Luo, Han Hu, Bo Du, Yonggang Wen, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: A typical task in the field of video understanding is hand action recognition, which has a wide range of applications. Existing works either mainly focus on full-body actions, or the defined action categories are relatively coarse-grained. In this paper, we propose FHA-Kitchens, a novel dataset of fine-grained hand actions in kitchen scenes. In particular, we focus on human hand interaction regions and perform deep excavation to further refine hand action information and interaction regions. Our FHA-Kitchens dataset consists of 2,377 video clips and 30,047 images collected from 8 different types of dishes, and all hand interaction regions in each image are labeled with high-quality fine-grained action classes and bounding boxes. We represent the action information in each hand interaction region as a triplet, resulting in a total of 878 action triplets. Based on the constructed dataset, we benchmark representative action recognition and detection models on the following three tracks: (1) supervised learning for hand interaction region and object detection, (2) supervised learning for fine-grained hand action recognition, and (3) intra- and inter-class domain generalization for hand interaction region detection. The experimental results offer compelling empirical evidence that highlights the challenges inherent in fine-grained hand action recognition, while also shedding light on potential avenues for future research, particularly in relation to pre-training strategy, model design, and domain generalization. The dataset will be released at https://github.com/tingZ123/FHA-Kitchens.



### Vision Transformer with Attention Map Hallucination and FFN Compaction
- **Arxiv ID**: http://arxiv.org/abs/2306.10875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10875v1)
- **Published**: 2023-06-19 12:08:55+00:00
- **Updated**: 2023-06-19 12:08:55+00:00
- **Authors**: Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformer(ViT) is now dominating many vision tasks. The drawback of quadratic complexity of its token-wise multi-head self-attention (MHSA), is extensively addressed via either token sparsification or dimension reduction (in spatial or channel). However, the therein redundancy of MHSA is usually overlooked and so is the feed-forward network (FFN). To this end, we propose attention map hallucination and FFN compaction to fill in the blank. Specifically, we observe similar attention maps exist in vanilla ViT and propose to hallucinate half of the attention maps from the rest with much cheaper operations, which is called hallucinated-MHSA (hMHSA). As for FFN, we factorize its hidden-to-output projection matrix and leverage the re-parameterization technique to strengthen its capability, making it compact-FFN (cFFN). With our proposed modules, a 10$\%$-20$\%$ reduction of floating point operations (FLOPs) and parameters (Params) is achieved for various ViT-based backbones, including straight (DeiT), hybrid (NextViT) and hierarchical (PVT) structures, meanwhile, the performances are quite competitive.



### Handwritten Text Recognition from Crowdsourced Annotations
- **Arxiv ID**: http://arxiv.org/abs/2306.10878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.10878v1)
- **Published**: 2023-06-19 12:11:13+00:00
- **Updated**: 2023-06-19 12:11:13+00:00
- **Authors**: Solne Tarride, Tristan Faine, Mlodie Boillet, Harold Mouchre, Christopher Kermorvant
- **Comment**: Accepted to the 7th International Workshop on Historical Document
  Imaging and Processing (HIP 23)
- **Journal**: None
- **Summary**: In this paper, we explore different ways of training a model for handwritten text recognition when multiple imperfect or noisy transcriptions are available. We consider various training configurations, such as selecting a single transcription, retaining all transcriptions, or computing an aggregated transcription from all available annotations. In addition, we evaluate the impact of quality-based data selection, where samples with low agreement are removed from the training set. Our experiments are carried out on municipal registers of the city of Belfort (France) written between 1790 and 1946. % results The results show that computing a consensus transcription or training on multiple transcriptions are good alternatives. However, selecting training samples based on the degree of agreement between annotators introduces a bias in the training data and does not improve the results. Our dataset is publicly available on Zenodo: https://zenodo.org/record/8041668.



### B-cos Alignment for Inherently Interpretable CNNs and Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2306.10898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10898v1)
- **Published**: 2023-06-19 12:54:28+00:00
- **Updated**: 2023-06-19 12:54:28+00:00
- **Authors**: Moritz Bhle, Navdeeppal Singh, Mario Fritz, Bernt Schiele
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2205.10268
- **Journal**: None
- **Summary**: We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transformations in DNNs by our novel B-cos transformation. As we show, a sequence (network) of such transformations induces a single linear transformation that faithfully summarises the full model computations. Moreover, the B-cos transformation is designed such that the weights align with relevant signals during optimisation. As a result, those induced linear transformations become highly interpretable and highlight task-relevant features. Importantly, the B-cos transformation is designed to be compatible with existing architectures and we show that it can easily be integrated into virtually all of the latest state of the art models for computer vision - e.g. ResNets, DenseNets, ConvNext models, as well as Vision Transformers - by combining the B-cos-based explanations with normalisation and attention layers, all whilst maintaining similar accuracy on ImageNet. Finally, we show that the resulting explanations are of high visual quality and perform well under quantitative interpretability metrics.



### MotionGPT: Finetuned LLMs are General-Purpose Motion Generators
- **Arxiv ID**: http://arxiv.org/abs/2306.10900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.10900v1)
- **Published**: 2023-06-19 12:58:17+00:00
- **Updated**: 2023-06-19 12:58:17+00:00
- **Authors**: Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, Wanli Ouyang
- **Comment**: 18 pages, 8 figures
- **Journal**: None
- **Summary**: Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Codes shall be released upon acceptance.



### Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023)
- **Arxiv ID**: http://arxiv.org/abs/2307.06396v1
- **DOI**: 10.6084/m9.figshare.14396195
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2307.06396v1)
- **Published**: 2023-06-19 13:06:57+00:00
- **Updated**: 2023-06-19 13:06:57+00:00
- **Authors**: Seyed Muhammad Hossein Mousavi
- **Comment**: This is the second edition of the book
- **Journal**: None
- **Summary**: The book attempts to introduce a gentle introduction to the field of Facial Micro Expressions Recognition (FMER) using Color and Depth images, with the aid of MATLAB programming environment. FMER is a subset of image processing and it is a multidisciplinary topic to analysis. So, it requires familiarity with other topics of Artifactual Intelligence (AI) such as machine learning, digital image processing, psychology and more. So, it is a great opportunity to write a book which covers all of these topics for beginner to professional readers in the field of AI and even without having background of AI. Our goal is to provide a standalone introduction in the field of MFER analysis in the form of theorical descriptions for readers with no background in image processing with reproducible Matlab practical examples. Also, we describe any basic definitions for FMER analysis and MATLAB library which is used in the text, that helps final reader to apply the experiments in the real-world applications. We believe that this book is suitable for students, researchers, and professionals alike, who need to develop practical skills, along with a basic understanding of the field. We expect that, after reading this book, the reader feels comfortable with different key stages such as color and depth image processing, color and depth image representation, classification, machine learning, facial micro-expressions recognition, feature extraction and dimensionality reduction. The book attempts to introduce a gentle introduction to the field of Facial Micro Expressions Recognition (FMER) using Color and Depth images, with the aid of MATLAB programming environment.



### Fairness Index Measures to Evaluate Bias in Biometric Recognition
- **Arxiv ID**: http://arxiv.org/abs/2306.10919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.10919v1)
- **Published**: 2023-06-19 13:28:37+00:00
- **Updated**: 2023-06-19 13:28:37+00:00
- **Authors**: Ketan Kotwal, Sebastien Marcel
- **Comment**: published in International Conference on Pattern Recognition
  Workshops (2022)
- **Journal**: None
- **Summary**: The demographic disparity of biometric systems has led to serious concerns regarding their societal impact as well as applicability of such systems in private and public domains. A quantitative evaluation of demographic fairness is an important step towards understanding, assessment, and mitigation of demographic bias in biometric applications. While few, existing fairness measures are based on post-decision data (such as verification accuracy) of biometric systems, we discuss how pre-decision data (score distributions) provide useful insights towards demographic fairness. In this paper, we introduce multiple measures, based on the statistical characteristics of score distributions, for the evaluation of demographic fairness of a generic biometric verification system. We also propose different variants for each fairness measure depending on how the contribution from constituent demographic groups needs to be combined towards the final measure. In each case, the behavior of the measure has been illustrated numerically and graphically on synthetic data. The demographic imbalance in benchmarking datasets is often overlooked during fairness assessment. We provide a novel weighing strategy to reduce the effect of such imbalance through a non-linear function of sample sizes of demographic groups. The proposed measures are independent of the biometric modality, and thus, applicable across commonly used biometric modalities (e.g., face, fingerprint, etc.).



### Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2306.10921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10921v1)
- **Published**: 2023-06-19 13:32:53+00:00
- **Updated**: 2023-06-19 13:32:53+00:00
- **Authors**: Xianhui Cheng, Shoumeng Qiu, Zhikang Zou, Jian Pu, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular 3D object detection aims to locate objects in different scenes with just a single image. Due to the absence of depth information, several monocular 3D detection techniques have emerged that rely on auxiliary depth maps from the depth estimation task. There are multiple approaches to understanding the representation of depth maps, including treating them as pseudo-LiDAR point clouds, leveraging implicit end-to-end learning of depth information, or considering them as an image input. However, these methods have certain drawbacks, such as their reliance on the accuracy of estimated depth maps and suboptimal utilization of depth maps due to their image-based nature. While LiDAR-based methods and convolutional neural networks (CNNs) can be utilized for pseudo point clouds and depth maps, respectively, it is always an alternative. In this paper, we propose a framework named the Adaptive Distance Interval Separation Network (ADISN) that adopts a novel perspective on understanding depth maps, as a form that lies between LiDAR and images. We utilize an adaptive separation approach that partitions the depth map into various subgraphs based on distance and treats each of these subgraphs as an individual image for feature extraction. After adaptive separations, each subgraph solely contains pixels within a learned interval range. If there is a truncated object within this range, an evident curved edge will appear, which we can leverage for texture extraction using CNNs to obtain rich depth information in pixels. Meanwhile, to mitigate the inaccuracy of depth estimation, we designed an uncertainty module. To take advantage of both images and depth maps, we use different branches to learn localization detection tasks and appearance tasks separately.



### TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2306.10940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.10940v2)
- **Published**: 2023-06-19 14:00:34+00:00
- **Updated**: 2023-08-02 13:04:50+00:00
- **Authors**: Ioannis Prapas, Nikolaos Ioannis Bountos, Spyros Kondylatos, Dimitrios Michail, Gustau Camps-Valls, Ioannis Papoutsis
- **Comment**: Accepted at the ICCV 2023 workshop on Artificial Intelligence for
  Humanitarian Assistance and Disaster Response
- **Journal**: None
- **Summary**: Wildfires are increasingly exacerbated as a result of climate change, necessitating advanced proactive measures for effective mitigation. It is important to forecast wildfires weeks and months in advance to plan forest fuel management, resource procurement and allocation. To achieve such accurate long-term forecasts at a global scale, it is crucial to employ models that account for the Earth system's inherent spatio-temporal interactions, such as memory effects and teleconnections. We propose a teleconnection-driven vision transformer (TeleViT), capable of treating the Earth as one interconnected system, integrating fine-grained local-scale inputs with global-scale inputs, such as climate indices and coarse-grained global variables. Through comprehensive experimentation, we demonstrate the superiority of TeleViT in accurately predicting global burned area patterns for various forecasting windows, up to four months in advance. The gain is especially pronounced in larger forecasting windows, demonstrating the improved ability of deep learning models that exploit teleconnections to capture Earth system dynamics. Code available at https://github.com/Orion-Ai-Lab/TeleViT.



### Detailed retinal vessel segmentation without human annotations using simulated optical coherence tomography angiographs
- **Arxiv ID**: http://arxiv.org/abs/2306.10941v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.10941v1)
- **Published**: 2023-06-19 14:01:47+00:00
- **Updated**: 2023-06-19 14:01:47+00:00
- **Authors**: Linus Kreitner, Johannes C. Paetzold, Nikolaus Rauch, Chen Chen, Ahmed M. Hagag, Alaa E. Fayed, Sobha Sivaprasad, Sebastian Rausch, Julian Weichsel, Bjoern H. Menze, Matthias Harders, Benjamin Knier, Daniel Rueckert, Martin J. Menten
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) is a non-invasive imaging modality that can acquire high-resolution volumes of the retinal vasculature and aid the diagnosis of ocular, neurological and cardiac diseases. Segmentation of the visible blood vessels is a common first step when extracting quantitative biomarkers from these images. Classical segmentation algorithms based on thresholding are strongly affected by image artifacts and limited signal-to-noise ratio. The use of modern, deep learning-based segmentation methods has been inhibited by a lack of large datasets with detailed annotations of the blood vessels. To address this issue, recent work has employed transfer learning, where a segmentation network is trained on synthetic OCTA images and is then applied to real data. However, the previously proposed simulation models are incapable of faithfully modeling the retinal vasculature and do not provide effective domain adaptation. Because of this, current methods are not able to fully segment the retinal vasculature, in particular the smallest capillaries. In this work, we present a lightweight simulation of the retinal vascular network based on space colonization for faster and more realistic OCTA synthesis. Moreover, we introduce three contrast adaptation pipelines to decrease the domain gap between real and artificial images. We demonstrate the superior performance of our approach in extensive quantitative and qualitative experiments on three public datasets that compare our method to traditional computer vision algorithms and supervised training using human annotations. Finally, we make our entire pipeline publicly available, including the source code, pretrained models, and a large dataset of synthetic OCTA images.



### Knowledge Transfer-Driven Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.10942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.10942v1)
- **Published**: 2023-06-19 14:02:45+00:00
- **Updated**: 2023-06-19 14:02:45+00:00
- **Authors**: Ye Wang, Yaxiong Wang, Guoshuai Zhao, Xueming Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) aims to continually learn new classes using a few samples while not forgetting the old classes. The key of this task is effective knowledge transfer from the base session to the incremental sessions. Despite the advance of existing FSCIL methods, the proposed knowledge transfer learning schemes are sub-optimal due to the insufficient optimization for the model's plasticity. To address this issue, we propose a Random Episode Sampling and Augmentation (RESA) strategy that relies on diverse pseudo incremental tasks as agents to achieve the knowledge transfer. Concretely, RESA mimics the real incremental setting and constructs pseudo incremental tasks globally and locally, where the global pseudo incremental tasks are designed to coincide with the learning objective of FSCIL and the local pseudo incremental tasks are designed to improve the model's plasticity, respectively. Furthermore, to make convincing incremental predictions, we introduce a complementary model with a squared Euclidean-distance classifier as the auxiliary module, which couples with the widely used cosine classifier to form our whole architecture. By such a way, equipped with model decoupling strategy, we can maintain the model's stability while enhancing the model's plasticity. Extensive quantitative and qualitative experiments on three popular FSCIL benchmark datasets demonstrate that our proposed method, named Knowledge Transfer-driven Relation Complementation Network (KT-RCNet), outperforms almost all prior methods. More precisely, the average accuracy of our proposed KT-RCNet outperforms the second-best method by a margin of 5.26%, 3.49%, and 2.25% on miniImageNet, CIFAR100, and CUB200, respectively. Our code is available at https://github.com/YeZiLaiXi/KT-RCNet.git.



### Semi-Supervised Learning for hyperspectral images by non parametrically predicting view assignment
- **Arxiv ID**: http://arxiv.org/abs/2306.10955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10955v1)
- **Published**: 2023-06-19 14:13:56+00:00
- **Updated**: 2023-06-19 14:13:56+00:00
- **Authors**: Shivam Pande, Nassim Ait Ali Braham, Yi Wang, Conrad M Albrecht, Biplab Banerjee, Xiao Xiang Zhu
- **Comment**: The paper was submitted in IGARSS, 2023 conference and is not
  accepted to appear in the proceedings. The page requirement is 4 pages,
  including references
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification is gaining a lot of momentum in present time because of high inherent spectral information within the images. However, these images suffer from the problem of curse of dimensionality and usually require a large number samples for tasks such as classification, especially in supervised setting. Recently, to effectively train the deep learning models with minimal labelled samples, the unlabeled samples are also being leveraged in self-supervised and semi-supervised setting. In this work, we leverage the idea of semi-supervised learning to assist the discriminative self-supervised pretraining of the models. The proposed method takes different augmented views of the unlabeled samples as input and assigns them the same pseudo-label corresponding to the labelled sample from the downstream task. We train our model on two HSI datasets, namely Houston dataset (from data fusion contest, 2013) and Pavia university dataset, and show that the proposed approach performs better than self-supervised approach and supervised training.



### RaViTT: Random Vision Transformer Tokens
- **Arxiv ID**: http://arxiv.org/abs/2306.10959v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2306.10959v1)
- **Published**: 2023-06-19 14:24:59+00:00
- **Updated**: 2023-06-19 14:24:59+00:00
- **Authors**: Felipe A. Quezada, Carlos F. Navarro, Cristian Muoz, Manuel Zamorano, Jorge Jara-Wilde, Violeta Chang, Cristbal A. Navarro, Mauricio Cerda
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have successfully been applied to image classification problems where large annotated datasets are available. On the other hand, when fewer annotations are available, such as in biomedical applications, image augmentation techniques like introducing image variations or combinations have been proposed. However, regarding ViT patch sampling, less has been explored outside grid-based strategies. In this work, we propose Random Vision Transformer Tokens (RaViTT), a random patch sampling strategy that can be incorporated into existing ViTs. We experimentally evaluated RaViTT for image classification, comparing it with a baseline ViT and state-of-the-art (SOTA) augmentation techniques in 4 datasets, including ImageNet-1k and CIFAR-100. Results show that RaViTT increases the accuracy of the baseline in all datasets and outperforms the SOTA augmentation techniques in 3 out of 4 datasets by a significant margin +1.23% to +4.32%. Interestingly, RaViTT accuracy improvements can be achieved even with fewer tokens, thus reducing the computational load of any ViT model for a given accuracy value.



### Eigenpatches -- Adversarial Patches from Principal Components
- **Arxiv ID**: http://arxiv.org/abs/2306.10963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10963v1)
- **Published**: 2023-06-19 14:27:07+00:00
- **Updated**: 2023-06-19 14:27:07+00:00
- **Authors**: Jens Bayer, Stefan Becker, David Mnch, Michael Arens
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial patches are still a simple yet powerful white box attack that can be used to fool object detectors by suppressing possible detections. The patches of these so-called evasion attacks are computational expensive to produce and require full access to the attacked detector. This paper addresses the problem of computational expensiveness by analyzing 375 generated patches, calculating the principal components of these and show, that linear combinations of the resulting "eigenpatches" can be used to fool object detections successfully.



### Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2306.11754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.11754v1)
- **Published**: 2023-06-19 14:35:28+00:00
- **Updated**: 2023-06-19 14:35:28+00:00
- **Authors**: Kamil Adamczewski, Yingchen He, Mijung Park
- **Comment**: arXiv admin note: text overlap with arXiv:2303.04612
- **Journal**: None
- **Summary**: Scalability is a significant challenge when it comes to applying differential privacy to training deep neural networks. The commonly used DP-SGD algorithm struggles to maintain a high level of privacy protection while achieving high accuracy on even moderately sized models. To tackle this challenge, we take advantage of the fact that neural networks are overparameterized, which allows us to improve neural network training with differential privacy. Specifically, we introduce a new training paradigm that uses \textit{pre-pruning} and \textit{gradient-dropping} to reduce the parameter space and improve scalability. The process starts with pre-pruning the parameters of the original network to obtain a smaller model that is then trained with DP-SGD. During training, less important gradients are dropped, and only selected gradients are updated. Our training paradigm introduces a tension between the rates of pre-pruning and gradient-dropping, privacy loss, and classification accuracy. Too much pre-pruning and gradient-dropping reduces the model's capacity and worsens accuracy, while training a smaller model requires less privacy budget for achieving good accuracy. We evaluate the interplay between these factors and demonstrate the effectiveness of our training paradigm for both training from scratch and fine-tuning pre-trained networks on several benchmark image classification datasets. The tools can also be readily incorporated into existing training paradigms.



### Tame a Wild Camera: In-the-Wild Monocular Camera Calibration
- **Arxiv ID**: http://arxiv.org/abs/2306.10988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.10988v1)
- **Published**: 2023-06-19 14:55:26+00:00
- **Updated**: 2023-06-19 14:55:26+00:00
- **Authors**: Shengjie Zhu, Abhinav Kumar, Masa Hu, Xiaoming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D sensing for monocular in-the-wild images, e.g., depth estimation and 3D object detection, has become increasingly important. However, the unknown intrinsic parameter hinders their development and deployment. Previous methods for the monocular camera calibration rely on specific 3D objects or strong geometry prior, such as using a checkerboard or imposing a Manhattan World assumption. This work solves the problem from the other perspective by exploiting the monocular 3D prior. Our method is assumption-free and calibrates the complete $4$ Degree-of-Freedom (DoF) intrinsic parameters. First, we demonstrate intrinsic is solved from two well-studied monocular priors, i.e., monocular depthmap, and surface normal map. However, this solution imposes a low-bias and low-variance requirement for depth estimation. Alternatively, we introduce a novel monocular 3D prior, the incidence field, defined as the incidence rays between points in 3D space and pixels in the 2D imaging plane. The incidence field is a pixel-wise parametrization of the intrinsic invariant to image cropping and resizing. With the estimated incidence field, a robust RANSAC algorithm recovers intrinsic. We demonstrate the effectiveness of our method by showing superior performance on synthetic and zero-shot testing datasets. Beyond calibration, we demonstrate downstream applications in image manipulation detection & restoration, uncalibrated two-view pose estimation, and 3D sensing. Codes, models, and data will be held in https://github.com/ShngJZ/WildCamera.



### PINQI: An End-to-End Physics-Informed Approach to Learned Quantitative MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2306.11023v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2306.11023v1)
- **Published**: 2023-06-19 15:37:53+00:00
- **Updated**: 2023-06-19 15:37:53+00:00
- **Authors**: Felix F Zimmermann, Christoph Kolbitsch, Patrick Schuenke, Andreas Kofler
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice
- **Journal**: None
- **Summary**: Quantitative Magnetic Resonance Imaging (qMRI) enables the reproducible measurement of biophysical parameters in tissue. The challenge lies in solving a nonlinear, ill-posed inverse problem to obtain the desired tissue parameter maps from acquired raw data. While various learned and non-learned approaches have been proposed, the existing learned methods fail to fully exploit the prior knowledge about the underlying MR physics, i.e. the signal model and the acquisition model. In this paper, we propose PINQI, a novel qMRI reconstruction method that integrates the knowledge about the signal, acquisition model, and learned regularization into a single end-to-end trainable neural network. Our approach is based on unrolled alternating optimization, utilizing differentiable optimization blocks to solve inner linear and non-linear optimization tasks, as well as convolutional layers for regularization of the intermediate qualitative images and parameter maps. This design enables PINQI to leverage the advantages of both the signal model and learned regularization. We evaluate the performance of our proposed network by comparing it with recently published approaches in the context of highly undersampled $T_1$-mapping, using both a simulated brain dataset, as well as real scanner data acquired from a physical phantom and in-vivo data from healthy volunteers. The results demonstrate the superiority of our proposed solution over existing methods and highlight the effectiveness of our method in real-world scenarios.



### RemoteCLIP: A Vision Language Foundation Model for Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2306.11029v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.11029v2)
- **Published**: 2023-06-19 15:46:41+00:00
- **Updated**: 2023-08-10 02:05:45+00:00
- **Authors**: Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Jun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: General-purpose foundation models have become increasingly important in the field of artificial intelligence. While self-supervised learning (SSL) and Masked Image Modeling (MIM) have led to promising results in building such foundation models for remote sensing, these models primarily learn low-level features, require annotated data for fine-tuning, and not applicable for retrieval and zero-shot applications due to the lack of language understanding. In response to these limitations, we propose RemoteCLIP, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics, as well as aligned text embeddings for seamless downstream application. To address the scarcity of pre-training data, we leverage data scaling, converting heterogeneous annotations based on Box-to-Caption (B2C) and Mask-to-Box (M2B) conversion, and further incorporating UAV imagery, resulting a 12xlarger pretraining dataset. RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot image classification, linear probing, k-NN classification, few-shot classification, image-text retrieval, and object counting. Evaluations on 16 datasets, including a newly introduced RemoteCount benchmark to test the object counting ability, show that RemoteCLIP consistently outperforms baseline foundation models across different model scales. Impressively, RemoteCLIP outperform previous SoTA by 9.14% mean recall on RSICD dataset and by 8.92% on RSICD dataset. For zero-shot classification, our RemoteCLIP outperform CLIP baseline by up to 6.39% average accuracy on 12 downstream datasets.Pretrained models is available at https://github.com/ChenDelong1999/RemoteCLIP .



### Learning-based sound speed reconstruction and aberration correction in linear-array photoacoustic/ultrasound imaging
- **Arxiv ID**: http://arxiv.org/abs/2306.11034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11034v1)
- **Published**: 2023-06-19 15:58:10+00:00
- **Updated**: 2023-06-19 15:58:10+00:00
- **Authors**: Mengjie Shi, Tom Vercauteren, Wenfeng Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Photoacoustic (PA) image reconstruction involves acoustic inversion that necessitates the specification of the speed of sound (SoS) within the medium of propagation. Due to the lack of information on the spatial distribution of the SoS within heterogeneous soft tissue, a homogeneous SoS distribution (such as 1540 m/s) is typically assumed in PA image reconstruction, similar to that of ultrasound (US) imaging. Failure to compensate the SoS variations leads to aberration artefacts, deteriorating the image quality. In this work, we developed a deep learning framework for SoS reconstruction and subsequent aberration correction in a dual-modal PA/US imaging system sharing a clinical US probe. As the PA and US data were inherently co-registered, the reconstructed SoS distribution from US channel data using deep neural networks was utilised for accurate PA image reconstruction. On a numerical and a tissue-mimicking phantom, this framework was able to significantly suppress US aberration artefacts, with the structural similarity index measure (SSIM) of up to 0.8109 and 0.8128 as compared to the conventional approach (0.6096 and 0.5985, respectively). The networks, trained only on simulated US data, also demonstrated a good generalisation ability on data from ex vivo tissues and the wrist and fingers of healthy human volunteers, and thus could be valuable in various in vivo applications to enhance PA image reconstruction.



### FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2306.11046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.11046v1)
- **Published**: 2023-06-19 16:18:14+00:00
- **Updated**: 2023-06-19 16:18:14+00:00
- **Authors**: Jingwen Guo, Hong Liu, Shitong Sun, Tianyu Guo, Min Zhang, Chenyang Si
- **Comment**: None
- **Journal**: None
- **Summary**: Existing skeleton-based action recognition methods typically follow a centralized learning paradigm, which can pose privacy concerns when exposing human-related videos. Federated Learning (FL) has attracted much attention due to its outstanding advantages in privacy-preserving. However, directly applying FL approaches to skeleton videos suffers from unstable training. In this paper, we investigate and discover that the heterogeneous human topology graph structure is the crucial factor hindering training stability. To address this limitation, we pioneer a novel Federated Skeleton-based Action Recognition (FSAR) paradigm, which enables the construction of a globally generalized model without accessing local sensitive data. Specifically, we introduce an Adaptive Topology Structure (ATS), separating generalization and personalization by learning a domain-invariant topology shared across clients and a domain-specific topology decoupled from global model aggregation.Furthermore, we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancy between clients and server caused by distinct updating patterns through aligning shallow block-wise motion features. Extensive experiments on multiple datasets demonstrate that FSAR outperforms state-of-the-art FL-based methods while inherently protecting privacy.



### UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM
- **Arxiv ID**: http://arxiv.org/abs/2306.11048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.11048v1)
- **Published**: 2023-06-19 16:26:25+00:00
- **Updated**: 2023-06-19 16:26:25+00:00
- **Authors**: Erik Sandstrm, Kevin Ta, Luc Van Gool, Martin R. Oswald
- **Comment**: 12 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: We present an uncertainty learning framework for dense neural simultaneous localization and mapping (SLAM). Estimating pixel-wise uncertainties for the depth input of dense SLAM methods allows to re-weigh the tracking and mapping losses towards image regions that contain more suitable information that is more reliable for SLAM. To this end, we propose an online framework for sensor uncertainty estimation that can be trained in a self-supervised manner from only 2D input data. We further discuss the advantages of the uncertainty learning for the case of multi-sensor input. Extensive analysis, experimentation, and ablations show that our proposed modeling paradigm improves both mapping and tracking accuracy and often performs better than alternatives that require ground truth depth or 3D. Our experiments show that we achieve a 38% and 27% lower absolute trajectory tracking error (ATE) on the 7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset on two types of depth sensors we report an 11% F1-score improvement on RGBD SLAM compared to the recent state-of-the-art neural implicit approaches. Our source code will be made available.



### Concavity-Induced Distance for Unoriented Point Cloud Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2306.11051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2306.11051v1)
- **Published**: 2023-06-19 16:35:09+00:00
- **Updated**: 2023-06-19 16:35:09+00:00
- **Authors**: Ruoyu Wang, Yanfei Xue, Bharath Surianarayanan, Dong Tian, Chen Feng
- **Comment**: 8 pages, 8 figures, accepted by IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: We propose Concavity-induced Distance (CID) as a novel way to measure the dissimilarity between a pair of points in an unoriented point cloud. CID indicates the likelihood of two points or two sets of points belonging to different convex parts of an underlying shape represented as a point cloud. After analyzing its properties, we demonstrate how CID can benefit point cloud analysis without the need for meshing or normal estimation, which is beneficial for robotics applications when dealing with raw point cloud observations. By randomly selecting very few points for manual labeling, a CID-based point cloud instance segmentation via label propagation achieves comparable average precision as recent supervised deep learning approaches, on S3DIS and ScanNet datasets. Moreover, CID can be used to group points into approximately convex parts whose convex hulls can be used as compact scene representations in robotics, and it outperforms the baseline method in terms of grouping quality. Our project website is available at: https://ai4ce.github.io/CID/



### A spatio-temporal network for video semantic segmentation in surgical videos
- **Arxiv ID**: http://arxiv.org/abs/2306.11052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.11052v1)
- **Published**: 2023-06-19 16:36:48+00:00
- **Updated**: 2023-06-19 16:36:48+00:00
- **Authors**: Maria Grammatikopoulou, Ricardo Sanchez-Matilla, Felix Bragman, David Owen, Lucy Culshaw, Karen Kerr, Danail Stoyanov, Imanol Luengo
- **Comment**: Accepted at IPCAI 2023
- **Journal**: None
- **Summary**: Semantic segmentation in surgical videos has applications in intra-operative guidance, post-operative analytics and surgical education. Segmentation models need to provide accurate and consistent predictions since temporally inconsistent identification of anatomical structures can impair usability and hinder patient safety. Video information can alleviate these challenges leading to reliable models suitable for clinical use. We propose a novel architecture for modelling temporal relationships in videos. The proposed model includes a spatio-temporal decoder to enable video semantic segmentation by improving temporal consistency across frames. The encoder processes individual frames whilst the decoder processes a temporal batch of adjacent frames. The proposed decoder can be used on top of any segmentation encoder to improve temporal consistency. Model performance was evaluated on the CholecSeg8k dataset and a private dataset of robotic Partial Nephrectomy procedures. Segmentation performance was improved when the temporal decoder was applied across both datasets. The proposed model also displayed improvements in temporal consistency.



### Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning
- **Arxiv ID**: http://arxiv.org/abs/2306.11065v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11065v1)
- **Published**: 2023-06-19 17:00:03+00:00
- **Updated**: 2023-06-19 17:00:03+00:00
- **Authors**: Shivaen Ramshetty, Gaurav Verma, Srijan Kumar
- **Comment**: Accepted full paper at ACL 2023; 15 pages, 7 figures
- **Journal**: None
- **Summary**: The robustness of multimodal deep learning models to realistic changes in the input text is critical for their applicability to important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but do so without leveraging the cross-modal information present in multimodal data. Information from the visual modality, such as color, size, and shape, provide additional attributes that users can include in their inputs. Thus, we propose cross-modal attribute insertions as a realistic perturbation strategy for vision-and-language data that inserts visual attributes of the objects in the image into the corresponding text (e.g., "girl on a chair" to "little girl on a wooden chair"). Our proposed approach for cross-modal attribute insertions is modular, controllable, and task-agnostic. We find that augmenting input text using cross-modal insertions causes state-of-the-art approaches for text-to-image retrieval and cross-modal entailment to perform poorly, resulting in relative drops of 15% in MRR and 20% in $F_1$ score, respectively. Crowd-sourced annotations demonstrate that cross-modal insertions lead to higher quality augmentations for multimodal data than augmentations using text-only data, and are equivalent in quality to original examples. We release the code to encourage robustness evaluations of deep vision-and-language models: https://github.com/claws-lab/multimodal-robustness-xmai.



### Primitive Generation and Semantic-related Alignment for Universal Zero-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2306.11087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2306.11087v1)
- **Published**: 2023-06-19 17:59:16+00:00
- **Updated**: 2023-06-19 17:59:16+00:00
- **Authors**: Shuting He, Henghui Ding, Wei Jiang
- **Comment**: CVPR 2023, Project Page: https://henghuiding.github.io/PADing/
- **Journal**: None
- **Summary**: We study universal zero-shot segmentation in this work to achieve panoptic, instance, and semantic segmentation for novel categories without any training samples. Such zero-shot segmentation ability relies on inter-class relationships in semantic space to transfer the visual knowledge learned from seen categories to unseen ones. Thus, it is desired to well bridge semantic-visual spaces and apply the semantic relationships to visual feature learning. We introduce a generative model to synthesize features for unseen categories, which links semantic and visual spaces as well as addresses the issue of lack of unseen training data. Furthermore, to mitigate the domain gap between semantic and visual spaces, firstly, we enhance the vanilla generator with learned primitives, each of which contains fine-grained attributes related to categories, and synthesize unseen features by selectively assembling these primitives. Secondly, we propose to disentangle the visual feature into the semantic-related part and the semantic-unrelated part that contains useful visual classification clues but is less relevant to semantic representation. The inter-class relationships of semantic-related visual features are then required to be aligned with those in semantic space, thereby transferring semantic knowledge to visual feature learning. The proposed approach achieves impressively state-of-the-art performance on zero-shot panoptic segmentation, instance segmentation, and semantic segmentation. Code is available at https://henghuiding.github.io/PADing/.



### Forest Parameter Prediction by Multiobjective Deep Learning of Regression Models Trained with Pseudo-Target Imputation
- **Arxiv ID**: http://arxiv.org/abs/2306.11103v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11103v1)
- **Published**: 2023-06-19 18:10:47+00:00
- **Updated**: 2023-06-19 18:10:47+00:00
- **Authors**: Sara Bjrk, Stian N. Anfinsen, Michael Kampffmeyer, Erik Nsset, Terje Gobakken, Lennart Noordermeer
- **Comment**: Submitted to IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: In prediction of forest parameters with data from remote sensing (RS), regression models have traditionally been trained on a small sample of ground reference data. This paper proposes to impute this sample of true prediction targets with data from an existing RS-based prediction map that we consider as pseudo-targets. This substantially increases the amount of target training data and leverages the use of deep learning (DL) for semi-supervised regression modelling. We use prediction maps constructed from airborne laser scanning (ALS) data to provide accurate pseudo-targets and free data from Sentinel-1's C-band synthetic aperture radar (SAR) as regressors. A modified U-Net architecture is adapted with a selection of different training objectives. We demonstrate that when a judicious combination of loss functions is used, the semi-supervised imputation strategy produces results that surpass traditional ALS-based regression models, even though \sen data are considered as inferior for forest monitoring. These results are consistent for experiments on above-ground biomass prediction in Tanzania and stem volume prediction in Norway, representing a diversity in parameters and forest types that emphasises the robustness of the approach.



### A Lightweight Causal Model for Interpretable Subject-level Prediction
- **Arxiv ID**: http://arxiv.org/abs/2306.11107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.11107v1)
- **Published**: 2023-06-19 18:20:29+00:00
- **Updated**: 2023-06-19 18:20:29+00:00
- **Authors**: Chiara Mauri, Stefano Cerri, Oula Puonti, Mark Mhlau, Koen Van Leemput
- **Comment**: 17 pages, 13 figures
- **Journal**: None
- **Summary**: Recent years have seen a growing interest in methods for predicting a variable of interest, such as a subject's diagnosis, from medical images. Methods based on discriminative modeling excel at making accurate predictions, but are challenged in their ability to explain their decisions in anatomically meaningful terms. In this paper, we propose a simple technique for single-subject prediction that is inherently interpretable. It augments the generative models used in classical human brain mapping techniques, in which cause-effect relations can be encoded, with a multivariate noise model that captures dominant spatial correlations. Experiments demonstrate that the resulting model can be efficiently inverted to make accurate subject-level predictions, while at the same time offering intuitive causal explanations of its inner workings. The method is easy to use: training is fast for typical training set sizes, and only a single hyperparameter needs to be set by the user. Our code is available at https://github.com/chiara-mauri/Interpretable-subject-level-prediction.



### Learn to Accumulate Evidence from All Training Samples: Theory and Practice
- **Arxiv ID**: http://arxiv.org/abs/2306.11113v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11113v2)
- **Published**: 2023-06-19 18:27:12+00:00
- **Updated**: 2023-06-24 17:45:51+00:00
- **Authors**: Deep Pandey, Qi Yu
- **Comment**: ICML 2023; Analysis and improvement of evidential deep learning
  theory
- **Journal**: None
- **Summary**: Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions based on our theoretical underpinning inspires the design of a novel regularizer that effectively alleviates this fundamental limitation. Extensive experiments over many challenging real-world datasets and settings confirm our theoretical findings and demonstrate the effectiveness of our proposed approach.



### To Fold or Not to Fold: Graph Regularized Tensor Train for Visual Data Completion
- **Arxiv ID**: http://arxiv.org/abs/2306.11123v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11123v1)
- **Published**: 2023-06-19 18:54:51+00:00
- **Updated**: 2023-06-19 18:54:51+00:00
- **Authors**: Le Xu, Lei Cheng, Ngai Wong, Yik-Chung Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor train (TT) representation has achieved tremendous success in visual data completion tasks, especially when it is combined with tensor folding. However, folding an image or video tensor breaks the original data structure, leading to local information loss as nearby pixels may be assigned into different dimensions and become far away from each other. In this paper, to fully preserve the local information of the original visual data, we explore not folding the data tensor, and at the same time adopt graph information to regularize local similarity between nearby entries. To overcome the high computational complexity introduced by the graph-based regularization in the TT completion problem, we propose to break the original problem into multiple sub-problems with respect to each TT core fiber, instead of each TT core as in traditional methods. Furthermore, to avoid heavy parameter tuning, a sparsity promoting probabilistic model is built based on the generalized inverse Gaussian (GIG) prior, and an inference algorithm is derived under the mean-field approximation. Experiments on both synthetic data and real-world visual data show the superiority of the proposed methods.



### The Psychophysics of Human Three-Dimensional Active Visuospatial Problem-Solving
- **Arxiv ID**: http://arxiv.org/abs/2306.11756v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11756v1)
- **Published**: 2023-06-19 19:36:42+00:00
- **Updated**: 2023-06-19 19:36:42+00:00
- **Authors**: Markus D. Solbach, John K. Tsotsos
- **Comment**: Submitted at PNAS Nexus
- **Journal**: None
- **Summary**: Our understanding of how visual systems detect, analyze and interpret visual stimuli has advanced greatly. However, the visual systems of all animals do much more; they enable visual behaviours. How well the visual system performs while interacting with the visual environment and how vision is used in the real world have not been well studied, especially in humans. It has been suggested that comparison is the most primitive of psychophysical tasks. Thus, as a probe into these active visual behaviours, we use a same-different task: are two physical 3D objects visually the same? This task seems to be a fundamental cognitive ability. We pose this question to human subjects who are free to move about and examine two real objects in an actual 3D space. Past work has dealt solely with a 2D static version of this problem. We have collected detailed, first-of-its-kind data of humans performing a visuospatial task in hundreds of trials. Strikingly, humans are remarkably good at this task without any training, with a mean accuracy of 93.82%. No learning effect was observed on accuracy after many trials, but some effect was seen for response time, number of fixations and extent of head movement. Subjects demonstrated a variety of complex strategies involving a range of movement and eye fixation changes, suggesting that solutions were developed dynamically and tailored to the specific task.



### Deep Learning Framework with Multi-Head Dilated Encoders for Enhanced Segmentation of Cervical Cancer on Multiparametric Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2306.11137v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2306.11137v1)
- **Published**: 2023-06-19 19:41:21+00:00
- **Updated**: 2023-06-19 19:41:21+00:00
- **Authors**: Reza Kalantar, Sebastian Curcean, Jessica M Winfield, Gigin Lin, Christina Messiou, Matthew D Blackledge, Dow-Mu Koh
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: T2-weighted magnetic resonance imaging (MRI) and diffusion-weighted imaging (DWI) are essential components for cervical cancer diagnosis. However, combining these channels for training deep learning models are challenging due to misalignment of images. Here, we propose a novel multi-head framework that uses dilated convolutions and shared residual connections for separate encoding of multiparametric MRI images. We employ a residual U-Net model as a baseline, and perform a series of architectural experiments to evaluate the tumor segmentation performance based on multiparametric input channels and feature encoding configurations. All experiments were performed using a cohort including 207 patients with locally advanced cervical cancer. Our proposed multi-head model using separate dilated encoding for T2W MRI, and combined b1000 DWI and apparent diffusion coefficient (ADC) images achieved the best median Dice coefficient similarity (DSC) score, 0.823 (95% confidence interval (CI), 0.595-0.797), outperforming the conventional multi-channel model, DSC 0.788 (95% CI, 0.568-0.776), although the difference was not statistically significant (p>0.05). We investigated channel sensitivity using 3D GRAD-CAM and channel dropout, and highlighted the critical importance of T2W and ADC channels for accurate tumor segmentations. However, our results showed that b1000 DWI had a minor impact on overall segmentation performance. We demonstrated that the use of separate dilated feature extractors and independent contextual learning improved the model's ability to reduce the boundary effects and distortion of DWI, leading to improved segmentation performance. Our findings can have significant implications for the development of robust and generalizable models that can extend to other multi-modal segmentation applications.



### Graph Self-Supervised Learning for Endoscopic Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2306.11141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.11141v1)
- **Published**: 2023-06-19 19:53:41+00:00
- **Updated**: 2023-06-19 19:53:41+00:00
- **Authors**: Manel Farhat, Achraf Ben-Hamadou
- **Comment**: 35 pages, under review
- **Journal**: None
- **Summary**: Accurate feature matching and correspondence in endoscopic images play a crucial role in various clinical applications, including patient follow-up and rapid anomaly localization through panoramic image generation. However, developing robust and accurate feature matching techniques faces challenges due to the lack of discriminative texture and significant variability between patients. To address these limitations, we propose a novel self-supervised approach that combines Convolutional Neural Networks for capturing local visual appearance and attention-based Graph Neural Networks for modeling spatial relationships between key-points. Our approach is trained in a fully self-supervised scheme without the need for labeled data. Our approach outperforms state-of-the-art handcrafted and deep learning-based methods, demonstrating exceptional performance in terms of precision rate (1) and matching score (99.3%). We also provide code and materials related to this work, which can be accessed at https://github.com/abenhamadou/graph-self-supervised-learning-for-endoscopic-image-matching.



### A labeled dataset of cloud types using data from GOES-16 and CloudSat
- **Arxiv ID**: http://arxiv.org/abs/2306.11159v1
- **DOI**: 10.1109/ARGENCON55245.2022.9940053
- **Categories**: **astro-ph.EP**, cs.CV, cs.LG, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2306.11159v1)
- **Published**: 2023-06-19 20:55:37+00:00
- **Updated**: 2023-06-19 20:55:37+00:00
- **Authors**: Paula V. Romero Jure, Sergio Masuelli, Juan Bautista Cabral
- **Comment**: 6 pages, 7 figures. Submitted to 2022 IEEE Biennial Congress of
  Argentina (ARGENCON). Presented in 2022 IEEE Biennial Congress of Argentina
  (ARGENCON), San Juan, Argentina
- **Journal**: 2022 IEEE Biennial Congress of Argentina (ARGENCON), 2022, pp. 1-6
- **Summary**: In this paper we present the development of a dataset consisting of 91 Multi-band Cloud and Moisture Product Full-Disk (MCMIPF) from the Advanced Baseline Imager (ABI) on board GOES-16 geostationary satellite with 91 temporally and spatially corresponding CLDCLASS products from the CloudSat polar satellite. The products are diurnal, corresponding to the months of January and February 2019 and were chosen such that the products from both satellites can be co-located over South America. The CLDCLASS product provides the cloud type observed for each of the orbit's steps and the GOES-16 multiband images contain pixels that can be co-located with these data. We develop an algorithm that returns a product in the form of a table that provides pixels from multiband images labelled with the type of cloud observed in them. These labelled data conformed in this particular structure are very useful to perform supervised learning. This was corroborated by training a simple linear artificial neural network based on the work of Gorooh et al. (2020), which gave good results, especially for the classification of deep convective clouds.



### GD-VDM: Generated Depth for better Diffusion-based Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2306.11173v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.11173v1)
- **Published**: 2023-06-19 21:32:10+00:00
- **Updated**: 2023-06-19 21:32:10+00:00
- **Authors**: Ariel Lapid, Idan Achituve, Lior Bracha, Ethan Fetaya
- **Comment**: None
- **Journal**: None
- **Summary**: The field of generative models has recently witnessed significant progress, with diffusion models showing remarkable performance in image generation. In light of this success, there is a growing interest in exploring the application of diffusion models to other modalities. One such challenge is the generation of coherent videos of complex scenes, which poses several technical difficulties, such as capturing temporal dependencies and generating long, high-resolution videos. This paper proposes GD-VDM, a novel diffusion model for video generation, demonstrating promising results. GD-VDM is based on a two-phase generation process involving generating depth videos followed by a novel diffusion Vid2Vid model that generates a coherent real-world video. We evaluated GD-VDM on the Cityscapes dataset and found that it generates more diverse and complex scenes compared to natural baselines, demonstrating the efficacy of our approach.



### Hyperbolic Active Learning for Semantic Segmentation under Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2306.11180v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2306.11180v2)
- **Published**: 2023-06-19 22:07:20+00:00
- **Updated**: 2023-06-26 20:03:07+00:00
- **Authors**: Luca Franco, Paolo Mandica, Konstantinos Kallidromitis, Devin Guillory, Yu-Teng Li, Fabio Galasso
- **Comment**: None
- **Journal**: None
- **Summary**: For the task of semantic segmentation (SS) under domain shift, active learning (AL) acquisition strategies based on image regions and pseudo labels are state-of-the-art (SoA). The presence of diverse pseudo-labels within a region identifies pixels between different classes, which is a labeling efficient active learning data acquisition strategy. However, by design, pseudo-label variations are limited to only select the contours of classes, limiting the final AL performance. We approach AL for SS in the Poincar\'e hyperbolic ball model for the first time and leverage the variations of the radii of pixel embeddings within regions as a novel data acquisition strategy. This stems from a novel geometric property of a hyperbolic space trained without enforced hierarchies, which we experimentally prove. Namely, classes are mapped into compact hyperbolic areas with a comparable intra-class radii variance, as the model places classes of increasing explainable difficulty at denser hyperbolic areas, i.e. closer to the Poincar\'e ball edge. The variation of pixel embedding radii identifies well the class contours, but they also select a few intra-class peculiar details, which boosts the final performance. Our proposed HALO (Hyperbolic Active Learning Optimization) surpasses the supervised learning performance for the first time in AL for SS under domain shift, by only using a small portion of labels (i.e., 1%). The extensive experimental analysis is based on two established benchmarks, i.e. GTAV $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes, where we set a new SoA. The code will be released.



### AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator
- **Arxiv ID**: http://arxiv.org/abs/2306.11203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2306.11203v1)
- **Published**: 2023-06-19 23:58:07+00:00
- **Updated**: 2023-06-19 23:58:07+00:00
- **Authors**: Elysia Q. Smyers, Sydney M. Katz, Anthony L. Corso, Mykel J. Kochenderfer
- **Comment**: Submitted to the NeurIPS 2023 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: Designing robust machine learning systems remains an open problem, and there is a need for benchmark problems that cover both environmental changes and evaluation on a downstream task. In this work, we introduce AVOIDDS, a realistic object detection benchmark for the vision-based aircraft detect-and-avoid problem. We provide a labeled dataset consisting of 72,000 photorealistic images of intruder aircraft with various lighting conditions, weather conditions, relative geometries, and geographic locations. We also provide an interface that evaluates trained models on slices of this dataset to identify changes in performance with respect to changing environmental conditions. Finally, we implement a fully-integrated, closed-loop simulator of the vision-based detect-and-avoid problem to evaluate trained models with respect to the downstream collision avoidance task. This benchmark will enable further research in the design of robust machine learning systems for use in safety-critical applications. The AVOIDDS dataset and code are publicly available at $\href{https://purl.stanford.edu/hj293cv5980}{purl.stanford.edu/hj293cv5980}$ and $\href{https://github.com/sisl/VisionBasedAircraftDAA}{github.com/sisl/VisionBasedAircraftDAA}$, respectively.




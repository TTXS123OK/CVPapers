# Arxiv Papers in cs.CV on 2018-06-17
### Comparative survey of visual object classifiers
- **Arxiv ID**: http://arxiv.org/abs/1806.06321v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.06321v1)
- **Published**: 2018-06-17 01:20:32+00:00
- **Updated**: 2018-06-17 01:20:32+00:00
- **Authors**: Hiliwi Leake Kidane
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Classification of Visual Object Classes represents one of the most elaborated areas of interest in Computer Vision. It is always challenging to get one specific detector, descriptor or classifier that provides the expected object classification result. Consequently, it critical to compare the different detection, descriptor and classifier methods available and chose a single or combination of two or three to get an optimal result. In this paper, we have presented a comparative survey of different feature descriptors and classifiers. From feature descriptors, SIFT (Sparse & Dense) and HeuSIFT combination colour descriptors; From classification techniques, Support Vector Classifier, K-Nearest Neighbor, ADABOOST, and fisher are covered in comparative practical implementation survey.



### Tensor-Tensor Product Toolbox
- **Arxiv ID**: http://arxiv.org/abs/1806.07247v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.MS
- **Links**: [PDF](http://arxiv.org/pdf/1806.07247v2)
- **Published**: 2018-06-17 08:14:42+00:00
- **Updated**: 2018-06-20 03:23:18+00:00
- **Authors**: Canyi Lu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1804.03728.
  Carnegie Mellon University
- **Journal**: None
- **Summary**: The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011] is a natural generalization of matrix multiplication. Based on t-product, many operations on matrix can be extended to tensor cases, including tensor SVD, tensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many others. The linear algebraic structure of tensors are similar to the matrix cases. We develop a Matlab toolbox to implement several basic operations on tensors based on t-product. The toolbox is available at https://github.com/canyilu/tproduct.



### MedGAN: Medical Image Translation using GANs
- **Arxiv ID**: http://arxiv.org/abs/1806.06397v2
- **DOI**: 10.1016/j.compmedimag.2019.101684
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06397v2)
- **Published**: 2018-06-17 15:45:10+00:00
- **Updated**: 2019-04-04 14:34:21+00:00
- **Authors**: Karim Armanious, Chenming Jiang, Marc Fischer, Thomas Küstner, Konstantin Nikolaou, Sergios Gatidis, Bin Yang
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: Image-to-image translation is considered a new frontier in the field of medical image analysis, with numerous potential applications. However, a large portion of recent approaches offers individualized solutions based on specialized task-specific architectures or require refinement through non-end-to-end training. In this paper, we propose a new framework, named MedGAN, for medical image-to-image translation which operates on the image level in an end-to-end manner. MedGAN builds upon recent advances in the field of generative adversarial networks (GANs) by merging the adversarial framework with a new combination of non-adversarial losses. We utilize a discriminator network as a trainable feature extractor which penalizes the discrepancy between the translated medical images and the desired modalities. Moreover, style-transfer losses are utilized to match the textures and fine-structures of the desired target images to the translated images. Additionally, we present a new generator architecture, titled CasNet, which enhances the sharpness of the translated medical outputs through progressive refinement via encoder-decoder pairs. Without any application-specific modifications, we apply MedGAN on three different tasks: PET-CT translation, correction of MR motion artefacts and PET image denoising. Perceptual analysis by radiologists and quantitative evaluations illustrate that the MedGAN outperforms other existing translation approaches.



### Fast Kernelized Correlation Filters without Boundary Effect
- **Arxiv ID**: http://arxiv.org/abs/1806.06406v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06406v5)
- **Published**: 2018-06-17 16:25:35+00:00
- **Updated**: 2020-09-23 01:17:57+00:00
- **Authors**: Ming Tang, Linyu Zheng, Bin Yu, Jinqiao Wang
- **Comment**: A minor revision of its last version. 11+6 pages, 4+2 figures, 5
  tables
- **Journal**: None
- **Summary**: In recent years, correlation filter based trackers (CF trackers) have attracted much attention from the vision community because of their top performance in both localization accuracy and efficiency. The society of visual tracking, however, still needs to deal with the following difficulty on CF trackers: avoiding or eliminating the boundary effect completely, in the meantime, exploiting non-linear kernels and running efficiently. In this paper, we propose a fast kernelized correlation filter without boundary effect (nBEKCF) to solve this problem. To avoid the boundary effect thoroughly, a set of \emph{real} and \emph{dense} patches is sampled through the traditional sliding window and used as the training samples to train nBEKCF to fit a Gaussian response map. Non-linear kernels can be applied naturally in nBEKCF due to its different theoretical foundation from the existing CF trackers'. To achieve the fast training and detection, a set of cyclic bases is introduced to construct the filter. Two algorithms, ACSII and CCIM, are developed to significantly accelerate the calculation of kernel correlation matrices. ACSII and CCIM fully exploit the density of training samples and cyclic structure of bases, and totally run in space domain. The efficiency of CCIM exceeds that of the FFT counterpart remarkably in our task. Extensive experiments on six public datasets, OTB-2013, OTB-2015, NfS, VOT2018, GOT10k, and TrackingNet, show that compared to the CF trackers designed to relax the boundary effect, BACF and SRDCF, our nBEKCF achieves higher localization accuracy without tricks, in the meanwhile, runs at higher FPS.



### High-speed Tracking with Multi-kernel Correlation Filters
- **Arxiv ID**: http://arxiv.org/abs/1806.06418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06418v1)
- **Published**: 2018-06-17 17:38:15+00:00
- **Updated**: 2018-06-17 17:38:15+00:00
- **Authors**: Ming Tang, Bin Yu, Fan Zhang, Jinqiao Wang
- **Comment**: 10+3 pages, 12 figures, 1 table, accepted by CVPR2018. This version
  corrects some typos, and supplements a proof
- **Journal**: None
- **Summary**: Correlation filter (CF) based trackers are currently ranked top in terms of their performances. Nevertheless, only some of them, such as KCF~\cite{henriques15} and MKCF~\cite{tangm15}, are able to exploit the powerful discriminability of non-linear kernels. Although MKCF achieves more powerful discriminability than KCF through introducing multi-kernel learning (MKL) into KCF, its improvement over KCF is quite limited and its computational burden increases significantly in comparison with KCF. In this paper, we will introduce the MKL into KCF in a different way than MKCF. We reformulate the MKL version of CF objective function with its upper bound, alleviating the negative mutual interference of different kernels significantly. Our novel MKCF tracker, MKCFup, outperforms KCF and MKCF with large margins and can still work at very high fps. Extensive experiments on public datasets show that our method is superior to state-of-the-art algorithms for target objects of small move at very high speed.



### Learning to Evaluate Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1806.06422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.06422v1)
- **Published**: 2018-06-17 17:57:32+00:00
- **Updated**: 2018-06-17 17:57:32+00:00
- **Authors**: Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, Serge Belongie
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.



### A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal Diseases
- **Arxiv ID**: http://arxiv.org/abs/1806.06423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1806.06423v1)
- **Published**: 2018-06-17 18:22:55+00:00
- **Updated**: 2018-06-17 18:22:55+00:00
- **Authors**: C. -H. Huck Yang, Jia-Hong Huang, Fangyu Liu, Fang-Yi Chiu, Mengya Gao, Weifeng Lyu, I-Hung Lin M. D., Jesper Tegner
- **Comment**: Accepted at the Joint ICML and IJCAI Workshop on Computational
  Biology (ICML-IJCAI WCB) to be held in Stockholm SWEDEN, 2018. Referring to
  https://sites.google.com/view/wcb2018/accepted-papers?authuser=0
- **Journal**: ICML-IJCAI Workshop 2018
- **Summary**: Automatic clinical diagnosis of retinal diseases has emerged as a promising approach to facilitate discovery in areas with limited access to specialists. We propose a novel visual-assisted diagnosis hybrid model based on the support vector machine (SVM) and deep neural networks (DNNs). The model incorporates complementary strengths of DNNs and SVM. Furthermore, we present a new clinical retina label collection for ophthalmology incorporating 32 retina diseases classes. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the model performance is comparable to the professional ophthalmologists.



### The RBO Dataset of Articulated Objects and Interactions
- **Arxiv ID**: http://arxiv.org/abs/1806.06465v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.06465v1)
- **Published**: 2018-06-17 23:51:02+00:00
- **Updated**: 2018-06-17 23:51:02+00:00
- **Authors**: Roberto Martín-Martín, Clemens Eppner, Oliver Brock
- **Comment**: 6 pages; Submitted to the International Journal of Robotics Research
  (Data Paper), Sage; Equal contribution by first two authors
- **Journal**: None
- **Summary**: We present a dataset with models of 14 articulated objects commonly found in human environments and with RGB-D video sequences and wrenches recorded of human interactions with them. The 358 interaction sequences total 67 minutes of human manipulation under varying experimental conditions (type of interaction, lighting, perspective, and background). Each interaction with an object is annotated with the ground truth poses of its rigid parts and the kinematic state obtained by a motion capture system. For a subset of 78 sequences (25 minutes), we also measured the interaction wrenches. The object models contain textured three-dimensional triangle meshes of each link and their motion constraints. We provide Python scripts to download and visualize the data. The data is available at https://tu-rbo.github.io/articulated-objects/ and hosted at https://zenodo.org/record/1036660/.




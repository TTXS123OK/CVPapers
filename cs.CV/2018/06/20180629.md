# Arxiv Papers in cs.CV on 2018-06-29
### Gated Feedback Refinement Network for Coarse-to-Fine Dense Semantic Image Labeling
- **Arxiv ID**: http://arxiv.org/abs/1806.11266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11266v1)
- **Published**: 2018-06-29 04:55:45+00:00
- **Updated**: 2018-06-29 04:55:45+00:00
- **Authors**: Md Amirul Islam, Mrigank Rochan, Shujon Naha, Neil D. B. Bruce, Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Effective integration of local and global contextual information is crucial for semantic segmentation and dense image labeling. We develop two encoder-decoder based deep learning architectures to address this problem. We first propose a network architecture called Label Refinement Network (LRN) that predicts segmentation labels in a coarse-to-fine fashion at several spatial resolutions. In this network, we also define loss functions at several stages to provide supervision at different stages of training. However, there are limits to the quality of refinement possible if ambiguous information is passed forward. In order to address this issue, we also propose Gated Feedback Refinement Network (G-FRNet) that addresses this limitation. Initially, G-FRNet makes a coarse-grained prediction which it progressively refines to recover details by effectively integrating local and global contextual information during the refinement stages. This is achieved by gate units proposed in this work, that control information passed forward in order to resolve the ambiguity. Experiments were conducted on four challenging dense labeling datasets (CamVid, PASCAL VOC 2012, Horse-Cow Parsing, PASCAL-Person-Part, and SUN-RGBD). G-FRNet achieves state-of-the-art semantic segmentation results on the CamVid and Horse-Cow Parsing datasets and produces results competitive with the best performing approaches that appear in the literature for the other three datasets.



### Action Recognition for Depth Video using Multi-view Dynamic Images
- **Arxiv ID**: http://arxiv.org/abs/1806.11269v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11269v3)
- **Published**: 2018-06-29 05:27:26+00:00
- **Updated**: 2018-12-27 16:21:49+00:00
- **Authors**: Yang Xiao, Jun Chen, Yancheng Wang, Zhiguo Cao, Joey Tianyi Zhou, Xiang Bai
- **Comment**: accepted by Information Sciences
- **Journal**: None
- **Summary**: Dynamic imaging is a recently proposed action description paradigm for simultaneously capturing motion and temporal evolution information, particularly in the context of deep convolutional neural networks (CNNs). Compared with optical flow for motion characterization, dynamic imaging exhibits superior efficiency and compactness. Inspired by the success of dynamic imaging in RGB video, this study extends it to the depth domain. To better exploit three-dimensional (3D) characteristics, multi-view dynamic images are proposed. In particular, the raw depth video is densely projected with respect to different virtual imaging viewpoints by rotating the virtual camera within the 3D space. Subsequently, dynamic images are extracted from the obtained multi-view depth videos and multi-view dynamic images are thus constructed from these images. Accordingly, more view-tolerant visual cues can be involved. A novel CNN model is then proposed to perform feature learning on multi-view dynamic images. Particularly, the dynamic images from different views share the same convolutional layers but correspond to different fully connected layers. This is aimed at enhancing the tuning effectiveness on shallow convolutional layers by alleviating the gradient vanishing problem. Moreover, as the spatial occurrence variation of the actions may impair the CNN, an action proposal approach is also put forth. In experiments, the proposed approach can achieve state-of-the-art performance on three challenging datasets.



### Fast Dynamic Convolutional Neural Networks for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1807.03132v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03132v2)
- **Published**: 2018-06-29 06:30:18+00:00
- **Updated**: 2018-07-25 08:06:56+00:00
- **Authors**: Zhiyan Cui, Na Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing tracking methods based on CNN(convolutional neural networks) are too slow for real-time application despite the excellent tracking precision compared with the traditional ones. In this paper, a fast dynamic visual tracking algorithm combining CNN based MDNet(Multi-Domain Network) and RoIAlign was developed. The major problem of MDNet also lies in the time efficiency. Considering the computational complexity of MDNet is mainly caused by the large amount of convolution operations and fine-tuning of the network during tracking, a RoIPool layer which could conduct the convolution over the whole image instead of each RoI is added to accelerate the convolution and a new strategy of fine-tuning the fully-connected layers is used to accelerate the update. With RoIPool employed, the computation speed has been increased but the tracking precision has dropped simultaneously. RoIPool could lose some positioning precision because it can not handle locations represented by floating numbers. So RoIAlign, instead of RoIPool, which can process floating numbers of locations by bilinear interpolation has been added to the network. The results show the target localization precision has been improved and it hardly increases the computational cost. These strategies can accelerate the processing and make it 7x faster than MDNet with very low impact on precision and it can run at around 7 fps. The proposed algorithm has been evaluated on two benchmarks: OTB100 and VOT2016, on which high precision and speed have been obtained. The influence of the network structure and training data are also discussed with experiments.



### Robust Heartbeat Detection from Multimodal Data via CNN-based Generalizable Information Fusion
- **Arxiv ID**: http://arxiv.org/abs/1807.03232v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1807.03232v1)
- **Published**: 2018-06-29 06:47:16+00:00
- **Updated**: 2018-06-29 06:47:16+00:00
- **Authors**: B S Chandra, C S Sastry, S Jana
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: Heartbeat detection remains central to cardiac disease diagnosis and management, and is traditionally performed based on electrocardiogram (ECG). To improve robustness and accuracy of detection, especially, in certain critical-care scenarios, the use of additional physiological signals such as arterial blood pressure (BP) has recently been suggested. There, estimation of heartbeat location requires information fusion from multiple signals. However, reported efforts in this direction often obtain multimodal estimates somewhat indirectly, by voting among separately obtained signal-specific intermediate estimates. In contrast, we propose to directly fuse information from multiple signals without requiring intermediate estimates, and thence estimate heartbeat location in a robust manner. Method: We propose as a heartbeat detector, a convolutional neural network (CNN) that learns fused features from multiple physiological signals. This method eliminates the need for hand-picked signal-specific features and ad hoc fusion schemes. Further, being data-driven, the same algorithm learns suitable features from arbitrary set of signals. Results: Using ECG and BP signals of PhysioNet 2014 Challenge database, we obtained a score of 94%. Further, using two ECG channels of MIT-BIH arrhythmia database, we scored 99.92\%. Both those scores compare favourably with previously reported database-specific results. Also, our detector achieved high accuracy in a variety of clinical conditions. Conclusion: The proposed CNN-based information fusion (CIF) algorithm is generalizable, robust and efficient in detecting heartbeat location from multiple signals. Significance: In medical signal monitoring systems, our technique would accurately estimate heartbeat locations even when only a subset of channels are reliable.



### YH Technologies at ActivityNet Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/1807.00686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00686v1)
- **Published**: 2018-06-29 07:49:08+00:00
- **Updated**: 2018-06-29 07:49:08+00:00
- **Authors**: Ting Yao, Xue Li
- **Comment**: Rank 2 in both Temporal Activity Detection Task & Kinetics Task @
  ActivityNet 2018. arXiv admin note: substantial text overlap with
  arXiv:1710.08011 by other authors
- **Journal**: None
- **Summary**: This notebook paper presents an overview and comparative analysis of our systems designed for the following five tasks in ActivityNet Challenge 2018: temporal action proposals, temporal action localization, dense-captioning events in videos, trimmed action recognition, and spatio-temporal action localization.



### Excavate Condition-invariant Space by Intrinsic Encoder
- **Arxiv ID**: http://arxiv.org/abs/1806.11306v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11306v4)
- **Published**: 2018-06-29 08:58:54+00:00
- **Updated**: 2019-03-29 04:12:59+00:00
- **Authors**: Jian Xu, Chunheng Wang, Cunzhao Shi, Baihua Xiao
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: As the human, we can recognize the places across a wide range of changing environmental conditions such as those caused by weathers, seasons, and day-night cycles. We excavate and memorize the stable semantic structure of different places and scenes. For example, we can recognize tree whether the bare tree in winter or lush tree in summer. Therefore, the intrinsic features that are corresponding to specific semantic contents and condition-invariant of appearance changes can be employed to improve the performance of long-term place recognition significantly.   In this paper, we propose a novel intrinsic encoder that excavates the condition-invariant latent space of different places under drastic appearance changes. Our method excavates the space of intrinsic structure and semantic information by proposed self-supervised encoder loss. Different from previous learning based place recognition methods that need paired training data of each place with appearance changes, we employ the weakly-supervised strategy to utilize unpaired set-based training data of different environmental conditions.   We conduct comprehensive experiments and show that our semi-supervised intrinsic encoder achieves remarkable performance for place recognition under drastic appearance changes. The proposed intrinsic encoder outperforms the state-of-the-art image-level place recognition methods on standard benchmark Nordland.



### Guaranteed Deterministic Bounds on the Total Variation Distance between Univariate Mixtures
- **Arxiv ID**: http://arxiv.org/abs/1806.11311v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.11311v1)
- **Published**: 2018-06-29 09:22:41+00:00
- **Updated**: 2018-06-29 09:22:41+00:00
- **Authors**: Frank Nielsen, Ke Sun
- **Comment**: 11 pages, 2 figures
- **Journal**: None
- **Summary**: The total variation distance is a core statistical distance between probability measures that satisfies the metric axioms, with value always falling in $[0,1]$. This distance plays a fundamental role in machine learning and signal processing: It is a member of the broader class of $f$-divergences, and it is related to the probability of error in Bayesian hypothesis testing. Since the total variation distance does not admit closed-form expressions for statistical mixtures (like Gaussian mixture models), one often has to rely in practice on costly numerical integrations or on fast Monte Carlo approximations that however do not guarantee deterministic lower and upper bounds. In this work, we consider two methods for bounding the total variation of univariate mixture models: The first method is based on the information monotonicity property of the total variation to design guaranteed nested deterministic lower bounds. The second method relies on computing the geometric lower and upper envelopes of weighted mixture components to derive deterministic bounds based on density ratio. We demonstrate the tightness of our bounds in a series of experiments on Gaussian, Gamma and Rayleigh mixture models.



### Hyperspectral Image Dataset for Benchmarking on Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.11314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11314v2)
- **Published**: 2018-06-29 09:31:56+00:00
- **Updated**: 2018-07-02 01:25:04+00:00
- **Authors**: Nevrez Imamoglu, Yu Oishi, Xiaoqiang Zhang, Guanqun Ding, Yuming Fang, Toru Kouyama, Ryosuke Nakamura
- **Comment**: 3 pages, 3 figures. 2 tables, appeared in the Proceedings of the 10th
  International Conference on Quality of Multimedia Experience (QoMEX 2018)
- **Journal**: None
- **Summary**: Many works have been done on salient object detection using supervised or unsupervised approaches on colour images. Recently, a few studies demonstrated that efficient salient object detection can also be implemented by using spectral features in visible spectrum of hyperspectral images from natural scenes. However, these models on hyperspectral salient object detection were tested with a very few number of data selected from various online public dataset, which are not specifically created for object detection purposes. Therefore, here, we aim to contribute to the field by releasing a hyperspectral salient object detection dataset with a collection of 60 hyperspectral images with their respective ground-truth binary images and representative rendered colour images (sRGB). We took several aspects in consideration during the data collection such as variation in object size, number of objects, foreground-background contrast, object position on the image, and etc. Then, we prepared ground truth binary images for each hyperspectral data, where salient objects are labelled on the images. Finally, we did performance evaluation using Area Under Curve (AUC) metric on some existing hyperspectral saliency detection models in literature.



### A flexible model for training action localization with varying levels of supervision
- **Arxiv ID**: http://arxiv.org/abs/1806.11328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11328v2)
- **Published**: 2018-06-29 09:56:41+00:00
- **Updated**: 2018-11-27 22:26:51+00:00
- **Authors**: Guilhem Ch√©ron, Jean-Baptiste Alayrac, Ivan Laptev, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.



### Ignition: An End-to-End Supervised Model for Training Simulated Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1806.11349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11349v1)
- **Published**: 2018-06-29 10:48:33+00:00
- **Updated**: 2018-06-29 10:48:33+00:00
- **Authors**: Rooz Mahdavian, Richard Diehl Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments. The model is a ResNet-18 variant, which is fed in images from the front of a simulated F1 car, and outputs optimal labels for steering, throttle, braking. Importantly, we never explicitly train the model to detect road features like the outline of a track or distance to other cars; instead, we illustrate that these latent features can be automatically encapsulated by the network.



### Detecting Mammals in UAV Images: Best Practices to address a substantially Imbalanced Dataset with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.11368v1
- **DOI**: 10.1016/j.rse.2018.06.028
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11368v1)
- **Published**: 2018-06-29 11:59:14+00:00
- **Updated**: 2018-06-29 11:59:14+00:00
- **Authors**: Benjamin Kellenberger, Diego Marcos, Devis Tuia
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge over the number of animals in large wildlife reserves is a vital necessity for park rangers in their efforts to protect endangered species. Manual animal censuses are dangerous and expensive, hence Unmanned Aerial Vehicles (UAVs) with consumer level digital cameras are becoming a popular alternative tool to estimate livestock. Several works have been proposed that semi-automatically process UAV images to detect animals, of which some employ Convolutional Neural Networks (CNNs), a recent family of deep learning algorithms that proved very effective in object detection in large datasets from computer vision. However, the majority of works related to wildlife focuses only on small datasets (typically subsets of UAV campaigns), which might be detrimental when presented with the sheer scale of real study areas for large mammal census. Methods may yield thousands of false alarms in such cases. In this paper, we study how to scale CNNs to large wildlife census tasks and present a number of recommendations to train a CNN on a large UAV dataset. We further introduce novel evaluation protocols that are tailored to censuses and model suitability for subsequent human verification of detections. Using our recommendations, we are able to train a CNN reducing the number of false positives by an order of magnitude compared to previous state-of-the-art. Setting the requirements at 90% recall, our CNN allows to reduce the amount of data required for manual verification by three times, thus making it possible for rangers to screen all the data acquired efficiently and to detect almost all animals in the reserve automatically.



### Towards real-time unsupervised monocular depth estimation on CPU
- **Arxiv ID**: http://arxiv.org/abs/1806.11430v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.11430v3)
- **Published**: 2018-06-29 14:18:24+00:00
- **Updated**: 2018-07-31 10:31:36+00:00
- **Authors**: Matteo Poggi, Filippo Aleotti, Fabio Tosi, Stefano Mattoccia
- **Comment**: 7 pages, 3 figures. Accepted to IROS 2018
- **Journal**: None
- **Summary**: Unsupervised depth estimation from a single image is a very attractive technique with several implications in robotic, autonomous navigation, augmented reality and so on. This topic represents a very challenging task and the advent of deep learning enabled to tackle this problem with excellent results. However, these architectures are extremely deep and complex. Thus, real-time performance can be achieved only by leveraging power-hungry GPUs that do not allow to infer depth maps in application fields characterized by low-power constraints. To tackle this issue, in this paper we propose a novel architecture capable to quickly infer an accurate depth map on a CPU, even of an embedded system, using a pyramid of features extracted from a single input image. Similarly to state-of-the-art, we train our network in an unsupervised manner casting depth estimation as an image reconstruction problem. Extensive experimental results on the KITTI dataset show that compared to the top performing approach our network has similar accuracy but a much lower complexity (about 6% of parameters) enabling to infer a depth map for a KITTI image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard CPU. Moreover, by trading accuracy for efficiency, our network allows to infer maps at about 2 Hz and 40 Hz respectively, still being more accurate than most state-of-the-art slower methods. To the best of our knowledge, it is the first method enabling such performance on CPUs paving the way for effective deployment of unsupervised monocular depth estimation even on embedded systems.



### MRFusion: A Deep Learning architecture to fuse PAN and MS imagery for land cover mapping
- **Arxiv ID**: http://arxiv.org/abs/1806.11452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11452v1)
- **Published**: 2018-06-29 14:43:48+00:00
- **Updated**: 2018-06-29 14:43:48+00:00
- **Authors**: Raffaele Gaetano, Dino Ienco, Kenji Ose, Remi Cresson
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, Earth Observation systems provide a multitude of heterogeneous remote sensing data. How to manage such richness leveraging its complementarity is a crucial chal- lenge in modern remote sensing analysis. Data Fusion techniques deal with this point proposing method to combine and exploit complementarity among the different data sensors. Considering optical Very High Spatial Resolution (VHSR) images, satellites obtain both Multi Spectral (MS) and panchro- matic (PAN) images at different spatial resolution. VHSR images are extensively exploited to produce land cover maps to deal with agricultural, ecological, and socioeconomic issues as well as assessing ecosystem status, monitoring biodiversity and provid- ing inputs to conceive food risk monitoring systems. Common techniques to produce land cover maps from such VHSR images typically opt for a prior pansharpening of the multi-resolution source for a full resolution processing. Here, we propose a new deep learning architecture to jointly use PAN and MS imagery for a direct classification without any prior image fusion or resampling process. By managing the spectral information at its native spatial resolution, our method, named MRFusion, aims at avoiding the possible infor- mation loss induced by pansharpening or any other hand-crafted preprocessing. Moreover, the proposed architecture is suitably designed to learn non-linear transformations of the sources with the explicit aim of taking as much as possible advantage of the complementarity of PAN and MS imagery. Experiments are carried out on two-real world scenarios depicting large areas with different land cover characteristics. The characteristics of the proposed scenarios underline the applicability and the generality of our method in operational settings.



### A Novel Geometric Framework on Gram Matrix Trajectories for Human Behavior Understanding
- **Arxiv ID**: http://arxiv.org/abs/1807.00676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00676v1)
- **Published**: 2018-06-29 15:15:58+00:00
- **Updated**: 2018-06-29 15:15:58+00:00
- **Authors**: Anis Kacem, Mohamed Daoudi, Boulbaba Ben Amor, Stefano Berretti, Juan Carlos Alvarez-Paiva
- **Comment**: Under minor revisions in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (T-PAMI). A preliminary version of this work appeared in
  ICCV 17 (A Kacem, M Daoudi, BB Amor, JC Alvarez-Paiva, A Novel Space-Time
  Representation on the Positive Semidefinite Cone for Facial Expression
  Recognition, ICCV 17). arXiv admin note: substantial text overlap with
  arXiv:1707.06440
- **Journal**: None
- **Summary**: In this paper, we propose a novel space-time geometric representation of human landmark configurations and derive tools for comparison and classification. We model the temporal evolution of landmarks as parametrized trajectories on the Riemannian manifold of positive semidefinite matrices of fixed-rank. Our representation has the benefit to bring naturally a second desirable quantity when comparing shapes, the spatial covariance, in addition to the conventional affine-shape representation. We derived then geometric and computational tools for rate-invariant analysis and adaptive re-sampling of trajectories, grounding on the Riemannian geometry of the underlying manifold. Specifically, our approach involves three steps: (1) landmarks are first mapped into the Riemannian manifold of positive semidefinite matrices of fixed-rank to build time-parameterized trajectories; (2) a temporal warping is performed on the trajectories, providing a geometry-aware (dis-)similarity measure between them; (3) finally, a pairwise proximity function SVM is used to classify them, incorporating the (dis-)similarity measure into the kernel function. We show that such representation and metric achieve competitive results in applications as action recognition and emotion recognition from 3D skeletal data, and facial expression recognition from videos. Experiments have been conducted on several publicly available up-to-date benchmarks.



### Simplified Active Calibration
- **Arxiv ID**: http://arxiv.org/abs/1806.11468v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11468v2)
- **Published**: 2018-06-29 15:17:32+00:00
- **Updated**: 2018-07-25 19:23:02+00:00
- **Authors**: Mehdi Faraji, Anup Basu
- **Comment**: 22 pages, 8 figures, and 2 tables. Preprint submitted to Journal of
  Image and Vision Computing. Journal version of arXiv:1806.03584
- **Journal**: None
- **Summary**: We present a new mathematical formulation to estimate the intrinsic parameters of a camera in active or robotic platforms. We show that the focal lengths can be estimated using only one point correspondence that relates images taken before and after a degenerate rotation of the camera. The estimated focal lengths are then treated as known parameters to obtain a linear set of equations to calculate the principal point. Assuming that the principal point is close to the image center, the accuracy of the linear equations are increased by integrating the image center into the formulation. We extensively evaluate the formulations on a simulated camera, 3D scenes and real-world images. Our error analysis over simulated and real images indicates that the proposed Simplified Active Calibration method estimates the parameters of a camera with low error rates that can be used as an initial guess for further non-linear refinement procedures. Simplified Active Calibration can be employed in real-time environments for automatic calibrations given the proposed closed-form solutions.



### SynNet: Structure-Preserving Fully Convolutional Networks for Medical Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1806.11475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11475v1)
- **Published**: 2018-06-29 15:32:57+00:00
- **Updated**: 2018-06-29 15:32:57+00:00
- **Authors**: Deepa Gunashekar, Sailesh Conjeti, Abhijit Guha Roy, Nassir Navab, Kuangyu Shi
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Cross modal image syntheses is gaining significant interests for its ability to estimate target images of a different modality from a given set of source images,like estimating MR to MR, MR to CT, CT to PET etc, without the need for an actual acquisition.Though they show potential for applications in radiation therapy planning,image super resolution, atlas construction, image segmentation etc.The synthesis results are not as accurate as the actual acquisition.In this paper,we address the problem of multi modal image synthesis by proposing a fully convolutional deep learning architecture called the SynNet.We extend the proposed architecture for various input output configurations. And finally, we propose a structure preserving custom loss function for cross-modal image synthesis.We validate the proposed SynNet and its extended framework on BRATS dataset with comparisons against three state-of-the art methods.And the results of the proposed custom loss function is validated against the traditional loss function used by the state-of-the-art methods for cross modal image synthesis.



### Mammographic Image Enhancement using Digital Image Processing Technique
- **Arxiv ID**: http://arxiv.org/abs/1806.11496v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.11496v1)
- **Published**: 2018-06-29 15:54:51+00:00
- **Updated**: 2018-06-29 15:54:51+00:00
- **Authors**: Ardymulya Iswardani, Wahyu Hidayat
- **Comment**: 3 tables, 5 pages, 7 figures
- **Journal**: None
- **Summary**: Abstract PURPOSES this study aims to perform microcalsification detection by performing image enhancement in mammography image by using transformation of negative image and histogram equalization. image mammography with .pgm format changed to. jpg format then processed into negative image result then processed again using histogram equalization. the results of the image enhancement process using negative image techniques and equalization histograms are compared and validated with MSE and PSNR on each mammographic image. CONCLUSION: Image enhancement process on mammography image can be done, however there are only some image that have improved quality, this affected by threshold usage, which have important role to get better visualization on mammographic image. Keywords-component; Image enhancement, image negative, histogram equalization, mammographic, breast cancer



### Recognition of Offline Handwritten Devanagari Numerals using Regional Weighted Run Length Features
- **Arxiv ID**: http://arxiv.org/abs/1806.11517v1
- **DOI**: 10.1109/ICCECE.2016.8009567
- **Categories**: **cs.CV**, 68T10, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1806.11517v1)
- **Published**: 2018-06-29 16:20:31+00:00
- **Updated**: 2018-06-29 16:20:31+00:00
- **Authors**: Pawan Kumar Singh, Supratim Das, Ram Sarkar, Mita Nasipuri
- **Comment**: 6 pages
- **Journal**: 1st IEEE International Conference on Computer, Electrical and
  Communication Engineering (ICCECE 2016)
- **Summary**: Recognition of handwritten Roman characters and numerals has been extensively studied in the last few decades and its accuracy reached to a satisfactory state. But the same cannot be said while talking about the Devanagari script which is one of most popular script in India. This paper proposes an efficient digit recognition system for handwritten Devanagari script. The system uses a novel 196-element Mask Oriented Directional (MOD) features for the recognition purpose. The methodology is tested using five conventional classifiers on 6000 handwritten digit samples. On applying 3-fold cross-validation scheme, the proposed system yields the highest recognition accuracy of 95.02% using Support Vector Machine (SVM) classifier.



### Visual Attention and its Intimate Links to Spatial Cognition
- **Arxiv ID**: http://arxiv.org/abs/1806.11530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11530v1)
- **Published**: 2018-06-29 16:50:47+00:00
- **Updated**: 2018-06-29 16:50:47+00:00
- **Authors**: John K. Tsotsos, Iuliia Kotseruba, Amir Rasouli, Markus D. Solbach
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: It is almost universal to regard attention as the facility that permits an agent, human or machine, to give priority processing resources to relevant stimuli while ignoring the irrelevant. The reality of how this might manifest itself throughout all the forms of perceptual and cognitive processes possessed by humans, however, is not as clear. Here we examine this reality with a broad perspective in order to highlight the myriad ways that attentional processes impact both perception and cognition. The paper concludes by showing two real world problems that exhibit sufficient complexity to illustrate the ways in which attention and cognition connect. These then point to new avenues of research that might illuminate the overall cognitive architecture of spatial cognition.



### End-to-end Learning of Multi-sensor 3D Tracking by Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.11534v1
- **DOI**: 10.1109/ICRA.2018.8462884
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.11534v1)
- **Published**: 2018-06-29 17:02:00+00:00
- **Updated**: 2018-06-29 17:02:00+00:00
- **Authors**: Davi Frossard, Raquel Urtasun
- **Comment**: Presented at IEEE International Conference on Robotics and Automation
  (ICRA), 2018
- **Journal**: In 2018 IEEE International Conference on Robotics and Automation
  (ICRA), pp. 635-642. IEEE, 2018
- **Summary**: In this paper we propose a novel approach to tracking by detection that can exploit both cameras as well as LIDAR data to produce very accurate 3D trajectories. Towards this goal, we formulate the problem as a linear program that can be solved exactly, and learn convolutional networks for detection as well as matching in an end-to-end manner. We evaluate our model in the challenging KITTI dataset and show very competitive results.



### Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/1806.11538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11538v2)
- **Published**: 2018-06-29 17:10:58+00:00
- **Updated**: 2018-08-27 17:06:21+00:00
- **Authors**: Yikang Li, Wanli Ouyang, Bolei Zhou, Jianping Shi, Chao Zhang, Xiaogang Wang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Generating scene graph to describe all the relations inside an image gains increasing interests these years. However, most of the previous methods use complicated structures with slow inference speed or rely on the external data, which limits the usage of the model in real-life scenarios. To improve the efficiency of scene graph generation, we propose a subgraph-based connection graph to concisely represent the scene graph during the inference. A bottom-up clustering method is first used to factorize the entire scene graph into subgraphs, where each subgraph contains several objects and a subset of their relationships. By replacing the numerous relationship representations of the scene graph with fewer subgraph and object features, the computation in the intermediate stage is significantly reduced. In addition, spatial information is maintained by the subgraph features, which is leveraged by our proposed Spatial-weighted Message Passing~(SMP) structure and Spatial-sensitive Relation Inference~(SRI) module to facilitate the relationship recognition. On the recent Visual Relationship Detection and Visual Genome datasets, our method outperforms the state-of-the-art method in both accuracy and speed.



### Deep Networks with Shape Priors for Nucleus Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.03135v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03135v1)
- **Published**: 2018-06-29 17:54:41+00:00
- **Updated**: 2018-06-29 17:54:41+00:00
- **Authors**: Mohammad Tofighi, Tiantong Guo, Jairam K. P. Vanamala, Vishal Monga
- **Comment**: Accepted paper to 2018 IEEE International Conference on Image
  Processing (ICIP 2018)
- **Journal**: None
- **Summary**: Detection of cell nuclei in microscopic images is a challenging research topic, because of limitations in cellular image quality and diversity of nuclear morphology, i.e. varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train for example convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many of these methods are supplemented by spatial or morphological processing. We develop a new approach that we call Shape Priors with Convolutional Neural Networks (SP-CNN) to perform significantly enhanced nuclei detection. A set of canonical shapes is prepared with the help of a domain expert. Subsequently, we present a new network structure that can incorporate `expected behavior' of nucleus shapes via two components: {\em learnable} layers that perform the nucleus detection and a {\em fixed} processing part that guides the learning with prior information. Analytically, we formulate a new regularization term that is targeted at penalizing false positives while simultaneously encouraging detection inside cell nucleus boundary. Experimental results on a challenging dataset reveal that SP-CNN is competitive with or outperforms several state-of-the-art methods.



### Outfit Generation and Style Extraction via Bidirectional LSTM and Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1807.03133v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03133v3)
- **Published**: 2018-06-29 18:00:03+00:00
- **Updated**: 2018-10-23 10:28:23+00:00
- **Authors**: Takuma Nakamura, Ryosuke Goto
- **Comment**: 9 pages, 5 figures, KDD Workshop AI for fashion
- **Journal**: None
- **Summary**: When creating an outfit, style is a criterion in selecting each fashion item. This means that style can be regarded as a feature of the overall outfit. However, in various previous studies on outfit generation, there have been few methods focusing on global information obtained from an outfit. To address this deficiency, we have incorporated an unsupervised style extraction module into a model to learn outfits. Using the style information of an outfit as a whole, the proposed model succeeded in generating outfits more flexibly without requiring additional information. Moreover, the style information extracted by the proposed model is easy to interpret. The proposed model was evaluated on two human-generated outfit datasets. In a fashion item prediction task (missing prediction task), the proposed model outperformed a baseline method. In a style extraction task, the proposed model extracted some easily distinguishable styles. In an outfit generation task, the proposed model generated an outfit while controlling its styles. This capability allows us to generate fashionable outfits according to various preferences.



### It All Matters: Reporting Accuracy, Inference Time and Power Consumption for Face Emotion Recognition on Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1807.00046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00046v1)
- **Published**: 2018-06-29 19:47:24+00:00
- **Updated**: 2018-06-29 19:47:24+00:00
- **Authors**: Jelena Milosevic, Dexmont Pena, Andrew Forembsky, David Moloney, Miroslaw Malek
- **Comment**: 13 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: While several approaches to face emotion recognition task are proposed in literature, none of them reports on power consumption nor inference time required to run the system in an embedded environment. Without adequate knowledge about these factors it is not clear whether we are actually able to provide accurate face emotion recognition in the embedded environment or not, and if not, how far we are from making it feasible and what are the biggest bottlenecks we face.   The main goal of this paper is to answer these questions and to convey the message that instead of reporting only detection accuracy also power consumption and inference time should be reported as real usability of the proposed systems and their adoption in human computer interaction strongly depends on it. In this paper, we identify the state-of-the art face emotion recognition methods that are potentially suitable for embedded environment and the most frequently used datasets for this task. Our study shows that most of the performed experiments use datasets with posed expressions or in a particular experimental setup with special conditions for image collection. Since our goal is to evaluate the performance of the identified promising methods in the realistic scenario, we collect a new dataset with non-exaggerated emotions and we use it, in addition to the publicly available datasets, for the evaluation of detection accuracy, power consumption and inference time on three frequently used embedded devices with different computational capabilities. Our results show that gray images are still more suitable for embedded environment than color ones and that for most of the analyzed systems either inference time or energy consumption or both are limiting factor for their adoption in real-life embedded applications.




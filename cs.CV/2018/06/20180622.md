# Arxiv Papers in cs.CV on 2018-06-22
### TriResNet: A Deep Triple-stream Residual Network for Histopathology Grading
- **Arxiv ID**: http://arxiv.org/abs/1806.08463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.08463v1)
- **Published**: 2018-06-22 01:18:14+00:00
- **Updated**: 2018-06-22 01:18:14+00:00
- **Authors**: Rene Bidart, Alexander Wong
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: While microscopic analysis of histopathological slides is generally considered as the gold standard method for performing cancer diagnosis and grading, the current method for analysis is extremely time consuming and labour intensive as it requires pathologists to visually inspect tissue samples in a detailed fashion for the presence of cancer. As such, there has been significant recent interest in computer aided diagnosis systems for analysing histopathological slides for cancer grading to aid pathologists to perform cancer diagnosis and grading in a more efficient, accurate, and consistent manner. In this work, we investigate and explore a deep triple-stream residual network (TriResNet) architecture for the purpose of tile-level histopathology grading, which is the critical first step to computer-aided whole-slide histopathology grading. In particular, the design mentality behind the proposed TriResNet network architecture is to facilitate for the learning of a more diverse set of quantitative features to better characterize the complex tissue characteristics found in histopathology samples. Experimental results on two widely-used computer-aided histopathology benchmark datasets (CAMELYON16 dataset and Invasive Ductal Carcinoma (IDC) dataset) demonstrated that the proposed TriResNet network architecture was able to achieve noticeably improved accuracies when compared with two other state-of-the-art deep convolutional neural network architectures. Based on these promising results, the hope is that the proposed TriResNet network architecture could become a useful tool to aiding pathologists increase the consistency, speed, and accuracy of the histopathology grading process.



### Learning a High Fidelity Pose Invariant Model for High-resolution Face Frontalization
- **Arxiv ID**: http://arxiv.org/abs/1806.08472v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08472v2)
- **Published**: 2018-06-22 02:45:22+00:00
- **Updated**: 2018-10-06 08:41:08+00:00
- **Authors**: Jie Cao, Yibo Hu, Hongwen Zhang, Ran He, Zhenan Sun
- **Comment**: To appear in NIPS 2018
- **Journal**: None
- **Summary**: Face frontalization refers to the process of synthesizing the frontal view of a face from a given profile. Due to self-occlusion and appearance distortion in the wild, it is extremely challenging to recover faithful results and preserve texture details in a high-resolution. This paper proposes a High Fidelity Pose Invariant Model (HF-PIM) to produce photographic and identity-preserving results. HF-PIM frontalizes the profiles through a novel texture warping procedure and leverages a dense correspondence field to bind the 2D and 3D surface spaces. We decompose the prerequisite of warping into dense correspondence field estimation and facial texture map recovering, which are both well addressed by deep networks. Different from those reconstruction methods relying on 3D data, we also propose Adversarial Residual Dictionary Learning (ARDL) to supervise facial texture map recovering with only monocular images. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only boosts the performance of pose-invariant face recognition but also dramatically improves high-resolution frontalization appearances.



### Video Inpainting by Jointly Learning Temporal Structure and Spatial Details
- **Arxiv ID**: http://arxiv.org/abs/1806.08482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08482v2)
- **Published**: 2018-06-22 03:32:57+00:00
- **Updated**: 2018-12-03 00:34:30+00:00
- **Authors**: Chuan Wang, Haibin Huang, Xiaoguang Han, Jue Wang
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: We present a new data-driven video inpainting method for recovering missing regions of video frames. A novel deep learning architecture is proposed which contains two sub-networks: a temporal structure inference network and a spatial detail recovering network. The temporal structure inference network is built upon a 3D fully convolutional architecture: it only learns to complete a low-resolution video volume given the expensive computational cost of 3D convolution. The low resolution result provides temporal guidance to the spatial detail recovering network, which performs image-based inpainting with a 2D fully convolutional network to produce recovered video frames in their original resolution. Such two-step network design ensures both the spatial quality of each frame and the temporal coherence across frames. Our method jointly trains both sub-networks in an end-to-end manner. We provide qualitative and quantitative evaluation on three datasets, demonstrating that our method outperforms previous learning-based video inpainting methods.



### Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction from Binary Mask Images
- **Arxiv ID**: http://arxiv.org/abs/1806.08485v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.08485v1)
- **Published**: 2018-06-22 04:00:37+00:00
- **Updated**: 2018-06-22 04:00:37+00:00
- **Authors**: Zhongping Ji, Xiao Qi, Yigang Wang, Gang Xu, Peng Du, Qing Wu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: 3D content creation is referred to as one of the most fundamental tasks of computer graphics. And many 3D modeling algorithms from 2D images or curves have been developed over the past several decades. Designers are allowed to align some conceptual images or sketch some suggestive curves, from front, side, and top views, and then use them as references in constructing a 3D model automatically or manually. However, to the best of our knowledge, no studies have investigated on 3D human body reconstruction in a similar manner. In this paper, we propose a deep learning based reconstruction of 3D human body shape from 2D orthographic views. A novel CNN-based regression network, with two branches corresponding to frontal and lateral views respectively, is designed for estimating 3D human body shape from 2D mask images. We train our networks separately to decouple the feature descriptors which encode the body parameters from different views, and fuse them to estimate an accurate human body shape. In addition, to overcome the shortage of training data required for this purpose, we propose some significantly data augmentation schemes for 3D human body shapes, which can be used to promote further research on this topic. Extensive experimen- tal results demonstrate that visually realistic and accurate reconstructions can be achieved effectively using our algorithm. Requiring only binary mask images, our method can help users create their own digital avatars quickly, and also make it easy to create digital human body for 3D game, virtual reality, online fashion shopping.



### Visual-Inertial Object Detection and Mapping
- **Arxiv ID**: http://arxiv.org/abs/1806.08498v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.08498v2)
- **Published**: 2018-06-22 05:29:31+00:00
- **Updated**: 2018-10-23 21:43:31+00:00
- **Authors**: Xiaohan Fei, Stefano Soatto
- **Comment**: None
- **Journal**: ECCV 2018
- **Summary**: We present a method to populate an unknown environment with models of previously seen objects, placed in a Euclidean reference frame that is inferred causally and on-line using monocular video along with inertial sensors. The system we implement returns a sparse point cloud for the regions of the scene that are visible but not recognized as a previously seen object, and a detailed object model and its pose in the Euclidean frame otherwise. The system includes bottom-up and top-down components, whereby deep networks trained for detection provide likelihood scores for object hypotheses provided by a nonlinear filter, whose state serves as memory. Additional networks provide likelihood scores for edges, which complements detection networks trained to be invariant to small deformations. We test our algorithm on existing datasets, and also introduce the VISMA dataset, that provides ground truth pose, point-cloud map, and object models, along with time-stamped inertial measurements.



### Global Semantic Consistency for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.08503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08503v1)
- **Published**: 2018-06-22 05:55:20+00:00
- **Updated**: 2018-06-22 05:55:20+00:00
- **Authors**: Fan Wu, Kai Tian, Jihong Guan, Shuigeng Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In image recognition, there are many cases where training samples cannot cover all target classes. Zero-shot learning (ZSL) utilizes the class semantic information to classify samples of the unseen categories that have no corresponding samples contained in the training set. In this paper, we propose an end-to-end framework, called Global Semantic Consistency Network (GSC-Net for short), which makes complete use of the semantic information of both seen and unseen classes, to support effective zero-shot learning. We also adopt a soft label embedding loss to further exploit the semantic relationships among classes. To adapt GSC-Net to a more practical setting, Generalized Zero-shot Learning (GZSL), we introduce a parametric novelty detection mechanism. Our approach achieves the state-of-the-art performance on both ZSL and GZSL tasks over three visual attribute datasets, which validates the effectiveness and advantage of the proposed framework.



### Virtual Codec Supervised Re-Sampling Network for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1806.08514v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.08514v2)
- **Published**: 2018-06-22 06:48:07+00:00
- **Updated**: 2018-07-10 02:45:39+00:00
- **Authors**: Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao
- **Comment**: 13 pages, 11 figures Our project can be found in the website:
  https://github.com/VirtualCodecNetwork
- **Journal**: None
- **Summary**: In this paper, we propose an image re-sampling compression method by learning virtual codec network (VCN) to resolve the non-differentiable problem of quantization function for image compression. Here, the image re-sampling not only refers to image full-resolution re-sampling but also low-resolution re-sampling. We generalize this method for standard-compliant image compression (SCIC) framework and deep neural networks based compression (DNNC) framework. Specifically, an input image is measured by re-sampling network (RSN) network to get re-sampled vectors. Then, these vectors are directly quantized in the feature space in SCIC, or discrete cosine transform coefficients of these vectors are quantized to further improve coding efficiency in DNNC. At the encoder, the quantized vectors or coefficients are losslessly compressed by arithmetic coding. At the receiver, the decoded vectors are utilized to restore input image by image decoder network (IDN). In order to train RSN network and IDN network together in an end-to-end fashion, our VCN network intimates projection from the re-sampled vectors to the IDN-decoded image. As a result, gradients from IDN network to RSN network can be approximated by VCN network's gradient. Because dimension reduction can be further achieved by quantization in some dimensional space after image re-sampling within auto-encoder architecture, we can well initialize our networks from pre-trained auto-encoder networks. Through extensive experiments and analysis, it is verified that the proposed method has more effectiveness and versatility than many state-of-the-art approaches.



### Efficient Semantic Segmentation using Gradual Grouping
- **Arxiv ID**: http://arxiv.org/abs/1806.08522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08522v1)
- **Published**: 2018-06-22 07:11:41+00:00
- **Updated**: 2018-06-22 07:11:41+00:00
- **Authors**: Nikitha Vallurupalli, Sriharsha Annamaneni, Girish Varma, C V Jawahar, Manu Mathew, Soyeb Nagori
- **Comment**: CVPR 2018 Workshop paper
- **Journal**: None
- **Summary**: Deep CNNs for semantic segmentation have high memory and run time requirements. Various approaches have been proposed to make CNNs efficient like grouped, shuffled, depth-wise separable convolutions. We study the effectiveness of these techniques on a real-time semantic segmentation architecture like ERFNet for improving run time by over 5X. We apply these techniques to CNN layers partially or fully and evaluate the testing accuracies on Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs, giving accuracy scores for models that can run under specified runtime budgets. We further propose a novel training procedure which starts out with a dense convolution but gradually evolves towards a grouped convolution. We show that our proposed training method and efficient architecture design can improve accuracies by over 8% with depth wise separable convolutions applied on the encoder of ERFNet and attaching a light weight decoder. This results in a model which has a 5X improvement in FLOPs while only suffering a 4% degradation in accuracy with respect to ERFNet.



### Focusing on What is Relevant: Time-Series Learning and Understanding using Attention
- **Arxiv ID**: http://arxiv.org/abs/1806.08523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08523v1)
- **Published**: 2018-06-22 07:16:08+00:00
- **Updated**: 2018-06-22 07:16:08+00:00
- **Authors**: Phongtharin Vinayavekhin, Subhajit Chaudhury, Asim Munawar, Don Joven Agravante, Giovanni De Magistris, Daiki Kimura, Ryuki Tachibana
- **Comment**: To appear in ICPR 2018
- **Journal**: None
- **Summary**: This paper is a contribution towards interpretability of the deep learning models in different applications of time-series. We propose a temporal attention layer that is capable of selecting the relevant information to perform various tasks, including data completion, key-frame detection and classification. The method uses the whole input sequence to calculate an attention value for each time step. This results in more focused attention values and more plausible visualisation than previous methods. We apply the proposed method to three different tasks. Experimental results show that the proposed network produces comparable results to a state of the art. In addition, the network provides better interpretability of the decision, that is, it generates more significant attention weight to related frames compared to similar techniques attempted in the past.



### Deep Spectral Convolution Network for HyperSpectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1806.08562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08562v1)
- **Published**: 2018-06-22 09:02:43+00:00
- **Updated**: 2018-06-22 09:02:43+00:00
- **Authors**: Savas Ozkan, Gozde Bozdagi Akar
- **Comment**: Accepted to ICIP 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel hyperspectral unmixing technique based on deep spectral convolution networks (DSCN). Particularly, three important contributions are presented throughout this paper. First, fully-connected linear operation is replaced with spectral convolutions to extract local spectral characteristics from hyperspectral signatures with a deeper network architecture. Second, instead of batch normalization, we propose a spectral normalization layer which improves the selectivity of filters by normalizing their spectral responses. Third, we introduce two fusion configurations that produce ideal abundance maps by using the abstract representations computed from previous layers. In experiments, we use two real datasets to evaluate the performance of our method with other baseline techniques. The experimental results validate that the proposed method outperforms baselines based on Root Mean Square Error (RMSE).



### An accurate retrieval through R-MAC+ descriptors for landmark recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.08565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08565v1)
- **Published**: 2018-06-22 09:16:10+00:00
- **Updated**: 2018-06-22 09:16:10+00:00
- **Authors**: Federico Magliani, Andrea Prati
- **Comment**: None
- **Journal**: None
- **Summary**: The landmark recognition problem is far from being solved, but with the use of features extracted from intermediate layers of Convolutional Neural Networks (CNNs), excellent results have been obtained. In this work, we propose some improvements on the creation of R-MAC descriptors in order to make the newly-proposed R-MAC+ descriptors more representative than the previous ones. However, the main contribution of this paper is a novel retrieval technique, that exploits the fine representativeness of the MAC descriptors of the database images. Using this descriptors called "db regions" during the retrieval stage, the performance is greatly improved. The proposed method is tested on different public datasets: Oxford5k, Paris6k and Holidays. It outperforms the state-of-the- art results on Holidays and reached excellent results on Oxford5k and Paris6k, overcame only by approaches based on fine-tuning strategies.



### Continuous Learning in Single-Incremental-Task Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1806.08568v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.08568v3)
- **Published**: 2018-06-22 09:22:42+00:00
- **Updated**: 2019-01-22 21:49:25+00:00
- **Authors**: Davide Maltoni, Vincenzo Lomonaco
- **Comment**: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4),
  several typos and minor mistakes corrected
- **Journal**: None
- **Summary**: It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in term of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.



### Point cloud segmentation using hierarchical tree for architectural models
- **Arxiv ID**: http://arxiv.org/abs/1806.08572v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1806.08572v1)
- **Published**: 2018-06-22 09:36:21+00:00
- **Updated**: 2018-06-22 09:36:21+00:00
- **Authors**: Omair Hassaan, Abeera Shamail, Zain Butt, Murtaza Taj
- **Comment**: 9 pages. 10 figures. Submitted in EuroGraphics 2018
- **Journal**: None
- **Summary**: Recent developments in the 3D scanning technologies have made the generation of highly accurate 3D point clouds relatively easy but the segmentation of these point clouds remains a challenging area. A number of techniques have set precedent of either planar or primitive based segmentation in literature. In this work, we present a novel and an effective primitive based point cloud segmentation algorithm. The primary focus, i.e. the main technical contribution of our method is a hierarchical tree which iteratively divides the point cloud into segments. This tree uses an exclusive energy function and a 3D convolutional neural network, HollowNets to classify the segments. We test the efficacy of our proposed approach using both real and synthetic data obtaining an accuracy greater than 90% for domes and minarets.



### KinshipGAN: Synthesizing of Kinship Faces From Family Photos by Regularizing a Deep Face Network
- **Arxiv ID**: http://arxiv.org/abs/1806.08600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08600v2)
- **Published**: 2018-06-22 11:07:57+00:00
- **Updated**: 2018-07-17 08:27:48+00:00
- **Authors**: Savas Ozkan, Akin Ozkan
- **Comment**: Accepted to IEEE ICIP 2018
- **Journal**: None
- **Summary**: In this paper, we propose a kinship generator network that can synthesize a possible child face by analyzing his/her parent's photo. For this purpose, we focus on to handle the scarcity of kinship datasets throughout the paper by proposing novel solutions in particular. To extract robust features, we integrate a pre-trained face model to the kinship face generator. Moreover, the generator network is regularized with an additional face dataset and adversarial loss to decrease the overfitting of the limited samples. Lastly, we adapt cycle-domain transformation to attain a more stable results. Experiments are conducted on Families in the Wild (FIW) dataset. The experimental results show that the contributions presented in the paper provide important performance improvements compared to the baseline architecture and our proposed method yields promising perceptual results.



### Ad-Net: Audio-Visual Convolutional Neural Network for Advertisement Detection In Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.08612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08612v1)
- **Published**: 2018-06-22 11:52:57+00:00
- **Updated**: 2018-06-22 11:52:57+00:00
- **Authors**: Shervin Minaee, Imed Bouazizi, Prakash Kolan, Hossein Najafzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Personalized advertisement is a crucial task for many of the online businesses and video broadcasters. Many of today's broadcasters use the same commercial for all customers, but as one can imagine different viewers have different interests and it seems reasonable to have customized commercial for different group of people, chosen based on their demographic features, and history. In this project, we propose a framework, which gets the broadcast videos, analyzes them, detects the commercial and replaces it with a more suitable commercial. We propose a two-stream audio-visual convolutional neural network, that one branch analyzes the visual information and the other one analyzes the audio information, and then the audio and visual embedding are fused together, and are used for commercial detection, and content categorization. We show that using both the visual and audio content of the videos significantly improves the model performance for video analysis. This network is trained on a dataset of more than 50k regular video and commercial shots, and achieved much better performance compared to the models based on hand-crafted features.



### Deploying Deep Neural Networks in the Embedded Space
- **Arxiv ID**: http://arxiv.org/abs/1806.08616v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.08616v1)
- **Published**: 2018-06-22 12:01:11+00:00
- **Updated**: 2018-06-22 12:01:11+00:00
- **Authors**: Stylianos I. Venieris, Alexandros Kouris, Christos-Savvas Bouganis
- **Comment**: Accepted at MobiSys18: 2nd International Workshop on Embedded and
  Mobile Deep Learning (EMDL) 2018
- **Journal**: None
- **Summary**: Recently, Deep Neural Networks (DNNs) have emerged as the dominant model across various AI applications. In the era of IoT and mobile systems, the efficient deployment of DNNs on embedded platforms is vital to enable the development of intelligent applications. This paper summarises our recent work on the optimised mapping of DNNs on embedded settings. By covering such diverse topics as DNN-to-accelerator toolflows, high-throughput cascaded classifiers and domain-specific model design, the presented set of works aim to enable the deployment of sophisticated deep learning models on cutting-edge mobile and embedded systems.



### A probabilistic atlas of the human thalamic nuclei combining ex vivo MRI and histology
- **Arxiv ID**: http://arxiv.org/abs/1806.08634v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1806.08634v1)
- **Published**: 2018-06-22 12:42:37+00:00
- **Updated**: 2018-06-22 12:42:37+00:00
- **Authors**: Juan Eugenio Iglesias, Ricardo Insausti, Garikoitz Lerma-Usabiaga, Martina Bocchetta, Koen Van Leemput, Douglas N Greve, Andre van der Kouwe, Bruce Fischl, Cesar Caballero-Gaudes, Pedro M Paz-Alonso
- **Comment**: None
- **Journal**: None
- **Summary**: The human thalamus is a brain structure that comprises numerous, highly specific nuclei. Since these nuclei are known to have different functions and to be connected to different areas of the cerebral cortex, it is of great interest for the neuroimaging community to study their volume, shape and connectivity in vivo with MRI. In this study, we present a probabilistic atlas of the thalamic nuclei built using ex vivo brain MRI scans and histological data, as well as the application of the atlas to in vivo MRI segmentation. The atlas was built using manual delineation of 26 thalamic nuclei on the serial histology of 12 whole thalami from six autopsy samples, combined with manual segmentations of the whole thalamus and surrounding structures (caudate, putamen, hippocampus, etc.) made on in vivo brain MR data from 39 subjects. The 3D structure of the histological data and corresponding manual segmentations was recovered using the ex vivo MRI as reference frame, and stacks of blockface photographs acquired during the sectioning as intermediate target. The atlas, which was encoded as an adaptive tetrahedral mesh, shows a good agreement with with previous histological studies of the thalamus in terms of volumes of representative nuclei. When applied to segmentation of in vivo scans using Bayesian inference, the atlas shows excellent test-retest reliability, robustness to changes in input MRI contrast, and ability to detect differential thalamic effects in subjects with Alzheimer's disease. The probabilistic atlas and companion segmentation tool are publicly available as part of the neuroimaging package FreeSurfer.



### Towards safe deep learning: accurately quantifying biomarker uncertainty in neural network predictions
- **Arxiv ID**: http://arxiv.org/abs/1806.08640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08640v1)
- **Published**: 2018-06-22 12:54:20+00:00
- **Updated**: 2018-06-22 12:54:20+00:00
- **Authors**: Zach Eaton-Rosen, Felix Bragman, Sotirios Bisdas, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: Automated medical image segmentation, specifically using deep learning, has shown outstanding performance in semantic segmentation tasks. However, these methods rarely quantify their uncertainty, which may lead to errors in downstream analysis. In this work we propose to use Bayesian neural networks to quantify uncertainty within the domain of semantic segmentation. We also propose a method to convert voxel-wise segmentation uncertainty into volumetric uncertainty, and calibrate the accuracy and reliability of confidence intervals of derived measurements. When applied to a tumour volume estimation application, we demonstrate that by using such modelling of uncertainty, deep learning systems can be made to report volume estimates with well-calibrated error-bars, making them safer for clinical use. We also show that the uncertainty estimates extrapolate to unseen data, and that the confidence intervals are robust in the presence of artificial noise. This could be used to provide a form of quality control and quality assurance, and may permit further adoption of deep learning tools in the clinic.



### Compact Deep Neural Networks for Computationally Efficient Gesture Classification From Electromyography Signals
- **Arxiv ID**: http://arxiv.org/abs/1806.08641v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08641v3)
- **Published**: 2018-06-22 13:01:48+00:00
- **Updated**: 2019-04-13 16:51:45+00:00
- **Authors**: Adam Hartwell, Visakan Kadirkamanathan, Sean R Anderson
- **Comment**: IEEE BioRob 2018
- **Journal**: None
- **Summary**: Machine learning classifiers using surface electromyography are important for human-machine interfacing and device control. Conventional classifiers such as support vector machines (SVMs) use manually extracted features based on e.g. wavelets. These features tend to be fixed and non-person specific, which is a key limitation due to high person-to-person variability of myography signals. Deep neural networks, by contrast, can automatically extract person specific features - an important advantage. However, deep neural networks typically have the drawback of large numbers of parameters, requiring large training data sets and powerful hardware not suited to embedded systems. This paper solves these problems by introducing a compact deep neural network architecture that is much smaller than existing counterparts. The performance of the compact deep net is benchmarked against an SVM and compared to other contemporary architectures across 10 human subjects, comparing Myo and Delsys Trigno electrode sets. The accuracy of the compact deep net was found to be 84.2 +/- 6% versus 70.5 +/- 7% for the SVM on the Myo, and 80.3+/- 7% versus 67.8 +/- 9% for the Delsys system, demonstrating the superior effectiveness of the proposed compact network, which had just 5,889 parameters - orders of magnitude less than some contemporary alternatives in this domain while maintaining better performance.



### Variational learning across domains with triplet information
- **Arxiv ID**: http://arxiv.org/abs/1806.08672v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.08672v2)
- **Published**: 2018-06-22 13:58:42+00:00
- **Updated**: 2018-11-19 19:47:50+00:00
- **Authors**: Rita Kuznetsova, Oleg Bakhteev, Alexandr Ogaltsov
- **Comment**: None
- **Journal**: None
- **Summary**: The work investigates deep generative models, which allow us to use training data from one domain to build a model for another domain. We propose the Variational Bi-domain Triplet Autoencoder (VBTA) that learns a joint distribution of objects from different domains. We extend the VBTAs objective function by the relative constraints or triplets that sampled from the shared latent space across domains. In other words, we combine the deep generative models with a metric learning ideas in order to improve the final objective with the triplets information. The performance of the VBTA model is demonstrated on different tasks: image-to-image translation, bi-directional image generation and cross-lingual document classification.



### Fully Convolutional Networks and Generative Adversarial Networks Applied to Sclera Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.08722v3
- **DOI**: 10.1109/BTAS.2018.8698597
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08722v3)
- **Published**: 2018-06-22 15:16:59+00:00
- **Updated**: 2018-07-09 15:14:36+00:00
- **Authors**: Diego R. Lucio, Rayson Laroca, Evair Severo, Alceu S. Britto Jr., David Menotti
- **Comment**: Accepted for presentation at the IEEE International Conference on
  Biometrics: Theory, Applications, and Systems (BTAS) 2018
- **Journal**: None
- **Summary**: Due to the world's demand for security systems, biometrics can be seen as an important topic of research in computer vision. One of the biometric forms that has been gaining attention is the recognition based on sclera. The initial and paramount step for performing this type of recognition is the segmentation of the region of interest, i.e. the sclera. In this context, two approaches for such task based on the Fully Convolutional Network (FCN) and on Generative Adversarial Network (GAN) are introduced in this work. FCN is similar to a common convolution neural network, however the fully connected layers (i.e., the classification layers) are removed from the end of the network and the output is generated by combining the output of pooling layers from different convolutional ones. The GAN is based on the game theory, where we have two networks competing with each other to generate the best segmentation. In order to perform fair comparison with baselines and quantitative and objective evaluations of the proposed approaches, we provide to the scientific community new 1,300 manually segmented images from two databases. The experiments are performed on the UBIRIS.v2 and MICHE databases and the best performing configurations of our propositions achieved F-score's measures of 87.48% and 88.32%, respectively.



### Keypoint Transfer for Fast Whole-Body Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.08723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08723v1)
- **Published**: 2018-06-22 15:18:10+00:00
- **Updated**: 2018-06-22 15:18:10+00:00
- **Authors**: Christian Wachinger, Matthew Toews, Georg Langs, William Wells, Polina Golland
- **Comment**: Accepted for publication at IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: We introduce an approach for image segmentation based on sparse correspondences between keypoints in testing and training images. Keypoints represent automatically identified distinctive image locations, where each keypoint correspondence suggests a transformation between images. We use these correspondences to transfer label maps of entire organs from the training images to the test image. The keypoint transfer algorithm includes three steps: (i) keypoint matching, (ii) voting-based keypoint labeling, and (iii) keypoint-based probabilistic transfer of organ segmentations. We report segmentation results for abdominal organs in whole-body CT and MRI, as well as in contrast-enhanced CT and MRI. Our method offers a speed-up of about three orders of magnitude in comparison to common multi-atlas segmentation, while achieving an accuracy that compares favorably. Moreover, keypoint transfer does not require the registration to an atlas or a training phase. Finally, the method allows for the segmentation of scans with highly variable field-of-view.



### Scalable Simple Linear Iterative Clustering (SSLIC) Using a Generic and Parallel Approach
- **Arxiv ID**: http://arxiv.org/abs/1806.08741v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08741v2)
- **Published**: 2018-06-22 16:02:44+00:00
- **Updated**: 2018-07-23 15:43:35+00:00
- **Authors**: Bradley C. Lowekamp, David T. Chen, Ziv Yaniv, Terry S. Yoo
- **Comment**: manuscript submitted to InsightJournal (ITK)
- **Journal**: None
- **Summary**: Superpixel algorithms have proven to be a useful initial step for segmentation and subsequent processing of images, reducing computational complexity by replacing the use of expensive per-pixel primitives with a higher-level abstraction, superpixels. They have been successfully applied both in the context of traditional image analysis and deep learning based approaches. In this work, we present a generalized implementation of the simple linear iterative clustering (SLIC) superpixel algorithm that has been generalized for n-dimensional scalar and multi-channel images. Additionally, the standard iterative implementation is replaced by a parallel, multi-threaded one. We describe the implementation details and analyze its scalability using a strong scaling formulation. Quantitative evaluation is performed using a 3D image, the Visible Human cryosection dataset, and a 2D image from the same dataset. Results show good scalability with runtime gains even when using a large number of threads that exceeds the physical number of available cores (hyperthreading).



### Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1806.08756v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.08756v2)
- **Published**: 2018-06-22 16:38:01+00:00
- **Updated**: 2018-09-07 17:53:32+00:00
- **Authors**: Peter R. Florence, Lucas Manuelli, Russ Tedrake
- **Comment**: None
- **Journal**: None
- **Summary**: What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.



### Augmented Reality-based Feedback for Technician-in-the-loop C-arm Repositioning
- **Arxiv ID**: http://arxiv.org/abs/1806.08814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08814v1)
- **Published**: 2018-06-22 18:34:48+00:00
- **Updated**: 2018-06-22 18:34:48+00:00
- **Authors**: Mathias Unberath, Javad Fotouhi, Jonas Hajek, Andreas Maier, Greg Osgood, Russell Taylor, Mehran Armand, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Interventional C-arm imaging is crucial to percutaneous orthopedic procedures as it enables the surgeon to monitor the progress of surgery on the anatomy level. Minimally invasive interventions require repeated acquisition of X-ray images from different anatomical views to verify tool placement. Achieving and reproducing these views often comes at the cost of increased surgical time and radiation dose to both patient and staff. This work proposes a marker-free "technician-in-the-loop" Augmented Reality (AR) solution for C-arm repositioning. The X-ray technician operating the C-arm interventionally is equipped with a head-mounted display capable of recording desired C-arm poses in 3D via an integrated infrared sensor. For C-arm repositioning to a particular target view, the recorded C-arm pose is restored as a virtual object and visualized in an AR environment, serving as a perceptual reference for the technician. We conduct experiments in a setting simulating orthopedic trauma surgery. Our proof-of-principle findings indicate that the proposed system can decrease the 2.76 X-ray images required per desired view down to zero, suggesting substantial reductions of radiation dose during C-arm repositioning. The proposed AR solution is a first step towards facilitating communication between the surgeon and the surgical staff, improving the quality of surgical image acquisition, and enabling context-aware guidance for surgery rooms of the future. The concept of technician-in-the-loop design will become relevant to various interventions considering the expected advancements of sensing and wearable computing in the near future.



### Multi-Task Handwritten Document Layout Analysis
- **Arxiv ID**: http://arxiv.org/abs/1806.08852v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08852v3)
- **Published**: 2018-06-22 21:00:07+00:00
- **Updated**: 2018-12-12 15:07:40+00:00
- **Authors**: Lorenzo Quir√≥s
- **Comment**: None
- **Journal**: None
- **Summary**: Document Layout Analysis is a fundamental step in Handwritten Text Processing systems, from the extraction of the text lines to the type of zone it belongs to. We present a system based on artificial neural networks which is able to determine not only the baselines of text lines present in the document, but also performs geometric and logic layout analysis of the document. Experiments in three different datasets demonstrate the potential of the method and show competitive results with respect to state-of-the-art methods.



### RUC+CMU: System Report for Dense Captioning Events in Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.08854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08854v1)
- **Published**: 2018-06-22 21:03:47+00:00
- **Updated**: 2018-06-22 21:03:47+00:00
- **Authors**: Shizhe Chen, Yuqing Song, Yida Zhao, Jiarong Qiu, Qin Jin, Alexander Hauptmann
- **Comment**: Winner in ActivityNet 2018 Dense Video Captioning challenge
- **Journal**: None
- **Summary**: This notebook paper presents our system in the ActivityNet Dense Captioning in Video task (task 3). Temporal proposal generation and caption generation are both important to the dense captioning task. Therefore, we propose a proposal ranking model to employ a set of effective feature representations for proposal generation, and ensemble a series of caption models enhanced with context information to generate captions robustly on predicted proposals. Our approach achieves the state-of-the-art performance on the dense video captioning task with 8.529 METEOR score on the challenge testing set.



### A deep learning framework for segmentation of retinal layers from OCT images
- **Arxiv ID**: http://arxiv.org/abs/1806.08859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08859v1)
- **Published**: 2018-06-22 21:24:58+00:00
- **Updated**: 2018-06-22 21:24:58+00:00
- **Authors**: Karthik Gopinath, Samrudhdhi B Rangrej, Jayanthi Sivaswamy
- **Comment**: Accepted in The 4th Asian Conference on Pattern Recognition (ACPR
  2017)
- **Journal**: None
- **Summary**: Segmentation of retinal layers from Optical Coherence Tomography (OCT) volumes is a fundamental problem for any computer aided diagnostic algorithm development. This requires preprocessing steps such as denoising, region of interest extraction, flattening and edge detection all of which involve separate parameter tuning. In this paper, we explore deep learning techniques to automate all these steps and handle the presence/absence of pathologies. A model is proposed consisting of a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM). The CNN is used to extract layers of interest image and extract the edges, while the LSTM is used to trace the layer boundary. This model is trained on a mixture of normal and AMD cases using minimal data. Validation results on three public datasets show that the pixel-wise mean absolute error obtained with our system is 1.30 plus or minus 0.48 which is lower than the inter-marker error of 1.79 plus or minus 0.76. Our model's performance is also on par with the existing methods.



### VUNet: Dynamic Scene View Synthesis for Traversability Estimation using an RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/1806.08864v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.08864v2)
- **Published**: 2018-06-22 21:40:10+00:00
- **Updated**: 2019-01-10 20:05:27+00:00
- **Authors**: Noriaki Hirose, Amir Sadeghian, Fei Xia, Roberto Martin-Martin, Silvio Savarese
- **Comment**: website: http://svl.stanford.edu/projects/vunet/
- **Journal**: IEEE Robotics and Automation Letters 2019
- **Summary**: We present VUNet, a novel view(VU) synthesis method for mobile robots in dynamic environments, and its application to the estimation of future traversability. Our method predicts future images for given virtual robot velocity commands using only RGB images at previous and current time steps. The future images result from applying two types of image changes to the previous and current images: 1) changes caused by different camera pose, and 2) changes due to the motion of the dynamic obstacles. We learn to predict these two types of changes disjointly using two novel network architectures, SNet and DNet. We combine SNet and DNet to synthesize future images that we pass to our previously presented method GONet to estimate the traversable areas around the robot. Our quantitative and qualitative evaluation indicate that our approach for view synthesis predicts accurate future images in both static and dynamic environments. We also show that these virtual images can be used to estimate future traversability correctly. We apply our view synthesis-based traversability estimation method to two applications for assisted teleoperation.



### Deep Semantic Segmentation in an AUV for Online Posidonia Oceanica Meadows identification
- **Arxiv ID**: http://arxiv.org/abs/1807.03117v2
- **DOI**: 10.1109/ACCESS.2018.2875412
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03117v2)
- **Published**: 2018-06-22 22:21:51+00:00
- **Updated**: 2018-12-11 19:46:42+00:00
- **Authors**: Miguel Martin-Abadal, Eric Guerrero-Font, Francisco Bonin-Font, Yolanda Gonzalez-Cid
- **Comment**: 11 pages, 16 figures
- **Journal**: None
- **Summary**: Recent studies have shown evidence of a significant decline of the Posidonia oceanica (P.O.) meadows on a global scale. The monitoring and mapping of these meadows are fundamental tools for measuring their status. We present an approach based on a deep neural network to automatically perform a high-precision semantic segmentation of P.O. meadows in sea-floor images, offering several improvements over the state of the art techniques. Our network demonstrates outstanding performance over diverse test sets, reaching a precision of 96.57% and an accuracy of 96.81%, surpassing the reliability of labelling the images manually. Also, the network is implemented in an Autonomous Underwater Vehicle (AUV), performing an online P.O. segmentation, which will be used to generate real-time semantic coverage maps.




# Arxiv Papers in cs.CV on 2018-06-16
### Object Level Visual Reasoning in Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.06157v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06157v3)
- **Published**: 2018-06-16 00:33:50+00:00
- **Updated**: 2018-09-20 08:59:32+00:00
- **Authors**: Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille, Greg Mori
- **Comment**: Accepted at ECCV 2018 - long version (16 pages + ref)
- **Journal**: ECCV 2018
- **Summary**: Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatiotemporal interactions in videos. The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them. Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.



### Semantic Video Segmentation: A Review on Recent Approaches
- **Arxiv ID**: http://arxiv.org/abs/1806.06172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06172v1)
- **Published**: 2018-06-16 02:31:00+00:00
- **Updated**: 2018-06-16 02:31:00+00:00
- **Authors**: Mohammad Hajizadeh Saffar, Mohsen Fayyaz, Mohammad Sabokrou, Mahmood Fathy
- **Comment**: None
- **Journal**: None
- **Summary**: This paper gives an overview on semantic segmentation consists of an explanation of this field, it's status and relation with other vision fundamental tasks, different datasets and common evaluation parameters that have been used by researchers. This survey also includes an overall review on a variety of recent approaches (RDF, MRF, CRF, etc.) and their advantages and challenges and shows the superiority of CNN-based semantic segmentation systems on CamVid and NYUDv2 datasets. In addition, some areas that is ideal for future work have mentioned.



### Learning Factorized Multimodal Representations
- **Arxiv ID**: http://arxiv.org/abs/1806.06176v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.06176v3)
- **Published**: 2018-06-16 03:48:50+00:00
- **Updated**: 2019-05-14 14:16:40+00:00
- **Authors**: Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, Ruslan Salakhutdinov
- **Comment**: ICLR 2019
- **Journal**: None
- **Summary**: Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning.



### Riemannian kernel based Nyström method for approximate infinite-dimensional covariance descriptors with application to image set classification
- **Arxiv ID**: http://arxiv.org/abs/1806.06177v2
- **DOI**: 10.1109/ICPR.2018.8545822
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06177v2)
- **Published**: 2018-06-16 04:28:20+00:00
- **Updated**: 2019-09-01 07:53:42+00:00
- **Authors**: Kai-Xuan Chen, Xiao-Jun Wu, Rui Wang, Josef Kittler
- **Comment**: 6 pages, 3 figures, International Conference on Pattern Recognition
  2018
- **Journal**: None
- **Summary**: In the domain of pattern recognition, using the CovDs (Covariance Descriptors) to represent data and taking the metrics of the resulting Riemannian manifold into account have been widely adopted for the task of image set classification. Recently, it has been proven that infinite-dimensional CovDs are more discriminative than their low-dimensional counterparts. However, the form of infinite-dimensional CovDs is implicit and the computational load is high. We propose a novel framework for representing image sets by approximating infinite-dimensional CovDs in the paradigm of the Nystr\"om method based on a Riemannian kernel. We start by modeling the images via CovDs, which lie on the Riemannian manifold spanned by SPD (Symmetric Positive Definite) matrices. We then extend the Nystr\"om method to the SPD manifold and obtain the approximations of CovDs in RKHS (Reproducing Kernel Hilbert Space). Finally, we approximate infinite-dimensional CovDs via these approximations. Empirically, we apply our framework to the task of image set classification. The experimental results obtained on three benchmark datasets show that our proposed approximate infinite-dimensional CovDs outperform the original CovDs.



### Component SPD Matrices: A lower-dimensional discriminative data descriptor for image set classification
- **Arxiv ID**: http://arxiv.org/abs/1806.06178v1
- **DOI**: 10.1007/s41095-018-0119-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06178v1)
- **Published**: 2018-06-16 04:31:59+00:00
- **Updated**: 2018-06-16 04:31:59+00:00
- **Authors**: Kai-Xuan Chen, Xiao-Jun Wu
- **Comment**: 8 pages,5 figures, Computational Visual Media, 2018
- **Journal**: None
- **Summary**: In the domain of pattern recognition, using the SPD (Symmetric Positive Definite) matrices to represent data and taking the metrics of resulting Riemannian manifold into account have been widely used for the task of image set classification. In this paper, we propose a new data representation framework for image sets named CSPD (Component Symmetric Positive Definite). Firstly, we obtain sub-image sets by dividing the image set into square blocks with the same size, and use traditional SPD model to describe them. Then, we use the results of the Riemannian kernel on SPD matrices as similarities of corresponding sub-image sets. Finally, the CSPD matrix appears in the form of the kernel matrix for all the sub-image sets, and CSPDi,j denotes the similarity between i-th sub-image set and j-th sub-image set. Here, the Riemannian kernel is shown to satisfy the Mercer's theorem, so our proposed CSPD matrix is symmetric and positive definite and also lies on a Riemannian manifold. On three benchmark datasets, experimental results show that CSPD is a lower-dimensional and more discriminative data descriptor for the task of image set classification.



### The Neural Painter: Multi-Turn Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1806.06183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06183v1)
- **Published**: 2018-06-16 04:52:03+00:00
- **Updated**: 2018-06-16 04:52:03+00:00
- **Authors**: Ryan Y. Benmalek, Claire Cardie, Serge Belongie, Xiadong He, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we combine two research threads from Vision/ Graphics and Natural Language Processing to formulate an image generation task conditioned on attributes in a multi-turn setting. By multiturn, we mean the image is generated in a series of steps of user-specified conditioning information. Our proposed approach is practically useful and offers insights into neural interpretability. We introduce a framework that includes a novel training algorithm as well as model improvements built for the multi-turn setting. We demonstrate that this framework generates a sequence of images that match the given conditioning information and that this task is useful for more detailed benchmarking and analysis of conditional image generation methods.



### Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.06193v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.06193v1)
- **Published**: 2018-06-16 06:11:06+00:00
- **Updated**: 2018-06-16 06:11:06+00:00
- **Authors**: Yin Cui, Yang Song, Chen Sun, Andrew Howard, Serge Belongie
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make and model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain-specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.



### Show, Attend and Translate: Unsupervised Image Translation with Self-Regularization and Attention
- **Arxiv ID**: http://arxiv.org/abs/1806.06195v3
- **DOI**: 10.1109/TIP.2019.2914583
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06195v3)
- **Published**: 2018-06-16 07:02:47+00:00
- **Updated**: 2019-06-04 20:59:05+00:00
- **Authors**: Chao Yang, Taehwan Kim, Ruizhe Wang, Hao Peng, C. -C. Jay Kuo
- **Comment**: Accepted at IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Image translation between two domains is a class of problems aiming to learn mapping from an input image in the source domain to an output image in the target domain. It has been applied to numerous domains, such as data augmentation, domain adaptation, and unsupervised training. When paired training data is not accessible, image translation becomes an ill-posed problem. We constrain the problem with the assumption that the translated image needs to be perceptually similar to the original image and also appears to be drawn from the new domain, and propose a simple yet effective image translation model consisting of a single generator trained with a self-regularization term and an adversarial term. We further notice that existing image translation techniques are agnostic to the subjects of interest and often introduce unwanted changes or artifacts to the input. Thus we propose to add an attention module to predict an attention map to guide the image translation process. The module learns to attend to key parts of the image while keeping everything else unaltered, essentially avoiding undesired artifacts or changes. The predicted attention map also opens door to applications such as unsupervised segmentation and saliency detection. Extensive experiments and evaluations show that our model while being simpler, achieves significantly better performance than existing image translation methods.



### Part-Aware Fine-grained Object Categorization using Weakly Supervised Part Detection Network
- **Arxiv ID**: http://arxiv.org/abs/1806.06198v2
- **DOI**: 10.1109/TMM.2019.2939747
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06198v2)
- **Published**: 2018-06-16 07:08:59+00:00
- **Updated**: 2019-12-04 14:32:09+00:00
- **Authors**: Yabin Zhang, Kui Jia, Zhixin Wang
- **Comment**: TMM paper version. Codes are available at:
  https://github.com/YBZh/PartNet
- **Journal**: IEEE Transactions on Multimedia, 2019
- **Summary**: Fine-grained object categorization aims for distinguishing objects of subordinate categories that belong to the same entry-level object category. The task is challenging due to the facts that (1) training images with ground-truth labels are difficult to obtain, and (2) variations among different subordinate categories are subtle. It is well established that characterizing features of different subordinate categories are located on local parts of object instances. In fact, careful part annotations are available in many fine-grained categorization datasets. However, manually annotating object parts requires expertise, which is also difficult to generalize to new fine-grained categorization tasks. In this work, we propose a Weakly Supervised Part Detection Network (PartNet) that is able to detect discriminative local parts for use of fine-grained categorization. A vanilla PartNet builds on top of a base subnetwork two parallel streams of upper network layers, which respectively compute scores of classification probabilities (over subordinate categories) and detection probabilities (over a specified number of discriminative part detectors) for local regions of interest (RoIs). The image-level prediction is obtained by aggregating element-wise products of these region-level probabilities. To generate a diverse set of RoIs as inputs of PartNet, we propose a simple Discretized Part Proposals module (DPP) that directly targets for proposing candidates of discriminative local parts, with no bridging via object-level proposals. Experiments on the benchmark CUB-200-2011 and Oxford Flower 102 datasets show the efficacy of our proposed method for both discriminative part detection and fine-grained categorization. In particular, we achieve the new state-of-the-art performance on CUB-200-2011 dataset when ground-truth part annotations are not available.



### G2D: from GTA to Data
- **Arxiv ID**: http://arxiv.org/abs/1806.07381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07381v1)
- **Published**: 2018-06-16 08:24:03+00:00
- **Updated**: 2018-06-16 08:24:03+00:00
- **Authors**: Anh-Dzung Doan, Abdul Mohsi Jawaid, Thanh-Toan Do, Tat-Jun Chin
- **Comment**: 9 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: This document describes G2D, a software that enables capturing videos from Grand Theft Auto V (GTA V), a popular role playing game set in an expansive virtual city. The target users of our software are computer vision researchers who wish to collect hyper-realistic computer-generated imagery of a city from the street level, under controlled 6DOF camera poses and varying environmental conditions (weather, season, time of day, traffic density, etc.).   G2D accesses/calls the native functions of the game; hence users can directly interact with G2D while playing the game. Specifically, G2D enables users to manipulate conditions of the virtual environment on the fly, while the gameplay camera is set to automatically retrace a predetermined 6DOF camera pose trajectory within the game coordinate system. Concurrently, automatic screen capture is executed while the virtual environment is being explored. G2D and its source code are publicly available at https://goo.gl/SS7fS6   In addition, we demonstrate an application of G2D to generate a large-scale dataset with groundtruth camera poses for testing structure-from-motion (SfM) algorithms. The dataset and generated 3D point clouds are also made available at https://goo.gl/DNzxHx



### Offline Extraction of Indic Regional Language from Natural Scene Image using Text Segmentation and Deep Convolutional Sequence
- **Arxiv ID**: http://arxiv.org/abs/1806.06208v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1806.06208v2)
- **Published**: 2018-06-16 08:31:06+00:00
- **Updated**: 2018-07-06 20:10:03+00:00
- **Authors**: Sauradip Nag, Pallab Kumar Ganguly, Sumit Roy, Sourab Jha, Krishna Bose, Abhishek Jha, Kousik Dasgupta
- **Comment**: Accepted in Second International Conference on Computational
  Intelligence, Communications, and Business Analytics (CICBA-2018)
- **Journal**: None
- **Summary**: Regional language extraction from a natural scene image is always a challenging proposition due to its dependence on the text information extracted from Image. Text Extraction on the other hand varies on different lighting condition, arbitrary orientation, inadequate text information, heavy background influence over text and change of text appearance. This paper presents a novel unified method for tackling the above challenges. The proposed work uses an image correction and segmentation technique on the existing Text Detection Pipeline an Efficient and Accurate Scene Text Detector (EAST). EAST uses standard PVAnet architecture to select features and non maximal suppression to detect text from image. Text recognition is done using combined architecture of MaxOut convolution neural network (CNN) and Bidirectional long short term memory (LSTM) network. After recognizing text using the Deep Learning based approach, the native Languages are translated to English and tokenized using standard Text Tokenizers. The tokens that very likely represent a location is used to find the Global Positioning System (GPS) coordinates of the location and subsequently the regional languages spoken in that location is extracted. The proposed method is tested on a self generated dataset collected from Government of India dataset and experimented on Standard Dataset to evaluate the performance of the proposed technique. Comparative study with a few state-of-the-art methods on text detection, recognition and extraction of regional language from images shows that the proposed method outperforms the existing methods.



### Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling
- **Arxiv ID**: http://arxiv.org/abs/1806.06228v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.06228v1)
- **Published**: 2018-06-16 12:05:24+00:00
- **Updated**: 2018-06-16 12:05:24+00:00
- **Authors**: N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria, S. Poria
- **Comment**: Accepted for publication at Knowledge Based Systems
- **Journal**: None
- **Summary**: Multimodal sentiment analysis is a very actively growing field of research. A promising area of opportunity in this field is to improve the multimodal fusion mechanism. We present a novel feature fusion strategy that proceeds in a hierarchical fashion, first fusing the modalities two in two and only then fusing all three modalities. On multimodal sentiment analysis of individual utterances, our strategy outperforms conventional concatenation of features by 1%, which amounts to 5% reduction in error rate. On utterance-level multimodal sentiment analysis of multi-utterance video clips, for which current state-of-the-art techniques incorporate contextual information from other utterances of the same clip, our hierarchical fusion gives up to 2.4% (almost 10% error rate reduction) over currently used concatenation. The implementation of our method is publicly available in the form of open-source code.



### Real-time Prediction of Segmentation Quality
- **Arxiv ID**: http://arxiv.org/abs/1806.06244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06244v1)
- **Published**: 2018-06-16 13:53:31+00:00
- **Updated**: 2018-06-16 13:53:31+00:00
- **Authors**: Robert Robinson, Ozan Oktay, Wenjia Bai, Vanya Valindria, Mihir Sanghvi, Nay Aung, José Paiva, Filip Zemrak, Kenneth Fung, Elena Lukaschuk, Aaron Lee, Valentina Carapella, Young Jin Kim, Bernhard Kainz, Stefan Piechnik, Stefan Neubauer, Steffen Petersen, Chris Page, Daniel Rueckert, Ben Glocker
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Recent advances in deep learning based image segmentation methods have enabled real-time performance with human-level accuracy. However, occasionally even the best method fails due to low image quality, artifacts or unexpected behaviour of black box algorithms. Being able to predict segmentation quality in the absence of ground truth is of paramount importance in clinical practice, but also in large-scale studies to avoid the inclusion of invalid data in subsequent analysis.   In this work, we propose two approaches of real-time automated quality control for cardiovascular MR segmentations using deep learning. First, we train a neural network on 12,880 samples to predict Dice Similarity Coefficients (DSC) on a per-case basis. We report a mean average error (MAE) of 0.03 on 1,610 test samples and 97% binary classification accuracy for separating low and high quality segmentations. Secondly, in the scenario where no manually annotated data is available, we train a network to predict DSC scores from estimated quality obtained via a reverse testing strategy. We report an MAE=0.14 and 91% binary classification accuracy for this case. Predictions are obtained in real-time which, when combined with real-time segmentation methods, enables instant feedback on whether an acquired scan is analysable while the patient is still in the scanner. This further enables new applications of optimising image acquisition towards best possible analysis results.



### Two Stream Self-Supervised Learning for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.07383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07383v1)
- **Published**: 2018-06-16 17:51:35+00:00
- **Updated**: 2018-06-16 17:51:35+00:00
- **Authors**: Ahmed Taha, Moustafa Meshry, Xitong Yang, Yi-Ting Chen, Larry Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We present a self-supervised approach using spatio-temporal signals between video frames for action recognition. A two-stream architecture is leveraged to tangle spatial and temporal representation learning. Our task is formulated as both a sequence verification and spatio-temporal alignment tasks. The former task requires motion temporal structure understanding while the latter couples the learned motion with the spatial representation. The self-supervised pre-trained weights effectiveness is validated on the action recognition task. Quantitative evaluation shows the self-supervised approach competence on three datasets: HMDB51, UCF101, and Honda driving dataset (HDD). Further investigations to boost performance and generalize validity are still required.



### Latent Convolutional Models
- **Arxiv ID**: http://arxiv.org/abs/1806.06284v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.06284v2)
- **Published**: 2018-06-16 19:31:32+00:00
- **Updated**: 2018-11-02 04:35:49+00:00
- **Authors**: ShahRukh Athar, Evgeny Burnaev, Victor Lempitsky
- **Comment**: Updated with more recent experiments
- **Journal**: None
- **Summary**: We present a new latent model of natural images that can be learned on large-scale datasets. The learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space. After training, the new model provides a strong and universal image prior for a variety of image restoration tasks such as large-hole inpainting, superresolution, and colorization. To model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models). To tackle this high dimensionality, we use latent spaces with a special manifold structure (convolutional manifolds) parameterized by a ConvNet of a certain architecture. In the experiments, we compare the learned latent models with latent models learned by autoencoders, advanced variants of generative adversarial networks, and a strong baseline system using simpler parameterization of the latent space. Our model outperforms the competing approaches over a range of restoration tasks.



### Right for the Right Reason: Training Agnostic Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.06296v1
- **DOI**: 10.1007/978-3-030-01768-2_14
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.06296v1)
- **Published**: 2018-06-16 21:09:40+00:00
- **Updated**: 2018-06-16 21:09:40+00:00
- **Authors**: Sen Jia, Thomas Lansdall-Welfare, Nello Cristianini
- **Comment**: Author's original version
- **Journal**: None
- **Summary**: We consider the problem of a neural network being requested to classify images (or other inputs) without making implicit use of a "protected concept", that is a concept that should not play any role in the decision of the network. Typically these concepts include information such as gender or race, or other contextual information such as image backgrounds that might be implicitly reflected in unknown correlations with other variables, making it insufficient to simply remove them from the input features. In other words, making accurate predictions is not good enough if those predictions rely on information that should not be used: predictive performance is not the only important metric for learning systems. We apply a method developed in the context of domain adaptation to address this problem of "being right for the right reason", where we request a classifier to make a decision in a way that is entirely 'agnostic' to a given protected concept (e.g. gender, race, background etc.), even if this could be implicitly reflected in other attributes via unknown correlations. After defining the concept of an 'agnostic model', we demonstrate how the Domain-Adversarial Neural Network can remove unwanted information from a model using a gradient reversal layer.



### Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry
- **Arxiv ID**: http://arxiv.org/abs/1806.06298v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.06298v3)
- **Published**: 2018-06-16 21:17:02+00:00
- **Updated**: 2020-01-14 01:26:23+00:00
- **Authors**: Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, Ying Nian Wu
- **Comment**: version 3
- **Journal**: None
- **Summary**: We present a deformable generator model to disentangle the appearance and geometric information for both image and video data in a purely unsupervised manner. The appearance generator network models the information related to appearance, including color, illumination, identity or category, while the geometric generator performs geometric warping, such as rotation and stretching, through generating deformation field which is used to warp the generated appearance to obtain the final image or video sequences. Two generators take independent latent vectors as input to disentangle the appearance and geometric information from image or video sequences. For video data, a nonlinear transition model is introduced to both the appearance and geometric generators to capture the dynamics over time. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric information can be well disentangled, and the learned geometric generator can be conveniently transferred to other image datasets to facilitate knowledge transfer tasks.



### In situ TensorView: In situ Visualization of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.07382v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07382v1)
- **Published**: 2018-06-16 22:51:12+00:00
- **Updated**: 2018-06-16 22:51:12+00:00
- **Authors**: Xinyu Chen, Qiang Guan, Li-Ta Lo, Simon Su, James Ahrens, Trilce Estrada
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks(CNNs) are complex systems. They are trained so they can adapt their internal connections to recognize images, texts and more. It is both interesting and helpful to visualize the dynamics within such deep artificial neural networks so that people can understand how these artificial networks are learning and making predictions. In the field of scientific simulations, visualization tools like Paraview have long been utilized to provide insights and understandings. We present in situ TensorView to visualize the training and functioning of CNNs as if they are systems of scientific simulations. In situ TensorView is a loosely coupled in situ visualization open framework that provides multiple viewers to help users to visualize and understand their networks. It leverages the capability of co-processing from Paraview to provide real-time visualization during training and predicting phases. This avoid heavy I/O overhead for visualizing large dynamic systems. Only a small number of lines of codes are injected in TensorFlow framework. The visualization can provide guidance to adjust the architecture of networks, or compress the pre-trained networks. We showcase visualizing the training of LeNet-5 and VGG16 using in situ TensorView.




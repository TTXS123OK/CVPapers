# Arxiv Papers in cs.CV on 2018-06-18
### Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance
- **Arxiv ID**: http://arxiv.org/abs/1806.06503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06503v1)
- **Published**: 2018-06-18 05:49:59+00:00
- **Updated**: 2018-06-18 05:49:59+00:00
- **Authors**: Zhixin Shu, Mihir Sahasrabudhe, Alp Guler, Dimitris Samaras, Nikos Paragios, Iasonas Kokkinos
- **Comment**: 17 pages including references, plus 12 pages appendix. Video
  available at : https://youtu.be/Oi7pyxKkF1g Code will be made available soon
- **Journal**: None
- **Summary**: In this work we introduce Deforming Autoencoders, a generative model for images that disentangles shape from appearance in an unsupervised manner. As in the deformable template paradigm, shape is represented as a deformation between a canonical coordinate system (`template') and an observed image, while appearance is modeled in `canonical', template, coordinates, thus discarding variability due to deformations. We introduce novel techniques that allow this approach to be deployed in the setting of autoencoders and show that this method can be used for unsupervised group-wise image alignment. We show experiments with expression morphing in humans, hands, and digits, face manipulation, such as shape and appearance interpolation, as well as unsupervised landmark localization. A more powerful form of unsupervised disentangling becomes possible in template coordinates, allowing us to successfully decompose face images into shading and albedo, and further manipulate face images.



### An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.06506v2
- **DOI**: 10.21437/Interspeech.2018-2413
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.06506v2)
- **Published**: 2018-06-18 06:04:12+00:00
- **Updated**: 2018-10-07 05:53:06+00:00
- **Authors**: Ahmed Imtiaz Humayun, Md. Tauhiduzzaman Khan, Shabnam Ghaffarzadegan, Zhe Feng, Taufiq Hasan
- **Comment**: 5 pages, 5 figures, Interspeech 2018 accepted manuscript
- **Journal**: None
- **Summary**: In this work, we propose an ensemble of classifiers to distinguish between various degrees of abnormalities of the heart using Phonocardiogram (PCG) signals acquired using digital stethoscopes in a clinical setting, for the INTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats SubChallenge. Our primary classification framework constitutes a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which uses features transferred from a model trained on the 2016 Physionet Heart Sound Database. We also employ a Representation Learning (RL) approach to generate features in an unsupervised manner using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we utilize an SVM classifier on a high-dimensional segment-level feature extracted using various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD). An ensemble of the three different approaches provides a relative improvement of 11.13% compared to our best single sub-system in terms of the Unweighted Average Recall (UAR) performance metric on the evaluation dataset.



### HitNet: a neural network with capsules embedded in a Hit-or-Miss layer, extended with hybrid data augmentation and ghost capsules
- **Arxiv ID**: http://arxiv.org/abs/1806.06519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.06519v1)
- **Published**: 2018-06-18 07:08:11+00:00
- **Updated**: 2018-06-18 07:08:11+00:00
- **Authors**: Adrien Deliège, Anthony Cioppa, Marc Van Droogenbroeck
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks designed for the task of classification have become a commodity in recent years. Many works target the development of better networks, which results in a complexification of their architectures with more layers, multiple sub-networks, or even the combination of multiple classifiers. In this paper, we show how to redesign a simple network to reach excellent performances, which are better than the results reproduced with CapsNet on several datasets, by replacing a layer with a Hit-or-Miss layer. This layer contains activated vectors, called capsules, that we train to hit or miss a central capsule by tailoring a specific centripetal loss function. We also show how our network, named HitNet, is capable of synthesizing a representative sample of the images of a given class by including a reconstruction network. This possibility allows to develop a data augmentation step combining information from the data space and the feature space, resulting in a hybrid data augmentation process. In addition, we introduce the possibility for HitNet, to adopt an alternative to the true target when needed by using the new concept of ghost capsules, which is used here to detect potentially mislabeled images in the training data.



### Segmentation of Photovoltaic Module Cells in Uncalibrated Electroluminescence Images
- **Arxiv ID**: http://arxiv.org/abs/1806.06530v4
- **DOI**: 10.1007/s00138-021-01191-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06530v4)
- **Published**: 2018-06-18 07:38:55+00:00
- **Updated**: 2021-05-24 20:46:34+00:00
- **Authors**: Sergiu Deitsch, Claudia Buerhop-Lutz, Evgenii Sovetkin, Ansgar Steland, Andreas Maier, Florian Gallwitz, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: High resolution electroluminescence (EL) images captured in the infrared spectrum allow to visually and non-destructively inspect the quality of photovoltaic (PV) modules. Currently, however, such a visual inspection requires trained experts to discern different kinds of defects, which is time-consuming and expensive. Automated segmentation of cells is therefore a key step in automating the visual inspection workflow. In this work, we propose a robust automated segmentation method for extraction of individual solar cells from EL images of PV modules. This enables controlled studies on large amounts of data to understanding the effects of module degradation over time-a process not yet fully understood. The proposed method infers in several steps a high-level solar module representation from low-level edge features. An important step in the algorithm is to formulate the segmentation problem in terms of lens calibration by exploiting the plumbline constraint. We evaluate our method on a dataset of various solar modules types containing a total of 408 solar cells with various defects. Our method robustly solves this task with a median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both indicating a very high similarity between automatically segmented and ground truth solar cell masks.



### RenderNet: A deep convolutional network for differentiable rendering from 3D shapes
- **Arxiv ID**: http://arxiv.org/abs/1806.06575v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06575v3)
- **Published**: 2018-06-18 09:45:33+00:00
- **Updated**: 2019-04-01 16:24:00+00:00
- **Authors**: Thu Nguyen-Phuoc, Chuan Li, Stephen Balaban, Yong-Liang Yang
- **Comment**: 14 pages, 9 figures
- **Journal**: 32nd Conference on Neural Information Processing Systems (NeurIPS
  2018)
- **Summary**: Traditional computer graphics rendering pipeline is designed for procedurally generating 2D quality images from 3D shapes with high performance. The non-differentiability due to discrete operations such as visibility computation makes it hard to explicitly correlate rendering parameters and the resulting image, posing a significant challenge for inverse rendering tasks. Recent work on differentiable rendering achieves differentiability either by designing surrogate gradients for non-differentiable operations or via an approximate but differentiable renderer. These methods, however, are still limited when it comes to handling occlusion, and restricted to particular rendering effects. We present RenderNet, a differentiable rendering convolutional network with a novel projection unit that can render 2D images from 3D shapes. Spatial occlusion and shading calculation are automatically encoded in the network. Our experiments show that RenderNet can successfully learn to implement different shaders, and can be used in inverse rendering tasks to estimate shape, pose, lighting and texture from a single image.



### Deep Recurrent Neural Network for Multi-target Filtering
- **Arxiv ID**: http://arxiv.org/abs/1806.06594v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06594v2)
- **Published**: 2018-06-18 10:47:14+00:00
- **Updated**: 2018-10-08 12:45:19+00:00
- **Authors**: Mehryar Emambakhsh, Alessandro Bay, Eduard Vazquez
- **Comment**: The 25th International Conference on MultiMedia Modeling (MMM)
- **Journal**: None
- **Summary**: This paper addresses the problem of fixed motion and measurement models for multi-target filtering using an adaptive learning framework. This is performed by defining target tuples with random finite set terminology and utilisation of recurrent neural networks with a long short-term memory architecture. A novel data association algorithm compatible with the predicted tracklet tuples is proposed, enabling the update of occluded targets, in addition to assigning birth, survival and death of targets. The algorithm is evaluated over a commonly used filtering simulation scenario, with highly promising results.



### Uncertainty in multitask learning: joint representations for probabilistic MR-only radiotherapy planning
- **Arxiv ID**: http://arxiv.org/abs/1806.06595v1
- **DOI**: 10.1007/978-3-030-00937-3_1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06595v1)
- **Published**: 2018-06-18 10:56:12+00:00
- **Updated**: 2018-06-18 10:56:12+00:00
- **Authors**: Felix J. S. Bragman, Ryutaro Tanno, Zach Eaton-Rosen, Wenqi Li, David J. Hawkes, Sebastien Ourselin, Daniel C. Alexander, Jamie R. McClelland, M. Jorge Cardoso
- **Comment**: Early-accept at MICCAI 2018, 8 pages, 4 figures
- **Journal**: None
- **Summary**: Multi-task neural network architectures provide a mechanism that jointly integrates information from distinct sources. It is ideal in the context of MR-only radiotherapy planning as it can jointly regress a synthetic CT (synCT) scan and segment organs-at-risk (OAR) from MRI. We propose a probabilistic multi-task network that estimates: 1) intrinsic uncertainty through a heteroscedastic noise model for spatially-adaptive task loss weighting and 2) parameter uncertainty through approximate Bayesian inference. This allows sampling of multiple segmentations and synCTs that share their network representation. We test our model on prostate cancer scans and show that it produces more accurate and consistent synCTs with a better estimation in the variance of the errors, state of the art results in OAR segmentation and a methodology for quality assurance in radiotherapy treatment planning.



### On Multi-resident Activity Recognition in Ambient Smart-Homes
- **Arxiv ID**: http://arxiv.org/abs/1806.06611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06611v1)
- **Published**: 2018-06-18 12:04:10+00:00
- **Updated**: 2018-06-18 12:04:10+00:00
- **Authors**: Son N. Tran, Qing Zhang, Mohan Karunanithi
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing attention to the research on activity monitoring in smart homes has motivated the employment of ambient intelligence to reduce the deployment cost and solve the privacy issue. Several approaches have been proposed for multi-resident activity recognition, however, there still lacks a comprehensive benchmark for future research and practical selection of models. In this paper we study different methods for multi-resident activity recognition and evaluate them on same sets of data. The experimental results show that recurrent neural network with gated recurrent units is better than other models and also considerably efficient, and that using combined activities as single labels is more effective than represent them as separate labels.



### Coupled Fluid Density and Motion from Single Views
- **Arxiv ID**: http://arxiv.org/abs/1806.06613v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, I.6.8; I.3.7; G.1.6; I.4.5; G.1.8
- **Links**: [PDF](http://arxiv.org/pdf/1806.06613v1)
- **Published**: 2018-06-18 12:05:20+00:00
- **Updated**: 2018-06-18 12:05:20+00:00
- **Authors**: Marie-Lena Eckert, Wolfgang Heidrich, Nils Thuerey
- **Comment**: Computer Graphics Forum (2018), further information:
  https://ge.in.tum.de/publications/2018-cgf-eckert/, video:
  https://www.youtube.com/watch?v=J2wkPNBJLaI
- **Journal**: None
- **Summary**: We present a novel method to reconstruct a fluid's 3D density and motion based on just a single sequence of images. This is rendered possible by using powerful physical priors for this strongly under-determined problem. More specifically, we propose a novel strategy to infer density updates strongly coupled to previous and current estimates of the flow motion. Additionally, we employ an accurate discretization and depth-based regularizers to compute stable solutions. Using only one view for the reconstruction reduces the complexity of the capturing setup drastically and could even allow for online video databases or smart-phone videos as inputs. The reconstructed 3D velocity can then be flexibly utilized, e.g., for re-simulation, domain modification or guiding purposes. We will demonstrate the capacity of our method with a series of synthetic test cases and the reconstruction of real smoke plumes captured with a Raspberry Pi camera.



### Banach Wasserstein GAN
- **Arxiv ID**: http://arxiv.org/abs/1806.06621v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/1806.06621v2)
- **Published**: 2018-06-18 12:15:43+00:00
- **Updated**: 2019-01-11 12:57:46+00:00
- **Authors**: Jonas Adler, Sebastian Lunz
- **Comment**: In NeurIPS2018. 10 pages, 9 page appendix
- **Journal**: None
- **Summary**: Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered $\ell^2$ as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.



### Assessing robustness of radiomic features by image perturbation
- **Arxiv ID**: http://arxiv.org/abs/1806.06719v1
- **DOI**: 10.1038/s41598-018-36938-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06719v1)
- **Published**: 2018-06-18 14:05:47+00:00
- **Updated**: 2018-06-18 14:05:47+00:00
- **Authors**: Alex Zwanenburg, Stefan Leger, Linda Agolli, Karoline Pilz, Esther G. C. Troost, Christian Richter, Steffen Löck
- **Comment**: 31 pages, 14 figures pre-submission version
- **Journal**: Scientific Reports (2019) 9:614
- **Summary**: Image features need to be robust against differences in positioning, acquisition and segmentation to ensure reproducibility. Radiomic models that only include robust features can be used to analyse new images, whereas models with non-robust features may fail to predict the outcome of interest accurately. Test-retest imaging is recommended to assess robustness, but may not be available for the phenotype of interest. We therefore investigated 18 methods to determine feature robustness based on image perturbations. Test-retest and perturbation robustness were compared for 4032 features that were computed from the gross tumour volume in two cohorts with computed tomography imaging: I) 31 non-small-cell lung cancer (NSCLC) patients; II): 19 head-and-neck squamous cell carcinoma (HNSCC) patients. Robustness was measured using the intraclass correlation coefficient (1,1) (ICC). Features with ICC$\geq0.90$ were considered robust. The NSCLC cohort contained more robust features for test-retest imaging than the HNSCC cohort ($73.5\%$ vs. $34.0\%$). A perturbation chain consisting of noise addition, affine translation, volume growth/shrinkage and supervoxel-based contour randomisation identified the fewest false positive robust features (NSCLC: $3.3\%$; HNSCC: $10.0\%$). Thus, this perturbation chain may be used to assess feature robustness.



### Kid-Net: Convolution Networks for Kidney Vessels Segmentation from CT-Volumes
- **Arxiv ID**: http://arxiv.org/abs/1806.06769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06769v1)
- **Published**: 2018-06-18 15:25:07+00:00
- **Updated**: 2018-06-18 15:25:07+00:00
- **Authors**: Ahmed Taha, Pechin Lo, Junning Li, Tao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image segmentation plays an important role in modeling patient-specific anatomy. We propose a convolution neural network, called Kid-Net, along with a training schema to segment kidney vessels: artery, vein and collecting system. Such segmentation is vital during the surgical planning phase in which medical decisions are made before surgical incision. Our main contribution is developing a training schema that handles unbalanced data, reduces false positives and enables high-resolution segmentation with a limited memory budget. These objectives are attained using dynamic weighting, random sampling and 3D patch segmentation. Manual medical image annotation is both time-consuming and expensive. Kid-Net reduces kidney vessels segmentation time from matter of hours to minutes. It is trained end-to-end using 3D patches from volumetric CT-images. A complete segmentation for a 512x512x512 CT-volume is obtained within a few minutes (1-2 mins) by stitching the output 3D patches together. Feature down-sampling and up-sampling are utilized to achieve higher classification and localization accuracies. Quantitative and qualitative evaluation results on a challenging testing dataset show Kid-Net competence.



### BinGAN: Learning Compact Binary Descriptors with a Regularized GAN
- **Arxiv ID**: http://arxiv.org/abs/1806.06778v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06778v5)
- **Published**: 2018-06-18 15:39:09+00:00
- **Updated**: 2018-11-07 06:55:58+00:00
- **Authors**: Maciej Zieba, Piotr Semberecki, Tarek El-Gaaly, Tomasz Trzcinski
- **Comment**: Paper accepted to NIPS 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel regularization method for Generative Adversarial Networks, which allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We employ the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train binarized low-dimensional representation of the penultimate layer to mimic the distribution of the higher-dimensional preceding layers. To achieve this, we introduce two loss terms that aim at: (i) reducing the correlation between the dimensions of the binarized low-dimensional representation of the penultimate layer i. e. maximizing joint entropy) and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications, image matching and retrieval, and achieve state-of-the-art results.



### Deep Spatiotemporal Representation of the Face for Automatic Pain Intensity Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.06793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06793v1)
- **Published**: 2018-06-18 16:02:40+00:00
- **Updated**: 2018-06-18 16:02:40+00:00
- **Authors**: Mohammad Tavakolian, Abdenour Hadid
- **Comment**: 5 pages, 4 figures, Accepted in ICPR 2018
- **Journal**: None
- **Summary**: Automatic pain intensity assessment has a high value in disease diagnosis applications. Inspired by the fact that many diseases and brain disorders can interrupt normal facial expression formation, we aim to develop a computational model for automatic pain intensity assessment from spontaneous and micro facial variations. For this purpose, we propose a 3D deep architecture for dynamic facial video representation. The proposed model is built by stacking several convolutional modules where each module encompasses a 3D convolution kernel with a fixed temporal depth, several parallel 3D convolutional kernels with different temporal depths, and an average pooling layer. Deploying variable temporal depths in the proposed architecture allows the model to effectively capture a wide range of spatiotemporal variations on the faces. Extensive experiments on the UNBC-McMaster Shoulder Pain Expression Archive database show that our proposed model yields in a promising performance compared to the state-of-the-art in automatic pain intensity estimation.



### Temporal coherence-based self-supervised learning for laparoscopic workflow analysis
- **Arxiv ID**: http://arxiv.org/abs/1806.06811v2
- **DOI**: 10.1007/978-3-030-01201-4_11
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06811v2)
- **Published**: 2018-06-18 16:31:25+00:00
- **Updated**: 2018-09-07 13:32:09+00:00
- **Authors**: Isabel Funke, Alexander Jenke, Sören Torge Mees, Jürgen Weitz, Stefanie Speidel, Sebastian Bodenstedt
- **Comment**: Accepted at the Workshop on Context-Aware Operating Theaters (OR
  2.0), a MICCAI satellite event
- **Journal**: CARE 2018, CLIP 2018, OR 2.0 2018, ISIC 2018. Lecture Notes in
  Computer Science, vol 11041 (2018) 85-93
- **Summary**: In order to provide the right type of assistance at the right time, computer-assisted surgery systems need context awareness. To achieve this, methods for surgical workflow analysis are crucial. Currently, convolutional neural networks provide the best performance for video-based workflow analysis tasks. For training such networks, large amounts of annotated data are necessary. However, collecting a sufficient amount of data is often costly, time-consuming, and not always feasible. In this paper, we address this problem by presenting and comparing different approaches for self-supervised pretraining of neural networks on unlabeled laparoscopic videos using temporal coherence. We evaluate our pretrained networks on Cholec80, a publicly available dataset for surgical phase segmentation, on which a maximum F1 score of 84.6 was reached. Furthermore, we were able to achieve an increase of the F1 score of up to 10 points when compared to a non-pretrained neural network.



### Automated Bridge Component Recognition using Video Data
- **Arxiv ID**: http://arxiv.org/abs/1806.06820v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06820v2)
- **Published**: 2018-06-18 16:59:05+00:00
- **Updated**: 2018-09-28 00:21:01+00:00
- **Authors**: Yasutaka Narazaki, Vedhus Hoskere, Tu A. Hoang, Billie F. Spencer Jr
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the automated recognition of structural bridge components using video data. Although understanding video data for structural inspections is straightforward for human inspectors, the implementation of the same task using machine learning methods has not been fully realized. In particular, single-frame image processing techniques, such as convolutional neural networks (CNNs), are not expected to identify structural components accurately when the image is a close-up view, lacking contextual information regarding where on the structure the image originates. Inspired by the significant progress in video processing techniques, this study investigates automated bridge component recognition using video data, where the information from the past frames is used to augment the understanding of the current frame. A new simulated video dataset is created to train the machine learning algorithms. Then, convolutional Neural Networks (CNNs) with recurrent architectures are designed and applied to implement the automated bridge component recognition task. Results are presented for simulated video data, as well as video collected in the field.



### Diving Deep onto Discriminative Ensemble of Histological Hashing & Class-Specific Manifold Learning for Multi-class Breast Carcinoma Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/1806.06876v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06876v3)
- **Published**: 2018-06-18 18:24:16+00:00
- **Updated**: 2019-03-24 05:20:22+00:00
- **Authors**: Sawon Pratiher, Subhankar Chattoraj
- **Comment**: This paper is accepted for presentation at 44th International
  Conference on Acoustics, Speech, and Signal Processing (IEEE ICASSP), UK,
  2019
- **Journal**: None
- **Summary**: Histopathological images (HI) encrypt resolution dependent heterogeneous textures & diverse color distribution variability, manifesting in micro-structural surface tissue convolutions. Also, inherently high coherency of cancerous cells poses significant challenges to breast cancer (BC) multi-classification. As such, multi-class stratification is sparsely explored & prior work mainly focus on benign & malignant tissue characterization only, which forestalls further quantitative analysis of subordinate classes like adenosis, mucinous carcinoma & fibroadenoma etc, for diagnostic competence. In this work, a fully-automated, near-real-time & computationally inexpensive robust multi-classification deep framework from HI is presented.   The proposed scheme employs deep neural network (DNN) aided discriminative ensemble of holistic class-specific manifold learning (CSML) for underlying HI sub-space embedding & HI hashing based local shallow signatures. The model achieves 95.8% accuracy pertinent to multi-classification & 2.8% overall performance improvement & 38.2% enhancement for Lobular carcinoma (LC) sub-class recognition rate as compared to the existing state-of-the-art on well known BreakHis dataset is achieved. Also, 99.3% recognition rate at 200X & a sensitivity of 100% for binary grading at all magnification validates its suitability for clinical deployment in hand-held smart devices.



### Learning to Decode 7T-like MR Image Reconstruction from 3T MR Images
- **Arxiv ID**: http://arxiv.org/abs/1806.06886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06886v1)
- **Published**: 2018-06-18 18:46:15+00:00
- **Updated**: 2018-06-18 18:46:15+00:00
- **Authors**: Aditya Sharma, Prabhjot Kaur, Aditya Nigam, Arnav Bhavsar
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing demand for high field magnetic resonance (MR) scanner indicates the need for high-quality MR images for accurate medical diagnosis. However, cost constraints, instead, motivate a need for algorithms to enhance images from low field scanners. We propose an approach to process the given low field (3T) MR image slices to reconstruct the corresponding high field (7T-like) slices. Our framework involves a novel architecture of a merged convolutional autoencoder with a single encoder and multiple decoders. Specifically, we employ three decoders with random initializations, and the proposed training approach involves selection of a particular decoder in each weight-update iteration for back propagation. We demonstrate that the proposed algorithm outperforms some related contemporary methods in terms of performance and reconstruction time.



### Learning Object Localization and 6D Pose Estimation from Simulation and Weakly Labeled Real Images
- **Arxiv ID**: http://arxiv.org/abs/1806.06888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06888v2)
- **Published**: 2018-06-18 18:48:12+00:00
- **Updated**: 2019-02-21 02:41:03+00:00
- **Authors**: Jean-Philippe Mercier, Chaitanya Mitash, Philippe Giguère, Abdeslam Boularias
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a process for efficiently training a point-wise object detector that enables localizing objects and computing their 6D poses in cluttered and occluded scenes. Accurate pose estimation is typically a requirement for robust robotic grasping and manipulation of objects placed in cluttered, tight environments, such as a shelf with multiple objects. To minimize the human labor required for annotation, the proposed object detector is first trained in simulation by using automatically annotated synthetic images. We then show that the performance of the detector can be substantially improved by using a small set of weakly annotated real images, where a human provides only a list of objects present in each image without indicating the location of the objects. To close the gap between real and synthetic images, we adopt a domain adaptation approach through adversarial training. The detector resulting from this training process can be used to localize objects by using its per-object activation maps. In this work, we use the activation maps to guide the search of 6D poses of objects. Our proposed approach is evaluated on several publicly available datasets for pose estimation. We also evaluated our model on classification and localization in unsupervised and semi-supervised settings. The results clearly indicate that this approach could provide an efficient way toward fully automating the training process of computer vision models used in robotics.



### Bayesian Prediction of Future Street Scenes through Importance Sampling based Optimization
- **Arxiv ID**: http://arxiv.org/abs/1806.06939v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.06939v2)
- **Published**: 2018-06-18 20:51:36+00:00
- **Updated**: 2018-09-28 09:30:26+00:00
- **Authors**: Apratim Bhattacharyya, Mario Fritz, Bernt Schiele
- **Comment**: The objective in (8) allows for trivial solutions e.g. the prior
- **Journal**: None
- **Summary**: For autonomous agents to successfully operate in the real world, anticipation of future events and states of their environment is a key competence. This problem can be formalized as a sequence prediction problem, where a number of observations are used to predict the sequence into the future. However, real-world scenarios demand a model of uncertainty of such predictions, as future states become increasingly uncertain and multi-modal -- in particular on long time horizons. This makes modelling and learning challenging. We cast state of the art semantic segmentation and future prediction models based on deep learning into a Bayesian formulation that in turn allows for a full Bayesian treatment of the prediction problem. We present a new sampling scheme for this model that draws from the success of variational autoencoders by incorporating a recognition network. In the experiments we show that our model outperforms prior work in accuracy of the predicted segmentation and provides calibrated probabilities that also better capture the multi-modal aspects of possible future states of street scenes.



### Deconvolving convolution neural network for cell detection
- **Arxiv ID**: http://arxiv.org/abs/1806.06970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06970v1)
- **Published**: 2018-06-18 22:26:49+00:00
- **Updated**: 2018-06-18 22:26:49+00:00
- **Authors**: Shan E Ahmed Raza, Khalid AbdulJabbar, Mariam Jamal-Hanjani, Selvaraju Veeriah, John Le Quesne, Charles Swanton, Yinyin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic cell detection in histology images is a challenging task due to varying size, shape and features of cells and stain variations across a large cohort. Conventional deep learning methods regress the probability of each pixel belonging to the centre of a cell followed by detection of local maxima. We present deconvolution as an alternate approach to local maxima detection. The ground truth points are convolved with a mapping filter to generate artifical labels. A convolutional neural network (CNN) is modified to convolve it's output with the same mapping filter and is trained for the mapped labels. Output of the trained CNN is then deconvolved to generate points as cell detection. We compare our method with state-of-the-art deep learning approaches where the results show that the proposed approach detects cells with comparatively high precision and F1-score.



### Repetition Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.06984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06984v1)
- **Published**: 2018-06-18 23:30:23+00:00
- **Updated**: 2018-06-18 23:30:23+00:00
- **Authors**: Tom F. H. Runia, Cees G. M. Snoek, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: Visual repetition is ubiquitous in our world. It appears in human activity (sports, cooking), animal behavior (a bee's waggle dance), natural phenomena (leaves in the wind) and in urban environments (flashing lights). Estimating visual repetition from realistic video is challenging as periodic motion is rarely perfectly static and stationary. To better deal with realistic video, we elevate the static and stationary assumptions often made by existing work. Our spatiotemporal filtering approach, established on the theory of periodic motion, effectively handles a wide variety of appearances and requires no learning. Starting from motion in 3D we derive three periodic motion types by decomposition of the motion field into its fundamental components. In addition, three temporal motion continuities emerge from the field's temporal dynamics. For the 2D perception of 3D motion we consider the viewpoint relative to the motion; what follows are 18 cases of recurrent motion perception. To estimate repetition under all circumstances, our theory implies constructing a mixture of differential motion maps: gradient, divergence and curl. We temporally convolve the motion maps with wavelet filters to estimate repetitive dynamics. Our method is able to spatially segment repetitive motion directly from the temporal filter responses densely computed over the motion maps. For experimental verification of our claims, we use our novel dataset for repetition estimation, better-reflecting reality with non-static and non-stationary repetitive motion. On the task of repetition counting, we obtain favorable results compared to a deep learning alternative.



### Classification of remote sensing images using attribute profiles and feature profiles from different trees: a comparative study
- **Arxiv ID**: http://arxiv.org/abs/1806.06985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06985v1)
- **Published**: 2018-06-18 23:34:55+00:00
- **Updated**: 2018-06-18 23:34:55+00:00
- **Authors**: Minh-Tan Pham, Erchan Aptoula, Sébastien Lefèvre
- **Comment**: 4 pages, to appear in IGARSS 2018
- **Journal**: None
- **Summary**: The motivation of this paper is to conduct a comparative study on remote sensing image classification using the morphological attribute profiles (APs) and feature profiles (FPs) generated from different types of tree structures. Over the past few years, APs have been among the most effective methods to model the image's spatial and contextual information. Recently, a novel extension of APs called FPs has been proposed by replacing pixel gray-levels with some statistical and geometrical features when forming the output profiles. FPs have been proved to be more efficient than the standard APs when generated from component trees (max-tree and min-tree). In this work, we investigate their performance on the inclusion tree (tree of shapes) and partition trees (alpha tree and omega tree). Experimental results from both panchromatic and hyperspectral images again confirm the efficiency of FPs compared to APs.



### Soft Sampling for Robust Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.06986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06986v2)
- **Published**: 2018-06-18 23:40:14+00:00
- **Updated**: 2019-07-22 03:29:55+00:00
- **Authors**: Zhe Wu, Navaneeth Bodla, Bharat Singh, Mahyar Najibi, Rama Chellappa, Larry S. Davis
- **Comment**: Accepted in BMVC 2019
- **Journal**: None
- **Summary**: We study the robustness of object detection under the presence of missing annotations. In this setting, the unlabeled object instances will be treated as background, which will generate an incorrect training signal for the detector. Interestingly, we observe that after dropping 30% of the annotations (and labeling them as background), the performance of CNN-based object detectors like Faster-RCNN only drops by 5% on the PASCAL VOC dataset. We provide a detailed explanation for this result. To further bridge the performance gap, we propose a simple yet effective solution, called Soft Sampling. Soft Sampling re-weights the gradients of RoIs as a function of overlap with positive instances. This ensures that the uncertain background regions are given a smaller weight compared to the hardnegatives. Extensive experiments on curated PASCAL VOC datasets demonstrate the effectiveness of the proposed Soft Sampling method at different annotation drop rates. Finally, we show that on OpenImagesV3, which is a real-world dataset with missing annotations, Soft Sampling outperforms standard detection baselines by over 3%.



### Fast Multiple Landmark Localisation Using a Patch-based Iterative Network
- **Arxiv ID**: http://arxiv.org/abs/1806.06987v2
- **DOI**: 10.1007/978-3-030-00928-1_64
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06987v2)
- **Published**: 2018-06-18 23:51:02+00:00
- **Updated**: 2018-10-07 00:27:22+00:00
- **Authors**: Yuanwei Li, Amir Alansary, Juan J. Cerrolaza, Bishesh Khanal, Matthew Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz, Daniel Rueckert
- **Comment**: 8 pages, 4 figures, Accepted for MICCAI 2018
- **Journal**: LNCS 11070 (2018) 563-571
- **Summary**: We propose a new Patch-based Iterative Network (PIN) for fast and accurate landmark localisation in 3D medical volumes. PIN utilises a Convolutional Neural Network (CNN) to learn the spatial relationship between an image patch and anatomical landmark positions. During inference, patches are repeatedly passed to the CNN until the estimated landmark position converges to the true landmark location. PIN is computationally efficient since the inference stage only selectively samples a small number of patches in an iterative fashion rather than a dense sampling at every location in the volume. Our approach adopts a multi-task learning framework that combines regression and classification to improve localisation accuracy. We extend PIN to localise multiple landmarks by using principal component analysis, which models the global anatomical relationships between landmarks. We have evaluated PIN using 72 3D ultrasound images from fetal screening examinations. PIN achieves quantitatively an average landmark localisation error of 5.59mm and a runtime of 0.44s to predict 10 landmarks per volume. Qualitatively, anatomical 2D standard scan planes derived from the predicted landmark locations are visually similar to the clinical ground truth. Source code is publicly available at https://github.com/yuanwei1989/landmark-detection.




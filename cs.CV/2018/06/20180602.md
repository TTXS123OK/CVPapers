# Arxiv Papers in cs.CV on 2018-06-02
### SCAN: Sliding Convolutional Attention Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.00578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00578v1)
- **Published**: 2018-06-02 03:28:43+00:00
- **Updated**: 2018-06-02 03:28:43+00:00
- **Authors**: Yi-Chao Wu, Fei Yin, Xu-Yao Zhang, Li Liu, Cheng-Lin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition has drawn great attentions in the community of computer vision and artificial intelligence due to its challenges and wide applications. State-of-the-art recurrent neural networks (RNN) based models map an input sequence to a variable length output sequence, but are usually applied in a black box manner and lack of transparency for further improvement, and the maintaining of the entire past hidden states prevents parallel computation in a sequence. In this paper, we investigate the intrinsic characteristics of text recognition, and inspired by human cognition mechanisms in reading texts, we propose a scene text recognition method with sliding convolutional attention network (SCAN). Similar to the eye movement during reading, the process of SCAN can be viewed as an alternation between saccades and visual fixations. Compared to the previous recurrent models, computations over all elements of SCAN can be fully parallelized during training. Experimental results on several challenging benchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate the superiority of SCAN over state-of-the-art methods in terms of both the model interpretability and performance.



### Detecting Adversarial Examples via Key-based Network
- **Arxiv ID**: http://arxiv.org/abs/1806.00580v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00580v1)
- **Published**: 2018-06-02 04:13:02+00:00
- **Updated**: 2018-06-02 04:13:02+00:00
- **Authors**: Pinlong Zhao, Zhouyu Fu, Ou wu, Qinghua Hu, Jun Wang
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Though deep neural networks have achieved state-of-the-art performance in visual classification, recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful deep neural networks. Various defense methods have been proposed to address this issue. However, they either require knowledge on the process of generating adversarial examples, or are not robust against new attacks specifically designed to penetrate the existing defense. In this work, we introduce key-based network, a new detection-based defense mechanism to distinguish adversarial examples from normal ones based on error correcting output codes, using the binary code vectors produced by multiple binary classifiers applied to randomly chosen label-sets as signatures to match normal images and reject adversarial examples. In contrast to existing defense methods, the proposed method does not require knowledge of the process for generating adversarial examples and can be applied to defend against different types of attacks. For the practical black-box and gray-box scenarios, where the attacker does not know the encoding scheme, we show empirically that key-based network can effectively detect adversarial examples generated by several state-of-the-art attacks.



### Monocular Depth Estimation with Augmented Ordinal Depth Relationships
- **Arxiv ID**: http://arxiv.org/abs/1806.00585v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00585v2)
- **Published**: 2018-06-02 05:52:32+00:00
- **Updated**: 2019-07-11 02:29:02+00:00
- **Authors**: Yuanzhouhan Cao, Tianqi Zhao, Ke Xian, Chunhua Shen, Zhiguo Cao, Shugong Xu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Most existing algorithms for depth estimation from single monocular images need large quantities of metric groundtruth depths for supervised learning. We show that relative depth can be an informative cue for metric depth estimation and can be easily obtained from vast stereo videos. Acquiring metric depths from stereo videos is sometimes impracticable due to the absence of camera parameters. In this paper, we propose to improve the performance of metric depth estimation with relative depths collected from stereo movie videos using existing stereo matching algorithm. We introduce a new "Relative Depth in Stereo" (RDIS) dataset densely labelled with relative depths. We first pretrain a ResNet model on our RDIS dataset. Then we finetune the model on RGB-D datasets with metric ground-truth depths. During our finetuning, we formulate depth estimation as a classification task. This re-formulation scheme enables us to obtain the confidence of a depth prediction in the form of probability distribution. With this confidence, we propose an information gain loss to make use of the predictions that are close to ground-truth. We evaluate our approach on both indoor and outdoor benchmark RGB-D datasets and achieve state-of-the-art performance.



### BoxNet: Deep Learning Based Biomedical Image Segmentation Using Boxes Only Annotation
- **Arxiv ID**: http://arxiv.org/abs/1806.00593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00593v1)
- **Published**: 2018-06-02 07:10:30+00:00
- **Updated**: 2018-06-02 07:10:30+00:00
- **Authors**: Lin Yang, Yizhe Zhang, Zhuo Zhao, Hao Zheng, Peixian Liang, Michael T. C. Ying, Anil T. Ahuja, Danny Z. Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning (DL) methods have become powerful tools for biomedical image segmentation. However, high annotation efforts and costs are commonly needed to acquire sufficient biomedical training data for DL models. To alleviate the burden of manual annotation, in this paper, we propose a new weakly supervised DL approach for biomedical image segmentation using boxes only annotation. First, we develop a method to combine graph search (GS) and DL to generate fine object masks from box annotation, in which DL uses box annotation to compute a rough segmentation for GS and then GS is applied to locate the optimal object boundaries. During the mask generation process, we carefully utilize information from box annotation to filter out potential errors, and then use the generated masks to train an accurate DL segmentation network. Extensive experiments on gland segmentation in histology images, lymph node segmentation in ultrasound images, and fungus segmentation in electron microscopy images show that our approach attains superior performance over the best known state-of-the-art weakly supervised DL method and is able to achieve (1) nearly the same accuracy compared to fully supervised DL methods with far less annotation effort, (2) significantly better results with similar annotation time, and (3) robust performance in various applications.



### Semantic-Aware Generative Adversarial Nets for Unsupervised Domain Adaptation in Chest X-ray Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.00600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00600v2)
- **Published**: 2018-06-02 07:59:55+00:00
- **Updated**: 2018-06-05 03:01:49+00:00
- **Authors**: Cheng Chen, Qi Dou, Hao Chen, Pheng-Ann Heng
- **Comment**: None
- **Journal**: None
- **Summary**: In spite of the compelling achievements that deep neural networks (DNNs) have made in medical image computing, these deep models often suffer from degraded performance when being applied to new test datasets with domain shift. In this paper, we present a novel unsupervised domain adaptation approach for segmentation tasks by designing semantic-aware generative adversarial networks (GANs). Specifically, we transform the test image into the appearance of source domain, with the semantic structural information being well preserved, which is achieved by imposing a nested adversarial learning in semantic label space. In this way, the segmentation DNN learned from the source domain is able to be directly generalized to the transformed test image, eliminating the need of training a new model for every new target dataset. Our domain adaptation procedure is unsupervised, without using any target domain labels. The adversarial learning of our network is guided by a GAN loss for mapping data distributions, a cycle-consistency loss for retaining pixel-level content, and a semantic-aware loss for enhancing structural information. We validated our method on two different chest X-ray public datasets for left/right lung segmentation. Experimental results show that the segmentation performance of our unsupervised approach is highly competitive with the upper bound of supervised transfer learning.



### DAQN: Deep Auto-encoder and Q-Network
- **Arxiv ID**: http://arxiv.org/abs/1806.00630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00630v1)
- **Published**: 2018-06-02 13:09:28+00:00
- **Updated**: 2018-06-02 13:09:28+00:00
- **Authors**: Daiki Kimura
- **Comment**: None
- **Journal**: None
- **Summary**: The deep reinforcement learning method usually requires a large number of training images and executing actions to obtain sufficient results. When it is extended a real-task in the real environment with an actual robot, the method will be required more training images due to complexities or noises of the input images, and executing a lot of actions on the real robot also becomes a serious problem. Therefore, we propose an extended deep reinforcement learning method that is applied a generative model to initialize the network for reducing the number of training trials. In this paper, we used a deep q-network method as the deep reinforcement learning method and a deep auto-encoder as the generative model. We conducted experiments on three different tasks: a cart-pole game, an atari game, and a real-game with an actual robot. The proposed method trained efficiently on all tasks than the previous method, especially 2.5 times faster on a task with real environment images.



### Squeeze-and-Excitation on Spatial and Temporal Deep Feature Space for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.00631v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00631v2)
- **Published**: 2018-06-02 13:09:50+00:00
- **Updated**: 2018-07-20 02:14:33+00:00
- **Authors**: Gaoyun An, Wen Zhou, Yuxuan Wu, Zhenxing Zheng, Yongwen Liu
- **Comment**: Need to be Revised
- **Journal**: None
- **Summary**: Spatial and temporal features are two key and complementary information for human action recognition. In order to make full use of the intra-frame spatial characteristics and inter-frame temporal relationships, we propose the Squeeze-and-Excitation Long-term Recurrent Convolutional Networks (SE-LRCN) for human action recognition. The Squeeze and Excitation operations are used to implement the feature recalibration. In SE-LRCN, Squeeze-and-Excitation ResNet-34 (SE-ResNet-34) network is adopted to extract spatial features to enhance the dependencies and importance of feature channels of pixel granularity. We also propose the Squeeze-and-Excitation Long Short-Term Memory (SE-LSTM) network to model the temporal relationship, and to enhance the dependencies and importance of feature channels of frame granularity. We evaluate the proposed model on two challenging benchmarks, HMDB51 and UCF101, and the proposed SE-LRCN achieves the competitive results with the state-of-the-art.



### Optimal Clustering under Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1806.00672v1
- **DOI**: 10.1371/journal.pone.0204627
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV, 62H30, 62F35, I.5.3; G.1.6; I.4.9; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1806.00672v1)
- **Published**: 2018-06-02 17:07:22+00:00
- **Updated**: 2018-06-02 17:07:22+00:00
- **Authors**: Lori A. Dalton, Marco E. Benalc√°zar, Edward R. Dougherty
- **Comment**: 19 pages, 5 eps figures, 1 table
- **Journal**: None
- **Summary**: Classical clustering algorithms typically either lack an underlying probability framework to make them predictive or focus on parameter estimation rather than defining and minimizing a notion of error. Recent work addresses these issues by developing a probabilistic framework based on the theory of random labeled point processes and characterizing a Bayes clusterer that minimizes the number of misclustered points. The Bayes clusterer is analogous to the Bayes classifier. Whereas determining a Bayes classifier requires full knowledge of the feature-label distribution, deriving a Bayes clusterer requires full knowledge of the point process. When uncertain of the point process, one would like to find a robust clusterer that is optimal over the uncertainty, just as one may find optimal robust classifiers with uncertain feature-label distributions. Herein, we derive an optimal robust clusterer by first finding an effective random point process that incorporates all randomness within its own probabilistic structure and from which a Bayes clusterer can be derived that provides an optimal robust clusterer relative to the uncertainty. This is analogous to the use of effective class-conditional distributions in robust classification. After evaluating the performance of robust clusterers in synthetic mixtures of Gaussians models, we apply the framework to granular imaging, where we make use of the asymptotic granulometric moment theory for granular images to relate robust clustering theory to the application.



### Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling
- **Arxiv ID**: http://arxiv.org/abs/1806.00681v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00681v4)
- **Published**: 2018-06-02 18:23:48+00:00
- **Updated**: 2019-01-25 03:08:46+00:00
- **Authors**: Yunzhe Tao, Qi Sun, Qiang Du, Wei Liu
- **Comment**: Accepted by NeurIPS 2018
- **Journal**: None
- **Summary**: Nonlocal neural networks have been proposed and shown to be effective in several computer vision tasks, where the nonlocal operations can directly capture long-range dependencies in the feature space. In this paper, we study the nature of diffusion and damping effect of nonlocal networks by doing spectrum analysis on the weight matrices of the well-trained networks, and then propose a new formulation of the nonlocal block. The new block not only learns the nonlocal interactions but also has stable dynamics, thus allowing deeper nonlocal structures. Moreover, we interpret our formulation from the general nonlocal modeling perspective, where we make connections between the proposed nonlocal network and other nonlocal models, such as nonlocal diffusion process and Markov jump process.



### A Hierarchical Fuzzy System for an Advanced Driving Assistance System
- **Arxiv ID**: http://arxiv.org/abs/1806.04611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.04611v1)
- **Published**: 2018-06-02 18:33:42+00:00
- **Updated**: 2018-06-02 18:33:42+00:00
- **Authors**: Mejdi Ben Dkhil, Ali Wali, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we present a hierarchical fuzzy system by evaluating the risk state for a Driver Assistance System in order to contribute in reducing the road accident's number. A key component of this system is its ability to continually detect and test the inside and outside risks in real time: The outside car risks by detecting various road moving objects; this proposed system stands on computer vision approaches. The inside risks by presenting an automatic system for drowsy driving identification or detection by evaluating EEG signals of the driver; this developed system is based on computer vision techniques and biometrics factors (electroencephalogram EEG). This proposed system is then composed of three main modules. The first module is responsible for identifying the driver drowsiness state through his eye movements (physical drowsiness). The second one is responsible for detecting and analysing his physiological signals to also identify his drowsiness state (moral drowsiness). The third module is responsible to evaluate the road driving risks by detecting of the road different moving objects in a real time. The final decision will be obtained by merging of the three detection systems through the use of fuzzy decision rules. Finally, the proposed approach has been improved on ten samples from a proposed dataset.



### Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction
- **Arxiv ID**: http://arxiv.org/abs/1806.00685v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00685v1)
- **Published**: 2018-06-02 18:46:50+00:00
- **Updated**: 2018-06-02 18:46:50+00:00
- **Authors**: Yunzhe Tao, Lin Ma, Weizhong Zhang, Jian Liu, Wei Liu, Qiang Du
- **Comment**: None
- **Journal**: None
- **Summary**: Time series prediction has been studied in a variety of domains. However, it is still challenging to predict future series given historical observations and past exogenous data. Existing methods either fail to consider the interactions among different components of exogenous variables which may affect the prediction accuracy, or cannot model the correlations between exogenous data and target data. Besides, the inherent temporal dynamics of exogenous data are also related to the target series prediction, and thus should be considered as well. To address these issues, we propose an end-to-end deep learning model, i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which incorporates spatio-temporal feature extraction of exogenous variables and temporal dynamics modeling of target variables into a single framework. Moreover, by introducing the hierarchical attention mechanism, HRHN can adaptively select the relevant exogenous features in different semantic levels. We carry out comprehensive empirical evaluations with various methods over several datasets, and show that HRHN outperforms the state of the arts in time series prediction, especially in capturing sudden changes and sudden oscillations of time series.



### An Interpretable Deep Hierarchical Semantic Convolutional Neural Network for Lung Nodule Malignancy Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.00712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.00712v1)
- **Published**: 2018-06-02 22:41:28+00:00
- **Updated**: 2018-06-02 22:41:28+00:00
- **Authors**: Shiwen Shen, Simon X. Han, Denise R. Aberle, Alex A. T. Bui, Willliam Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning methods are increasingly being applied to tasks such as computer-aided diagnosis, these models are difficult to interpret, do not incorporate prior domain knowledge, and are often considered as a "black-box." The lack of model interpretability hinders them from being fully understood by target users such as radiologists. In this paper, we present a novel interpretable deep hierarchical semantic convolutional neural network (HSCNN) to predict whether a given pulmonary nodule observed on a computed tomography (CT) scan is malignant. Our network provides two levels of output: 1) low-level radiologist semantic features, and 2) a high-level malignancy prediction score. The low-level semantic outputs quantify the diagnostic features used by radiologists and serve to explain how the model interprets the images in an expert-driven manner. The information from these low-level tasks, along with the representations learned by the convolutional layers, are then combined and used to infer the high-level task of predicting nodule malignancy. This unified architecture is trained by optimizing a global loss function including both low- and high-level tasks, thereby learning all the parameters within a joint framework. Our experimental results using the Lung Image Database Consortium (LIDC) show that the proposed method not only produces interpretable lung cancer predictions but also achieves significantly better results compared to common 3D CNN approaches.




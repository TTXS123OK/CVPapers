# Arxiv Papers in cs.CV on 2018-06-20
### Fluid Annotation: A Human-Machine Collaboration Interface for Full Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1806.07527v5
- **DOI**: 10.1145/3240508.3241916
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07527v5)
- **Published**: 2018-06-20 02:32:00+00:00
- **Updated**: 2018-12-20 14:20:56+00:00
- **Authors**: Mykhaylo Andriluka, Jasper R. R. Uijlings, Vittorio Ferrari
- **Comment**: ACM MultiMedia 2018. Live demo is available at fluidann.appspot.com
- **Journal**: None
- **Summary**: We introduce Fluid Annotation, an intuitive human-machine collaboration interface for annotating the class label and outline of every object and background region in an image. Fluid annotation is based on three principles: (I) Strong Machine-Learning aid. We start from the output of a strong neural network model, which the annotator can edit by correcting the labels of existing regions, adding new regions to cover missing objects, and removing incorrect regions. The edit operations are also assisted by the model. (II) Full image annotation in a single pass. As opposed to performing a series of small annotation tasks in isolation, we propose a unified interface for full image annotation in a single pass. (III) Empower the annotator. We empower the annotator to choose what to annotate and in which order. This enables concentrating on what the machine does not already know, i.e. putting human effort only on the errors it made. This helps using the annotation budget effectively. Through extensive experiments on the COCO+Stuff dataset, we demonstrate that Fluid Annotation leads to accurate annotations very efficiently, taking three times less annotation time than the popular LabelMe interface.



### Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?
- **Arxiv ID**: http://arxiv.org/abs/1806.07550v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07550v2)
- **Published**: 2018-06-20 04:48:18+00:00
- **Updated**: 2018-12-03 07:08:37+00:00
- **Authors**: Shilin Zhu, Xin Dong, Hao Su
- **Comment**: None
- **Journal**: None
- **Summary**: Binary neural networks (BNN) have been studied extensively since they run dramatically faster at lower memory and power consumption than floating-point networks, thanks to the efficiency of bit operations. However, contemporary BNNs whose weights and activations are both single bits suffer from severe accuracy degradation. To understand why, we investigate the representation ability, speed and bias/variance of BNNs through extensive experiments. We conclude that the error of BNNs is predominantly caused by the intrinsic instability (training time) and non-robustness (train & test time). Inspired by this investigation, we propose the Binary Ensemble Neural Network (BENN) which leverages ensemble methods to improve the performance of BNNs with limited efficiency cost. While ensemble techniques have been broadly believed to be only marginally helpful for strong classifiers such as deep neural networks, our analyses and experiments show that they are naturally a perfect fit to boost BNNs. We find that our BENN, which is faster and much more robust than state-of-the-art binary networks, can even surpass the accuracy of the full-precision floating number network with the same architecture.



### Automatic detection of lumen and media in the IVUS images using U-Net with VGG16 Encoder
- **Arxiv ID**: http://arxiv.org/abs/1806.07554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07554v1)
- **Published**: 2018-06-20 05:01:17+00:00
- **Updated**: 2018-06-20 05:01:17+00:00
- **Authors**: Chirag Balakrishna, Sarshar Dadashzadeh, Sara Soltaninejad
- **Comment**: 10 pages, under review for International Conference of Smart
  Multimedia (ICSM) 2018
- **Journal**: None
- **Summary**: Coronary heart disease is one of the top rank leading cause of mortality in the world which can be because of plaque burden inside the arteries. Intravascular Ultrasound (IVUS) has been recognized as power- ful imaging technology which captures the real time and high resolution images of the coronary arteries and can be used for the analysis of these plaques. The IVUS segmentation involves the extraction of two arterial walls components namely, lumen and media. In this paper, we investi- gate the effectiveness of Convolutional Neural Networks including U-Net to segment ultrasound scans of arteries. In particular, the proposed seg- mentation network was built based on the the U-Net with the VGG16 encoder. Experiments were done for evaluating the proposed segmen- tation architecture which show promising quantitative and qualitative results.



### Locating Objects Without Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1806.07564v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07564v2)
- **Published**: 2018-06-20 05:57:26+00:00
- **Updated**: 2019-04-03 05:14:07+00:00
- **Authors**: Javier Ribera, David Güera, Yuhao Chen, Edward J. Delp
- **Comment**: 12 pages, double-column, 8 figures, accepted at Computer Vision and
  Pattern Recognition (CVPR) 2019
- **Journal**: None
- **Summary**: Recent advances in convolutional neural networks (CNN) have achieved remarkable results in locating objects in images. In these networks, the training procedure usually requires providing bounding boxes or the maximum number of expected objects. In this paper, we address the task of estimating object locations without annotated bounding boxes which are typically hand-drawn and time consuming to label. We propose a loss function that can be used in any fully convolutional network (FCN) to estimate object locations. This loss function is a modification of the average Hausdorff distance between two unordered sets of points. The proposed method has no notion of bounding boxes, region proposals, or sliding windows. We evaluate our method with three datasets designed to locate people's heads, pupil centers and plant centers. We outperform state-of-the-art generic object detectors and methods fine-tuned for pupil tracking.



### Doubly Nested Network for Resource-Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/1806.07568v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07568v1)
- **Published**: 2018-06-20 06:11:35+00:00
- **Updated**: 2018-06-20 06:11:35+00:00
- **Authors**: Jaehong Kim, Sungeun Hong, Yongseok Choi, Jiwon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose doubly nested network(DNNet) where all neurons represent their own sub-models that solve the same task. Every sub-model is nested both layer-wise and channel-wise. While nesting sub-models layer-wise is straight-forward with deep-supervision as proposed in \cite{xie2015holistically}, channel-wise nesting has not been explored in the literature to our best knowledge. Channel-wise nesting is non-trivial as neurons between consecutive layers are all connected to each other. In this work, we introduce a technique to solve this problem by sorting channels topologically and connecting neurons accordingly. For the purpose, channel-causal convolutions are used. Slicing doubly nested network gives a working sub-network. The most notable application of our proposed network structure with slicing operation is resource-efficient inference. At test time, computing resources such as time and memory available for running the prediction algorithm can significantly vary across devices and applications. Given a budget constraint, we can slice the network accordingly and use a sub-model for inference within budget, requiring no additional computation such as training or fine-tuning after deployment. We demonstrate the effectiveness of our approach in several practical scenarios of utilizing available resource efficiently.



### Wall Stress Estimation of Cerebral Aneurysm based on Zernike Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.07441v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07441v1)
- **Published**: 2018-06-20 06:51:25+00:00
- **Updated**: 2018-06-20 06:51:25+00:00
- **Authors**: Zhiyu Sun, Jia Lu, Stephen Baek
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Convolutional neural networks (ConvNets) have demonstrated an exceptional capacity to discern visual patterns from digital images and signals. Unfortunately, such powerful ConvNets do not generalize well to arbitrary-shaped manifolds, where data representation does not fit into a tensor-like grid. Hence, many fields of science and engineering, where data points possess some manifold structure, cannot enjoy the full benefits of the recent advances in ConvNets. The aneurysm wall stress estimation problem introduced in this paper is one of many such problems. The problem is well-known to be of a paramount clinical importance, but yet, traditional ConvNets cannot be applied due to the manifold structure of the data, neither does the state-of-the-art geometric ConvNets perform well. Motivated by this, we propose a new geometric ConvNet method named ZerNet, which builds upon our novel mathematical generalization of convolution and pooling operations on manifolds. Our study shows that the ZerNet outperforms the other state-of-the-art geometric ConvNets in terms of accuracy.



### Classifying Object Manipulation Actions based on Grasp-types and Motion-Constraints
- **Arxiv ID**: http://arxiv.org/abs/1806.07574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07574v1)
- **Published**: 2018-06-20 06:57:34+00:00
- **Updated**: 2018-06-20 06:57:34+00:00
- **Authors**: Kartik Gupta, Darius Burschka, Arnav Bhavsar
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.



### A CADe System for Gliomas in Brain MRI using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.07589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07589v1)
- **Published**: 2018-06-20 07:30:38+00:00
- **Updated**: 2018-06-20 07:30:38+00:00
- **Authors**: Subhasis Banerjee, Sushmita Mitra, Anmol Sharma, B. Uma Shankar
- **Comment**: The paper consists of 11 Pages, 6 Figures, 7 Tables, 56 References
- **Journal**: None
- **Summary**: Inspired by the success of Convolutional Neural Networks (CNN), we develop a novel Computer Aided Detection (CADe) system using CNN for Glioblastoma Multiforme (GBM) detection and segmentation from multi channel MRI data. A two-stage approach first identifies the presence of GBM. This is followed by a GBM localization in each "abnormal" MR slice. As part of the CADe system, two CNN architectures viz. Classification CNN (C-CNN) and Detection CNN (D-CNN) are employed. The CADe system considers MRI data consisting of four sequences ($T_1$, $T_{1c},$ $T_2$, and $T_{2FLAIR}$) as input, and automatically generates the bounding boxes encompassing the tumor regions in each slice which is deemed abnormal. Experimental results demonstrate that the proposed CADe system, when used as a preliminary step before segmentation, can allow improved delineation of tumor region while reducing false positives arising in normal areas of the brain. The GrowCut method, employed for tumor segmentation, typically requires a foreground and background seed region for initialization. Here the algorithm is initialized with seeds automatically generated from the output of the proposed CADe system, thereby resulting in improved performance as compared to that using random seeds.



### Deep Similarity Metric Learning for Real-Time Pedestrian Tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.07592v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07592v2)
- **Published**: 2018-06-20 07:45:50+00:00
- **Updated**: 2019-11-11 14:13:21+00:00
- **Authors**: Michael Thoreau, Navinda Kottege
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking by detection is a common approach to solving the Multiple Object Tracking problem. In this paper we show how learning a deep similarity metric can improve three key aspects of pedestrian tracking on a multiple object tracking benchmark. We train a convolutional neural network to learn an embedding function in a Siamese configuration on a large person re-identification dataset. The offline-trained embedding network is integrated in to the tracking formulation to improve performance while retaining real-time performance. The proposed tracker stores appearance metrics while detections are strong, using this appearance information to: prevent ID switches, associate tracklets through occlusion, and propose new detections where detector confidence is low. This method achieves competitive results in evaluation, especially among online, real-time approaches. We present an ablative study showing the impact of each of the three uses of our deep appearance metric.



### Semi-supervised Seizure Prediction with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.08235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.08235v1)
- **Published**: 2018-06-20 07:47:57+00:00
- **Updated**: 2018-06-20 07:47:57+00:00
- **Authors**: Nhan Duy Truong, Levin Kuhlmann, Mohammad Reza Bonyadi, Omid Kavehei
- **Comment**: 6 pages, 5 figures, 3 tables. arXiv admin note: text overlap with
  arXiv:1707.01976
- **Journal**: None
- **Summary**: In this article, we propose an approach that can make use of not only labeled EEG signals but also the unlabeled ones which is more accessible. We also suggest the use of data fusion to further improve the seizure prediction accuracy. Data fusion in our vision includes EEG signals, cardiogram signals, body temperature and time. We use the short-time Fourier transform on 28-s EEG windows as a pre-processing step. A generative adversarial network (GAN) is trained in an unsupervised manner where information of seizure onset is disregarded. The trained Discriminator of the GAN is then used as feature extractor. Features generated by the feature extractor are classified by two fully-connected layers (can be replaced by any classifier) for the labeled EEG signals. This semi-supervised seizure prediction method achieves area under the operating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp EEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively. Unsupervised training without the need of labeling is important because not only it can be performed in real-time during EEG signal recording, but also it does not require feature engineering effort for each patient.



### Dynamic Risk Assessment for Vehicles of Higher Automation Levels by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.07635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.07635v1)
- **Published**: 2018-06-20 09:41:14+00:00
- **Updated**: 2018-06-20 09:41:14+00:00
- **Authors**: Patrik Feth, Mohammed Naveed Akram, René Schuster, Oliver Wasenmüller
- **Comment**: None
- **Journal**: International Workshop on Artificial Intelligence Safety
  Engineering (WAISE) 2018
- **Summary**: Vehicles of higher automation levels require the creation of situation awareness. One important aspect of this situation awareness is an understanding of the current risk of a driving situation. In this work, we present a novel approach for the dynamic risk assessment of driving situations based on images of a front stereo camera using deep learning. To this end, we trained a deep neural network with recorded monocular images, disparity maps and a risk metric for diverse traffic scenes. Our approach can be used to create the aforementioned situation awareness of vehicles of higher automation levels and can serve as a heterogeneous channel to systems based on radar or lidar sensors that are used traditionally for the calculation of risk metrics.



### Cross-Domain Deep Face Matching for Real Banking Security Systems
- **Arxiv ID**: http://arxiv.org/abs/1806.07644v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07644v3)
- **Published**: 2018-06-20 10:00:52+00:00
- **Updated**: 2020-04-10 12:56:58+00:00
- **Authors**: Johnatan S. Oliveira, Gustavo B. Souza, Anderson R. Rocha, Flávio E. Deus, Aparecido N. Marana
- **Comment**: None
- **Journal**: None
- **Summary**: Ensuring the security of transactions is currently one of the major challenges that banking systems deal with. The usage of face for biometric authentication of users is attracting large investments from banks worldwide due to its convenience and acceptability by people, especially in cross-domain scenarios, in which facial images from ID documents are compared with digital self-portraits (selfies) for the automated opening of new checking accounts, e.g, or financial transactions authorization. Actually, the comparison of selfies and IDs has also been applied in another wide variety of tasks nowadays, such as automated immigration control. The major difficulty in such process consists in attenuating the differences between the facial images compared given their different domains. In this work, in addition to collecting a large cross-domain face dataset, with 27,002 real facial images of selfies and ID documents (13,501 subjects) captured from the databases of the major public Brazilian bank, we propose a novel architecture for such cross-domain matching problem based on deep features extracted by two well-referenced Convolutional Neural Networks (CNN). Results obtained on the dataset collected, called FaceBank, with accuracy rates higher than 93%, demonstrate the robustness of the proposed approach to the cross-domain face matching problem and its feasible application in real banking security systems.



### Dynamic voting in multi-view learning for radiomics applications
- **Arxiv ID**: http://arxiv.org/abs/1806.07686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07686v2)
- **Published**: 2018-06-20 12:11:48+00:00
- **Updated**: 2018-06-26 10:18:28+00:00
- **Authors**: Hongliu Cao, Simon Bernard, Laurent Heutte, Robert Sabourin
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Cancer diagnosis and treatment often require a personalized analysis for each patient nowadays, due to the heterogeneity among the different types of tumor and among patients. Radiomics is a recent medical imaging field that has shown during the past few years to be promising for achieving this personalization. However, a recent study shows that most of the state-of-the-art works in Radiomics fail to identify this problem as a multi-view learning task and that multi-view learning techniques are generally more efficient. In this work, we propose to further investigate the potential of one family of multi-view learning methods based on Multiple Classifiers Systems where one classifier is learnt on each view and all classifiers are combined afterwards. In particular, we propose a random forest based dynamic weighted voting scheme, which personalizes the combination of views for each new patient for classification tasks. The proposed method is validated on several real-world Radiomics problems.



### DEFRAG: Deep Euclidean Feature Representations through Adaptation on the Grassmann Manifold
- **Arxiv ID**: http://arxiv.org/abs/1806.07688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07688v1)
- **Published**: 2018-06-20 12:13:53+00:00
- **Updated**: 2018-06-20 12:13:53+00:00
- **Authors**: Breton Minnehan, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel technique for training deep networks with the objective of obtaining feature representations that exist in a Euclidean space and exhibit strong clustering behavior. Our desired features representations have three traits: they can be compared using a standard Euclidian distance metric, samples from the same class are tightly clustered, and samples from different classes are well separated. However, most deep networks do not enforce such feature representations. The DEFRAG training technique consists of two steps: first good feature clustering behavior is encouraged though an auxiliary loss function based on the Silhouette clustering metric. Then the feature space is retracted onto a Grassmann manifold to ensure that the L_2 Norm forms a similarity metric. The DEFRAG technique achieves state of the art results on standard classification datasets using a relatively small network architecture with significantly fewer parameters than many standard networks.



### Self-weighted Multiple Kernel Learning for Graph-based Clustering and Semi-supervised Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.07697v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07697v1)
- **Published**: 2018-06-20 12:46:43+00:00
- **Updated**: 2018-06-20 12:46:43+00:00
- **Authors**: Zhao Kang, Xiao Lu, Jinfeng Yi, Zenglin Xu
- **Comment**: Accepted by IJCAI 2018, Code is available
- **Journal**: None
- **Summary**: Multiple kernel learning (MKL) method is generally believed to perform better than single kernel method. However, some empirical studies show that this is not always true: the combination of multiple kernels may even yield an even worse performance than using a single kernel. There are two possible reasons for the failure: (i) most existing MKL methods assume that the optimal kernel is a linear combination of base kernels, which may not hold true; and (ii) some kernel weights are inappropriately assigned due to noises and carelessly designed algorithms. In this paper, we propose a novel MKL framework by following two intuitive assumptions: (i) each kernel is a perturbation of the consensus kernel; and (ii) the kernel that is close to the consensus kernel should be assigned a large weight. Impressively, the proposed method can automatically assign an appropriate weight to each kernel without introducing additional parameters, as existing methods do. The proposed framework is integrated into a unified framework for graph-based clustering and semi-supervised classification. We have conducted experiments on multiple benchmark datasets and our empirical results verify the superiority of the proposed framework.



### Accurate and Diverse Sampling of Sequences based on a "Best of Many" Sample Objective
- **Arxiv ID**: http://arxiv.org/abs/1806.07772v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07772v2)
- **Published**: 2018-06-20 14:49:45+00:00
- **Updated**: 2018-10-15 12:56:36+00:00
- **Authors**: Apratim Bhattacharyya, Bernt Schiele, Mario Fritz
- **Comment**: Added additional references and baselines. (Appeared in CVPR 2018)
- **Journal**: None
- **Summary**: For autonomous agents to successfully operate in the real world, anticipation of future events and states of their environment is a key competence. This problem has been formalized as a sequence extrapolation problem, where a number of observations are used to predict the sequence into the future. Real-world scenarios demand a model of uncertainty of such predictions, as predictions become increasingly uncertain -- in particular on long time horizons. While impressive results have been shown on point estimates, scenarios that induce multi-modal distributions over future sequences remain challenging. Our work addresses these challenges in a Gaussian Latent Variable model for sequence prediction. Our core contribution is a "Best of Many" sample objective that leads to more accurate and more diverse predictions that better capture the true variations in real-world sequence data. Beyond our analysis of improved model fit, our models also empirically outperform prior work on three diverse tasks ranging from traffic scenes to weather data.



### Generative Adversarial Networks for Image-to-Image Translation on Multi-Contrast MR Images - A Comparison of CycleGAN and UNIT
- **Arxiv ID**: http://arxiv.org/abs/1806.07777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07777v1)
- **Published**: 2018-06-20 14:56:10+00:00
- **Updated**: 2018-06-20 14:56:10+00:00
- **Authors**: Per Welander, Simon Karlsson, Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: In medical imaging, a general problem is that it is costly and time consuming to collect high quality data from healthy and diseased subjects. Generative adversarial networks (GANs) is a deep learning method that has been developed for synthesizing data. GANs can thereby be used to generate more realistic training data, to improve classification performance of machine learning algorithms. Another application of GANs is image-to-image translations, e.g. generating magnetic resonance (MR) images from computed tomography (CT) images, which can be used to obtain multimodal datasets from a single modality. Here, we evaluate two unsupervised GAN models (CycleGAN and UNIT) for image-to-image translation of T1- and T2-weighted MR images, by comparing generated synthetic MR images to ground truth images. We also evaluate two supervised models; a modification of CycleGAN and a pure generator model. A small perceptual study was also performed to evaluate how visually realistic the synthesized images are. It is shown that the implemented GAN models can synthesize visually realistic MR images (incorrectly labeled as real by a human). It is also shown that models producing more visually realistic synthetic images not necessarily have better quantitative error measurements, when compared to ground truth data. Code is available at https://github.com/simontomaskarlsson/GAN-MRI



### Histological images segmentation of mucous glands
- **Arxiv ID**: http://arxiv.org/abs/1806.07781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07781v1)
- **Published**: 2018-06-20 15:07:31+00:00
- **Updated**: 2018-06-20 15:07:31+00:00
- **Authors**: A. Khvostikov, A. Krylov, O. Kharlova, N. Oleynikova, I. Mikhailov, P. Malkov
- **Comment**: None
- **Journal**: None
- **Summary**: Mucous glands lesions analysis and assessing of malignant potential of colon polyps are very important tasks of surgical pathology. However, differential diagnosis of colon polyps often seems impossible by classical methods and it is necessary to involve computer methods capable of assessing minimal differences to extend the capabilities of the classical pathology examination. Accurate segmentation of mucous glands from histology images is a crucial step to obtain reliable morphometric criteria for quantitative diagnostic methods. We review major trends in histological images segmentation and design a new convolutional neural network for mucous gland segmentation.



### Metric-Driven Learning of Correspondence Weighting for 2-D/3-D Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1806.07812v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07812v3)
- **Published**: 2018-06-20 16:02:27+00:00
- **Updated**: 2018-10-26 12:57:45+00:00
- **Authors**: Roman Schaffert, Jian Wang, Peter Fischer, Anja Borsdorf, Andreas Maier
- **Comment**: accepted for publication at the German Conference on Pattern
  Recognition (GCPR) 2018
- **Journal**: None
- **Summary**: Registration of pre-operative 3-D volumes to intra-operative 2-D X-ray images is important in minimally invasive medical procedures. Rigid registration can be performed by estimating a global rigid motion that optimizes the alignment of local correspondences. However, inaccurate correspondences challenge the registration performance. To minimize their influence, we estimate optimal weights for correspondences using PointNet. We train the network directly with the criterion to minimize the registration error. We propose an objective function which includes point-to-plane correspondence-based motion estimation and projection error computation, thereby enabling the learning of a weighting strategy that optimally fits the underlying formulation of the registration task in an end-to-end fashion. For single-vertebra registration, we achieve an accuracy of 0.74$\pm$0.26 mm and highly improved robustness. The success rate is increased from 79.3 % to 94.3 % and the capture range from 3 mm to 13 mm.



### Disentangling Multiple Conditional Inputs in GANs
- **Arxiv ID**: http://arxiv.org/abs/1806.07819v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07819v1)
- **Published**: 2018-06-20 16:15:37+00:00
- **Updated**: 2018-06-20 16:15:37+00:00
- **Authors**: Gökhan Yildirim, Calvin Seward, Urs Bergmann
- **Comment**: 5 pages, 9 figures, Paper is accepted to the workshop "AI for
  Fashion" in KDD Conference, 2018, London, United Kingdom
- **Journal**: None
- **Summary**: In this paper, we propose a method that disentangles the effects of multiple input conditions in Generative Adversarial Networks (GANs). In particular, we demonstrate our method in controlling color, texture, and shape of a generated garment image for computer-aided fashion design. To disentangle the effect of input attributes, we customize conditional GANs with consistency loss functions. In our experiments, we tune one input at a time and show that we can guide our network to generate novel and realistic images of clothing articles. In addition, we present a fashion design process that estimates the input attributes of an existing garment and modifies them using our generator.



### Learning Neural Parsers with Deterministic Differentiable Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.07822v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07822v2)
- **Published**: 2018-06-20 16:15:54+00:00
- **Updated**: 2018-09-19 14:58:04+00:00
- **Authors**: Tanmay Shankar, Nicholas Rhinehart, Katharina Muelling, Kris M. Kitani
- **Comment**: Accepted to Conference on Robot Learning, CoRL 2018
- **Journal**: None
- **Summary**: We explore the problem of learning to decompose spatial tasks into segments, as exemplified by the problem of a painting robot covering a large object. Inspired by the ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate the problem of decomposing objects into segments as a parsing approach. We make the insight that the derivation of a parse-tree that decomposes the object into segments closely resembles a decision tree constructed by ID3, which can be done when the ground-truth available. We learn to imitate an expert parsing oracle, such that our neural parser can generalize to parse natural images without ground truth. We introduce a novel deterministic policy gradient update, DRAG (i.e., DeteRministically AGgrevate) in the form of a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. From another perspective, our approach is a variant of the Deterministic Policy Gradient suitable for the imitation learning setting. The deterministic policy representation offered by training our neural parser with DRAG allows it to outperform state of the art imitation and reinforcement learning approaches.



### Unsupervised Learning of Object Landmarks through Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1806.07823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07823v2)
- **Published**: 2018-06-20 16:17:00+00:00
- **Updated**: 2018-12-13 21:56:29+00:00
- **Authors**: Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi
- **Comment**: In NeurIPS 2018. Project page:
  http://www.robots.ox.ac.uk/~vgg/research/unsupervised_landmarks/
- **Journal**: None
- **Summary**: We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.



### How Bad is Good enough: Noisy annotations for instrument pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.07836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07836v1)
- **Published**: 2018-06-20 16:37:35+00:00
- **Updated**: 2018-06-20 16:37:35+00:00
- **Authors**: David Kügler, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Though analysis of Medical Images by Deep Learning achieves unprecedented results across various applications, the effect of \emph{noisy training annotations} is rarely studied in a systematic manner. In Medical Image Analysis, most reports addressing this question concentrate on studying segmentation performance of deep learning classifiers. The absence of continuous ground truth annotations in these studies limits the value of conclusions for applications, where regression is the primary method of choice. In the application of surgical instrument pose estimation, where precision has a direct clinical impact on patient outcome, studying the effect of \emph{noisy annotations} on deep learning pose estimation techniques is of supreme importance. Real x-ray images are inadequate for this evaluation due to the unavailability of ground truth annotations. We circumvent this problem by generating synthetic radiographs, where the ground truth pose is known and therefore the pose estimation error made by the medical-expert can be estimated from experiments. Furthermore, this study shows the property of deep neural networks to learn dominant signals from noisy annotations with sufficient data in a regression setting.



### Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy
- **Arxiv ID**: http://arxiv.org/abs/1806.07840v4
- **DOI**: None
- **Categories**: **cs.DC**, cs.AI, cs.CV, cs.MM, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1806.07840v4)
- **Published**: 2018-06-20 16:56:54+00:00
- **Updated**: 2018-12-27 11:49:55+00:00
- **Authors**: En Li, Zhi Zhou, Xu Chen
- **Comment**: ACM SIGCOMM Workshop on Mobile Edge Communications, Budapest,
  Hungary, August 21-23, 2018. https://dl.acm.org/authorize?N665473
- **Journal**: None
- **Summary**: As the backbone technology of machine learning, deep neural networks (DNNs) have have quickly ascended to the spotlight. Running DNNs on resource-constrained mobile devices is, however, by no means trivial, since it incurs high performance and energy overhead. While offloading DNNs to the cloud for execution suffers unpredictable performance, due to the uncontrolled long wide-area network latency. To address these challenges, in this paper, we propose Edgent, a collaborative and on-demand DNN co-inference framework with device-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that adaptively partitions DNN computation between device and edge, in order to leverage hybrid computation resources in proximity for real-time DNN inference. (2) DNN right-sizing that accelerates DNN inference through early-exit at a proper intermediate DNN layer to further reduce the computation latency. The prototype implementation and extensive evaluations based on Raspberry Pi demonstrate Edgent's effectiveness in enabling on-demand low-latency edge intelligence.



### Hide and Seek tracker: Real-time recovery from target loss
- **Arxiv ID**: http://arxiv.org/abs/1806.07844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07844v1)
- **Published**: 2018-06-20 17:09:02+00:00
- **Updated**: 2018-06-20 17:09:02+00:00
- **Authors**: Alessandro Bay, Panagiotis Sidiropoulos, Eduard Vazquez, Michele Sasdelli
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we examine the real-time recovery of a video tracker from a target loss, using information that is already available from the original tracker and without a significant computational overhead. More specifically, before using the tracker output to update the target position we estimate the detection confidence. In the case of a low confidence, the position update is rejected and the tracker passes to a single-frame failure mode, during which the patch low-level visual content is used to swiftly update the object position, before recovering from the target loss in the next frame. Orthogonally to this improvement, we further enhance the running average method used for creating the query model in tracking-through-similarity. The experimental evidence provided by evaluation on standard tracking datasets (OTB-50, OTB-100 and OTB-2013) validate that target recovery can be successfully achieved without compromising the real-time update of the target position.



### iMapper: Interaction-guided Joint Scene and Human Motion Mapping from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.07889v1
- **DOI**: 10.1145/3306346.3322961
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.07889v1)
- **Published**: 2018-06-20 17:47:50+00:00
- **Updated**: 2018-06-20 17:47:50+00:00
- **Authors**: Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer, Niloy J. Mitra
- **Comment**: None
- **Journal**: Siggraph 2019
- **Summary**: A long-standing challenge in scene analysis is the recovery of scene arrangements under moderate to heavy occlusion, directly from monocular video. While the problem remains a subject of active research, concurrent advances have been made in the context of human pose reconstruction from monocular video, including image-space feature point detection and 3D pose recovery. These methods, however, start to fail under moderate to heavy occlusion as the problem becomes severely under-constrained. We approach the problems differently. We observe that people interact similarly in similar scenes. Hence, we exploit the correlation between scene object arrangement and motions performed in that scene in both directions: first, typical motions performed when interacting with objects inform us about possible object arrangements; and second, object arrangements, in turn, constrain the possible motions.   We present iMapper, a data-driven method that focuses on identifying human-object interactions, and jointly reasons about objects and human movement over space-time to recover both a plausible scene arrangement and consistent human interactions. We first introduce the notion of characteristic interactions as regions in space-time when an informative human-object interaction happens. This is followed by a novel occlusion-aware matching procedure that searches and aligns such characteristic snapshots from an interaction database to best explain the input monocular video. Through extensive evaluations, both quantitative and qualitative, we demonstrate that iMapper significantly improves performance over both dedicated state-of-the-art scene analysis and 3D human pose recovery approaches, especially under medium to heavy occlusion.



### Hybrid Bayesian Eigenobjects: Combining Linear Subspace and Deep Network Methods for 3D Robot Vision
- **Arxiv ID**: http://arxiv.org/abs/1806.07872v2
- **DOI**: 10.1109/IROS.2018.8593795
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07872v2)
- **Published**: 2018-06-20 17:57:56+00:00
- **Updated**: 2018-07-27 16:01:19+00:00
- **Authors**: Benjamin Burchfiel, George Konidaris
- **Comment**: To appear in the International Conference on Intelligent Robots
  (IROS) - Madrid, 2018
- **Journal**: None
- **Summary**: We introduce Hybrid Bayesian Eigenobjects (HBEOs), a novel representation for 3D objects designed to allow a robot to jointly estimate the pose, class, and full 3D geometry of a novel object observed from a single viewpoint in a single practical framework. By combining both linear subspace methods and deep convolutional prediction, HBEOs efficiently learn nonlinear object representations without directly regressing into high-dimensional space. HBEOs also remove the onerous and generally impractical necessity of input data voxelization prior to inference. We experimentally evaluate the suitability of HBEOs to the challenging task of joint pose, class, and shape inference on novel objects and show that, compared to preceding work, HBEOs offer dramatically improved performance in all three tasks along with several orders of magnitude faster runtime performance.



### Como funciona o Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.07908v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07908v1)
- **Published**: 2018-06-20 18:04:09+00:00
- **Updated**: 2018-06-20 18:04:09+00:00
- **Authors**: Moacir Antonelli Ponti, Gabriel B. Paranhos da Costa
- **Comment**: Book chapter, in Portuguese, 31 pages
- **Journal**: In: T\'opicos em Gerenciamento de Dados e Informa\c{c}\~oes, SBC,
  Cap.3, ISBN 978-85-7669-400-7, pp.63-93, 2017
- **Summary**: Deep Learning methods are currently the state-of-the-art in many problems which can be tackled via machine learning, in particular classification problems. However there is still lack of understanding on how those methods work, why they work and what are the limitations involved in using them. In this chapter we will describe in detail the transition from shallow to deep networks, include examples of code on how to implement them, as well as the main issues one faces when training a deep network. Afterwards, we introduce some theoretical background behind the use of deep models, and discuss their limitations.



### Task-Driven Convolutional Recurrent Models of the Visual System
- **Arxiv ID**: http://arxiv.org/abs/1807.00053v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1807.00053v2)
- **Published**: 2018-06-20 20:27:23+00:00
- **Updated**: 2018-10-27 03:49:01+00:00
- **Authors**: Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli, David Sussillo, James J. DiCarlo, Daniel L. K. Yamins
- **Comment**: NIPS 2018 Camera Ready Version, 16 pages including supplementary
  information, 6 figures
- **Journal**: None
- **Summary**: Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.



### A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.07987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07987v2)
- **Published**: 2018-06-20 21:12:43+00:00
- **Updated**: 2018-09-13 17:20:44+00:00
- **Authors**: Alex D. Pon, Oles Andrienko, Ali Harakeh, Steven L. Waslander
- **Comment**: Accepted in the IEEE 15th Conference on Computer and Robot Vision
- **Journal**: None
- **Summary**: Traffic light and sign detectors on autonomous cars are integral for road scene perception. The literature is abundant with deep learning networks that detect either lights or signs, not both, which makes them unsuitable for real-life deployment due to the limited graphics processing unit (GPU) memory and power available on embedded systems. The root cause of this issue is that no public dataset contains both traffic light and sign labels, which leads to difficulties in developing a joint detection framework. We present a deep hierarchical architecture in conjunction with a mini-batch proposal selection mechanism that allows a network to detect both traffic lights and signs from training on separate traffic light and sign datasets. Our method solves the overlapping issue where instances from one dataset are not labelled in the other dataset. We are the first to present a network that performs joint detection on traffic lights and signs. We measure our network on the Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small Traffic Lights benchmark for traffic light detection and show it outperforms the existing Bosch Small Traffic light state-of-the-art method. We focus on autonomous car deployment and show our network is more suitable than others because of its low memory footprint and real-time image processing time. Qualitative results can be viewed at https://youtu.be/_YmogPzBXOw



### Novel Convolution Kernels for Computer Vision and Shape Analysis based on Electromagnetism
- **Arxiv ID**: http://arxiv.org/abs/1806.07996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07996v1)
- **Published**: 2018-06-20 21:31:00+00:00
- **Updated**: 2018-06-20 21:31:00+00:00
- **Authors**: Dominique Beaini, Sofiane Achiche, Yann-Seing Law-Kam Cio, Maxime Raison
- **Comment**: Keywords: Shape analysis; Stroke analysis; Computer vision;
  Electromagnetic potential field; Feature extraction; Image filtering; Image
  convolution Published in PolyPublie: https://publications.polymtl.ca/3162/
- **Journal**: Beaini, D., Achiche, S., Law-Kam Cio, Y.-S. & Raison, M. (2018).
  Novel convolution kernels for computer vision and shape analysis based on
  electromagnetism (Report). https://publications.polymtl.ca/3162/
- **Summary**: Computer vision is a growing field with a lot of new applications in automation and robotics, since it allows the analysis of images and shapes for the generation of numerical or analytical information. One of the most used method of information extraction is image filtering through convolution kernels, with each kernel specialized for specific applications. The objective of this paper is to present a novel convolution kernels, based on principles of electromagnetic potentials and fields, for a general use in computer vision and to demonstrate its usage for shape and stroke analysis. Such filtering possesses unique geometrical properties that can be interpreted using well understood physics theorems. Therefore, this paper focuses on the development of the electromagnetic kernels and on their application on images for shape and stroke analysis. It also presents several interesting features of electromagnetic kernels, such as resolution, size and orientation independence, robustness to noise and deformation, long distance stroke interaction and ability to work with 3D images



### Stability of Scattering Decoder For Nonlinear Diffractive Imaging
- **Arxiv ID**: http://arxiv.org/abs/1806.08015v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08015v4)
- **Published**: 2018-06-20 23:00:15+00:00
- **Updated**: 2018-12-04 17:27:31+00:00
- **Authors**: Yu Sun, Ulugbek S. Kamilov
- **Comment**: in Proceedings of iTWIST'18, Paper-ID: 31, Marseille, France,
  November, 21-23, 2018
- **Journal**: None
- **Summary**: The problem of image reconstruction under multiple light scattering is usually formulated as a regularized non-convex optimization. A deep learning architecture, Scattering Decoder (ScaDec), was recently proposed to solve this problem in a purely data-driven fashion. The proposed method was shown to substantially outperform optimization-based baselines and achieve state-of-the-art results. In this paper, we thoroughly test the robustness of ScaDec to different permittivity contrasts, number of transmissions, and input signal-to-noise ratios. The results on high-fidelity simulated datasets show that the performance of ScaDec is stable in different settings.




# Arxiv Papers in cs.CV on 2018-10-07
### Deep Model-Based 6D Pose Refinement in RGB
- **Arxiv ID**: http://arxiv.org/abs/1810.03065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03065v1)
- **Published**: 2018-10-07 01:22:57+00:00
- **Updated**: 2018-10-07 01:22:57+00:00
- **Authors**: Fabian Manhardt, Wadim Kehl, Nassir Navab, Federico Tombari
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: We present a novel approach for model-based 6D pose refinement in color data. Building on the established idea of contour-based pose tracking, we teach a deep neural network to predict a translational and rotational update. At the core, we propose a new visual loss that drives the pose update by aligning object contours, thus avoiding the definition of any explicit appearance model. In contrast to previous work our method is correspondence-free, segmentation-free, can handle occlusion and is agnostic to geometrical symmetry as well as visual ambiguities. Additionally, we observe a strong robustness towards rough initialization. The approach can run in real-time and produces pose accuracies that come close to 3D ICP without the need for depth data. Furthermore, our networks are trained from purely synthetic data and will be published together with the refinement code to ensure reproducibility.



### Training Convolutional Neural Networks and Compressed Sensing End-to-End for Microscopy Cell Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.03075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03075v1)
- **Published**: 2018-10-07 02:34:54+00:00
- **Updated**: 2018-10-07 02:34:54+00:00
- **Authors**: Yao Xue, Gilbert Bigras, Judith Hugh, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Automated cell detection and localization from microscopy images are significant tasks in biomedical research and clinical practice. In this paper, we design a new cell detection and localization algorithm that combines deep convolutional neural network (CNN) and compressed sensing (CS) or sparse coding (SC) for end-to-end training. We also derive, for the first time, a backpropagation rule, which is applicable to train any algorithm that implements a sparse code recovery layer. The key observation behind our algorithm is that cell detection task is a point object detection task in computer vision, where the cell centers (i.e., point objects) occupy only a tiny fraction of the total number of pixels in an image. Thus, we can apply compressed sensing (or, equivalently sparse coding) to compactly represent a variable number of cells in a projected space. Then, CNN regresses this compressed vector from the input microscopy image. Thanks to the SC/CS recovery algorithm (L1 optimization) that can recover sparse cell locations from the output of CNN. We train this entire processing pipeline end-to-end and demonstrate that end-to-end training provides accuracy improvements over a training paradigm that treats CNN and CS-recovery layers separately. Our algorithm design also takes into account a form of ensemble average of trained models naturally to further boost accuracy of cell detection. We have validated our algorithm on benchmark datasets and achieved excellent performances.



### DeepGeo: Photo Localization with Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1810.03077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03077v1)
- **Published**: 2018-10-07 03:10:57+00:00
- **Updated**: 2018-10-07 03:10:57+00:00
- **Authors**: Sudharshan Suresh, Nathaniel Chodosh, Montiel Abello
- **Comment**: 7 pages, 9 figures. Pre-print after submission to conference
- **Journal**: None
- **Summary**: In this paper we address the task of determining the geographical location of an image, a pertinent problem in learning and computer vision. This research was inspired from playing GeoGuessr, a game that tests a humans' ability to localize themselves using just images of their surroundings. In particular, we wish to investigate how geographical, ecological and man-made features generalize for random location prediction. This is framed as a classification problem: given images sampled from the USA, the most-probable state among 50 is predicted. Previous work uses models extensively trained on large, unfiltered online datasets that are primed towards specific locations. To this end, we create (and open-source) the 50States10K dataset - with 0.5 million Google Street View images of the country. A deep neural network based on the ResNet architecture is trained, and four different strategies of incorporating low-level cardinality information are presented. This model achieves an accuracy 20 times better than chance on a test dataset, which rises to 71.87% when taking the best of top-5 guesses. The network also beats human subjects in 4 out of 5 rounds of GeoGuessr.



### Hartley Spectral Pooling for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1810.04028v2
- **DOI**: 10.4208/csiam-am.2020
- **Categories**: **cs.CV**, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.04028v2)
- **Published**: 2018-10-07 06:57:01+00:00
- **Updated**: 2020-10-08 20:05:06+00:00
- **Authors**: Hao Zhang, Jianwei Ma
- **Comment**: 5 pages, 6 figures, letter
- **Journal**: CSIAM Transactions on Applied Mathematics, 2020, 1(3):518-529
- **Summary**: In most convolution neural networks (CNNs), downsampling hidden layers is adopted for increasing computation efficiency and the receptive field size. Such operation is commonly so-called pooling. Maximation and averaging over sliding windows (max/average pooling), and plain downsampling in the form of strided convolution are popular pooling methods. Since the pooling is a lossy procedure, a motivation of our work is to design a new pooling approach for less lossy in the dimensionality reduction. Inspired by the Fourier spectral pooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform based spectral pooling method in CNNs. Compared with FSP, the proposed spectral pooling avoids the use of complex arithmetic for frequency representation and reduces the computation. Spectral pooling preserves more structure features for network's discriminability than max and average pooling. We empirically show that Hartley spectral pooling gives rise to the convergence of training CNNs on MNIST and CIFAR-10 datasets.



### ASVRG: Accelerated Proximal SVRG
- **Arxiv ID**: http://arxiv.org/abs/1810.03105v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03105v2)
- **Published**: 2018-10-07 08:43:05+00:00
- **Updated**: 2018-11-17 17:38:34+00:00
- **Authors**: Fanhua Shang, Licheng Jiao, Kaiwen Zhou, James Cheng, Yan Ren, Yufei Jin
- **Comment**: 32 pages, 3 figures
- **Journal**: None
- **Summary**: This paper proposes an accelerated proximal stochastic variance reduced gradient (ASVRG) method, in which we design a simple and effective momentum acceleration trick. Unlike most existing accelerated stochastic variance reduction methods such as Katyusha, ASVRG has only one additional variable and one momentum parameter. Thus, ASVRG is much simpler than those methods, and has much lower per-iteration complexity. We prove that ASVRG achieves the best known oracle complexities for both strongly convex and non-strongly convex objectives. In addition, we extend ASVRG to mini-batch and non-smooth settings. We also empirically verify our theoretical results and show that the performance of ASVRG is comparable with, and sometimes even better than that of the state-of-the-art stochastic methods.



### Coronary Artery Centerline Extraction in Cardiac CT Angiography Using a CNN-Based Orientation Classifier
- **Arxiv ID**: http://arxiv.org/abs/1810.03143v2
- **DOI**: 10.1016/j.media.2018.10.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03143v2)
- **Published**: 2018-10-07 13:45:51+00:00
- **Updated**: 2018-10-24 13:20:35+00:00
- **Authors**: Jelmer M. Wolterink, Robbert W. van Hamersvelt, Max A. Viergever, Tim Leiner, Ivana Išgum
- **Comment**: Accepted in Medical Image Analysis
- **Journal**: None
- **Summary**: Coronary artery centerline extraction in cardiac CT angiography (CCTA) images is a prerequisite for evaluation of stenoses and atherosclerotic plaque. We propose an algorithm that extracts coronary artery centerlines in CCTA using a convolutional neural network (CNN).   A 3D dilated CNN is trained to predict the most likely direction and radius of an artery at any given point in a CCTA image based on a local image patch. Starting from a single seed point placed manually or automatically anywhere in a coronary artery, a tracker follows the vessel centerline in two directions using the predictions of the CNN. Tracking is terminated when no direction can be identified with high certainty.   The CNN was trained using 32 manually annotated centerlines in a training set consisting of 8 CCTA images provided in the MICCAI 2008 Coronary Artery Tracking Challenge (CAT08). Evaluation using 24 test images of the CAT08 challenge showed that extracted centerlines had an average overlap of 93.7% with 96 manually annotated reference centerlines. Extracted centerline points were highly accurate, with an average distance of 0.21 mm to reference centerline points. In a second test set consisting of 50 CCTA scans, 5,448 markers in the coronary arteries were used as seed points to extract single centerlines. This showed strong correspondence between extracted centerlines and manually placed markers. In a third test set containing 36 CCTA scans, fully automatic seeding and centerline extraction led to extraction of on average 92% of clinically relevant coronary artery segments.   The proposed method is able to accurately and efficiently determine the direction and radius of coronary arteries. The method can be trained with limited training data, and once trained allows fast automatic or interactive extraction of coronary artery trees from CCTA images.



### Finding Correspondences for Optical Flow and Disparity Estimations using a Sub-pixel Convolution-based Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1810.03155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.03155v1)
- **Published**: 2018-10-07 14:41:37+00:00
- **Updated**: 2018-10-07 14:41:37+00:00
- **Authors**: Juan Luis Gonzalez, Muhammad Sarmad, Hyunjoo J. Lee, Munchurl Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNN) have recently shown promising results in low-level computer vision problems such as optical flow and disparity estimation, but still, have much room to further improve their performance. In this paper, we propose a novel sub-pixel convolution-based encoder-decoder network for optical flow and disparity estimations, which can extend FlowNetS and DispNet by replacing the deconvolution layers with sup-pixel convolution blocks. By using sub-pixel refinement and estimation on the decoder stages instead of deconvolution, we can significantly improve the estimation accuracy for optical flow and disparity, even with reduced numbers of parameters. We show a supervised end-to-end training of our proposed networks for optical flow and disparity estimations, and an unsupervised end-to-end training for monocular depth and pose estimations. In order to verify the effectiveness of our proposed networks, we perform intensive experiments for (i) optical flow and disparity estimations, and (ii) monocular depth and pose estimations. Throughout the extensive experiments, our proposed networks outperform the baselines such as FlowNetS and DispNet in terms of estimation accuracy and training times.



### Fast and Robust Small Infrared Target Detection Using Absolute Directional Mean Difference Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1810.03173v4
- **DOI**: 10.1016/j.sigpro.2020.107727
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03173v4)
- **Published**: 2018-10-07 16:16:42+00:00
- **Updated**: 2020-07-29 03:44:00+00:00
- **Authors**: Saed Moradi, Payman Moallem, Mohamad Farzan Sabahi
- **Comment**: The Final version (Accepted in Signal Processing journal)
- **Journal**: None
- **Summary**: Infrared small target detection in an infrared search and track (IRST) system is a challenging task. This situation becomes more complicated when high gray-intensity structural backgrounds appear in the field of view (FoV) of the infrared seeker. While the majority of the infrared small target detection algorithms neglect directional information, in this paper, a directional approach is presented to suppress structural backgrounds and develop a more effective detection algorithm. To this end, a similar concept to the average absolute gray difference (AAGD) is utilized to construct a novel directional small target detection algorithm called absolute directional mean difference (ADMD). Also, an efficient implementation procedure is presented for the proposed algorithm. The proposed algorithm effectively enhances the target area and eliminates background clutter. Simulation results on real infrared images prove the significant effectiveness of the proposed algorithm.



### Conversational Group Detection With Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.04039v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.04039v2)
- **Published**: 2018-10-07 20:27:26+00:00
- **Updated**: 2019-05-05 16:52:03+00:00
- **Authors**: Mason Swofford, John Peruzzi, Marynel Vázquez
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Detection of interacting and conversational groups from images has applications in video surveillance and social robotics. In this paper we build on prior attempts to find conversational groups by detection of social gathering spaces called o-spaces used to assign people to groups. As our contributions to the task, we are the first paper to incorporate features extracted from the room layout image, and the first to incorporate a deep network to generate an image representation of the proposed o-spaces. Specifically, this novel network builds on the PointNet architecture which allows unordered inputs of variable sizes. We present accuracies which demonstrate the ability to rival and sometimes outperform the best models, but due to a data imbalance issue we do not yet outperform existing models in our test results.



### Image Completion on CIFAR-10
- **Arxiv ID**: http://arxiv.org/abs/1810.03213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03213v1)
- **Published**: 2018-10-07 21:44:32+00:00
- **Updated**: 2018-10-07 21:44:32+00:00
- **Authors**: Mason Swofford
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: This project performed image completion on CIFAR-10, a dataset of 60,000 32x32 RGB images, using three different neural network architectures: fully convolutional networks, convolutional networks with fully connected layers, and encoder-decoder convolutional networks. The highest performing model was a deep fully convolutional network, which was able to achieve a mean squared error of .015 when comparing the original image pixel values with the predicted pixel values. As well, this network was able to output in-painted images which appeared real to the human eye.




# Arxiv Papers in cs.CV on 2018-04-25
### Robust Anomaly-Based Ship Proposals Detection from Pan-sharpened High-Resolution Satellite Image
- **Arxiv ID**: http://arxiv.org/abs/1804.09322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09322v1)
- **Published**: 2018-04-25 02:19:50+00:00
- **Updated**: 2018-04-25 02:19:50+00:00
- **Authors**: Viet Hung Luu, Nguyen Hoang Hoa Luong, Quang Hung Bui, Thi Nhat Thanh Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Pre-screening of ship proposals is now employed by top ship detectors to avoid exhaustive search across image. In very high resolution (VHR) optical image, ships appeared as a cluster of abnormal bright pixels in open sea clutter (noise-like background). Anomaly-based detector utilizing Panchromatic (PAN) data has been widely used in many researches to detect ships, however, still facing two main drawbacks: 1) detection rate tend to be low particularly when a ship is low contrast and 2) these models require a high manual configuration to select a threshold value best separate ships from sea surface background. This paper aims at further investigation of anomaly-based model to solve those issues. First, pan-sharpened Multi Spectral (MS) data is incorporated together with PAN to enhance ship discrimination. Second, we propose an improved anomaly-based model combining both global intensity anomaly and local texture anomaly map. Regarding noise appeared due to the present of sea clutter and because of pan-sharpen process, texture abnormality suppression term based on quantization theory is introduced. Experimental results on VNREDSat-1 VHR optical satellite images suggest that the pan-sharpened near-infrared (P-NIR) band can improve discrimination of ships from surrounding waters. Compared to state-of-the-art anomaly-based detectors, our proposed anomaly-based model on the combination of PAN and P-NIR data cannot only achieved highest ship detection's recall rate (91.14% and 45.9% on high-contrast and low-contrast dataset respectively) but also robust to different automatic threshold selection techniques.



### Object Tracking in Satellite Videos Based on a Multi-Frame Optical Flow Tracker
- **Arxiv ID**: http://arxiv.org/abs/1804.09323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09323v1)
- **Published**: 2018-04-25 02:27:30+00:00
- **Updated**: 2018-04-25 02:27:30+00:00
- **Authors**: Bo Du, Shihan Cai, Chen Wu, Liangpei Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking is a hot topic in computer vision. Thanks to the booming of the very high resolution (VHR) remote sensing techniques, it is now possible to track targets of interests in satellite videos. However, since the targets in the satellite videos are usually too small compared with the entire image, and too similar with the background, most state-of-the-art algorithms failed to track the target in satellite videos with a satisfactory accuracy. Due to the fact that optical flow shows the great potential to detect even the slight movement of the targets, we proposed a multi-frame optical flow tracker (MOFT) for object tracking in satellite videos. The Lucas-Kanade optical flow method was fused with the HSV color system and integral image to track the targets in the satellite videos, while multi-frame difference method was utilized in the optical flow tracker for a better interpretation. The experiments with three VHR remote sensing satellite video datasets indicate that compared with state-of-the-art object tracking algorithms, the proposed method can track the target more accurately.



### Multi-focus Noisy Image Fusion using Low-Rank Representation
- **Arxiv ID**: http://arxiv.org/abs/1804.09325v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09325v7)
- **Published**: 2018-04-25 02:36:35+00:00
- **Updated**: 2022-01-29 06:34:11+00:00
- **Authors**: Hui Li, Xiao-Jun Wu, Tariq Durrani
- **Comment**: 17 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Multi-focus noisy image fusion represents an important task in the field of image fusion which generates a single, clear and focused image from all source images. In this paper, we propose a novel multi-focus noisy image fusion method based on low-rank representation (LRR) which is a powerful tool in representation learning. A multi-scale transform framework is adopted in which source images are decomposed into low frequency and high frequency coefficients, respectively. For low frequency coefficients, the fused low frequency coefficients are determined by a spatial frequency strategy, while the high frequency coefficients are fused by the LRR-based fusion strategy. Finally, the fused image is reconstructed by inverse multi-scale transforms with fused coefficients. Experimental results demonstrate that the proposed algorithm offers state-of-the-art performance even when the source images contain noise. The Code of our fusion method is available at https://github.com/hli1221/imagefusion_noisy_lrr



### Learning a Discriminative Feature Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.09337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09337v1)
- **Published**: 2018-04-25 03:49:30+00:00
- **Updated**: 2018-04-25 03:49:30+00:00
- **Authors**: Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang
- **Comment**: Accepted to CVPR 2018. 10 pages, 9 figures
- **Journal**: None
- **Summary**: Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset.



### Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1804.09347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09347v1)
- **Published**: 2018-04-25 04:54:34+00:00
- **Updated**: 2018-04-25 04:54:34+00:00
- **Authors**: Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, Yu-Chiang Frank Wang
- **Comment**: 7 pages, 3 figures. CVPR 2018 workshop paper
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.



### Driving Policy Transfer via Modularity and Abstraction
- **Arxiv ID**: http://arxiv.org/abs/1804.09364v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.09364v3)
- **Published**: 2018-04-25 06:20:12+00:00
- **Updated**: 2018-12-13 15:42:35+00:00
- **Authors**: Matthias Müller, Alexey Dosovitskiy, Bernard Ghanem, Vladlen Koltun
- **Comment**: Accepted at Conference on Robotic Learning (CoRL'18)
  http://proceedings.mlr.press/v87/mueller18a.html
- **Journal**: None
- **Summary**: End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents. The supplementary video can be viewed at https://youtu.be/BrMDJqI6H5U



### Quantitative Susceptibility Map Reconstruction Using Annihilating Filter-based Low-Rank Hankel Matrix Approach
- **Arxiv ID**: http://arxiv.org/abs/1804.09396v2
- **DOI**: 10.1002/mrm.27976
- **Categories**: **cs.CV**, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1804.09396v2)
- **Published**: 2018-04-25 07:22:48+00:00
- **Updated**: 2019-08-09 06:59:29+00:00
- **Authors**: Hyun-Seo Ahn, Sung-Hong Park, Jong Chul Ye
- **Comment**: accepted for Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Quantitative susceptibility mapping (QSM) inevitably suffers from streaking artifacts caused by zeros on the conical surface of the dipole kernel in k-space. This work proposes a novel and accurate QSM reconstruction method based on a direct k-space interpolation approach, avoiding problems of over smoothing and streaking artifacts. Inspired by the recent theory of annihilating filter-based low-rank Hankel matrix approach (ALOHA), QSM reconstruction problem is formulated as deconvolution problem under low-rank Hankel matrix constraint in the k-space. To reduce the computational complexity and the memory requirement, the problem is formulated as successive reconstruction of 2-D planes along three independent axes of the 3-D phase image in Fourier domain. Extensive experiments were performed to verify and compare the proposed method with existing QSM reconstruction methods. The proposed ALOHA-QSM effectively reduced streaking artifacts and accurately estimated susceptibility values in deep gray matter structures, compared to the existing QSM methods. Our suggested ALOHA-QSM algorithm successfully solves the three-dimensional QSM dipole inversion problem without additional anatomical information or prior assumption and provides good image quality and quantitative accuracy.



### Learnable Histogram: Statistical Context Features for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.09398v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09398v3)
- **Published**: 2018-04-25 07:30:47+00:00
- **Updated**: 2018-10-15 03:00:53+00:00
- **Authors**: Zhe Wang, Hongsheng Li, Wanli Ouyang, Xiaogang Wang
- **Comment**: refined some typos, ECCV 2016
- **Journal**: None
- **Summary**: Statistical features, such as histogram, Bag-of-Words (BoW) and Fisher Vector, were commonly used with hand-crafted features in conventional classification methods, but attract less attention since the popularity of deep learning methods. In this paper, we propose a learnable histogram layer, which learns histogram features within deep neural networks in end-to-end training. Such a layer is able to back-propagate (BP) errors, learn optimal bin centers and bin widths, and be jointly optimized with other layers in deep networks during training. Two vision problems, semantic segmentation and object detection, are explored by integrating the learnable histogram layer into deep networks, which show that the proposed layer could be well generalized to different applications. In-depth investigations are conducted to provide insights on the newly introduced layer.



### 3D Consistent & Robust Segmentation of Cardiac Images by Deep Learning with Spatial Propagation
- **Arxiv ID**: http://arxiv.org/abs/1804.09400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.09400v1)
- **Published**: 2018-04-25 07:39:36+00:00
- **Updated**: 2018-04-25 07:39:36+00:00
- **Authors**: Qiao Zheng, Hervé Delingette, Nicolas Duchateau, Nicholas Ayache
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: We propose a method based on deep learning to perform cardiac segmentation on short axis MRI image stacks iteratively from the top slice (around the base) to the bottom slice (around the apex). At each iteration, a novel variant of U-net is applied to propagate the segmentation of a slice to the adjacent slice below it. In other words, the prediction of a segmentation of a slice is dependent upon the already existing segmentation of an adjacent slice. 3D-consistency is hence explicitly enforced. The method is trained on a large database of 3078 cases from UK Biobank. It is then tested on 756 different cases from UK Biobank and three other state-of-the-art cohorts (ACDC with 100 cases, Sunnybrook with 30 cases, RVSC with 16 cases). Results comparable or even better than the state-of-the-art in terms of distance measures are achieved. They also emphasize the assets of our method, namely enhanced spatial consistency (currently neither considered nor achieved by the state-of-the-art), and the generalization ability to unseen cases even from other databases.



### Probabilistic Plant Modeling via Multi-View Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1804.09404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1804.09404v1)
- **Published**: 2018-04-25 07:44:52+00:00
- **Updated**: 2018-04-25 07:44:52+00:00
- **Authors**: Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, Yasushi Yagi
- **Comment**: To appear in CVPR2018. The first two authors contributed equally.
  Project website:
  http://www.am.sanken.osaka-u.ac.jp/~okura/project/cvpr2018_plant.html
- **Journal**: None
- **Summary**: This paper describes a method for inferring three-dimensional (3D) plant branch structures that are hidden under leaves from multi-view observations. Unlike previous geometric approaches that heavily rely on the visibility of the branches or use parametric branching models, our method makes statistical inferences of branch structures in a probabilistic framework. By inferring the probability of branch existence using a Bayesian extension of image-to-image translation applied to each of multi-view images, our method generates a probabilistic plant 3D model, which represents the 3D branching pattern that cannot be directly observed. Experiments demonstrate the usefulness of the proposed approach in generating convincing branch structures in comparison to prior approaches.



### Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents
- **Arxiv ID**: http://arxiv.org/abs/1804.09412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09412v1)
- **Published**: 2018-04-25 08:10:35+00:00
- **Updated**: 2018-04-25 08:10:35+00:00
- **Authors**: Bo Wang, Youjiang Xu, Yahong Han, Richang Hong
- **Comment**: Accepted by AAAI2018
- **Journal**: None
- **Summary**: Movies provide us with a mass of visual content as well as attracting stories. Existing methods have illustrated that understanding movie stories through only visual content is still a hard problem. In this paper, for answering questions about movies, we put forward a Layered Memory Network (LMN) that represents frame-level and clip-level movie content by the Static Word Memory module and the Dynamic Subtitle Memory module, respectively. Particularly, we firstly extract words and sentences from the training movie subtitles. Then the hierarchically formed movie representations, which are learned from LMN, not only encode the correspondence between words and visual content inside frames, but also encode the temporal alignment between sentences and frames inside movie clips. We also extend our LMN model into three variant frameworks to illustrate the good extendable capabilities. We conduct extensive experiments on the MovieQA dataset. With only visual content as inputs, LMN with frame-level representation obtains a large performance improvement. When incorporating subtitles into LMN to form the clip-level representation, we achieve the state-of-the-art performance on the online evaluation task of 'Video+Subtitles'. The good performance successfully demonstrates that the proposed framework of LMN is effective and the hierarchically formed movie representations have good potential for the applications of movie question answering.



### Weakly-Supervised Learning-Based Feature Localization in Confocal Laser Endomicroscopy Glioma Images
- **Arxiv ID**: http://arxiv.org/abs/1804.09428v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09428v2)
- **Published**: 2018-04-25 08:48:56+00:00
- **Updated**: 2018-08-18 06:18:39+00:00
- **Authors**: Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Claudio Cavallo, Xiaochun Zhao, Sirin Gandhi, Leandro Borba Moreira, Jennifer Eschbacher, Peter Nakaji, Mark C. Preul, Yezhou Yang
- **Comment**: Accepted in MICCAI 2018
- **Journal**: None
- **Summary**: Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imaging device that has shown promise for rapid intraoperative diagnosis of brain tumor tissue. Currently CLE is capable of image display only and lacks an automatic system to aid the surgeon in analyzing the images. The goal of this project was to develop a computer-aided diagnostic approach for CLE imaging of human glioma with feature localization function. Despite the tremendous progress in object detection and image segmentation methods in recent years, most of such methods require large annotated datasets for training. However, manual annotation of thousands of histopathological images by physicians is costly and time consuming. To overcome this problem, we propose a Weakly-Supervised Learning (WSL)-based model for feature localization that trains on image-level annotations, and then localizes incidences of a class-of-interest in the test image. We developed a novel convolutional neural network for diagnostic features localization from CLE images by employing a novel multiscale activation map that is laterally inhibited and collaterally integrated. To validate our method, we compared proposed model's output to the manual annotation performed by four neurosurgeons on test images. Proposed model achieved 88% mean accuracy and 86% mean intersection over union on intermediate features and 87% mean accuracy and 88% mean intersection over union on restrictive fine features, while outperforming other state of the art methods tested. This system can improve accuracy and efficiency in characterization of CLE images of glioma tissue during surgery, augment intraoperative decision-making process regarding tumor margin and affect resection rates.



### Dynamic Few-Shot Visual Learning without Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1804.09458v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.09458v1)
- **Published**: 2018-04-25 09:57:10+00:00
- **Updated**: 2018-04-25 09:57:10+00:00
- **Authors**: Spyros Gidaris, Nikos Komodakis
- **Comment**: Accepted at CVPR 2018. Code and models will be published on:
  https://github.com/gidariss/FewShotWithoutForgetting
- **Journal**: None
- **Summary**: The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on "unseen" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results. The code and models of our paper will be published on: https://github.com/gidariss/FewShotWithoutForgetting



### Analytical Modeling of Vanishing Points and Curves in Catadioptric Cameras
- **Arxiv ID**: http://arxiv.org/abs/1804.09460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.09460v1)
- **Published**: 2018-04-25 09:59:34+00:00
- **Updated**: 2018-04-25 09:59:34+00:00
- **Authors**: Pedro Miraldo, Francisco Eiras, Srikumar Ramalingam
- **Comment**: None
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2018
- **Summary**: Vanishing points and vanishing lines are classical geometrical concepts in perspective cameras that have a lineage dating back to 3 centuries. A vanishing point is a point on the image plane where parallel lines in 3D space appear to converge, whereas a vanishing line passes through 2 or more vanishing points. While such concepts are simple and intuitive in perspective cameras, their counterparts in catadioptric cameras (obtained using mirrors and lenses) are more involved. For example, lines in the 3D space map to higher degree curves in catadioptric cameras. The projection of a set of 3D parallel lines converges on a single point in perspective images, whereas they converge to more than one point in catadioptric cameras. To the best of our knowledge, we are not aware of any systematic development of analytical models for vanishing points and vanishing curves in different types of catadioptric cameras. In this paper, we derive parametric equations for vanishing points and vanishing curves using the calibration parameters, mirror shape coefficients, and direction vectors of parallel lines in 3D space. We show compelling experimental results on vanishing point estimation and absolute pose estimation for a wide range of catadioptric cameras in both simulations and real experiments.



### Zigzag Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1804.09466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09466v1)
- **Published**: 2018-04-25 10:26:57+00:00
- **Updated**: 2018-04-25 10:26:57+00:00
- **Authors**: Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong, Qi Tian
- **Comment**: accepted by CVPR 2018
- **Journal**: None
- **Summary**: This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overfitting initial seeds. Towards this goal, we first develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difficulty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difficulty. In this way, the model can be well prepared by training on easy examples for learning from more difficult ones and thus gain a stronger detection ability more efficiently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overfitting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difficulty of finding object instances properly. We achieve 47.6% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.



### RIFT: Multi-modal Image Matching Based on Radiation-invariant Feature Transform
- **Arxiv ID**: http://arxiv.org/abs/1804.09493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09493v1)
- **Published**: 2018-04-25 11:58:27+00:00
- **Updated**: 2018-04-25 11:58:27+00:00
- **Authors**: Jiayuan Li, Qingwu Hu, Mingyao Ai
- **Comment**: 14 pages,17 figures
- **Journal**: None
- **Summary**: Traditional feature matching methods such as scale-invariant feature transform (SIFT) usually use image intensity or gradient information to detect and describe feature points; however, both intensity and gradient are sensitive to nonlinear radiation distortions (NRD). To solve the problem, this paper proposes a novel feature matching algorithm that is robust to large NRD. The proposed method is called radiation-invariant feature transform (RIFT). There are three main contributions in RIFT: first, RIFT uses phase congruency (PC) instead of image intensity for feature point detection. RIFT considers both the number and repeatability of feature points, and detects both corner points and edge points on the PC map. Second, RIFT originally proposes a maximum index map (MIM) for feature description. MIM is constructed from the log-Gabor convolution sequence and is much more robust to NRD than traditional gradient map. Thus, RIFT not only largely improves the stability of feature detection, but also overcomes the limitation of gradient information for feature description. Third, RIFT analyzes the inherent influence of rotations on the values of MIM, and realizes rotation invariance. We use six different types of multi-model image datasets to evaluate RIFT, including optical-optical, infrared-optical, synthetic aperture radar (SAR)-optical, depth-optical, map-optical, and day-night datasets. Experimental results show that RIFT is much more superior to SIFT and SAR-SIFT. To the best of our knowledge, RIFT is the first feature matching algorithm that can achieve good performance on all the above-mentioned types of multi-model images. The source code of RIFT and multi-modal remote sensing image datasets are made public .



### Hand Pose Estimation via Latent 2.5D Heatmap Regression
- **Arxiv ID**: http://arxiv.org/abs/1804.09534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.09534v1)
- **Published**: 2018-04-25 13:16:26+00:00
- **Updated**: 2018-04-25 13:16:26+00:00
- **Authors**: Umar Iqbal, Pavlo Molchanov, Thomas Breuel, Juergen Gall, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the 3D pose of a hand is an essential part of human-computer interaction. Estimating 3D pose using depth or multi-view sensors has become easier with recent advances in computer vision, however, regressing pose from a single RGB image is much less straightforward. The main difficulty arises from the fact that 3D pose requires some form of depth estimates, which are ambiguous given only an RGB image. In this paper we propose a new method for 3D hand pose estimation from a monocular image through a novel 2.5D pose representation. Our new representation estimates pose up to a scaling factor, which can be estimated additionally if a prior of the hand size is given. We implicitly learn depth maps and heatmap distributions with a novel CNN architecture. Our system achieves the state-of-the-art estimation of 2D and 3D hand pose on several challenging datasets in presence of severe occlusions.



### Deep Convolutional AutoEncoder-based Lossy Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1804.09535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09535v1)
- **Published**: 2018-04-25 13:19:30+00:00
- **Updated**: 2018-04-25 13:19:30+00:00
- **Authors**: Zhengxue Cheng, Heming Sun, Masaru Takeuchi, Jiro Katto
- **Comment**: accepted by Picture Coding Symposium 2018
- **Journal**: None
- **Summary**: Image compression has been investigated as a fundamental research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and is gradually being used in image compression. In this paper, we present a lossy image compression architecture, which utilizes the advantages of convolutional autoencoder (CAE) to achieve a high coding efficiency. First, we design a novel CAE architecture to replace the conventional transforms and train this CAE using a rate-distortion loss function. Second, to generate a more energy-compact representation, we utilize the principal components analysis (PCA) to rotate the feature maps produced by the CAE, and then apply the quantization and entropy coder to generate the codes. Experimental results demonstrate that our method outperforms traditional image coding algorithms, by achieving a 13.7% BD-rate decrement on the Kodak database images compared to JPEG2000. Besides, our method maintains a moderate complexity similar to JPEG2000.



### Applying Faster R-CNN for Object Detection on Malaria Images
- **Arxiv ID**: http://arxiv.org/abs/1804.09548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09548v2)
- **Published**: 2018-04-25 13:30:39+00:00
- **Updated**: 2019-03-11 16:14:46+00:00
- **Authors**: Jane Hung, Deepali Ravel, Stefanie C. P. Lopes, Gabriel Rangel, Odailton Amaral Nery, Benoit Malleret, Francois Nosten, Marcus V. G. Lacerda, Marcelo U. Ferreira, Laurent Rénia, Manoj T. Duraisingh, Fabio T. M. Costa, Matthias Marti, Anne E. Carpenter
- **Comment**: CVPR 2017: computer vision for microscopy image analysis (CVMI)
  Workshop
- **Journal**: None
- **Summary**: Deep learning based models have had great success in object detection, but the state of the art models have not yet been widely applied to biological image data. We apply for the first time an object detection model previously used on natural images to identify cells and recognize their stages in brightfield microscopy images of malaria-infected blood. Many micro-organisms like malaria parasites are still studied by expert manual inspection and hand counting. This type of object detection task is challenging due to factors like variations in cell shape, density, and color, and uncertainty of some cell classes. In addition, annotated data useful for training is scarce, and the class distribution is inherently highly imbalanced due to the dominance of uninfected red blood cells. We use Faster Region-based Convolutional Neural Network (Faster R-CNN), one of the top performing object detection models in recent years, pre-trained on ImageNet but fine tuned with our data, and compare it to a baseline, which is based on a traditional approach consisting of cell segmentation, extraction of several single-cell features, and classification using random forests. To conduct our initial study, we collect and label a dataset of 1300 fields of view consisting of around 100,000 individual cells. We demonstrate that Faster R-CNN outperforms our baseline and put the results in context of human performance.



### SegMap: 3D Segment Mapping using Data-Driven Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1804.09557v2
- **DOI**: 10.15607/RSS.2018.XIV.003
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.09557v2)
- **Published**: 2018-04-25 13:41:55+00:00
- **Updated**: 2019-01-15 11:03:34+00:00
- **Authors**: Renaud Dubé, Andrei Cramariuc, Daniel Dugas, Juan Nieto, Roland Siegwart, Cesar Cadena
- **Comment**: None
- **Journal**: None
- **Summary**: When performing localization and mapping, working at the level of structure can be advantageous in terms of robustness to environmental changes and differences in illumination. This paper presents SegMap: a map representation solution to the localization and mapping problem based on the extraction of segments in 3D point clouds. In addition to facilitating the computationally intensive task of processing 3D point clouds, working at the level of segments addresses the data compression requirements of real-time single- and multi-robot systems. While current methods extract descriptors for the single task of localization, SegMap leverages a data-driven descriptor in order to extract meaningful features that can also be used for reconstructing a dense 3D map of the environment and for extracting semantic information. This is particularly interesting for navigation tasks and for providing visual feedback to end-users such as robot operators, for example in search and rescue scenarios. These capabilities are demonstrated in multiple urban driving and search and rescue experiments. Our method leads to an increase of area under the ROC curve of 28.3% over current state of the art using eigenvalue based features. We also obtain very similar reconstruction capabilities to a model specifically trained for this task. The SegMap implementation will be made available open-source along with easy to run demonstrations at www.github.com/ethz-asl/segmap. A video demonstration is available at https://youtu.be/CMk4w4eRobg.



### Unsupervised Domain Adaptation with Adversarial Residual Transform Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.09578v2
- **DOI**: 10.1109/TNNLS.2019.2935384
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09578v2)
- **Published**: 2018-04-25 14:02:25+00:00
- **Updated**: 2019-09-18 11:29:01+00:00
- **Authors**: Guanyu Cai, Yuqin Wang, Mengchu Zhou, Lianghua He
- **Comment**: accepted by IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Domain adaptation is widely used in learning problems lacking labels. Recent studies show that deep adversarial domain adaptation models can make markable improvements in performance, which include symmetric and asymmetric architectures. However, the former has poor generalization ability whereas the latter is very hard to train. In this paper, we propose a novel adversarial domain adaptation method named Adversarial Residual Transform Networks (ARTNs) to improve the generalization ability, which directly transforms the source features into the space of target features. In this model, residual connections are used to share features and adversarial loss is reconstructed, thus making the model more generalized and easier to train. Moreover, a special regularization term is added to the loss function to alleviate a vanishing gradient problem, which enables its training process stable. A series of experiments based on Amazon review dataset, digits datasets and Office-31 image datasets are conducted to show that the proposed ARTN can be comparable with the methods of the state-of-the-art.



### Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.09626v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09626v2)
- **Published**: 2018-04-25 15:30:27+00:00
- **Updated**: 2018-04-30 16:57:00+00:00
- **Authors**: Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, Karteek Alahari
- **Comment**: None
- **Journal**: None
- **Summary**: In Actor and Observer we introduced a dataset linking the first and third-person video understanding domains, the Charades-Ego Dataset. In this paper we describe the egocentric aspect of the dataset and present annotations for Charades-Ego with 68,536 activity instances in 68.8 hours of first and third-person video, making it one of the largest and most diverse egocentric datasets available. Charades-Ego furthermore shares activity classes, scripts, and methodology with the Charades dataset, that consist of additional 82.3 hours of third-person video with 66,500 activity instances. Charades-Ego has temporal annotations and textual descriptions, making it suitable for egocentric video classification, localization, captioning, and new tasks utilizing the cross-modal nature of the data.



### Actor and Observer: Joint Modeling of First and Third-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.09627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09627v1)
- **Published**: 2018-04-25 15:30:34+00:00
- **Updated**: 2018-04-25 15:30:34+00:00
- **Authors**: Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, Karteek Alahari
- **Comment**: CVPR 2018 spotlight presentation
- **Journal**: CVPR 2018
- **Summary**: Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos, involving 112 people, with 4000 paired videos. This enables learning the link between the two, actor and observer perspectives. Thereby, we address one of the biggest bottlenecks facing egocentric vision research, providing a link from first-person to the abundant third-person data on the web. We use this data to learn a joint representation of first and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the first-person domain.



### 3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object Deformations
- **Arxiv ID**: http://arxiv.org/abs/1805.00328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00328v2)
- **Published**: 2018-04-25 15:53:03+00:00
- **Updated**: 2018-10-24 09:57:44+00:00
- **Authors**: Zhihua Wang, Stefano Rosa, Bo Yang, Sen Wang, Niki Trigoni, Andrew Markham
- **Comment**: in IJCAI 2018
- **Journal**: None
- **Summary**: The ability to interact and understand the environment is a fundamental prerequisite for a wide range of applications from robotics to augmented reality. In particular, predicting how deformable objects will react to applied forces in real time is a significant challenge. This is further confounded by the fact that shape information about encountered objects in the real world is often impaired by occlusions, noise and missing regions e.g. a robot manipulating an object will only be able to observe a partial view of the entire solid. In this work we present a framework, 3D-PhysNet, which is able to predict how a three-dimensional solid will deform under an applied force using intuitive physics modelling. In particular, we propose a new method to encode the physical properties of the material and the applied force, enabling generalisation over materials. The key is to combine deep variational autoencoders with adversarial training, conditioned on the applied force and the material properties. We further propose a cascaded architecture that takes a single 2.5D depth view of the object and predicts its deformation. Training data is provided by a physics simulator. The network is fast enough to be used in real-time applications from partial views. Experimental results show the viability and the generalisation properties of the proposed architecture.



### Automatic Latent Fingerprint Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.09650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09650v2)
- **Published**: 2018-04-25 16:09:02+00:00
- **Updated**: 2018-09-04 02:20:28+00:00
- **Authors**: Dinh-Luan Nguyen, Kai Cao, Anil K. Jain
- **Comment**: Accepted (Oral) in BTAS 2018
- **Journal**: None
- **Summary**: We present a simple but effective method for automatic latent fingerprint segmentation, called SegFinNet. SegFinNet takes a latent image as an input and outputs a binary mask highlighting the friction ridge pattern. Our algorithm combines fully convolutional neural network and detection-based approaches to process the entire input latent image in one shot instead of using latent patches. Experimental results on three different latent databases (i.e. NIST SD27, WVU, and an operational forensic database) show that SegFinNet outperforms both human markup for latents and the state-of-the-art latent segmentation algorithms. We further show that this improved cropping boosts the hit rate of a latent fingerprint matcher.



### DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1804.09669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09669v2)
- **Published**: 2018-04-25 16:32:07+00:00
- **Updated**: 2018-04-26 04:24:21+00:00
- **Authors**: Skand Vishwanath Peri, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes our approach for the Disguised Faces in the Wild (DFW) 2018 challenge. The task here is to verify the identity of a person among disguised and impostors images. Given the importance of the task of face verification it is essential to compare methods across a common platform. Our approach is based on VGG-face architecture paired with Contrastive loss based on cosine distance metric. For augmenting the data set, we source more data from the internet. The experiments show the effectiveness of the approach on the DFW data. We show that adding extra data to the DFW dataset with noisy labels also helps in increasing the generalization performance of the network. The proposed network achieves 27.13% absolute increase in accuracy over the DFW baseline.



### Fast View Synthesis with Deep Stereo Vision
- **Arxiv ID**: http://arxiv.org/abs/1804.09690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.09690v2)
- **Published**: 2018-04-25 17:35:50+00:00
- **Updated**: 2018-05-07 11:54:58+00:00
- **Authors**: Tewodros Habtegebrial, Kiran Varanasi, Christian Bailer, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Novel view synthesis is an important problem in computer vision and graphics. Over the years a large number of solutions have been put forward to solve the problem. However, the large-baseline novel view synthesis problem is far from being "solved". Recent works have attempted to use Convolutional Neural Networks (CNNs) to solve view synthesis tasks. Due to the difficulty of learning scene geometry and interpreting camera motion, CNNs are often unable to generate realistic novel views. In this paper, we present a novel view synthesis approach based on stereo-vision and CNNs that decomposes the problem into two sub-tasks: view dependent geometry estimation and texture inpainting. Both tasks are structured prediction problems that could be effectively learned with CNNs. Experiments on the KITTI Odometry dataset show that our approach is more accurate and significantly faster than the current state-of-the-art. The code and supplementary material will be publicly available. Results could be found here https://youtu.be/5pzS9jc-5t0



### Surveillance Face Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/1804.09691v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09691v6)
- **Published**: 2018-04-25 17:36:06+00:00
- **Updated**: 2018-08-29 15:51:31+00:00
- **Authors**: Zhiyi Cheng, Xiatian Zhu, Shaogang Gong
- **Comment**: The QMUL-SurvFace challenge is publicly available at
  https://qmul-survface.github.io/
- **Journal**: None
- **Summary**: Face recognition (FR) is one of the most extensively investigated problems in computer vision. Significant progress in FR has been made due to the recent introduction of the larger scale FR challenges, particularly with constrained social media web images, e.g. high-resolution photos of celebrity faces taken by professional photo-journalists. However, the more challenging FR in unconstrained and low-resolution surveillance images remains largely under-studied. To facilitate more studies on developing FR models that are effective and robust for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, which we call the QMUL-SurvFace benchmark. This new benchmark is the largest and more importantly the only true surveillance FR benchmark to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time. As a consequence, it presents an extremely challenging FR benchmark. We benchmark the FR performance on this challenge using five representative deep learning face recognition models, in comparison to existing benchmarks. We show that the current state of the arts are still far from being satisfactory to tackle the under-investigated surveillance FR problem in practical forensic scenarios. Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, owing to a large number of non-target people (distractors) appearing open spaced scenes. This is evidently so that on the new Surveillance FR Challenge, the top-performing CentreFace deep learning FR model on the MegaFace benchmark can now only achieve 13.2% success rate (at Rank-20) at a 10% false alarm rate.



### Towards Fast Computation of Certified Robustness for ReLU Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.09699v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.09699v4)
- **Published**: 2018-04-25 17:47:56+00:00
- **Updated**: 2018-10-02 08:25:08+00:00
- **Authors**: Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S. Dhillon, Luca Daniel
- **Comment**: Tsui-Wei Weng and Huan Zhang contributed equally
- **Journal**: None
- **Summary**: Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer CAV17]. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or delivering low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms Fast-Lin and Fast-Lip that are able to certify non-trivial lower bounds of minimum distortions, by bounding the ReLU units with appropriate linear functions Fast-Lin, or by bounding the local Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods deliver bounds close to (the gap is 2-3X) exact minimum distortion found by Reluplex in small MNIST networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core.   In addition, we show that, in fact, there is no polynomial time algorithm that can approximately find the minimum $\ell_1$ adversarial distortion of a ReLU network with a $0.99\ln n$ approximation ratio unless $\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network.



### Progressive Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.09803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.09803v1)
- **Published**: 2018-04-25 21:22:27+00:00
- **Updated**: 2018-04-25 21:22:27+00:00
- **Authors**: Zhi Zhang, Guanghan Ning, Yigang Cen, Yang Li, Zhiqun Zhao, Hao Sun, Zhihai He
- **Comment**: None
- **Journal**: None
- **Summary**: The inference structures and computational complexity of existing deep neural networks, once trained, are fixed and remain the same for all test images. However, in practice, it is highly desirable to establish a progressive structure for deep neural networks which is able to adapt its inference process and complexity for images with different visual recognition complexity. In this work, we develop a multi-stage progressive structure with integrated confidence analysis and decision policy learning for deep neural networks. This new framework consists of a set of network units to be activated in a sequential manner with progressively increased complexity and visual recognition power. Our extensive experimental results on the CIFAR-10 and ImageNet datasets demonstrate that the proposed progressive deep neural network is able to obtain more than 10 fold complexity scalability while achieving the state-of-the-art performance using a single network model satisfying different complexity-accuracy requirements.



### The Intelligent ICU Pilot Study: Using Artificial Intelligence Technology for Autonomous Patient Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1804.10201v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1804.10201v2)
- **Published**: 2018-04-25 21:24:46+00:00
- **Updated**: 2018-09-26 18:25:32+00:00
- **Authors**: Anis Davoudi, Kumar Rohit Malhotra, Benjamin Shickel, Scott Siegel, Seth Williams, Matthew Ruppert, Emel Bihorac, Tezcan Ozrazgat-Baslanti, Patrick J. Tighe, Azra Bihorac, Parisa Rashidi
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, many critical care indices are repetitively assessed and recorded by overburdened nurses, e.g. physical function or facial pain expressions of nonverbal patients. In addition, many essential information on patients and their environment are not captured at all, or are captured in a non-granular manner, e.g. sleep disturbance factors such as bright light, loud background noise, or excessive visitations. In this pilot study, we examined the feasibility of using pervasive sensing technology and artificial intelligence for autonomous and granular monitoring of critically ill patients and their environment in the Intensive Care Unit (ICU). As an exemplar prevalent condition, we also characterized delirious and non-delirious patients and their environment. We used wearable sensors, light and sound sensors, and a high-resolution camera to collected data on patients and their environment. We analyzed collected data using deep learning and statistical analysis. Our system performed face detection, face recognition, facial action unit detection, head pose detection, facial expression recognition, posture recognition, actigraphy analysis, sound pressure and light level detection, and visitation frequency detection. We were able to detect patient's face (Mean average precision (mAP)=0.94), recognize patient's face (mAP=0.80), and their postures (F1=0.94). We also found that all facial expressions, 11 activity features, visitation frequency during the day, visitation frequency during the night, light levels, and sound pressure levels during the night were significantly different between delirious and non-delirious patients (p-value<0.05). In summary, we showed that granular and autonomous monitoring of critically ill patients and their environment is feasible and can be used for characterizing critical care conditions and related environment factors.




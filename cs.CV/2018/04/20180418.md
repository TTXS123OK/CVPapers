# Arxiv Papers in cs.CV on 2018-04-18
### Structure from Recurrent Motion: From Rigidity to Recurrency
- **Arxiv ID**: http://arxiv.org/abs/1804.06510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06510v1)
- **Published**: 2018-04-18 00:17:23+00:00
- **Updated**: 2018-04-18 00:17:23+00:00
- **Authors**: Xiu Li, Hongdong Li, Hanbyul Joo, Yebin Liu, Yaser Sheikh
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: This paper proposes a new method for Non-Rigid Structure-from-Motion (NRSfM) from a long monocular video sequence observing a non-rigid object performing recurrent and possibly repetitive dynamic action. Departing from the traditional idea of using linear low-order or lowrank shape model for the task of NRSfM, our method exploits the property of shape recurrency (i.e., many deforming shapes tend to repeat themselves in time). We show that recurrency is in fact a generalized rigidity. Based on this, we reduce NRSfM problems to rigid ones provided that certain recurrency condition is satisfied. Given such a reduction, standard rigid-SfM techniques are directly applicable (without any change) to the reconstruction of non-rigid dynamic shapes. To implement this idea as a practical approach, this paper develops efficient algorithms for automatic recurrency detection, as well as camera view clustering via a rigidity-check. Experiments on both simulated sequences and real data demonstrate the effectiveness of the method. Since this paper offers a novel perspective on rethinking structure-from-motion, we hope it will inspire other new problems in the field.



### Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization
- **Arxiv ID**: http://arxiv.org/abs/1804.06516v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06516v3)
- **Published**: 2018-04-18 00:48:40+00:00
- **Updated**: 2018-04-23 18:03:02+00:00
- **Authors**: Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, Stan Birchfield
- **Comment**: CVPR 2018 Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator$-$such as lighting, pose, object textures, etc.$-$are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds$-$both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.



### Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.06534v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06534v2)
- **Published**: 2018-04-18 03:03:42+00:00
- **Updated**: 2018-07-10 23:14:49+00:00
- **Authors**: Jonathan Tremblay, Thang To, Stan Birchfield
- **Comment**: CVPR 2018 Workshop on Real World Challenges and New Benchmarks for
  Deep Learning in Robotic Vision
- **Journal**: None
- **Summary**: We present a new dataset, called Falling Things (FAT), for advancing the state-of-the-art in object detection and 3D pose estimation in the context of robotics. By synthetically combining object models and backgrounds of complex composition and high graphical quality, we are able to generate photorealistic images with accurate 3D pose annotations for all objects in all images. Our dataset contains 60k annotated photos of 21 household objects taken from the YCB dataset. For each image, we provide the 3D poses, per-pixel class segmentation, and 2D/3D bounding box coordinates for all objects. To facilitate testing different input modalities, we provide mono and stereo RGB images, along with registered dense depth images. We describe in detail the generation process and statistical analysis of the data.



### SFace: An Efficient Network for Face Detection in Large Scale Variations
- **Arxiv ID**: http://arxiv.org/abs/1804.06559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06559v2)
- **Published**: 2018-04-18 05:25:57+00:00
- **Updated**: 2018-04-23 09:40:22+00:00
- **Authors**: Jianfeng Wang, Ye Yuan, Boxun Li, Gang Yu, Sun Jian
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection serves as a fundamental research topic for many applications like face recognition. Impressive progress has been made especially with the recent development of convolutional neural networks. However, the issue of large scale variations, which widely exists in high resolution images/videos, has not been well addressed in the literature. In this paper, we present a novel algorithm called SFace, which efficiently integrates the anchor-based method and anchor-free method to address the scale issues. A new dataset called 4K-Face is also introduced to evaluate the performance of face detection with extreme large scale variations. The SFace architecture shows promising results on the new 4K-Face benchmarks. In addition, our method can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset, which outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieves comparative performance.



### Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines
- **Arxiv ID**: http://arxiv.org/abs/1804.06579v2
- **DOI**: 10.1145/3182158
- **Categories**: **cs.GR**, cs.CV, 68U05, H.4.2
- **Links**: [PDF](http://arxiv.org/pdf/1804.06579v2)
- **Published**: 2018-04-18 07:23:18+00:00
- **Updated**: 2022-01-15 05:43:13+00:00
- **Authors**: Fenggen Yu, Yan Zhang, Kai Xu, Ali Mahdavi-Amiri, Hao Zhang
- **Comment**: 17 pages, 25 figures
- **Journal**: ACM Transactions on Graphics 37(2). February 2018
- **Summary**: We present a semi-supervised co-analysis method for learning 3D shape styles from projected feature lines, achieving style patch localization with only weak supervision. Given a collection of 3D shapes spanning multiple object categories and styles, we perform style co-analysis over projected feature lines of each 3D shape and then backproject the learned style features onto the 3D shapes. Our core analysis pipeline starts with mid-level patch sampling and pre-selection of candidate style patches. Projective features are then encoded via patch convolution. Multi-view feature integration and style clustering are carried out under the framework of partially shared latent factor (PSLF) learning, a multi-view feature learning scheme. PSLF achieves effective multi-view feature fusion by distilling and exploiting consistent and complementary feature information from multiple views, while also selecting style patches from the candidates. Our style analysis approach supports both unsupervised and semi-supervised analysis. For the latter, our method accepts both user-specified shape labels and style-ranked triplets as clustering constraints.We demonstrate results from 3D shape style analysis and patch localization as well as improvements over state-of-the-art methods. We also present several applications enabled by our style analysis.



### PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation
- **Arxiv ID**: http://arxiv.org/abs/1804.06604v2
- **DOI**: 10.1145/3240508.3240599
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1804.06604v2)
- **Published**: 2018-04-18 08:44:11+00:00
- **Updated**: 2018-08-07 09:10:34+00:00
- **Authors**: Ana García del Molino, Michael Gygli
- **Comment**: Accepted for publication at the 2018 ACM Multimedia Conference (MM
  '18)
- **Journal**: None
- **Summary**: Highlight detection models are typically trained to identify cues that make visual content appealing or interesting for the general public, with the objective of reducing a video to such moments. However, the "interestingness" of a video segment or image is subjective. Thus, such highlight models provide results of limited relevance for the individual user. On the other hand, training one model per user is inefficient and requires large amounts of personal information which is typically not available. To overcome these limitations, we present a global ranking model which conditions on each particular user's interests. Rather than training one model per user, our model is personalized via its inputs, which allows it to effectively adapt its predictions, given only a few user-specific examples. To train this model, we create a large-scale dataset of users and the GIFs they created, giving us an accurate indication of their interests. Our experiments show that using the user history substantially improves the prediction accuracy. On our test set of 850 videos, our model improves the recall by 8% with respect to generic highlight detectors. Furthermore, our method proves more precise than the user-agnostic baselines even with just one person-specific example.



### Variational Disparity Estimation Framework for Plenoptic Image
- **Arxiv ID**: http://arxiv.org/abs/1804.06633v1
- **DOI**: 10.1109/ICME.2017.8019377
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06633v1)
- **Published**: 2018-04-18 10:26:52+00:00
- **Updated**: 2018-04-18 10:26:52+00:00
- **Authors**: Trung-Hieu Tran, Zhe Wang, Sven Simon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a computational framework for accurately estimating the disparity map of plenoptic images. The proposed framework is based on the variational principle and provides intrinsic sub-pixel precision. The light-field motion tensor introduced in the framework allows us to combine advanced robust data terms as well as provides explicit treatments for different color channels. A warping strategy is embedded in our framework for tackling the large displacement problem. We also show that by applying a simple regularization term and a guided median filtering, the accuracy of displacement field at occluded area could be greatly enhanced. We demonstrate the excellent performance of the proposed framework by intensive comparisons with the Lytro software and contemporary approaches on both synthetic and real-world datasets.



### Superframes, A Temporal Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.06642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06642v2)
- **Published**: 2018-04-18 10:39:01+00:00
- **Updated**: 2019-03-06 15:42:10+00:00
- **Authors**: Hajar Sadeghi Sokeh, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: The goal of video segmentation is to turn video data into a set of concrete motion clusters that can be easily interpreted as building blocks of the video. There are some works on similar topics like detecting scene cuts in a video, but there is few specific research on clustering video data into the desired number of compact segments. It would be more intuitive, and more efficient, to work with perceptually meaningful entity obtained from a low-level grouping process which we call it superframe. This paper presents a new simple and efficient technique to detect superframes of similar content patterns in videos. We calculate the similarity of content-motion to obtain the strength of change between consecutive frames. With the help of existing optical flow technique using deep models, the proposed method is able to perform more accurate motion estimation efficiently. We also propose two criteria for measuring and comparing the performance of different algorithms on various databases. Experimental results on the videos from benchmark databases have demonstrated the effectiveness of the proposed method.



### Deep Face Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1804.06655v9
- **DOI**: 10.1016/j.neucom.2020.10.081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06655v9)
- **Published**: 2018-04-18 11:20:32+00:00
- **Updated**: 2020-08-01 13:12:07+00:00
- **Authors**: Mei Wang, Weihong Deng
- **Comment**: Neurocomputing
- **Journal**: Neurocomputing, 2021, 429:215-244
- **Summary**: Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: "one-to-many augmentation" and "many-to-one normalization". Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.



### Active Learning for Breast Cancer Identification
- **Arxiv ID**: http://arxiv.org/abs/1804.06670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06670v1)
- **Published**: 2018-04-18 12:08:25+00:00
- **Updated**: 2018-04-18 12:08:25+00:00
- **Authors**: Xinpeng Xie, Yuexiang Li, Linlin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is the second most common malignancy among women and has become a major public health problem in current society. Traditional breast cancer identification requires experienced pathologists to carefully read the breast slice, which is laborious and suffers from inter-observer variations. Consequently, an automatic classification framework for breast cancer identification is worthwhile to develop. Recent years witnessed the development of deep learning technique. Increasing number of medical applications start to use deep learning to improve diagnosis accuracy. In this paper, we proposed a novel training strategy, namely reversed active learning (RAL), to train network to automatically classify breast cancer images. Our RAL is applied to the training set of a simple convolutional neural network (CNN) to remove mislabeled images. We evaluate the CNN trained with RAL on publicly available ICIAR 2018 Breast Cancer Dataset (IBCD). The experimental results show that our RAL increases the slice-based accuracy of CNN from 93.75% to 96.25%.



### Understanding Neural Networks and Individual Neuron Importance via Information-Ordered Cumulative Ablation
- **Arxiv ID**: http://arxiv.org/abs/1804.06679v4
- **DOI**: 10.1109/TNNLS.2021.3088685
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.06679v4)
- **Published**: 2018-04-18 12:29:24+00:00
- **Updated**: 2021-06-09 15:28:37+00:00
- **Authors**: Rana Ali Amjad, Kairen Liu, Bernhard C. Geiger
- **Comment**: 12 pages; accepted for publication in IEEE Transactions on Neural
  Networks and Learning Systems
- **Journal**: IEEE Trans. Neural Networks and Learning Systems 33(12):7842-7852
- **Summary**: In this work, we investigate the use of three information-theoretic quantities -- entropy, mutual information with the class variable, and a class selectivity measure based on Kullback-Leibler divergence -- to understand and study the behavior of already trained fully-connected feed-forward neural networks. We analyze the connection between these information-theoretic quantities and classification performance on the test set by cumulatively ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our results parallel those recently published by Morcos et al., indicating that class selectivity is not a good indicator for classification performance. However, looking at individual layers separately, both mutual information and class selectivity are positively correlated with classification performance, at least for networks with ReLU activation functions. We provide explanations for this phenomenon and conclude that it is ill-advised to compare the proposed information-theoretic quantities across layers. Furthermore, we show that cumulative ablation of neurons with ascending or descending information-theoretic quantities can be used to formulate hypotheses regarding the joint behavior of multiple neurons, such as redundancy and synergy, with comparably low computational cost. We also draw connections to the information bottleneck theory for neural networks.



### Temporal Unknown Incremental Clustering (TUIC) Model for Analysis of Traffic Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/1804.06680v1
- **DOI**: 10.1109/TITS.2018.2834958
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06680v1)
- **Published**: 2018-04-18 12:29:42+00:00
- **Updated**: 2018-04-18 12:29:42+00:00
- **Authors**: Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Optimized scene representation is an important characteristic of a framework for detecting abnormalities on live videos. One of the challenges for detecting abnormalities in live videos is real-time detection of objects in a non-parametric way. Another challenge is to efficiently represent the state of objects temporally across frames. In this paper, a Gibbs sampling based heuristic model referred to as Temporal Unknown Incremental Clustering (TUIC) has been proposed to cluster pixels with motion. Pixel motion is first detected using optical flow and a Bayesian algorithm has been applied to associate pixels belonging to similar cluster in subsequent frames. The algorithm is fast and produces accurate results in $\Theta(kn)$ time, where $k$ is the number of clusters and $n$ the number of pixels. Our experimental validation with publicly available datasets reveals that the proposed framework has good potential to open-up new opportunities for real-time traffic analysis.



### Liveness Detection Using Implicit 3D Features
- **Arxiv ID**: http://arxiv.org/abs/1804.06702v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06702v2)
- **Published**: 2018-04-18 13:12:35+00:00
- **Updated**: 2018-04-19 13:16:10+00:00
- **Authors**: J. Matias Di Martino, Qiang Qiu, Trishul Nagenalli, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Spoofing attacks are a threat to modern face recognition systems. In this work we present a simple yet effective liveness detection approach to enhance 2D face recognition methods and make them robust against spoofing attacks. We show that the risk to spoofing attacks can be re- duced through the use of an additional source of light, for example a flash. From a pair of input images taken under different illumination, we define discriminative features that implicitly contain facial three-dimensional in- formation. Furthermore, we show that when multiple sources of light are considered, we are able to validate which one has been activated. This makes possible the design of a highly secure active-light authentication framework. Finally, further investigating the use of 3D features without 3D reconstruction, we introduce an approximated disparity-based implicit 3D feature obtained from an uncalibrated stereo-pair of cameras. Valida- tion experiments show that the proposed methods produce state-of-the-art results in challenging scenarios with nearly no feature extraction latency.



### Quantifying the visual concreteness of words and topics in multimodal datasets
- **Arxiv ID**: http://arxiv.org/abs/1804.06786v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1804.06786v2)
- **Published**: 2018-04-18 15:23:04+00:00
- **Updated**: 2018-05-23 19:15:45+00:00
- **Authors**: Jack Hessel, David Mimno, Lillian Lee
- **Comment**: NAACL HLT 2018, 14 pages, 6 figures, data available at
  http://www.cs.cornell.edu/~jhessel/concreteness/concreteness.html
- **Journal**: 2018 North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies (NAACL HLT)
- **Summary**: Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.



### ECG arrhythmia classification using a 2-D convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1804.06812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06812v1)
- **Published**: 2018-04-18 16:54:57+00:00
- **Updated**: 2018-04-18 16:54:57+00:00
- **Authors**: Tae Joon Jun, Hoang Minh Nguyen, Daeyoun Kang, Dohyeun Kim, Daeyoung Kim, Young-Hak Kim
- **Comment**: Submitted to journal
- **Journal**: None
- **Summary**: In this paper, we propose an effective electrocardiogram (ECG) arrhythmia classification method using a deep two-dimensional convolutional neural network (CNN) which recently shows outstanding performance in the field of pattern recognition. Every ECG beat was transformed into a two-dimensional grayscale image as an input data for the CNN classifier. Optimization of the proposed CNN classifier includes various deep learning techniques such as batch normalization, data augmentation, Xavier initialization, and dropout. In addition, we compared our proposed classifier with two well-known CNN models; AlexNet and VGGNet. ECG recordings from the MIT-BIH arrhythmia database were used for the evaluation of the classifier. As a result, our classifier achieved 99.05% average accuracy with 97.85% average sensitivity. To precisely validate our CNN classifier, 10-fold cross-validation was performed at the evaluation which involves every ECG recording as a test data. Our experimental results have successfully validated that the proposed CNN classifier with the transformed ECG images can achieve excellent classification accuracy without any manual pre-processing of the ECG signals such as noise filtering, feature extraction, and feature reduction.



### Automated detection of vulnerable plaque in intravascular ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/1804.06817v1
- **DOI**: 10.1007/s11517-018-1925-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06817v1)
- **Published**: 2018-04-18 17:09:24+00:00
- **Updated**: 2018-04-18 17:09:24+00:00
- **Authors**: Tae Joon Jun, Soo-Jin Kang, June-Goo Lee, Jihoon Kweon, Wonjun Na, Daeyoun Kang, Dohyeun Kim, Daeyoung Kim, Young-Hak Kim
- **Comment**: Submitted to journal
- **Journal**: None
- **Summary**: Acute Coronary Syndrome (ACS) is a syndrome caused by a decrease in blood flow in the coronary arteries. The ACS is usually related to coronary thrombosis and is primarily caused by plaque rupture followed by plaque erosion and calcified nodule. Thin-cap fibroatheroma (TCFA) is known to be the most similar lesion morphologically to a plaque rupture. In this paper, we propose methods to classify TCFA using various machine learning classifiers including Feed-forward Neural Network (FNN), K-Nearest Neighbor (KNN), Random Forest (RF) and Convolutional Neural Network (CNN) to figure out a classifier that shows optimal TCFA classification accuracy. In addition, we suggest pixel range based feature extraction method to extract the ratio of pixels in the different region of interests to reflect the physician's TCFA discrimination criteria. A total of 12,325 IVUS images were labeled with corresponding OCT images to train and evaluate the classifiers. We achieved 0.884, 0.890, 0.878 and 0.933 Area Under the ROC Curve (AUC) in the order of using FNN, KNN, RF and CNN classifier. As a result, the CNN classifier performed best and the top 10 features of the feature-based classifiers (FNN, KNN, RF) were found to be similar to the physician's TCFA diagnostic criteria.



### Automated diagnosis of pneumothorax using an ensemble of convolutional neural networks with multi-sized chest radiography images
- **Arxiv ID**: http://arxiv.org/abs/1804.06821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06821v1)
- **Published**: 2018-04-18 17:14:54+00:00
- **Updated**: 2018-04-18 17:14:54+00:00
- **Authors**: Tae Joon Jun, Dohyeun Kim, Daeyoung Kim
- **Comment**: Submitted to journal
- **Journal**: None
- **Summary**: Pneumothorax is a relatively common disease, but in some cases, it may be difficult to find with chest radiography. In this paper, we propose a novel method of detecting pneumothorax in chest radiography. We propose an ensemble model of identical convolutional neural networks (CNN) with three different sizes of radiography images. Conventional methods may not properly characterize lost features while resizing large size images into 256 x 256 or 224 x 224 sizes. Our model is evaluated with ChestX-ray dataset which contains over 100,000 chest radiography images. As a result of the experiment, the proposed model showed AUC 0.911, which is the state of the art result in pneumothorax detection. Our method is expected to be effective when applying CNN to large size medical images.



### Unveiling the Power of Deep Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.06833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06833v1)
- **Published**: 2018-04-18 17:40:44+00:00
- **Updated**: 2018-04-18 17:40:44+00:00
- **Authors**: Goutam Bhat, Joakim Johnander, Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of generic object tracking numerous attempts have been made to exploit deep features. Despite all expectations, deep trackers are yet to reach an outstanding level of performance compared to methods solely based on handcrafted features. In this paper, we investigate this key issue and propose an approach to unlock the true potential of deep features for tracking. We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness. We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking. Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy. Extensive experiments are performed on four challenging datasets. On VOT2017, our approach significantly outperforms the top performing tracker from the challenge with a relative gain of 17% in EAO.



### Object Ordering with Bidirectional Matchings for Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1804.06870v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.06870v2)
- **Published**: 2018-04-18 18:39:17+00:00
- **Updated**: 2018-09-06 16:56:32+00:00
- **Authors**: Hao Tan, Mohit Bansal
- **Comment**: NAACL 2018 (8 pages; added pointer-ordering examples)
- **Journal**: None
- **Summary**: Visual reasoning with compositional natural language instructions, e.g., based on the newly-released Cornell Natural Language Visual Reasoning (NLVR) dataset, is a challenging task, where the model needs to have the ability to create an accurate mapping between the diverse phrases and the several objects placed in complex arrangements in the image. Further, this mapping needs to be processed to answer the question in the statement given the ordering and relationship of the objects across three similar images. In this paper, we propose a novel end-to-end neural model for the NLVR task, where we first use joint bidirectional attention to build a two-way conditioning between the visual information and the language phrases. Next, we use an RL-based pointer network to sort and process the varying number of unordered objects (so as to match the order of the statement phrases) in each of the three images and then pool over the three decisions. Our model achieves strong improvements (of 4-6% absolute) over the state-of-the-art on both the structured representation and raw image versions of the dataset.



### Pelee: A Real-Time Object Detection System on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1804.06882v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06882v3)
- **Published**: 2018-04-18 19:27:27+00:00
- **Updated**: 2019-01-18 05:46:59+00:00
- **Authors**: Robert J. Wang, Xiang Li, Charles X. Ling
- **Comment**: Accepted to NeurIPS 2018
- **Journal**: None
- **Summary**: An increasing need of running Convolutional Neural Network (CNN) models on mobile devices with limited computing power and memory resource encourages studies on efficient model design. A number of efficient architectures have been proposed in recent years, for example, MobileNet, ShuffleNet, and MobileNetV2. However, all these models are heavily dependent on depthwise separable convolution which lacks efficient implementation in most deep learning frameworks. In this study, we propose an efficient architecture named PeleeNet, which is built with conventional convolution instead. On ImageNet ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and over 1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile, PeleeNet is only 66% of the model size of MobileNet. We then propose a real-time object detection system by combining PeleeNet with Single Shot MultiBox Detector (SSD) method and optimizing the architecture for fast speed. Our proposed detection system2, named Pelee, achieves 76.4% mAP (mean average precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of 23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size.



### Video Compression through Image Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1804.06919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.06919v1)
- **Published**: 2018-04-18 21:18:01+00:00
- **Updated**: 2018-04-18 21:18:01+00:00
- **Authors**: Chao-Yuan Wu, Nayan Singhal, Philipp Krähenbühl
- **Comment**: Project page: https://chaoyuaw.github.io/vcii/
- **Journal**: None
- **Summary**: An ever increasing amount of our digital communication, media consumption, and content creation revolves around videos. We share, watch, and archive many aspects of our lives through them, all of which are powered by strong video compression. Traditional video compression is laboriously hand designed and hand optimized. This paper presents an alternative in an end-to-end deep learning codec. Our codec builds on one simple idea: Video compression is repeated image interpolation. It thus benefits from recent advances in deep image interpolation and generation. Our deep video codec outperforms today's prevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with H.264.



### One-Shot Learning using Mixture of Variational Autoencoders: a Generalization Learning approach
- **Arxiv ID**: http://arxiv.org/abs/1804.07645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.07645v1)
- **Published**: 2018-04-18 22:00:20+00:00
- **Updated**: 2018-04-18 22:00:20+00:00
- **Authors**: Decebal Constantin Mocanu, Elena Mocanu
- **Comment**: None
- **Journal**: 17th International Conference on Autonomous Agents and Multiagent
  Systems (AAMAS 2018)
- **Summary**: Deep learning, even if it is very successful nowadays, traditionally needs very large amounts of labeled data to perform excellent on the classification task. In an attempt to solve this problem, the one-shot learning paradigm, which makes use of just one labeled sample per class and prior knowledge, becomes increasingly important. In this paper, we propose a new one-shot learning method, dubbed MoVAE (Mixture of Variational AutoEncoders), to perform classification. Complementary to prior studies, MoVAE represents a shift of paradigm in comparison with the usual one-shot learning methods, as it does not use any prior knowledge. Instead, it starts from zero knowledge and one labeled sample per class. Afterward, by using unlabeled data and the generalization learning concept (in a way, more as humans do), it is capable to gradually improve by itself its performance. Even more, if there are no unlabeled data available MoVAE can still perform well in one-shot learning classification. We demonstrate empirically the efficiency of our proposed approach on three datasets, i.e. the handwritten digits (MNIST), fashion products (Fashion-MNIST), and handwritten characters (Omniglot), showing that MoVAE outperforms state-of-the-art one-shot learning algorithms.




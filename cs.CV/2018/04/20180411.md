# Arxiv Papers in cs.CV on 2018-04-11
### Nonlinear 3D Face Morphable Model
- **Arxiv ID**: http://arxiv.org/abs/1804.03786v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03786v3)
- **Published**: 2018-04-11 02:22:10+00:00
- **Updated**: 2018-08-26 14:11:24+00:00
- **Authors**: Luan Tran, Xiaoming Liu
- **Comment**: CVPR 2018 (Spotlight). Source code:
  https://github.com/tranluan/Nonlinear_Face_3DMM . Project webpage:
  http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html . v3: minor revision,
  adding source code
- **Journal**: None
- **Summary**: As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.



### Multi-Scale Generalized Plane Match for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1804.03787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03787v1)
- **Published**: 2018-04-11 02:26:34+00:00
- **Updated**: 2018-04-11 02:26:34+00:00
- **Authors**: Inchul Choi, Arunava Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances, estimating optical flow remains a challenging problem in the presence of illumination change, large occlusions or fast movement. In this paper, we propose a novel optical flow estimation framework which can provide accurate dense correspondence and occlusion localization through a multi-scale generalized plane matching approach. In our method, we regard the scene as a collection of planes at multiple scales, and for each such plane, compensate motion in consensus to improve match quality. We estimate the square patch plane distortion using a robust plane model detection method and iteratively apply a plane matching scheme within a multi-scale framework. During the flow estimation process, our enhanced plane matching method also clearly localizes the occluded regions. In experiments on MPI-Sintel datasets, our method robustly estimated optical flow from given noisy correspondences, and also revealed the occluded regions accurately. Compared to other state-of-the-art optical flow methods, our method shows accurate occlusion localization, comparable optical flow quality, and better thin object detection.



### Geometric Consistency for Self-Supervised End-to-End Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1804.03789v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.03789v1)
- **Published**: 2018-04-11 02:45:00+00:00
- **Updated**: 2018-04-11 02:45:00+00:00
- **Authors**: Ganesh Iyer, J. Krishna Murthy, Gunshi Gupta, K. Madhava Krishna, Liam Paull
- **Comment**: None
- **Journal**: None
- **Summary**: With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.



### Decoupled Novel Object Captioner
- **Arxiv ID**: http://arxiv.org/abs/1804.03803v2
- **DOI**: 10.1145/3240508.3240640
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03803v2)
- **Published**: 2018-04-11 04:21:22+00:00
- **Updated**: 2018-08-11 08:36:55+00:00
- **Authors**: Yu Wu, Linchao Zhu, Lu Jiang, Yi Yang
- **Comment**: Accepted to ACM MM 2018
- **Journal**: None
- **Summary**: Image captioning is a challenging task where the machine automatically describes an image by sentences or phrases. It often requires a large number of paired image-sentence annotations for training. However, a pre-trained captioning model can hardly be applied to a new domain in which some novel object categories exist, i.e., the objects and their description words are unseen during model training. To correctly caption the novel object, it requires professional human workers to annotate the images by sentences with the novel words. It is labor expensive and thus limits its usage in real-world applications.   In this paper, we introduce the zero-shot novel object captioning task where the machine generates descriptions without extra sentences about the novel object. To tackle the challenging problem, we propose a Decoupled Novel Object Captioner (DNOC) framework that can fully decouple the language sequence model from the object descriptions. DNOC has two components. 1) A Sequence Model with the Placeholder (SM-P) generates a sentence containing placeholders. The placeholder represents an unseen novel object. Thus, the sequence model can be decoupled from the novel object descriptions. 2) A key-value object memory built upon the freely available detection model, contains the visual information and the corresponding word for each object. The SM-P will generate a query to retrieve the words from the object memory. The placeholder will then be filled with the correct word, resulting in a caption with novel object descriptions. The experimental results on the held-out MSCOCO dataset demonstrate the ability of DNOC in describing novel concepts in the zero-shot novel object captioning task.



### Demoiréing of Camera-Captured Screen Images Using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1804.03809v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.03809v1)
- **Published**: 2018-04-11 04:51:25+00:00
- **Updated**: 2018-04-11 04:51:25+00:00
- **Authors**: Bolin Liu, Xiao Shu, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Taking photos of optoelectronic displays is a direct and spontaneous way of transferring data and keeping records, which is widely practiced. However, due to the analog signal interference between the pixel grids of the display screen and camera sensor array, objectionable moir\'e (alias) patterns appear in captured screen images. As the moir\'e patterns are structured and highly variant, they are difficult to be completely removed without affecting the underneath latent image. In this paper, we propose an approach of deep convolutional neural network for demoir\'eing screen photos. The proposed DCNN consists of a coarse-scale network and a fine-scale network. In the coarse-scale network, the input image is first downsampled and then processed by stacked residual blocks to remove the moir\'e artifacts. After that, the fine-scale network upsamples the demoir\'ed low-resolution image back to the original resolution. Extensive experimental results have demonstrated that the proposed technique can efficiently remove the moir\'e patterns for camera acquired screen images; the new technique outperforms the existing ones.



### ExFuse: Enhancing Feature Fusion for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.03821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03821v1)
- **Published**: 2018-04-11 05:54:31+00:00
- **Updated**: 2018-04-11 05:54:31+00:00
- **Authors**: Zhenli Zhang, Xiangyu Zhang, Chao Peng, Dazhi Cheng, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0\% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9\% mean IoU, which outperforms the previous state-of-the-art results.



### Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means
- **Arxiv ID**: http://arxiv.org/abs/1804.03828v1
- **DOI**: 10.1117/12.2292172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03828v1)
- **Published**: 2018-04-11 06:28:27+00:00
- **Updated**: 2018-04-11 06:28:27+00:00
- **Authors**: Takayasu Moriya, Holger R. Roth, Shota Nakamura, Hirohisa Oda, Kai Nagara, Masahiro Oda, Kensaku Mori
- **Comment**: This paper was presented at SPIE Medical Imaging 2018, Houston, TX,
  USA
- **Journal**: Proc. SPIE 10581, Medical Imaging 2018: Digital Pathology, 1058111
  (6 March 2018)
- **Summary**: This paper presents a novel method for unsupervised segmentation of pathology images. Staging of lung cancer is a major factor of prognosis. Measuring the maximum dimensions of the invasive component in a pathology images is an essential task. Therefore, image segmentation methods for visualizing the extent of invasive and noninvasive components on pathology images could support pathological examination. However, it is challenging for most of the recent segmentation methods that rely on supervised learning to cope with unlabeled pathology images. In this paper, we propose a unified approach to unsupervised representation learning and clustering for pathology image segmentation. Our method consists of two phases. In the first phase, we learn feature representations of training patches from a target image using the spherical k-means. The purpose of this phase is to obtain cluster centroids which could be used as filters for feature extraction. In the second phase, we apply conventional k-means to the representations extracted by the centroids and then project cluster labels to the target images. We evaluated our methods on pathology images of lung cancer specimen. Our experiments showed that the proposed method outperforms traditional k-means segmentation and the multithreshold Otsu method both quantitatively and qualitatively with an improved normalized mutual information (NMI) score of 0.626 compared to 0.168 and 0.167, respectively. Furthermore, we found that the centroids can be applied to the segmentation of other slices from the same sample.



### Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.03830v1
- **DOI**: 10.1117/12.2293414
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03830v1)
- **Published**: 2018-04-11 06:30:30+00:00
- **Updated**: 2018-04-11 06:30:30+00:00
- **Authors**: Takayasu Moriya, Holger R. Roth, Shota Nakamura, Hirohisa Oda, Kai Nagara, Masahiro Oda, Kensaku Mori
- **Comment**: This paper was presented at SPIE Medical Imaging 2018, Houston, TX,
  USA
- **Journal**: Proc. SPIE 10578, Medical Imaging 2018: Biomedical Applications in
  Molecular, Structural, and Functional Imaging, 1057820 (12 March 2018)
- **Summary**: This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation.



### Real-world plant species identification based on deep convolutional neural networks and visual attention
- **Arxiv ID**: http://arxiv.org/abs/1804.03853v4
- **DOI**: 10.1016/j.ecoinf.2018.09.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03853v4)
- **Published**: 2018-04-11 07:51:59+00:00
- **Updated**: 2019-03-01 02:08:45+00:00
- **Authors**: Qingguo Xiao, Guangyao Li, Li Xie, Qiaochuan Chen
- **Comment**: published
- **Journal**: Ecological Informatics, 2018, 48: 117-124
- **Summary**: This paper investigates the issue of real-world identification to fulfill better species protection. We focus on plant species identification as it is a classic and hot issue. In tradition plant species identification the samples are scanned specimen and the background is simple. However, real-world species recognition is more challenging. We first systematically investigate what is realistic species recognition and the difference from tradition plant species recognition. To deal with the challenging task, an interdisciplinary collaboration is presented based on the latest advances in computer science and technology. We propose a novel framework and an effective data augmentation method for deep learning in this paper. We first crop the image in terms of visual attention before general recognition. Besides, we apply it as a data augmentation method. We call the novel data augmentation approach attention cropping (AC). Deep convolutional neural networks are trained to predict species from a large amount of data. Extensive experiments on traditional dataset and specific dataset for real-world recognition are conducted to evaluate the performance of our approach. Experiments first demonstrate that our approach achieves state-of-the-art results on different types of datasets. Besides, we also evaluate the performance of data augmentation method AC. Results show that AC provides superior performance. Compared with the precision of methods without AC, the results with AC achieve substantial improvement.



### MaskReID: A Mask Based Deep Ranking Neural Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.03864v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03864v2)
- **Published**: 2018-04-11 08:17:14+00:00
- **Updated**: 2019-04-18 02:16:24+00:00
- **Authors**: Lei Qi, Jing Huo, Lei Wang, Yinghuan Shi, Yang Gao
- **Comment**: ICME2019
- **Journal**: None
- **Summary**: Person retrieval faces many challenges including cluttered background, appearance variations (e.g., illumination, pose, occlusion) among different camera views and the similarity among different person's images. To address these issues, we put forward a novel mask based deep ranking neural network with a skipped fusing layer. Firstly, to alleviate the problem of cluttered background, masked images with only the foreground regions are incorporated as input in the proposed neural network. Secondly, to reduce the impact of the appearance variations, the multi-layer fusion scheme is developed to obtain more discriminative fine-grained information. Lastly, considering person retrieval is a special image retrieval task, we propose a novel ranking loss to optimize the whole network. The proposed ranking loss can further mitigate the interference problem of similar negative samples when producing ranking results. The extensive experiments validate the superiority of the proposed method compared with the state-of-the-art methods on many benchmark datasets.



### Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory
- **Arxiv ID**: http://arxiv.org/abs/1804.03867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03867v1)
- **Published**: 2018-04-11 08:27:49+00:00
- **Updated**: 2018-04-11 08:27:49+00:00
- **Authors**: Ameya Prabhu, Vishal Batchu, Rohit Gajawada, Sri Aurobindo Munagala, Anoop Namboodiri
- **Comment**: Accepted in WACV'18 (Oral)
- **Journal**: None
- **Summary**: Binarization is an extreme network compression approach that provides large computational speedups along with energy and memory savings, albeit at significant accuracy costs. We investigate the question of where to binarize inputs at layer-level granularity and show that selectively binarizing the inputs to specific layers in the network could lead to significant improvements in accuracy while preserving most of the advantages of binarization. We analyze the binarization tradeoff using a metric that jointly models the input binarization-error and computational cost and introduce an efficient algorithm to select layers whose inputs are to be binarized. Practical guidelines based on insights obtained from applying the algorithm to a variety of models are discussed. Experiments on Imagenet dataset using AlexNet and ResNet-18 models show 3-4% improvements in accuracy over fully binarized networks with minimal impact on compression and computational speed. The improvements are even more substantial on sketch datasets like TU-Berlin, where we match state-of-the-art accuracy as well, getting over 8% increase in accuracies. We further show that our approach can be applied in tandem with other forms of compression that deal with individual layers or overall model compression (e.g., SqueezeNets). Unlike previous quantization approaches, we are able to binarize the weights in the last layers of a network, which often have a large number of parameters, resulting in significant improvement in accuracy over fully binarized models.



### Plaque Classification in Coronary Arteries from IVOCT Images Using Convolutional Neural Networks and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.03904v1
- **DOI**: 10.1007/s11548-018-1766-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03904v1)
- **Published**: 2018-04-11 09:50:58+00:00
- **Updated**: 2018-04-11 09:50:58+00:00
- **Authors**: Nils Gessert, Markus Heyder, Sarah Latus, Matthias Lutz, Alexander Schlaefer
- **Comment**: Submitted to CARS 2018, accepted for publication
- **Journal**: None
- **Summary**: Advanced atherosclerosis in the coronary arteries is one of the leading causes of deaths worldwide while being preventable and treatable. In order to image atherosclerotic lesions (plaque), intravascular optical coherence tomography (IVOCT) can be used. The technique provides high-resolution images of arterial walls which allows for early plaque detection by experts. Due to the vast amount of IVOCT images acquired in clinical routines, automatic plaque detection has been addressed. For example, attenuation profiles in single A-Scans of IVOCT images are examined to detect plaque. We address automatic plaque classification from entire IVOCT images, the cross-sectional view of the artery, using deep feature learning. In this way, we take context between A-Scans into account and we directly learn relevant features from the image source without the need for handcrafting features.



### Fusing Saliency Maps with Region Proposals for Unsupervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1804.03905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03905v1)
- **Published**: 2018-04-11 09:51:08+00:00
- **Updated**: 2018-04-11 09:51:08+00:00
- **Authors**: Hakan Karaoguz, Patric Jensfelt
- **Comment**: 7 pages, 4 figures, 6 tables, submitted to IEEE IROS2018 conference
- **Journal**: None
- **Summary**: In this paper we address the problem of unsupervised localization of objects in single images. Compared to previous state-of-the-art method our method is fully unsupervised in the sense that there is no prior instance level or category level information about the image. Furthermore, we treat each image individually and do not rely on any neighboring image similarity. We employ deep-learning based generation of saliency maps and region proposals to tackle this problem. First salient regions in the image are determined using an encoder/decoder architecture. The resulting saliency map is matched with region proposals from a class agnostic region proposal network to roughly localize the candidate object regions. These regions are further refined based on the overlap and similarity ratios. Our experimental evaluations on a benchmark dataset show that the method gets close to current state-of-the-art methods in terms of localization accuracy even though these make use of multiple frames. Furthermore, we created a more challenging and realistic dataset with multiple object categories and varying viewpoint and illumination conditions for evaluating the method's performance in real world scenarios.



### Deep Learning For Computer Vision Tasks: A review
- **Arxiv ID**: http://arxiv.org/abs/1804.03928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03928v1)
- **Published**: 2018-04-11 11:13:35+00:00
- **Updated**: 2018-04-11 11:13:35+00:00
- **Authors**: Rajat Kumar Sinha, Ruchi Pandey, Rohan Pattnaik
- **Comment**: Accepted in 2017 International Conference on Intelligent Computing
  and Control (I2C2)
- **Journal**: None
- **Summary**: Deep learning has recently become one of the most popular sub-fields of machine learning owing to its distributed data representation with multiple levels of abstraction. A diverse range of deep learning algorithms are being employed to solve conventional artificial intelligence problems. This paper gives an overview of some of the most widely used deep learning algorithms applied in the field of computer vision. It first inspects the various approaches of deep learning algorithms, followed by a description of their applications in image classification, object identification, image extraction and semantic segmentation in the presence of noise. The paper concludes with the discussion of the future scope and challenges for construction and training of deep neural networks.



### Offline Object Extraction from Dynamic Occupancy Grid Map Sequences
- **Arxiv ID**: http://arxiv.org/abs/1804.03933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03933v1)
- **Published**: 2018-04-11 11:26:00+00:00
- **Updated**: 2018-04-11 11:26:00+00:00
- **Authors**: Daniel Stumper, Fabian Gies, Stefan Hoermann, Klaus Dietmayer
- **Comment**: 8 Pages, 7 Figures, submitted to IEEE IV2018, waiting for acceptance
- **Journal**: None
- **Summary**: A dynamic occupancy grid map (DOGMa) allows a fast, robust, and complete environment representation for automated vehicles. Dynamic objects in a DOGMa, however, are commonly represented as independent cells while modeled objects with shape and pose are favorable. The evaluation of algorithms for object extraction or the training and validation of learning algorithms rely on labeled ground truth data. Manually annotating objects in a DOGMa to obtain ground truth data is a time consuming and expensive process. Additionally the quality of labeled data depend strongly on the variation of filtered input data. The presented work introduces an automatic labeling process, where a full sequence is used to extract the best possible object pose and shape in terms of temporal consistency. A two direction temporal search is executed to trace single objects over a sequence, where the best estimate of its extent and pose is refined in every time step. Furthermore, the presented algorithm only uses statistical constraints of the cell clusters for the object extraction instead of fixed heuristic parameters. Experimental results show a well-performing automatic labeling algorithm with real sensor data even at challenging scenarios.



### Measurement of exceptional motion in VR video contents for VR sickness assessment using deep convolutional autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1804.03939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03939v1)
- **Published**: 2018-04-11 11:41:47+00:00
- **Updated**: 2018-04-11 11:41:47+00:00
- **Authors**: Hak Gu Kim, Wissam J. Baddar, Heoun-taek Lim, Hyunwook Jeong, Yong Man Ro
- **Comment**: In Proceedings of the 23rd ACM Symposium on Virtual Reality Software
  and Technology (VRST 2017)
- **Journal**: None
- **Summary**: This paper proposes a new objective metric of exceptional motion in VR video contents for VR sickness assessment. In VR environment, VR sickness can be caused by several factors which are mismatched motion, field of view, motion parallax, viewing angle, etc. Similar to motion sickness, VR sickness can induce a lot of physical symptoms such as general discomfort, headache, stomach awareness, nausea, vomiting, fatigue, and disorientation. To address the viewing safety issues in virtual environment, it is of great importance to develop an objective VR sickness assessment method that predicts and analyses the degree of VR sickness induced by the VR content. The proposed method takes into account motion information that is one of the most important factors in determining the overall degree of VR sickness. In this paper, we detect the exceptional motion that is likely to induce VR sickness. Spatio-temporal features of the exceptional motion in the VR video content are encoded using a convolutional autoencoder. For objectively assessing the VR sickness, the level of exceptional motion in VR video content is measured by using the convolutional autoencoder as well. The effectiveness of the proposed method has been successfully evaluated by subjective assessment experiment using simulator sickness questionnaires (SSQ) in VR environment.



### VR IQA NET: Deep Virtual Reality Image Quality Assessment using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1804.03943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03943v1)
- **Published**: 2018-04-11 11:45:56+00:00
- **Updated**: 2018-04-11 11:45:56+00:00
- **Authors**: Heoun-taek Lim, Hak Gu Kim, Yong Man Ro
- **Comment**: To appear at IEEE ICASSP 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel virtual reality image quality assessment (VR IQA) with adversarial learning for omnidirectional images. To take into account the characteristics of the omnidirectional image, we devise deep networks including novel quality score predictor and human perception guider. The proposed quality score predictor automatically predicts the quality score of distorted image using the latent spatial and position feature. The proposed human perception guider criticizes the predicted quality score of the predictor with the human perceptual score using adversarial learning. For evaluation, we conducted extensive subjective experiments with omnidirectional image dataset. Experimental results show that the proposed VR IQA metric outperforms the 2-D IQA and the state-of-the-arts VR IQA.



### Projection image-to-image translation in hybrid X-ray/MR imaging
- **Arxiv ID**: http://arxiv.org/abs/1804.03955v2
- **DOI**: 10.1117/12.2512195
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03955v2)
- **Published**: 2018-04-11 12:23:03+00:00
- **Updated**: 2019-05-08 14:40:43+00:00
- **Authors**: Bernhard Stimpel, Christopher Syben, Tobias Würfl, Katharina Breininger, Katrin Mentl, Jonathan M. Lommen, Arnd Dörfler, Andreas Maier
- **Comment**: In proceedings of SPIE Medical Imaging 2019
- **Journal**: None
- **Summary**: The potential benefit of hybrid X-ray and MR imaging in the interventional environment is large due to the combination of fast imaging with high contrast variety. However, a vast amount of existing image enhancement methods requires the image information of both modalities to be present in the same domain. To unlock this potential, we present a solution to image-to-image translation from MR projections to corresponding X-ray projection images. The approach is based on a state-of-the-art image generator network that is modified to fit the specific application. Furthermore, we propose the inclusion of a gradient map in the loss function to allow the network to emphasize high-frequency details in image generation. Our approach is capable of creating X-ray projection images with natural appearance. Additionally, our extensions show clear improvement compared to the baseline method.



### Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.03959v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03959v3)
- **Published**: 2018-04-11 12:39:19+00:00
- **Updated**: 2018-07-26 06:38:54+00:00
- **Authors**: Markus Oberweger, Mahdi Rad, Vincent Lepetit
- **Comment**: None
- **Journal**: Proc. of ECCV 2018
- **Summary**: We introduce a novel method for robust and accurate 3D object pose estimation from a single color image under large occlusions. Following recent approaches, we first predict the 2D projections of 3D points related to the target object and then compute the 3D pose from these correspondences using a geometric method. Unfortunately, as the results of our experiments show, predicting these 2D projections using a regular CNN or a Convolutional Pose Machine is highly sensitive to partial occlusions, even when these methods are trained with partially occluded examples. Our solution is to predict heatmaps from multiple small patches independently and to accumulate the results to obtain accurate and robust predictions. Training subsequently becomes challenging because patches with similar appearances but different positions on the object correspond to different heatmaps. However, we provide a simple yet effective solution to deal with such ambiguities. We show that our approach outperforms existing methods on two challenging datasets: The Occluded LineMOD dataset and the YCB-Video dataset, both exhibiting cluttered scenes with highly occluded objects. Project website: https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/robust-object-pose-estimation/



### Edge-based LBP description of surfaces with colorimetric patterns
- **Arxiv ID**: http://arxiv.org/abs/1804.03977v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, 68T45, 68U05, I.3.6; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1804.03977v1)
- **Published**: 2018-04-11 13:34:31+00:00
- **Updated**: 2018-04-11 13:34:31+00:00
- **Authors**: Elia Moscoso Thompson, Silvia Biasotti
- **Comment**: Eurographics Workshop on 3D Object Retrieval 2018
- **Journal**: None
- **Summary**: In this paper we target the problem of the retrieval of colour patterns over surfaces. We generalize to surface tessellations the well known Local Binary Pattern (LBP) descriptor for images. The key concept of the LBP is to code the variability of the colour values around each pixel. In the case of a surface tessellation we adopt rings around vertices that are obtained with a sphere-mesh intersection driven by the edges of the mesh; for this reason, we name our method edgeLBP. Experimental results are provided to show how this description performs well for pattern retrieval, also when patterns come from degraded and corrupted archaeological fragments.



### DLL: A Blazing Fast Deep Neural Network Library
- **Arxiv ID**: http://arxiv.org/abs/1804.04512v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04512v1)
- **Published**: 2018-04-11 13:56:07+00:00
- **Updated**: 2018-04-11 13:56:07+00:00
- **Authors**: Baptiste Wicht, Jean Hennebert, Andreas Fischer
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Deep Learning Library (DLL) is a new library for machine learning with deep neural networks that focuses on speed. It supports feed-forward neural networks such as fully-connected Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs). It also has very comprehensive support for Restricted Boltzmann Machines (RBMs) and Convolutional RBMs. Our main motivation for this work was to propose and evaluate novel software engineering strategies with potential to accelerate runtime for training and inference. Such strategies are mostly independent of the underlying deep learning algorithms. On three different datasets and for four different neural network models, we compared DLL to five popular deep learning frameworks. Experimentally, it is shown that the proposed framework is systematically and significantly faster on CPU and GPU. In terms of classification performance, similar accuracies as the other frameworks are reported.



### Attention U-Net: Learning Where to Look for the Pancreas
- **Arxiv ID**: http://arxiv.org/abs/1804.03999v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.03999v3)
- **Published**: 2018-04-11 14:13:03+00:00
- **Updated**: 2018-05-20 23:33:30+00:00
- **Authors**: Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, Ben Glocker, Daniel Rueckert
- **Comment**: Accepted to published in MIDL'18 (Revised Version) / OpenReview link:
  https://openreview.net/forum?id=Skft7cijM
- **Journal**: None
- **Summary**: We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.



### Dynamic Multi-Context Segmentation of Remote Sensing Images based on Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04020v3
- **DOI**: 10.1109/TGRS.2019.2913861
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04020v3)
- **Published**: 2018-04-11 14:32:15+00:00
- **Updated**: 2019-04-23 02:01:11+00:00
- **Authors**: Keiller Nogueira, Mauro Dalla Mura, Jocelyn Chanussot, William R. Schwartz, Jefersson A. dos Santos
- **Comment**: Accepted to Transactions on Geoscience & Remote Sensing (TGRS)
- **Journal**: None
- **Summary**: Semantic segmentation requires methods capable of learning high-level features while dealing with large volume of data. Towards such goal, Convolutional Networks can learn specific and adaptable features based on the data. However, these networks are not capable of processing a whole remote sensing image, given its huge size. To overcome such limitation, the image is processed using fixed size patches. The definition of the input patch size is usually performed empirically (evaluating several sizes) or imposed (by network constraint). Both strategies suffer from drawbacks and could not lead to the best patch size. To alleviate this problem, several works exploited multi-context information by combining networks or layers. This process increases the number of parameters resulting in a more difficult model to train. In this work, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits a multi-context paradigm without increasing the number of parameters while defining, in training time, the best patch size. The main idea is to train a dilated network with distinct patch sizes, allowing it to capture multi-context characteristics from heterogeneous contexts. While processing these varying patches, the network provides a score for each patch size, helping in the definition of the best size for the current scenario. A systematic evaluation of the proposed algorithm is conducted using four high-resolution remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements in pixelwise classification accuracy when compared to state-of-the-art methods.



### Coloring with Words: Guiding Image Colorization Through Text-based Palette Generation
- **Arxiv ID**: http://arxiv.org/abs/1804.04128v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04128v2)
- **Published**: 2018-04-11 15:16:14+00:00
- **Updated**: 2018-08-07 06:40:18+00:00
- **Authors**: Hyojin Bahng, Seungjoo Yoo, Wonwoong Cho, David K. Park, Ziming Wu, Xiaojuan Ma, Jaegul Choo
- **Comment**: 25 pages, 22 figures
- **Journal**: ECCV 2018
- **Summary**: This paper proposes a novel approach to generate multiple color palettes that reflect the semantics of input text and then colorize a given grayscale image according to the generated color palette. In contrast to existing approaches, our model can understand rich text, whether it is a single word, a phrase, or a sentence, and generate multiple possible palettes from it. For this task, we introduce our manually curated dataset called Palette-and-Text (PAT). Our proposed model called Text2Colors consists of two conditional generative adversarial networks: the text-to-palette generation networks and the palette-based colorization networks. The former captures the semantics of the text input and produce relevant color palettes. The latter colorizes a grayscale image using the generated color palette. Our evaluation results show that people preferred our generated palettes over ground truth palettes and that our model can effectively reflect the given palette when colorizing an image.



### Learning to Extract a Video Sequence from a Single Motion-Blurred Image
- **Arxiv ID**: http://arxiv.org/abs/1804.04065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04065v1)
- **Published**: 2018-04-11 16:01:26+00:00
- **Updated**: 2018-04-11 16:01:26+00:00
- **Authors**: Meiguang Jin, Givi Meishvili, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to extract a video sequence from a single motion-blurred image. Motion-blurred images are the result of an averaging process, where instant frames are accumulated over time during the exposure of the sensor. Unfortunately, reversing this process is nontrivial. Firstly, averaging destroys the temporal ordering of the frames. Secondly, the recovery of a single frame is a blind deconvolution task, which is highly ill-posed. We present a deep learning scheme that gradually reconstructs a temporal ordering by sequentially extracting pairs of frames. Our main contribution is to introduce loss functions invariant to the temporal order. This lets a neural network choose during training what frame to output among the possible combinations. We also address the ill-posedness of deblurring by designing a network with a large receptive field and implemented via resampling to achieve a higher computational efficiency. Our proposed method can successfully retrieve sharp image sequences from a single motion blurred image and can generalize well on synthetic and real datasets captured with different cameras.



### Seed-Point Detection of Clumped Convex Objects by Short-Range Attractive Long-Range Repulsive Particle Clustering
- **Arxiv ID**: http://arxiv.org/abs/1804.04071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04071v1)
- **Published**: 2018-04-11 16:18:16+00:00
- **Updated**: 2018-04-11 16:18:16+00:00
- **Authors**: James Kapaldo, Xu Han, Domingo Mery
- **Comment**: 10 pages, 8 figures, with supplemental notes and videos. Submitted to
  be published
- **Journal**: None
- **Summary**: Locating the center of convex objects is important in both image processing and unsupervised machine learning/data clustering fields. The automated analysis of biological images uses both of these fields for locating cell nuclei and for discovering new biological effects or cell phenotypes. In this work, we develop a novel clustering method for locating the centers of overlapping convex objects by modeling particles that interact by a short-range attractive and long-range repulsive potential and are confined to a potential well created from the data. We apply this method to locating the centers of clumped nuclei in cultured cells, where we show that it results in a significant improvement over existing methods (8.2% in F$_1$ score); and we apply it to unsupervised learning on a difficult data set that has rare classes without local density maxima, and show it is able to well locate cluster centers when other clustering techniques fail.



### Detail-Preserving Pooling in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.04076v1)
- **Published**: 2018-04-11 16:28:11+00:00
- **Updated**: 2018-04-11 16:28:11+00:00
- **Authors**: Faraz Saeedan, Nicolas Weber, Michael Goesele, Stefan Roth
- **Comment**: To appear at CVPR 2018
- **Journal**: None
- **Summary**: Most convolutional neural networks use some method for gradually downscaling the size of the hidden layers. This is commonly referred to as pooling, and is applied to reduce the number of parameters, improve invariance to certain distortions, and increase the receptive field size. Since pooling by nature is a lossy process, it is crucial that each such layer maintains the portion of the activations that is most important for the network's discriminability. Yet, simple maximization or averaging over blocks, max or average pooling, or plain downsampling in the form of strided convolutions are the standard. In this paper, we aim to leverage recent results on image downscaling for the purposes of deep learning. Inspired by the human visual system, which focuses on local spatial changes, we propose detail-preserving pooling (DPP), an adaptive pooling method that magnifies spatial changes and preserves important structural detail. Importantly, its parameters can be learned jointly with the rest of the network. We analyze some of its theoretical properties and show its empirical benefits on several datasets and networks, where DPP consistently outperforms previous pooling approaches.



### Ranking CGANs: Subjective Control over Semantic Image Attributes
- **Arxiv ID**: http://arxiv.org/abs/1804.04082v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04082v3)
- **Published**: 2018-04-11 16:40:42+00:00
- **Updated**: 2018-07-24 11:50:12+00:00
- **Authors**: Yassir Saquil, Kwang In Kim, Peter Hall
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the use of generative adversarial networks in the task of image generation according to subjective measures of semantic attributes. Unlike the standard (CGAN) that generates images from discrete categorical labels, our architecture handles both continuous and discrete scales. Given pairwise comparisons of images, our model, called RankCGAN, performs two tasks: it learns to rank images using a subjective measure; and it learns a generative model that can be controlled by that measure. RankCGAN associates each subjective measure of interest to a distinct dimension of some latent space. We perform experiments on UT-Zap50K, PubFig and OSR datasets and demonstrate that the model is expressive and diverse enough to conduct two-attribute exploration and image editing.



### Beamformed Fingerprint Learning for Accurate Millimeter Wave Positioning
- **Arxiv ID**: http://arxiv.org/abs/1804.04112v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04112v1)
- **Published**: 2018-04-11 17:36:30+00:00
- **Updated**: 2018-04-11 17:36:30+00:00
- **Authors**: João Gante, Gabriel Falcão, Leonel Sousa
- **Comment**: 5 pages, 7 figures. Submitted to VTC2018-Fall (Chicago)
- **Journal**: None
- **Summary**: With millimeter wave wireless communications, the resulting radiation reflects on most visible objects, creating rich multipath environments, namely in urban scenarios. The radiation captured by a listening device is thus shaped by the obstacles encountered, which carry latent information regarding their relative positions. In this paper, a system to convert the received millimeter wave radiation into the device's position is proposed, making use of the aforementioned hidden information. Using deep learning techniques and a pre-established codebook of beamforming patterns transmitted by a base station, the simulations show that average estimation errors below 10 meters are achievable in realistic outdoors scenarios that contain mostly non-line-of-sight positions, paving the way for new positioning systems.



### Personalized Dynamics Models for Adaptive Assistive Navigation Systems
- **Arxiv ID**: http://arxiv.org/abs/1804.04118v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04118v2)
- **Published**: 2018-04-11 17:55:00+00:00
- **Updated**: 2018-10-08 12:20:33+00:00
- **Authors**: Eshed Ohn-Bar, Kris Kitani, Chieko Asakawa
- **Comment**: Oral Presentation in 2nd Conference on Robot Learning (CoRL, 2018)
- **Journal**: None
- **Summary**: Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt the instructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model-based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.



### The Conversation: Deep Audio-Visual Speech Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1804.04121v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1804.04121v2)
- **Published**: 2018-04-11 17:57:28+00:00
- **Updated**: 2018-06-19 17:51:32+00:00
- **Authors**: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
- **Comment**: To appear in Interspeech 2018. We provide supplementary material with
  interactive demonstrations on
  http://www.robots.ox.ac.uk/~vgg/demo/theconversation
- **Journal**: None
- **Summary**: Our goal is to isolate individual speakers from multi-talker simultaneous speech in videos. Existing works in this area have focussed on trying to separate utterances from known speakers in controlled environments. In this paper, we propose a deep audio-visual speech enhancement network that is able to separate a speaker's voice given lip regions in the corresponding video, by predicting both the magnitude and the phase of the target signal. The method is applicable to speakers unheard and unseen during training, and for unconstrained environments. We demonstrate strong quantitative and qualitative results, isolating extremely challenging real-world examples.



### Deep Differential Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.04192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04192v1)
- **Published**: 2018-04-11 20:02:25+00:00
- **Updated**: 2018-04-11 20:02:25+00:00
- **Authors**: Naifan Zhuang, The Duc Kieu, Guo-Jun Qi, Kien A. Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the special gating schemes of Long Short-Term Memory (LSTM), LSTMs have shown greater potential to process complex sequential information than the traditional Recurrent Neural Network (RNN). The conventional LSTM, however, fails to take into consideration the impact of salient spatio-temporal dynamics present in the sequential input data. This problem was first addressed by the differential Recurrent Neural Network (dRNN), which uses a differential gating scheme known as Derivative of States (DoS). DoS uses higher orders of internal state derivatives to analyze the change in information gain caused by the salient motions between the successive frames. The weighted combination of several orders of DoS is then used to modulate the gates in dRNN. While each individual order of DoS is good at modeling a certain level of salient spatio-temporal sequences, the sum of all the orders of DoS could distort the detected motion patterns. To address this problem, we propose to control the LSTM gates via individual orders of DoS and stack multiple levels of LSTM cells in an increasing order of state derivatives. The proposed model progressively builds up the ability of the LSTM gates to detect salient dynamical patterns in deeper stacked layers modeling higher orders of DoS, and thus the proposed LSTM model is termed deep differential Recurrent Neural Network (d2RNN). The effectiveness of the proposed model is demonstrated on two publicly available human activity datasets: NUS-HGA and Violent-Flows. The proposed model outperforms both LSTM and non-LSTM based state-of-the-art algorithms.



### Multi-scale Neural Networks for Retinal Blood Vessels Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.04206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.04206v1)
- **Published**: 2018-04-11 20:25:36+00:00
- **Updated**: 2018-04-11 20:25:36+00:00
- **Authors**: Boheng Zhang, Shenglei Huang, Shaohan Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing supervised approaches didn't make use of the low-level features which are actually effective to this task. And another deficiency is that they didn't consider the relation between pixels, which means effective features are not extracted. In this paper, we proposed a novel convolutional neural network which make sufficient use of low-level features together with high-level features and involves atrous convolution to get multi-scale features which should be considered as effective features. Our model is tested on three standard benchmarks - DRIVE, STARE, and CHASE databases. The results presents that our model significantly outperforms existing approaches in terms of accuracy, sensitivity, specificity, the area under the ROC curve and the highest prediction speed. Our work provides evidence of the power of wide and deep neural networks in retinal blood vessels segmentation task which could be applied on other medical images tasks.



### View Extrapolation of Human Body from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1804.04213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04213v1)
- **Published**: 2018-04-11 20:41:19+00:00
- **Updated**: 2018-04-11 20:41:19+00:00
- **Authors**: Hao Zhu, Hao Su, Peng Wang, Xun Cao, Ruigang Yang
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: We study how to synthesize novel views of human body from a single image. Though recent deep learning based methods work well for rigid objects, they often fail on objects with large articulation, like human bodies. The core step of existing methods is to fit a map from the observable views to novel views by CNNs; however, the rich articulation modes of human body make it rather challenging for CNNs to memorize and interpolate the data well. To address the problem, we propose a novel deep learning based pipeline that explicitly estimates and leverages the geometry of the underlying human body. Our new pipeline is a composition of a shape estimation network and an image generation network, and at the interface a perspective transformation is applied to generate a forward flow for pixel value transportation. Our design is able to factor out the space of data variation and makes learning at each step much easier. Empirically, we show that the performance for pose-varying objects can be improved dramatically. Our method can also be applied on real data captured by 3D sensors, and the flow generated by our methods can be used for generating high quality results in higher resolution.



### VoroTop: Voronoi Cell Topology Visualization and Analysis Toolkit
- **Arxiv ID**: http://arxiv.org/abs/1804.04221v1
- **DOI**: 10.1088/1361-651X/aa9a01
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/1804.04221v1)
- **Published**: 2018-04-11 21:02:58+00:00
- **Updated**: 2018-04-11 21:02:58+00:00
- **Authors**: Emanuel A. Lazar
- **Comment**: 17 pages, 10 figures
- **Journal**: Model. Simul. Mater. Sci. Eng 26:1 (2017)
- **Summary**: This paper introduces a new open-source software program called VoroTop, which uses Voronoi topology to analyze local structure in atomic systems. Strengths of this approach include its abilities to analyze high-temperature systems and to characterize complex structure such as grain boundaries. This approach enables the automated analysis of systems and mechanisms previously not possible.



### Capsules for Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.04241v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.04241v1)
- **Published**: 2018-04-11 21:57:57+00:00
- **Updated**: 2018-04-11 21:57:57+00:00
- **Authors**: Rodney LaLonde, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al., referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutional-deconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 x 512) as opposed to baseline capsules (typically less than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4% while still providing a better segmentation accuracy.




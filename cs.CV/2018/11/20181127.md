# Arxiv Papers in cs.CV on 2018-11-27
### A Coarse-to-fine Deep Convolutional Neural Network Framework for Frame Duplication Detection and Localization in Forged Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.10762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10762v2)
- **Published**: 2018-11-27 01:08:05+00:00
- **Updated**: 2019-05-06 00:52:46+00:00
- **Authors**: Chengjiang Long, Arslan Basharat, Anthony Hoogs
- **Comment**: None
- **Journal**: None
- **Summary**: Videos can be manipulated by duplicating a sequence of consecutive frames with the goal of concealing or imitating a specific content in the same video. In this paper, we propose a novel coarse-to-fine framework based on deep Convolutional Neural Networks to automatically detect and localize such frame duplication. First, an I3D network finds coarse-level matches between candidate duplicated frame sequences and the corresponding selected original frame sequences. Then a Siamese network based on ResNet architecture identifies fine-level correspondences between an individual duplicated frame and the corresponding selected frame. We also propose a robust statistical approach to compute a video-level score indicating the likelihood of manipulation or forgery. Additionally, for providing manipulation localization information we develop an inconsistency detector based on the I3D network to distinguish the duplicated frames from the selected original frames. Quantified evaluation on two challenging video forgery datasets clearly demonstrates that this approach performs significantly better than four recent state-of-the-art methods.



### Quality-Aware Multimodal Saliency Detection via Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.10763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10763v1)
- **Published**: 2018-11-27 01:10:34+00:00
- **Updated**: 2018-11-27 01:10:34+00:00
- **Authors**: Xiao Wang, Tao Sun, Rui Yang, Chenglong Li, Bin Luo, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Incorporating various modes of information into the machine learning procedure is becoming a new trend. And data from various source can provide more information than single one no matter they are heterogeneous or homogeneous. Existing deep learning based algorithms usually directly concatenate features from each domain to represent the input data. Seldom of them take the quality of data into consideration which is a key issue in related multimodal problems. In this paper, we propose an efficient quality-aware deep neural network to model the weight of data from each domain using deep reinforcement learning (DRL). Specifically, we take the weighting of each domain as a decision-making problem and teach an agent learn to interact with the environment. The agent can tune the weight of each domain through discrete action selection and obtain a positive reward if the saliency results are improved. The target of the agent is to achieve maximum rewards after finished its sequential action selection. We validate the proposed algorithms on multimodal saliency detection in a coarse-to-fine way. The coarse saliency maps are generated from an encoder-decoder framework which is trained with content loss and adversarial loss. The final results can be obtained via adaptive weighting of maps from each domain. Experiments conducted on two kinds of salient object detection benchmarks validated the effectiveness of our proposed quality-aware deep neural network.



### Generating Attention from Classifier Activations for Fine-grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.10770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10770v1)
- **Published**: 2018-11-27 02:02:50+00:00
- **Updated**: 2018-11-27 02:02:50+00:00
- **Authors**: Wei Shen, Rujie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in fine-grained recognition utilize attention maps to localize objects of interest. Although there are many ways to generate attention maps, most of them rely on sophisticated loss functions or complex training processes. In this work, we propose a simple and straightforward attention generation model based on the output activations of classifiers. The advantage of our model is that it can be easily trained with image level labels and softmax loss functions. More specifically, multiple linear local classifiers are firstly adopted to perform fine-grained classification at each location of high level CNN feature maps. The attention map is generated by aggregating and max-pooling the output activations. Then the attention map serves as a surrogate target object mask to train those local classifiers, similar to training models for semantic segmentation. Our model achieves state-of-the-art results on three heavily benchmarked datasets, i.e. 87.9% on CUB-200-2011 dataset, 94.1% on Stanford Cars dataset and 92.1% on FGVC-Aircraft dataset, demonstrating its effectiveness on fine-grained recognition tasks.



### Event-Based Structured Light for Depth Reconstruction using Frequency Tagged Light Patterns
- **Arxiv ID**: http://arxiv.org/abs/1811.10771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10771v1)
- **Published**: 2018-11-27 02:03:44+00:00
- **Updated**: 2018-11-27 02:03:44+00:00
- **Authors**: T. Leroux, S. -H. Ieng, R. Benosman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new method for 3D depth estimation using the output of an asynchronous time driven image sensor. In association with a high speed Digital Light Processing projection system, our method achieves real-time reconstruction of 3D points cloud, up to several hundreds of hertz. Unlike state of the art methodology, we introduce a method that relies on the use of frequency tagged light pattern that make use of the high temporal resolution of event based sensors. This approch eases matching as each pattern unique frequency allow for any easy matching between displayed patterns and the event based sensor. Results are show on real scenes.



### Tackling Early Sparse Gradients in Softmax Activation Using Leaky Squared Euclidean Distance
- **Arxiv ID**: http://arxiv.org/abs/1811.10779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10779v1)
- **Published**: 2018-11-27 02:37:03+00:00
- **Updated**: 2018-11-27 02:37:03+00:00
- **Authors**: Wei Shen, Rujie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Softmax activation is commonly used to output the probability distribution over categories based on certain distance metric. In scenarios like one-shot learning, the distance metric is often chosen to be squared Euclidean distance between the query sample and the category prototype. This practice works well in most time. However, we find that choosing squared Euclidean distance may cause distance explosion leading gradients to be extremely sparse in the early stage of back propagation. We term this phenomena as the early sparse gradients problem. Though it doesn't deteriorate the convergence of the model, it may set up a barrier to further model improvement. To tackle this problem, we propose to use leaky squared Euclidean distance to impose a restriction on distances. In this way, we can avoid distance explosion and increase the magnitude of gradients. Extensive experiments are conducted on Omniglot and miniImageNet datasets. We show that using leaky squared Euclidean distance can improve one-shot classification accuracy on both datasets.



### Unsupervised Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1811.10787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10787v2)
- **Published**: 2018-11-27 03:16:20+00:00
- **Updated**: 2019-04-06 10:57:25+00:00
- **Authors**: Yang Feng, Lin Ma, Wei Liu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved great successes on the image captioning task. However, most of the existing models depend heavily on paired image-sentence datasets, which are very expensive to acquire. In this paper, we make the first attempt to train an image captioning model in an unsupervised manner. Instead of relying on manually labeled image-sentence pairs, our proposed model merely requires an image set, a sentence corpus, and an existing visual concept detector. The sentence corpus is used to teach the captioning model how to generate plausible sentences. Meanwhile, the knowledge in the visual concept detector is distilled into the captioning model to guide the model to recognize the visual concepts in an image. In order to further encourage the generated captions to be semantically consistent with the image, the image and caption are projected into a common latent space so that they can reconstruct each other. Given that the existing sentence corpora are mainly designed for linguistic research and are thus with little reference to image contents, we crawl a large-scale image description corpus of two million natural sentences to facilitate the unsupervised image captioning scenario. Experimental results show that our proposed model is able to produce quite promising results without any caption annotations.



### Reconstruction Loss Minimized FCN for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1811.10788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10788v1)
- **Published**: 2018-11-27 03:26:26+00:00
- **Updated**: 2018-11-27 03:26:26+00:00
- **Authors**: Shirsendu Sukanta Halder, Sanchayan Santra, Bhabatosh Chanda
- **Comment**: 12 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Haze and fog reduce the visibility of outdoor scenes as a veil like semi-transparent layer appears over the objects. As a result, images captured under such conditions lack contrast. Image dehazing methods try to alleviate this problem by recovering a clear version of the image. In this paper, we propose a Fully Convolutional Neural Network based model to recover the clear scene radiance by estimating the environmental illumination and the scene transmittance jointly from a hazy image. The method uses a relaxed haze imaging model to allow for the situations with non-uniform illumination. We have trained the network by minimizing a custom-defined loss that measures the error of reconstructing the hazy image in three different ways. Additionally, we use a multilevel approach to determine the scene transmittance and the environmental illumination in order to reduce the dependence of the estimate on image scale. Evaluations show that our model performs well compared to the existing state-of-the-art methods. It also verifies the potential of our model in diverse situations and various lighting conditions.



### Sequentially Aggregated Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.10798v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10798v3)
- **Published**: 2018-11-27 04:15:35+00:00
- **Updated**: 2019-08-31 07:22:32+00:00
- **Authors**: Yiwen Huang, Rihui Wu, Pinglai Ou, Ziyong Feng
- **Comment**: To appear in ICCV 2019 workshop
- **Journal**: None
- **Summary**: Modern deep networks generally implement a certain form of shortcut connections to alleviate optimization difficulties. However, we observe that such network topology alters the nature of deep networks. In many ways, these networks behave similarly to aggregated wide networks. We thus exploit the aggregation nature of shortcut connections at a finer architectural level and place them within wide convolutional layers. We end up with a sequentially aggregated convolutional (SeqConv) layer that combines the benefits of both wide and deep representations by aggregating features of various depths in sequence. The proposed SeqConv serves as a drop-in replacement of regular wide convolutional layers and thus could be handily integrated into any backbone network. We apply SeqConv to widely adopted backbones including ResNet and ResNeXt, and conduct experiments for image classification on public benchmark datasets. Our ResNet based network with a model size of ResNet-50 easily surpasses the performance of the 2.35$\times$ larger ResNet-152, while our ResNeXt based model sets a new state-of-the-art accuracy on ImageNet classification for networks with similar model complexity. The code and pre-trained models of our work are publicly available at https://github.com/GroupOfAlchemists/SeqConv.



### Probabilistic Object Detection: Definition and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1811.10800v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10800v4)
- **Published**: 2018-11-27 04:27:40+00:00
- **Updated**: 2020-01-30 05:49:50+00:00
- **Authors**: David Hall, Feras Dayoub, John Skinner, Haoyang Zhang, Dimity Miller, Peter Corke, Gustavo Carneiro, Anelia Angelova, Niko SÃ¼nderhauf
- **Comment**: 21 pages, 25 figures, to appear in the proceedings of the winter
  conference on applications of computer vision WACV 2020
- **Journal**: None
- **Summary**: We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ).Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections. We contrast PDQ with existing mAP and moLRP measures by evaluating state-of-the-art detectors and a Bayesian object detector based on Monte Carlo Dropout. Our experiments indicate that conventional object detectors tend to be spatially overconfident and thus perform poorly on the task of probabilistic object detection. Our paper aims to encourage the development of new object detection approaches that provide detections with accurately estimated spatial and label uncertainties and are of critical importance for deployment on robots and embodied AI systems in the real world.



### Perceptual Conditional Generative Adversarial Networks for End-to-End Image Colourization
- **Arxiv ID**: http://arxiv.org/abs/1811.10801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10801v1)
- **Published**: 2018-11-27 04:28:08+00:00
- **Updated**: 2018-11-27 04:28:08+00:00
- **Authors**: Shirsendu Sukanta Halder, Kanjar De, Partha Pratim Roy
- **Comment**: 16 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Colours are everywhere. They embody a significant part of human visual perception. In this paper, we explore the paradigm of hallucinating colours from a given gray-scale image. The problem of colourization has been dealt in previous literature but mostly in a supervised manner involving user-interference. With the emergence of Deep Learning methods numerous tasks related to computer vision and pattern recognition have been automatized and carried in an end-to-end fashion due to the availability of large data-sets and high-power computing systems. We investigate and build upon the recent success of Conditional Generative Adversarial Networks (cGANs) for Image-to-Image translations. In addition to using the training scheme in the basic cGAN, we propose an encoder-decoder generator network which utilizes the class-specific cross-entropy loss as well as the perceptual loss in addition to the original objective function of cGAN. We train our model on a large-scale dataset and present illustrative qualitative and quantitative analysis of our results. Our results vividly display the versatility and proficiency of our methods through life-like colourization outcomes.



### Noise-tolerant Audio-visual Online Person Verification using an Attention-based Neural Network Fusion
- **Arxiv ID**: http://arxiv.org/abs/1811.10813v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1811.10813v1)
- **Published**: 2018-11-27 04:58:10+00:00
- **Updated**: 2018-11-27 04:58:10+00:00
- **Authors**: Suwon Shon, Tae-Hyun Oh, James Glass
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a multi-modal online person verification system using both speech and visual signals. Inspired by neuroscientific findings on the association of voice and face, we propose an attention-based end-to-end neural network that learns multi-sensory associations for the task of person verification. The attention mechanism in our proposed network learns to conditionally select a salient modality between speech and facial representations that provides a balance between complementary inputs. By virtue of this capability, the network is robust to missing or corrupted data from either modality. In the VoxCeleb2 dataset, we show that our method performs favorably against competing multi-modal methods. Even for extreme cases of large corruption or an entirely missing modality, our method demonstrates robustness over other unimodal methods.



### From Recognition to Cognition: Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1811.10830v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1811.10830v2)
- **Published**: 2018-11-27 06:22:26+00:00
- **Updated**: 2019-03-26 17:50:34+00:00
- **Authors**: Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi
- **Comment**: CVPR 2019 oral. Project page at https://visualcommonsense.com
- **Journal**: None
- **Summary**: Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer.   Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (~45%).   To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (~65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.



### Part-level Car Parsing and Reconstruction from Single Street View
- **Arxiv ID**: http://arxiv.org/abs/1811.10837v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10837v2)
- **Published**: 2018-11-27 06:38:36+00:00
- **Updated**: 2019-08-26 14:46:00+00:00
- **Authors**: Qichuan Geng, Hong Zhang, Xinyu Huang, Sen Wang, Feixiang Lu, Xinjing Cheng, Zhong Zhou, Ruigang Yang
- **Comment**: Version 2: 1. A major revision; 2. Experiments based on ApolloScape
  dataset are added
- **Journal**: None
- **Summary**: Part information has been shown to be resistant to occlusions and viewpoint changes, which is beneficial for various vision-related tasks. However, we found very limited work in car pose estimation and reconstruction from street views leveraging the part information. There are two major contributions in this paper. Firstly, we make the first attempt to build a framework to simultaneously estimate shape, translation, orientation, and semantic parts of cars in 3D space from a single street view. As it is labor-intensive to annotate semantic parts on real street views, we propose a specific approach to implicitly transfer part features from synthesized images to real street views. For pose and shape estimation, we propose a novel network structure that utilizes both part features and 3D losses. Secondly, we are the first to construct a high-quality dataset that contains 348 different car models with physical dimensions and part-level annotations based on global and local deformations. Given these models, we further generate 60K synthesized images with randomization of orientation, illumination, occlusion, and texture. Our results demonstrate that our part segmentation performance is significantly improved after applying our implicit transfer approach. Our network for pose and shape estimation achieves the state-of-the-art performance on the ApolloCar3D dataset and outperforms 3D-RCNN and DeepMANTA by 12.57 and 8.91 percentage points in terms of mean A3DP-Abs.



### CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10842v2)
- **Published**: 2018-11-27 07:03:12+00:00
- **Updated**: 2020-01-30 06:10:51+00:00
- **Authors**: Junsong Fan, Zhaoxiang Zhang, Tieniu Tan, Chunfeng Song, Jun Xiao
- **Comment**: 9 pages, 4 figures, AAAI 2020
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation with only image-level labels saves large human effort to annotate pixel-level labels. Cutting-edge approaches rely on various innovative constraints and heuristic rules to generate the masks for every single image. Although great progress has been achieved by these methods, they treat each image independently and do not take account of the relationships across different images. In this paper, however, we argue that the cross-image relationship is vital for weakly supervised segmentation. Because it connects related regions across images, where supplementary representations can be propagated to obtain more consistent and integral regions. To leverage this information, we propose an end-to-end cross-image affinity module, which exploits pixel-level cross-image relationships with only image-level labels. By means of this, our approach achieves 64.3% and 65.3% mIoU on Pascal VOC 2012 validation and test set respectively, which is a new state-of-the-art result by only using image-level labels for weakly supervised semantic segmentation, demonstrating the superiority of our approach.



### Algae Detection Using Computer Vision and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.10847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.10847v1)
- **Published**: 2018-11-27 07:31:26+00:00
- **Updated**: 2018-11-27 07:31:26+00:00
- **Authors**: Arabinda Samantaray, Baijian Yang, J. Eric Dietz, Byung-Cheol Min
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: A disconcerting ramification of water pollution caused by burgeoning populations, rapid industrialization and modernization of agriculture, has been the exponential increase in the incidence of algal growth across the globe. Harmful algal blooms (HABs) have devastated fisheries, contaminated drinking water and killed livestock, resulting in economic losses to the tune of millions of dollars. Therefore, it is important to constantly monitor water bodies and identify any algae build-up so that prompt action against its accumulation can be taken and the harmful consequences can be avoided. In this paper, we propose a computer vision system based on deep learning for algae monitoring. The proposed system is fast, accurate and cheap, and it can be installed on any robotic platforms such as USVs and UAVs for autonomous algae monitoring. The experimental results demonstrate that the proposed system can detect algae in distinct environments regardless of the underlying hardware with high accuracy and in real time.



### Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects
- **Arxiv ID**: http://arxiv.org/abs/1811.10862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10862v2)
- **Published**: 2018-11-27 08:14:02+00:00
- **Updated**: 2019-04-21 06:52:28+00:00
- **Authors**: Yusuke Niitani, Takuya Akiba, Tommi Kerola, Toru Ogawa, Shotaro Sano, Shuji Suzuki
- **Comment**: CVPR2019 oral
- **Journal**: None
- **Summary**: Efficient and reliable methods for training of object detectors are in higher demand than ever, and more and more data relevant to the field is becoming available. However, large datasets like Open Images Dataset v4 (OID) are sparsely annotated, and some measure must be taken in order to ensure the training of a reliable detector. In order to take the incompleteness of these datasets into account, one possibility is to use pretrained models to detect the presence of the unverified objects. However, the performance of such a strategy depends largely on the power of the pretrained model. In this study, we propose part-aware sampling, a method that uses human intuition for the hierarchical relation between objects. In terse terms, our method works by making assumptions like "a bounding box for a car should contain a bounding box for a tire". We demonstrate the power of our method on OID and compare the performance against a method based on a pretrained model. Our method also won the first and second place on the public and private test sets of the Google AI Open Images Competition 2018.



### Object Tracking by Reconstruction with View-Specific Discriminative Correlation Filters
- **Arxiv ID**: http://arxiv.org/abs/1811.10863v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10863v1)
- **Published**: 2018-11-27 08:16:29+00:00
- **Updated**: 2018-11-27 08:16:29+00:00
- **Authors**: Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: Standard RGB-D trackers treat the target as an inherently 2D structure, which makes modelling appearance changes related even to simple out-of-plane rotation highly challenging. We address this limitation by proposing a novel long-term RGB-D tracker - Object Tracking by Reconstruction (OTR). The tracker performs online 3D target reconstruction to facilitate robust learning of a set of view-specific discriminative correlation filters (DCFs). The 3D reconstruction supports two performance-enhancing features: (i) generation of accurate spatial support for constrained DCF learning from its 2D projection and (ii) point cloud based estimation of 3D pose change for selection and storage of view-specific DCFs which are used to robustly localize the target after out-of-view rotation or heavy occlusion. Extensive evaluation of OTR on the challenging Princeton RGB-D tracking and STC Benchmarks shows it outperforms the state-of-the-art by a large margin.



### Affinity Derivation and Graph Merge for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10870v1)
- **Published**: 2018-11-27 08:34:28+00:00
- **Updated**: 2018-11-27 08:34:28+00:00
- **Authors**: Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu, Houqiang Li, Yan Lu
- **Comment**: Published in ECCV 2018
- **Journal**: None
- **Summary**: We present an instance segmentation scheme based on pixel affinity information, which is the relationship of two pixels belonging to a same instance. In our scheme, we use two neural networks with similar structure. One is to predict pixel level semantic score and the other is designed to derive pixel affinities.   Regarding pixels as the vertexes and affinities as edges, we then propose a simple yet effective graph merge algorithm to cluster pixels into instances. Experimental results show that our scheme can generate fine-grained instance mask.   With Cityscapes training data, the proposed scheme achieves 27.3 AP on test set.



### Automatic Image Stylization Using Deep Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.10872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10872v1)
- **Published**: 2018-11-27 08:39:48+00:00
- **Updated**: 2018-11-27 08:39:48+00:00
- **Authors**: Feida Zhu, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Color and tone stylization strives to enhance unique themes with artistic color and tone adjustments. It has a broad range of applications from professional image postprocessing to photo sharing over social networks. Mainstream photo enhancement softwares provide users with predefined styles, which are often hand-crafted through a trial-and-error process. Such photo adjustment tools lack a semantic understanding of image contents and the resulting global color transform limits the range of artistic styles it can represent. On the other hand, stylistic enhancement needs to apply distinct adjustments to various semantic regions. Such an ability enables a broader range of visual styles. In this paper, we propose a novel deep learning architecture for automatic image stylization, which learns local enhancement styles from image pairs. Our deep learning architecture is an end-to-end deep fully convolutional network performing semantics-aware feature extraction as well as automatic image adjustment prediction. Image stylization can be efficiently accomplished with a single forward pass through our deep network. Experiments on existing datasets for image stylization demonstrate the effectiveness of our deep learning architecture.



### UnDEMoN 2.0: Improved Depth and Ego Motion Estimation through Deep Image Sampling
- **Arxiv ID**: http://arxiv.org/abs/1811.10884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10884v1)
- **Published**: 2018-11-27 09:19:03+00:00
- **Updated**: 2018-11-27 09:19:03+00:00
- **Authors**: Madhu Babu V, Swagat Kumar, Anima Majumder, Kaushik Das
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we provide an improved version of UnDEMoN model for depth and ego motion estimation from monocular images. The improvement is achieved by combining the standard bi-linear sampler with a deep network based image sampling model (DIS-NET) to provide better image reconstruction capabilities on which the depth estimation accuracy depends in un-supervised learning models. While DIS-NET provides higher order regression and larger input search space, the bi-linear sampler provides geometric constraints necessary for reducing the size of the solution space for an ill-posed problem of this kind. This combination is shown to provide significant improvement in depth and pose estimation accuracy outperforming all existing state-of-the-art methods in this category. In addition, the modified network uses far less number of tunable parameters making it one of the lightest deep network model for depth estimation. The proposed model is labeled as "UnDEMoN 2.0" indicating an improvement over the existing UnDEMoN model. The efficacy of the proposed model is demonstrated through rigorous experimental analysis on the standard KITTI dataset.



### DSBI: Double-Sided Braille Image Dataset and Algorithm Evaluation for Braille Dots Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.10893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10893v2)
- **Published**: 2018-11-27 09:49:08+00:00
- **Updated**: 2019-04-02 02:43:53+00:00
- **Authors**: Renqiang Li, Hong Liu, Xiangdong Wan, Yueliang Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Braille is an effective way for the visually impaired to learn knowledge and obtain information. Braille image recognition aims to automatically detect Braille dots in the whole Braille image. There is no available public datasets for Braille image recognition to push relevant research and evaluate algorithms. This paper constructs a large-scale Double-Sided Braille Image dataset DSBI with detailed Braille recto dots, verso dots and Braille cells annotation. To quickly annotate Braille images, an auxiliary annotation strategy is proposed, which adopts initial automatic detection of Braille dots and modifies annotation results by convenient human-computer interaction method. This labeling strategy can averagely increase label efficiency by six times for recto dots annotation in one Braille image. Braille dots detection is the core and basic step for Braille image recognition. This paper also evaluates some Braille dots detection methods on our dataset DSBI and gives the benchmark performance of recto dots detection. We have released our Braille images dataset on the GitHub website.



### Are 2D-LSTM really dead for offline text recognition?
- **Arxiv ID**: http://arxiv.org/abs/1811.10899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10899v1)
- **Published**: 2018-11-27 10:13:01+00:00
- **Updated**: 2018-11-27 10:13:01+00:00
- **Authors**: Bastien Moysset, Ronaldo Messina
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: There is a recent trend in handwritten text recognition with deep neural networks to replace 2D recurrent layers with 1D, and in some cases even completely remove the recurrent layers, relying on simple feed-forward convolutional only architectures. The most used type of recurrent layer is the Long-Short Term Memory (LSTM). The motivations to do so are many: there are few open-source implementations of 2D-LSTM, even fewer supporting GPU implementations (currently cuDNN only implements 1D-LSTM); 2D recurrences reduce the amount of computations that can be parallelized, and thus possibly increase the training/inference time; recurrences create global dependencies with respect to the input, and sometimes this may not be desirable.   Many recent competitions were won by systems that employed networks that use 2D-LSTM layers. Most previous work that compared 1D or pure feed-forward architectures to 2D recurrent models have done so on simple datasets or did not fully optimize the "baseline" 2D model compared to the challenger model, which was dully optimized.   In this work, we aim at a fair comparison between 2D and competing models and also extensively evaluate them on more complex datasets that are more representative of challenging "real-world" data, compared to "academic" datasets that are more restricted in their complexity. We aim at determining when and why the 1D and 2D recurrent models have different results. We also compare the results with a language model to assess if linguistic constraints do level the performance of the different networks.   Our results show that for challenging datasets, 2D-LSTM networks still seem to provide the highest performances and we propose a visualization strategy to explain it.



### Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing
- **Arxiv ID**: http://arxiv.org/abs/1811.10907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1811.10907v2)
- **Published**: 2018-11-27 10:52:26+00:00
- **Updated**: 2019-01-04 11:12:31+00:00
- **Authors**: Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, Shin'ichi Satoh
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Diffusion is commonly used as a ranking or re-ranking method in retrieval tasks to achieve higher retrieval performance, and has attracted lots of attention in recent years. A downside to diffusion is that it performs slowly in comparison to the naive k-NN search, which causes a non-trivial online computational cost on large datasets. To overcome this weakness, we propose a novel diffusion technique in this paper. In our work, instead of applying diffusion to the query, we pre-compute the diffusion results of each element in the database, making the online search a simple linear combination on top of the k-NN search process. Our proposed method becomes 10~ times faster in terms of online search speed. Moreover, we propose to use late truncation instead of early truncation in previous works to achieve better retrieval performance.



### Beyond One Glance: Gated Recurrent Architecture for Hand Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.10914v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10914v3)
- **Published**: 2018-11-27 11:16:41+00:00
- **Updated**: 2018-12-12 13:26:20+00:00
- **Authors**: Wei Wang, Kaicheng Yu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann
- **Comment**: The first two authors contribute equally
- **Journal**: None
- **Summary**: As mixed reality is gaining increased momentum, the development of effective and efficient solutions to egocentric hand segmentation is becoming critical. Traditional segmentation techniques typically follow a one-shot approach, where the image is passed forward only once through a model that produces a segmentation mask. This strategy, however, does not reflect the perception of humans, who continuously refine their representation of the world. In this paper, we therefore introduce a novel gated recurrent architecture. It goes beyond both iteratively passing the predicted segmentation mask through the network and adding a standard recurrent unit to it. Instead, it incorporates multiple encoder-decoder layers of the segmentation network, so as to keep track of its internal state in the refinement process. As evidenced by our results on standard hand segmentation benchmarks and on our own dataset, our approach outperforms these other, simpler recurrent segmentation techniques, as well as the state-of-the-art hand segmentation one. Furthermore, we demonstrate the generality of our approach by applying it to road segmentation, where it also outperforms other baseline methods.



### Continuous Trade-off Optimization between Fast and Accurate Deep Face Detectors
- **Arxiv ID**: http://arxiv.org/abs/1811.11582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11582v1)
- **Published**: 2018-11-27 12:16:22+00:00
- **Updated**: 2018-11-27 12:16:22+00:00
- **Authors**: Petru Soviany, Radu Tudor Ionescu
- **Comment**: Accepted at ICONIP 2018. arXiv admin note: substantial text overlap
  with arXiv:1803.08707
- **Journal**: None
- **Summary**: Although deep neural networks offer better face detection results than shallow or handcrafted models, their complex architectures come with higher computational requirements and slower inference speeds than shallow neural networks. In this context, we study five straightforward approaches to achieve an optimal trade-off between accuracy and speed in face detection. All the approaches are based on separating the test images in two batches, an easy batch that is fed to a faster face detector and a difficult batch that is fed to a more accurate yet slower detector. We conduct experiments on the AFW and the FDDB data sets, using MobileNet-SSD as the fast face detector and S3FD (Single Shot Scale-invariant Face Detector) as the accurate face detector, both models being pre-trained on the WIDER FACE data set. Our experiments show that the proposed difficulty metrics compare favorably to a random split of the images.



### Deep Geometric Prior for Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1811.10943v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.10943v2)
- **Published**: 2018-11-27 12:50:46+00:00
- **Updated**: 2019-04-05 00:31:43+00:00
- **Authors**: Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, Daniele Panozzo
- **Comment**: None
- **Journal**: None
- **Summary**: The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.



### Deep Learned Frame Prediction for Video Compression
- **Arxiv ID**: http://arxiv.org/abs/1811.10946v1
- **DOI**: 10.13140/RG.2.2.28590.54085
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.10946v1)
- **Published**: 2018-11-27 12:53:30+00:00
- **Updated**: 2018-11-27 12:53:30+00:00
- **Authors**: Serkan Sulun
- **Comment**: None
- **Journal**: None
- **Summary**: Motion compensation is one of the most essential methods for any video compression algorithm. Video frame prediction is a task analogous to motion compensation. In recent years, the task of frame prediction is undertaken by deep neural networks (DNNs). In this thesis we create a DNN to perform learned frame prediction and additionally implement a codec that contains our DNN. We train our network using two methods for two different goals. Firstly we train our network based on mean square error (MSE) only, aiming to obtain highest PSNR values at frame prediction and video compression. Secondly we use adversarial training to produce visually more realistic frame predictions. For frame prediction, we compare our method with the baseline methods of frame difference and 16x16 block motion compensation. For video compression we further include x264 video codec in the comparison. We show that in frame prediction, adversarial training produces frames that look sharper and more realistic, compared MSE based training, but in video compression it consistently performs worse. This proves that even though adversarial training is useful for generating video frames that are more pleasing to the human eye, they should not be employed for video compression. Moreover, our network trained with MSE produces accurate frame predictions, and in quantitative results, for both tasks, it produces comparable results in all videos and outperforms other methods on average. More specifically, learned frame prediction outperforms other methods in terms of rate-distortion performance in case of high motion video, while the rate-distortion performance of our method is competitive with that of x264 in low motion video.



### MagicVO: End-to-End Monocular Visual Odometry through Deep Bi-directional Recurrent Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.10964v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10964v2)
- **Published**: 2018-11-27 13:22:32+00:00
- **Updated**: 2018-11-28 02:13:06+00:00
- **Authors**: Jian Jiao, Jichao Jiao, Yaokai Mo, Weilun Liu, Zhongliang Deng
- **Comment**: 9 pages,5 figures,CVPR2019
- **Journal**: None
- **Summary**: This paper proposes a new framework to solve the problem of monocular visual odometry, called MagicVO . Based on Convolutional Neural Network (CNN) and Bi-directional LSTM (Bi-LSTM), MagicVO outputs a 6-DoF absolute-scale pose at each position of the camera with a sequence of continuous monocular images as input. It not only utilizes the outstanding performance of CNN in image feature processing to extract the rich features of image frames fully but also learns the geometric relationship from image sequences pre and post through Bi-LSTM to get a more accurate prediction. A pipeline of the MagicVO is shown in Fig. 1. The MagicVO system is end-to-end, and the results of experiments on the KITTI dataset and the ETH-asl cla dataset show that MagicVO has a better performance than traditional visual odometry (VO) systems in the accuracy of pose and the generalization ability.



### One-Shot Item Search with Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/1811.10969v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10969v2)
- **Published**: 2018-11-27 13:30:13+00:00
- **Updated**: 2018-11-28 16:42:18+00:00
- **Authors**: Jonghwa Yim, Junghun James Kim, Daekyu Shin
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In the task of near similar image search, features from Deep Neural Network is often used to compare images and measure similarity. In the past, we only focused visual search in image dataset without text data. However, since deep neural network emerged, the performance of visual search becomes high enough to apply it in many industries from 3D data to multimodal data. Compared to the needs of multimodal search, there has not been sufficient researches.   In this paper, we present a method of near similar search with image and text multimodal dataset. Earlier time, similar image search, especially when searching shopping items, treated image and text separately to search similar items and reorder the results. This regards two tasks of image search and text matching as two different tasks. Our method, however, explore the vast data to compute k-nearest neighbors using both image and text.   In our experiment of similar item search, our system using multimodal data shows better performance than single data while it only increases minute computing time. For the experiment, we collected more than 15 million of accessory and six million of digital product items from online shopping websites, in which the product item comprises item images, titles, categories, and descriptions. Then we compare the performance of multimodal searching to single space searching in these datasets.



### Noise2Void - Learning Denoising from Single Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/1811.10980v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10980v2)
- **Published**: 2018-11-27 13:48:42+00:00
- **Updated**: 2019-04-05 16:07:38+00:00
- **Authors**: Alexander Krull, Tim-Oliver Buchholz, Florian Jug
- **Comment**: 9 pages, 6 Figures
- **Journal**: None
- **Summary**: The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.



### GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping
- **Arxiv ID**: http://arxiv.org/abs/1811.10983v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10983v3)
- **Published**: 2018-11-27 13:55:01+00:00
- **Updated**: 2019-08-21 13:07:58+00:00
- **Authors**: Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, Pascal Fua
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body. Specifically, we build upon the recent progress in 3D point cloud processing with deep networks to extract garment features at varying levels of detail, including point-wise, patch-wise and global features. We fuse these features with those extracted in parallel from the 3D body, so as to model the cloth-body interactions. The resulting two-stream architecture, which we call as GarNet, is trained using a loss function inspired by physics-based modeling, and delivers visually plausible garment shapes whose 3D points are, on average, less than 1 cm away from those of a PBS method, while running 100 times faster. Moreover, the proposed method can model various garment types with different cutting patterns when parameters of those patterns are given as input to the network.



### Eliminating Exposure Bias and Loss-Evaluation Mismatch in Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.10984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.10984v1)
- **Published**: 2018-11-27 13:57:17+00:00
- **Updated**: 2018-11-27 13:57:17+00:00
- **Authors**: Andrii Maksai, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Identity Switching remains one of the main difficulties Multiple Object Tracking (MOT) algorithms have to deal with. Many state-of-the-art approaches now use sequence models to solve this problem but their training can be affected by biases that decrease their efficiency. In this paper, we introduce a new training procedure that confronts the algorithm to its own mistakes while explicitly attempting to minimize the number of switches, which results in better training. We propose an iterative scheme of building a rich training set and using it to learn a scoring function that is an explicit proxy for the target tracking metric. Whether using only simple geometric features or more sophisticated ones that also take appearance into account, our approach outperforms the state-of-the-art on several MOT benchmarks.



### Dense xUnit Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.11051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11051v1)
- **Published**: 2018-11-27 15:21:50+00:00
- **Updated**: 2018-11-27 15:21:50+00:00
- **Authors**: Idan Kligvasser, Tomer Michaeli
- **Comment**: None
- **Journal**: None
- **Summary**: Deep net architectures have constantly evolved over the past few years, leading to significant advancements in a wide array of computer vision tasks. However, besides high accuracy, many applications also require a low computational load and limited memory footprint. To date, efficiency has typically been achieved either by architectural choices at the macro level (e.g. using skip connections or pruning techniques) or modifications at the level of the individual layers (e.g. using depth-wise convolutions or channel shuffle operations). Interestingly, much less attention has been devoted to the role of the activation functions in constructing efficient nets. Recently, Kligvasser et al. showed that incorporating spatial connections within the activation functions, enables a significant boost in performance in image restoration tasks, at any given budget of parameters. However, the effectiveness of their xUnit module has only been tested on simple small models, which are not characteristic of those used in high-level vision tasks. In this paper, we adopt and improve the xUnit activation, show how it can be incorporated into the DenseNet architecture, and illustrate its high effectiveness for classification and image restoration tasks alike. While the DenseNet architecture is extremely efficient to begin with, our dense xUnit net (DxNet) can typically achieve the same performance with far fewer parameters. For example, on ImageNet, our DxNet outperforms a ReLU-based DenseNet having 30% more parameters and achieves state-of-the-art results for this budget of parameters. Furthermore, in denoising and super-resolution, DxNet significantly improves upon all existing lightweight solutions, including the xUnit-based nets of Kligvasser et al.



### Understanding the Importance of Single Directions via Representative Substitution
- **Arxiv ID**: http://arxiv.org/abs/1811.11053v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11053v2)
- **Published**: 2018-11-27 15:25:03+00:00
- **Updated**: 2018-12-06 22:34:56+00:00
- **Authors**: Li Chen, Hailun Ding, Qi Li, Zhuo Li, Jian Peng, Haifeng Li
- **Comment**: 4 pages, 6 figures
- **Journal**: AAAI-19 Workshop on Network Interpretability for Deep Learning,
  2019
- **Summary**: Understanding the internal representations of deep neural networks (DNNs) is crucal to explain their behavior. The interpretation of individual units, which are neurons in MLPs or convolution kernels in convolutional networks, has been paid much attention given their fundamental role. However, recent research (Morcos et al. 2018) presented a counterintuitive phenomenon, which suggests that an individual unit with high class selectivity, called interpretable units, has poor contributions to generalization of DNNs. In this work, we provide a new perspective to understand this counterintuitive phenomenon, which makes sense when we introduce Representative Substitution (RS). Instead of individually selective units with classes, the RS refers to the independence of a unit's representations in the same layer without any annotation. Our experiments demonstrate that interpretable units have high RS which are not critical to network's generalization. The RS provides new insights into the interpretation of DNNs and suggests that we need to focus on the independence and relationship of the representations.



### Fast Object Detection in Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/1811.11057v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11057v3)
- **Published**: 2018-11-27 15:35:53+00:00
- **Updated**: 2019-08-17 08:00:45+00:00
- **Authors**: Shiyao Wang, Hongchao Lu, Zhidong Deng
- **Comment**: 10 pages, 7 figures and 2 tables
- **Journal**: None
- **Summary**: Object detection in videos has drawn increasing attention since it is more practical in real scenarios. Most of the deep learning methods use CNNs to process each decoded frame in a video stream individually. However, the free of charge yet valuable motion information already embedded in the video compression format is usually overlooked. In this paper, we propose a fast object detection method by taking advantage of this with a novel Motion aided Memory Network (MMNet). The MMNet has two major advantages: 1) It significantly accelerates the procedure of feature extraction for compressed videos. It only need to run a complete recognition network for I-frames, i.e. a few reference frames in a video, and it produces the features for the following P frames (predictive frames) with a light weight memory network, which runs fast; 2) Unlike existing methods that establish an additional network to model motion of frames, we take full advantage of both motion vectors and residual errors that are freely available in video streams. To our best knowledge, the MMNet is the first work that investigates a deep convolutional detector on compressed videos. Our method is evaluated on the large-scale ImageNet VID dataset, and the results show that it is 3x times faster than single image detector R-FCN and 10x times faster than high-performance detector MANet at a minor accuracy loss.



### MobiFace: A Lightweight Deep Learning Face Recognition on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1811.11080v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11080v2)
- **Published**: 2018-11-27 16:34:01+00:00
- **Updated**: 2019-04-17 04:00:16+00:00
- **Authors**: Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le, Khoa Luu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been widely used in numerous computer vision applications, particularly in face recognition. However, deploying deep neural network face recognition on mobile devices has recently become a trend but still limited since most high-accuracy deep models are both time and GPU consumption in the inference stage. Therefore, developing a lightweight deep neural network is one of the most practical solutions to deploy face recognition on mobile devices. Such the lightweight deep neural network requires efficient memory with small number of weights representation and low cost operators. In this paper, a novel deep neural network named MobiFace, a simple but effective approach, is proposed for productively deploying face recognition on mobile devices. The experimental results have shown that our lightweight MobiFace is able to achieve high performance with 99.73% on LFW database and 91.3% on large-scale challenging Megaface database. It is also eventually competitive against large-scale deep-networks face recognition while significant reducing computational time and memory consumption.



### Automatic Face Aging in Videos via Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.11082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11082v2)
- **Published**: 2018-11-27 16:41:39+00:00
- **Updated**: 2019-04-24 06:03:56+00:00
- **Authors**: Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Nghia Nguyen, Eric Patterson, Tien D. Bui, Ngan Le
- **Comment**: CVPR2019 Camera Ready, https://face-aging.github.io/RL-VAP/
- **Journal**: None
- **Summary**: This paper presents a novel approach to synthesize automatically age-progressed facial images in video sequences using Deep Reinforcement Learning. The proposed method models facial structures and the longitudinal face-aging process of given subjects coherently across video frames. The approach is optimized using a long-term reward, Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network. Unlike previous age-progression methods that are only able to synthesize an aged likeness of a face from a single input image, the proposed approach is capable of age-progressing facial likenesses in videos with consistently synthesized facial features across frames. In addition, the deep reinforcement learning method guarantees preservation of the visual identity of input faces after age-progression. Results on videos of our new collected aging face AGFW-v2 database demonstrate the advantages of the proposed solution in terms of both quality of age-progressed faces, temporal smoothness, and cross-age face verification.



### Bilinear Parameterization For Differentiable Rank-Regularization
- **Arxiv ID**: http://arxiv.org/abs/1811.11088v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11088v3)
- **Published**: 2018-11-27 16:48:11+00:00
- **Updated**: 2019-07-23 13:32:59+00:00
- **Authors**: Marcus Valtonen Ãrnhag, Carl Olsson, Anders Heyden
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: Low rank approximation is a commonly occurring problem in many computer vision and machine learning applications. There are two common ways of optimizing the resulting models. Either the set of matrices with a given rank can be explicitly parametrized using a bilinear factorization, or low rank can be implicitly enforced using regularization terms penalizing non-zero singular values. While the former approach results in differentiable problems that can be efficiently optimized using local quadratic approximation, the latter is typically not differentiable (sometimes even discontinuous) and requires first order subgradient or splitting methods. It is well known that gradient based methods exhibit slow convergence for ill-conditioned problems.   In this paper we show how many non-differentiable regularization methods can be reformulated into smooth objectives using bilinear parameterization. This allows us to use standard second order methods, such as Levenberg--Marquardt (LM) and Variable Projection (VarPro), to achieve accurate solutions for ill-conditioned cases. We show on several real and synthetic experiments that our second order formulation converges to substantially more accurate solutions than competing state-of-the-art methods.



### Unprocessing Images for Learned Raw Denoising
- **Arxiv ID**: http://arxiv.org/abs/1811.11127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11127v1)
- **Published**: 2018-11-27 17:38:14+00:00
- **Updated**: 2018-11-27 17:38:14+00:00
- **Authors**: Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, Jonathan T. Barron
- **Comment**: http://timothybrooks.com/tech/unprocessing/
- **Journal**: None
- **Summary**: Machine learning techniques work best when the data used for training resembles the data used for evaluation. This holds true for learned single-image denoising algorithms, which are applied to real raw camera sensor readings but, due to practical constraints, are often trained on synthetic image data. Though it is understood that generalizing from synthetic to real data requires careful consideration of the noise properties of image sensors, the other aspects of a camera's image processing pipeline (gain, color correction, tone mapping, etc) are often overlooked, despite their significant effect on how raw measurements are transformed into finished images. To address this, we present a technique to "unprocess" images by inverting each step of an image processing pipeline, thereby allowing us to synthesize realistic raw sensor measurements from commonly available internet photos. We additionally model the relevant components of an image processing pipeline when evaluating our loss function, which allows training to be aware of all relevant photometric processing that will occur after denoising. By processing and unprocessing model outputs and training data in this way, we are able to train a simple convolutional neural network that has 14%-38% lower error rates and is 9x-18x faster than the previous state of the art on the Darmstadt Noise Dataset, and generalizes to sensors outside of that dataset as well.



### Real-time high speed motion prediction using fast aperture-robust event-driven visual flow
- **Arxiv ID**: http://arxiv.org/abs/1811.11135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11135v2)
- **Published**: 2018-11-27 17:52:27+00:00
- **Updated**: 2020-07-16 23:02:27+00:00
- **Authors**: Himanshu Akolkar, SioHoi Ieng, Ryad Benosman
- **Comment**: Pre-print version, 12 pages, 12 figures. Accepted in IEEE tPAMI July
  2020
- **Journal**: None
- **Summary**: Optical flow is a crucial component of the feature space for early visual processing of dynamic scenes especially in new applications such as self-driving vehicles, drones and autonomous robots. The dynamic vision sensors are well suited for such applications because of their asynchronous, sparse and temporally precise representation of the visual dynamics. Many algorithms proposed for computing visual flow for these sensors suffer from the aperture problem as the direction of the estimated flow is governed by the curvature of the object rather than the true motion direction. Some methods that do overcome this problem by temporal windowing under-utilize the true precise temporal nature of the dynamic sensors. In this paper, we propose a novel multi-scale plane fitting based visual flow algorithm that is robust to the aperture problem and also computationally fast and efficient. Our algorithm performs well in many scenarios ranging from fixed camera recording simple geometric shapes to real world scenarios such as camera mounted on a moving car and can successfully perform event-by-event motion estimation of objects in the scene to allow for predictions of upto 500 ms i.e. equivalent to 10 to 25 frames with traditional cameras.



### Understanding and Improving Kernel Local Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1811.11147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11147v1)
- **Published**: 2018-11-27 18:18:13+00:00
- **Updated**: 2018-11-27 18:18:13+00:00
- **Authors**: Arun Mukundan, Giorgos Tolias, Andrei Bursuc, HervÃ© JÃ©gou, OndÅej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a multiple-kernel local-patch descriptor based on efficient match kernels from pixel gradients. It combines two parametrizations of gradient position and direction, each parametrization provides robustness to a different type of patch mis-registration: polar parametrization for noise in the patch dominant orientation detection, Cartesian for imprecise location of the feature point. Combined with whitening of the descriptor space, that is learned with or without supervision, the performance is significantly improved. We analyze the effect of the whitening on patch similarity and demonstrate its semantic meaning. Our unsupervised variant is the best performing descriptor constructed without the need of labeled data. Despite the simplicity of the proposed descriptor, it competes well with deep learning approaches on a number of different tasks.



### FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery
- **Arxiv ID**: http://arxiv.org/abs/1811.11155v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11155v2)
- **Published**: 2018-11-27 18:44:37+00:00
- **Updated**: 2019-04-09 17:44:24+00:00
- **Authors**: Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee
- **Comment**: None
- **Journal**: CVPR 2019 (Oral Presentation)
- **Summary**: We propose FineGAN, a novel unsupervised GAN framework, which disentangles the background, object shape, and object appearance to hierarchically generate images of fine-grained object categories. To disentangle the factors without supervision, our key idea is to use information theory to associate each factor to a latent code, and to condition the relationships between the codes in a specific way to induce the desired hierarchy. Through extensive experiments, we show that FineGAN achieves the desired disentanglement to generate realistic and diverse images belonging to fine-grained classes of birds, dogs, and cars. Using FineGAN's automatically learned features, we also cluster real images as a first attempt at solving the novel problem of unsupervised fine-grained object category discovery. Our code/models/demo can be found at https://github.com/kkanshul/finegan



### Class-Distinct and Class-Mutual Image Generation with GANs
- **Arxiv ID**: http://arxiv.org/abs/1811.11163v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11163v2)
- **Published**: 2018-11-27 18:56:19+00:00
- **Updated**: 2019-07-24 17:51:04+00:00
- **Authors**: Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted to BMVC 2019 (Spotlight). Project page:
  https://takuhirok.github.io/CP-GAN/
- **Journal**: None
- **Summary**: Class-conditional extensions of generative adversarial networks (GANs), such as auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), have garnered attention owing to their ability to decompose representations into class labels and other factors and to boost the training stability. However, a limitation is that they assume that each class is separable and ignore the relationship between classes even though class overlapping frequently occurs in a real-world scenario when data are collected on the basis of diverse or ambiguous criteria. To overcome this limitation, we address a novel problem called class-distinct and class-mutual image generation, in which the goal is to construct a generator that can capture between-class relationships and generate an image selectively conditioned on the class specificity. To solve this problem without additional supervision, we propose classifier's posterior GAN (CP-GAN), in which we redesign the generator input and the objective function of AC-GAN for class-overlapping data. Precisely, we incorporate the classifier's posterior into the generator input and optimize the generator so that the classifier's posterior of generated data corresponds with that of real data. We demonstrate the effectiveness of CP-GAN using both controlled and real-world class-overlapping data with a model configuration analysis and comparative study. Our code is available at https://github.com/takuhirok/CP-GAN/.



### Label-Noise Robust Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.11165v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11165v2)
- **Published**: 2018-11-27 18:56:21+00:00
- **Updated**: 2019-05-02 18:42:42+00:00
- **Authors**: Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted to CVPR 2019 (Oral). Project page:
  https://takuhirok.github.io/rGAN/
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class-conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the label-noise robust classification model, and rcGAN, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total). Our code is available at https://github.com/takuhirok/rGAN/.



### Integrated Object Detection and Tracking with Tracklet-Conditioned Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.11167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11167v1)
- **Published**: 2018-11-27 18:58:07+00:00
- **Updated**: 2018-11-27 18:58:07+00:00
- **Authors**: Zheng Zhang, Dazhi Cheng, Xizhou Zhu, Stephen Lin, Jifeng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate detection and tracking of objects is vital for effective video understanding. In previous work, the two tasks have been combined in a way that tracking is based heavily on detection, but the detection benefits marginally from the tracking. To increase synergy, we propose to more tightly integrate the tasks by conditioning the object detection in the current frame on tracklets computed in prior frames. With this approach, the object detection results not only have high detection responses, but also improved coherence with the existing tracklets. This greater coherence leads to estimated object trajectories that are smoother and more stable than the jittered paths obtained without tracklet-conditioned detection. Over extensive experiments, this approach is shown to achieve state-of-the-art performance in terms of both detection and tracking accuracy, as well as noticeable improvements in tracking stability.



### Deformable ConvNets v2: More Deformable, Better Results
- **Arxiv ID**: http://arxiv.org/abs/1811.11168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11168v2)
- **Published**: 2018-11-27 18:58:11+00:00
- **Updated**: 2018-11-28 09:33:04+00:00
- **Authors**: Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation.



### Scan2CAD: Learning CAD Model Alignment in RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/1811.11187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11187v1)
- **Published**: 2018-11-27 19:00:06+00:00
- **Updated**: 2018-11-27 19:00:06+00:00
- **Authors**: Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, Matthias NieÃner
- **Comment**: Video: https://youtu.be/PiHSYpgLTfA
- **Journal**: None
- **Summary**: We present Scan2CAD, a novel data-driven method that learns to align clean 3D CAD models from a shape database to the noisy and incomplete geometry of a commodity RGB-D scan. For a 3D reconstruction of an indoor scene, our method takes as input a set of CAD models, and predicts a 9DoF pose that aligns each model to the underlying scan geometry. To tackle this problem, we create a new scan-to-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoint pairs between 14225 CAD models from ShapeNet and their counterpart objects in the scans. Our method selects a set of representative keypoints in a 3D scan for which we find correspondences to the CAD geometry. To this end, we design a novel 3D CNN architecture that learns a joint embedding between real and synthetic objects, and from this predicts a correspondence heatmap. Based on these correspondence heatmaps, we formulate a variational energy minimization that aligns a given set of CAD models to the reconstruction. We evaluate our approach on our newly introduced Scan2CAD benchmark where we outperform both handcrafted feature descriptor as well as state-of-the-art CNN based methods by 21.39%.



### You Look Twice: GaterNet for Dynamic Filter Selection in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.11205v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11205v2)
- **Published**: 2018-11-27 19:14:49+00:00
- **Updated**: 2019-04-01 17:57:14+00:00
- **Authors**: Zhourong Chen, Yang Li, Samy Bengio, Si Si
- **Comment**: CVPR2019; Google Research, The Hong Kong University of Science and
  Technology
- **Journal**: None
- **Summary**: The concept of conditional computation for deep nets has been proposed previously to improve model performance by selectively using only parts of the model conditioned on the sample it is processing. In this paper, we investigate input-dependent dynamic filter selection in deep convolutional neural networks (CNNs). The problem is interesting because the idea of forcing different parts of the model to learn from different types of samples may help us acquire better filters in CNNs, improve the model generalization performance and potentially increase the interpretability of model behavior. We propose a novel yet simple framework called GaterNet, which involves a backbone and a gater network. The backbone network is a regular CNN that performs the major computation needed for making a prediction, while a global gater network is introduced to generate binary gates for selectively activating filters in the backbone network based on each input. Extensive experiments on CIFAR and ImageNet datasets show that our models consistently outperform the original models with a large margin. On CIFAR-10, our model also improves upon state-of-the-art results.



### AI Matrix - Synthetic Benchmarks for DNN
- **Arxiv ID**: http://arxiv.org/abs/1812.00886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.00886v1)
- **Published**: 2018-11-27 19:20:35+00:00
- **Updated**: 2018-11-27 19:20:35+00:00
- **Authors**: Wei Wei, Lingjie Xu, Lingling Jin, Wei Zhang, Tianjun Zhang
- **Comment**: Accepted by SC' 18
  https://sc18.supercomputing.org/proceedings/tech_poster/tech_poster_pages/post153.html
- **Journal**: None
- **Summary**: Deep neural network (DNN) architectures, such as convolutional neural networks (CNN), involve heavy computation and require hardware, such as CPU, GPU, and AI accelerators, to provide the massive computing power. With the many varieties of AI hardware prevailing on the market, it is often hard to decide which one is the best to use. Thus, benchmarking AI hardware effectively becomes important and is of great help to select and optimize AI hardware. Unfortunately, there are few AI benchmarks available in both academia and industry. Examples are BenchNN[1], DeepBench[2], and Dawn Bench[3], which are usually a collection of typical real DNN applications. While these benchmarks provide performance comparison across different AI hardware, they suffer from a number of drawbacks. First, they cannot adapt to the emerging changes of DNN algorithms and are fixed once selected. Second, they contain tens to hundreds of applications and take very long time to finish running. Third, they are mainly selected from open sources, which are restricted by copyright and are not representable to proprietary applications. In this work, a synthetic benchmarks framework is firstly proposed to address the above drawbacks of AI benchmarks. Instead of pre-selecting a set of open-sourced benchmarks and running all of them, the synthetic approach generates only a one or few benchmarks that best represent a broad range of applications using profiled workload characteristics data of these applications. Thus, it can adapt to emerging changes of new DNN algorithms by re-profiling new applications and updating itself, greatly reduce benchmark count and running time, and strongly represent DNN applications of interests. The generated benchmarks are called AI Matrix, serving as a performance benchmarks matching the statistical workload characteristics of a combination of applications of interests.



### Iterative Transformer Network for 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1811.11209v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11209v2)
- **Published**: 2018-11-27 19:22:24+00:00
- **Updated**: 2019-10-18 02:48:32+00:00
- **Authors**: Wentao Yuan, David Held, Christoph Mertz, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point cloud is an efficient and flexible representation of 3D structures. Recently, neural networks operating on point clouds have shown superior performance on 3D understanding tasks such as shape classification and part segmentation. However, performance on such tasks is evaluated on complete shapes aligned in a canonical frame, while real world 3D data are partial and unaligned. A key challenge in learning from partial, unaligned point cloud data is to learn features that are invariant or equivariant with respect to geometric transformations. To address this challenge, we propose the Iterative Transformer Network (IT-Net), a network module that canonicalizes the pose of a partial object with a series of 3D rigid transformations predicted in an iterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose estimator from partial point clouds without using complete object models. Further, we show that IT-Net achieves superior performance over alternative 3D transformer networks on various tasks, such as partial shape classification and object part segmentation.



### Self-Supervised GANs via Auxiliary Rotation Loss
- **Arxiv ID**: http://arxiv.org/abs/1811.11212v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11212v2)
- **Published**: 2018-11-27 19:30:40+00:00
- **Updated**: 2019-04-09 14:25:35+00:00
- **Authors**: Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, Neil Houlsby
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.



### CT organ segmentation using GPU data augmentation, unsupervised labels and IOU loss
- **Arxiv ID**: http://arxiv.org/abs/1811.11226v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11226v1)
- **Published**: 2018-11-27 19:53:59+00:00
- **Updated**: 2018-11-27 19:53:59+00:00
- **Authors**: Blaine Rister, Darvin Yi, Kaushik Shivakumar, Tomomi Nobashi, Daniel L. Rubin
- **Comment**: Journal submission pre-print
- **Journal**: None
- **Summary**: Fully-convolutional neural networks have achieved superior performance in a variety of image segmentation tasks. However, their training requires laborious manual annotation of large datasets, as well as acceleration by parallel processors with high-bandwidth memory, such as GPUs. We show that simple models can achieve competitive accuracy for organ segmentation on CT images when trained with extensive data augmentation, which leverages existing graphics hardware to quickly apply geometric and photometric transformations to 3D image data. On 3 mm^3 CT volumes, our GPU implementation is 2.6-8X faster than a widely-used CPU version, including communication overhead. We also show how to automatically generate training labels using rudimentary morphological operations, which are efficiently computed by 3D Fourier transforms. We combined fully-automatic labels for the lungs and bone with semi-automatic ones for the liver, kidneys and bladder, to create a dataset of 130 labeled CT scans. To achieve the best results from data augmentation, our model uses the intersection-over-union (IOU) loss function, a close relative of the Dice loss. We discuss its mathematical properties and explain why it outperforms the usual weighted cross-entropy loss for unbalanced segmentation tasks. We conclude that there is no unique IOU loss function, as the naive one belongs to a broad family of functions with the same essential properties. When combining data augmentation with the IOU loss, our model achieves a Dice score of 78-92% for each organ. The trained model, code and dataset will be made publicly available, to further medical imaging research.



### A Compositional Textual Model for Recognition of Imperfect Word Images
- **Arxiv ID**: http://arxiv.org/abs/1811.11239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11239v1)
- **Published**: 2018-11-27 20:23:02+00:00
- **Updated**: 2018-11-27 20:23:02+00:00
- **Authors**: Wei Tang, John Corring, Ying Wu, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Printed text recognition is an important problem for industrial OCR systems. Printed text is constructed in a standard procedural fashion in most settings. We develop a mathematical model for this process that can be applied to the backward inference problem of text recognition from an image. Through ablation experiments we show that this model is realistic and that a multi-task objective setting can help to stabilize estimation of its free parameters, enabling use of conventional deep learning methods. Furthermore, by directly modeling the geometric perturbations of text synthesis we show that our model can help recover missing characters from incomplete text regions, the bane of multicomponent OCR systems, enabling recognition even when the detection returns incomplete information.



### eXclusive Autoencoder (XAE) for Nucleus Detection and Classification on Hematoxylin and Eosin (H&E) Stained Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/1811.11243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11243v1)
- **Published**: 2018-11-27 20:27:46+00:00
- **Updated**: 2018-11-27 20:27:46+00:00
- **Authors**: Chao-Hui Huang, Daniel Racoceanu
- **Comment**: 11 pages, 7 figures, 5 tables, conference paper
- **Journal**: None
- **Summary**: In this paper, we introduced a novel feature extraction approach, named exclusive autoencoder (XAE), which is a supervised version of autoencoder (AE), able to largely improve the performance of nucleus detection and classification on hematoxylin and eosin (H&E) histopathological images. The proposed XAE can be used in any AE-based algorithm, as long as the data labels are also provided in the feature extraction phase. In the experiments, we evaluated the performance of an approach which is the combination of an XAE and a fully connected neural network (FCN) and compared with some AE-based methods. For a nucleus detection problem (considered as a nucleus/non-nucleus classification problem) on breast cancer H&E images, the F-score of the proposed XAE+FCN approach achieved 96.64% while the state-of-the-art was at 84.49%. For nucleus classification on colorectal cancer H&E images, with the annotations of four categories of epithelial, inflammatory, fibroblast and miscellaneous nuclei. The F-score of the proposed method reached 70.4%. We also proposed a lymphocyte segmentation method. In the step of lymphocyte detection, we have compared with cutting-edge technology and gained improved performance from 90% to 98.67%. We also proposed an algorithm for lymphocyte segmentation based on nucleus detection and classification. The obtained Dice coefficient achieved 88.31% while the cutting-edge approach was at 74%.



### Multiview Supervision By Registration
- **Arxiv ID**: http://arxiv.org/abs/1811.11251v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11251v2)
- **Published**: 2018-11-27 20:46:11+00:00
- **Updated**: 2019-03-24 03:00:13+00:00
- **Authors**: Yilun Zhang, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a semi-supervised learning framework to train a keypoint detector using multiview image streams given the limited labeled data (typically $<$4\%). We leverage the complementary relationship between multiview geometry and visual tracking to provide three types of supervisionary signals to utilize the unlabeled data: (1) keypoint detection in one view can be supervised by other views via the epipolar geometry; (2) a keypoint moves smoothly over time where its optical flow can be used to temporally supervise consecutive image frames to each other; (3) visible keypoint in one view is likely to be visible in the adjacent view. We integrate these three signals in a differentiable fashion to design a new end-to-end neural network composed of three pathways. This design allows us to extensively use the unlabeled data to train the keypoint detector. We show that our approach outperforms existing detectors including DeepLabCut tailored to the keypoint detection of non-human species such as monkeys, dogs, and mice.



### ShelfNet for Fast Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.11254v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11254v6)
- **Published**: 2018-11-27 20:57:48+00:00
- **Updated**: 2019-09-04 03:59:15+00:00
- **Authors**: Juntang Zhuang, Junlin Yang, Lin Gu, Nicha Dvornek
- **Comment**: None
- **Journal**: 2019 IEEE International Conference on Computer Vision (ICCV),
  CVRSUAD
- **Summary**: In this paper, we present ShelfNet, a novel architecture for accurate fast semantic segmentation. Different from the single encoder-decoder structure, ShelfNet has multiple encoder-decoder branch pairs with skip connections at each spatial level, which looks like a shelf with multiple columns. The shelf-shaped structure can be viewed as an ensemble of multiple deep and shallow paths, thus improving accuracy. We significantly reduce computation burden by reducing channel number, at the same time achieving high accuracy with this unique structure. In addition, we propose a shared-weight strategy in the residual block which reduces parameter number without sacrificing performance. Compared with popular non real-time methods such as PSPNet, our ShelfNet achieves 4$\times$ faster inference speed with similar accuracy on PASCAL VOC dataset. Compared with real-time segmentation models such as BiSeNet, our model achieves higher accuracy at comparable speed on the Cityscapes Dataset, enabling the application in speed-demanding tasks such as street-scene understanding for autonomous driving. Furthermore, our ShelfNet achieves 79.0\% mIoU on Cityscapes Dataset with ResNet34 backbone, outperforming PSPNet and BiSeNet with large backbones such as ResNet101. Through extensive experiments, we validated the superior performance of ShelfNet. We provide link to the implementation \url{https://github.com/juntang-zhuang/ShelfNet-lw-cityscapes}.



### Generalizing semi-supervised generative adversarial networks to regression using feature contrasting
- **Arxiv ID**: http://arxiv.org/abs/1811.11269v3
- **DOI**: 10.1016/j.cviu.2019.06.004
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11269v3)
- **Published**: 2018-11-27 21:31:33+00:00
- **Updated**: 2019-09-03 17:36:13+00:00
- **Authors**: Greg Olmschenk, Zhigang Zhu, Hao Tang
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding, Volume 186, September
  2019, Pages 1-12
- **Summary**: In this work, we generalize semi-supervised generative adversarial networks (GANs) from classification problems to regression problems. In the last few years, the importance of improving the training of neural networks using semi-supervised training has been demonstrated for classification problems. We present a novel loss function, called feature contrasting, resulting in a discriminator which can distinguish between fake and real data based on feature statistics. This method avoids potential biases and limitations of alternative approaches. The generalization of semi-supervised GANs to the regime of regression problems of opens their use to countless applications as well as providing an avenue for a deeper understanding of how GANs function. We first demonstrate the capabilities of semi-supervised regression GANs on a toy dataset which allows for a detailed understanding of how they operate in various circumstances. This toy dataset is used to provide a theoretical basis of the semi-supervised regression GAN. We then apply the semi-supervised regression GANs to a number of real-world computer vision applications: age estimation, driving steering angle prediction, and crowd counting from single images. We perform extensive tests of what accuracy can be achieved with significantly reduced annotated data. Through the combination of the theoretical example and real-world scenarios, we demonstrate how semi-supervised GANs can be generalized to regression problems.



### Learning to Synthesize Motion Blur
- **Arxiv ID**: http://arxiv.org/abs/1811.11745v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11745v2)
- **Published**: 2018-11-27 21:55:55+00:00
- **Updated**: 2019-06-20 07:28:50+00:00
- **Authors**: Tim Brooks, Jonathan T. Barron
- **Comment**: http://timothybrooks.com/tech/motion-blur/ . IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: We present a technique for synthesizing a motion blurred image from a pair of unblurred images captured in succession. To build this system we motivate and design a differentiable "line prediction" layer to be used as part of a neural network architecture, with which we can learn a system to regress from image pairs to motion blurred images that span the capture time of the input image pair. Training this model requires an abundance of data, and so we design and execute a strategy for using frame interpolation techniques to generate a large-scale synthetic dataset of motion blurred images and their respective inputs. We additionally capture a high quality test set of real motion blurred images, synthesized from slow motion videos, with which we evaluate our model against several baseline techniques that can be used to synthesize motion blur. Our model produces higher accuracy output than our baselines, and is significantly faster than baselines with competitive accuracy.



### A Compact Embedding for Facial Expression Similarity
- **Arxiv ID**: http://arxiv.org/abs/1811.11283v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1811.11283v2)
- **Published**: 2018-11-27 22:00:06+00:00
- **Updated**: 2019-01-09 22:46:44+00:00
- **Authors**: Raviteja Vemulapalli, Aseem Agarwala
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between expressions measured in the action unit space need not correspond to how humans perceive expression similarity. Different from previous work, our goal is to describe facial expressions in a continuous fashion using a compact embedding space that mimics human visual preferences. To achieve this goal, we collect a large-scale faces-in-the-wild dataset with human annotations in the form: Expressions A and B are visually more similar when compared to expression C, and use this dataset to train a neural network that produces a compact (16-dimensional) expression embedding. We experimentally demonstrate that the learned embedding can be successfully used for various applications such as expression retrieval, photo album summarization, and emotion recognition. We also show that the embedding learned using the proposed dataset performs better than several other embeddings learned using existing emotion or action unit datasets.



### Patch-based Progressive 3D Point Set Upsampling
- **Arxiv ID**: http://arxiv.org/abs/1811.11286v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11286v3)
- **Published**: 2018-11-27 22:01:55+00:00
- **Updated**: 2019-03-21 17:08:50+00:00
- **Authors**: Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga Sorkine-Hornung
- **Comment**: accepted to cvpr2019, code available at https://github.com/yifita/P3U
- **Journal**: None
- **Summary**: We present a detail-driven deep neural network for point set upsampling. A high-resolution point set is essential for point-based rendering and surface reconstruction. Inspired by the recent success of neural image super-resolution techniques, we progressively train a cascade of patch-based upsampling networks on different levels of detail end-to-end. We propose a series of architectural design contributions that lead to a substantial performance boost. The effect of each technical contribution is demonstrated in an ablation study. Qualitative and quantitative experiments show that our method significantly outperforms the state-of-the-art learning-based and optimazation-based approaches, both in terms of handling low-resolution inputs and revealing high-fidelity details.



### Taking Control of Intra-class Variation in Conditional GANs Under Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1811.11296v2
- **DOI**: 10.1109/FG47880.2020.00042
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11296v2)
- **Published**: 2018-11-27 22:38:29+00:00
- **Updated**: 2020-12-19 00:10:05+00:00
- **Authors**: Richard T. Marriott, Sami Romdhani, Liming Chen
- **Comment**: None
- **Journal**: in 2020 15th IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2020) (FG), Buenos Aires, AR, 2020 pp. 283-290
- **Summary**: Generative Adversarial Networks (GANs) are able to learn mappings between simple, relatively low-dimensional, random distributions and points on the manifold of realistic images in image-space. The semantics of this mapping, however, are typically entangled such that meaningful image properties cannot be controlled independently of one another. Conditional GANs (cGANs) provide a potential solution to this problem, allowing specific semantics to be enforced during training. This solution, however, depends on the availability of precise labels, which are sometimes difficult or near impossible to obtain, e.g. labels representing lighting conditions or describing the background. In this paper we introduce a new formulation of the cGAN that is able to learn disentangled, multivariate models of semantically meaningful variation and which has the advantage of requiring only the weak supervision of binary attribute labels. For example, given only labels of ambient / non-ambient lighting, our method is able to learn multivariate lighting models disentangled from other factors such as the identity and pose. We coin the method intra-class variation isolation (IVI) and the resulting network the IVI-GAN. We evaluate IVI-GAN on the CelebA dataset and on synthetic 3D morphable model data, learning to disentangle attributes such as lighting, pose, expression, and even the background.



### Universal Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1811.11304v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11304v2)
- **Published**: 2018-11-27 23:09:27+00:00
- **Updated**: 2019-11-20 20:57:36+00:00
- **Authors**: Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S. Davis, Tom Goldstein
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: Standard adversarial attacks change the predicted class label of a selected image by adding specially tailored small perturbations to its pixels. In contrast, a universal perturbation is an update that can be added to any image in a broad class of images, while still changing the predicted class label. We study the efficient generation of universal adversarial perturbations, and also efficient methods for hardening networks to these attacks. We propose a simple optimization-based universal attack that reduces the top-1 accuracy of various network architectures on ImageNet to less than 20%, while learning the universal perturbation 13X faster than the standard method.   To defend against these perturbations, we propose universal adversarial training, which models the problem of robust classifier generation as a two-player min-max game, and produces robust models with only 2X the cost of natural training. We also propose a simultaneous stochastic gradient method that is almost free of extra computation, which allows us to do universal adversarial training on ImageNet.



### Skin lesion segmentation using U-Net and good training strategies
- **Arxiv ID**: http://arxiv.org/abs/1811.11314v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1811.11314v1)
- **Published**: 2018-11-27 23:45:58+00:00
- **Updated**: 2018-11-27 23:45:58+00:00
- **Authors**: Fred Guth, Teofilo E. deCampos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we approach the problem of skin lesion segmentation using a convolutional neural network based on the U-Net architecture. We present a set of training strategies that had a significant impact on the performance of this model. We evaluated this method on the ISIC Challenge 2018 - Skin Lesion Analysis Towards Melanoma Detection, obtaining threshold Jaccard index of 77.5%.




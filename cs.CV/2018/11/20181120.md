# Arxiv Papers in cs.CV on 2018-11-20
### See far with TPNET: a Tile Processor and a CNN Symbiosis
- **Arxiv ID**: http://arxiv.org/abs/1811.08032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08032v1)
- **Published**: 2018-11-20 00:26:42+00:00
- **Updated**: 2018-11-20 00:26:42+00:00
- **Authors**: Andrey Filippov, Oleg Dzhimiev
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Throughout the evolution of the neural networks more specialized cells were added to the set of basic building blocks. These cells aim to improve training convergence, increase the overall performance, and reduce the number of required labels, all while preserving the expressive power of the universal network. Inspired by the partitioning of the human visual perception system between the eyes and the cerebral cortex, we present TPNET, which offloads universal and application-specific CNN from the bulk processing of the high resolution pixel data and performs the translation-variant image correction while delegating all non-linear decision making to the network.   In this work, we explore application of TPNET to 3D perception with a narrow-baseline (0.0001-0.0025) quad stereo camera and prove that a trained network provides a disparity prediction from the 2D phase correlation output by the Tile Processor (TP) that is twice as accurate as the prediction from a carefully hand-crafted algorithm. The TP in turn reduces the dimensions of the input features of the network and provides instrument-invariant and translation-invariant data, making real-time high resolution stereo 3D perception feasible and easing the requirement to have a complete end-to-end network.



### Recurrent Iterative Gating Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.08043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08043v1)
- **Published**: 2018-11-20 02:26:48+00:00
- **Updated**: 2018-11-20 02:26:48+00:00
- **Authors**: Rezaul Karim, Md Amirul Islam, Neil D. B. Bruce
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: In this paper, we present an approach for Recurrent Iterative Gating called RIGNet. The core elements of RIGNet involve recurrent connections that control the flow of information in neural networks in a top-down manner, and different variants on the core structure are considered. The iterative nature of this mechanism allows for gating to spread in both spatial extent and feature space. This is revealed to be a powerful mechanism with broad compatibility with common existing networks. Analysis shows how gating interacts with different network characteristics, and we also show that more shallow networks with gating may be made to perform better than much deeper networks that do not include RIGNet modules.



### Learning without Memorizing
- **Arxiv ID**: http://arxiv.org/abs/1811.08051v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08051v2)
- **Published**: 2018-11-20 03:20:16+00:00
- **Updated**: 2019-04-15 15:39:30+00:00
- **Authors**: Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, Rama Chellappa
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called `Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ($L_{AD}$), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding $L_{AD}$ to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.



### Gradient-Coherent Strong Regularization for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08056v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08056v2)
- **Published**: 2018-11-20 03:41:56+00:00
- **Updated**: 2019-10-18 01:52:12+00:00
- **Authors**: Dae Hoon Park, Chiu Man Ho, Yi Chang, Huaqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization plays an important role in generalization of deep neural networks, which are often prone to overfitting with their numerous parameters. L1 and L2 regularizers are common regularization tools in machine learning with their simplicity and effectiveness. However, we observe that imposing strong L1 or L2 regularization with stochastic gradient descent on deep neural networks easily fails, which limits the generalization ability of the underlying neural networks. To understand this phenomenon, we first investigate how and why learning fails when strong regularization is imposed on deep neural networks. We then propose a novel method, gradient-coherent strong regularization, which imposes regularization only when the gradients are kept coherent in the presence of strong regularization. Experiments are performed with multiple deep architectures on three benchmark data sets for image recognition. Experimental results show that our proposed approach indeed endures strong regularization and significantly improves both accuracy and compression (up to 9.9x), which could not be achieved otherwise.



### Visual Localization Under Appearance Change: Filtering Approaches
- **Arxiv ID**: http://arxiv.org/abs/1811.08063v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08063v4)
- **Published**: 2018-11-20 04:02:12+00:00
- **Updated**: 2020-11-04 06:21:03+00:00
- **Authors**: Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Yu Liu, Shin-Fang Ch'ng, Thanh-Toan Do, Ian Reid
- **Comment**: To appear in Neural Computing and Applications-Special Issue on Best
  of DICTA 2019. Its conference version won APRS/IAPR Best paper award at DICTA
  2019
- **Journal**: None
- **Summary**: A major focus of current research on place recognition is visual localization for autonomous driving. In this scenario, as cameras will be operating continuously, it is realistic to expect videos as an input to visual localization algorithms, as opposed to the single-image querying approach used in other visual localization works. In this paper, we show that exploiting temporal continuity in the testing sequence significantly improves visual localization - qualitatively and quantitatively. Although intuitive, this idea has not been fully explored in recent works. To this end, we propose two filtering approaches to exploit the temporal smoothness of image sequences: i) filtering on discrete domain with Hidden Markov Model, and ii) filtering on continuous domain with Monte Carlo-based visual localization. Our approaches rely on local features with an encoding technique to represent an image as a single vector. The experimental results on synthetic and real datasets show that our proposed methods achieve better results than state of the art (i.e., deep learning-based pose regression approaches) for the task on visual localization under significant appearance change. Our synthetic dataset and source code are made publicly available at https://sites.google.com/view/g2d-software/home and https://github.com/dadung/Visual-Localization-Filtering.



### Reinforcement Learning of Active Vision for Manipulating Objects under Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1811.08067v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08067v2)
- **Published**: 2018-11-20 04:24:38+00:00
- **Updated**: 2019-02-16 15:10:51+00:00
- **Authors**: Ricson Cheng, Arpit Agarwal, Katerina Fragkiadaki
- **Comment**: The paper was present in Conference of Robot Learning 2018
- **Journal**: Proceedings of Machine Learning Research 87 (2018) 422--431
- **Summary**: We consider artificial agents that learn to jointly control their gripperand camera in order to reinforcement learn manipulation policies in the presenceof occlusions from distractor objects. Distractors often occlude the object of in-terest and cause it to disappear from the field of view. We propose hand/eye con-trollers that learn to move the camera to keep the object within the field of viewand visible, in coordination to manipulating it to achieve the desired goal, e.g.,pushing it to a target location. We incorporate structural biases of object-centricattention within our actor-critic architectures, which our experiments suggest tobe a key for good performance. Our results further highlight the importance ofcurriculum with regards to environment difficulty. The resulting active vision /manipulation policies outperform static camera setups for a variety of clutteredenvironments.



### Factorized Distillation: Training Holistic Person Re-identification Model by Distilling an Ensemble of Partial ReID Models
- **Arxiv ID**: http://arxiv.org/abs/1811.08073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08073v1)
- **Published**: 2018-11-20 04:50:09+00:00
- **Updated**: 2018-11-20 04:50:09+00:00
- **Authors**: Pengyuan Ren, Jianmin Li
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Person re-identification (ReID) is aimed at identifying the same person across videos captured from different cameras. In the view that networks extracting global features using ordinary network architectures are difficult to extract local features due to their weak attention mechanisms, researchers have proposed a lot of elaborately designed ReID networks, while greatly improving the accuracy, the model size and the feature extraction latency are also soaring. We argue that a relatively compact ordinary network extracting globally pooled features has the capability to extract discriminative local features and can achieve state-of-the-art precision if only the model's parameters are properly learnt. In order to reduce the difficulty in learning hard identity labels, we propose a novel knowledge distillation method: Factorized Distillation, which factorizes both feature maps and retrieval features of holistic ReID network to mimic representations of multiple partial ReID models, thus transferring the knowledge from partial ReID models to the holistic network. Experiments show that the performance of model trained with the proposed method can outperform state-of-the-art with relatively few network parameters.



### Scene Graph Generation via Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1811.08075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08075v1)
- **Published**: 2018-11-20 04:55:07+00:00
- **Updated**: 2018-11-20 04:55:07+00:00
- **Authors**: Weilin Cong, William Wang, Wang-Chien Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success object detection and segmentation models have achieved in recognizing individual objects in images, performance on cognitive tasks such as image caption, semantic image retrieval, and visual QA is far from satisfactory. To achieve better performance on these cognitive tasks, merely recognizing individual object instances is insufficient. Instead, the interactions between object instances need to be captured in order to facilitate reasoning and understanding of the visual scenes in an image. Scene graph, a graph representation of images that captures object instances and their relationships, offers a comprehensive understanding of an image. However, existing techniques on scene graph generation fail to distinguish subjects and objects in the visual scenes of images and thus do not perform well with real-world datasets where exist ambiguous object instances. In this work, we propose a novel scene graph generation model for predicting object instances and its corresponding relationships in an image. Our model, SG-CRF, learns the sequential order of subject and object in a relationship triplet, and the semantic compatibility of object instance nodes and relationship nodes in a scene graph efficiently. Experiments empirically show that SG-CRF outperforms the state-of-the-art methods, on three different datasets, i.e., CLEVR, VRD, and Visual Genome, raising the Recall@100 from 24.99% to 49.95%, from 41.92% to 50.47%, and from 54.69% to 54.77%, respectively.



### Lightweight Lipschitz Margin Training for Certified Defense against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1811.08080v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08080v1)
- **Published**: 2018-11-20 05:22:55+00:00
- **Updated**: 2018-11-20 05:22:55+00:00
- **Authors**: Hajime Ono, Tsubasa Takahashi, Kazuya Kakizaki
- **Comment**: None
- **Journal**: None
- **Summary**: How can we make machine learning provably robust against adversarial examples in a scalable way? Since certified defense methods, which ensure $\epsilon$-robust, consume huge resources, they can only achieve small degree of robustness in practice. Lipschitz margin training (LMT) is a scalable certified defense, but it can also only achieve small robustness due to over-regularization. How can we make certified defense more efficiently? We present LC-LMT, a light weight Lipschitz margin training which solves the above problem. Our method has the following properties; (a) efficient: it can achieve $\epsilon$-robustness at early epoch, and (b) robust: it has a potential to get higher robustness than LMT. In the evaluation, we demonstrate the benefits of the proposed method. LC-LMT can achieve required robustness more than 30 epoch earlier than LMT in MNIST, and shows more than 90 $\%$ accuracy against both legitimate and adversarial inputs.



### ChainGAN: A sequential approach to GANs
- **Arxiv ID**: http://arxiv.org/abs/1811.08081v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08081v2)
- **Published**: 2018-11-20 05:30:32+00:00
- **Updated**: 2018-11-22 18:56:15+00:00
- **Authors**: Safwan Hossain, Kiarash Jamali, Yuchen Li, Frank Rudzicz
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new architecture and training methodology for generative adversarial networks. Current approaches attempt to learn the transformation from a noise sample to a generated data sample in one shot. Our proposed generator architecture, called $\textit{ChainGAN}$, uses a two-step process. It first attempts to transform a noise vector into a crude sample, similar to a traditional generator. Next, a chain of networks, called $\textit{editors}$, attempt to sequentially enhance this sample. We train each of these units independently, instead of with end-to-end backpropagation on the entire chain. Our model is robust, efficient, and flexible as we can apply it to various network architectures. We provide rationale for our choices and experimentally evaluate our model, achieving competitive results on several datasets.



### Bi-Adversarial Auto-Encoder for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.08103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08103v1)
- **Published**: 2018-11-20 07:29:28+00:00
- **Updated**: 2018-11-20 07:29:28+00:00
- **Authors**: Yunlong Yu, Zhong Ji, Yanwei Pang, Jichang Guo, Zhongfei Zhang, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing generative Zero-Shot Learning (ZSL) methods only consider the unidirectional alignment from the class semantics to the visual features while ignoring the alignment from the visual features to the class semantics, which fails to construct the visual-semantic interactions well. In this paper, we propose to synthesize visual features based on an auto-encoder framework paired with bi-adversarial networks respectively for visual and semantic modalities to reinforce the visual-semantic interactions with a bi-directional alignment, which ensures the synthesized visual features to fit the real visual distribution and to be highly related to the semantics. The encoder aims at synthesizing real-like visual features while the decoder forces both the real and the synthesized visual features to be more related to the class semantics. To further capture the discriminative information of the synthesized visual features, both the real and synthesized visual features are forced to be classified into the correct classes via a classification network. Experimental results on four benchmark datasets show that the proposed approach is particularly competitive on both the traditional ZSL and the generalized ZSL tasks.



### Learning Better Features for Face Detection with Feature Fusion and Segmentation Supervision
- **Arxiv ID**: http://arxiv.org/abs/1811.08557v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08557v3)
- **Published**: 2018-11-20 07:31:03+00:00
- **Updated**: 2019-04-25 10:10:12+00:00
- **Authors**: Wanxin Tian, Zixuan Wang, Haifeng Shen, Weihong Deng, Yiping Meng, Binghui Chen, Xiubao Zhang, Yuan Zhao, Xiehe Huang
- **Comment**: 10 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1711.07246, arXiv:1712.00721 by other authors
- **Journal**: None
- **Summary**: The performance of face detectors has been largely improved with the development of convolutional neural network. However, it remains challenging for face detectors to detect tiny, occluded or blurry faces. Besides, most face detectors can't locate face's position precisely and can't achieve high Intersection-over-Union (IoU) scores. We assume that problems inside are inadequate use of supervision information and imbalance between semantics and details at all level feature maps in CNN even with Feature Pyramid Networks (FPN). In this paper, we present a novel single-shot face detection network, named DF$^2$S$^2$ (Detection with Feature Fusion and Segmentation Supervision), which introduces a more effective feature fusion pyramid and a more efficient segmentation branch on ResNet-50 to handle mentioned problems. Specifically, inspired by FPN and SENet, we apply semantic information from higher-level feature maps as contextual cues to augment low-level feature maps via a spatial and channel-wise attention style, preventing details from being covered by too much semantics and making semantics and details complement each other. We further propose a semantic segmentation branch to best utilize detection supervision information meanwhile applying attention mechanism in a self-supervised manner. The segmentation branch is supervised by weak segmentation ground-truth (no extra annotation is required) in a hierarchical manner, deprecated in the inference time so it wouldn't compromise the inference speed. We evaluate our model on WIDER FACE dataset and achieved state-of-art results.



### Pyramid Embedded Generative Adversarial Network for Automated Font Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.08106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08106v1)
- **Published**: 2018-11-20 07:37:46+00:00
- **Updated**: 2018-11-20 07:37:46+00:00
- **Authors**: Donghui Sun, Qing Zhang, Jun Yang
- **Comment**: 6 pages, 7 figures, accepted by International Conference on Pattern
  Recognition (ICPR 2018)
- **Journal**: None
- **Summary**: In this paper, we investigate the Chinese font synthesis problem and propose a Pyramid Embedded Generative Adversarial Network (PEGAN) to automatically generate Chinese character images. The PEGAN consists of one generator and one discriminator. The generator is built using one encoder-decoder structure with cascaded refinement connections and mirror skip connections. The cascaded refinement connections embed a multiscale pyramid of downsampled original input into the encoder feature maps of different layers, and multi-scale feature maps from the encoder are connected to the corresponding feature maps in the decoder to make the mirror skip connections. Through combining the generative adversarial loss, pixel-wise loss, category loss and perceptual loss, the generator and discriminator can be trained alternately to synthesize character images. In order to verify the effectiveness of our proposed PEGAN, we first build one evaluation set, in which the characters are selected according to their stroke number and frequency of use, and then use both qualitative and quantitative metrics to measure the performance of our model comparing with the baseline method. The experimental results demonstrate the effectiveness of our proposed model, it shows the potential to automatically extend small font banks into complete ones.



### Sequence-based Person Attribute Recognition with Joint CTC-Attention Model
- **Arxiv ID**: http://arxiv.org/abs/1811.08115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08115v2)
- **Published**: 2018-11-20 08:18:31+00:00
- **Updated**: 2018-11-27 07:48:28+00:00
- **Authors**: Hao Liu, Jingjing Wu, Jianguo Jiang, Meibin Qi, Bo Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Attribute recognition has become crucial because of its wide applications in many computer vision tasks, such as person re-identification. Like many object recognition problems, variations in viewpoints, illumination, and recognition at far distance, all make this task challenging. In this work, we propose a joint CTC-Attention model (JCM), which maps attribute labels into sequences to learn the semantic relationship among attributes. Besides, this network uses neural network to encode images into sequences, and employs connectionist temporal classification (CTC) loss to train the network with the aim of improving the encoding performance of the network. At the same time, it adopts the attention model to decode the sequences, which can realize aligning the sequences and better learning the semantic information from attributes. Extensive experiments on three public datasets, i.e., Market-1501 attribute dataset, Duke attribute dataset and PETA dataset, demonstrate the effectiveness of the proposed method.



### Adversarial Feedback Loop
- **Arxiv ID**: http://arxiv.org/abs/1811.08126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08126v1)
- **Published**: 2018-11-20 08:53:27+00:00
- **Updated**: 2018-11-20 08:53:27+00:00
- **Authors**: Firas Shama, Roey Mechrez, Alon Shoshan, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to their remarkable generative capabilities, GANs have gained great popularity, and are used abundantly in state-of-the-art methods and applications. In a GAN based model, a discriminator is trained to learn the real data distribution. To date, it has been used only for training purposes, where it's utilized to train the generator to provide real-looking outputs. In this paper we propose a novel method that makes an explicit use of the discriminator in test-time, in a feedback manner in order to improve the generator results. To the best of our knowledge it is the first time a discriminator is involved in test-time. We claim that the discriminator holds significant information on the real data distribution, that could be useful for test-time as well, a potential that has not been explored before.   The approach we propose does not alter the conventional training stage. At test-time, however, it transfers the output from the generator into the discriminator, and uses feedback modules (convolutional blocks) to translate the features of the discriminator layers into corrections to the features of the generator layers, which are used eventually to get a better generator result. Our method can contribute to both conditional and unconditional GANs. As demonstrated by our experiments, it can improve the results of state-of-the-art networks for super-resolution, and image generation.



### Learning to Detect Instantaneous Changes with Retrospective Convolution and Static Sample Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1811.08138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08138v1)
- **Published**: 2018-11-20 09:17:30+00:00
- **Updated**: 2018-11-20 09:17:30+00:00
- **Authors**: Chao Chen, Sheng Zhang, Cuibing Du
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Change detection has been a challenging visual task due to the dynamic nature of real-world scenes. Good performance of existing methods depends largely on prior background images or a long-term observation. These methods, however, suffer severe degradation when they are applied to detection of instantaneously occurred changes with only a few preceding frames provided. In this paper, we exploit spatio-temporal convolutional networks to address this challenge, and propose a novel retrospective convolution, which features efficient change information extraction between the current frame and frames from historical observation. To address the problem of foreground-specific over-fitting in learning-based methods, we further propose a data augmentation method, named static sample synthesis, to guide the network to focus on learning change-cued information rather than specific spatial features of foreground. Trained end-to-end with complex scenarios, our framework proves to be accurate in detecting instantaneous changes and robust in combating diverse noises. Extensive experiments demonstrate that our proposed method significantly outperforms existing methods.



### Adversarial point set registration
- **Arxiv ID**: http://arxiv.org/abs/1811.08139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08139v1)
- **Published**: 2018-11-20 09:20:31+00:00
- **Updated**: 2018-11-20 09:20:31+00:00
- **Authors**: Sergei Divakov, Ivan Oseledets
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to point set registration which is based on one-shot adversarial learning. The idea of the algorithm is inspired by recent successes of generative adversarial networks. Treating the point clouds as three-dimensional probability distributions, we develop a one-shot adversarial optimization procedure, in which we train a critic neural network to distinguish between source and target point sets, while simultaneously learning the parameters of the transformation to trick the critic into confusing the points. In contrast to most existing algorithms for point set registration, ours does not rely on any correspondences between the point clouds. We demonstrate the performance of the algorithm on several challenging benchmarks and compare it to the existing baselines.



### How You See Me
- **Arxiv ID**: http://arxiv.org/abs/1811.08152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08152v1)
- **Published**: 2018-11-20 09:44:10+00:00
- **Updated**: 2018-11-20 09:44:10+00:00
- **Authors**: Rohit Gandikota, Deepak Mishra
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution Neural Networks is one of the most powerful tools in the present era of science. There has been a lot of research done to improve their performance and robustness while their internal working was left unexplored to much extent. They are often defined as black boxes that can map non-linear data very effectively. This paper tries to show how CNN has learned to look at an image. The proposed algorithm exploits the basic math of CNN to backtrack the important pixels it is considering to predict. This is a simple algorithm which does not involve any training of its own over a pre-trained CNN which can classify.



### Weakly Supervised Estimation of Shadow Confidence Maps in Fetal Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/1811.08164v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08164v3)
- **Published**: 2018-11-20 10:20:39+00:00
- **Updated**: 2019-05-06 17:03:28+00:00
- **Authors**: Qingjie Meng, Matthew Sinclair, Veronika Zimmer, Benjamin Hou, Martin Rajchl, Nicolas Toussaint, Ozan Oktay, Jo Schlemper, Alberto Gomez, James Housden, Jacqueline Matthew, Daniel Rueckert, Julia Schnabel, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting acoustic shadows in ultrasound images is important in many clinical and engineering applications. Real-time feedback of acoustic shadows can guide sonographers to a standardized diagnostic viewing plane with minimal artifacts and can provide additional information for other automatic image analysis algorithms. However, automatically detecting shadow regions using learning-based algorithms is challenging because pixel-wise ground truth annotation of acoustic shadows is subjective and time consuming. In this paper we propose a weakly supervised method for automatic confidence estimation of acoustic shadow regions. Our method is able to generate a dense shadow-focused confidence map. In our method, a shadow-seg module is built to learn general shadow features for shadow segmentation, based on global image-level annotations as well as a small number of coarse pixel-wise shadow annotations. A transfer function is introduced to extend the obtained binary shadow segmentation to a reference confidence map. Additionally, a confidence estimation network is proposed to learn the mapping between input images and the reference confidence maps. This network is able to predict shadow confidence maps directly from input images during inference. We use evaluation metrics such as DICE, inter-class correlation and etc. to verify the effectiveness of our method. Our method is more consistent than human annotation, and outperforms the state-of-the-art quantitatively in shadow segmentation and qualitatively in confidence estimation of shadow regions. We further demonstrate the applicability of our method by integrating shadow confidence maps into tasks such as ultrasound image classification, multi-view image fusion and automated biometric measurements.



### Unsupervised Learning of Shape Concepts - From Real-World Objects to Mental Simulation
- **Arxiv ID**: http://arxiv.org/abs/1811.08165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.08165v1)
- **Published**: 2018-11-20 10:21:06+00:00
- **Updated**: 2018-11-20 10:21:06+00:00
- **Authors**: Christian A. Mueller, Andreas Birk
- **Comment**: Submitted (preprint version) to IEEE Transactions on Cognitive and
  Developmental Systems. arXiv admin note: substantial text overlap with
  arXiv:1803.02140
- **Journal**: None
- **Summary**: An unsupervised shape analysis is proposed to learn concepts reflecting shape commonalities. Our approach is two-fold: i) a spatial topology analysis of point cloud segment constellations within objects is used in which constellations are decomposed and described in a hierarchical and symbolic manner. ii) A topology analysis of the description space is used in which segment decompositions are exposed in. Inspired by Persistent Homology, groups of shape commonality are revealed. Experiments show that extracted persistent commonality groups can feature semantically meaningful shape concepts; the generalization of the proposed approach is evaluated by different real-world datasets. We extend this by not only learning shape concepts using real-world data, but by also using mental simulation of artificial abstract objects for training purposes. This extended approach is unsupervised in two respects: label-agnostic (no label information is used) and instance-agnostic (no instances preselected by human supervision are used for training). Experiments show that concepts generated with mental simulation, generalize and discriminate real object observations. Consequently, a robot may train and learn its own internal representation of concepts regarding shape appearance in a self-driven and machine-centric manner while omitting the tedious process of supervised dataset generation including the ambiguity in instance labeling and selection.



### Sketch-R2CNN: An Attentive Network for Vector Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.08170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1811.08170v1)
- **Published**: 2018-11-20 10:44:34+00:00
- **Updated**: 2018-11-20 10:44:34+00:00
- **Authors**: Lei Li, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, Chiew-Lan Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Freehand sketching is a dynamic process where points are sequentially sampled and grouped as strokes for sketch acquisition on electronic devices. To recognize a sketched object, most existing methods discard such important temporal ordering and grouping information from human and simply rasterize sketches into binary images for classification. In this paper, we propose a novel single-branch attentive network architecture RNN-Rasterization-CNN (Sketch-R2CNN for short) to fully leverage the dynamics in sketches for recognition. Sketch-R2CNN takes as input only a vector sketch with grouped sequences of points, and uses an RNN for stroke attention estimation in the vector space and a CNN for 2D feature extraction in the pixel space respectively. To bridge the gap between these two spaces in neural networks, we propose a neural line rasterization module to convert the vector sketch along with the attention estimated by RNN into a bitmap image, which is subsequently consumed by CNN. The neural line rasterization module is designed in a differentiable way to yield a unified pipeline for end-to-end learning. We perform experiments on existing large-scale sketch recognition benchmarks and show that by exploiting the sketch dynamics with the attention mechanism, our method is more robust and achieves better performance than the state-of-the-art methods.



### Convolutional Neural Networks with Transformed Input based on Robust Tensor Network Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1812.02622v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.02622v2)
- **Published**: 2018-11-20 10:57:25+00:00
- **Updated**: 2018-12-11 10:24:34+00:00
- **Authors**: Jenn-Bing Ong, Wee-Keong Ng, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor network decomposition, originated from quantum physics to model entangled many-particle quantum systems, turns out to be a promising mathematical technique to efficiently represent and process big data in parsimonious manner. In this study, we show that tensor networks can systematically partition structured data, e.g. color images, for distributed storage and communication in privacy-preserving manner. Leveraging the sea of big data and metadata privacy, empirical results show that neighbouring subtensors with implicit information stored in tensor network formats cannot be identified for data reconstruction. This technique complements the existing encryption and randomization techniques which store explicit data representation at one place and highly susceptible to adversarial attacks such as side-channel attacks and de-anonymization. Furthermore, we propose a theory for adversarial examples that mislead convolutional neural networks to misclassification using subspace analysis based on singular value decomposition (SVD). The theory is extended to analyze higher-order tensors using tensor-train SVD (TT-SVD); it helps to explain the level of susceptibility of different datasets to adversarial attacks, the structural similarity of different adversarial attacks including global and localized attacks, and the efficacy of different adversarial defenses based on input transformation. An efficient and adaptive algorithm based on robust TT-SVD is then developed to detect strong and static adversarial attacks.



### Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints
- **Arxiv ID**: http://arxiv.org/abs/1811.08180v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.CY, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08180v3)
- **Published**: 2018-11-20 11:11:21+00:00
- **Updated**: 2019-08-16 17:11:32+00:00
- **Authors**: Ning Yu, Larry Davis, Mario Fritz
- **Comment**: Accepted to ICCV'19
- **Journal**: None
- **Summary**: Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.



### Orthographic Feature Transform for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.08188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08188v1)
- **Published**: 2018-11-20 11:31:53+00:00
- **Updated**: 2018-11-20 11:31:53+00:00
- **Authors**: Thomas Roddick, Alex Kendall, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10\% of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark.\footnote{We will release full source code and pretrained models upon acceptance of this manuscript for publication.



### SpherePHD: Applying CNNs on a Spherical PolyHeDron Representation of 360 degree Images
- **Arxiv ID**: http://arxiv.org/abs/1811.08196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08196v2)
- **Published**: 2018-11-20 11:56:52+00:00
- **Updated**: 2019-04-08 10:19:11+00:00
- **Authors**: Yeonkun Lee, Jaeseok Jeong, Jongseob Yun, Wonjune Cho, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Omni-directional cameras have many advantages overconventional cameras in that they have a much wider field-of-view (FOV). Accordingly, several approaches have beenproposed recently to apply convolutional neural networks(CNNs) to omni-directional images for various visual tasks.However, most of them use image representations defined inthe Euclidean space after transforming the omni-directionalviews originally formed in the non-Euclidean space. Thistransformation leads to shape distortion due to nonuniformspatial resolving power and the loss of continuity. Theseeffects make existing convolution kernels experience diffi-culties in extracting meaningful information.This paper presents a novel method to resolve such prob-lems of applying CNNs to omni-directional images. Theproposed method utilizes a spherical polyhedron to rep-resent omni-directional views. This method minimizes thevariance of the spatial resolving power on the sphere sur-face, and includes new convolution and pooling methodsfor the proposed representation. The proposed method canalso be adopted by any existing CNN-based methods. Thefeasibility of the proposed method is demonstrated throughclassification, detection, and semantic segmentation taskswith synthetic and real datasets.



### CGNet: A Light-weight Context Guided Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.08201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08201v2)
- **Published**: 2018-11-20 12:04:50+00:00
- **Updated**: 2019-04-12 07:32:07+00:00
- **Authors**: Tianyi Wu, Sheng Tang, Rui Zhang, Yongdong Zhang
- **Comment**: Code: https://github.com/wutianyiRosun/CGNet
- **Journal**: None
- **Summary**: The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters hence unsuitable for mobile devices, while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem, we propose a novel Context Guided Network (CGNet), which is a light-weight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block, which learns the joint feature of both local feature and surrounding context, and further improves the joint feature with the global context. Based on the CG block, we develop CGNet which captures contextual information in all stages of the network and is specially tailored for increasing segmentation accuracy. CGNet is also elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically, without any post-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters. The source code for the complete system can be found at https://github.com/wutianyiRosun/CGNet.



### A Semi-supervised Spatial Spectral Regularized Manifold Local Scaling Cut With HGF for Dimensionality Reduction of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1811.08223v1
- **DOI**: 10.1109/TGRS.2018.2884771
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08223v1)
- **Published**: 2018-11-20 12:56:39+00:00
- **Updated**: 2018-11-20 12:56:39+00:00
- **Authors**: Ramanarayan Mohanty, SL Happy, Aurobinda Routray
- **Comment**: None
- **Journal**: IEEE Transaction on Geoscience and Remote Sensing, 2018
- **Summary**: Hyperspectral images (HSI) contain a wealth of information over hundreds of contiguous spectral bands, making it possible to classify materials through subtle spectral discrepancies. However, the classification of this rich spectral information is accompanied by the challenges like high dimensionality, singularity, limited training samples, lack of labeled data samples, heteroscedasticity and nonlinearity. To address these challenges, we propose a semi-supervised graph based dimensionality reduction method named `semi-supervised spatial spectral regularized manifold local scaling cut' (S3RMLSC). The underlying idea of the proposed method is to exploit the limited labeled information from both the spectral and spatial domains along with the abundant unlabeled samples to facilitate the classification task by retaining the original distribution of the data. In S3RMLSC, a hierarchical guided filter (HGF) is initially used to smoothen the pixels of the HSI data to preserve the spatial pixel consistency. This step is followed by the construction of linear patches from the nonlinear manifold by using the maximal linear patch (MLP) criterion. Then the inter-patch and intra-patch dissimilarity matrices are constructed in both spectral and spatial domains by regularized manifold local scaling cut (RMLSC) and neighboring pixel manifold local scaling cut (NPMLSC) respectively. Finally, we obtain the projection matrix by optimizing the updated semi-supervised spatial-spectral between-patch and total-patch dissimilarity. The effectiveness of the proposed DR algorithm is illustrated with publicly available real-world HSI datasets.



### Event-based High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08230v1
- **DOI**: 10.1109/CVPR.2019.01032
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08230v1)
- **Published**: 2018-11-20 13:05:22+00:00
- **Updated**: 2018-11-20 13:05:22+00:00
- **Authors**: S. Mohammad Mostafavi I., Lin Wang, Yo-Sung Ho, Kuk-Jin Yoon
- **Comment**: 10 pages,
- **Journal**: None
- **Summary**: Event cameras have a lot of advantages over traditional cameras, such as low latency, high temporal resolution, and high dynamic range. However, since the outputs of event cameras are the sequences of asynchronous events overtime rather than actual intensity images, existing algorithms could not be directly applied. Therefore, it is demanding to generate intensity images from events for other tasks. In this paper, we unlock the potential of event camera-based conditional generative adversarial networks to create images/videos from an adjustable portion of the event data stream. The stacks of space-time coordinates of events are used as inputs and the network is trained to reproduce images based on the spatio-temporal intensity changes. The usefulness of event cameras to generate high dynamic range(HDR) images even in extreme illumination conditions and also non blurred images under rapid motion is also shown.In addition, the possibility of generating very high frame rate videos is demonstrated, theoretically up to 1 million frames per second (FPS) since the temporal resolution of event cameras are about 1{\mu}s. Proposed methods are evaluated by comparing the results with the intensity images captured on the same pixel grid-line of events using online available real datasets and synthetic datasets produced by the event camera simulator.



### Transferable Interactiveness Knowledge for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.08264v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08264v4)
- **Published**: 2018-11-20 14:20:55+00:00
- **Updated**: 2019-06-11 14:08:41+00:00
- **Authors**: Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yan-Feng Wang, Cewu Lu
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) Detection is an important problem to understand how humans interact with objects. In this paper, we explore Interactiveness Knowledge which indicates whether human and object interact with each other or not. We found that interactiveness knowledge can be learned across HOI datasets, regardless of HOI category settings. Our core idea is to exploit an Interactiveness Network to learn the general interactiveness knowledge from multiple HOI datasets and perform Non-Interaction Suppression before HOI classification in inference. On account of the generalization of interactiveness, interactiveness network is a transferable knowledge learner and can be cooperated with any HOI detection models to achieve desirable results. We extensively evaluate the proposed method on HICO-DET and V-COCO datasets. Our framework outperforms state-of-the-art HOI detection results by a great margin, verifying its efficacy and flexibility. Code is available at https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network.



### Stability Based Filter Pruning for Accelerating Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.08321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08321v1)
- **Published**: 2018-11-20 15:44:36+00:00
- **Updated**: 2018-11-20 15:44:36+00:00
- **Authors**: Pravendra Singh, Vinay Sameer Raja Kadi, Nikhil Verma, Vinay P. Namboodiri
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2019
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have achieved impressive performance on the wide variety of tasks (classification, detection, etc.) across multiple domains at the cost of high computational and memory requirements. Thus, leveraging CNNs for real-time applications necessitates model compression approaches that not only reduce the total number of parameters but reduce the overall computation as well. In this work, we present a stability-based approach for filter-level pruning of CNNs. We evaluate our proposed approach on different architectures (LeNet, VGG-16, ResNet, and Faster RCNN) and datasets and demonstrate its generalizability through extensive experiments. Moreover, our compressed models can be used at run-time without requiring any special libraries or hardware. Our model compression method reduces the number of FLOPS by an impressive factor of 6.03X and GPU memory footprint by more than 17X, significantly outperforming other state-of-the-art filter pruning methods.



### Sensor Adaptation for Improved Semantic Segmentation of Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/1811.08328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08328v1)
- **Published**: 2018-11-20 15:53:38+00:00
- **Updated**: 2018-11-20 15:53:38+00:00
- **Authors**: Marc Bosch, Gordon A. Christie, Christopher M. Gifford
- **Comment**: Accepted publication at WACV 2019
- **Journal**: None
- **Summary**: Semantic segmentation is a powerful method to facilitate visual scene understanding. Each pixel is assigned a label according to a pre-defined list of object classes and semantic entities. This becomes very useful as a means to summarize large scale overhead imagery. In this paper we present our work on semantic segmentation with applications to overhead imagery. We propose an algorithm that builds and extends upon the DeepLab framework to be able to refine and resolve small objects (relative to the image size) such as vehicles. We have also investigated sensor adaptation as a means to augment available training data to be able to reduce some of the shortcomings of neural networks when deployed in new environments and to new sensors. We report results on several datasets and compare performance with other state-of-the-art architectures.



### Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector
- **Arxiv ID**: http://arxiv.org/abs/1811.08342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08342v1)
- **Published**: 2018-11-20 16:19:44+00:00
- **Updated**: 2018-11-20 16:19:44+00:00
- **Authors**: Pravendra Singh, Manikandan R, Neeraj Matiyali, Vinay P. Namboodiri
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2019
- **Journal**: None
- **Summary**: We propose a framework for compressing state-of-the-art Single Shot MultiBox Detector (SSD). The framework addresses compression in the following stages: Sparsity Induction, Filter Selection, and Filter Pruning. In the Sparsity Induction stage, the object detector model is sparsified via an improved global threshold. In Filter Selection & Pruning stage, we select and remove filters using sparsity statistics of filter weights in two consecutive convolutional layers. This results in the model with the size smaller than most existing compact architectures. We evaluate the performance of our framework with multiple datasets and compare over multiple methods. Experimental results show that our method achieves state-of-the-art compression of 6.7X and 4.9X on PASCAL VOC dataset on models SSD300 and SSD512 respectively. We further show that the method produces maximum compression of 26X with SSD512 on German Traffic Sign Detection Benchmark (GTSDB). Additionally, we also empirically show our method's adaptability for classification based architecture VGG16 on datasets CIFAR and German Traffic Sign Recognition Benchmark (GTSRB) achieving a compression rate of 125X and 200X with the reduction in flops by 90.50% and 96.6% respectively with no loss of accuracy. In addition to this, our method does not require any special libraries or hardware support for the resulting compressed models.



### LGLG-WPCA: An Effective Texture-based Method for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.08345v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08345v4)
- **Published**: 2018-11-20 16:21:20+00:00
- **Updated**: 2019-06-07 06:22:10+00:00
- **Authors**: Chaorong Li, Huang Wei, Huafu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we proposed an effective face feature extraction method by Learning Gabor Log-Euclidean Gaussian with Whitening Principal Component Analysis (WPCA), called LGLG-WPCA. The proposed method learns face features from the embedded multivariate Gaussian in Gabor wavelet domain; it has the robust performance to adverse conditions such as varying poses, skin aging and uneven illumination. Because the space of Gaussian is a Riemannian manifold and it is difficult to incorporate learning mechanism in the model. To address this issue, we use L2EMG to map the multidimensional Gaussian model to the linear space, and then use WPCA to learn face features. We also implemented the key-point-based version of LGLG-WPCA, called LGLG(KP)-WPCA. Experiments show the proposed methods are effective and promising for face texture feature extraction and the combination of the feature of the proposed methods and the features of Deep Convolutional Network (DCNN) achieved the best recognition accuracies on FERET database compared to the state-of-the-art methods. In the next version of this paper, we will test the performance of the proposed methods on the large-varying pose databases.



### Multi-Task Learning of Generalizable Representations for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.08362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08362v2)
- **Published**: 2018-11-20 16:49:17+00:00
- **Updated**: 2020-04-12 02:56:41+00:00
- **Authors**: Zhiyu Yao, Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S Yu, Jiaguang Sun
- **Comment**: ICME 2020
- **Journal**: None
- **Summary**: In classic video action recognition, labels may not contain enough information about the diverse video appearance and dynamics, thus, existing models that are trained under the standard supervised learning paradigm may extract less generalizable features. We evaluate these models under a cross-dataset experiment setting, as the above label bias problem in video analysis is even more prominent across different data sources. We find that using the optical flows as model inputs harms the generalization ability of most video recognition models.   Based on these findings, we present a multi-task learning paradigm for video classification. Our key idea is to avoid label bias and improve the generalization ability by taking data as its own supervision or supervising constraints on the data. First, we take the optical flows and the RGB frames by taking them as auxiliary supervisions, and thus naming our model as Reversed Two-Stream Networks (Rev2Net). Further, we collaborate the auxiliary flow prediction task and the frame reconstruction task by introducing a new training objective to Rev2Net, named Decoding Discrepancy Penalty (DDP), which constraints the discrepancy of the multi-task features in a self-supervised manner. Rev2Net is shown to be effective on the classic action recognition task. It specifically shows a strong generalization ability in the cross-dataset experiments.



### TSM: Temporal Shift Module for Efficient Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1811.08383v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08383v3)
- **Published**: 2018-11-20 17:39:26+00:00
- **Updated**: 2019-08-22 16:31:16+00:00
- **Authors**: Ji Lin, Chuang Gan, Song Han
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github.com/mit-han-lab/temporal-shift-module.



### Structured Pruning for Efficient ConvNets via Incremental Regularization
- **Arxiv ID**: http://arxiv.org/abs/1811.08390v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.08390v2)
- **Published**: 2018-11-20 17:52:54+00:00
- **Updated**: 2018-12-18 23:49:32+00:00
- **Authors**: Huan Wang, Qiming Zhang, Yuehai Wang, Haoji Hu
- **Comment**: Accepted by NIPS 2018 workshop on "Compact Deep Neural Network
  Representation with Industrial Applications"
- **Journal**: None
- **Summary**: Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance loss. Despite its effectiveness, existing regularization-based parameter pruning methods usually drive weights towards zero with large and constant regularization factors, which neglects the fact that the expressiveness of CNNs is fragile and needs a more gentle way of regularization for the networks to adapt during pruning. To solve this problem, we propose a new regularization-based pruning method (named IncReg) to incrementally assign different regularization factors to different weight groups based on their relative importance, whose effectiveness is proved on popular CNNs compared with state-of-the-art methods.



### Shape-only Features for Plant Leaf Identification
- **Arxiv ID**: http://arxiv.org/abs/1811.08398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08398v1)
- **Published**: 2018-11-20 18:09:06+00:00
- **Updated**: 2018-11-20 18:09:06+00:00
- **Authors**: Charlie Hewitt, Marwa Mahmoud
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel feature set for shape-only leaf identification motivated by real-world, mobile deployment. The feature set includes basic shape features, as well as signal features extracted from local area integral invariants (LAIIs), similar to curvature maps, at multiple scales. The proposed methodology is evaluated on a number of publicly available leaf datasets with comparable results to existing methods which make use of colour and texture features in addition to shape. Over 90% classification accuracy is achieved on most datasets, with top-four accuracy for these datasets reaching over 98%. Rotation and scale invariance of the proposed features are demonstrated, along with an evaluation of the generalisability of the approach for generic shape matching.



### Single-Label Multi-Class Image Classification by Deep Logistic Regression
- **Arxiv ID**: http://arxiv.org/abs/1811.08400v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08400v2)
- **Published**: 2018-11-20 18:19:36+00:00
- **Updated**: 2019-05-14 11:29:40+00:00
- **Authors**: Qi Dong, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted by AAAI-19, code at
  https://github.com/qd301/FocusRectificationLogisticRegression
- **Journal**: The Thirty-Third AAAI Conference on Artificial Intelligence
  (AAAI), 2019
- **Summary**: The objective learning formulation is essential for the success of convolutional neural networks. In this work, we analyse thoroughly the standard learning objective functions for multi-class classification CNNs: softmax regression (SR) for single-label scenario and logistic regression (LR) for multi-label scenario. Our analyses lead to an inspiration of exploiting LR for single-label classification learning, and then the disclosing of the negative class distraction problem in LR. To address this problem, we develop two novel LR based objective functions that not only generalise the conventional LR but importantly turn out to be competitive alternatives to SR in single label classification. Extensive comparative evaluations demonstrate the model learning advantages of the proposed LR functions over the commonly adopted SR in single-label coarse-grained object categorisation and cross-class fine-grained person instance identification tasks. We also show the performance superiority of our method on clothing attribute classification in comparison to the vanilla LR function.



### Deep Convolutional Neural Network for Plant Seedlings Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.08404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08404v1)
- **Published**: 2018-11-20 18:22:54+00:00
- **Updated**: 2018-11-20 18:22:54+00:00
- **Authors**: Daniel K. Nkemelu, Daniel Omeiza, Nancy Lubalo
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Agriculture is vital for human survival and remains a major driver of several economies around the world; more so in underdeveloped and developing economies. With increasing demand for food and cash crops, due to a growing global population and the challenges posed by climate change, there is a pressing need to increase farm outputs while incurring minimal costs. Previous machine vision technologies developed for selective weeding have faced the challenge of reliable and accurate weed detection. We present approaches for plant seedlings classification with a dataset that contains 4,275 images of approximately 960 unique plants belonging to 12 species at several growth stages. We compare the performances of two traditional algorithms and a Convolutional Neural Network (CNN), a deep learning technique widely applied to image recognition, for this task. Our findings show that CNN-driven seedling classification applications when used in farming automation has the potential to optimize crop yield and improve productivity and efficiency when designed appropriately.



### A Baseline for Multi-Label Image Classification Using An Ensemble of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.08412v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1811.08412v3)
- **Published**: 2018-11-20 18:34:22+00:00
- **Updated**: 2019-05-09 10:06:24+00:00
- **Authors**: Qian Wang, Ning Jia, Toby P. Breckon
- **Comment**: IEEE International Conference on Image Processing 2019
- **Journal**: None
- **Summary**: Recent studies on multi-label image classification have focused on designing more complex architectures of deep neural networks such as the use of attention mechanisms and region proposal networks. Although performance gains have been reported, the backbone deep models of the proposed approaches and the evaluation metrics employed in different works vary, making it difficult to compare each fairly. Moreover, due to the lack of properly investigated baselines, the advantage introduced by the proposed techniques are often ambiguous. To address these issues, we make a thorough investigation of the mainstream deep convolutional neural network architectures for multi-label image classification and present a strong baseline. With the use of proper data augmentation techniques and model ensembles, the basic deep architectures can achieve better performance than many existing more complex ones on three benchmark datasets, providing great insight for the future studies on multi-label image classification.



### Visual SLAM-based Localization and Navigation for Service Robots: The Pepper Case
- **Arxiv ID**: http://arxiv.org/abs/1811.08414v1
- **DOI**: 10.1007/978-3-030-27544-0_3
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.08414v1)
- **Published**: 2018-11-20 18:41:35+00:00
- **Updated**: 2018-11-20 18:41:35+00:00
- **Authors**: Cristopher Gmez, Matas Mattamala, Tim Resink, Javier Ruiz-del-Solar
- **Comment**: 12 pages, 6 figures. Presented in RoboCup Symposium 2018. Final
  version will appear in Springer
- **Journal**: None
- **Summary**: We propose a Visual-SLAM based localization and navigation system for service robots. Our system is built on top of the ORB-SLAM monocular system but extended by the inclusion of wheel odometry in the estimation procedures. As a case study, the proposed system is validated using the Pepper robot, whose short-range LIDARs and RGB-D camera do not allow the robot to self-localize in large environments. The localization system is tested in navigation tasks using Pepper in two different environments: a medium-size laboratory, and a large-size hall.



### Artificial Color Constancy via GoogLeNet with Angular Loss Function
- **Arxiv ID**: http://arxiv.org/abs/1811.08456v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08456v2)
- **Published**: 2018-11-20 19:34:18+00:00
- **Updated**: 2019-12-04 07:07:52+00:00
- **Authors**: Oleksii Sidorov
- **Comment**: None
- **Journal**: International Journal of Imaging and Robotics, 19(3):1-10, 2019
- **Summary**: Color Constancy is the ability of the human visual system to perceive colors unchanged independently of the illumination. Giving a machine this feature will be beneficial in many fields where chromatic information is used. Particularly, it significantly improves scene understanding and object recognition. In this paper, we propose transfer learning-based algorithm, which has two main features: accuracy higher than many state-of-the-art algorithms and simplicity of implementation. Despite the fact that GoogLeNet was used in the experiments, given approach may be applied to any CNN. Additionally, we discuss design of a new loss function oriented specifically to this problem, and propose a few the most suitable options.



### Intermediate Level Adversarial Attack for Enhanced Transferability
- **Arxiv ID**: http://arxiv.org/abs/1811.08458v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08458v1)
- **Published**: 2018-11-20 19:40:24+00:00
- **Updated**: 2018-11-20 19:40:24+00:00
- **Authors**: Qian Huang, Zeqi Gu, Isay Katsman, Horace He, Pian Pawakapan, Zhiqiu Lin, Serge Belongie, Ser-Nam Lim
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples may be overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. This leads us to introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model. We show that our method can effectively achieve this goal and that we can decide a nearly-optimal layer of the source model to perturb without any knowledge of the target models.



### Double Refinement Network for Efficient Indoor Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.08466v2
- **DOI**: 10.1109/IROS40897.2019.8968227
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08466v2)
- **Published**: 2018-11-20 19:56:10+00:00
- **Updated**: 2019-04-04 17:17:16+00:00
- **Authors**: Nikita Durasov, Mikhail Romanov, Valeriya Bubnova, Pavel Bogomolov, Anton Konushin
- **Comment**: None
- **Journal**: 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Summary**: Monocular depth estimation is the task of obtaining a measure of distance for each pixel using a single image. It is an important problem in computer vision and is usually solved using neural networks. Though recent works in this area have shown significant improvement in accuracy, the state-of-the-art methods tend to require massive amounts of memory and time to process an image. The main purpose of this work is to improve the performance of the latest solutions with no decrease in accuracy. To this end, we introduce the Double Refinement Network architecture. The proposed method achieves state-of-the-art results on the standard benchmark RGB-D dataset NYU Depth v2, while its frames per second rate is significantly higher (up to 18 times speedup per image at batch size 1) and the RAM usage per image is lower.



### VQA with no questions-answers training
- **Arxiv ID**: http://arxiv.org/abs/1811.08481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08481v2)
- **Published**: 2018-11-20 20:52:46+00:00
- **Updated**: 2020-05-26 09:53:47+00:00
- **Authors**: Ben-Zion Vatashsky, Shimon Ullman
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Methods for teaching machines to answer visual questions have made significant progress in recent years, but current methods still lack important human capabilities, including integrating new visual classes and concepts in a modular manner, providing explanations for the answers and handling new domains without explicit examples. We propose a novel method that consists of two main parts: generating a question graph representation, and an answering procedure, guided by the abstract structure of the question graph to invoke an extendable set of visual estimators. Training is performed for the language part and the visual part on their own, but unlike existing schemes, the method does not require any training using images with associated questions and answers. This approach is able to handle novel domains (extended question types and new object classes, properties and relations) as long as corresponding visual estimators are available. In addition, it can provide explanations to its answers and suggest alternatives when questions are not grounded in the image. We demonstrate that this approach achieves both high performance and domain extensibility without any questions-answers training.



### MimicGAN: Corruption-Mimicking for Blind Image Recovery & Adversarial Defense
- **Arxiv ID**: http://arxiv.org/abs/1811.08484v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.08484v1)
- **Published**: 2018-11-20 20:59:38+00:00
- **Updated**: 2018-11-20 20:59:38+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo Bremer
- **Comment**: None
- **Journal**: None
- **Summary**: Solving inverse problems continues to be a central challenge in computer vision. Existing techniques either explicitly construct an inverse mapping using prior knowledge about the corruption, or learn the inverse directly using a large collection of examples. However, in practice, the nature of corruption may be unknown, and thus it is challenging to regularize the problem of inferring a plausible solution. On the other hand, collecting task-specific training data is tedious for known corruptions and impossible for unknown ones. We present MimicGAN, an unsupervised technique to solve general inverse problems based on image priors in the form of generative adversarial networks (GANs). Using a GAN prior, we show that one can reliably recover solutions to underdetermined inverse problems through a surrogate network that learns to mimic the corruption at test time. Our system successively estimates the corruption and the clean image without the need for supervisory training, while outperforming existing baselines in blind image recovery. We also demonstrate that MimicGAN improves upon recent GAN-based defenses against adversarial attacks and represents one of the strongest test-time defenses available today.



### Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1811.08489v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08489v4)
- **Published**: 2018-11-20 21:11:53+00:00
- **Updated**: 2019-10-11 00:18:49+00:00
- **Authors**: Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, Vicente Ordonez
- **Comment**: 10 pages, 7 figures, ICCV 2019
- **Journal**: None
- **Summary**: In this work, we present a framework to measure and mitigate intrinsic biases with respect to protected variables --such as gender-- in visual recognition tasks. We show that trained models significantly amplify the association of target labels with gender beyond what one would expect from biased datasets. Surprisingly, we show that even when datasets are balanced such that each label co-occurs equally with each gender, learned models amplify the association between labels and gender, as much as if data had not been balanced! To mitigate this, we adopt an adversarial approach to remove unwanted features corresponding to protected variables from intermediate representations in a deep neural network -- and provide a detailed analysis of its effectiveness. Experiments on two datasets: the COCO dataset (objects), and the imSitu dataset (actions), show reductions in gender bias amplification while maintaining most of the accuracy of the original models.



### Are pre-trained CNNs good feature extractors for anomaly detection in surveillance videos?
- **Arxiv ID**: http://arxiv.org/abs/1811.08495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08495v1)
- **Published**: 2018-11-20 21:29:58+00:00
- **Updated**: 2018-11-20 21:29:58+00:00
- **Authors**: Tiago S. Nazare, Rodrigo F. de Mello, Moacir A. Ponti
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several techniques have been explored to detect unusual behaviour in surveillance videos. Nevertheless, few studies leverage features from pre-trained CNNs and none of then present a comparison of features generate by different models. Motivated by this gap, we compare features extracted by four state-of-the-art image classification networks as a way of describing patches from security video frames. We carry out experiments on the Ped1 and Ped2 datasets and analyze the usage of different feature normalization techniques. Our results indicate that choosing the appropriate normalization is crucial to improve the anomaly detection performance when working with CNN features. Also, in the Ped2 dataset our approach was able to obtain results comparable to the ones of several state-of-the-art methods. Lastly, as our method only considers the appearance of each frame, we believe that it can be combined with approaches that focus on motion patterns to further improve performance.



### A Proposal-Based Solution to Spatio-Temporal Action Detection in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.08496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.08496v2)
- **Published**: 2018-11-20 21:35:07+00:00
- **Updated**: 2018-11-23 03:06:34+00:00
- **Authors**: Joshua Gleason, Rajeev Ranjan, Steven Schwarcz, Carlos D. Castillo, Jun-Chen Cheng, Rama Chellappa
- **Comment**: To appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV) 2019
- **Journal**: None
- **Summary**: Existing approaches for spatio-temporal action detection in videos are limited by the spatial extent and temporal duration of the actions. In this paper, we present a modular system for spatio-temporal action detection in untrimmed security videos. We propose a two stage approach. The first stage generates dense spatio-temporal proposals using hierarchical clustering and temporal jittering techniques on frame-wise object detections. The second stage is a Temporal Refinement I3D (TRI-3D) network that performs action classification and temporal refinement on the generated proposals. The object detection-based proposal generation step helps in detecting actions occurring in a small spatial region of a video frame, while temporal jittering and refinement helps in detecting actions of variable lengths. Experimental results on the spatio-temporal action detection dataset - DIVA - show the effectiveness of our system. For comparison, the performance of our system is also evaluated on the THUMOS14 temporal action detection dataset.



### Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides
- **Arxiv ID**: http://arxiv.org/abs/1811.08513v2
- **DOI**: 10.1001/jamanetworkopen.2019.14645
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.08513v2)
- **Published**: 2018-11-20 22:19:44+00:00
- **Updated**: 2019-12-03 20:41:23+00:00
- **Authors**: Naofumi Tomita, Behnaz Abdollahi, Jason Wei, Bing Ren, Arief Suriawinata, Saeed Hassanpour
- **Comment**: Accepted for publication at the Journal of JAMA Network Open
- **Journal**: JAMA Netw Open. 2019;2(11):e1914645
- **Summary**: Deep learning-based methods, such as the sliding window approach for cropped-image classification and heuristic aggregation for whole-slide inference, for analyzing histological patterns in high-resolution microscopy images have shown promising results. These approaches, however, require a laborious annotation process and are fragmented. This diagnostic study collected deidentified high-resolution histological images (N = 379) for training a new model composed of a convolutional neural network and a grid-based attention network, trainable without region-of-interest annotations. Histological images of patients who underwent endoscopic esophagus and gastroesophageal junction mucosal biopsy between January 1, 2016, and December 31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were collected. The method achieved a mean accuracy of 0.83 in classifying 123 test images. These results were comparable with or better than the performance from the current state-of-the-art sliding window approach, which was trained with regions of interest. Results of this study suggest that the proposed attention-based deep neural network framework for Barrett esophagus and esophageal adenocarcinoma detection is important because it is based solely on tissue-level annotations, unlike existing methods that are based on regions of interest. This new model is expected to open avenues for applying deep learning to digital pathology.




# Arxiv Papers in cs.CV on 2018-11-28
### Deep Regionlets: Blended Representation and Deep Learning for Generic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.11318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11318v2)
- **Published**: 2018-11-28 00:13:00+00:00
- **Updated**: 2019-12-02 05:37:18+00:00
- **Authors**: Hongyu Xu, Xutao Lv, Xiaoyu Wang, Zhou Ren, Navaneeth Bodla, Rama Chellappa
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1712.02408
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2019
- **Summary**: In this paper, we propose a novel object detection algorithm named "Deep Regionlets" by integrating deep neural networks and a conventional detection schema for accurate generic object detection. Motivated by the effectiveness of regionlets for modeling object deformations and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network provides guidance on where to select sub-regions from which features can be learned from. An object proposal typically contains 3-16 sub-regions. The regionlet learning module focuses on local feature selection and transformations to alleviate the effects of appearance variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a "gating network" within the regionlet leaning module to enable instance dependent soft feature selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We present ablation studies and extensive experiments on the PASCAL VOC dataset and the Microsoft COCO dataset. The proposed method yields competitive performance over state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.



### Image Labeling with Markov Random Fields and Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1811.11323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11323v2)
- **Published**: 2018-11-28 00:36:58+00:00
- **Updated**: 2019-05-19 20:20:43+00:00
- **Authors**: Shangxuan Wu, Xinshuo Weng
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing methods for object segmentation in computer vision are formulated as a labeling task. This, in general, could be transferred to a pixel-wise label assignment task, which is quite similar to the structure of hidden Markov random field. In terms of Markov random field, each pixel can be regarded as a state and has a transition probability to its neighbor pixel, the label behind each pixel is a latent variable and has an emission probability from its corresponding state. In this paper, we reviewed several modern image labeling methods based on Markov random field and conditional random Field. And we compare the result of these methods with some classical image labeling methods. The experiment demonstrates that the introduction of Markov random field and conditional random field make a big difference in the segmentation result.



### CyLKs: Unsupervised Cycle Lucas-Kanade Network for Landmark Tracking
- **Arxiv ID**: http://arxiv.org/abs/1811.11325v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11325v4)
- **Published**: 2018-11-28 00:41:29+00:00
- **Updated**: 2019-05-21 01:50:54+00:00
- **Authors**: Xinshuo Weng, Wentao Han
- **Comment**: None
- **Journal**: None
- **Summary**: Across a majority of modern learning-based tracking systems, expensive annotations are needed to achieve state-of-the-art performance. In contrast, the Lucas-Kanade (LK) algorithm works well without any annotation. However, LK has a strong assumption of photometric (brightness) consistency on image intensity and is easy to drift because of large motion, occlusion, and aperture problem. To relax the assumption and alleviate the drift problem, we propose CyLKs, a data-driven way of training Lucas-Kanade in an unsupervised manner. CyLKs learns a feature transformation through CNNs, transforming the input images to a feature space which is especially favorable to LK tracking. During training, we perform differentiable Lucas-Kanade forward and backward on the convolutional feature maps, and then minimize the re-projection error. During testing, we perform the LK tracking on the learned features. We apply our model to the task of landmark tracking and perform experiments on datasets of THUMOS and 300VW.



### Deep Reinforcement Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1811.11329v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.11329v3)
- **Published**: 2018-11-28 00:56:57+00:00
- **Updated**: 2019-05-19 20:51:26+00:00
- **Authors**: Sen Wang, Daoyuan Jia, Xinshuo Weng
- **Comment**: no time for further improvement
- **Journal**: None
- **Summary**: Reinforcement learning has steadily improved and outperform human in lots of traditional games since the resurgence of deep neural network. However, these success is not easy to be copied to autonomous driving because the state spaces in real world are extreme complex and action spaces are continuous and fine control is required. Moreover, the autonomous driving vehicles must also keep functional safety under the complex environments. To deal with these challenges, we first adopt the deep deterministic policy gradient (DDPG) algorithm, which has the capacity to handle complex state and action spaces in continuous domain. We then choose The Open Racing Car Simulator (TORCS) as our environment to avoid physical damage. Meanwhile, we select a set of appropriate sensor information from TORCS and design our own rewarder. In order to fit DDPG algorithm to TORCS, we design our network architecture for both actor and critic inside DDPG paradigm. To demonstrate the effectiveness of our model, We evaluate on different modes in TORCS and show both quantitative and qualitative results.



### Robust neural circuit reconstruction from serial electron microscopy with convolutional recurrent networks
- **Arxiv ID**: http://arxiv.org/abs/1811.11356v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11356v4)
- **Published**: 2018-11-28 02:26:33+00:00
- **Updated**: 2020-01-16 22:04:45+00:00
- **Authors**: Drew Linsley, Junkyung Kim, David Berson, Thomas Serre
- **Comment**: None
- **Journal**: None
- **Summary**: Recent successes in deep learning have started to impact neuroscience. Of particular significance are claims that current segmentation algorithms achieve "super-human" accuracy in an area known as connectomics. However, as we will show, these algorithms do not effectively generalize beyond the particular source and brain tissues used for training -- severely limiting their usability by the broader neuroscience community. To fill this gap, we describe a novel connectomics challenge for source- and tissue-agnostic reconstruction of neurons (STAR), which favors broad generalization over fitting specific datasets. We first demonstrate that current state-of-the-art approaches to neuron segmentation perform poorly on the challenge. We further describe a novel convolutional recurrent neural network module that combines short-range horizontal connections within a processing stage and long-range top-down connections between stages. The resulting architecture establishes the state of the art on the STAR challenge and represents a significant step towards widely usable and fully-automated connectomics analysis.



### Future Segmentation Using 3D Structure
- **Arxiv ID**: http://arxiv.org/abs/1811.11358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11358v1)
- **Published**: 2018-11-28 02:31:13+00:00
- **Updated**: 2018-11-28 02:31:13+00:00
- **Authors**: Suhani Vora, Reza Mahjourian, Soeren Pirk, Anelia Angelova
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future to anticipate the outcome of events and actions is a critical attribute of autonomous agents; particularly for agents which must rely heavily on real time visual data for decision making. Working towards this capability, we address the task of predicting future frame segmentation from a stream of monocular video by leveraging the 3D structure of the scene. Our framework is based on learnable sub-modules capable of predicting pixel-wise scene semantic labels, depth, and camera ego-motion of adjacent frames. We further propose a recurrent neural network based model capable of predicting future ego-motion trajectory as a function of a series of past ego-motion steps. Ultimately, we observe that leveraging 3D structure in the model facilitates successful prediction, achieving state of the art accuracy in future semantic segmentation.



### Unsupervised Multi-modal Neural Machine Translation
- **Arxiv ID**: http://arxiv.org/abs/1811.11365v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1811.11365v2)
- **Published**: 2018-11-28 02:48:25+00:00
- **Updated**: 2019-05-27 02:00:51+00:00
- **Authors**: Yuanhang Su, Kai Fan, Nguyen Bach, C. -C. Jay Kuo, Fei Huang
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Unsupervised neural machine translation (UNMT) has recently achieved remarkable results with only large monolingual corpora in each language. However, the uncertainty of associating target with source sentences makes UNMT theoretically an ill-posed problem. This work investigates the possibility of utilizing images for disambiguation to improve the performance of UNMT. Our assumption is intuitively based on the invariant property of image, i.e., the description of the same visual content by different languages should be approximately similar. We propose an unsupervised multi-modal machine translation (UMNMT) framework based on the language translation cycle consistency loss conditional on the image, targeting to learn the bidirectional multi-modal translation simultaneously. Through an alternate training between multi-modal and uni-modal, our inference model can translate with or without the image. On the widely used Multi30K dataset, the experimental results of our approach are significantly better than those of the text-only UNMT on the 2016 test dataset.



### Formal Verification of CNN-based Perception Systems
- **Arxiv ID**: http://arxiv.org/abs/1811.11373v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11373v1)
- **Published**: 2018-11-28 03:36:25+00:00
- **Updated**: 2018-11-28 03:36:25+00:00
- **Authors**: Panagiotis Kouvaros, Alessio Lomuscio
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of verifying neural-based perception systems implemented by convolutional neural networks. We define a notion of local robustness based on affine and photometric transformations. We show the notion cannot be captured by previously employed notions of robustness. The method proposed is based on reachability analysis for feed-forward neural networks and relies on MILP encodings of both the CNNs and transformations under question. We present an implementation and discuss the experimental results obtained for a CNN trained from the MNIST data set.



### Instance-level Sketch-based Retrieval by Deep Triplet Classification Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/1811.11375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11375v2)
- **Published**: 2018-11-28 03:52:18+00:00
- **Updated**: 2019-05-25 11:05:17+00:00
- **Authors**: Peng Lu, Hangyu Lin, Yanwei Fu, Shaogang Gong, Yu-Gang Jiang, Xiangyang Xue
- **Comment**: we will significantly change the content of this paper which makes it
  another paper. In order not to misleading, we decide to withdraw it now
- **Journal**: None
- **Summary**: Sketch has been employed as an effective communicative tool to express the abstract and intuitive meanings of object. Recognizing the free-hand sketch drawing is extremely useful in many real-world applications. While content-based sketch recognition has been studied for several decades, the instance-level Sketch-Based Image Retrieval (SBIR) tasks have attracted significant research attention recently. The existing datasets such as QMUL-Chair and QMUL-Shoe, focus on the retrieval tasks of chairs and shoes. However, there are several key limitations in previous instance-level SBIR works. The state-of-the-art works have to heavily rely on the pre-training process, quality of edge maps, multi-cropping testing strategy, and augmenting sketch images. To efficiently solve the instance-level SBIR, we propose a new Deep Triplet Classification Siamese Network (DeepTCNet) which employs DenseNet-169 as the basic feature extractor and is optimized by the triplet loss and classification loss. Critically, our proposed DeepTCNet can break the limitations existed in previous works. The extensive experiments on five benchmark sketch datasets validate the effectiveness of the proposed model. Additionally, to study the tasks of sketch-based hairstyle retrieval, this paper contributes a new instance-level photo-sketch dataset - Hairstyle Photo-Sketch dataset, which is composed of 3600 sketches and photos, and 2400 sketch-photo pairs.



### Self-Supervised Spatiotemporal Feature Learning via Video Rotation Prediction
- **Arxiv ID**: http://arxiv.org/abs/1811.11387v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11387v2)
- **Published**: 2018-11-28 05:04:34+00:00
- **Updated**: 2019-04-04 02:51:59+00:00
- **Authors**: Longlong Jing, Xiaodong Yang, Jingen Liu, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep neural networks generally requires a vast amount of training data to be labeled, which is expensive and unfeasible in scale, especially for video collections. To alleviate this problem, in this paper, we propose 3DRotNet: a fully self-supervised approach to learn spatiotemporal features from unlabeled videos. A set of rotations are applied to all videos, and a pretext task is defined as prediction of these rotations. When accomplishing this task, 3DRotNet is actually trained to understand the semantic concepts and motions in videos. In other words, it learns a spatiotemporal video representation, which can be transferred to improve video understanding tasks in small datasets. Our extensive experiments successfully demonstrate the effectiveness of the proposed framework on action recognition, leading to significant improvements over the state-of-the-art self-supervised methods. With the self-supervised pre-trained 3DRotNet from large datasets, the recognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51 respectively, compared to the models trained from scratch.



### Image Generation from Layout
- **Arxiv ID**: http://arxiv.org/abs/1811.11389v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11389v3)
- **Published**: 2018-11-28 05:11:14+00:00
- **Updated**: 2019-10-14 21:17:06+00:00
- **Authors**: Bo Zhao, Lili Meng, Weidong Yin, Leonid Sigal
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Despite significant recent progress on generative models, controlled generation of images depicting multiple and complex object layouts is still a difficult problem. Among the core challenges are the diversity of appearance a given object may possess and, as a result, exponential set of images consistent with a specified layout. To address these challenges, we propose a novel approach for layout-based image generation; we call it Layout2Im. Given the coarse spatial layout (bounding boxes + object categories), our model can generate a set of realistic images which have the correct objects in the desired locations. The representation of each object is disentangled into a specified/certain part (category) and an unspecified/uncertain part (appearance). The category is encoded using a word embedding and the appearance is distilled into a low-dimensional vector sampled from a normal distribution. Individual object representations are composed together using convolutional LSTM, to obtain an encoding of the complete layout, and then decoded to an image. Several loss terms are introduced to encourage accurate and diverse generation. The proposed Layout2Im model significantly outperforms the previous state of the art, boosting the best reported inception score by 24.66% and 28.57% on the very challenging COCO-Stuff and Visual Genome datasets, respectively. Extensive experiments also demonstrate our method's ability to generate complex and diverse images with multiple objects.



### General-to-Detailed GAN for Infrequent Class Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1812.01690v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01690v1)
- **Published**: 2018-11-28 05:40:15+00:00
- **Updated**: 2018-11-28 05:40:15+00:00
- **Authors**: Tatsuki Koga, Naoki Nonaka, Jun Sakuma, Jun Seita
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Deep learning has significant potential for medical imaging. However, since the incident rate of each disease varies widely, the frequency of classes in a medical image dataset is imbalanced, leading to poor accuracy for such infrequent classes. One possible solution is data augmentation of infrequent classes using synthesized images created by Generative Adversarial Networks (GANs), but conventional GANs also require certain amount of images to learn. To overcome this limitation, here we propose General-to-detailed GAN (GDGAN), serially connected two GANs, one for general labels and the other for detailed labels. GDGAN produced diverse medical images, and the network trained with an augmented dataset outperformed other networks using existing methods with respect to Area-Under-Curve (AUC) of Receiver Operating Characteristic (ROC) curve.



### DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1811.11397v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1811.11397v2)
- **Published**: 2018-11-28 05:51:38+00:00
- **Updated**: 2019-04-09 04:01:20+00:00
- **Authors**: Li Ding, Chen Feng
- **Comment**: This paper has been accepted for CVPR'19 oral presentation
- **Journal**: None
- **Summary**: We propose DeepMapping, a novel registration framework using deep neural networks (DNNs) as auxiliary functions to align multiple point clouds from scratch to a globally consistent frame. We use DNNs to model the highly non-convex mapping process that traditionally involves hand-crafted data association, sensor pose initialization, and global refinement. Our key novelty is that "training" these DNNs with properly defined unsupervised losses is equivalent to solving the underlying registration problem, but less sensitive to good initialization than ICP. Our framework contains two DNNs: a localization network that estimates the poses for input point clouds, and a map network that models the scene structure by estimating the occupancy status of global coordinates. This allows us to convert the registration problem to a binary occupancy classification, which can be solved efficiently using gradient-based optimization. We further show that DeepMapping can be readily extended to address the problem of Lidar SLAM by imposing geometric constraints between consecutive point clouds. Experiments are conducted on both simulated and real datasets. Qualitative and quantitative comparisons demonstrate that DeepMapping often enables more robust and accurate global registration of multiple point clouds than existing techniques. Our code is available at https://ai4ce.github.io/DeepMapping/.



### Spectral Feature Transformation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1811.11405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11405v1)
- **Published**: 2018-11-28 06:46:38+00:00
- **Updated**: 2018-11-28 06:46:38+00:00
- **Authors**: Chuanchen Luo, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: With the surge of deep learning techniques, the field of person re-identification has witnessed rapid progress in recent years. Deep learning based methods focus on learning a feature space where samples are clustered compactly according to their corresponding identities. Most existing methods rely on powerful CNNs to transform the samples individually. In contrast, we propose to consider the sample relations in the transformation. To achieve this goal, we incorporate spectral clustering technique into CNN. We derive a novel module named Spectral Feature Transformation and seamlessly integrate it into existing CNN pipeline with negligible cost,which makes our method enjoy the best of two worlds. Empirical studies show that the proposed approach outperforms previous state-of-the-art methods on four public benchmarks by a considerable margin without bells and whistles.



### MeshNet: Mesh Neural Network for 3D Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/1811.11424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11424v1)
- **Published**: 2018-11-28 07:48:51+00:00
- **Updated**: 2018-11-28 07:48:51+00:00
- **Authors**: Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, Yue Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named MeshNet, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, MeshNet is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed MeshNet method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed MeshNet can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.



### ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.11431v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11431v3)
- **Published**: 2018-11-28 08:01:09+00:00
- **Updated**: 2019-03-30 04:34:47+00:00
- **Authors**: Sachin Mehta, Mohammad Rastegari, Linda Shapiro, Hannaneh Hajishirzi
- **Comment**: Accepted at CVPR'19
- **Journal**: None
- **Summary**: We introduce a light-weight, power efficient, and general purpose convolutional neural network, ESPNetv2, for modeling visual and sequential data. Our network uses group point-wise and depth-wise dilated separable convolutions to learn representations from a large effective receptive field with fewer FLOPs and parameters. The performance of our network is evaluated on four different tasks: (1) object classification, (2) semantic segmentation, (3) object detection, and (4) language modeling. Experiments on these tasks, including image classification on the ImageNet and language modeling on the PenTree bank dataset, demonstrate the superior performance of our method over the state-of-the-art methods. Our network outperforms ESPNet by 4-5% and has 2-4x fewer FLOPs on the PASCAL VOC and the Cityscapes dataset. Compared to YOLOv2 on the MS-COCO object detection, ESPNetv2 delivers 4.4% higher accuracy with 6x fewer FLOPs. Our experiments show that ESPNetv2 is much more power efficient than existing state-of-the-art efficient methods including ShuffleNets and MobileNets. Our code is open-source and available at https://github.com/sacmehta/ESPNetv2



### Neural Sign Language Translation based on Human Keypoint Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.11436v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11436v2)
- **Published**: 2018-11-28 08:28:54+00:00
- **Updated**: 2019-06-21 06:27:34+00:00
- **Authors**: Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, Choongsang Cho
- **Comment**: 18 pages, 5 figures
- **Journal**: None
- **Summary**: We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive amount of dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI (short for Korea Electronics Technology Institute) sign language dataset which consists of 14,672 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting line for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from a face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieves 93.28% (55.28%, respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compare several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.



### CrowdCam: Dynamic Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.11455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11455v2)
- **Published**: 2018-11-28 09:19:42+00:00
- **Updated**: 2019-06-23 19:15:29+00:00
- **Authors**: Nir Zarrabi, Shai Avidan, Yael Moses
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of segmenting dynamic regions in CrowdCam images, where a dynamic region is the projection of a moving 3D object on the image plane. Quite often, these regions are the most interesting parts of an image. CrowdCam images is a set of images of the same dynamic event, captured by a group of non-collaborating users. Almost every event of interest today is captured this way. This new type of images raises the need to develop new algorithms tailored specifically for it. We propose a comprehensive solution to the problem. Our solution combines cues that are based on geometry, appearance and proximity. First, geometric reasoning is used to produce rough score maps that determine, for every pixel, how likely it is to be the projection of a static or dynamic scene point. These maps are noisy because CrowdCam images are usually few and far apart both in space and in time. Then, we use similarity in appearance space and proximity in the image plane to encourage neighboring pixels to be labeled similarly as either static or dynamic. We collected a new, and challenging, data set to evaluate our algorithm. Results show that the success score of our algorithm is nearly double that of the current state of the art approach.



### Coordinate-based Texture Inpainting for Pose-Guided Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1811.11459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11459v2)
- **Published**: 2018-11-28 09:33:00+00:00
- **Updated**: 2019-07-19 08:50:27+00:00
- **Authors**: Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, Victor Lempitsky
- **Comment**: Published in Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR). 2019
- **Journal**: None
- **Summary**: We present a new deep learning approach to pose-guided resynthesis of human photographs. At the heart of the new approach is the estimation of the complete body surface texture based on a single photograph. Since the input photograph always observes only a part of the surface, we suggest a new inpainting method that completes the texture of the human body. Rather than working directly with colors of texture elements, the inpainting network estimates an appropriate source location in the input image for each element of the body surface. This correspondence field between the input image and the texture is then further warped into the target image coordinate frame based on the desired pose, effectively establishing the correspondence between the source and the target view even when the pose change is drastic. The final convolutional network then uses the established correspondence and all other available information to synthesize the output image. A fully-convolutional architecture with deformable skip connections guided by the estimated correspondence field is used. We show state-of-the-art result for pose-guided image synthesis. Additionally, we demonstrate the performance of our system for garment transfer and pose-guided face resynthesis.



### Isospectralization, or how to hear shape, style, and correspondence
- **Arxiv ID**: http://arxiv.org/abs/1811.11465v2
- **DOI**: 10.1109/CVPR.2019.00771
- **Categories**: **cs.CG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1811.11465v2)
- **Published**: 2018-11-28 09:51:28+00:00
- **Updated**: 2019-04-05 11:17:46+00:00
- **Authors**: Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael M. Bronstein, Emanuele Rodol√†
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: The question whether one can recover the shape of a geometric object from its Laplacian spectrum ('hear the shape of the drum') is a classical problem in spectral geometry with a broad range of implications and applications. While theoretically the answer to this question is negative (there exist examples of iso-spectral but non-isometric manifolds), little is known about the practical possibility of using the spectrum for shape reconstruction and optimization. In this paper, we introduce a numerical procedure called isospectralization, consisting of deforming one shape to make its Laplacian spectrum match that of another. We implement the isospectralization procedure using modern differentiable programming techniques and exemplify its applications in some of the classical and notoriously hard problems in geometry processing, computer vision, and graphics such as shape reconstruction, pose and style transfer, and dense deformable correspondence.



### Image Reconstruction with Predictive Filter Flow
- **Arxiv ID**: http://arxiv.org/abs/1811.11482v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11482v1)
- **Published**: 2018-11-28 10:17:14+00:00
- **Updated**: 2018-11-28 10:17:14+00:00
- **Authors**: Shu Kong, Charless Fowlkes
- **Comment**: https://www.ics.uci.edu/~skong2/pff.html
- **Journal**: None
- **Summary**: We propose a simple, interpretable framework for solving a wide range of image reconstruction problems such as denoising and deconvolution. Given a corrupted input image, the model synthesizes a spatially varying linear filter which, when applied to the input image, reconstructs the desired output. The model parameters are learned using supervised or self-supervised training. We test this model on three tasks: non-uniform motion blur removal, lossy-compression artifact reduction and single image super resolution. We demonstrate that our model substantially outperforms state-of-the-art methods on all these tasks and is significantly faster than optimization-based approaches to deconvolution. Unlike models that directly predict output pixel values, the predicted filter flow is controllable and interpretable, which we demonstrate by visualizing the space of predicted filters for different tasks.



### Fixed-length Bit-string Representation of Fingerprint by Normalized Local Structures
- **Arxiv ID**: http://arxiv.org/abs/1811.11489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11489v1)
- **Published**: 2018-11-28 10:54:30+00:00
- **Updated**: 2018-11-28 10:54:30+00:00
- **Authors**: Jun Beom Kho, Andrew B. J. Teoh, Wonjune Lee, Jaihie Kim
- **Comment**: 16 pages, 15 figures, 10 tables
- **Journal**: None
- **Summary**: In this paper, we propose a method to represent a fingerprint image by an ordered, fixed-length bit-string providing improved accuracy performance, faster matching time and compressibility. First, we devise a novel minutia-based local structure modeled by a mixture of 2D elliptical Gaussian functions in the pixel space. Each local structure is mapped to the Euclidean space by normalizing the local structure with the number of minutiae that associates to it. This simple yet crucial crux enables fast dissimilarity computation of two local structures with Euclidean distance without distortion. A complementary texture-based local structure to the minutia-based local structure is also introduced whereby both can be compressed via principal component analysis and fused easily in the Euclidean space. The fused local structure is then converted to a K-bit ordered string via a K-means clustering algorithm. This chain of computation with sole use of Euclidean distance is vital for speedy and discriminative bit-string conversion. The accuracy can be further improved by a finger-specific bit-training algorithm in which two criteria are leveraged to select useful bit positions for matching. Experiments are performed on Fingerprint Verification Competition (FVC) databases for comparison with existing techniques to show the superiority of the proposed method.



### One-Shot Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.11507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11507v2)
- **Published**: 2018-11-28 11:36:21+00:00
- **Updated**: 2019-05-28 08:48:08+00:00
- **Authors**: Claudio Michaelis, Ivan Ustyuzhaninov, Matthias Bethge, Alexander S. Ecker
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of one-shot instance segmentation: Given an example image of a novel, previously unknown object category, find and segment all objects of this category within a complex scene. To address this challenging new task, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We demonstrate empirical results on MS Coco highlighting challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories works very well, targeting the detection network towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research into more powerful and flexible scene analysis algorithms. Code is available at: https://github.com/bethgelab/siamese-mask-rcnn



### Identity Preserving Generative Adversarial Network for Cross-Domain Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1811.11510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11510v1)
- **Published**: 2018-11-28 11:52:34+00:00
- **Updated**: 2018-11-28 11:52:34+00:00
- **Authors**: Jialun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is to retrieval pedestrian images from no-overlap camera views detected by pedestrian detectors. Most existing person re-identification (re-ID) models often fail to generalize well from the source domain where the models are trained to a new target domain without labels, because of the bias between the source and target domain. This issue significantly limits the scalability and usability of the models in the real world. Providing a labeled source training set and an unlabeled target training set, the aim of this paper is to improve the generalization ability of re-ID models to the target domain. To this end, we propose an image generative network named identity preserving generative adversarial network (IPGAN). The proposed method has two excellent properties: 1) only a single model is employed to translate the labeled images from the source domain to the target camera domains in an unsupervised manner; 2) The identity information of images from the source domain is preserved before and after translation. Furthermore, we propose IBN-reID model for the person re-identification task. It has better generalization ability than baseline models, especially in the cases without any domain adaptation. The IBN-reID model is trained on the translated images by supervised methods. Experimental results on Market-1501 and DukeMTMC-reID show that the images generated by IPGAN are more suitable for cross-domain person re-identification. Very competitive re-ID accuracy is achieved by our method.



### Multi-granularity Generator for Temporal Action Proposal
- **Arxiv ID**: http://arxiv.org/abs/1811.11524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11524v2)
- **Published**: 2018-11-28 12:47:16+00:00
- **Updated**: 2019-04-12 06:06:42+00:00
- **Authors**: Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, Shih-Fu Chang
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Temporal action proposal generation is an important task, aiming to localize the video segments containing human actions in an untrimmed video. In this paper, we propose a multi-granularity generator (MGG) to perform the temporal action proposal from different granularity perspectives, relying on the video visual features equipped with the position embedding information. First, we propose to use a bilinear matching model to exploit the rich local information within the video sequence. Afterwards, two components, namely segment proposal producer (SPP) and frame actionness producer (FAP), are combined to perform the task of temporal action proposal at two distinct granularities. SPP considers the whole video in the form of feature pyramid and generates segment proposals from one coarse perspective, while FAP carries out a finer actionness evaluation for each video frame. Our proposed MGG can be trained in an end-to-end fashion. By temporally adjusting the segment proposals with fine-grained frame actionness information, MGG achieves the superior performance over state-of-the-art methods on the public THUMOS-14 and ActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to perform the classification of the proposals generated by MGG, leading to significant improvements compared against the competing methods for the video detection task.



### Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects
- **Arxiv ID**: http://arxiv.org/abs/1811.11553v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11553v3)
- **Published**: 2018-11-28 13:39:27+00:00
- **Updated**: 2019-04-18 13:54:20+00:00
- **Authors**: Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, Anh Nguyen
- **Comment**: Poster at the 2019 Conference on Computer Vision and Pattern
  Recognition
- **Journal**: None
- **Summary**: Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9% and 99.4% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5% transfer to the YOLOv3 object detector trained on MS COCO.



### Automatic Liver Segmentation with Adversarial Loss and Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1811.11566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11566v1)
- **Published**: 2018-11-28 13:57:36+00:00
- **Updated**: 2018-11-28 13:57:36+00:00
- **Authors**: Bora Baydar, Savas Ozkan, Gozde Bozdagi Akar
- **Comment**: Technical Report(In Turkish)
- **Journal**: None
- **Summary**: Automatic segmentation of medical images is among most demanded works in the medical information field since it saves time of the experts in the field and avoids human error factors. In this work, a method based on Conditional Adversarial Networks and Fully Convolutional Networks is proposed for the automatic segmentation of the liver MRIs. The proposed method, without any post-processing, is achieved the second place in the SIU Liver Segmentation Challenge 2018, data of which is provided by Dokuz Eyl\"ul University. In this paper, some improvements for the post-processing step are also proposed and it is shown that with these additions, the method outperforms other baseline methods.



### Exploring Hypergraph Representation on Face Anti-spoofing Beyond 2D Attacks
- **Arxiv ID**: http://arxiv.org/abs/1811.11594v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11594v2)
- **Published**: 2018-11-28 14:36:55+00:00
- **Updated**: 2018-12-13 12:09:07+00:00
- **Authors**: Wei Hu, Gusi Te, Ju He, Dong Chen, Zongming Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing plays a crucial role in protecting face recognition systems from various attacks. Previous model-based and deep learning approaches achieve satisfactory performance for 2D face spoofs, but remain limited for more advanced 3D attacks such as vivid masks. In this paper, we address 3D face anti-spoofing via the proposed Hypergraph Convolutional Neural Networks (HGCNN). Firstly, we construct a computation-efficient and posture-invariant face representation with only a few key points on hypergraphs. The hypergraph representation is then fed into the designed HGCNN with hypergraph convolution for feature extraction, while the depth auxiliary is also exploited for 3D mask anti-spoofing. Further, we build a 3D face attack database with color, depth and infrared light information to overcome the deficiency of 3D face anti-spoofing data. Experiments show that our method achieves the state-of-the-art performance over widely used 3D and 2D databases as well as the proposed one under various tests.



### Escaping Plato's Cave: 3D Shape From Adversarial Rendering
- **Arxiv ID**: http://arxiv.org/abs/1811.11606v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1811.11606v4)
- **Published**: 2018-11-28 14:58:22+00:00
- **Updated**: 2021-06-10 09:17:27+00:00
- **Authors**: Philipp Henzler, Niloy Mitra, Tobias Ritschel
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PlatonicGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PlatonicGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.



### Cluster-Based Learning from Weakly Labeled Bags in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1812.00884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.00884v1)
- **Published**: 2018-11-28 15:05:22+00:00
- **Updated**: 2018-11-28 15:05:22+00:00
- **Authors**: Shazia Akbar, Anne L. Martel
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
- **Journal**: None
- **Summary**: To alleviate the burden of gathering detailed expert annotations when training deep neural networks, we propose a weakly supervised learning approach to recognize metastases in microscopic images of breast lymph nodes. We describe an alternative training loss which clusters weakly labeled bags in latent space to inform relevance of patch-instances during training of a convolutional neural network. We evaluate our method on the Camelyon dataset which contains high-resolution digital slides of breast lymph nodes, where labels are provided at the image-level and only subsets of patches are made available during training.



### A Generative Appearance Model for End-to-end Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.11611v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11611v2)
- **Published**: 2018-11-28 15:11:43+00:00
- **Updated**: 2018-12-07 13:50:46+00:00
- **Authors**: Joakim Johnander, Martin Danelljan, Emil Brissman, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: One of the fundamental challenges in video object segmentation is to find an effective representation of the target and background appearance. The best performing approaches resort to extensive fine-tuning of a convolutional neural network for this purpose. Besides being prohibitively expensive, this strategy cannot be truly trained end-to-end since the online fine-tuning procedure is not integrated into the offline training of the network.   To address these issues, we propose a network architecture that learns a powerful representation of the target and background appearance in a single forward pass. The introduced appearance module learns a probabilistic generative model of target and background feature distributions. Given a new image, it predicts the posterior class probabilities, providing a highly discriminative cue, which is processed in later network modules. Both the learning and prediction stages of our appearance module are fully differentiable, enabling true end-to-end training of the entire segmentation pipeline. Comprehensive experiments demonstrate the effectiveness of the proposed approach on three video object segmentation benchmarks. We close the gap to approaches based on online fine-tuning on DAVIS17, while operating at 15 FPS on a single GPU. Furthermore, our method outperforms all published approaches on the large-scale YouTube-VOS dataset.



### Large Scale Audio-Visual Video Analytics Platform for Forensic Investigations of Terroristic Attacks
- **Arxiv ID**: http://arxiv.org/abs/1811.11623v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11623v1)
- **Published**: 2018-11-28 15:22:03+00:00
- **Updated**: 2018-11-28 15:22:03+00:00
- **Authors**: Alexander Schindler, Martin Boyer, Andrew Lindley, David Schreiber, Thomas Philipp
- **Comment**: None
- **Journal**: 25th International Conference on MultiMedia Modeling (MMM2019)
- **Summary**: The forensic investigation of a terrorist attack poses a huge challenge to the investigative authorities, as several thousand hours of video footage need to be spotted. To assist law enforcement agencies (LEA) in identifying suspects and securing evidences, we present a platform which fuses information of surveillance cameras and video uploads from eyewitnesses. The platform integrates analytical modules for different input-modalities on a scalable architecture. Videos are analyzed according their acoustic and visual content. Specifically, Audio Event Detection is applied to index the content according to attack-specific acoustic concepts. Audio similarity search is utilized to identify similar video sequences recorded from different perspectives. Visual object detection and tracking are used to index the content according to relevant concepts. The heterogeneous results of the analytical modules are fused into a distributed index of visual and acoustic concepts to facilitate rapid start of investigations, following traits and investigating witness reports.



### WaveletNet: Logarithmic Scale Efficient Convolutional Neural Networks for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/1811.11644v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11644v1)
- **Published**: 2018-11-28 16:04:30+00:00
- **Updated**: 2018-11-28 16:04:30+00:00
- **Authors**: Li Jing, Rumen Dangovski, Marin Soljacic
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: We present a logarithmic-scale efficient convolutional neural network architecture for edge devices, named WaveletNet. Our model is based on the well-known depthwise convolution, and on two new layers, which we introduce in this work: a wavelet convolution and a depthwise fast wavelet transform. By breaking the symmetry in channel dimensions and applying a fast algorithm, WaveletNet shrinks the complexity of convolutional blocks by an O(logD/D) factor, where D is the number of channels. Experiments on CIFAR-10 and ImageNet classification show superior and comparable performances of WaveletNet compared to state-of-the-art models such as MobileNetV2.



### Robust Face Detection via Learning Small Faces on Hard Images
- **Arxiv ID**: http://arxiv.org/abs/1811.11662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11662v1)
- **Published**: 2018-11-28 16:43:06+00:00
- **Updated**: 2018-11-28 16:43:06+00:00
- **Authors**: Zhishuai Zhang, Wei Shen, Siyuan Qiao, Yan Wang, Bo Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Recent anchor-based deep face detectors have achieved promising performance, but they are still struggling to detect hard faces, such as small, blurred and partially occluded faces. A reason is that they treat all images and faces equally, without putting more effort on hard ones; however, many training images only contain easy faces, which are less helpful to achieve better performance on hard images. In this paper, we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images. Our intuitions are (1) hard images are the images which contain at least one hard face, thus they facilitate training robust face detectors; (2) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking. We build an anchor-based deep face detector, which only output a single feature map with small anchors, to specifically learn small faces and train it by a novel hard image mining strategy. Extensive experiments have been conducted on WIDER FACE, FDDB, Pascal Faces, and AFW datasets to show the effectiveness of our method. Our method achieves APs of 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset respectively, which surpass the previous state-of-the-arts, especially on the hard subset. Code and model are available at https://github.com/bairdzhang/smallhardface.



### A Graph-CNN for 3D Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.01711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.01711v1)
- **Published**: 2018-11-28 17:00:34+00:00
- **Updated**: 2018-11-28 17:00:34+00:00
- **Authors**: Yingxue Zhang, Michael Rabbat
- **Comment**: Published as a conference paper at ICASSP 2018
- **Journal**: None
- **Summary**: Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to handle data that is supported on a graph. Major challenges when working with data on graphs are that the support set (the vertices of the graph) do not typically have a natural ordering, and in general, the topology of the graph is not regular (i.e., vertices do not all have the same number of neighbors). Thus, Graph-CNNs have huge potential to deal with 3D point cloud data which has been obtained from sampling a manifold. In this paper, we develop a Graph-CNN for classifying 3D point cloud data, called PointGCN. The architecture combines localized graph convolutions with two types of graph downsampling operations (also known as pooling). By the effective exploration of the point cloud local structure using the Graph-CNN, the proposed architecture achieves competitive performance on the 3D object classification benchmark ModelNet, and our architecture is more stable than competing schemes.



### Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/1811.11683v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11683v2)
- **Published**: 2018-11-28 17:05:27+00:00
- **Updated**: 2019-05-29 20:49:53+00:00
- **Authors**: Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, Shih-Fu Chang
- **Comment**: Accepted in CVPR 2019
- **Journal**: None
- **Summary**: We address the problem of phrase grounding by lear ing a multi-level common semantic space shared by the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolutional Neural Network, as well as contextualized word and sentence embeddings extracted from a character-based language model. Following dedicated non-linear mappings for visual features at each level, word, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity. We guide the model by a multi-level multimodal attention mechanism which outputs attended visual features at each level. The best level is chosen to be compared with text content for maximizing the pertinence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly available datasets show significant performance gains (20%-60% relative) over the state-of-the-art in phrase localization and set a new performance record on those datasets. We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub.



### Partial Convolution based Padding
- **Arxiv ID**: http://arxiv.org/abs/1811.11718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11718v1)
- **Published**: 2018-11-28 18:13:02+00:00
- **Updated**: 2018-11-28 18:13:02+00:00
- **Authors**: Guilin Liu, Kevin J. Shih, Ting-Chun Wang, Fitsum A. Reda, Karan Sapra, Zhiding Yu, Andrew Tao, Bryan Catanzaro
- **Comment**: 11 pages; code is available at https://github.com/NVIDIA/partialconv
- **Journal**: None
- **Summary**: In this paper, we present a simple yet effective padding scheme that can be used as a drop-in module for existing convolutional neural networks. We call it partial convolution based padding, with the intuition that the padded region can be treated as holes and the original input as non-holes. Specifically, during the convolution operation, the convolution results are re-weighted near image borders based on the ratios between the padded area and the convolution sliding window area. Extensive experiments with various deep network models on ImageNet classification and semantic segmentation demonstrate that the proposed padding scheme consistently outperforms standard zero padding with better accuracy.



### CCNet: Criss-Cross Attention for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.11721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11721v2)
- **Published**: 2018-11-28 18:18:27+00:00
- **Updated**: 2020-07-09 12:17:28+00:00
- **Authors**: Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, Thomas S. Huang
- **Comment**: IEEE TPAMI 2020 & ICCV 2019
- **Journal**: None
- **Summary**: Contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a Criss-Cross Network (CCNet) for obtaining full-image contextual information in a very effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9%, 45.76% and 55.47% on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at \url{https://github.com/speedinghzl/CCNet}.



### Future-State Predicting LSTM for Early Surgery Type Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.11727v2
- **DOI**: 10.1109/TMI.2019.2931158
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11727v2)
- **Published**: 2018-11-28 18:26:24+00:00
- **Updated**: 2019-09-05 03:00:53+00:00
- **Authors**: Siddharth Kannan, Gaurav Yengera, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: This work presents a novel approach for the early recognition of the type of a laparoscopic surgery from its video. Early recognition algorithms can be beneficial to the development of 'smart' OR systems that can provide automatic context-aware assistance, and also enable quick database indexing. The task is however ridden with challenges specific to videos belonging to the domain of laparoscopy, such as high visual similarity across surgeries and large variations in video durations. To capture the spatio-temporal dependencies in these videos, we choose as our model a combination of a Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) network. We then propose two complementary approaches for improving early recognition performance. The first approach is a CNN fine-tuning method that encourages surgeries to be distinguished based on the initial frames of laparoscopic videos. The second approach, referred to as 'Future-State Predicting LSTM', trains an LSTM to predict information related to future frames, which helps in distinguishing between the different types of surgeries. We evaluate our approaches on a large dataset of 425 laparoscopic videos containing 9 types of surgeries (Laparo425), and achieve on average an accuracy of 75% having observed only the first 10 minutes of a surgery. These results are quite promising from a practical standpoint and also encouraging for other types of image-guided surgeries.



### SegET: Deep Neural Network with Rich Contextual Features for Cellular Structures Segmentation in Electron Tomography Image
- **Arxiv ID**: http://arxiv.org/abs/1811.11729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11729v1)
- **Published**: 2018-11-28 18:30:37+00:00
- **Updated**: 2018-11-28 18:30:37+00:00
- **Authors**: Enze Zhang, Fa Zhang, Zhiyong Liu, Xiaohua Wan, Lifa Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Electron tomography (ET) allows high-resolution reconstructions of macromolecular complexes at nearnative state. Cellular structures segmentation in the reconstruction data from electron tomographic images is often required for analyzing and visualizing biological structures, making it a powerful tool for quantitative descriptions of whole cell structures and understanding biological functions. However, these cellular structures are rather difficult to automatically separate or quantify from view owing to complex molecular environment and the limitations of reconstruction data of ET. In this paper, we propose a single end-to-end deep fully-convolutional semantic segmentation network dubbed SegET with rich contextual features which fully exploitsthe multi-scale and multi-level contextual information and reduces the loss of details of cellular structures in ET images. We trained and evaluated our network on the electron tomogram of the CTL Immunological Synapse from Cell Image library. Our results demonstrate that SegET can automatically segment accurately and outperform all other baseline methods on each individual structure in our ET dataset.



### CAPNet: Continuous Approximation Projection For 3D Point Cloud Reconstruction Using 2D Supervision
- **Arxiv ID**: http://arxiv.org/abs/1811.11731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11731v1)
- **Published**: 2018-11-28 18:31:46+00:00
- **Updated**: 2018-11-28 18:31:46+00:00
- **Authors**: Navaneet K L, Priyanka Mandikal, Mayank Agarwal, R. Venkatesh Babu
- **Comment**: Accepted at AAAI-2019; Codes are available at
  https://github.com/val-iisc/capnet
- **Journal**: None
- **Summary**: Knowledge of 3D properties of objects is a necessity in order to build effective computer vision systems. However, lack of large scale 3D datasets can be a major constraint for data-driven approaches in learning such properties. We consider the task of single image 3D point cloud reconstruction, and aim to utilize multiple foreground masks as our supervisory data to alleviate the need for large scale 3D datasets. A novel differentiable projection module, called 'CAPNet', is introduced to obtain such 2D masks from a predicted 3D point cloud. The key idea is to model the projections as a continuous approximation of the points in the point cloud. To overcome the challenges of sparse projection maps, we propose a loss formulation termed 'affinity loss' to generate outlier-free reconstructions. We significantly outperform the existing projection based approaches on a large-scale synthetic dataset. We show the utility and generalizability of such a 2D supervised approach through experiments on a real-world dataset, where lack of 3D data can be a serious concern. To further enhance the reconstructions, we also propose a test stage optimization procedure to obtain reconstructions that display high correspondence with the observed input image.



### 3D human pose estimation in video with temporal convolutions and semi-supervised training
- **Arxiv ID**: http://arxiv.org/abs/1811.11742v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11742v2)
- **Published**: 2018-11-28 18:56:36+00:00
- **Updated**: 2019-03-29 13:36:46+00:00
- **Authors**: Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D



### Formulating Camera-Adaptive Color Constancy as a Few-shot Meta-Learning Problem
- **Arxiv ID**: http://arxiv.org/abs/1811.11788v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.11788v2)
- **Published**: 2018-11-28 19:16:41+00:00
- **Updated**: 2019-04-03 19:08:48+00:00
- **Authors**: Steven McDonagh, Sarah Parisot, Fengwei Zhou, Xing Zhang, Ales Leonardis, Zhenguo Li, Gregory Slabaugh
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Digital camera pipelines employ color constancy methods to estimate an unknown scene illuminant, in order to re-illuminate images as if they were acquired under an achromatic light source. Fully-supervised learning approaches exhibit state-of-the-art estimation accuracy with camera-specific labelled training imagery. Resulting models typically suffer from domain gaps and fail to generalise across imaging devices. In this work, we propose a new approach that affords fast adaptation to previously unseen cameras, and robustness to changes in capture device by leveraging annotated samples across different cameras and datasets. We present a general approach that utilizes the concept of color temperature to frame color constancy as a set of distinct, homogeneous few-shot regression tasks, each associated with an intuitive physical meaning. We integrate this novel formulation within a meta-learning framework, enabling fast generalisation to previously unseen cameras using only handfuls of camera specific training samples. Consequently, the time spent for data collection and annotation substantially diminishes in practice whenever a new sensor is used. To quantify this gain, we evaluate our pipeline on three publicly available datasets comprising 12 different cameras and diverse scene content. Our approach delivers competitive results both qualitatively and quantitatively while requiring a small fraction of the camera-specific samples compared to standard approaches.



### Cartoon-to-real: An Approach to Translate Cartoon to Realistic Images using GAN
- **Arxiv ID**: http://arxiv.org/abs/1811.11796v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11796v3)
- **Published**: 2018-11-28 19:40:32+00:00
- **Updated**: 2019-03-22 17:03:09+00:00
- **Authors**: K M Arefeen Sultan, Labiba Kanij Rupty, Nahidul Islam Pranto, Sayed Khan Shuvo, Mohammad Imrul Jubair
- **Comment**: This is an ongoing work and this draft contains the future plan to
  accomplish the tasks
- **Journal**: None
- **Summary**: We propose a method to translate cartoon images to real world images using Generative Aderserial Network (GAN). Existing GAN-based image-to-image translation methods which are trained on paired datasets are impractical as the data is difficult to accumulate. Therefore, in this paper we exploit the Cycle-Consistent Adversarial Networks (CycleGAN) method for images translation which needs an unpaired dataset. By applying CycleGAN we show that our model is able to generate meaningful real world images from cartoon images. However, we implement another state of the art technique $-$ Deep Analogy $-$ to compare the performance of our approach.



### Phase Collaborative Network for Two-Phase Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.11814v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11814v3)
- **Published**: 2018-11-28 20:26:36+00:00
- **Updated**: 2019-09-12 15:12:44+00:00
- **Authors**: Huangjie Zheng, Lingxi Xie, Tianwei Ni, Ya Zhang, Yan-Feng Wang, Qi Tian, Elliot K. Fishman, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In real-world practice, medical images acquired in different phases possess complementary information, {\em e.g.}, radiologists often refer to both arterial and venous scans in order to make the diagnosis. However, in medical image analysis, fusing prediction from two phases is often difficult, because (i) there is a domain gap between two phases, and (ii) the semantic labels are not pixel-wise corresponded even for images scanned from the same patient. This paper studies organ segmentation in two-phase CT scans. We propose Phase Collaborative Network (PCN), an end-to-end framework that contains both generative and discriminative modules. PCN can be mathematically explained to formulate phase-to-phase and data-to-label relations jointly. Experiments are performed on a two-phase CT dataset, on which PCN outperforms the baselines working with one-phase data by a large margin, and we empirically verify that the gain comes from inter-phase collaboration. Besides, PCN transfers well to two public single-phase datasets, demonstrating its potential applications.



### Unrepresentative video data: A review and evaluation
- **Arxiv ID**: http://arxiv.org/abs/1811.11815v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11815v2)
- **Published**: 2018-11-28 20:28:11+00:00
- **Updated**: 2019-01-06 21:14:00+00:00
- **Authors**: Georgios Mastorakis
- **Comment**: 35 pages, 14 figures. Submitted to Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: It is well known that the quality and quantity of training data are significant factors which affect the development and performance of machine intelligence algorithms. Without representative data, neither scientists nor algorithms would be able to accurately capture the visual details of objects, actions or scenes. An evaluation methodology which filters data quality does not yet exist, and currently, the validation of the data depends solely on human factor. This study reviews several public datasets and discusses their limitations and issues regarding quality, feasibility, adaptation and availability of training data. A simple approach to evaluate (i.e. automatically "clean" samples) training data is proposed with the use of real events recorded on the YouTube platform. This study focuses on action recognition data and particularly on human fall detection datasets. However, the limitations described in this paper apply in virtually all datasets.



### 2D/3D Megavoltage Image Registration Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.11816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11816v1)
- **Published**: 2018-11-28 20:30:27+00:00
- **Updated**: 2018-11-28 20:30:27+00:00
- **Authors**: Hector N. B. Pinheiro, Tsang Ing Ren, Stefan Scheib, Armel Rosselet, Stefan Thieme-Marti
- **Comment**: None
- **Journal**: None
- **Summary**: We presented a 2D/3D MV image registration method based on a Convolutional Neural Network. Most of the traditional image registration method intensity-based, which use optimization algorithms to maximize the similarity between to images. Although these methods can achieve good results for kilovoltage images, the same does not occur for megavoltage images due to the lower image quality. Also, these methods most of the times do not present a good capture range. To deal with this problem, we propose the use of Convolutional Neural Network. The experiments were performed using a dataset of 50 brain images. The results showed to be promising compared to traditional image registration methods.



### Unsupervised Meta-Learning For Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.11819v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.11819v2)
- **Published**: 2018-11-28 20:38:59+00:00
- **Updated**: 2019-11-07 00:01:57+00:00
- **Authors**: Siavash Khodadadeh, Ladislau B√∂l√∂ni, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot or one-shot learning of classifiers requires a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning for classification tasks. The meta-learning step of UMTRA is performed on a flat collection of unlabeled images. While we assume that these images can be grouped into a diverse set of classes and are relevant to the target task, no explicit information about the classes or any labels are needed. UMTRA uses random sampling and augmentation to create synthetic training tasks for meta-learning phase. Labels are only needed at the final target task learning step, and they can be as little as one sample per class. On the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations, while alternating for the best performance with the recent CACTUs algorithm. Compared to supervised model-agnostic meta-learning approaches, UMTRA trades off some classification accuracy for a reduction in the required labels of several orders of magnitude.



### Semantic Part Detection via Matching: Learning to Generalize to Novel Viewpoints from Limited Training Data
- **Arxiv ID**: http://arxiv.org/abs/1811.11823v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11823v4)
- **Published**: 2018-11-28 20:48:18+00:00
- **Updated**: 2019-09-13 03:54:50+00:00
- **Authors**: Yutong Bai, Qing Liu, Lingxi Xie, Weichao Qiu, Yan Zheng, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting semantic parts of an object is a challenging task in computer vision, particularly because it is hard to construct large annotated datasets due to the difficulty of annotating semantic parts. In this paper we present an approach which learns from a small training dataset of annotated semantic parts, where the object is seen from a limited range of viewpoints, but generalizes to detect semantic parts from a much larger range of viewpoints. Our approach is based on a matching algorithm for finding accurate spatial correspondence between two images, which enables semantic parts annotated on one image to be transplanted to another. In particular, this enables images in the training dataset to be matched to a virtual 3D model of the object (for simplicity, we assume that the object viewpoint can be estimated by standard techniques). Then a clustering algorithm is used to annotate the semantic parts of the 3D virtual model. This virtual 3D model can be used to synthesize annotated images from a large range of viewpoint. These can be matched to images in the test set, using the same matching algorithm, to detect semantic parts in novel viewpoints of the object. Our algorithm is very simple, intuitive, and contains very few parameters. We evaluate our approach in the car subclass of the VehicleSemanticPart dataset. We show it outperforms standard deep network approaches and, in particular, performs much better on novel viewpoints. For facilitating the future research, code is available: https://github.com/ytongbai/SemanticPartDetection



### Towards Task Understanding in Visual Settings
- **Arxiv ID**: http://arxiv.org/abs/1811.11833v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11833v1)
- **Published**: 2018-11-28 21:06:27+00:00
- **Updated**: 2018-11-28 21:06:27+00:00
- **Authors**: Sebastin Santy, Wazeer Zulfikar, Rishabh Mehrotra, Emine Yilmaz
- **Comment**: Accepted as Student Abstract at 33rd AAAI Conference on Artificial
  Intelligence, 2019
- **Journal**: None
- **Summary**: We consider the problem of understanding real world tasks depicted in visual images. While most existing image captioning methods excel in producing natural language descriptions of visual scenes involving human tasks, there is often the need for an understanding of the exact task being undertaken rather than a literal description of the scene. We leverage insights from real world task understanding systems, and propose a framework composed of convolutional neural networks, and an external hierarchical task ontology to produce task descriptions from input images. Detailed experiments highlight the efficacy of the extracted descriptions, which could potentially find their way in many applications, including image alt text generation.



### Deep learning based automatic segmentation of lumbosacral nerves on non-contrast CT for radiographic evaluation: a pilot study
- **Arxiv ID**: http://arxiv.org/abs/1811.11843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11843v1)
- **Published**: 2018-11-28 21:24:56+00:00
- **Updated**: 2018-11-28 21:24:56+00:00
- **Authors**: Guoxin Fan, Huaqing Liu, Zhenhua Wu, Yufeng Li, Chaobo Feng, Dongdong Wang, Jie Luo, Xiaofei Guan, William M. Wells III, Shisheng He
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Background and objective: Combined evaluation of lumbosacral structures (e.g. nerves, bone) on multimodal radiographic images is routinely conducted prior to spinal surgery and interventional procedures. Generally, magnetic resonance imaging is conducted to differentiate nerves, while computed tomography (CT) is used to observe bony structures. The aim of this study is to investigate the feasibility of automatically segmenting lumbosacral structures (e.g. nerves & bone) on non-contrast CT with deep learning. Methods: a total of 50 cases with spinal CT were manually labeled for lumbosacral nerves and bone with Slicer 4.8. The ratio of training: validation: testing is 32:8:10. A 3D-Unet is adopted to build the model SPINECT for automatically segmenting lumbosacral structures. Pixel accuracy, IoU, and Dice score are used to assess the segmentation performance of lumbosacral structures. Results: the testing results reveals successful segmentation of lumbosacral bone and nerve on CT. The average pixel accuracy is 0.940 for bone and 0.918 for nerve. The average IoU is 0.897 for bone and 0.827 for nerve. The dice score is 0.945 for bone and 0.905 for nerve. Conclusions: this pilot study indicated that automatic segmenting lumbosacral structures (nerves and bone) on non-contrast CT is feasible and may have utility for planning and navigating spinal interventions and surgery.



### Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos
- **Arxiv ID**: http://arxiv.org/abs/1811.11849v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11849v4)
- **Published**: 2018-11-28 21:35:23+00:00
- **Updated**: 2022-03-23 05:41:56+00:00
- **Authors**: Kha Gia Quach, Ngan Le, Chi Nhan Duong, Ibsa Jalata, Kaushik Roy, Khoa Luu
- **Comment**: In press at Patter Recognition Journal
- **Journal**: None
- **Summary**: Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes are becoming an interest in both the security arena as well as social media. This work extends the earlier ER investigations, which focused on either group-level ER on single images or within a video, by fully investigating group-level expression recognition on crowd videos. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. In our approach, the fusing process is performed on the deep feature domain by a generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that models spatial information relationships. Furthermore, we extend our proposed spatial NVPF approach to the spatial-temporal NVPF approach to learn the temporal information between frames. To demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on AffectNet database to benchmark the proposed EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from publicly available sources. GECV dataset is a collection of videos containing crowds of people. Each video is labeled with emotion categories at three levels: individual faces, group of people, and the entire video frame.



### Joint Correction of Attenuation and Scatter Using Deep Convolutional Neural Networks (DCNN) for Time-of-Flight PET
- **Arxiv ID**: http://arxiv.org/abs/1811.11852v1
- **DOI**: 10.1088/1361-6560/ab0606
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11852v1)
- **Published**: 2018-11-28 21:44:59+00:00
- **Updated**: 2018-11-28 21:44:59+00:00
- **Authors**: Jaewon Yang, Dookun Park, Jae Ho Sohn, Zhen Jane Wang, Grant T. Gullberg, Youngho Seo
- **Comment**: 4 pages, 7 figures, IEEE MIC 2018 conference
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNN) have demonstrated its capability to convert MR image to pseudo CT for PET attenuation correction in PET/MRI. Conventionally, attenuated events are corrected in sinogram space using attenuation maps derived from CT or MR-derived pseudo CT. Separately, scattered events are iteratively estimated by a 3D model-based simulation using down-sampled attenuation and emission sinograms. However, no studies have investigated joint correction of attenuation and scatter using DCNN in image space. Therefore, we aim to develop and optimize a DCNN model for attenuation and scatter correction (ASC) simultaneously in PET image space without additional anatomical imaging or time-consuming iterative scatter simulation. For the first time, we demonstrated the feasibility of directly producing PET images corrected for attenuation and scatter using DCNN (PET-DCNN) from noncorrected PET (PET-NC) images.



### PointCloud Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1812.01687v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.01687v6)
- **Published**: 2018-11-28 21:50:49+00:00
- **Updated**: 2019-09-12 19:29:12+00:00
- **Authors**: Tianhang Zheng, Changyou Chen, Junsong Yuan, Bo Li, Kui Ren
- **Comment**: Accepted to ICCV19 (oral)
- **Journal**: None
- **Summary**: 3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate point-wise importance w.r.t.\! classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on \url{https://github.com/tianzheng4/PointCloud-Saliency-Maps}.



### Guided patch-wise nonlocal SAR despeckling
- **Arxiv ID**: http://arxiv.org/abs/1811.11872v1
- **DOI**: 10.1109/TGRS.2019.2906412
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11872v1)
- **Published**: 2018-11-28 23:05:35+00:00
- **Updated**: 2018-11-28 23:05:35+00:00
- **Authors**: Sergio Vitale, Davide Cozzolino, Giuseppe Scarpa, Luisa Verdoliva, Giovanni Poggi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new method for SAR image despeckling which leverages information drawn from co-registered optical imagery. Filtering is performed by plain patch-wise nonlocal means, operating exclusively on SAR data. However, the filtering weights are computed by taking into account also the optical guide, which is much cleaner than the SAR data, and hence more discriminative. To avoid injecting optical-domain information into the filtered image, a SAR-domain statistical test is preliminarily performed to reject right away any risky predictor. Experiments on two SAR-optical datasets prove the proposed method to suppress very effectively the speckle, preserving structural details, and without introducing visible filtering artifacts. Overall, the proposed method compares favourably with all state-of-the-art despeckling filters, and also with our own previous optical-guided filter.



### RetinaMatch: Efficient Template Matching of Retina Images for Teleophthalmology
- **Arxiv ID**: http://arxiv.org/abs/1811.11874v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.11874v1)
- **Published**: 2018-11-28 23:06:54+00:00
- **Updated**: 2018-11-28 23:06:54+00:00
- **Authors**: Chen Gong, N. Benjamin Erichson, John P. Kelly, Laura Trutoiu, Brian T. Schowengerdt, Steven L. Brunton, Eric J. Seibel
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal template matching and registration is an important challenge in teleophthalmology with low-cost imaging devices. However, the images from such devices generally have a small field of view (FOV) and image quality degradations, making matching difficult. In this work, we develop an efficient and accurate retinal matching technique that combines dimension reduction and mutual information (MI), called RetinaMatch. The dimension reduction initializes the MI optimization as a coarse localization process, which narrows the optimization domain and avoids local optima. The effectiveness of RetinaMatch is demonstrated on the open fundus image database STARE with simulated reduced FOV and anticipated degradations, and on retinal images acquired by adapter-based optics attached to a smartphone. RetinaMatch achieves a success rate over 94\% on human retinal images with the matched target registration errors below 2 pixels on average, excluding the observer variability. It outperforms the standard template matching solutions. In the application of measuring vessel diameter repeatedly, single pixel errors are expected. In addition, our method can be used in the process of image mosaicking with area-based registration, providing a robust approach when the feature based methods fail. To the best of our knowledge, this is the first template matching algorithm for retina images with small template images from unconstrained retinal areas. In the context of the emerging mixed reality market, we envision automated retinal image matching and registration methods as transformative for advanced teleophthalmology and long-term retinal monitoring.



### Adversarial Attacks for Optical Flow-Based Action Recognition Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1811.11875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.11875v1)
- **Published**: 2018-11-28 23:10:47+00:00
- **Updated**: 2018-11-28 23:10:47+00:00
- **Authors**: Nathan Inkawhich, Matthew Inkawhich, Yiran Chen, Hai Li
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classification models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classifier attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classifier trained on the UCF-101 dataset. We find that our attacks can significantly degrade a model's performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.




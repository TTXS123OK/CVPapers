# Arxiv Papers in cs.CV on 2018-11-12
### A Smart System for Selection of Optimal Product Images in E-Commerce
- **Arxiv ID**: http://arxiv.org/abs/1811.07996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07996v1)
- **Published**: 2018-11-12 02:35:48+00:00
- **Updated**: 2018-11-12 02:35:48+00:00
- **Authors**: Abon Chaudhuri, Paolo Messina, Samrat Kokkula, Aditya Subramanian, Abhinandan Krishnan, Shreyansh Gandhi, Alessandro Magnani, Venkatesh Kandaswamy
- **Comment**: Accepted in IEEE Big Data Conference 2018 (Industry & Government
  Track)
- **Journal**: None
- **Summary**: In e-commerce, content quality of the product catalog plays a key role in delivering a satisfactory experience to the customers. In particular, visual content such as product images influences customers' engagement and purchase decisions. With the rapid growth of e-commerce and the advent of artificial intelligence, traditional content management systems are giving way to automated scalable systems. In this paper, we present a machine learning driven visual content management system for extremely large e-commerce catalogs. For a given product, the system aggregates images from various suppliers, understands and analyzes them to produce a superior image set with optimal image count and quality, and arranges them in an order tailored to the demands of the customers. The system makes use of an array of technologies, ranging from deep learning to traditional computer vision, at different stages of analysis. In this paper, we outline how the system works and discuss the unique challenges related to applying machine learning techniques to real-world data from e-commerce domain. We emphasize how we tune state-of-the-art image classification techniques to develop solutions custom made for a massive, diverse, and constantly evolving product catalog. We also provide the details of how we measure the system's impact on various customer engagement metrics.



### M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network
- **Arxiv ID**: http://arxiv.org/abs/1811.04533v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04533v3)
- **Published**: 2018-11-12 03:01:50+00:00
- **Updated**: 2019-01-06 14:32:16+00:00
- **Authors**: Qijie Zhao, Tao Sheng, Yongtao Wang, Zhi Tang, Ying Chen, Ling Cai, Haibin Ling
- **Comment**: AAAI19
- **Journal**: None
- **Summary**: Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object detectors (e.g., Mask R-CNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multi-scale, pyramidal architecture of the backbones which are actually designed for object classification task. Newly, in this work, we present a method called Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each u-shape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to develop a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det by integrating it into the architecture of SSD, which gets better detection performance than state-of-the-art one-stage detectors. Specifically, on MS-COCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which is the new state-of-the-art results among one-stage detectors. The code will be made available on \url{https://github.com/qijiezhao/M2Det.



### Road Damage Detection And Classification In Smartphone Captured Images Using Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1811.04535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04535v1)
- **Published**: 2018-11-12 03:16:42+00:00
- **Updated**: 2018-11-12 03:16:42+00:00
- **Authors**: Janpreet Singh, Shashank Shekhar
- **Comment**: Both authors contributed equally to the paper. Under submission to
  the IEEE International Conference On Big Data Cup 2018
- **Journal**: None
- **Summary**: This paper summarizes the design, experiments and results of our solution to the Road Damage Detection and Classification Challenge held as part of the 2018 IEEE International Conference On Big Data Cup. Automatic detection and classification of damage in roads is an essential problem for multiple applications like maintenance and autonomous driving. We demonstrate that convolutional neural net based instance detection and classfication approaches can be used to solve this problem. In particular we show that Mask-RCNN, one of the state-of-the-art algorithms for object detection, localization and instance segmentation of natural images, can be used to perform this task in a fast manner with effective results. We achieve a mean F1 score of 0.528 at an IoU of 50% on the task of detection and classification of different types of damages in real-world road images acquired using a smartphone camera and our average inference time for each image is 0.105 seconds on an NVIDIA GeForce 1080Ti graphic card. The code and saved models for our approach can be found here : https://github.com/sshkhr/BigDataCup18 Submission



### Identification of Internal Faults in Indirect Symmetrical Phase Shift Transformers Using Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.04537v1
- **DOI**: 10.1109/ISSPIT.2018.8705100
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04537v1)
- **Published**: 2018-11-12 03:22:28+00:00
- **Updated**: 2018-11-12 03:22:28+00:00
- **Authors**: Pallav Kumar Bera, Rajesh Kumar, Can Isik
- **Comment**: 18th IEEE International Symposium on Signal Processing and
  Information Technology (ISSPIT), 2018
- **Journal**: IEEE International Symposium on Signal Processing and Information
  Technology (ISSPIT), 2018, pp. 1-6,
- **Summary**: This paper proposes methods to identify 40 different types of internal faults in an Indirect Symmetrical Phase Shift Transformer (ISPST). The ISPST was modeled using Power System Computer Aided Design (PSCAD)/ Electromagnetic Transients including DC (EMTDC). The internal faults were simulated by varying the transformer tapping, backward and forward phase shifts, loading, and percentage of winding faulted. Data for 960 cases of each type of fault was recorded. A series of features were extracted for a, b, and c phases from time, frequency, time-frequency, and information theory domains. The importance of the extracted features was evaluated through univariate tests which helped to reduce the number of features. The selected features were then used for training five state-of-the-art machine learning classifiers. Extremely Random Trees and Random Forest, the ensemble-based learners, achieved the accuracy of 98.76% and 97.54% respectively outperforming Multilayer Perceptron (96.13%), Logistic Regression (93.54%), and Support Vector Machines (92.60%)



### Visual Saliency Maps Can Apply to Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1811.04544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04544v1)
- **Published**: 2018-11-12 03:51:33+00:00
- **Updated**: 2018-11-12 03:51:33+00:00
- **Authors**: Zhenyue Qin, Jie Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Human eyes concentrate different facial regions during distinct cognitive activities. We study utilising facial visual saliency maps to classify different facial expressions into different emotions. Our results show that our novel method of merely using facial saliency maps can achieve a descent accuracy of 65\%, much higher than the chance level of $1/7$. Furthermore, our approach is of semi-supervision, i.e., our facial saliency maps are generated from a general saliency prediction algorithm that is not explicitly designed for face images. We also discovered that the classification accuracies of each emotional class using saliency maps demonstrate a strong positive correlation with the accuracies produced by face images. Our work implies that humans may look at different facial areas in order to perceive different emotions.



### Holistic Multi-modal Memory Network for Movie Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1811.04595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04595v1)
- **Published**: 2018-11-12 08:10:21+00:00
- **Updated**: 2018-11-12 08:10:21+00:00
- **Authors**: Anran Wang, Anh Tuan Luu, Chuan-Sheng Foo, Hongyuan Zhu, Yi Tay, Vijay Chandrasekhar
- **Comment**: None
- **Journal**: None
- **Summary**: Answering questions according to multi-modal context is a challenging problem as it requires a deep integration of different data sources. Existing approaches only employ partial interactions among data sources in one attention hop. In this paper, we present the Holistic Multi-modal Memory Network (HMMN) framework which fully considers the interactions between different input sources (multi-modal context, question) in each hop. In addition, it takes answer choices into consideration during the context retrieval stage. Therefore, the proposed framework effectively integrates multi-modal context, question, and answer information, which leads to more informative context retrieved for question answering. Our HMMN framework achieves state-of-the-art accuracy on MovieQA dataset. Extensive ablation studies show the importance of holistic reasoning and contributions of different attention strategies.



### Learning The Invisible: A Hybrid Deep Learning-Shearlet Framework for Limited Angle Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/1811.04602v1
- **DOI**: 10.1088/1361-6420/ab10ca
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04602v1)
- **Published**: 2018-11-12 08:36:42+00:00
- **Updated**: 2018-11-12 08:36:42+00:00
- **Authors**: T. A. Bubba, G. Kutyniok, M. Lassas, M. MÃ¤rz, W. Samek, S. Siltanen, V. Srinivasan
- **Comment**: None
- **Journal**: None
- **Summary**: The high complexity of various inverse problems poses a significant challenge to model-based reconstruction schemes, which in such situations often reach their limits. At the same time, we witness an exceptional success of data-based methodologies such as deep learning. However, in the context of inverse problems, deep neural networks mostly act as black box routines, used for instance for a somewhat unspecified removal of artifacts in classical image reconstructions. In this paper, we will focus on the severely ill-posed inverse problem of limited angle computed tomography, in which entire boundary sections are not captured in the measurements. We will develop a hybrid reconstruction framework that fuses model-based sparse regularization with data-driven deep learning. Our method is reliable in the sense that we only learn the part that can provably not be handled by model-based methods, while applying the theoretically controllable sparse regularization technique to the remaining parts. Such a decomposition into visible and invisible segments is achieved by means of the shearlet transform that allows to resolve wavefront sets in the phase space. Furthermore, this split enables us to assign the clear task of inferring unknown shearlet coefficients to the neural network and thereby offering an interpretation of its performance in the context of limited angle computed tomography. Our numerical experiments show that our algorithm significantly surpasses both pure model- and more data-based reconstruction methods.



### Matrix Product Operator Restricted Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1811.04608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04608v1)
- **Published**: 2018-11-12 08:58:20+00:00
- **Updated**: 2018-11-12 08:58:20+00:00
- **Authors**: Cong Chen, Kim Batselier, Ching-Yun Ko, Ngai Wong
- **Comment**: None
- **Journal**: None
- **Summary**: A restricted Boltzmann machine (RBM) learns a probability distribution over its input samples and has numerous uses like dimensionality reduction, classification and generative modeling. Conventional RBMs accept vectorized data that dismisses potentially important structural information in the original tensor (multi-way) input. Matrix-variate and tensor-variate RBMs, named MvRBM and TvRBM, have been proposed but are all restrictive by model construction, which leads to a weak model expression power. This work presents the matrix product operator RBM (MPORBM) that utilizes a tensor network generalization of Mv/TvRBM, preserves input formats in both the visible and hidden layers, and results in higher expressive power. A novel training algorithm integrating contrastive divergence and an alternating optimization procedure is also developed. Numerical experiments compare the MPORBM with the traditional RBM and MvRBM for data classification and image completion and denoising tasks. The expressive power of the MPORBM as a function of the MPO-rank is also investigated.



### Depth Image Upsampling based on Guided Filter with Low Gradient Minimization
- **Arxiv ID**: http://arxiv.org/abs/1811.04620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04620v1)
- **Published**: 2018-11-12 09:36:12+00:00
- **Updated**: 2018-11-12 09:36:12+00:00
- **Authors**: Hang Yang, Zhongbo Zhang
- **Comment**: 28 pages, 7figures
- **Journal**: None
- **Summary**: In this paper, we present a novel upsampling framework to enhance the spatial resolution of the depth image. In our framework, the upscaling of a low-resolution depth image is guided by a corresponding intensity images, we formulate it as a cost aggregation problem with the guided filter. However, the guided filter does not make full use of the properties of the depth image. Since depth images have quite sparse gradients, it inspires us to regularize the gradients for improving depth upscaling results. Statistics show a special property of depth images, that is, there is a non-ignorable part of pixels whose horizontal or vertical derivatives are equal to $\pm 1$. Considering this special property, we propose a low gradient regularization method which reduces the penalty for horizontal or vertical derivative $\pm1$. The proposed low gradient regularization is integrated with the guided filter into the depth image upsampling method. Experimental results demonstrate the effectiveness of our proposed approach both qualitatively and quantitatively compared with the state-of-the-art methods.



### Parameterized Synthetic Image Data Set for Fisheye Lens
- **Arxiv ID**: http://arxiv.org/abs/1811.04627v1
- **DOI**: 10.1109/ICISCE.2018.00084
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04627v1)
- **Published**: 2018-11-12 09:48:43+00:00
- **Updated**: 2018-11-12 09:48:43+00:00
- **Authors**: Zhen Chen, Anthimos Georgiadis
- **Comment**: 2018 5th International Conference on Information Science and Control
  Engineering
- **Journal**: None
- **Summary**: Based on different projection geometry, a fisheye image can be presented as a parameterized non-rectilinear image. Deep neural networks(DNN) is one of the solutions to extract parameters for fisheye image feature description. However, a large number of images are required for training a reasonable prediction model for DNN. In this paper, we propose to extend the scale of the training dataset using parameterized synthetic images. It effectively boosts the diversity of images and avoids the data scale limitation. To simulate different viewing angles and distances, we adopt controllable parameterized projection processes on transformation. The reliability of the proposed method is proved by testing images captured by our fisheye camera. The synthetic dataset is the first dataset that is able to extend to a big scale labeled fisheye image dataset. It is accessible via: http://www2.leuphana.de/misl/fisheye-data-set/.



### Hallucinating very low-resolution and obscured face images
- **Arxiv ID**: http://arxiv.org/abs/1811.04645v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04645v4)
- **Published**: 2018-11-12 10:40:09+00:00
- **Updated**: 2018-12-12 02:24:34+00:00
- **Authors**: Lianping Yang, Bin Shao, Ting Sun, Song Ding, Xiangde Zhang
- **Comment**: 20 pages, Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: Most of the face hallucination methods are designed for complete inputs. They will not work well if the inputs are very tiny or contaminated by large occlusion. Inspired by this fact, we propose an obscured face hallucination network(OFHNet). The OFHNet consists of four parts: an inpainting network, an upsampling network, a discriminative network, and a fixed facial landmark detection network. The inpainting network restores the low-resolution(LR) obscured face images. The following upsampling network is to upsample the output of inpainting network. In order to ensure the generated high-resolution(HR) face images more photo-realistic, we utilize the discriminative network and the facial landmark detection network to better the result of upsampling network. In addition, we present a semantic structure loss, which makes the generated HR face images more pleasing. Extensive experiments show that our framework can restore the appealing HR face images from 1/4 missing area LR face images with a challenging scaling factor of 8x.



### Distributionally Robust Semi-Supervised Learning for People-Centric Sensing
- **Arxiv ID**: http://arxiv.org/abs/1811.05299v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.05299v1)
- **Published**: 2018-11-12 10:53:33+00:00
- **Updated**: 2018-11-12 10:53:33+00:00
- **Authors**: Kaixuan Chen, Lina Yao, Dalin Zhang, Xiaojun Chang, Guodong Long, Sen Wang
- **Comment**: 8 pages, accepted by AAAI2019
- **Journal**: None
- **Summary**: Semi-supervised learning is crucial for alleviating labelling burdens in people-centric sensing. However, human-generated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-specific discrepancy and preserve task-specific consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods.



### RelDenClu: A Relative Density based Biclustering Method for identifying non-linear feature relations
- **Arxiv ID**: http://arxiv.org/abs/1811.04661v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04661v5)
- **Published**: 2018-11-12 11:11:26+00:00
- **Updated**: 2021-05-11 11:32:37+00:00
- **Authors**: Namita Jain, Susmita Ghosh, C. A. Murthy
- **Comment**: None
- **Journal**: None
- **Summary**: The existing biclustering algorithms for finding feature relation based biclusters often depend on assumptions like monotonicity or linearity. Though a few algorithms overcome this problem by using density-based methods, they tend to miss out many biclusters because they use global criteria for identifying dense regions. The proposed method, RelDenClu uses the local variations in marginal and joint densities for each pair of features to find the subset of observations, which forms the bases of the relation between them. It then finds the set of features connected by a common set of observations, resulting in a bicluster.   To show the effectiveness of the proposed methodology, experimentation has been carried out on fifteen types of simulated datasets. Further, it has been applied to six real-life datasets. For three of these real-life datasets, the proposed method is used for unsupervised learning, while for other three real-life datasets it is used as an aid to supervised learning. For all the datasets the performance of the proposed method is compared with that of seven different state-of-the-art algorithms and the proposed algorithm is seen to produce better results. The efficacy of proposed algorithm is also seen by its use on COVID-19 dataset for identifying some features (genetic, demographics and others) that are likely to affect the spread of COVID-19.



### Towards Adversarial Denoising of Radar Micro-Doppler Signatures
- **Arxiv ID**: http://arxiv.org/abs/1811.04678v3
- **DOI**: 10.1109/RADAR41533.2019.171396
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04678v3)
- **Published**: 2018-11-12 11:52:02+00:00
- **Updated**: 2019-08-07 14:23:22+00:00
- **Authors**: Sherif Abdulatif, Karim Armanious, Fady Aziz, Urs Schneider, Bin Yang
- **Comment**: Accepted in IEEE International Radar Conference 2019
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are considered the state-of-the-art in the field of image generation. They learn the joint distribution of the training data and attempt to generate new data samples in high dimensional space following the same distribution as the input. Recent improvements in GANs opened the field to many other computer vision applications based on improving and changing the characteristics of the input image to follow some given training requirements. In this paper, we propose a novel technique for the denoising and reconstruction of the micro-Doppler ($\boldsymbol{\mu}$-D) spectra of walking humans based on GANs. Two sets of experiments were collected on 22 subjects walking on a treadmill at an intermediate velocity using a \unit[25]{GHz} CW radar. In one set, a clean $\boldsymbol{\mu}$-D spectrum is collected for each subject by placing the radar at a close distance to the subject. In the other set, variations are introduced in the experiment setup to introduce different noise and clutter effects on the spectrum by changing the distance and placing reflective objects between the radar and the target. Synthetic paired noisy and noise-free spectra were used for training, while validation was carried out on the real noisy measured data. Finally, qualitative and quantitative comparison with other classical radar denoising approaches in the literature demonstrated the proposed GANs framework is better and more robust to different noise levels.



### Learning Segmentation Masks with the Independence Prior
- **Arxiv ID**: http://arxiv.org/abs/1811.04682v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04682v2)
- **Published**: 2018-11-12 12:06:30+00:00
- **Updated**: 2018-11-13 11:40:15+00:00
- **Authors**: Songmin Dai, Xiaoqiang Li, Lu Wang, Pin Wu, Weiqin Tong, Yimin Chen
- **Comment**: 7+5 pages, 13 figures, Accepted to AAAI 2019
- **Journal**: None
- **Summary**: An instance with a bad mask might make a composite image that uses it look fake. This encourages us to learn segmentation by generating realistic composite images. To achieve this, we propose a novel framework that exploits a new proposed prior called the independence prior based on Generative Adversarial Networks (GANs). The generator produces an image with multiple category-specific instance providers, a layout module and a composition module. Firstly, each provider independently outputs a category-specific instance image with a soft mask. Then the provided instances' poses are corrected by the layout module. Lastly, the composition module combines these instances into a final image. Training with adversarial loss and penalty for mask area, each provider learns a mask that is as small as possible but enough to cover a complete category-specific instance. Weakly supervised semantic segmentation methods widely use grouping cues modeling the association between image parts, which are either artificially designed or learned with costly segmentation labels or only modeled on local pairs. Unlike them, our method automatically models the dependence between any parts and learns instance segmentation. We apply our framework in two cases: (1) Foreground segmentation on category-specific images with box-level annotation. (2) Unsupervised learning of instance appearances and masks with only one image of homogeneous object cluster (HOC). We get appealing results in both tasks, which shows the independence prior is useful for instance segmentation and it is possible to unsupervisedly learn instance masks with only one image.



### Deep-learning the Latent Space of Light Transport
- **Arxiv ID**: http://arxiv.org/abs/1811.04756v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.04756v2)
- **Published**: 2018-11-12 14:55:58+00:00
- **Updated**: 2019-06-30 10:22:08+00:00
- **Authors**: Pedro Hermosilla, Sebastian Maisch, Tobias Ritschel, Timo Ropinski
- **Comment**: Eurographics Symposium on Rendering 2019
- **Journal**: None
- **Summary**: We suggest a method to directly deep-learn light transport, i. e., the mapping from a 3D geometry-illumination-material configuration to a shaded 2D image. While many previous learning methods have employed 2D convolutional neural networks applied to images, we show for the first time that light transport can be learned directly in 3D. The benefit of 3D over 2D is, that the former can also correctly capture illumination effects related to occluded and/or semi-transparent geometry. To learn 3D light transport, we represent the 3D scene as an unstructured 3D point cloud, which is later, during rendering, projected to the 2D output image. Thus, we suggest a two-stage operator comprising of a 3D network that first transforms the point cloud into a latent representation, which is later on projected to the 2D output image using a dedicated 3D-2D network in a second step. We will show that our approach results in improved quality in terms of temporal coherence while retaining most of the computational efficiency of common 2D methods. As a consequence, the proposed two stage-operator serves as a valuable extension to modern deferred shading approaches.



### Learning data augmentation policies using augmented random search
- **Arxiv ID**: http://arxiv.org/abs/1811.04768v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.04768v1)
- **Published**: 2018-11-12 15:14:18+00:00
- **Updated**: 2018-11-12 15:14:18+00:00
- **Authors**: Mingyang Geng, Kele Xu, Bo Ding, Huaimin Wang, Lei Zhang
- **Comment**: Submitted to ICASSP
- **Journal**: None
- **Summary**: Previous attempts for data augmentation are designed manually, and the augmentation policies are dataset-specific. Recently, an automatic data augmentation approach, named AutoAugment, is proposed using reinforcement learning. AutoAugment searches for the augmentation polices in the discrete search space, which may lead to a sub-optimal solution. In this paper, we employ the Augmented Random Search method (ARS) to improve the performance of AutoAugment. Our key contribution is to change the discrete search space to continuous space, which will improve the searching performance and maintain the diversities between sub-policies. With the proposed method, state-of-the-art accuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without additional data). Our code is available at https://github.com/gmy2013/ARS-Aug.



### Adaptive Target Recognition: A Case Study Involving Airport Baggage Screening
- **Arxiv ID**: http://arxiv.org/abs/1811.04772v2
- **DOI**: 10.1117/12.2557638
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1811.04772v2)
- **Published**: 2018-11-12 15:15:39+00:00
- **Updated**: 2018-11-30 19:20:45+00:00
- **Authors**: Ankit Manerikar, Tanmay Prakash, Avinash C. Kak
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses the question whether it is possible to design a computer-vision based automatic threat recognition (ATR) system so that it can adapt to changing specifications of a threat without having to create a new ATR each time. The changes in threat specifications, which may be warranted by intelligence reports and world events, are typically regarding the physical characteristics of what constitutes a threat: its material composition, its shape, its method of concealment, etc. Here we present our design of an AATR system (Adaptive ATR) that can adapt to changing specifications in materials characterization (meaning density, as measured by its x-ray attenuation coefficient), its mass, and its thickness. Our design uses a two-stage cascaded approach, in which the first stage is characterized by a high recall rate over the entire range of possibilities for the threat parameters that are allowed to change. The purpose of the second stage is to then fine-tune the performance of the overall system for the current threat specifications. The computational effort for this fine-tuning for achieving a desired PD/PFA rate is far less than what it would take to create a new classifier with the same overall performance for the new set of threat specifications.



### Automatic kidney segmentation in ultrasound images using subsequent boundary distance regression and pixelwise classification networks
- **Arxiv ID**: http://arxiv.org/abs/1811.04815v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04815v3)
- **Published**: 2018-11-12 15:54:59+00:00
- **Updated**: 2019-03-30 16:11:52+00:00
- **Authors**: Shi Yin, Qinmu Peng, Hongming Li, Zhengqiang Zhang, Xinge You, Susan L. Furth, Gregory E. Tasian, Yong Fan
- **Comment**: The paper has been submitted to the Medical Image Analysis for review
- **Journal**: None
- **Summary**: It remains challenging to automatically segment kidneys in clinical ultrasound (US) images due to the kidneys' varied shapes and image intensity distributions, although semi-automatic methods have achieved promising performance. In this study, we propose subsequent boundary distance regression and pixel classification networks to segment the kidneys, informed by the fact that the kidney boundaries have relatively homogenous texture patterns across images. Particularly, we first use deep neural networks pre-trained for classification of natural images to extract high-level image features from US images, then these features are used as input to learn kidney boundary distance maps using a boundary distance regression network, and finally the predicted boundary distance maps are classified as kidney pixels or non-kidney pixels using a pixel classification network in an end-to-end learning fashion. We also adopted a data-augmentation method based on kidney shape registration to generate enriched training data from a small number of US images with manually segmented kidney labels. Experimental results have demonstrated that our method could effectively improve the performance of automatic kidney segmentation, significantly better than deep learning-based pixel classification networks.



### A test case for application of convolutional neural networks to spatio-temporal climate data: Re-identifying clustered weather patterns
- **Arxiv ID**: http://arxiv.org/abs/1811.04817v1
- **DOI**: 10.1038/s41598-020-57897-9
- **Categories**: **physics.ao-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04817v1)
- **Published**: 2018-11-12 15:56:30+00:00
- **Updated**: 2018-11-12 15:56:30+00:00
- **Authors**: Ashesh Chattopadhyay, Pedram Hassanzadeh, Saba Pasha
- **Comment**: None
- **Journal**: Scientific Reports, 2020
- **Summary**: Convolutional neural networks (CNNs) can potentially provide powerful tools for classifying and identifying patterns in climate and environmental data. However, because of the inherent complexities of such data, which are often spatio-temporal, chaotic, and non-stationary, the CNN algorithms must be designed/evaluated for each specific dataset and application. Yet to start, CNN, a supervised technique, requires a large labeled dataset. Labeling demands (human) expert time, which combined with the limited number of relevant examples in this area, can discourage using CNNs for new problems. To address these challenges, here we (1) Propose an effective auto-labeling strategy based on using an unsupervised clustering algorithm and evaluating the performance of CNNs in re-identifying these clusters; (2) Use this approach to label thousands of daily large-scale weather patterns over North America in the outputs of a fully-coupled climate model and show the capabilities of CNNs in re-identifying the 4 clustered regimes. The deep CNN trained with $1000$ samples or more per cluster has an accuracy of $90\%$ or better. Accuracy scales monotonically but nonlinearly with the size of the training set, e.g. reaching $94\%$ with $3000$ training samples per cluster. Effects of architecture and hyperparameters on the performance of CNNs are examined and discussed.



### Generative Dual Adversarial Network for Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.04857v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04857v4)
- **Published**: 2018-11-12 17:04:51+00:00
- **Updated**: 2019-05-25 08:40:29+00:00
- **Authors**: He Huang, Changhu Wang, Philip S. Yu, Chang-Dong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of generalized zero-shot learning which requires the model to train on image-label pairs from some seen classes and test on the task of classifying new images from both seen and unseen classes. Most previous models try to learn a fixed one-directional mapping between visual and semantic space, while some recently proposed generative methods try to generate image features for unseen classes so that the zero-shot learning problem becomes a traditional fully-supervised classification problem. In this paper, we propose a novel model that provides a unified framework for three different approaches: visual-> semantic mapping, semantic->visual mapping, and metric learning. Specifically, our proposed model consists of a feature generator that can generate various visual features given class embeddings as input, a regressor that maps each visual feature back to its corresponding class embedding, and a discriminator that learns to evaluate the closeness of an image feature and a class embedding. All three components are trained under the combination of cyclic consistency loss and dual adversarial loss. Experimental results show that our model not only preserves higher accuracy in classifying images from seen classes, but also performs better than existing state-of-the-art models in in classifying images from unseen classes.



### A Framework of Transfer Learning in Object Detection for Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1811.04863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04863v2)
- **Published**: 2018-11-12 17:12:16+00:00
- **Updated**: 2018-11-24 22:08:42+00:00
- **Authors**: Ioannis Athanasiadis, Panagiotis Mousouliotis, Loukas Petrou
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning is one of the subjects undergoing intense study in the area of machine learning. In object recognition and object detection there are known experiments for the transferability of parameters, but not for neural networks which are suitable for object detection in real time embedded applications, such as the SqueezeDet neural network. We use transfer learning to accelerate the training of SqueezeDet to a new group of classes. Also, experiments are conducted to study the transferability and co-adaptation phenomena introduced by the transfer learning process. To accelerate training, we propose a new implementation of the SqueezeDet training which provides a faster pipeline for data processing and achieves 1.8 times speedup compared to the initial implementation. Finally, we created a mechanism for automatic hyperparameter optimization using an empirical method.



### A Perceptual Prediction Framework for Self Supervised Event Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1811.04869v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04869v3)
- **Published**: 2018-11-12 17:26:28+00:00
- **Updated**: 2019-04-05 22:40:36+00:00
- **Authors**: Sathyanarayanan N. Aakur, Sudeep Sarkar
- **Comment**: CVPR 2019 Camera Ready
- **Journal**: None
- **Summary**: Temporal segmentation of long videos is an important problem, that has largely been tackled through supervised learning, often requiring large amounts of annotated training data. In this paper, we tackle the problem of self-supervised temporal segmentation of long videos that alleviate the need for any supervision. We introduce a self-supervised, predictive learning framework that draws inspiration from cognitive psychology to segment long, visually complex videos into individual, stable segments that share the same semantics. We also introduce a new adaptive learning paradigm that helps reduce the effect of catastrophic forgetting in recurrent neural networks. Extensive experiments on three publicly available datasets - Breakfast Actions, 50 Salads, and INRIA Instructional Videos datasets show the efficacy of the proposed approach. We show that the proposed approach is able to outperform weakly-supervised and other unsupervised learning approaches by up to 24% and have competitive performance compared to fully supervised approaches. We also show that the proposed approach is able to learn highly discriminative features that help improve action recognition when used in a representation learning paradigm.



### Focusing on the Big Picture: Insights into a Systems Approach to Deep Learning for Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1811.04893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04893v1)
- **Published**: 2018-11-12 18:25:20+00:00
- **Updated**: 2018-11-12 18:25:20+00:00
- **Authors**: Ritwik Gupta, Carson D. Sestili, Javier A. Vazquez-Trejo, Matthew E. Gaston
- **Comment**: Accepted to IEEE Big Data 2018
- **Journal**: None
- **Summary**: Deep learning tasks are often complicated and require a variety of components working together efficiently to perform well. Due to the often large scale of these tasks, there is a necessity to iterate quickly in order to attempt a variety of methods and to find and fix bugs. While participating in IARPA's Functional Map of the World challenge, we identified challenges along the entire deep learning pipeline and found various solutions to these challenges. In this paper, we present the performance, engineering, and deep learning considerations with processing and modeling data, as well as underlying infrastructure considerations that support large-scale deep learning tasks. We also discuss insights and observations with regard to satellite imagery and deep learning for image classification.



### Deep Learning versus Classical Regression for Brain Tumor Patient Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1811.04907v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.04907v1)
- **Published**: 2018-11-12 18:45:08+00:00
- **Updated**: 2018-11-12 18:45:08+00:00
- **Authors**: Yannick Suter, Alain Jungo, Michael Rebsamen, Urspeter Knecht, Evelyn Herrmann, Roland Wiest, Mauricio Reyes
- **Comment**: Contribution to The International Multimodal Brain Tumor Segmentation
  (BraTS) Challenge 2018, survival prediction task
- **Journal**: None
- **Summary**: Deep learning for regression tasks on medical imaging data has shown promising results. However, compared to other approaches, their power is strongly linked to the dataset size. In this study, we evaluate 3D-convolutional neural networks (CNNs) and classical regression methods with hand-crafted features for survival time regression of patients with high grade brain tumors. The tested CNNs for regression showed promising but unstable results. The best performing deep learning approach reached an accuracy of 51.5% on held-out samples of the training set. All tested deep learning experiments were outperformed by a Support Vector Classifier (SVC) using 30 radiomic features. The investigated features included intensity, shape, location and deep features. The submitted method to the BraTS 2018 survival prediction challenge is an ensemble of SVCs, which reached a cross-validated accuracy of 72.2% on the BraTS 2018 training set, 57.1% on the validation set, and 42.9% on the testing set. The results suggest that more training data is necessary for a stable performance of a CNN model for direct regression from magnetic resonance images, and that non-imaging clinical patient information is crucial along with imaging information.



### OriNet: A Fully Convolutional Network for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.04989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.04989v1)
- **Published**: 2018-11-12 20:18:30+00:00
- **Updated**: 2018-11-12 20:18:30+00:00
- **Authors**: Chenxu Luo, Xiao Chu, Alan Yuille
- **Comment**: BMVC 2018. Code available at https://github.com/chenxuluo/OriNet-demo
- **Journal**: BMVC 2018 - Proceedings of the British Machine Vision Conference
  2018
- **Summary**: In this paper, we propose a fully convolutional network for 3D human pose estimation from monocular images. We use limb orientations as a new way to represent 3D poses and bind the orientation together with the bounding box of each limb region to better associate images and predictions. The 3D orientations are modeled jointly with 2D keypoint detections. Without additional constraints, this simple method can achieve good results on several large-scale benchmarks. Further experiments show that our method can generalize well to novel scenes and is robust to inaccurate bounding boxes.



### A new approach for pedestrian density estimation using moving sensors and computer vision
- **Arxiv ID**: http://arxiv.org/abs/1811.05006v2
- **DOI**: 10.1145/3397575
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05006v2)
- **Published**: 2018-11-12 21:19:38+00:00
- **Updated**: 2020-07-03 16:12:23+00:00
- **Authors**: Eric K. Tokuda, Yitzchak Lockerman, Gabriel B. A. Ferreira, Ethan Sorrelgreen, David Boyle, Roberto M. Cesar-Jr., Claudio T. Silva
- **Comment**: Submitted to ACM-TSAS
- **Journal**: None
- **Summary**: An understanding of pedestrian dynamics is indispensable for numerous urban applications including the design of transportation networks and planing for business development. Pedestrian counting often requires utilizing manual or technical means to count individuals in each location of interest. However, such methods do not scale to the size of a city and a new approach to fill this gap is here proposed. In this project, we used a large dense dataset of images of New York City along with computer vision techniques to construct a spatio-temporal map of relative person density. Due to the limitations of state of the art computer vision methods, such automatic detection of person is inherently subject to errors. We model these errors as a probabilistic process, for which we provide theoretical analysis and thorough numerical simulations. We demonstrate that, within our assumptions, our methodology can supply a reasonable estimate of person densities and provide theoretical bounds for the resulting error.



### Blindfold Baselines for Embodied QA
- **Arxiv ID**: http://arxiv.org/abs/1811.05013v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.05013v1)
- **Published**: 2018-11-12 21:45:41+00:00
- **Updated**: 2018-11-12 21:45:41+00:00
- **Authors**: Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron Courville
- **Comment**: NIPS 2018 Visually-Grounded Interaction and Language (ViGilL)
  Workshop
- **Journal**: None
- **Summary**: We explore blindfold (question-only) baselines for Embodied Question Answering. The EmbodiedQA task requires an agent to answer a question by intelligently navigating in a simulated environment, gathering necessary visual information only through first-person vision before finally answering. Consequently, a blindfold baseline which ignores the environment and visual information is a degenerate solution, yet we show through our experiments on the EQAv1 dataset that a simple question-only baseline achieves state-of-the-art results on the EmbodiedQA task in all cases except when the agent is spawned extremely close to the object.



### NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.05014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05014v1)
- **Published**: 2018-11-12 21:53:28+00:00
- **Updated**: 2018-11-12 21:53:28+00:00
- **Authors**: Rongcheng Lin, Jing Xiao, Jianping Fan
- **Comment**: ECCV 2018 workshop
- **Journal**: None
- **Summary**: This paper introduces a fast and efficient network architecture, NeXtVLAD, to aggregate frame-level features into a compact feature vector for large-scale video classification. Briefly speaking, the basic idea is to decompose a high-dimensional feature into a group of relatively low-dimensional vectors with attention before applying NetVLAD aggregation over time. This NeXtVLAD approach turns out to be both effective and parameter efficient in aggregating temporal information. In the 2nd Youtube-8M video understanding challenge, a single NeXtVLAD model with less than 80M parameters achieves a GAP score of 0.87846 in private leaderboard. A mixture of 3 NeXtVLAD models results in 0.88722, which is ranked 3rd over 394 teams. The code is publicly available at https://github.com/linrongc/youtube-8m.



### Deep Neural Network Augmentation: Generating Faces for Affect Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.05027v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.05027v2)
- **Published**: 2018-11-12 22:42:40+00:00
- **Updated**: 2019-07-16 21:33:58+00:00
- **Authors**: Dimitrios Kollias, Shiyang Cheng, Evangelos Ververas, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach for synthesizing facial affect; either in terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness and surprise), or in terms of valence (i.e., how positive or negative is an emotion) and arousal (i.e., power of the emotion activation). The proposed approach accepts the following inputs: i) a neutral 2D image of a person; ii) a basic facial expression or a pair of valence-arousal (VA) emotional state descriptors to be generated, or a path of affect in the 2D VA Space to be generated as an image sequence. In order to synthesize affect in terms of VA, for this person, $600,000$ frames from the 4DFAB database were annotated. The affect synthesis is implemented by fitting a 3D Morphable Model on the neutral image, then deforming the reconstructed face and adding the inputted affect, and blending the new face with the given affect into the original image. Qualitative experiments illustrate the generation of realistic images, when the neutral image is sampled from thirteen well known lab-controlled or in-the-wild databases, including Aff-Wild, AffectNet, RAF-DB; comparisons with Generative Adversarial Networks (GANs) show the higher quality achieved by the proposed approach. Then, quantitative experiments are conducted, in which the synthesized images are used for data augmentation in training Deep Neural Networks to perform affect recognition over all databases; greatly improved performances are achieved when compared with state-of-the-art methods, as well as with GAN-based data augmentation, in all cases.



### LookinGood: Enhancing Performance Capture with Real-time Neural Re-Rendering
- **Arxiv ID**: http://arxiv.org/abs/1811.05029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05029v1)
- **Published**: 2018-11-12 22:51:19+00:00
- **Updated**: 2018-11-12 22:51:19+00:00
- **Authors**: Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, Adarsh Kowdle, Christoph Rhemann, Dan B Goldman, Cem Keskin, Steve Seitz, Shahram Izadi, Sean Fanello
- **Comment**: The supplementary video is available at: http://youtu.be/Md3tdAKoLGU
  To be presented at SIGGRAPH Asia 2018
- **Journal**: None
- **Summary**: Motivated by augmented and virtual reality applications such as telepresence, there has been a recent focus in real-time performance capture of humans under motion. However, given the real-time constraint, these systems often suffer from artifacts in geometry and texture such as holes and noise in the final rendering, poor lighting, and low-resolution textures. We take the novel approach to augment such real-time performance capture systems with a deep architecture that takes a rendering from an arbitrary viewpoint, and jointly performs completion, super resolution, and denoising of the imagery in real-time. We call this approach neural (re-)rendering, and our live system "LookinGood". Our deep architecture is trained to produce high resolution and high quality images from a coarse rendering in real-time. First, we propose a self-supervised training method that does not require manual ground-truth annotation. We contribute a specialized reconstruction error that uses semantic information to focus on relevant parts of the subject, e.g. the face. We also introduce a salient reweighing scheme of the loss function that is able to discard outliers. We specifically design the system for virtual and augmented reality headsets where the consistency between the left and right eye plays a crucial role in the final user experience. Finally, we generate temporally stable results by explicitly minimizing the difference between two consecutive frames. We tested the proposed system in two different scenarios: one involving a single RGB-D sensor, and upper body reconstruction of an actor, the second consisting of full body 360 degree capture. Through extensive experimentation, we demonstrate how our system generalizes across unseen sequences and subjects. The supplementary video is available at http://youtu.be/Md3tdAKoLGU.




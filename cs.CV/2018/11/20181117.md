# Arxiv Papers in cs.CV on 2018-11-17
### DSCnet: Replicating Lidar Point Clouds with Deep Sensor Cloning
- **Arxiv ID**: http://arxiv.org/abs/1811.07070v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07070v2)
- **Published**: 2018-11-17 01:03:37+00:00
- **Updated**: 2018-11-27 02:45:13+00:00
- **Authors**: Paden Tomasello, Sammy Sidhu, Anting Shen, Matthew W. Moskewicz, Nobie Redmon, Gayatri Joshi, Romi Phadte, Paras Jain, Forrest Iandola
- **Comment**: V2
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have become increasingly popular for solving a variety of computer vision tasks, ranging from image classification to image segmentation. Recently, autonomous vehicles have created a demand for depth information, which is often obtained using hardware sensors such as Light detection and ranging (LIDAR). Although it can provide precise distance measurements, most LIDARs are still far too expensive to sell in mass-produced consumer vehicles, which has motivated methods to generate depth information from commodity automotive sensors like cameras.   In this paper, we propose an approach called Deep Sensor Cloning (DSC). The idea is to use Convolutional Neural Networks in conjunction with inexpensive sensors to replicate the 3D point-clouds that are created by expensive LIDARs. To accomplish this, we develop a new dataset (DSDepth) and a new family of CNN architectures (DSCnets). While previous tasks such as KITTI depth prediction use an interpolated RGB-D images as ground-truth for training, we instead use DSCnets to directly predict LIDAR point-clouds. When we compare the output of our models to a $75,000 LIDAR, we find that our most accurate DSCnet achieves a relative error of 5.77% using a single camera and 4.69% using stereo cameras.



### Semi-Supervised Semantic Image Segmentation with Self-correcting Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.07073v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07073v3)
- **Published**: 2018-11-17 01:20:03+00:00
- **Updated**: 2020-02-26 04:58:15+00:00
- **Authors**: Mostafa S. Ibrahim, Arash Vahdat, Mani Ranjbar, William G. Macready
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Building a large image dataset with high-quality object masks for semantic segmentation is costly and time consuming. In this paper, we introduce a principled semi-supervised framework that only uses a small set of fully supervised images (having semantic segmentation labels and box labels) and a set of images with only object bounding box labels (we call it the weak set). Our framework trains the primary segmentation model with the aid of an ancillary model that generates initial segmentation labels for the weak set and a self-correction module that improves the generated labels during training using the increasingly accurate primary model. We introduce two variants of the self-correction module using either linear or convolutional functions. Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models trained with a small fully supervised set perform similar to, or better than, models trained with a large fully supervised set while requiring ~7x less annotation effort.



### Skeleton-based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module
- **Arxiv ID**: http://arxiv.org/abs/1811.07081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07081v2)
- **Published**: 2018-11-17 02:41:38+00:00
- **Updated**: 2018-12-07 13:38:26+00:00
- **Authors**: Chenyang Li, Xin Zhang, Lufan Liao, Lianwen Jin, Weixin Yang
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: The skeleton based gesture recognition is gaining more popularity due to its wide possible applications. The key issues are how to extract discriminative features and how to design the classification model. In this paper, we first leverage a robust feature descriptor, path signature (PS), and propose three PS features to explicitly represent the spatial and temporal motion characteristics, i.e., spatial PS (S_PS), temporal PS (T_PS) and temporal spatial PS (T_S_PS). Considering the significance of fine hand movements in the gesture, we propose an "attention on hand" (AOH) principle to define joint pairs for the S_PS and select single joint for the T_PS. In addition, the dyadic method is employed to extract the T_PS and T_S_PS features that encode global and local temporal dynamics in the motion. Secondly, without the recurrent strategy, the classification model still faces challenges on temporal variation among different sequences. We propose a new temporal transformer module (TTM) that can match the sequence key frames by learning the temporal shifting parameter for each input. This is a learning-based module that can be included into standard neural network architecture. Finally, we design a multi-stream fully connected layer based network to treat spatial and temporal features separately and fused them together for the final result. We have tested our method on three benchmark gesture datasets, i.e., ChaLearn 2016, ChaLearn 2013 and MSRC-12. Experimental results demonstrate that we achieve the state-of-the-art performance on skeleton-based gesture recognition with high computational efficiency.



### Hierarchical Bipartite Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.03813v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.03813v2)
- **Published**: 2018-11-17 02:43:59+00:00
- **Updated**: 2018-12-13 02:05:11+00:00
- **Authors**: Marcel Nassar
- **Comment**: Appeared in the Workshop on Relational Representation Learning (R2L)
  at NIPS 2018. 5 pages, 2 figures
- **Journal**: None
- **Summary**: Recently, graph neural networks have been adopted in a wide variety of applications ranging from relational representations to modeling irregular data domains such as point clouds and social graphs. However, the space of graph neural network architectures remains highly fragmented impeding the development of optimized implementations similar to what is available for convolutional neural networks. In this work, we present BiGraphNet, a graph neural network architecture that generalizes many popular graph neural network models and enables new efficient operations similar to those supported by ConvNets. By explicitly separating the input and output nodes, BiGraphNet: (i) generalizes the graph convolution to support new efficient operations such as coarsened graph convolutions (similar to strided convolution in convnets), multiple input graphs convolution and graph expansions (unpooling) which can be used to implement various graph architectures such as graph autoencoders, and graph residual nets; and (ii) accelerates and scales the computations and memory requirements in hierarchical networks by performing computations only at specified output nodes.



### PydMobileNet: Improved Version of MobileNets with Pyramid Depthwise Separable Convolution
- **Arxiv ID**: http://arxiv.org/abs/1811.07083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07083v1)
- **Published**: 2018-11-17 02:58:31+00:00
- **Updated**: 2018-11-17 02:58:31+00:00
- **Authors**: Van-Thanh Hoang, Kang-Hyun Jo
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown remarkable performance in various computer vision tasks in recent years. However, the increasing model size has raised challenges in adopting them in real-time applications as well as mobile and embedded vision applications. Many works try to build networks as small as possible while still have acceptable performance. The state-of-the-art architecture is MobileNets. They use Depthwise Separable Convolution (DWConvolution) in place of standard Convolution to reduce the size of networks. This paper describes an improved version of MobileNet, called Pyramid Mobile Network. Instead of using just a $3\times 3$ kernel size for DWConvolution like in MobileNet, the proposed network uses a pyramid kernel size to capture more spatial information. The proposed architecture is evaluated on two highly competitive object recognition benchmark datasets (CIFAR-10, CIFAR-100). The experiments demonstrate that the proposed network achieves better performance compared with MobileNet as well as other state-of-the-art networks. Additionally, it is more flexible in fine-tuning the trade-off between accuracy, latency and model size than MobileNets.



### Alternating Segmentation and Simulation for Contrast Adaptive Tissue Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.07087v1
- **DOI**: 10.1117/12.2295047
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07087v1)
- **Published**: 2018-11-17 03:28:50+00:00
- **Updated**: 2018-11-17 03:28:50+00:00
- **Authors**: Dzung L. Pham, Snehashis Roy
- **Comment**: Proceedings Volume 10578, Medical Imaging 2018: Biomedical
  Applications in Molecular, Structural, and Functional Imaging; SPIE Medical
  Imaging 2018
- **Journal**: None
- **Summary**: A key feature of magnetic resonance (MR) imaging is its ability to manipulate how the intrinsic tissue parameters of the anatomy ultimately contribute to the contrast properties of the final, acquired image. This flexibility, however, can lead to substantial challenges for segmentation algorithms, particularly supervised methods. These methods require atlases or training data, which are composed of MR image and labeled image pairs. In most cases, the training data are obtained with a fixed acquisition protocol, leading to suboptimal performance when an input data set that requires segmentation has differing contrast properties. This drawback is increasingly significant with the recent movement towards multi-center research studies involving multiple scanners and acquisition protocols. In this work, we propose a new framework for supervised segmentation approaches that is robust to contrast differences between the training MR image and the input image. Our approach uses a generative simulation model within the segmentation process to compensate for the contrast differences. We allow the contrast of the MR image in the training data to vary by simulating a new contrast from the corresponding label image. The model parameters are optimized by a cost function measuring the consistency between the input MR image and its simulation based on a current estimate of the segmentation labels. We provide a proof of concept of this approach by combining a supervised classifier with a simple simulation model, and apply the resulting algorithm to synthetic images and actual MR images.



### RelationNet2: Deep Comparison Columns for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.07100v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07100v3)
- **Published**: 2018-11-17 04:46:05+00:00
- **Updated**: 2020-04-28 02:12:18+00:00
- **Authors**: Xueting Zhang, Yuting Qiang, Flood Sung, Yongxin Yang, Timothy M. Hospedales
- **Comment**: 10 pages, 5 figures, Published in IJCNN 2020
- **Journal**: None
- **Summary**: Few-shot deep learning is a topical challenge area for scaling visual recognition to open ended growth of unseen new classes with limited labeled examples. A promising approach is based on metric learning, which trains a deep embedding to support image similarity matching. Our insight is that effective general purpose matching requires non-linear comparison of features at multiple abstraction levels. We thus propose a new deep comparison network comprised of embedding and relation modules that learn multiple non-linear distance metrics based on different levels of features simultaneously. Furthermore, to reduce over-fitting and enable the use of deeper embeddings, we represent images as distributions rather than vectors via learning parameterized Gaussian noise regularization. The resulting network achieves excellent performance on both miniImageNet and tieredImageNet.



### Cross-modality deep learning brings bright-field microscopy contrast to holography
- **Arxiv ID**: http://arxiv.org/abs/1811.07103v1
- **DOI**: 10.1038/s41377-019-0139-9
- **Categories**: **cs.CV**, cs.LG, physics.app-ph, 68T01, 68T05, 68U10, 62M45, 78M32, 92C55, 94A08, I.2, I.2.1, I.2.6, I.2.10, I.4.5, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1811.07103v1)
- **Published**: 2018-11-17 05:20:13+00:00
- **Updated**: 2018-11-17 05:20:13+00:00
- **Authors**: Yichen Wu, Yilin Luo, Gunvant Chaudhari, Yair Rivenson, Ayfer Calis, Kevin De Haan, Aydogan Ozcan
- **Comment**: 3 pages
- **Journal**: Light: Science & Applications (2019)
- **Summary**: Deep learning brings bright-field microscopy contrast to holographic images of a sample volume, bridging the volumetric imaging capability of holography with the speckle- and artifact-free image contrast of bright-field incoherent microscopy.



### On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs
- **Arxiv ID**: http://arxiv.org/abs/1811.07104v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07104v3)
- **Published**: 2018-11-17 05:52:13+00:00
- **Updated**: 2020-01-12 02:45:51+00:00
- **Authors**: Sandipan Banerjee, Walter J. Scheirer, Kevin W. Bowyer, Patrick J. Flynn
- **Comment**: Extended version of WACV 2020 paper
- **Journal**: None
- **Summary**: We propose a multi-scale GAN model to hallucinate realistic context (forehead, hair, neck, clothes) and background pixels automatically from a single input face mask. Instead of swapping a face on to an existing picture, our model directly generates realistic context and background pixels based on the features of the provided face mask. Unlike face inpainting algorithms, it can generate realistic hallucinations even for a large number of missing pixels. Our model is composed of a cascaded network of GAN blocks, each tasked with hallucination of missing pixels at a particular resolution while guiding the synthesis process of the next GAN block. The hallucinated full face image is made photo-realistic by using a combination of reconstruction, perceptual, adversarial and identity preserving losses at each block of the network. With a set of extensive experiments, we demonstrate the effectiveness of our model in hallucinating context and background pixels from face masks varying in facial pose, expression and lighting, collected from multiple datasets subject disjoint with our training data. We also compare our method with two popular face swapping and face completion methods in terms of visual quality and recognition performance. Additionally, we analyze our cascaded pipeline and compare it with the recently proposed progressive growing of GANs.



### Augmented LiDAR Simulator for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1811.07112v2
- **DOI**: 10.1109/LRA.2020.2969927
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07112v2)
- **Published**: 2018-11-17 07:09:13+00:00
- **Updated**: 2019-04-10 09:59:47+00:00
- **Authors**: Jin Fang, Dingfu Zhou, Feilong Yan, Tongtong Zhao, Feihu Zhang, Yu Ma, Liang Wang, Ruigang Yang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this paper, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., cars, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG models and game engines, our augmented simulator bypasses the requirement to create high-fidelity background CAD models. Instead, we can simply deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background point cloud, based on which annotated point cloud can be automatically generated. This unique "scan-and-simulate" capability makes our approach scalable and practical, ready for large-scale industrial applications. In this paper, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95% accuracy.



### Not just a matter of semantics: the relationship between visual similarity and semantic similarity
- **Arxiv ID**: http://arxiv.org/abs/1811.07120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07120v2)
- **Published**: 2018-11-17 08:00:41+00:00
- **Updated**: 2019-05-31 10:00:12+00:00
- **Authors**: Clemens-Alexander Brust, Joachim Denzler
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: Knowledge transfer, zero-shot learning and semantic image retrieval are methods that aim at improving accuracy by utilizing semantic information, e.g. from WordNet. It is assumed that this information can augment or replace missing visual data in the form of labeled training images because semantic similarity correlates with visual similarity. This assumption may seem trivial, but is crucial for the application of such semantic methods. Any violation can cause mispredictions. Thus, it is important to examine the visual-semantic relationship for a certain target problem. In this paper, we use five different semantic and visual similarity measures each to thoroughly analyze the relationship without relying too much on any single definition. We postulate and verify three highly consequential hypotheses on the relationship. Our results show that it indeed exists and that WordNet semantic similarity carries more information about visual similarity than just the knowledge of "different classes look different". They suggest that classification is not the ideal application for semantic methods and that wrong semantic information is much worse than none.



### Explicit Spatiotemporal Joint Relation Learning for Tracking Human Pose
- **Arxiv ID**: http://arxiv.org/abs/1811.07123v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07123v3)
- **Published**: 2018-11-17 08:12:50+00:00
- **Updated**: 2019-03-27 04:00:35+00:00
- **Authors**: Xiao Sun, Chuankang Li, Stephen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for human pose tracking that is based on learning spatiotemporal relationships among joints. Beyond generating the heatmap of a joint in a given frame, our system also learns to predict the offset of the joint from a neighboring joint in the frame. Additionally, it is trained to predict the displacement of the joint from its position in the previous frame, in a manner that can account for possibly changing joint appearance, unlike optical flow. These relational cues in the spatial domain and temporal domain are inferred in a robust manner by attending only to relevant areas in the video frames. By explicitly learning and exploiting these joint relationships, our system achieves state-of-the-art performance on standard benchmarks for various pose tracking tasks including 3D body pose tracking in RGB video, 3D hand pose tracking in depth sequences, and 3D hand gesture tracking in RGB video.



### VommaNet: an End-to-End Network for Disparity Estimation from Reflective and Texture-less Light Field Images
- **Arxiv ID**: http://arxiv.org/abs/1811.07124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07124v1)
- **Published**: 2018-11-17 08:13:17+00:00
- **Updated**: 2018-11-17 08:13:17+00:00
- **Authors**: Haoxin Ma, Haotian Li, Zhiwen Qian, Shengxian Shi, Tingting Mu
- **Comment**: None
- **Journal**: None
- **Summary**: The precise combination of image sensor and micro-lens array enables lenslet light field cameras to record both angular and spatial information of incoming light, therefore, one can calculate disparity and depth from light field images. In turn, 3D models of the recorded objects can be recovered, which is a great advantage over other imaging system. However, reflective and texture-less areas in light field images have complicated conditions, making it hard to correctly calculate disparity with existing algorithms. To tackle this problem, we introduce a novel end-to-end network VommaNet to retrieve multi-scale features from reflective and texture-less regions for accurate disparity estimation. Meanwhile, our network has achieved similar or better performance in other regions for both synthetic light field images and real-world data compared to the state-of-the-art algorithms. Currently, we achieve the best score for mean squared error (MSE) on HCI 4D Light Field Benchmark.



### Integrating domain knowledge: using hierarchies to improve deep classifiers
- **Arxiv ID**: http://arxiv.org/abs/1811.07125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07125v2)
- **Published**: 2018-11-17 08:23:32+00:00
- **Updated**: 2020-01-23 16:27:23+00:00
- **Authors**: Clemens-Alexander Brust, Joachim Denzler
- **Comment**: Accepted at ACPR 2019
- **Journal**: None
- **Summary**: One of the most prominent problems in machine learning in the age of deep learning is the availability of sufficiently large annotated datasets. For specific domains, e.g. animal species, a long-tail distribution means that some classes are observed and annotated insufficiently. Additional labels can be prohibitively expensive, e.g. because domain experts need to be involved. However, there is more information available that is to the best of our knowledge not exploited accordingly. In this paper, we propose to make use of preexisting class hierarchies like WordNet to integrate additional domain knowledge into classification. We encode the properties of such a class hierarchy into a probabilistic model. From there, we derive a novel label encoding and a corresponding loss function. On the ImageNet and NABirds datasets our method offers a relative improvement of 10.4% and 9.6% in accuracy over the baseline respectively. After less than a third of training time, it is already able to match the baseline's fine-grained recognition performance. Both results show that our suggested method is efficient and effective.



### SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects
- **Arxiv ID**: http://arxiv.org/abs/1811.07126v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07126v4)
- **Published**: 2018-11-17 08:24:25+00:00
- **Updated**: 2019-08-10 02:53:31+00:00
- **Authors**: Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Sun Xian, Kun Fu
- **Comment**: 10 pages, 10 figures, 6 tables, ICCV2019
- **Journal**: None
- **Summary**: Object detection has been a building block in computer vision. Though considerable progress has been made, there still exist challenges for objects with small size, arbitrary direction, and dense distribution. Apart from natural images, such issues are especially pronounced for aerial images of great importance. This paper presents a novel multi-category rotation detector for small, cluttered and rotated objects, namely SCRDet. Specifically, a sampling fusion network is devised which fuses multi-layer feature with effective anchor sampling, to improve the sensitivity to small objects. Meanwhile, the supervised pixel attention network and the channel attention network are jointly explored for small and cluttered object detection by suppressing the noise and highlighting the objects feature. For more accurate rotation estimation, the IoU constant factor is added to the smooth L1 loss to address the boundary problem for the rotating bounding box. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as natural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the state-of-the-art performance of our detector. The code and models will be available at https://github.com/DetectionTeamUCAS.



### Batch DropBlock Network for Person Re-identification and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1811.07130v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07130v3)
- **Published**: 2018-11-17 08:49:04+00:00
- **Updated**: 2023-02-10 08:38:01+00:00
- **Authors**: Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu, Ping Tan
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Since the person re-identification task often suffers from the problem of pose changes and occlusions, some attentive local features are often suppressed when training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network which is a two branch network composed of a conventional ResNet-50 as the global branch and a feature dropping branch. The global branch encodes the global salient representations. Meanwhile, the feature dropping branch consists of an attentive feature learning module called Batch DropBlock, which randomly drops the same region of all input feature maps in a batch to reinforce the attentive feature learning of local regions. The network then concatenates features from both branches and provides a more comprehensive and spatially distributed feature representation. Albeit simple, our method achieves state-of-the-art on person re-identification and it is also applicable to general metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existing works by a large margin (more than 6%).



### Recurrent Convolutions for Causal 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/1811.07157v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.07157v2)
- **Published**: 2018-11-17 13:07:30+00:00
- **Updated**: 2019-08-31 09:28:25+00:00
- **Authors**: Gurkirt Singh, Fabio Cuzzolin
- **Comment**: Workshop on Large Scale Holistic Video Understanding, ICCVW, 2019
- **Journal**: None
- **Summary**: Recently, three dimensional (3D) convolutional neural networks (CNNs) have emerged as dominant methods to capture spatiotemporal representations in videos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D CNNs, however, are anti-causal (i.e., they exploit information from both the past and the future frames to produce feature representations, thus preventing their use in online settings), constrain the temporal reasoning horizon to the size of the temporal convolution kernel, and are not temporal resolution-preserving for video sequence-to-sequence modelling, as, for instance, in action detection. To address these serious limitations, here we present a new 3D CNN architecture for the causal/online processing of videos.   Namely, we propose a novel Recurrent Convolutional Network (RCN), which relies on recurrence to capture the temporal context across frames at each network level. Our network decomposes 3D convolutions into (1) a 2D spatial convolution component, and (2) an additional hidden state $1\times 1$ convolution, applied across time. The hidden state at any time $t$ is assumed to depend on the hidden state at $t-1$ and on the current output of the spatial convolution component. As a result, the proposed network: (i) produces causal outputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal resolution. Our experiments on the large-scale large Kinetics and MultiThumos datasets show that the proposed method performs comparably to anti-causal 3D CNNs, while being causal and using fewer parameters.



### Edge-Based Blur Kernel Estimation Using Sparse Representation and Self-Similarity
- **Arxiv ID**: http://arxiv.org/abs/1811.07161v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07161v1)
- **Published**: 2018-11-17 14:00:37+00:00
- **Updated**: 2018-11-17 14:00:37+00:00
- **Authors**: Jing Yu, Zhenchun Chang, Chuangbai Xiao
- **Comment**: 26 pages, 10 figure
- **Journal**: None
- **Summary**: Blind image deconvolution is the problem of recovering the latent image from the only observed blurry image when the blur kernel is unknown. In this paper, we propose an edge-based blur kernel estimation method for blind motion deconvolution. In our previous work, we incorporate both sparse representation and self-similarity of image patches as priors into our blind deconvolution model to regularize the recovery of the latent image. Since almost any natural image has properties of sparsity and multi-scale self-similarity, we construct a sparsity regularizer and a cross-scale non-local regularizer based on our patch priors. It has been observed that our regularizers often favor sharp images over blurry ones only for image patches of the salient edges and thus we define an edge mask to locate salient edges that we want to apply our regularizers. Experimental results on both simulated and real blurry images demonstrate that our method outperforms existing state-of-the-art blind deblurring methods even for handling of very large blurs, thanks to the use of the edge mask.



### Optical Flow Dataset and Benchmark for Visual Crowd Analysis
- **Arxiv ID**: http://arxiv.org/abs/1811.07170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07170v1)
- **Published**: 2018-11-17 14:43:42+00:00
- **Updated**: 2018-11-17 14:43:42+00:00
- **Authors**: Gregory Schr√∂der, Tobias Senst, Erik Bochinski, Thomas Sikora
- **Comment**: submission to International Conference on Advanced Video and
  Signal-Based Surveillance
- **Journal**: None
- **Summary**: The performance of optical flow algorithms greatly depends on the specifics of the content and the application for which it is used. Existing and well established optical flow datasets are limited to rather particular contents from which none is close to crowd behavior analysis; whereas such applications heavily utilize optical flow. We introduce a new optical flow dataset exploiting the possibilities of a recent video engine to generate sequences with ground-truth optical flow for large crowds in different scenarios. We break with the development of the last decade of introducing ever increasing displacements to pose new difficulties. Instead we focus on real-world surveillance scenarios where numerous small, partly independent, non rigidly moving objects observed over a long temporal range pose a challenge. By evaluating different optical flow algorithms, we find that results of established datasets can not be transferred to these new challenges. In exhaustive experiments we are able to provide new insight into optical flow for crowd analysis. Finally, the results have been validated on the real-world UCF crowd tracking benchmark while achieving competitive results compared to more sophisticated state-of-the-art crowd tracking approaches.



### Person Identification and Body Mass Index: A Deep Learning-Based Study on Micro-Dopplers
- **Arxiv ID**: http://arxiv.org/abs/1811.07173v2
- **DOI**: 10.1109/RADAR.2019.8835652
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07173v2)
- **Published**: 2018-11-17 14:53:22+00:00
- **Updated**: 2019-02-26 13:47:38+00:00
- **Authors**: Sherif Abdulatif, Fady Aziz, Karim Armanious, Bernhard Kleiner, Bin Yang, Urs Schneider
- **Comment**: Accepted in IEEE Radarconf19
- **Journal**: None
- **Summary**: Obtaining a smart surveillance requires a sensing system that can capture accurate and detailed information for the human walking style. The radar micro-Doppler ($\boldsymbol{\mu}$-D) analysis is proved to be a reliable metric for studying human locomotions. Thus, $\boldsymbol{\mu}$-D signatures can be used to identify humans based on their walking styles. Additionally, the signatures contain information about the radar cross section (RCS) of the moving subject. This paper investigates the effect of human body characteristics on human identification based on their $\boldsymbol{\mu}$-D signatures. In our proposed experimental setup, a treadmill is used to collect $\boldsymbol{\mu}$-D signatures of 22 subjects with different genders and body characteristics. Convolutional autoencoders (CAE) are then used to extract the latent space representation from the $\boldsymbol{\mu}$-D signatures. It is then interpreted in two dimensions using t-distributed stochastic neighbor embedding (t-SNE). Our study shows that the body mass index (BMI) has a correlation with the $\boldsymbol{\mu}$-D signature of the walking subject. A 50-layer deep residual network is then trained to identify the walking subject based on the $\boldsymbol{\mu}$-D signature. We achieve an accuracy of 98% on the test set with high signal-to-noise-ratio (SNR) and 84% in case of different SNR levels.



### Stacking-Based Deep Neural Network: Deep Analytic Network for Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/1811.07184v2
- **DOI**: 10.1109/TCYB.2019.2908387
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07184v2)
- **Published**: 2018-11-17 16:19:14+00:00
- **Updated**: 2019-05-08 05:49:30+00:00
- **Authors**: Cheng-Yaw Low, Jaewoo Park, Andrew Beng-Jin Teoh
- **Comment**: 14 pages, 7 figures, 11 tables
- **Journal**: None
- **Summary**: Stacking-based deep neural network (S-DNN) is aggregated with pluralities of basic learning modules, one after another, to synthesize a deep neural network (DNN) alternative for pattern classification. Contrary to the DNNs trained end to end by backpropagation (BP), each S-DNN layer, i.e., a self-learnable module, is to be trained decisively and independently without BP intervention. In this paper, a ridge regression-based S-DNN, dubbed deep analytic network (DAN), along with its kernelization (K-DAN), are devised for multilayer feature re-learning from the pre-extracted baseline features and the structured features. Our theoretical formulation demonstrates that DAN/K-DAN re-learn by perturbing the intra/inter-class variations, apart from diminishing the prediction errors. We scrutinize the DAN/K-DAN performance for pattern classification on datasets of varying domains - faces, handwritten digits, generic objects, to name a few. Unlike the typical BP-optimized DNNs to be trained from gigantic datasets by GPU, we disclose that DAN/K-DAN are trainable using only CPU even for small-scale training sets. Our experimental results disclose that DAN/K-DAN outperform the present S-DNNs and also the BP-trained DNNs, including multiplayer perceptron, deep belief network, etc., without data augmentation applied.



### Sequential Image-based Attention Network for Inferring Force Estimation without Haptic Sensor
- **Arxiv ID**: http://arxiv.org/abs/1811.07190v4
- **DOI**: 10.1109/ACCESS.2019.2947090
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07190v4)
- **Published**: 2018-11-17 17:12:59+00:00
- **Updated**: 2019-10-20 11:26:21+00:00
- **Authors**: Hochul Shin, Hyeon Cho, Dongyi Kim, Daekwan Ko, Soochul Lim, Wonjun Hwang
- **Comment**: Accepted by IEEE Access on Oct. 08, 2019
- **Journal**: None
- **Summary**: Humans can infer approximate interaction force between objects from only vision information because we already have learned it through experiences. Based on this idea, we propose a recurrent convolutional neural network-based method using sequential images for inferring interaction force without using a haptic sensor. For training and validating deep learning methods, we collected a large number of images and corresponding interaction forces through an electronic motor-based device. To concentrate on changing shapes of a target object by the external force in images, we propose a sequential image-based attention module, which learns a salient model from temporal dynamics. The proposed sequential image-based attention module consists of a sequential spatial attention module and a sequential channel attention module, which are extended to exploit multiple sequential images. For gaining better accuracy, we also created a weighted average pooling layer for both spatial and channel attention modules. The extensive experimental results verified that the proposed method successfully infers interaction forces under the various conditions, such as different target materials, illumination changes, and external force directions.



### Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep Learning Transferable Examples
- **Arxiv ID**: http://arxiv.org/abs/1811.07211v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07211v2)
- **Published**: 2018-11-17 19:39:54+00:00
- **Updated**: 2018-11-20 18:55:55+00:00
- **Authors**: Jacob M. Springer, Charles S. Strauss, Austin M. Thresher, Edward Kim, Garrett T. Kenyon
- **Comment**: 8 pages, 8 figures, fixed typos
- **Journal**: None
- **Summary**: Although deep learning has shown great success in recent years, researchers have discovered a critical flaw where small, imperceptible changes in the input to the system can drastically change the output classification. These attacks are exploitable in nearly all of the existing deep learning classification frameworks. However, the susceptibility of deep sparse coding models to adversarial examples has not been examined. Here, we show that classifiers based on a deep sparse coding model whose classification accuracy is competitive with a variety of deep neural network models are robust to adversarial examples that effectively fool those same deep learning models. We demonstrate both quantitatively and qualitatively that the robustness of deep sparse coding models to adversarial examples arises from two key properties. First, because deep sparse coding models learn general features corresponding to generators of the dataset as a whole, rather than highly discriminative features for distinguishing specific classes, the resulting classifiers are less dependent on idiosyncratic features that might be more easily exploited. Second, because deep sparse coding models utilize fixed point attractor dynamics with top-down feedback, it is more difficult to find small changes to the input that drive the resulting representations out of the correct attractor basin.



### Revisiting Image-Language Networks for Open-ended Phrase Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.07212v3
- **DOI**: 10.1109/TPAMI.2020.3029008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07212v3)
- **Published**: 2018-11-17 19:45:05+00:00
- **Updated**: 2020-10-12 19:13:55+00:00
- **Authors**: Bryan A. Plummer, Kevin J. Shih, Yichen Li, Ke Xu, Svetlana Lazebnik, Stan Sclaroff, Kate Saenko
- **Comment**: Accepted to TPAMI
- **Journal**: None
- **Summary**: Most existing work that grounds natural language phrases in images starts with the assumption that the phrase in question is relevant to the image. In this paper we address a more realistic version of the natural language grounding task where we must both identify whether the phrase is relevant to an image and localize the phrase. This can also be viewed as a generalization of object detection to an open-ended vocabulary, introducing elements of few- and zero-shot detection. We propose an approach for this task that extends Faster R-CNN to relate image regions and phrases. By carefully initializing the classification layers of our network using canonical correlation analysis (CCA), we encourage a solution that is more discerning when reasoning between similar phrases, resulting in over double the performance compared to a naive adaptation on three popular phrase grounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, with test-time phrase vocabulary sizes of 5K, 32K, and 159K, respectively.



### GroundNet: Monocular Ground Plane Normal Estimation with Geometric Consistency
- **Arxiv ID**: http://arxiv.org/abs/1811.07222v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07222v4)
- **Published**: 2018-11-17 21:25:53+00:00
- **Updated**: 2019-08-10 01:59:14+00:00
- **Authors**: Yunze Man, Xinshuo Weng, Xi Li, Kris Kitani
- **Comment**: Camera Ready for ACM MM 2019
- **Journal**: None
- **Summary**: We focus on estimating the 3D orientation of the ground plane from a single image. We formulate the problem as an inter-mingled multi-task prediction problem by jointly optimizing for pixel-wise surface normal direction, ground plane segmentation, and depth estimates. Specifically, our proposed model, GroundNet, first estimates the depth and surface normal in two separate streams, from which two ground plane normals are then computed deterministically. To leverage the geometric correlation between depth and normal, we propose to add a consistency loss on top of the computed ground plane normals. In addition, a ground segmentation stream is used to isolate the ground regions so that we can selectively back-propagate parameter updates through only the ground regions in the image. Our method achieves the top-ranked performance on ground plane normal estimation and horizon line detection on the real-world outdoor datasets of ApolloScape and KITTI, improving the performance of previous art by up to 17.7% relatively.



### PointConv: Deep Convolutional Networks on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1811.07246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.07246v3)
- **Published**: 2018-11-17 23:42:13+00:00
- **Updated**: 2020-11-09 21:20:22+00:00
- **Authors**: Wenxuan Wu, Zhongang Qi, Li Fuxin
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Unlike images which are represented in regular dense grids, 3D point clouds are irregular and unordered, hence applying convolution on them can be difficult. In this paper, we extend the dynamic filter to a new convolution operation, named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point, the weight functions are learned with multi-layer perceptron networks and density functions through kernel density estimation. The most important contribution of this work is a novel reformulation proposed for efficiently computing the weight functions, which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides, PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.




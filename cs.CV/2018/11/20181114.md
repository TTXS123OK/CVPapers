# Arxiv Papers in cs.CV on 2018-11-14
### YOLO-LITE: A Real-Time Object Detection Algorithm Optimized for Non-GPU Computers
- **Arxiv ID**: http://arxiv.org/abs/1811.05588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05588v1)
- **Published**: 2018-11-14 01:20:08+00:00
- **Updated**: 2018-11-14 01:20:08+00:00
- **Authors**: Jonathan Pedoeem, Rachel Huang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on YOLO-LITE, a real-time object detection model developed to run on portable devices such as a laptop or cellphone lacking a Graphics Processing Unit (GPU). The model was first trained on the PASCAL VOC dataset then on the COCO dataset, achieving a mAP of 33.81% and 12.26% respectively. YOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after implemented onto a website with only 7 layers and 482 million FLOPS. This speed is 3.8x faster than the fastest state of art model, SSD MobilenetvI. Based on the original object detection algorithm YOLOV2, YOLO- LITE was designed to create a smaller, faster, and more efficient model increasing the accessibility of real-time object detection to a variety of devices.



### Style and Content Disentanglement in Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.05621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05621v1)
- **Published**: 2018-11-14 03:25:07+00:00
- **Updated**: 2018-11-14 03:25:07+00:00
- **Authors**: Hadi Kazemi, Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: Disentangling factors of variation within data has become a very challenging problem for image generation tasks. Current frameworks for training a Generative Adversarial Network (GAN), learn to disentangle the representations of the data in an unsupervised fashion and capture the most significant factors of the data variations. However, these approaches ignore the principle of content and style disentanglement in image generation, which means their learned latent code may alter the content and style of the generated images at the same time. This paper describes the Style and Content Disentangled GAN (SC-GAN), a new unsupervised algorithm for training GANs that learns disentangled style and content representations of the data. We assume that the representation of an image can be decomposed into a content code that represents the geometrical information of the data, and a style code that captures textural properties. Consequently, by fixing the style portion of the latent representation, we can generate diverse images in a particular style. Reversely, we can set the content code and generate a specific scene in a variety of styles. The proposed SC-GAN has two components: a content code which is the input to the generator, and a style code which modifies the scene style through modification of the Adaptive Instance Normalization (AdaIN) layers' parameters. We evaluate the proposed SC-GAN framework on a set of baseline datasets.



### Model-guided Multi-path Knowledge Aggregation for Aerial Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1811.05625v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05625v2)
- **Published**: 2018-11-14 03:56:01+00:00
- **Updated**: 2020-06-08 06:53:48+00:00
- **Authors**: Kui Fu, Jia Li, Yu Zhang, Hongze Shen, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: As an emerging vision platform, a drone can look from many abnormal viewpoints which brings many new challenges into the classic vision task of video saliency prediction. To investigate these challenges, this paper proposes a large-scale video dataset for aerial saliency prediction, which consists of ground-truth salient object regions of 1,000 aerial videos, annotated by 24 subjects. To the best of our knowledge, it is the first large-scale video dataset that focuses on visual saliency prediction on drones. Based on this dataset, we propose a Model-guided Multi-path Network (MM-Net) that serves as a baseline model for aerial video saliency prediction. Inspired by the annotation process in eye-tracking experiments, MM-Net adopts multiple information paths, each of which is initialized under the guidance of a classic saliency model. After that, the visual saliency knowledge encoded in the most representative paths is selected and aggregated to improve the capability of MM-Net in predicting spatial saliency in aerial scenarios. Finally, these spatial predictions are adaptively combined with the temporal saliency predictions via a spatiotemporal optimization algorithm. Experimental results show that MM-Net outperforms ten state-of-the-art models in predicting aerial video saliency.



### SUGAMAN: Describing Floor Plans for Visually Impaired by Annotation Learning and Proximity based Grammar
- **Arxiv ID**: http://arxiv.org/abs/1812.00874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.00874v1)
- **Published**: 2018-11-14 05:38:40+00:00
- **Updated**: 2018-11-14 05:38:40+00:00
- **Authors**: Shreya Goyal, Satya Bhavsar, Shreya Patel, Chiranjoy Chattopadhyay, Gaurav Bhatnagar
- **Comment**: 19 pages, 20 figures, Under review in IET Image Processing
- **Journal**: None
- **Summary**: In this paper, we propose SUGAMAN (Supervised and Unified framework using Grammar and Annotation Model for Access and Navigation). SUGAMAN is a Hindi word meaning "easy passage from one place to another". SUGAMAN synthesizes textual description from a given floor plan image for the visually impaired. A visually impaired person can navigate in an indoor environment using the textual description generated by SUGAMAN. With the help of a text reader software, the target user can understand the rooms within the building and arrangement of furniture to navigate. SUGAMAN is the first framework for describing a floor plan and giving direction for obstacle-free movement within a building. We learn $5$ classes of room categories from $1355$ room image samples under a supervised learning paradigm. These learned annotations are fed into a description synthesis framework to yield a holistic description of a floor plan image. We demonstrate the performance of various supervised classifiers on room learning. We also provide a comparative analysis of system generated and human written descriptions. SUGAMAN gives state of the art performance on challenging, real-world floor plan images. This work can be applied to areas like understanding floor plans of historical monuments, stability analysis of buildings, and retrieval.



### A Radiomics Approach to Traumatic Brain Injury Prediction in CT Scans
- **Arxiv ID**: http://arxiv.org/abs/1811.05699v1
- **DOI**: 10.1109/ISBI.2019.8759229
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.05699v1)
- **Published**: 2018-11-14 09:29:29+00:00
- **Updated**: 2018-11-14 09:29:29+00:00
- **Authors**: Ezequiel de la Rosa, Diana M. Sima, Thijs Vande Vyvere, Jan S. Kirschke, Bjoern Menze
- **Comment**: Submitted to ISBI 2019
- **Journal**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019) (pp. 732-735). IEEE
- **Summary**: Computer Tomography (CT) is the gold standard technique for brain damage evaluation after acute Traumatic Brain Injury (TBI). It allows identification of most lesion types and determines the need of surgical or alternative therapeutic procedures. However, the traditional approach for lesion classification is restricted to visual image inspection. In this work, we characterize and predict TBI lesions by using CT-derived radiomics descriptors. Relevant shape, intensity and texture biomarkers characterizing the different lesions are isolated and a lesion predictive model is built by using Partial Least Squares. On a dataset containing 155 scans (105 train, 50 test) the methodology achieved 89.7 % accuracy over the unseen data. When a model was build using only texture features, a 88.2 % accuracy was obtained. Our results suggest that selected radiomics descriptors could play a key role in brain injury prediction. Besides, the proposed methodology is close to reproduce radiologists decision making. These results open new possibilities for radiomics-inspired brain lesion detection, segmentation and prediction.



### LoANs: Weakly Supervised Object Detection with Localizer Assessor Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.05773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05773v2)
- **Published**: 2018-11-14 13:45:54+00:00
- **Updated**: 2018-11-15 15:55:55+00:00
- **Authors**: Christian Bartz, Haojin Yang, Joseph Bethge, Christoph Meinel
- **Comment**: To appear in AMV18. Code, datasets and models available at
  https://github.com/Bartzi/loans
- **Journal**: None
- **Summary**: Recently, deep neural networks have achieved remarkable performance on the task of object detection and recognition. The reason for this success is mainly grounded in the availability of large scale, fully annotated datasets, but the creation of such a dataset is a complicated and costly task. In this paper, we propose a novel method for weakly supervised object detection that simplifies the process of gathering data for training an object detector. We train an ensemble of two models that work together in a student-teacher fashion. Our student (localizer) is a model that learns to localize an object, the teacher (assessor) assesses the quality of the localization and provides feedback to the student. The student uses this feedback to learn how to localize objects and is thus entirely supervised by the teacher, as we are using no labels for training the localizer. In our experiments, we show that our model is very robust to noise and reaches competitive performance compared to a state-of-the-art fully supervised approach. We also show the simplicity of creating a new dataset, based on a few videos (e.g. downloaded from YouTube) and artificially generated data.



### Creatures great and SMAL: Recovering the shape and motion of animals from video
- **Arxiv ID**: http://arxiv.org/abs/1811.05804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05804v1)
- **Published**: 2018-11-14 14:24:07+00:00
- **Updated**: 2018-11-14 14:24:07+00:00
- **Authors**: Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon, Roberto Cipolla
- **Comment**: 17 pages, ACCV 2018 oral paper
- **Journal**: None
- **Summary**: We present a system to recover the 3D shape and motion of a wide variety of quadrupeds from video. The system comprises a machine learning front-end which predicts candidate 2D joint positions, a discrete optimization which finds kinematically plausible joint correspondences, and an energy minimization stage which fits a detailed 3D model to the image. In order to overcome the limited availability of motion capture training data from animals, and the difficulty of generating realistic synthetic training images, the system is designed to work on silhouette data. The joint candidate predictor is trained on synthetically generated silhouette images, and at test time, deep learning methods or standard video segmentation tools are used to extract silhouettes from real data. The system is tested on animal videos from several species, and shows accurate reconstructions of 3D shape and pose.



### ProstateGAN: Mitigating Data Bias via Prostate Diffusion Imaging Synthesis with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1811.05817v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1811.05817v2)
- **Published**: 2018-11-14 14:44:42+00:00
- **Updated**: 2018-11-21 01:35:54+00:00
- **Authors**: Xiaodan Hu, Audrey G. Chung, Paul Fieguth, Farzad Khalvati, Masoom A. Haider, Alexander Wong
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown considerable promise for mitigating the challenge of data scarcity when building machine learning-driven analysis algorithms. Specifically, a number of studies have shown that GAN-based image synthesis for data augmentation can aid in improving classification accuracy in a number of medical image analysis tasks, such as brain and liver image analysis. However, the efficacy of leveraging GANs for tackling prostate cancer analysis has not been previously explored. Motivated by this, in this study we introduce ProstateGAN, a GAN-based model for synthesizing realistic prostate diffusion imaging data. More specifically, in order to generate new diffusion imaging data corresponding to a particular cancer grade (Gleason score), we propose a conditional deep convolutional GAN architecture that takes Gleason scores into consideration during the training process. Experimental results show that high-quality synthetic prostate diffusion imaging data can be generated using the proposed ProstateGAN for specified Gleason scores.



### On the use of FHT, its modification for practical applications and the structure of Hough image
- **Arxiv ID**: http://arxiv.org/abs/1811.06378v1
- **DOI**: 10.1117/12.2522803
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.06378v1)
- **Published**: 2018-11-14 14:49:35+00:00
- **Updated**: 2018-11-14 14:49:35+00:00
- **Authors**: M. Aliev, E. I. Ershov, D. P. Nikolaev
- **Comment**: 8 pages, 8 figures. Submitted and presented at ICMV 2018
- **Journal**: None
- **Summary**: This work focuses on the Fast Hough Transform (FHT) algorithm proposed by M.L. Brady. We propose how to modify the standard FHT to calculate sums along lines within any given range of their inclination angles. We also describe a new way to visualise Hough-image based on regrouping of accumulator space around its center. Finally, we prove that using Brady parameterization transforms any line into a figure of type "angle".



### Distortion Robust Image Classification using Deep Convolutional Neural Network with Discrete Cosine Transform
- **Arxiv ID**: http://arxiv.org/abs/1811.05819v4
- **DOI**: 10.1109/ICIP.2019.8803787
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05819v4)
- **Published**: 2018-11-14 14:52:06+00:00
- **Updated**: 2020-08-06 09:32:41+00:00
- **Authors**: Md Tahmid Hossain, Shyh Wei Teng, Dengsheng Zhang, Suryani Lim, Guojun Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network is good at image classification. However, it is found to be vulnerable to image quality degradation. Even a small amount of distortion such as noise or blur can severely hamper the performance of these CNN architectures. Most of the work in the literature strives to mitigate this problem simply by fine-tuning a pre-trained CNN on mutually exclusive or a union set of distorted training data. This iterative fine-tuning process with all known types of distortion is exhaustive and the network struggles to handle unseen distortions. In this work, we propose distortion robust DCT-Net, a Discrete Cosine Transform based module integrated into a deep network which is built on top of VGG16. Unlike other works in the literature, DCT-Net is "blind" to the distortion type and level in an image both during training and testing. As a part of the training process, the proposed DCT module discards input information which mostly represents the contribution of high frequencies. The DCT-Net is trained "blindly" only once and applied in generic situation without further retraining. We also extend the idea of traditional dropout and present a training adaptive version of the same. We evaluate our proposed method against Gaussian blur, motion blur, salt and pepper noise, Gaussian noise and speckle noise added to CIFAR-10/100 and ImageNet test sets. Experimental results demonstrate that once trained, DCT-Net not only generalizes well to a variety of unseen image distortions but also outperforms other methods in the literature.



### Robust low-rank multilinear tensor approximation for a joint estimation of the multilinear rank and the loading matrices
- **Arxiv ID**: http://arxiv.org/abs/1811.05863v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1811.05863v3)
- **Published**: 2018-11-14 15:46:34+00:00
- **Updated**: 2021-08-21 08:10:52+00:00
- **Authors**: Xu Han, Laurent Albera, Amar Kachenoura, Huazhong Shu, Lotfi Senhadji
- **Comment**: Some contents should be reconsidered
- **Journal**: None
- **Summary**: In order to compute the best low-rank tensor approximation using the Multilinear Tensor Decomposition (MTD) model, it is essential to estimate the rank of the underlying multilinear tensor from the noisy observation tensor. In this paper, we propose a Robust MTD (R-MTD) method, which jointly estimates the multilinear rank and the loading matrices. Based on the low-rank property and an over-estimation of the core tensor, this joint estimation problem is solved by promoting (group) sparsity of the over-estimated core tensor. Group sparsity is promoted using mixed-norms. Then we establish a link between the mixed-norms and the nuclear norm, showing that mixed-norms are better candidates for a convex envelope of the rank. After several iterations of the Alternating Direction Method of Multipliers (ADMM), the Minimum Description Length (MDL) criterion computed from the eigenvalues of the unfolding matrices of the estimated core tensor is minimized in order to estimate the multilinear rank. The latter is then used to estimate more accurately the loading matrices. We further develop another R-MTD method, called R-OMTD, by imposing an orthonormality constraint on each loading matrix in order to decrease the computation complexity. A series of simulated noisy tensor and real-world data are used to show the effectiveness of the proposed methods compared with state-of-the-art methods.



### Development of Real-time ADAS Object Detector for Deployment on CPU
- **Arxiv ID**: http://arxiv.org/abs/1811.05894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05894v1)
- **Published**: 2018-11-14 16:37:09+00:00
- **Updated**: 2018-11-14 16:37:09+00:00
- **Authors**: Alexander Kozlov, Daniil Osokin
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we outline the set of problems, which any Object Detection CNN faces when its development comes to the deployment stage and propose methods to deal with such difficulties. We show that these practices allow one to get Object Detection network, which can recognize two classes: vehicles and pedestrians and achieves more than 60 frames per second inference speed on Core$^{TM}$ i5-6500 CPU. The proposed model is built on top of the popular Single Shot MultiBox Object Detection framework but with substantial improvements, which were inspired by the discovered problems. The network has just 1.96 GMAC complexity and less than 7 MB model size. It is publicly available as a part of Intel$\circledR$ OpenVINO$^{TM}$ Toolkit.



### The Greedy Dirichlet Process Filter - An Online Clustering Multi-Target Tracker
- **Arxiv ID**: http://arxiv.org/abs/1811.05911v2
- **DOI**: 10.1109/GlobalSIP.2018.8646554
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05911v2)
- **Published**: 2018-11-14 17:09:22+00:00
- **Updated**: 2019-03-08 17:20:41+00:00
- **Authors**: Benjamin Naujoks, Patrick Burger, Hans-Joachim Wuensche
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable collision avoidance is one of the main requirements for autonomous driving. Hence, it is important to correctly estimate the states of an unknown number of static and dynamic objects in real-time. Here, data association is a major challenge for every multi-target tracker. We propose a novel multi-target tracker called Greedy Dirichlet Process Filter (GDPF) based on the non-parametric Bayesian model called Dirichlet Processes and the fast posterior computation algorithm Sequential Updating and Greedy Search (SUGS). By adding a temporal dependence we get a real-time capable tracking framework without the need of a previous clustering or data association step. Real-world tests show that GDPF outperforms other multi-target tracker in terms of accuracy and stability.



### Generating a Training Dataset for Land Cover Classification to Advance Global Development
- **Arxiv ID**: http://arxiv.org/abs/1811.07998v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.07998v1)
- **Published**: 2018-11-14 17:50:46+00:00
- **Updated**: 2018-11-14 17:50:46+00:00
- **Authors**: Yoni Nachmany, Hamed Alemohammad
- **Comment**: Presented at NIPS 2018 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Semantic segmentation of land cover classes is fundamental for agricultural and economic development work, from sustainable forestry to urban planning, yet existing training datasets have significant limitations. To generate an open and comprehensive training library of high resolution Earth imagery and high quality land cover classifications, public Sentinel-2 data at 10 m spatial resolution was matched with accurate GlobeLand30 labels from 2010, which were filtered by agreement with an intermediary Sentinel-2 classification at 20 m produced during atmospheric correction. Scene-level classifications were predicted by Random Forests trained on valid reflectance data and the filtered labels, and achieved over 80% model accuracy for a variety of locations. Further work is required to aggregate individual scene classifications for annual labels and to test the approach in more locations, before crowdsourcing human validation. The goal is to create a sustained community-wide effort to generate image labels not only for land cover, but also very specific images for major agriculture crops across the world and other thematic categories of interest to the global development community.



### Domain Randomization for Scene-Specific Car Detection and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1811.05939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05939v1)
- **Published**: 2018-11-14 18:13:34+00:00
- **Updated**: 2018-11-14 18:13:34+00:00
- **Authors**: Rawal Khirodkar, Donghyun Yoo, Kris M. Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: We address the issue of domain gap when making use of synthetic data to train a scene-specific object detector and pose estimator. While previous works have shown that the constraints of learning a scene-specific model can be leveraged to create geometrically and photometrically consistent synthetic data, care must be taken to design synthetic content which is as close as possible to the real-world data distribution. In this work, we propose to solve domain gap through the use of appearance randomization to generate a wide range of synthetic objects to span the space of realistic images for training. An ablation study of our results is presented to delineate the individual contribution of different components in the randomization process. We evaluate our method on VIRAT, UA-DETRAC, EPFL-Car datasets, where we demonstrate that using scene specific domain randomized synthetic data is better than fine-tuning off-the-shelf models on limited real data.



### No-Frills Human-Object Interaction Detection: Factorization, Layout Encodings, and Training Techniques
- **Arxiv ID**: http://arxiv.org/abs/1811.05967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.05967v2)
- **Published**: 2018-11-14 18:58:14+00:00
- **Updated**: 2019-08-22 17:58:17+00:00
- **Authors**: Tanmay Gupta, Alexander Schwing, Derek Hoiem
- **Comment**: Accepted to ICCV 2019. Project Page:
  http://tanmaygupta.info/no_frills/
- **Journal**: None
- **Summary**: We show that for human-object interaction detection a relatively simple factorized model with appearance and layout encodings constructed from pre-trained object detectors outperforms more sophisticated approaches. Our model includes factors for detection scores, human and object appearance, and coarse (box-pair configuration) and optionally fine-grained layout (human pose). We also develop training techniques that improve learning efficiency by: (1) eliminating a train-inference mismatch; (2) rejecting easy negatives during mini-batch training; and (3) using a ratio of negatives to positives that is two orders of magnitude larger than existing approaches. We conduct a thorough ablation study to understand the importance of different factors and training techniques using the challenging HICO-Det dataset.



### Performance Estimation of Synthesis Flows cross Technologies using LSTMs and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1811.06017v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06017v1)
- **Published**: 2018-11-14 19:17:14+00:00
- **Updated**: 2018-11-14 19:17:14+00:00
- **Authors**: Cunxi Yu, Wang Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the increasing complexity of Integrated Circuits (ICs) and System-on-Chip (SoC), developing high-quality synthesis flows within a short market time becomes more challenging. We propose a general approach that precisely estimates the Quality-of-Result (QoR), such as delay and area, of unseen synthesis flows for specific designs. The main idea is training a Recurrent Neural Network (RNN) regressor, where the flows are inputs and QoRs are ground truth. The RNN regressor is constructed with Long Short-Term Memory (LSTM) and fully-connected layers. This approach is demonstrated with 1.2 million data points collected using 14nm, 7nm regular-voltage (RVT), and 7nm low-voltage (LVT) FinFET technologies with twelve IC designs. The accuracy of predicting the QoRs (delay and area) within one technology is $\boldsymbol{\geq}$\textbf{98.0}\% over $\sim$240,000 test points. To enable accurate predictions cross different technologies and different IC designs, we propose a transfer-learning approach that utilizes the model pre-trained with 14nm datasets. Our transfer learning approach obtains estimation accuracy $\geq$96.3\% over $\sim$960,000 test points, using only 100 data points for training.



### Focus Quality Assessment of High-Throughput Whole Slide Imaging in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1811.06038v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.06038v1)
- **Published**: 2018-11-14 20:07:26+00:00
- **Updated**: 2018-11-14 20:07:26+00:00
- **Authors**: Mahdi S. Hosseini, Yueyang Zhang, Lyndon Chan, Konstantinos N. Plataniotis, Jasper A. Z. Brawley-Hayes, Savvas Damaskinos
- **Comment**: 10 pages, This work has been submitted to the IEEE for possible
  publication
- **Journal**: None
- **Summary**: One of the challenges facing the adoption of digital pathology workflows for clinical use is the need for automated quality control. As the scanners sometimes determine focus inaccurately, the resultant image blur deteriorates the scanned slide to the point of being unusable. Also, the scanned slide images tend to be extremely large when scanned at greater or equal 20X image resolution. Hence, for digital pathology to be clinically useful, it is necessary to use computational tools to quickly and accurately quantify the image focus quality and determine whether an image needs to be re-scanned. We propose a no-reference focus quality assessment metric specifically for digital pathology images, that operates by using a sum of even-derivative filter bases to synthesize a human visual system-like kernel, which is modeled as the inverse of the lens' point spread function. This kernel is then applied to a digital pathology image to modify high-frequency image information deteriorated by the scanner's optics and quantify the focus quality at the patch level. We show in several experiments that our method correlates better with ground-truth $z$-level data than other methods, and is more computationally efficient. We also extend our method to generate a local slide-level focus quality heatmap, which can be used for automated slide quality control, and demonstrate the utility of our method for clinical scan quality control by comparison with subjective slide quality scores.



### Unsupervised domain adaptation for medical imaging segmentation with self-ensembling
- **Arxiv ID**: http://arxiv.org/abs/1811.06042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06042v2)
- **Published**: 2018-11-14 20:18:13+00:00
- **Updated**: 2019-01-10 22:38:53+00:00
- **Authors**: Christian S. Perone, Pedro Ballester, Rodrigo C. Barros, Julien Cohen-Adad
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: Recent advances in deep learning methods have come to define the state-of-the-art for many medical imaging applications, surpassing even human judgment in several tasks. Those models, however, when trained to reduce the empirical risk on a single domain, fail to generalize when applied to other domains, a very common scenario in medical imaging due to the variability of images and anatomical structures, even across the same imaging modality. In this work, we extend the method of unsupervised domain adaptation using self-ensembling for the semantic segmentation task and explore multiple facets of the method on a small and realistic publicly-available magnetic resonance (MRI) dataset. Through an extensive evaluation, we show that self-ensembling can indeed improve the generalization of the models even when using a small amount of unlabelled data.



### Looking at the Driver/Rider in Autonomous Vehicles to Predict Take-Over Readiness
- **Arxiv ID**: http://arxiv.org/abs/1811.06047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06047v1)
- **Published**: 2018-11-14 20:29:37+00:00
- **Updated**: 2018-11-14 20:29:37+00:00
- **Authors**: Nachiket Deo, Mohan M. Trivedi
- **Comment**: Submitted to IEEE transactions on Intelligent Vehicles
- **Journal**: None
- **Summary**: Continuous estimation the driver's take-over readiness is critical for safe and timely transfer of control during the failure modes of autonomous vehicles. In this paper, we propose a data-driven approach for estimating the driver's take-over readiness based purely on observable cues from in-vehicle vision sensors. We present an extensive naturalistic drive dataset of drivers in a conditionally autonomous vehicle running on Californian freeways. We collect subjective ratings for the driver's take-over readiness from multiple human observers viewing the sensor feed. Analysis of the ratings in terms of intra-class correlation coefficients (ICCs) shows a high degree of consistency in the ratings across raters. We define a metric for the driver's take-over readiness termed the 'Observable Readiness Index (ORI)' based on the ratings. Finally, we propose an LSTM model for continuous estimation of the driver's ORI based on a holistic representation of the driver's state, capturing gaze, hand, pose and foot activity. Our model estimates the ORI with a mean absolute error of 0.449 on a 5 point scale.



### A multi-level convolutional LSTM model for the segmentation of left ventricle myocardium in infarcted porcine cine MR images
- **Arxiv ID**: http://arxiv.org/abs/1811.06051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06051v1)
- **Published**: 2018-11-14 20:37:53+00:00
- **Updated**: 2018-11-14 20:37:53+00:00
- **Authors**: Dongqing Zhang, Ilknur Icke, Belma Dogdas, Sarayu Parimal, Smita Sampath, Joseph Forbes, Ansuman Bagchi, Chih-Liang Chin, Antong Chen
- **Comment**: 4 pages, 3 figures, 1 table. IEEE International Symposium on
  Biomedical Imaging (ISBI) 2018
- **Journal**: None
- **Summary**: Automatic segmentation of left ventricle (LV) myocardium in cardiac short-axis cine MR images acquired on subjects with myocardial infarction is a challenging task, mainly because of the various types of image inhomogeneity caused by the infarctions. Among the approaches proposed to automate the LV myocardium segmentation task, methods based upon deep convolutional neural networks (CNN) have demonstrated their exceptional accuracy and robustness in recent years. However, most of the CNN-based approaches treat the frames in a cardiac cycle independently, which fails to capture the valuable dynamics of heart motion. Herein, an approach based on recurrent neural network (RNN), specifically a multi-level convolutional long short-term memory (ConvLSTM) model, is proposed to take the motion of the heart into consideration. Based on a ResNet-56 CNN, LV-related image features in consecutive frames of a cardiac cycle are extracted at both the low- and high-resolution levels, which are processed by the corresponding multi-level ConvLSTM models to generate the myocardium segmentations. A leave-one-out experiment was carried out on a set of 3,600 cardiac cine MR slices collected in-house for 8 porcine subjects with surgically induced myocardial infarction. Compared with a solely CNN-based approach, the proposed approach demonstrated its superior robustness against image inhomogeneity by incorporating information from adjacent frames. It also outperformed a one-level ConvLSTM approach thanks to its capabilities to take advantage of image features at multiple resolution levels.



### Spatial Logics and Model Checking for Medical Imaging (Extended Version)
- **Arxiv ID**: http://arxiv.org/abs/1811.06065v1
- **DOI**: None
- **Categories**: **cs.LO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1811.06065v1)
- **Published**: 2018-11-14 21:02:31+00:00
- **Updated**: 2018-11-14 21:02:31+00:00
- **Authors**: Fabrizio Banci Buonamici, Gina Belmonte, Vincenzo Ciancia, Diego Latella, Mieke Massink
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research on spatial and spatio-temporal model checking provides novel image analysis methodologies, rooted in logical methods for topological spaces. Medical Imaging (MI) is a field where such methods show potential for ground-breaking innovation. Our starting point is SLCS, the Spatial Logic for Closure Spaces -- Closure Spaces being a generalisation of topological spaces, covering also discrete space structures -- and topochecker, a model-checker for SLCS (and extensions thereof). We introduce the logical language ImgQL ("Image Query Language"). ImgQL extends SLCS with logical operators describing distance and region similarity. The spatio-temporal model checker topochecker is correspondingly enhanced with state-of-the-art algorithms, borrowed from computational image processing, for efficient implementation of distancebased operators, namely distance transforms. Similarity between regions is defined by means of a statistical similarity operator, based on notions from statistical texture analysis. We illustrate our approach by means of two examples of analysis of Magnetic Resonance images: segmentation of glioblastoma and its oedema, and segmentation of rectal carcinoma.



### Interpretable deep learning for guided structure-property explorations in photovoltaics
- **Arxiv ID**: http://arxiv.org/abs/1811.06067v3
- **DOI**: 10.1038/s41524-019-0231-y
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06067v3)
- **Published**: 2018-11-14 21:08:08+00:00
- **Updated**: 2018-12-12 02:14:14+00:00
- **Authors**: Balaji Sesha Sarath Pokuri, Sambuddha Ghosal, Apurva Kokate, Baskar Ganapathysubramanian, Soumik Sarkar
- **Comment**: Workshop on Machine Learning for Molecules and Materials (MLMM),
  Neural Information Processing Systems (NeurIPS) 2018, Montreal, Canada
- **Journal**: npj Comput Mater 5, 95 (2019)
- **Summary**: The performance of an organic photovoltaic device is intricately connected to its active layer morphology. This connection between the active layer and device performance is very expensive to evaluate, either experimentally or computationally. Hence, designing morphologies to achieve higher performances is non-trivial and often intractable. To solve this, we first introduce a deep convolutional neural network (CNN) architecture that can serve as a fast and robust surrogate for the complex structure-property map. Several tests were performed to gain trust in this trained model. Then, we utilize this fast framework to perform robust microstructural design to enhance device performance.



### ReSIFT: Reliability-Weighted SIFT-based Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1811.06090v1
- **DOI**: 10.1109/ICIP.2016.7532718
- **Categories**: **eess.IV**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1811.06090v1)
- **Published**: 2018-11-14 22:08:26+00:00
- **Updated**: 2018-11-14 22:08:26+00:00
- **Authors**: Dogancan Temel, Ghassan AlRegib
- **Comment**: 5 pages, 3 figures, 4 tables
- **Journal**: D. Temel and G. AlRegib, "ReSIFT: Reliability-weighted sift-based
  image quality assessment," 2016 IEEE International Conference on Image
  Processing (ICIP), Phoenix, AZ, 2016, pp. 2047-2051
- **Summary**: This paper presents a full-reference image quality estimator based on SIFT descriptor matching over reliability-weighted feature maps. Reliability assignment includes a smoothing operation, a transformation to perceptual color domain, a local normalization stage, and a spectral residual computation with global normalization. The proposed method ReSIFT is tested on the LIVE and the LIVE Multiply Distorted databases and compared with 11 state-of-the-art full-reference quality estimators. In terms of the Pearson and the Spearman correlation, ReSIFT is the best performing quality estimator in the overall databases. Moreover, ReSIFT is the best performing quality estimator in at least one distortion group in compression, noise, and blur category.



### Improving Fingerprint Pore Detection with a Small FCN
- **Arxiv ID**: http://arxiv.org/abs/1811.06846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1811.06846v1)
- **Published**: 2018-11-14 22:29:33+00:00
- **Updated**: 2018-11-14 22:29:33+00:00
- **Authors**: Gabriel Dahia, Maur√≠cio Pamplona Segundo
- **Comment**: arXiv admin note: text overlap with arXiv:1809.10229
- **Journal**: None
- **Summary**: In this work, we investigate if previously proposed CNNs for fingerprint pore detection overestimate the number of required model parameters for this task. We show that this is indeed the case by proposing a fully convolutional neural network that has significantly fewer parameters. We evaluate this model using a rigorous and reproducible protocol, which was, prior to our work, not available to the community. Using our protocol, we show that the proposed model, when combined with post-processing, performs better than previous methods, albeit being much more efficient. All our code is available at https://github.com/gdahia/fingerprint-pore-detection



### Multivariate Time-series Similarity Assessment via Unsupervised Representation Learning and Stratified Locality Sensitive Hashing: Application to Early Acute Hypotensive Episode Detection
- **Arxiv ID**: http://arxiv.org/abs/1811.06106v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1811.06106v3)
- **Published**: 2018-11-14 22:53:40+00:00
- **Updated**: 2018-12-04 15:25:50+00:00
- **Authors**: Jwala Dhamala, Emmanuel Azuh, Abdullah Al-Dujaili, Jonathan Rubin, Una-May O'Reilly
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216
- **Journal**: None
- **Summary**: Timely prediction of clinically critical events in Intensive Care Unit (ICU) is important for improving care and survival rate. Most of the existing approaches are based on the application of various classification methods on explicitly extracted statistical features from vital signals. In this work, we propose to eliminate the high cost of engineering hand-crafted features from multivariate time-series of physiologic signals by learning their representation with a sequence-to-sequence auto-encoder. We then propose to hash the learned representations to enable signal similarity assessment for the prediction of critical events. We apply this methodological framework to predict Acute Hypotensive Episodes (AHE) on a large and diverse dataset of vital signal recordings. Experiments demonstrate the ability of the presented framework in accurately predicting an upcoming AHE.



### Deep Learning in the Wavelet Domain
- **Arxiv ID**: http://arxiv.org/abs/1811.06115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1811.06115v1)
- **Published**: 2018-11-14 23:33:09+00:00
- **Updated**: 2018-11-14 23:33:09+00:00
- **Authors**: Fergal Cotter, Nick Kingsbury
- **Comment**: 4 pages + 1 reference. 2 figures 1 table
- **Journal**: None
- **Summary**: This paper examines the possibility of, and the possible advantages to learning the filters of convolutional neural networks (CNNs) for image analysis in the wavelet domain. We are stimulated by both Mallat's scattering transform and the idea of filtering in the Fourier domain. It is important to explore new spaces in which to learn, as these may provide inherent advantages that are not available in the pixel space. However, the scattering transform is limited by its inability to learn in between scattering orders, and any Fourier domain filtering is limited by the large number of filter parameters needed to get localized filters. Instead we consider filtering in the wavelet domain with learnable filters. The wavelet space allows us to have local, smooth filters with far fewer parameters, and learnability can give us flexibility. We present a novel layer which takes CNN activations into the wavelet space, learns parameters and returns to the pixel space. This allows it to be easily dropped in to any neural network without affecting the structure. As part of this work, we show how to pass gradients through a multirate system and give preliminary results.




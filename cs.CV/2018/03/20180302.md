# Arxiv Papers in cs.CV on 2018-03-02
### Meta-Learning for Semi-Supervised Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.00676v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.00676v1)
- **Published**: 2018-03-02 01:07:49+00:00
- **Updated**: 2018-03-02 01:07:49+00:00
- **Authors**: Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel
- **Comment**: Published as a conference paper at ICLR 2018. 15 pages
- **Journal**: None
- **Summary**: In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.



### Constrained Neural Style Transfer for Decorated Logo Generation
- **Arxiv ID**: http://arxiv.org/abs/1803.00686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00686v2)
- **Published**: 2018-03-02 02:47:33+00:00
- **Updated**: 2018-07-14 03:42:19+00:00
- **Authors**: Gantugs Atarsaikhan, Brian Kenji Iwana, Seiichi Uchida
- **Comment**: Accepted by DAS2018
- **Journal**: None
- **Summary**: Making decorated logos requires image editing skills, without sufficient skills, it could be a time-consuming task. While there are many on-line web services to make new logos, they have limited designs and duplicates can be made. We propose using neural style transfer with clip art and text for the creation of new and genuine logos. We introduce a new loss function based on distance transform of the input image, which allows the preservation of the silhouettes of text and objects. The proposed method constrains style transfer only around the designated area. We demonstrate the characteristics of proposed method. Finally, we show the results of logo generation with various input images.



### Deep-neural-network based sinogram synthesis for sparse-view CT image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.00694v2
- **DOI**: 10.1109/TRPMS.2018.2867611
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00694v2)
- **Published**: 2018-03-02 03:38:47+00:00
- **Updated**: 2018-03-06 02:22:59+00:00
- **Authors**: Hoyeon Lee, Jongha Lee, Hyeongseok Kim, Byungchul Cho, Seungryong Cho
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a number of approaches to low-dose computed tomography (CT) have been developed and deployed in commercialized CT scanners. Tube current reduction is perhaps the most actively explored technology with advanced image reconstruction algorithms. Sparse data sampling is another viable option to the low-dose CT, and sparse-view CT has been particularly of interest among the researchers in CT community. Since analytic image reconstruction algorithms would lead to severe image artifacts, various iterative algorithms have been developed for reconstructing images from sparsely view-sampled projection data. However, iterative algorithms take much longer computation time than the analytic algorithms, and images are usually prone to different types of image artifacts that heavily depend on the reconstruction parameters. Interpolation methods have also been utilized to fill the missing data in the sinogram of sparse-view CT thus providing synthetically full data for analytic image reconstruction. In this work, we introduce a deep-neural-network-enabled sinogram synthesis method for sparse-view CT, and show its outperformance to the existing interpolation methods and also to the iterative image reconstruction approach.



### Raw Multi-Channel Audio Source Separation using Multi-Resolution Convolutional Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/1803.00702v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.MM, 68T01, 68T10, 68T45, 62H25, H.5.5; I.5; I.2.6; I.4.3; I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/1803.00702v1)
- **Published**: 2018-03-02 03:47:47+00:00
- **Updated**: 2018-03-02 03:47:47+00:00
- **Authors**: Emad M. Grais, Dominic Ward, Mark D. Plumbley
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised multi-channel audio source separation requires extracting useful spectral, temporal, and spatial features from the mixed signals. The success of many existing systems is therefore largely dependent on the choice of features used for training. In this work, we introduce a novel multi-channel, multi-resolution convolutional auto-encoder neural network that works on raw time-domain signals to determine appropriate multi-resolution features for separating the singing-voice from stereo music. Our experimental results show that the proposed method can achieve multi-channel audio source separation without the need for hand-crafted features or any pre- or post-processing.



### Fusion of multispectral satellite imagery using a cluster of graphics processing unit
- **Arxiv ID**: http://arxiv.org/abs/1803.00737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1803.00737v1)
- **Published**: 2018-03-02 07:09:20+00:00
- **Updated**: 2018-03-02 07:09:20+00:00
- **Authors**: Anas M. Al-Oraiqat, E. A. Bashkov, V. Babkov, C. Titarenko
- **Comment**: 7 pages, 5 Figures, 3 tables
- **Journal**: International Geoinformatics Research and Development Journal,
  2013
- **Summary**: The paper presents a parallel implementation of existing image fusion methods on a graphical cluster. Parallel implementations of methods based on discrete wavelet transformation (Haars and Daubechies discrete wavelet transform) are developed. Experiments were performed on a cluster using GPU and CPU and performance gains were estimated for the use of the developed parallel implementations to process satellite images from satellite Landsat 7. The implementation on a graphic cluster provides performance improvement from 2 to 18 times. The quality of the considered methods was evaluated by ERGAS and QNR metrics. The results show performance gains and retaining of quality with the cluster of GPU compared to the results obtained by the authors and other researchers for a CPU and single GPU.



### Driving Digital Rock towards Machine Learning: predicting permeability with Gradient Boosting and Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.00758v2
- **DOI**: 10.1016/j.cageo.2019.02.002
- **Categories**: **physics.geo-ph**, cs.CV, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/1803.00758v2)
- **Published**: 2018-03-02 08:41:58+00:00
- **Updated**: 2018-03-14 19:21:22+00:00
- **Authors**: Oleg Sudakov, Evgeny Burnaev, Dmitry Koroteev
- **Comment**: 21 pages, 7 figures
- **Journal**: None
- **Summary**: We present a research study aimed at testing of applicability of machine learning techniques for prediction of permeability of digitized rock samples. We prepare a training set containing 3D images of sandstone samples imaged with X-ray microtomography and corresponding permeability values simulated with Pore Network approach. We also use Minkowski functionals and Deep Learning-based descriptors of 3D images and 2D slices as input features for predictive model training and prediction. We compare predictive power of various feature sets and methods. The later include Gradient Boosting and various architectures of Deep Neural Networks (DNN). The results demonstrate applicability of machine learning for image-based permeability prediction and open a new area of Digital Rock research.



### Aspl{ü}nd's metric defined in the Logarithmic Image Processing (LIP) framework for colour and multivariate images
- **Arxiv ID**: http://arxiv.org/abs/1803.00764v1
- **DOI**: 10.1109/ICIP.2015.7351540
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00764v1)
- **Published**: 2018-03-02 09:06:54+00:00
- **Updated**: 2018-03-02 09:06:54+00:00
- **Authors**: Guillaume Noyel, Michel Jourlin
- **Comment**: None
- **Journal**: Proceedings IEEE International Conference on Image Processing
  (ICIP 2015, Sep 2015, Qu{\'e}bec City, Canada). ISBN: 978-1-4799-8338-4
  (CFP15CIP-USB), pp.3921 - 3925, 2015, http://www.icip2015.org/
- **Summary**: Aspl{\"u}nd's metric, which is useful for pattern matching, consists in a double-sided probing, i.e. the over-graph and the sub-graph of a function are probed jointly. It has previously been defined for grey-scale images using the Logarithmic Image Processing (LIP) framework. LIP is a non-linear model to perform operations between images while being consistent with the human visual system. Our contribution consists in extending the Aspl{\"u}nd's metric to colour and multivariate images using the LIP framework. Aspl{\"u}nd's metric is insensitive to lighting variations and we propose a colour variant which is robust to noise.



### Automated Map Reading: Image Based Localisation in 2-D Maps Using Binary Semantic Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1803.00788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.00788v1)
- **Published**: 2018-03-02 10:14:27+00:00
- **Updated**: 2018-03-02 10:14:27+00:00
- **Authors**: Pilailuck Panphattarasap, Andrew Calway
- **Comment**: 8 pages, submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems 2018
- **Journal**: None
- **Summary**: We describe a novel approach to image based localisation in urban environments using semantic matching between images and a 2-D map. It contrasts with the vast majority of existing approaches which use image to image database matching. We use highly compact binary descriptors to represent semantic features at locations, significantly increasing scalability compared with existing methods and having the potential for greater invariance to variable imaging conditions. The approach is also more akin to human map reading, making it more suited to human-system interaction. The binary descriptors indicate the presence or not of semantic features relating to buildings and road junctions in discrete viewing directions. We use CNN classifiers to detect the features in images and match descriptor estimates with a database of location tagged descriptors derived from the 2-D map. In isolation, the descriptors are not sufficiently discriminative, but when concatenated sequentially along a route, their combination becomes highly distinctive and allows localisation even when using non-perfect classifiers. Performance is further improved by taking into account left or right turns over a route. Experimental results obtained using Google StreetView and OpenStreetMap data show that the approach has considerable potential, achieving localisation accuracy of around 85% using routes corresponding to approximately 200 meters.



### Unsupervised Deep Single-Image Intrinsic Decomposition using Illumination-Varying Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1803.00805v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00805v2)
- **Published**: 2018-03-02 11:06:50+00:00
- **Updated**: 2018-09-03 08:41:15+00:00
- **Authors**: Louis Lettry, Kenneth Vanhoey, Luc van Gool
- **Comment**: To appear in Pacific Graphics 2018
- **Journal**: None
- **Summary**: Machine learning based Single Image Intrinsic Decomposition (SIID) methods decompose a captured scene into its albedo and shading images by using the knowledge of a large set of known and realistic ground truth decompositions. Collecting and annotating such a dataset is an approach that cannot scale to sufficient variety and realism. We free ourselves from this limitation by training on unannotated images.   Our method leverages the observation that two images of the same scene but with different lighting provide useful information on their intrinsic properties: by definition, albedo is invariant to lighting conditions, and cross-combining the estimated albedo of a first image with the estimated shading of a second one should lead back to the second one's input image. We transcribe this relationship into a siamese training scheme for a deep convolutional neural network that decomposes a single image into albedo and shading. The siamese setting allows us to introduce a new loss function including such cross-combinations, and to train solely on (time-lapse) images, discarding the need for any ground truth annotations.   As a result, our method has the good properties of i) taking advantage of the time-varying information of image sequences in the (pre-computed) training step, ii) not requiring ground truth data to train on, and iii) being able to decompose single images of unseen scenes at runtime. To demonstrate and evaluate our work, we additionally propose a new rendered dataset containing illumination-varying scenes and a set of quantitative metrics to evaluate SIID algorithms. Despite its unsupervised nature, our results compete with state of the art methods, including supervised and non data-driven methods.



### Deep Cocktail Network: Multi-source Unsupervised Domain Adaptation with Category Shift
- **Arxiv ID**: http://arxiv.org/abs/1803.00830v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00830v1)
- **Published**: 2018-03-02 12:58:51+00:00
- **Updated**: 2018-03-02 12:58:51+00:00
- **Authors**: Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, Liang Lin
- **Comment**: Accepted for publication in Conference on Computer Vision and Pattern
  Recognition(CVPR), 2018
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) conventionally assumes labeled source samples coming from a single underlying source distribution. Whereas in practical scenario, labeled data are typically collected from diverse sources. The multiple sources are different not only from the target but also from each other, thus, domain adaptater should not be modeled in the same way. Moreover, those sources may not completely share their categories, which further brings a new transfer challenge called category shift. In this paper, we propose a deep cocktail network (DCTN) to battle the domain and category shifts among multiple sources. Motivated by the theoretical results in \cite{mansour2009domain}, the target distribution can be represented as the weighted combination of source distributions, and, the multi-source unsupervised domain adaptation via DCTN is then performed as two alternating steps: i) It deploys multi-way adversarial learning to minimize the discrepancy between the target and each of the multiple source domains, which also obtains the source-specific perplexity scores to denote the possibilities that a target sample belongs to different source domains. ii) The multi-source category classifiers are integrated with the perplexity scores to classify target sample, and the pseudo-labeled target samples together with source samples are utilized to update the multi-source category classifier and the feature extractor. We evaluate DCTN in three domain adaptation benchmarks, which clearly demonstrate the superiority of our framework.



### Pose-Robust Face Recognition via Deep Residual Equivariant Mapping
- **Arxiv ID**: http://arxiv.org/abs/1803.00839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00839v1)
- **Published**: 2018-03-02 13:25:34+00:00
- **Updated**: 2018-03-02 13:25:34+00:00
- **Authors**: Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, Chen Change Loy
- **Comment**: Accepted to CVPR2018
- **Journal**: None
- **Summary**: Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead.



### Quantum distance-based classifier with constant size memory, distributed knowledge and state recycling
- **Arxiv ID**: http://arxiv.org/abs/1803.00853v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1803.00853v1)
- **Published**: 2018-03-02 14:05:21+00:00
- **Updated**: 2018-03-02 14:05:21+00:00
- **Authors**: Przemysław Sadowski
- **Comment**: 17 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: In this work we examine recently proposed distance-based classification method designed for near-term quantum processing units with limited resources. We further study possibilities to reduce the quantum resources without any efficiency decrease. We show that only a part of the information undergoes coherent evolution and this fact allows us to introduce an algorithm with significantly reduced quantum memory size. Additionally, considering only partial information at a time, we propose a classification protocol with information distributed among a number of agents. Finally, we show that the information evolution during a measurement can lead to a better solution and that accuracy of the algorithm can be improved by harnessing the state after the final measurement.



### Protecting JPEG Images Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1803.00940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1803.00940v1)
- **Published**: 2018-03-02 16:35:44+00:00
- **Updated**: 2018-03-02 16:35:44+00:00
- **Authors**: Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer
- **Comment**: Accepted to IEEE Data Compression Conference
- **Journal**: None
- **Summary**: As deep neural networks (DNNs) have been integrated into critical systems, several methods to attack these systems have been developed. These adversarial attacks make imperceptible modifications to an image that fool DNN classifiers. We present an adaptive JPEG encoder which defends against many of these attacks. Experimentally, we show that our method produces images with high visual quality while greatly reducing the potency of state-of-the-art attacks. Our algorithm requires only a modest increase in encoding time, produces a compressed image which can be decompressed by an off-the-shelf JPEG decoder, and classified by an unmodified classifier



### Tree Species Identification from Bark Images Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.00949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00949v2)
- **Published**: 2018-03-02 17:01:17+00:00
- **Updated**: 2018-07-31 14:13:13+00:00
- **Authors**: Mathieu Carpentier, Philippe Giguère, Jonathan Gaudreault
- **Comment**: 2018 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Journal**: None
- **Summary**: Tree species identification using bark images is a challenging problem that could prove useful for many forestry related tasks. However, while the recent progress in deep learning showed impressive results on standard vision problems, a lack of datasets prevented its use on tree bark species classification. In this work, we present, and make publicly available, a novel dataset called BarkNet 1.0 containing more than 23,000 high-resolution bark images from 23 different tree species over a wide range of tree diameters. With it, we demonstrate the feasibility of species recognition through bark images, using deep learning. More specifically, we obtain an accuracy of 93.88% on single crop, and an accuracy of 97.81% using a majority voting approach on all of the images of a tree. We also empirically demonstrate that, for a fixed number of images, it is better to maximize the number of tree individuals in the training database, thus directing future data collection efforts.



### Multimodal Registration of Retinal Images Using Domain-Specific Landmarks and Vessel Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1803.00951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00951v2)
- **Published**: 2018-03-02 17:07:44+00:00
- **Updated**: 2018-04-02 18:03:36+00:00
- **Authors**: Álvaro S. Hervella, José Rouco, Jorge Novo, Marcos Ortega
- **Comment**: None
- **Journal**: None
- **Summary**: The analysis of different image modalities is frequently performed in ophthalmology as it provides complementary information for the diagnosis and follow-up of relevant diseases, like hypertension or diabetes. This work presents a hybrid method for the multimodal registration of color fundus retinography and fluorescein angiography. The proposed method combines a feature-based approach, using domain-specific landmarks, with an intensity-based approach that employs a domain-adapted similarity metric. The methodology is tested on a dataset of 59 image pairs containing both healthy and pathological cases. The results show a satisfactory performance of the proposed combined approach in this multimodal scenario, improving the registration accuracy achieved by the feature-based and the intensity-based approaches.



### Hashing with Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/1803.00974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00974v2)
- **Published**: 2018-03-02 18:12:04+00:00
- **Updated**: 2018-06-25 03:51:33+00:00
- **Authors**: Fatih Cakir, Kun He, Sarah Adel Bargal, Stan Sclaroff
- **Comment**: None
- **Journal**: None
- **Summary**: Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity: mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.



### High-Dynamic-Range Imaging for Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.01071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01071v1)
- **Published**: 2018-03-02 23:30:06+00:00
- **Updated**: 2018-03-02 23:30:06+00:00
- **Authors**: Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in Atmospheric Measurement Techniques (AMT), 2018
- **Journal**: None
- **Summary**: Sky/cloud images obtained from ground-based sky-cameras are usually captured using a fish-eye lens with a wide field of view. However, the sky exhibits a large dynamic range in terms of luminance, more than a conventional camera can capture. It is thus difficult to capture the details of an entire scene with a regular camera in a single shot. In most cases, the circumsolar region is over-exposed, and the regions near the horizon are under-exposed. This renders cloud segmentation for such images difficult. In this paper, we propose HDRCloudSeg -- an effective method for cloud segmentation using High-Dynamic-Range (HDR) imaging based on multi-exposure fusion. We describe the HDR image generation process and release a new database to the community for benchmarking. Our proposed approach is the first using HDR radiance maps for cloud segmentation and achieves very good results.




# Arxiv Papers in cs.CV on 2018-08-28
### Localization Guided Learning for Pedestrian Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.09102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09102v1)
- **Published**: 2018-08-28 03:39:43+00:00
- **Updated**: 2018-08-28 03:39:43+00:00
- **Authors**: Pengze Liu, Xihui Liu, Junjie Yan, Jing Shao
- **Comment**: Accepted by BMVC 2018
- **Journal**: None
- **Summary**: Pedestrian attribute recognition has attracted many attentions due to its wide applications in scene understanding and person analysis from surveillance videos. Existing methods try to use additional pose, part or viewpoint information to complement the global feature representation for attribute classification. However, these methods face difficulties in localizing the areas corresponding to different attributes. To address this problem, we propose a novel Localization Guided Network which assigns attribute-specific weights to local features based on the affinity between proposals pre-extracted proposals and attribute locations. The advantage of our model is that our local features are learned automatically for each attribute and emphasized by the interaction with global features. We demonstrate the effectiveness of our Localization Guided Network on two pedestrian attribute benchmarks (PA-100K and RAP). Our result surpasses the previous state-of-the-art in all five metrics on both datasets.



### Multiple Lane Detection Algorithm Based on Optimised Dense Disparity Map Estimation
- **Arxiv ID**: http://arxiv.org/abs/1808.09128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09128v1)
- **Published**: 2018-08-28 05:42:37+00:00
- **Updated**: 2018-08-28 05:42:37+00:00
- **Authors**: Han Ma, Yixin Ma, Jianhao Jiao, M Usman Maqbool Bhutta, Mohammud Junaid Bocus, Lujia Wang, Ming Liu, Rui Fan
- **Comment**: 5 pages, 7 figures, IEEE International Conference on Imaging Systems
  and Techniques (IST) 2018
- **Journal**: None
- **Summary**: Lane detection is very important for self-driving vehicles. In recent years, computer stereo vision has been prevalently used to enhance the accuracy of the lane detection systems. This paper mainly presents a multiple lane detection algorithm developed based on optimised dense disparity map estimation, where the disparity information obtained at time t_{n} is utilised to optimise the process of disparity estimation at time t_{n+1}. This is achieved by estimating the road model at time t_{n} and then controlling the search range for the disparity estimation at time t_{n+1}. The lanes are then detected using our previously published algorithm, where the vanishing point information is used to model the lanes. The experimental results illustrate that the runtime of the disparity estimation is reduced by around 37% and the accuracy of the lane detection is about 99%.



### Cognitive Action Laws: The Case of Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1808.09162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09162v1)
- **Published**: 2018-08-28 08:12:23+00:00
- **Updated**: 2018-08-28 08:12:23+00:00
- **Authors**: Alessandro Betti, Marco Gori, Stefano Melacci
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a theory for understanding perceptual learning processes within the general framework of laws of nature. Neural networks are regarded as systems whose connections are Lagrangian variables, namely functions depending on time. They are used to minimize the cognitive action, an appropriate functional index that measures the agent interactions with the environment. The cognitive action contains a potential and a kinetic term that nicely resemble the classic formulation of regularization in machine learning. A special choice of the functional index, which leads to forth-order differential equations---Cognitive Action Laws (CAL)---exhibits a structure that mirrors classic formulation of machine learning. In particular, unlike the action of mechanics, the stationarity condition corresponds with the global minimum. Moreover, it is proven that typical asymptotic learning conditions on the weights can coexist with the initialization provided that the system dynamics is driven under a policy referred to as information overloading control. Finally, the theory is experimented for the problem of feature extraction in computer vision.



### Removing out-of-focus blur from a single image
- **Arxiv ID**: http://arxiv.org/abs/1808.09166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09166v1)
- **Published**: 2018-08-28 08:28:23+00:00
- **Updated**: 2018-08-28 08:28:23+00:00
- **Authors**: Guodong Xu, Chaoqiang Liu, Hui Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Reproducing an all-in-focus image from an image with defocus regions is of practical value in many applications, eg, digital photography, and robotics. Using the output of some existing defocus map estimator, existing approaches first segment a de-focused image into multiple regions blurred by Gaussian kernels with different variance each, and then de-blur each region using the corresponding Gaussian kernel. In this paper, we proposed a blind deconvolution method specifically designed for removing defocus blurring from an image, by providing effective solutions to two critical problems: 1) suppressing the artifacts caused by segmentation error by introducing an additional variable regularized by weighted $\ell_0$-norm; and 2) more accurate defocus kernel estimation using non-parametric symmetry and low-rank based constraints on the kernel. The experiments on real datasets showed the advantages of the proposed method over existing ones, thanks to the effective treatments of the two important issues mentioned above during deconvolution.



### A Multi-channel DART Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1808.09170v1
- **DOI**: 10.1007/978-3-030-05288-1_13
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09170v1)
- **Published**: 2018-08-28 08:41:55+00:00
- **Updated**: 2018-08-28 08:41:55+00:00
- **Authors**: Mathé Zeegers, Felix Lucka, Kees Joost Batenburg
- **Comment**: 16 pages. 17 figures. Paper for IWCIA 2018 conference
- **Journal**: None
- **Summary**: Tomography deals with the reconstruction of objects from their projections, acquired along a range of angles. Discrete tomography is concerned with objects that consist of a small number of materials, which makes it possible to compute accurate reconstructions from highly limited projection data. For cases where the allowed intensity values in the reconstruction are known a priori, the discrete algebraic reconstruction technique (DART) has shown to yield accurate reconstructions from few projections. However, a key limitation is that the benefit of DART diminishes as the number of different materials increases. Many tomographic imaging techniques can simultaneously record tomographic data at multiple channels, each corresponding to a different weighting of the materials in the object. Whenever projection data from more than one channel is available, this additional information can potentially be exploited by the reconstruction algorithm. In this paper we present Multi-Channel DART (MC-DART), which deals effectively with multi-channel data. This class of algorithms is a generalization of DART to multiple channels and combines the information for each separate channel-reconstruction in a multi-channel segmentation step. We demonstrate that in a range of simulation experiments, MC-DART is capable of producing more accurate reconstructions compared to single-channel DART.



### A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units
- **Arxiv ID**: http://arxiv.org/abs/1808.09183v1
- **DOI**: 10.1016/j.patrec.2018.07.027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09183v1)
- **Published**: 2018-08-28 09:06:13+00:00
- **Updated**: 2018-08-28 09:06:13+00:00
- **Authors**: Wassim Swaileh, Yann Soullard, Thierry Paquet
- **Comment**: preprint
- **Journal**: Pattern Recognition Letter 2018
- **Summary**: We address the design of a unified multilingual system for handwriting recognition. Most of multi- lingual systems rests on specialized models that are trained on a single language and one of them is selected at test time. While some recognition systems are based on a unified optical model, dealing with a unified language model remains a major issue, as traditional language models are generally trained on corpora composed of large word lexicons per language. Here, we bring a solution by con- sidering language models based on sub-lexical units, called multigrams. Dealing with multigrams strongly reduces the lexicon size and thus decreases the language model complexity. This makes pos- sible the design of an end-to-end unified multilingual recognition system where both a single optical model and a single language model are trained on all the languages. We discuss the impact of the language unification on each model and show that our system reaches state-of-the-art methods perfor- mance with a strong reduction of the complexity.



### DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning from Synthetic Depth
- **Arxiv ID**: http://arxiv.org/abs/1808.09208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09208v1)
- **Published**: 2018-08-28 10:29:20+00:00
- **Updated**: 2018-08-28 10:29:20+00:00
- **Authors**: Jameel Malik, Ahmed Elhayek, Fabrizio Nunnari, Kiran Varanasi, Kiarash Tamaddon, Alexis Heloir, Didier Stricker
- **Comment**: Accepted for publication in 3DV-2018 (http://3dv18.uniud.it/)
- **Journal**: None
- **Summary**: Articulated hand pose and shape estimation is an important problem for vision-based applications such as augmented reality and animation. In contrast to the existing methods which optimize only for joint positions, we propose a fully supervised deep network which learns to jointly estimate a full 3D hand mesh representation and pose from a single depth image. To this end, a CNN architecture is employed to estimate parametric representations i.e. hand pose, bone scales and complex shape parameters. Then, a novel hand pose and shape layer, embedded inside our deep framework, produces 3D joint positions and hand mesh. Lack of sufficient training data with varying hand shapes limits the generalized performance of learning based methods. Also, manually annotating real data is suboptimal. Therefore, we present SynHand5M: a million-scale synthetic dataset with accurate joint annotations, segmentation masks and mesh files of depth maps. Among model based learning (hybrid) methods, we show improved results on our dataset and two of the public benchmarks i.e. NYU and ICVL. Also, by employing a joint training strategy with real and synthetic data, we recover 3D hand mesh and pose from real images in 3.7ms.



### DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1808.09211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09211v1)
- **Published**: 2018-08-28 10:32:43+00:00
- **Updated**: 2018-08-28 10:32:43+00:00
- **Authors**: Stéphane Lathuilière, Pablo Mesejo, Xavier Alameda-Pineda, Radu Horaud
- **Comment**: accepted at ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we address the problem of how to robustly train a ConvNet for regression, or deep robust regression. Traditionally, deep regression employs the L2 loss function, known to be sensitive to outliers, i.e. samples that either lie at an abnormal distance away from the majority of the training samples, or that correspond to wrongly annotated targets. This means that, during back-propagation, outliers may bias the training process due to the high magnitude of their gradient. In this paper, we propose DeepGUM: a deep regression model that is robust to outliers thanks to the use of a Gaussian-uniform mixture model. We derive an optimization algorithm that alternates between the unsupervised detection of outliers using expectation-maximization, and the supervised training with cleaned samples using stochastic gradient descent. DeepGUM is able to adapt to a continuously evolving outlier distribution, avoiding to manually impose any threshold on the proportion of outliers in the training set. Extensive experimental evaluations on four different tasks (facial and fashion landmark detection, age and head pose estimation) lead us to conclude that our novel robust technique provides reliability in the presence of various types of noise and protection against a high percentage of outliers.



### Motorcycle Classification in Urban Scenarios using Convolutional Neural Networks for Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1808.09273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09273v1)
- **Published**: 2018-08-28 13:16:17+00:00
- **Updated**: 2018-08-28 13:16:17+00:00
- **Authors**: Jorge E. Espinosa, Sergio A. Velastin, John W. Branch
- **Comment**: This paper were published by IET for the ICPRS 2017 Conference,
  Madrid Spain July 2017
- **Journal**: ISBN: 978-1-78561-652-5 July 2017
- **Summary**: This paper presents a motorcycle classification system for urban scenarios using Convolutional Neural Network (CNN). Significant results on image classification has been achieved using CNNs at the expense of a high computational cost for training with thousands or even millions of examples. Nevertheless, features can be extracted from CNNs already trained. In this work AlexNet, included in the framework CaffeNet, is used to extract features from frames taken on a real urban scenario. The extracted features from the CNN are used to train a support vector machine (SVM) classifier to discriminate motorcycles from other road users. The obtained results show a mean accuracy of 99.40% and 99.29% on a classification task of three and five classes respectively. Further experiments are performed on a validation set of images showing a satisfactory classification.



### How Robust is 3D Human Pose Estimation to Occlusion?
- **Arxiv ID**: http://arxiv.org/abs/1808.09316v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.09316v2)
- **Published**: 2018-08-28 14:14:42+00:00
- **Updated**: 2018-08-29 12:08:09+00:00
- **Authors**: István Sárándi, Timm Linder, Kai O. Arras, Bastian Leibe
- **Comment**: Accepted for IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS'18) - Workshop on Robotic Co-workers 4.0: Human Safety and
  Comfort in Human-Robot Interactive Social Environments
- **Journal**: None
- **Summary**: Occlusion is commonplace in realistic human-robot shared environments, yet its effects are not considered in standard 3D human pose estimation benchmarks. This leaves the question open: how robust are state-of-the-art 3D pose estimation methods against partial occlusions? We study several types of synthetic occlusions over the Human3.6M dataset and find a method with state-of-the-art benchmark performance to be sensitive even to low amounts of occlusion. Addressing this issue is key to progress in applications such as collaborative and service robotics. We take a first step in this direction by improving occlusion-robustness through training data augmentation with synthetic occlusions. This also turns out to be an effective regularizer that is beneficial even for non-occluded test cases.



### Iterative Deep Learning for Road Topology Extraction
- **Arxiv ID**: http://arxiv.org/abs/1808.09814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09814v1)
- **Published**: 2018-08-28 14:32:44+00:00
- **Updated**: 2018-08-28 14:32:44+00:00
- **Authors**: Carles Ventura, Jordi Pont-Tuset, Sergi Caelles, Kevis-Kokitsi Maninis, Luc Van Gool
- **Comment**: BMVC 2018 camera ready. Code:
  https://github.com/carlesventura/iterative-deep-learning. arXiv admin note:
  substantial text overlap with arXiv:1712.01217
- **Journal**: None
- **Summary**: This paper tackles the task of estimating the topology of road networks from aerial images. Building on top of a global model that performs a dense semantical classification of the pixels of the image, we design a Convolutional Neural Network (CNN) that predicts the local connectivity among the central pixel of an input patch and its border points. By iterating this local connectivity we sweep the whole image and infer the global topology of the road network, inspired by a human delineating a complex network with the tip of their finger. We perform an extensive and comprehensive qualitative and quantitative evaluation on the road network estimation task, and show that our method also generalizes well when moving to networks of retinal vessels.



### Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1808.09347v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.09347v2)
- **Published**: 2018-08-28 15:04:32+00:00
- **Updated**: 2018-11-03 07:19:39+00:00
- **Authors**: Chao Chen, Zhihong Chen, Boyuan Jiang, Xinyu Jin
- **Comment**: This paper has been accepted by AAAI-2019
- **Journal**: None
- **Summary**: Recently, considerable effort has been devoted to deep domain adaptation in computer vision and machine learning communities. However, most of existing work only concentrates on learning shared feature representation by minimizing the distribution discrepancy across different domains. Due to the fact that all the domain alignment approaches can only reduce, but not remove the domain shift. Target domain samples distributed near the edge of the clusters, or far from their corresponding class centers are easily to be misclassified by the hyperplane learned from the source domain. To alleviate this issue, we propose to joint domain alignment and discriminative feature learning, which could benefit both domain alignment and final classification. Specifically, an instance-based discriminative feature learning method and a center-based discriminative feature learning method are proposed, both of which guarantee the domain invariant features with better intra-class compactness and inter-class separability. Extensive experiments show that learning the discriminative features in the shared feature space can significantly boost the performance of deep domain adaptation methods.



### 3D-Aware Scene Manipulation via Inverse Graphics
- **Arxiv ID**: http://arxiv.org/abs/1808.09351v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1808.09351v4)
- **Published**: 2018-08-28 15:16:07+00:00
- **Updated**: 2018-12-18 18:57:20+00:00
- **Authors**: Shunyu Yao, Tzu Ming Harry Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, William T. Freeman, Joshua B. Tenenbaum
- **Comment**: NeurIPS 2018. Code: https://github.com/ysymyth/3D-SDN Website:
  http://3dsdn.csail.mit.edu/
- **Journal**: None
- **Summary**: We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.



### Learning Discriminative Representation with Signed Laplacian Restricted Boltzmann Machine
- **Arxiv ID**: http://arxiv.org/abs/1808.09389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09389v1)
- **Published**: 2018-08-28 16:27:47+00:00
- **Updated**: 2018-08-28 16:27:47+00:00
- **Authors**: Dongdong Chen, Jiancheng Lv, Mike E. Davies
- **Comment**: To appear in iTWIST'18
- **Journal**: None
- **Summary**: We investigate the potential of a restricted Boltzmann Machine (RBM) for discriminative representation learning. By imposing the class information preservation constraints on the hidden layer of the RBM, we propose a Signed Laplacian Restricted Boltzmann Machine (SLRBM) for supervised discriminative representation learning. The model utilizes the label information and preserves the global data locality of data points simultaneously. Experimental results on the benchmark data set show the effectiveness of our method.



### Quantitative Evaluation of Base and Detail Decomposition Filters Based on their Artifacts
- **Arxiv ID**: http://arxiv.org/abs/1808.09411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09411v1)
- **Published**: 2018-08-28 17:02:20+00:00
- **Updated**: 2018-08-28 17:02:20+00:00
- **Authors**: Charles Hessel, Jean-Michel Morel
- **Comment**: 12 pages; 11 figures; 2 tables; supplementary material available
  (link given in the paper)
- **Journal**: None
- **Summary**: This paper introduces a quantitative evaluation of filters that seek to separate an image into its large-scale variations, the base layer, and its fine-scale variations, the detail layer. Such methods have proliferated with the development of HDR imaging and the proposition of many new tone-mapping operators. We argue that an objective quality measurement for all methods can be based on their artifacts. To this aim, the four main recurrent artifacts are described and mathematically characterized. Among them two are classic, the luminance halo and the staircase effect, but we show the relevance of two more, the contrast halo and the compartmentalization effect. For each of these artifacts we design a test-pattern and its attached measurement formula. Then we fuse these measurements into a single quality mark, and obtain in that way a ranking method valid for all filters performing a base+detail decomposition. This synthetic ranking is applied to seven filters representative of the literature and shown to agree with expert artifact rejection criteria.



### Contextual Audio-Visual Switching For Speech Enhancement in Real-World Environments
- **Arxiv ID**: http://arxiv.org/abs/1808.09825v1
- **DOI**: 10.1016/j.inffus.2019.08.008
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1808.09825v1)
- **Published**: 2018-08-28 17:27:11+00:00
- **Updated**: 2018-08-28 17:27:11+00:00
- **Authors**: Ahsan Adeel, Mandar Gogate, Amir Hussain
- **Comment**: 16 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1808.00046
- **Journal**: Information Fusion, 2019
- **Summary**: Human speech processing is inherently multimodal, where visual cues (lip movements) help to better understand the speech in noise. Lip-reading driven speech enhancement significantly outperforms benchmark audio-only approaches at low signal-to-noise ratios (SNRs). However, at high SNRs or low levels of background noise, visual cues become fairly less effective for speech enhancement. Therefore, a more optimal, context-aware audio-visual (AV) system is required, that contextually utilises both visual and noisy audio features and effectively accounts for different noisy conditions. In this paper, we introduce a novel contextual AV switching component that contextually exploits AV cues with respect to different operating conditions to estimate clean audio, without requiring any SNR estimation. The switching module switches between visual-only (V-only), audio-only (A-only), and both AV cues at low, high and moderate SNR levels, respectively. The contextual AV switching component is developed by integrating a convolutional neural network and long-short-term memory network. For testing, the estimated clean audio features are utilised by the developed novel enhanced visually derived Wiener filter for clean audio power spectrum estimation. The contextual AV speech enhancement method is evaluated under real-world scenarios using benchmark Grid and ChiME3 corpora. For objective testing, perceptual evaluation of speech quality is used to evaluate the quality of the restored speech. For subjective testing, the standard mean-opinion-score method is used. The critical analysis and comparative study demonstrate the outperformance of proposed contextual AV approach, over A-only, V-only, spectral subtraction, and log-minimum mean square error based speech enhancement methods at both low and high SNRs, revealing its capability to tackle spectro-temporal variation in any real-world noisy condition.



### Deep Lidar CNN to Understand the Dynamics of Moving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1808.09526v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.09526v2)
- **Published**: 2018-08-28 20:27:16+00:00
- **Updated**: 2018-08-30 13:03:16+00:00
- **Authors**: Victor Vaquero, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: Presented in IEEE ICRA 2018. IEEE Copyrights: Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses. (V2 just corrected comments on arxiv submission)
- **Journal**: None
- **Summary**: Perception technologies in Autonomous Driving are experiencing their golden age due to the advances in Deep Learning. Yet, most of these systems rely on the semantically rich information of RGB images. Deep Learning solutions applied to the data of other sensors typically mounted on autonomous cars (e.g. lidars or radars) are not explored much. In this paper we propose a novel solution to understand the dynamics of moving vehicles of the scene from only lidar information. The main challenge of this problem stems from the fact that we need to disambiguate the proprio-motion of the 'observer' vehicle from that of the external 'observed' vehicles. For this purpose, we devise a CNN architecture which at testing time is fed with pairs of consecutive lidar scans. However, in order to properly learn the parameters of this network, during training we introduce a series of so-called pretext tasks which also leverage on image data. These tasks include semantic information about vehicleness and a novel lidar-flow feature which combines standard image-based optical flow with lidar scans. We obtain very promising results and show that including distilled image information only during training, allows improving the inference results of the network at test time, even when image data is no longer used.



### Temporal Saliency Adaptation in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1808.09559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09559v2)
- **Published**: 2018-08-28 22:24:10+00:00
- **Updated**: 2018-09-04 20:54:52+00:00
- **Authors**: Panagiotis Linardos, Eva Mohedano, Monica Cherto, Cathal Gurrin, Xavier Giro-i-Nieto
- **Comment**: Extended abstract at the ECCV 2018 Workshop on Egocentric Perception,
  Interaction and Computing (EPIC)
- **Journal**: None
- **Summary**: This work adapts a deep neural model for image saliency prediction to the temporal domain of egocentric video. We compute the saliency map for each video frame, firstly with an off-the-shelf model trained from static images, secondly by adding a a convolutional or conv-LSTM layers trained with a dataset for video saliency prediction. We study each configuration on EgoMon, a new dataset made of seven egocentric videos recorded by three subjects in both free-viewing and task-driven set ups. Our results indicate that the temporal adaptation is beneficial when the viewer is not moving and observing the scene from a narrow field of view. Encouraged by this observation, we compute and publish the saliency maps for the EPIC Kitchens dataset, in which viewers are cooking. Source code and models available at https://imatge-upc.github.io/saliency-2018-videosalgan/



### On Learning 3D Face Morphable Model from In-the-wild Images
- **Arxiv ID**: http://arxiv.org/abs/1808.09560v2
- **DOI**: 10.1109/TPAMI.2019.2927975
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09560v2)
- **Published**: 2018-08-28 22:25:01+00:00
- **Updated**: 2019-07-14 05:01:58+00:00
- **Authors**: Luan Tran, Xiaoming Liu
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI). Conference version: arXiv:1804.03786 (CVPR'18). Source code:
  https://github.com/tranluan/Nonlinear_Face_3DMM , Project webpage:
  http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html
- **Journal**: None
- **Summary**: As a classic statistical model of 3D facial shape and albedo, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of 3D face scans with associated well-controlled 2D face images, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of in-the-wild face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, lighting, shape and albedo parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape and albedo, respectively. With the projection parameter, lighting, 3D shape, and albedo, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment, 3D reconstruction, and face editing.



### ARBEE: Towards Automated Recognition of Bodily Expression of Emotion In the Wild
- **Arxiv ID**: http://arxiv.org/abs/1808.09568v2
- **DOI**: 10.1007/s11263-019-01215-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.09568v2)
- **Published**: 2018-08-28 22:39:21+00:00
- **Updated**: 2019-07-09 22:46:10+00:00
- **Authors**: Yu Luo, Jianbo Ye, Reginald B. Adams, Jr., Jia Li, Michelle G. Newman, James Z. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are arguably innately prepared to comprehend others' emotional expressions from subtle body movements. If robots or computers can be empowered with this capability, a number of robotic applications become possible. Automatically recognizing human bodily expression in unconstrained situations, however, is daunting given the incomplete understanding of the relationship between emotional expressions and body movements. The current research, as a multidisciplinary effort among computer and information sciences, psychology, and statistics, proposes a scalable and reliable crowdsourcing approach for collecting in-the-wild perceived emotion data for computers to learn to recognize body languages of humans. To accomplish this task, a large and growing annotated dataset with 9,876 video clips of body movements and 13,239 human characters, named BoLD (Body Language Dataset), has been created. Comprehensive statistical analysis of the dataset revealed many interesting insights. A system to model the emotional expressions based on bodily movements, named ARBEE (Automated Recognition of Bodily Expression of Emotion), has also been developed and evaluated. Our analysis shows the effectiveness of Laban Movement Analysis (LMA) features in characterizing arousal, and our experiments using LMA features further demonstrate computability of bodily expression. We report and compare results of several other baseline methods which were developed for action recognition based on two different modalities, body skeleton, and raw image. The dataset and findings presented in this work will likely serve as a launchpad for future discoveries in body language understanding that will enable future robots to interact and collaborate more effectively with humans.



### Probabilistic Sparse Subspace Clustering Using Delayed Association
- **Arxiv ID**: http://arxiv.org/abs/1808.09574v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.09574v1)
- **Published**: 2018-08-28 23:03:55+00:00
- **Updated**: 2018-08-28 23:03:55+00:00
- **Authors**: Maryam Jaberi, Marianna Pensky, Hassan Foroosh
- **Comment**: None
- **Journal**: ICPR 2018
- **Summary**: Discovering and clustering subspaces in high-dimensional data is a fundamental problem of machine learning with a wide range of applications in data mining, computer vision, and pattern recognition. Earlier methods divided the problem into two separate stages of finding the similarity matrix and finding clusters. Similar to some recent works, we integrate these two steps using a joint optimization approach. We make the following contributions: (i) we estimate the reliability of the cluster assignment for each point before assigning a point to a subspace. We group the data points into two groups of "certain" and "uncertain", with the assignment of latter group delayed until their subspace association certainty improves. (ii) We demonstrate that delayed association is better suited for clustering subspaces that have ambiguities, i.e. when subspaces intersect or data are contaminated with outliers/noise. (iii) We demonstrate experimentally that such delayed probabilistic association leads to a more accurate self-representation and final clusters. The proposed method has higher accuracy both for points that exclusively lie in one subspace, and those that are on the intersection of subspaces. (iv) We show that delayed association leads to huge reduction of computational cost, since it allows for incremental spectral clustering.




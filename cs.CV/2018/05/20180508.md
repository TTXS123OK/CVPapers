# Arxiv Papers in cs.CV on 2018-05-08
### FFNet: Video Fast-Forwarding via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.02792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02792v1)
- **Published**: 2018-05-08 01:04:19+00:00
- **Updated**: 2018-05-08 01:04:19+00:00
- **Authors**: Shuyue Lan, Rameswar Panda, Qi Zhu, Amit K. Roy-Chowdhury
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: For many applications with limited computation, communication, storage and energy resources, there is an imperative need of computer vision methods that could select an informative subset of the input video for efficient processing at or near real time. In the literature, there are two relevant groups of approaches: generating a trailer for a video or fast-forwarding while watching/processing the video. The first group is supported by video summarization techniques, which require processing of the entire video to select an important subset for showing to users. In the second group, current fast-forwarding methods depend on either manual control or automatic adaptation of playback speed, which often do not present an accurate representation and may still require processing of every frame. In this paper, we introduce FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration from video summarization and does fast-forwarding differently. It is an online framework that automatically fast-forwards a video and presents a representative subset of frames to users on the fly. It does not require processing the entire video, but just the portion that is selected by the fast-forward agent, which makes the process very computationally efficient. The online nature of our proposed method also enables the users to begin fast-forwarding at any point of the video. Experiments on two real-world datasets demonstrate that our method can provide better representation of the input video with much less processing requirement.



### A Performance Evaluation of Convolutional Neural Networks for Face Anti Spoofing
- **Arxiv ID**: http://arxiv.org/abs/1805.04176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.04176v2)
- **Published**: 2018-05-08 01:34:59+00:00
- **Updated**: 2019-03-27 12:40:19+00:00
- **Authors**: Chaitanya Nagpal, Shiv Ram Dubey
- **Comment**: Accepted in 2019 International Joint Conference on Neural Networks
  (IJCNN)
- **Journal**: None
- **Summary**: In the current era, biometric based access control is becoming more popular due to its simplicity and ease to use by the users. It reduces the manual work of identity recognition and facilitates the automatic processing. The face is one of the most important biometric visual information that can be easily captured without user cooperation in an uncontrolled environment. Precise detection of spoofed faces should be on the high priority to make face based identity recognition and access control robust against possible attacks. The recently evolved Convolutional Neural Network (CNN) based deep learning technique has proven as one of the excellent method to deal with the visual information very effectively. The CNN learns the hierarchical features at intermediate layers automatically from the data. Several CNN based methods such as Inception and ResNet have shown outstanding performance for image classification problem. This paper does a performance evaluation of CNNs for face anti-spoofing. The Inception and ResNet CNN architectures are used in this study. The results are computed over benchmark MSU Mobile Face Spoofing Database. The experiments are done by considering the different aspects such as the depth of the model, random weight initialization vs weight transfer, fine tuning vs training from scratch and different learning rate. The favorable results are obtained using these CNN architectures for face anti-spoofing in different settings.



### Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1805.02798v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02798v6)
- **Published**: 2018-05-08 01:39:59+00:00
- **Updated**: 2021-09-15 07:46:34+00:00
- **Authors**: Saeid Asgari Taghanaki, Yefeng Zheng, S. Kevin Zhou, Bogdan Georgescu, Puneet Sharma, Daguang Xu, Dorin Comaniciu, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Simultaneous segmentation of multiple organs from different medical imaging modalities is a crucial task as it can be utilized for computer-aided diagnosis, computer-assisted surgery, and therapy planning. Thanks to the recent advances in deep learning, several deep neural networks for medical image segmentation have been introduced successfully for this purpose. In this paper, we focus on learning a deep multi-organ segmentation network that labels voxels. In particular, we examine the critical choice of a loss function in order to handle the notorious imbalance problem that plagues both the input and output of a learning model. The input imbalance refers to the class-imbalance in the input training samples (i.e., small foreground objects embedded in an abundance of background voxels, as well as organs of varying sizes). The output imbalance refers to the imbalance between the false positives and false negatives of the inference model. In order to tackle both types of imbalance during training and inference, we introduce a new curriculum learning based loss function. Specifically, we leverage Dice similarity coefficient to deter model parameters from being held at bad local minima and at the same time gradually learn better model parameters by penalizing for false positives/negatives using a cross entropy term. We evaluated the proposed loss function on three datasets: whole body positron emission tomography (PET) scans with 5 target organs, magnetic resonance imaging (MRI) prostate scans, and ultrasound echocardigraphy images with a single target organ i.e., left ventricular. We show that a simple network architecture with the proposed integrative loss function can outperform state-of-the-art methods and results of the competing methods can be improved when our proposed loss is used.



### N2RPP: An Adversarial Network to Rebuild Plantar Pressure for ACLD Patients
- **Arxiv ID**: http://arxiv.org/abs/1805.02825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02825v1)
- **Published**: 2018-05-08 04:14:24+00:00
- **Updated**: 2018-05-08 04:14:24+00:00
- **Authors**: Yi Zhang, Zhengfei Wang, Guoxiong Xu, Hongshi Huang, Wenxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Foot is a vital part of human, and lots of valuable information is embedded. Plantar pressure is one of which contains this information and it describes human walking features. It is proved that once one has trouble with lower limb, the distribution of plantar pressure will change to some degree. Plantar pressure can be converted into images according to some simple standards. In this paper, we take full advantage of these plantar pressure images for medical usage. We present N2RPP, a generative adversarial network (GAN) based method to rebuild plantar pressure images of anterior cruciate ligament deficiency (ACLD) patients from low dimension features, which are extracted from an autoencoder. Through the result of experiments, the extracted features are a useful representation to describe and rebuild plantar pressure images. According to N2RPP's results, we find out that there are several noteworthy differences between normal people and patients. This can provide doctors a rough direction of adjusting plantar pressure to a better distribution to reduce patients' sore and pain during the rehabilitation treatment for ACLD.



### Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/1805.02834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02834v2)
- **Published**: 2018-05-08 05:05:56+00:00
- **Updated**: 2018-07-20 15:53:44+00:00
- **Authors**: Luowei Zhou, Nathan Louis, Jason J. Corso
- **Comment**: 16 pages including Appendix
- **Journal**: None
- **Summary**: We study weakly-supervised video object grounding: given a video segment and a corresponding descriptive sentence, the goal is to localize objects that are mentioned from the sentence in the video. During training, no object bounding boxes are available, but the set of possible objects to be grounded is known beforehand. Existing approaches in the image domain use Multiple Instance Learning (MIL) to ground objects by enforcing matches between visual and semantic features. A naive extension of this approach to the video domain is to treat the entire segment as a bag of spatial object proposals. However, an object existing sparsely across multiple frames might not be detected completely since successfully spotting it from one single frame would trigger a satisfactory match. To this end, we propagate the weak supervisory signal from the segment level to frames that likely contain the target object. For frames that are unlikely to contain the target objects, we use an alternative penalty loss. We also leverage the interactions among objects as a textual guide for the grounding. We evaluate our model on the newly-collected benchmark YouCook2-BoundingBox and show improvements over competitive baselines.



### A Memory Network Approach for Story-based Temporal Summarization of 360° Videos
- **Arxiv ID**: http://arxiv.org/abs/1805.02838v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02838v3)
- **Published**: 2018-05-08 05:22:18+00:00
- **Updated**: 2018-06-18 15:05:21+00:00
- **Authors**: Sangho Lee, Jinyoung Sung, Youngjae Yu, Gunhee Kim
- **Comment**: Accepted paper at CVPR 2018
- **Journal**: None
- **Summary**: We address the problem of story-based temporal summarization of long 360{\deg} videos. We propose a novel memory network model named Past-Future Memory Network (PFMN), in which we first compute the scores of 81 normal field of view (NFOV) region proposals cropped from the input 360{\deg} video, and then recover a latent, collective summary using the network with two external memories that store the embeddings of previously selected subshots and future candidate subshots. Our major contributions are two-fold. First, our work is the first to address story-based temporal summarization of 360{\deg} videos. Second, our model is the first attempt to leverage memory networks for video summarization tasks. For evaluation, we perform three sets of experiments. First, we investigate the view selection capability of our model on the Pano2Vid dataset. Second, we evaluate the temporal summarization with a newly collected 360{\deg} video dataset. Finally, we experiment our model's performance in another domain, with image-based storytelling VIST dataset. We verify that our model achieves state-of-the-art performance on all the tasks.



### Joint Cell Nuclei Detection and Segmentation in Microscopy Images Using 3D Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.02850v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.02850v2)
- **Published**: 2018-05-08 06:15:33+00:00
- **Updated**: 2018-09-06 16:45:07+00:00
- **Authors**: Sundaresh Ram, Vicky T. Nguyen, Kirsten H. Limesand, Mert R. Sabuncu
- **Comment**: We were not able to reproduce the results
- **Journal**: None
- **Summary**: We propose a 3D convolutional neural network to simultaneously segment and detect cell nuclei in confocal microscopy images. Mirroring the co-dependency of these tasks, our proposed model consists of two serial components: the first part computes a segmentation of cell bodies, while the second module identifies the centers of these cells. Our model is trained end-to-end from scratch on a mouse parotid salivary gland stem cell nuclei dataset comprising 107 image stacks from three independent cell preparations, each containing several hundred individual cell nuclei in 3D. In our experiments, we conduct a thorough evaluation of both detection accuracy and segmentation quality, on two different datasets. The results show that the proposed method provides significantly improved detection and segmentation accuracy compared to state-of-the-art and benchmark algorithms. Finally, we use a previously described test-time drop-out strategy to obtain uncertainty estimates on our predictions and validate these estimates by demonstrating that they are strongly correlated with accuracy.



### Tile2Vec: Unsupervised representation learning for spatially distributed data
- **Arxiv ID**: http://arxiv.org/abs/1805.02855v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.02855v2)
- **Published**: 2018-05-08 06:40:40+00:00
- **Updated**: 2018-05-30 09:26:16+00:00
- **Authors**: Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, Stefano Ermon
- **Comment**: 8 pages, 4 figures in main text; 9 pages, 11 figures in appendix
- **Journal**: None
- **Summary**: Geospatial analysis lacks methods like the word vector representations and pre-trained networks that significantly boost performance across a wide range of natural language and computer vision tasks. To fill this gap, we introduce Tile2Vec, an unsupervised representation learning algorithm that extends the distributional hypothesis from natural language -- words appearing in similar contexts tend to have similar meanings -- to spatially distributed data. We demonstrate empirically that Tile2Vec learns semantically meaningful representations on three datasets. Our learned representations significantly improve performance in downstream classification tasks and, similar to word vectors, visual analogies can be obtained via simple arithmetic in the latent space.



### Visual Attribute-augmented Three-dimensional Convolutional Neural Network for Enhanced Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.02860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02860v1)
- **Published**: 2018-05-08 07:09:12+00:00
- **Updated**: 2018-05-08 07:09:12+00:00
- **Authors**: Yunfeng Wang, Wengang Zhou, Qilin Zhang, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attributes in individual video frames, such as the presence of characteristic objects and scenes, offer substantial information for action recognition in videos. With individual 2D video frame as input, visual attributes extraction could be achieved effectively and efficiently with more sophisticated convolutional neural network than current 3D CNNs with spatio-temporal filters, thanks to fewer parameters in 2D CNNs. In this paper, the integration of visual attributes (including detection, encoding and classification) into multi-stream 3D CNN is proposed for action recognition in trimmed videos, with the proposed visual Attribute-augmented 3D CNN (A3D) framework. The visual attribute pipeline includes an object detection network, an attributes encoding network and a classification network. Our proposed A3D framework achieves state-of-the-art performance on both the HMDB51 and the UCF101 datasets.



### Low-Latency Human Action Recognition with Weighted Multi-Region Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1805.02877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02877v1)
- **Published**: 2018-05-08 07:57:54+00:00
- **Updated**: 2018-05-08 07:57:54+00:00
- **Authors**: Yunfeng Wang, Wengang Zhou, Qilin Zhang, Xiaotian Zhu, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Spatio-temporal contexts are crucial in understanding human actions in videos. Recent state-of-the-art Convolutional Neural Network (ConvNet) based action recognition systems frequently involve 3D spatio-temporal ConvNet filters, chunking videos into fixed length clips and Long Short Term Memory (LSTM) networks. Such architectures are designed to take advantage of both short term and long term temporal contexts, but also requires the accumulation of a predefined number of video frames (e.g., to construct video clips for 3D ConvNet filters, to generate enough inputs for LSTMs). For applications that require low-latency online predictions of fast-changing action scenes, a new action recognition system is proposed in this paper. Termed "Weighted Multi-Region Convolutional Neural Network" (WMR ConvNet), the proposed system is LSTM-free, and is based on 2D ConvNet that does not require the accumulation of video frames for 3D ConvNet filtering. Unlike early 2D ConvNets that are based purely on RGB frames and optical flow frames, the WMR ConvNet is designed to simultaneously capture multiple spatial and short term temporal cues (e.g., human poses, occurrences of objects in the background) with both the primary region (foreground) and secondary regions (mostly background). On both the UCF101 and HMDB51 datasets, the proposed WMR ConvNet achieves the state-of-the-art performance among competing low-latency algorithms. Furthermore, WMR ConvNet even outperforms the 3D ConvNet based C3D algorithm that requires video frame accumulation. In an ablation study with the optical flow ConvNet stream removed, the ablated WMR ConvNet nevertheless outperforms competing algorithms.



### Image Ordinal Classification and Understanding: Grid Dropout with Masking Label
- **Arxiv ID**: http://arxiv.org/abs/1805.02901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02901v1)
- **Published**: 2018-05-08 08:58:54+00:00
- **Updated**: 2018-05-08 08:58:54+00:00
- **Authors**: Chao Zhang, Ce Zhu, Jimin Xiao, Xun Xu, Yipeng Liu
- **Comment**: IEEE International Conference on Multimedia Expo (ICME Oral
  Presentation)
- **Journal**: None
- **Summary**: Image ordinal classification refers to predicting a discrete target value which carries ordering correlation among image categories. The limited size of labeled ordinal data renders modern deep learning approaches easy to overfit. To tackle this issue, neuron dropout and data augmentation were proposed which, however, still suffer from over-parameterization and breaking spatial structure, respectively. To address the issues, we first propose a grid dropout method that randomly dropout/blackout some areas of the raining image. Then we combine the objective of predicting the blackout patches with classification to take advantage of the spatial information. Finally we demonstrate the effectiveness of both approaches by visualizing the Class Activation Map (CAM) and discover that grid dropout is more aware of the whole facial areas and more robust than neuron dropout for small training dataset. Experiments are conducted on a challenging age estimation dataset - Adience dataset with very competitive results compared with state-of-the-art methods.



### Learning Short-Cut Connections for Object Counting
- **Arxiv ID**: http://arxiv.org/abs/1805.02919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02919v2)
- **Published**: 2018-05-08 09:31:51+00:00
- **Updated**: 2018-11-15 11:58:05+00:00
- **Authors**: Daniel Oñoro-Rubio, Mathias Niepert, Roberto J. López-Sastre
- **Comment**: None
- **Journal**: None
- **Summary**: Object counting is an important task in computer vision due to its growing demand in applications such as traffic monitoring or surveillance. In this paper, we consider object counting as a learning problem of a joint feature extraction and pixel-wise object density estimation with Convolutional-Deconvolutional networks. We introduce a novel counting model, named Gated U-Net (GU-Net). Specifically, we propose to enrich the U-Net architecture with the concept of learnable short-cut connections. Standard short-cut connections are connections between layers in deep neural networks which skip at least one intermediate layer. Instead of simply setting short-cut connections, we propose to learn these connections from data. Therefore, our short-cuts can work as gating units, which optimize the flow of information between convolutional and deconvolutional layers in the U-Net architecture. We evaluate the introduced GU-Net architecture on three commonly used benchmark data sets for object counting. GU-Nets consistently outperform the base U-Net architecture, and achieve state-of-the-art performance.



### Comparing phonemes and visemes with DNN-based lipreading
- **Arxiv ID**: http://arxiv.org/abs/1805.02924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.02924v1)
- **Published**: 2018-05-08 09:51:34+00:00
- **Updated**: 2018-05-08 09:51:34+00:00
- **Authors**: Kwanchiva Thangthai, Helen L Bear, Richard Harvey
- **Comment**: None
- **Journal**: BMVC Lipreading Workshop 2017
- **Summary**: There is debate if phoneme or viseme units are the most effective for a lipreading system. Some studies use phoneme units even though phonemes describe unique short sounds; other studies tried to improve lipreading accuracy by focusing on visemes with varying results. We compare the performance of a lipreading system by modeling visual speech using either 13 viseme or 38 phoneme units. We report the accuracy of our system at both word and unit levels. The evaluation task is large vocabulary continuous speech using the TCD-TIMIT corpus. We complete our visual speech modeling via hybrid DNN-HMMs and our visual speech decoder is a Weighted Finite-State Transducer (WFST). We use DCT and Eigenlips as a representation of mouth ROI image. The phoneme lipreading system word accuracy outperforms the viseme based system word accuracy. However, the phoneme system achieved lower accuracy at the unit level which shows the importance of the dictionary for decoding classification outputs into words.



### Phoneme-to-viseme mappings: the good, the bad, and the ugly
- **Arxiv ID**: http://arxiv.org/abs/1805.02934v1
- **DOI**: 10.1016/j.specom.2017.07.001
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.02934v1)
- **Published**: 2018-05-08 10:32:57+00:00
- **Updated**: 2018-05-08 10:32:57+00:00
- **Authors**: Helen L Bear, Richard Harvey
- **Comment**: None
- **Journal**: Speech Communication, Special Issue on AV expressive speech. 2017
- **Summary**: Visemes are the visual equivalent of phonemes. Although not precisely defined, a working definition of a viseme is "a set of phonemes which have identical appearance on the lips". Therefore a phoneme falls into one viseme class but a viseme may represent many phonemes: a many to one mapping. This mapping introduces ambiguity between phonemes when using viseme classifiers. Not only is this ambiguity damaging to the performance of audio-visual classifiers operating on real expressive speech, there is also considerable choice between possible mappings. In this paper we explore the issue of this choice of viseme-to-phoneme map. We show that there is definite difference in performance between viseme-to-phoneme mappings and explore why some maps appear to work better than others. We also devise a new algorithm for constructing phoneme-to-viseme mappings from labeled speech data. These new visemes, `Bear' visemes, are shown to perform better than previously known units.



### Comparing heterogeneous visual gestures for measuring the diversity of visual speech signals
- **Arxiv ID**: http://arxiv.org/abs/1805.02948v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1805.02948v1)
- **Published**: 2018-05-08 11:17:52+00:00
- **Updated**: 2018-05-08 11:17:52+00:00
- **Authors**: Helen L Bear, Richard Harvey
- **Comment**: None
- **Journal**: Computer Speech and Language, May 2018
- **Summary**: Visual lip gestures observed whilst lipreading have a few working definitions, the most common two are; `the visual equivalent of a phoneme' and `phonemes which are indistinguishable on the lips'. To date there is no formal definition, in part because to date we have not established a two-way relationship or mapping between visemes and phonemes. Some evidence suggests that visual speech is highly dependent upon the speaker. So here, we use a phoneme-clustering method to form new phoneme-to-viseme maps for both individual and multiple speakers. We test these phoneme to viseme maps to examine how similarly speakers talk visually and we use signed rank tests to measure the distance between individuals. We conclude that broadly speaking, speakers have the same repertoire of mouth gestures, where they differ is in the use of the gestures.



### Moiré Photo Restoration Using Multiresolution Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.02996v1
- **DOI**: 10.1109/TIP.2018.2834737
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.02996v1)
- **Published**: 2018-05-08 13:17:44+00:00
- **Updated**: 2018-05-08 13:17:44+00:00
- **Authors**: Yujing Sun, Yizhou Yu, Wenping Wang
- **Comment**: 13 pages, 19 figures, accepted to appear in IEEE Transactions on
  Image Processing
- **Journal**: None
- **Summary**: Digital cameras and mobile phones enable us to conveniently record precious moments. While digital image quality is constantly being improved, taking high-quality photos of digital screens still remains challenging because the photos are often contaminated with moir\'{e} patterns, a result of the interference between the pixel grids of the camera sensor and the device screen. Moir\'{e} patterns can severely damage the visual quality of photos. However, few studies have aimed to solve this problem. In this paper, we introduce a novel multiresolution fully convolutional network for automatically removing moir\'{e} patterns from photos. Since a moir\'{e} pattern spans over a wide range of frequencies, our proposed network performs a nonlinear multiresolution analysis of the input image before computing how to cancel moir\'{e} artefacts within every frequency band. We also create a large-scale benchmark dataset with $100,000^+$ image pairs for investigating and evaluating moir\'{e} pattern removal algorithms. Our network achieves state-of-the-art performance on this dataset in comparison to existing learning architectures for image restoration problems.



### Category-Based Deep CCA for Fine-Grained Venue Discovery from Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/1805.02997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.02997v1)
- **Published**: 2018-05-08 13:17:57+00:00
- **Updated**: 2018-05-08 13:17:57+00:00
- **Authors**: Yi Yu, Suhua Tang, Kiyoharu Aizawa, Akiko Aizawa
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, travel destination and business location are taken as venues. Discovering a venue by a photo is very important for context-aware applications. Unfortunately, few efforts paid attention to complicated real images such as venue photos generated by users. Our goal is fine-grained venue discovery from heterogeneous social multimodal data. To this end, we propose a novel deep learning model, Category-based Deep Canonical Correlation Analysis (C-DCCA). Given a photo as input, this model performs (i) exact venue search (find the venue where the photo was taken), and (ii) group venue search (find relevant venues with the same category as that of the photo), by the cross-modal correlation between the input photo and textual description of venues. In this model, data in different modalities are projected to a same space via deep networks. Pairwise correlation (between different modal data from the same venue) for exact venue search and category-based correlation (between different modal data from different venues with the same category) for group venue search are jointly optimized. Because a photo cannot fully reflect rich text description of a venue, the number of photos per venue in the training phase is increased to capture more aspects of a venue. We build a new venue-aware multimodal dataset by integrating Wikipedia featured articles and Foursquare venue photos. Experimental results on this dataset confirm the feasibility of the proposed method. Moreover, the evaluation over another publicly available dataset confirms that the proposed method outperforms state-of-the-arts for cross-modal retrieval between image and text.



### Towards Accurate and High-Speed Spiking Neuromorphic Systems with Data Quantization-Aware Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.03054v3
- **DOI**: 10.1145/3195970.3196131
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03054v3)
- **Published**: 2018-05-08 14:30:19+00:00
- **Updated**: 2019-09-08 08:47:23+00:00
- **Authors**: Fuqiang Liu, C. Liu
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have gained immense success in cognitive applications and greatly pushed today's artificial intelligence forward. The biggest challenge in executing DNNs is their extremely data-extensive computations. The computing efficiency in speed and energy is constrained when traditional computing platforms are employed in such computational hungry executions. Spiking neuromorphic computing (SNC) has been widely investigated in deep networks implementation own to their high efficiency in computation and communication. However, weights and signals of DNNs are required to be quantized when deploying the DNNs on the SNC, which results in unacceptable accuracy loss. %However, the system accuracy is limited by quantizing data directly in deep networks deployment. Previous works mainly focus on weights discretize while inter-layer signals are mainly neglected. In this work, we propose to represent DNNs with fixed integer inter-layer signals and fixed-point weights while holding good accuracy. We implement the proposed DNNs on the memristor-based SNC system as a deployment example. With 4-bit data representation, our results show that the accuracy loss can be controlled within 0.02% (2.3%) on MNIST (CIFAR-10). Compared with the 8-bit dynamic fixed-point DNNs, our system can achieve more than 9.8x speedup, 89.1% energy saving, and 30% area saving.



### Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues
- **Arxiv ID**: http://arxiv.org/abs/1805.03064v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03064v3)
- **Published**: 2018-05-08 14:44:18+00:00
- **Updated**: 2018-09-17 11:09:42+00:00
- **Authors**: Cristina Palmero, Javier Selva, Mohammad Ali Bagheri, Sergio Escalera
- **Comment**: Proc. of British Machine Vision Conference (BMVC), BMVC 2018. Errata:
  in pg.5 the camera matrices of the transformation matrix W should be
  interchanged (correct version: W=C_n*M*(C_o)^-1)
- **Journal**: None
- **Summary**: Gaze behavior is an important non-verbal cue in social signal processing and human-computer interaction. In this paper, we tackle the problem of person- and head pose-independent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame. Our multi-modal static solution is evaluated on a wide range of head poses and gaze directions, achieving a significant improvement of 14.6% over the state of the art on EYEDIAP dataset, further improved by 4% when the temporal modality is included.



### Active Object Reconstruction Using a Guided View Planner
- **Arxiv ID**: http://arxiv.org/abs/1805.03081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03081v1)
- **Published**: 2018-05-08 15:00:23+00:00
- **Updated**: 2018-05-08 15:00:23+00:00
- **Authors**: Xin Yang, Yuanbo Wang, Yaru Wang, Baocai Yin, Qiang Zhang, Xiaopeng Wei, Hongbo Fu
- **Comment**: 7 pages,5 figures
- **Journal**: None
- **Summary**: Inspired by the recent advance of image-based object reconstruction using deep learning, we present an active reconstruction model using a guided view planner. We aim to reconstruct a 3D model using images observed from a planned sequence of informative and discriminative views. But where are such informative and discriminative views around an object? To address this we propose a unified model for view planning and object reconstruction, which is utilized to learn a guided information acquisition model and to aggregate information from a sequence of images for reconstruction. Experiments show that our model (1) increases our reconstruction accuracy with an increasing number of views (2) and generally predicts a more informative sequence of views for object reconstruction compared to other alternative methods.



### Fast Feature Extraction with CNNs with Pooling Layers
- **Arxiv ID**: http://arxiv.org/abs/1805.03096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.03096v1)
- **Published**: 2018-05-08 15:14:20+00:00
- **Updated**: 2018-05-08 15:14:20+00:00
- **Authors**: Christian Bailer, Tewodros Habtegebrial, Kiran varanasi, Didier Stricker
- **Comment**: Accepted at BMVC 2017
- **Journal**: None
- **Summary**: In recent years, many publications showed that convolutional neural network based features can have a superior performance to engineered features. However, not much effort was taken so far to extract local features efficiently for a whole image. In this paper, we present an approach to compute patch-based local feature descriptors efficiently in presence of pooling and striding layers for whole images at once. Our approach is generic and can be applied to nearly all existing network architectures. This includes networks for all local feature extraction tasks like camera calibration, Patchmatching, optical flow estimation and stereo matching. In addition, our approach can be applied to other patch-based approaches like sliding window object detection and recognition. We complete our paper with a speed benchmark of popular CNN based feature extraction approaches applied on a whole image, with and without our speedup, and example code (for Torch) that shows how an arbitrary CNN architecture can be easily converted by our approach.



### Learning on the Edge: Explicit Boundary Handling in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1805.03106v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1805.03106v1)
- **Published**: 2018-05-08 15:29:17+00:00
- **Updated**: 2018-05-08 15:29:17+00:00
- **Authors**: Carlo Innamorati, Tobias Ritschel, Tim Weyrich, Niloy J. Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) handle the case where filters extend beyond the image boundary using several heuristics, such as zero, repeat or mean padding. These schemes are applied in an ad-hoc fashion and, being weakly related to the image content and oblivious of the target task, result in low output quality at the boundary. In this paper, we propose a simple and effective improvement that learns the boundary handling itself. At training-time, the network is provided with a separate set of explicit boundary filters. At testing-time, we use these filters which have learned to extrapolate features at the boundary in an optimal way for the specific task. Our extensive evaluation, over a wide range of architectural changes (variations of layers, feature channels, or both), shows how the explicit filters result in improved boundary handling. Consequently, we demonstrate an improvement of 5% to 20% across the board of typical CNN applications (colorization, de-Bayering, optical flow, and disparity estimation).



### Image Retrieval with Mixed Initiative and Multimodal Feedback
- **Arxiv ID**: http://arxiv.org/abs/1805.03134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03134v1)
- **Published**: 2018-05-08 16:13:24+00:00
- **Updated**: 2018-05-08 16:13:24+00:00
- **Authors**: Nils Murrugarra-Llerena, Adriana Kovashka
- **Comment**: In submission to BMVC 2018
- **Journal**: None
- **Summary**: How would you search for a unique, fashionable shoe that a friend wore and you want to buy, but you didn't take a picture? Existing approaches propose interactive image search as a promising venue. However, they either entrust the user with taking the initiative to provide informative feedback, or give all control to the system which determines informative questions to ask. Instead, we propose a mixed-initiative framework where both the user and system can be active participants, depending on whose initiative will be more beneficial for obtaining high-quality search results. We develop a reinforcement learning approach which dynamically decides which of three interaction opportunities to give to the user: drawing a sketch, providing free-form attribute feedback, or answering attribute-based questions. By allowing these three options, our system optimizes both the informativeness and exploration capabilities allowing faster image retrieval. We outperform three baselines on three datasets and extensive experimental settings.



### High-resolution medical image synthesis using progressively grown generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1805.03144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03144v2)
- **Published**: 2018-05-08 16:25:13+00:00
- **Updated**: 2018-05-09 14:23:53+00:00
- **Authors**: Andrew Beers, James Brown, Ken Chang, J. Peter Campbell, Susan Ostmo, Michael F. Chiang, Jayashree Kalpathy-Cramer
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are a class of unsupervised machine learning algorithms that can produce realistic images from randomly-sampled vectors in a multi-dimensional space. Until recently, it was not possible to generate realistic high-resolution images using GANs, which has limited their applicability to medical images that contain biomarkers only detectable at native resolution. Progressive growing of GANs is an approach wherein an image generator is trained to initially synthesize low resolution synthetic images (8x8 pixels), which are then fed to a discriminator that distinguishes these synthetic images from real downsampled images. Additional convolutional layers are then iteratively introduced to produce images at twice the previous resolution until the desired resolution is reached. In this work, we demonstrate that this approach can produce realistic medical images in two different domains; fundus photographs exhibiting vascular pathology associated with retinopathy of prematurity (ROP), and multi-modal magnetic resonance images of glioma. We also show that fine-grained details associated with pathology, such as retinal vessels or tumor heterogeneity, can be preserved and enhanced by including segmentation maps as additional channels. We envisage several applications of the approach, including image augmentation and unsupervised classification of pathology.



### PAD-Net: A Perception-Aided Single Image Dehazing Network
- **Arxiv ID**: http://arxiv.org/abs/1805.03146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03146v1)
- **Published**: 2018-05-08 16:31:07+00:00
- **Updated**: 2018-05-08 16:31:07+00:00
- **Authors**: Yu Liu, Guanlong Zhao
- **Comment**: 8 pages, 4 figures; project page:
  https://github.com/guanlongzhao/single-image-dehazing
- **Journal**: None
- **Summary**: In this work, we investigate the possibility of replacing the $\ell_2$ loss with perceptually derived loss functions (SSIM, MS-SSIM, etc.) in training an end-to-end dehazing neural network. Objective experimental results suggest that by merely changing the loss function we can obtain significantly higher PSNR and SSIM scores on the SOTS set in the RESIDE dataset, compared with a state-of-the-art end-to-end dehazing neural network (AOD-Net) that uses the $\ell_2$ loss. The best PSNR we obtained was 23.50 (4.2% relative improvement), and the best SSIM we obtained was 0.8747 (2.3% relative improvement.)



### Superresolution method for data deconvolution by superposition of point sources
- **Arxiv ID**: http://arxiv.org/abs/1805.03170v2
- **DOI**: None
- **Categories**: **cs.CV**, math-ph, math.MP, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1805.03170v2)
- **Published**: 2018-05-08 17:13:31+00:00
- **Updated**: 2018-12-05 20:11:10+00:00
- **Authors**: Sandra Martínez, Oscar E. Martínez
- **Comment**: Title was changed to clarify that it is a single acquisition method
  and the sources superposed are virtual. We add a new example of the two
  straight segments synthesize an artificial image. We explain how we construct
  the IRF and we explain how we implement the genetic algorithm
- **Journal**: None
- **Summary**: In this work we present a new algorithm for data deconvolution that allows the retrieval of the target function with super-resolution with a simple approach that after a precis e measurement of the instrument response function (IRF), the measured data are fit by a superposition of point sources (SUPPOSe) of equal intensity. In this manner only the positions of the sources need to be determined by an algorithm that minimizes the norm of the difference between the measured data and the convolution of the superposed point sources with the IRF. An upper bound for the uncertainty in the position of the sources was derived and two very different experimental situations were used for the test (an optical spectrum and fluorescent microscopy images) showing excellent reconstructions and agreement with the predicted uncertainties, achieving {\lambda}/10 resolution for the microscope and a fivefold improvement in the spectral resolution for the spectrometer. The method also provides a way to determine the optimum number of sources to be used for the fit.



### Visual Global Localization with a Hybrid WNN-CNN Approach
- **Arxiv ID**: http://arxiv.org/abs/1805.03183v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1805.03183v2)
- **Published**: 2018-05-08 17:34:31+00:00
- **Updated**: 2018-05-14 18:57:37+00:00
- **Authors**: Avelino Forechi, Thiago Oliveira-Santos, Claudine Badue, Alberto F. De Souza
- **Comment**: Accepted by IEEE 2018 International Joint Conference on Neural
  Networks (IJCNN)
- **Journal**: None
- **Summary**: Currently, self-driving cars rely greatly on the Global Positioning System (GPS) infrastructure, albeit there is an increasing demand for alternative methods for GPS-denied environments. One of them is known as place recognition, which associates images of places with their corresponding positions. We previously proposed systems based on Weightless Neural Networks (WNN) to address this problem as a classification task. This encompasses solely one part of the global localization, which is not precise enough for driverless cars. Instead of just recognizing past places and outputting their poses, it is desired that a global localization system estimates the pose of current place images. In this paper, we propose to tackle this problem as follows. Firstly, given a live image, the place recognition system returns the most similar image and its pose. Then, given live and recollected images, a visual localization system outputs the relative camera pose represented by those images. To estimate the relative camera pose between the recollected and the current images, a Convolutional Neural Network (CNN) is trained with the two images as input and a relative pose vector as output. Together, these systems solve the global localization problem using the topological and metric information to approximate the current vehicle pose. The full approach is compared to a Real- Time Kinematic GPS system and a Simultaneous Localization and Mapping (SLAM) system. Experimental results show that the proposed approach correctly localizes a vehicle 90% of the time with a mean error of 1.20m compared to 1.12m of the SLAM system and 0.37m of the GPS, 89% of the time.



### Learning image-to-image translation using paired and unpaired training samples
- **Arxiv ID**: http://arxiv.org/abs/1805.03189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03189v1)
- **Published**: 2018-05-08 17:44:28+00:00
- **Updated**: 2018-05-08 17:44:28+00:00
- **Authors**: Soumya Tripathy, Juho Kannala, Esa Rahtu
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image translation is a general name for a task where an image from one domain is converted to a corresponding image in another domain, given sufficient training data. Traditionally different approaches have been proposed depending on whether aligned image pairs or two sets of (unaligned) examples from both domains are available for training. While paired training samples might be difficult to obtain, the unpaired setup leads to a highly under-constrained problem and inferior results. In this paper, we propose a new general purpose image-to-image translation model that is able to utilize both paired and unpaired training data simultaneously. We compare our method with two strong baselines and obtain both qualitatively and quantitatively improved results. Our model outperforms the baselines also in the case of purely paired and unpaired training data. To our knowledge, this is the first work to consider such hybrid setup in image-to-image translation.



### A Mixed Classification-Regression Framework for 3D Pose Estimation from 2D Images
- **Arxiv ID**: http://arxiv.org/abs/1805.03225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03225v1)
- **Published**: 2018-05-08 18:32:04+00:00
- **Updated**: 2018-05-08 18:32:04+00:00
- **Authors**: Siddharth Mahendran, Haider Ali, Rene Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: 3D pose estimation from a single 2D image is an important and challenging task in computer vision with applications in autonomous driving, robot manipulation and augmented reality. Since 3D pose is a continuous quantity, a natural formulation for this task is to solve a pose regression problem. However, since pose regression methods return a single estimate of the pose, they have difficulties handling multimodal pose distributions (e.g. in the case of symmetric objects). An alternative formulation, which can capture multimodal pose distributions, is to discretize the pose space into bins and solve a pose classification problem. However, pose classification methods can give large pose estimation errors depending on the coarseness of the discretization. In this paper, we propose a mixed classification-regression framework that uses a classification network to produce a discrete multimodal pose estimate and a regression network to produce a continuous refinement of the discrete estimate. The proposed framework can accommodate different architectures and loss functions, leading to multiple classification-regression models, some of which achieve state-of-the-art performance on the challenging Pascal3D+ dataset.



### Sparse Blind Deconvolution for Distributed Radar Autofocus Imaging
- **Arxiv ID**: http://arxiv.org/abs/1805.03269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.03269v1)
- **Published**: 2018-05-08 20:30:31+00:00
- **Updated**: 2018-05-08 20:30:31+00:00
- **Authors**: Hassan Mansour, Dehong Liu, Ulugbek S. Kamilov, Petros T. Boufounos
- **Comment**: None
- **Journal**: None
- **Summary**: A common problem that arises in radar imaging systems, especially those mounted on mobile platforms, is antenna position ambiguity. Approaches to resolve this ambiguity and correct position errors are generally known as radar autofocus. Common techniques that attempt to resolve the antenna ambiguity generally assume an unknown gain and phase error afflicting the radar measurements. However, ensuring identifiability and tractability of the unknown error imposes strict restrictions on the allowable antenna perturbations. Furthermore, these techniques are often not applicable in near-field imaging, where mapping the position ambiguity to phase errors breaks down.   In this paper, we propose an alternate formulation where the position error of each antenna is mapped to a spatial shift operator in the image-domain. Thus, the radar autofocus problem becomes a multichannel blind deconvolution problem, in which the radar measurements correspond to observations of a static radar image that is convolved with the spatial shift kernel associated with each antenna. To solve the reformulated problem, we also develop a block coordinate descent framework that leverages the sparsity and piece-wise smoothness of the radar scene, as well as the one-sparse property of the two dimensional shift kernels. We evaluate the performance of our approach using both simulated and experimental radar measurements, and demonstrate its superior performance compared to state-of-the-art methods.



### Fully Automated Segmentation of Hyperreflective Foci in Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1805.03278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.03278v1)
- **Published**: 2018-05-08 20:49:57+00:00
- **Updated**: 2018-05-08 20:49:57+00:00
- **Authors**: Thomas Schlegl, Hrvoje Bogunovic, Sophie Klimscha, Philipp Seeböck, Amir Sadeghipour, Bianca Gerendas, Sebastian M. Waldstein, Georg Langs, Ursula Schmidt-Erfurth
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic detection of disease related entities in retinal imaging data is relevant for disease- and treatment monitoring. It enables the quantitative assessment of large amounts of data and the corresponding study of disease characteristics. The presence of hyperreflective foci (HRF) is related to disease progression in various retinal diseases. Manual identification of HRF in spectral-domain optical coherence tomography (SD-OCT) scans is error-prone and tedious. We present a fully automated machine learning approach for segmenting HRF in SD-OCT scans. Evaluation on annotated OCT images of the retina demonstrates that a residual U-Net allows to segment HRF with high accuracy. As our dataset comprised data from different retinal diseases including age-related macular degeneration, diabetic macular edema and retinal vein occlusion, the algorithm can safely be applied in all of them though different pathophysiological origins are known.



### Highly Scalable Image Reconstruction using Deep Neural Networks with Bandpass Filtering
- **Arxiv ID**: http://arxiv.org/abs/1805.03300v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1805.03300v2)
- **Published**: 2018-05-08 21:42:53+00:00
- **Updated**: 2018-11-26 18:47:24+00:00
- **Authors**: Joseph Y. Cheng, Feiyu Chen, Marcus T. Alley, John M. Pauly, Shreyas S. Vasanawala
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: To increase the flexibility and scalability of deep neural networks for image reconstruction, a framework is proposed based on bandpass filtering. For many applications, sensing measurements are performed indirectly. For example, in magnetic resonance imaging, data are sampled in the frequency domain. The introduction of bandpass filtering enables leveraging known imaging physics while ensuring that the final reconstruction is consistent with actual measurements to maintain reconstruction accuracy. We demonstrate this flexible architecture for reconstructing subsampled datasets of MRI scans. The resulting high subsampling rates increase the speed of MRI acquisitions and enable the visualization rapid hemodynamics.



### The Effectiveness of Instance Normalization: a Strong Baseline for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1805.03305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.03305v1)
- **Published**: 2018-05-08 22:08:35+00:00
- **Updated**: 2018-05-08 22:08:35+00:00
- **Authors**: Zheng Xu, Xitong Yang, Xue Li, Xiaoshuai Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel deep neural network architecture for the challenging problem of single image dehazing, which aims to recover the clear image from a degraded hazy image. Instead of relying on hand-crafted image priors or explicitly estimating the components of the widely used atmospheric scattering model, our end-to-end system directly generates the clear image from an input hazy image. The proposed network has an encoder-decoder architecture with skip connections and instance normalization. We adopt the convolutional layers of the pre-trained VGG network as encoder to exploit the representation power of deep features, and demonstrate the effectiveness of instance normalization for image dehazing. Our simple yet effective network outperforms the state-of-the-art methods by a large margin on the benchmark datasets.




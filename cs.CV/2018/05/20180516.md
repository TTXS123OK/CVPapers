# Arxiv Papers in cs.CV on 2018-05-16
### Optical Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.06082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06082v2)
- **Published**: 2018-05-16 01:12:11+00:00
- **Updated**: 2018-05-29 12:36:27+00:00
- **Authors**: Grant Fennessy, Yevgeniy Vorobeychik
- **Comment**: Submitted to NIPS 2018
- **Journal**: None
- **Summary**: We develop a novel optical neural network (ONN) framework which introduces a degree of scalar invariance to image classification estima- tion. Taking a hint from the human eye, which has higher resolution near the center of the retina, images are broken out into multiple levels of varying zoom based on a focal point. Each level is passed through an identical convolutional neural network (CNN) in a Siamese fashion, and the results are recombined to produce a high accuracy estimate of the object class. ONNs act as a wrapper around existing CNNs, and can thus be applied to many existing algorithms to produce notable accuracy improvements without having to change the underlying architecture.



### PACT: Parameterized Clipping Activation for Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.06085v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.06085v2)
- **Published**: 2018-05-16 01:19:43+00:00
- **Updated**: 2018-07-17 07:33:19+00:00
- **Authors**: Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, Kailash Gopalakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemes have been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation. This technique, PArameterized Clipping acTivation (PACT), uses an activation clipping parameter $\alpha$ that is optimized during training to find the right quantization scale. PACT allows quantizing activations to arbitrary bit precisions, while achieving much better accuracy relative to published state-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance due to a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.



### An Evaluation of Deep CNN Baselines for Scene-Independent Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1805.06086v1
- **DOI**: 10.1109/CRV.2018.00049
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06086v1)
- **Published**: 2018-05-16 01:23:56+00:00
- **Updated**: 2018-05-16 01:23:56+00:00
- **Authors**: Paul Marchwica, Michael Jamieson, Parthipan Siva
- **Comment**: To be published in 2018 15th Conference on Computer and Robot Vision
  (CRV)
- **Journal**: None
- **Summary**: In recent years, a variety of proposed methods based on deep convolutional neural networks (CNNs) have improved the state of the art for large-scale person re-identification (ReID). While a large number of optimizations and network improvements have been proposed, there has been relatively little evaluation of the influence of training data and baseline network architecture. In particular, it is usually assumed either that networks are trained on labeled data from the deployment location (scene-dependent), or else adapted with unlabeled data, both of which complicate system deployment. In this paper, we investigate the feasibility of achieving scene-independent person ReID by forming a large composite dataset for training. We present an in-depth comparison of several CNN baseline architectures for both scene-dependent and scene-independent ReID, across a range of training dataset sizes. We show that scene-independent ReID can produce leading-edge results, competitive with unsupervised domain adaption techniques. Finally, we introduce a new dataset for comparing within-camera and across-camera person ReID.



### Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid
- **Arxiv ID**: http://arxiv.org/abs/1805.06115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06115v2)
- **Published**: 2018-05-16 03:27:02+00:00
- **Updated**: 2018-09-20 07:57:59+00:00
- **Authors**: Di Kang, Antoni Chan
- **Comment**: camera ready, accepted to BMVC 2018
- **Journal**: None
- **Summary**: Because of the powerful learning capability of deep neural networks, counting performance via density map estimation has improved significantly during the past several years. However, it is still very challenging due to severe occlusion, large scale variations, and perspective distortion. Scale variations (from image to image) coupled with perspective distortion (within one image) result in huge scale changes of the object size. Earlier methods based on convolutional neural networks (CNN) typically did not handle this scale variation explicitly, until Hydra-CNN and MCNN. MCNN uses three columns, each with different filter sizes, to extract features at different scales. In this paper, in contrast to using filters of different sizes, we utilize an image pyramid to deal with scale variations. It is more effective and efficient to resize the input fed into the network, as compared to using larger filter sizes. Secondly, we adaptively fuse the predictions from different scales (using adaptively changing per-pixel weights), which makes our method adapt to scale changes within an image. The adaptive fusing is achieved by generating an across-scale attention map, which softly selects a suitable scale for each pixel, followed by a 1x1 convolution. Extensive experiments on three popular datasets show very compelling results.



### Feature Affinity based Pseudo Labeling for Semi-supervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1805.06118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06118v1)
- **Published**: 2018-05-16 03:42:36+00:00
- **Updated**: 2018-05-16 03:42:36+00:00
- **Authors**: Guodong Ding, Shanshan Zhang, Salman Khan, Zhenmin Tang, Jian Zhang, Fatih Porikli
- **Comment**: 12 pages, 4 figures, 9 tables
- **Journal**: None
- **Summary**: Person re-identification aims to match a person's identity across multiple camera streams. Deep neural networks have been successfully applied to the challenging person re-identification task. One remarkable bottleneck is that the existing deep models are data hungry and require large amounts of labeled training data. Acquiring manual annotations for pedestrian identity matchings in large-scale surveillance camera installations is a highly cumbersome task. Here, we propose the first semi-supervised approach that performs pseudo-labeling by considering complex relationships between unlabeled and labeled training samples in the feature space. Our approach first approximates the actual data manifold by learning a generative model via adversarial training. Given the trained model, data augmentation can be performed by generating new synthetic data samples which are unlabeled. An open research problem is how to effectively use this additional data for improved feature learning. To this end, this work proposes a novel Feature Affinity based Pseudo-Labeling (FAPL) approach with two possible label encodings under a unified setting. Our approach measures the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. FAPL trains with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. Our extensive experiments on two standard large-scale datasets, Market-1501 and DukeMTMC-reID, demonstrate significant performance boosts over closely related competitors and outperforms state-of-the-art person re-identification techniques in most cases.



### Photorealistic Image Reconstruction from Hybrid Intensity and Event based Sensor
- **Arxiv ID**: http://arxiv.org/abs/1805.06140v4
- **DOI**: 10.1117/1.JEI.28.6.063012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06140v4)
- **Published**: 2018-05-16 05:49:25+00:00
- **Updated**: 2019-02-11 07:30:42+00:00
- **Authors**: Prasan A Shedligeri, Kaushik Mitra
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Event sensors output a stream of asynchronous brightness changes (called ``events'') at a very high temporal rate. Previous works on recovering the lost intensity information from the event sensor data have heavily relied on the event stream, which makes the reconstructed images non-photorealistic and also susceptible to noise in the event stream. We propose to reconstruct photorealistic intensity images from a hybrid sensor consisting of a low frame rate conventional camera, which has the scene texture information, along with the event sensor. To accomplish our task, we warp the low frame rate intensity images to temporally dense locations of the event data by estimating a spatially dense scene depth and temporally dense sensor ego-motion. The results obtained from our algorithm are more photorealistic compared to any of the previous state-of-the-art algorithms. We also demonstrate our algorithm's robustness to abrupt camera motion and noise in the event sensor data.



### Critical Points to Determine Persistence Homology
- **Arxiv ID**: http://arxiv.org/abs/1805.06148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1805.06148v1)
- **Published**: 2018-05-16 06:25:21+00:00
- **Updated**: 2018-05-16 06:25:21+00:00
- **Authors**: Charmin Asirimath, Jayampathy Ratnayake, Chathuranga Weeraddana
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Computation of the simplicial complexes of a large point cloud often relies on extracting a sample, to reduce the associated computational burden. The study considers sampling critical points of a Morse function associated to a point cloud, to approximate the Vietoris-Rips complex or the witness complex and compute persistence homology. The effectiveness of the novel approach is compared with the farthest point sampling, in a context of classifying human face images into ethnics groups using persistence homology.



### Zero-Shot Object Detection by Hybrid Region Embedding
- **Arxiv ID**: http://arxiv.org/abs/1805.06157v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06157v2)
- **Published**: 2018-05-16 07:01:27+00:00
- **Updated**: 2018-05-17 07:59:37+00:00
- **Authors**: Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis
- **Comment**: None
- **Journal**: Published in British Machine Vision Conference 2018
- **Summary**: Object detection is considered as one of the most challenging problems in computer vision, since it requires correct prediction of both classes and locations of objects in images. In this study, we define a more difficult scenario, namely zero-shot object detection (ZSD) where no visual training data is available for some of the target object classes. We present a novel approach to tackle this ZSD problem, where a convex combination of embeddings are used in conjunction with a detection framework. For evaluation of ZSD methods, we propose a simple dataset constructed from Fashion-MNIST images and also a custom zero-shot split for the Pascal VOC detection challenge. The experimental results suggest that our method yields promising results for ZSD.



### Lightweight Pyramid Networks for Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1805.06173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06173v1)
- **Published**: 2018-05-16 07:48:20+00:00
- **Updated**: 2018-05-16 07:48:20+00:00
- **Authors**: Xueyang Fu, Borong Liang, Yue Huang, Xinghao Ding, John Paisley
- **Comment**: submitted to IEEE Transactions on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Existing deep convolutional neural networks have found major success in image deraining, but at the expense of an enormous number of parameters. This limits their potential application, for example in mobile devices. In this paper, we propose a lightweight pyramid of networks (LPNet) for single image deraining. Instead of designing a complex network structures, we use domain-specific knowledge to simplify the learning process. Specifically, we find that by introducing the mature Gaussian-Laplacian image pyramid decomposition technology to the neural network, the learning problem at each pyramid level is greatly simplified and can be handled by a relatively shallow network with few parameters. We adopt recursive and residual network structures to build the proposed LPNet, which has less than 8K parameters while still achieving state-of-the-art performance on rain removal. We also discuss the potential value of LPNet for other low- and high-level vision tasks.



### Graph Edge Convolutional Neural Networks for Skeleton Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.06184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06184v2)
- **Published**: 2018-05-16 08:18:33+00:00
- **Updated**: 2018-05-31 01:42:04+00:00
- **Authors**: Xikun Zhang, Chang Xu, Xinmei Tian, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates body bones from skeleton data for skeleton based action recognition. Body joints, as the direct result of mature pose estimation technologies, are always the key concerns of traditional action recognition methods. However, instead of joints, we humans naturally identify how the human body moves according to shapes, lengths and places of bones, which are more obvious and stable for observation. Hence given graphs generated from skeleton data, we propose to develop convolutions over graph edges that correspond to bones in human skeleton. We describe an edge by integrating its spatial neighboring edges to explore the cooperation between different bones, as well as its temporal neighboring edges to address the consistency of movements in an action. A graph edge convolutional neural network is then designed for skeleton based action recognition. Considering the complementarity between graph node convolution and graph edge convolution, we additionally construct two hybrid neural networks to combine graph node convolutional neural network and graph edge convolutional neural network using shared intermediate layers. Experimental results on Kinetics and NTU-RGB+D datasets demonstrate that our graph edge convolution is effective to capture characteristic of actions and our graph edge convolutional neural network significantly outperforms existing state-of-art skeleton based action recognition methods. Additionally, more performance improvements can be achieved by the hybrid networks.



### Predicting the Next Best View for 3D Mesh Refinement
- **Arxiv ID**: http://arxiv.org/abs/1805.06207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1805.06207v1)
- **Published**: 2018-05-16 09:31:24+00:00
- **Updated**: 2018-05-16 09:31:24+00:00
- **Authors**: Luca Morreale, Andrea Romanoni, Matteo Matteucci
- **Comment**: 13 pages, 5 figures, to be published in IAS-15
- **Journal**: None
- **Summary**: 3D reconstruction is a core task in many applications such as robot navigation or sites inspections. Finding the best poses to capture part of the scene is one of the most challenging topic that goes under the name of Next Best View. Recently, many volumetric methods have been proposed; they choose the Next Best View by reasoning over a 3D voxelized space and by finding which pose minimizes the uncertainty decoded into the voxels. Such methods are effective, but they do not scale well since the underlaying representation requires a huge amount of memory. In this paper we propose a novel mesh-based approach which focuses on the worst reconstructed region of the environment mesh. We define a photo-consistent index to evaluate the 3D mesh accuracy, and an energy function over the worst regions of the mesh which takes into account the mutual parallax with respect to the previous cameras, the angle of incidence of the viewing ray to the surface and the visibility of the region. We test our approach over a well known dataset and achieve state-of-the-art results.



### Adversarial Training for Patient-Independent Feature Learning with IVOCT Data for Plaque Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.06223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06223v1)
- **Published**: 2018-05-16 10:08:15+00:00
- **Updated**: 2018-05-16 10:08:15+00:00
- **Authors**: Nils Gessert, Markus Heyder, Sarah Latus, David M. Leistner, Youssef S. Abdelwahed, Matthias Lutz, Alexander Schlaefer
- **Comment**: Presented at MIDL 2018 Conference
  https://openreview.net/forum?id=SJWY1Ujsz
- **Journal**: None
- **Summary**: Deep learning methods have shown impressive results for a variety of medical problems over the last few years. However, datasets tend to be small due to time-consuming annotation. As datasets with different patients are often very heterogeneous generalization to new patients can be difficult. This is complicated further if large differences in image acquisition can occur, which is common during intravascular optical coherence tomography for coronary plaque imaging. We address this problem with an adversarial training strategy where we force a part of a deep neural network to learn features that are independent of patient- or acquisitionspecific characteristics. We compare our regularization method to typical data augmentation strategies and show that our approach improves performance for a small medical dataset.



### Image Classification Based on Quantum KNN Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1805.06260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06260v1)
- **Published**: 2018-05-16 11:59:54+00:00
- **Updated**: 2018-05-16 11:59:54+00:00
- **Authors**: Yijie Dang, Nan Jiang, Hao Hu, Zhuoxiao Ji, Wenyin Zhang
- **Comment**: 19 pages, 9 Postscript figures
- **Journal**: None
- **Summary**: Image classification is an important task in the field of machine learning and image processing. However, the usually used classification method --- the K Nearest-Neighbor algorithm has high complexity, because its two main processes: similarity computing and searching are time-consuming. Especially in the era of big data, the problem is prominent when the amount of images to be classified is large. In this paper, we try to use the powerful parallel computing ability of quantum computers to optimize the efficiency of image classification. The scheme is based on quantum K Nearest-Neighbor algorithm. Firstly, the feature vectors of images are extracted on classical computers. Then the feature vectors are inputted into a quantum superposition state, which is used to achieve parallel computing of similarity. Next, the quantum minimum search algorithm is used to speed up searching process for similarity. Finally, the image is classified by quantum measurement. The complexity of the quantum algorithm is only O((kM)^(1/2)), which is superior to the classical algorithms. Moreover, the measurement step is executed only once to ensure the validity of the scheme. The experimental results show that, the classification accuracy is 83.1% on Graz-01 dataset and 78% on Caltech-101 dataset, which is close to existing classical algorithms. Hence, our quantum scheme has a good classification performance while greatly improving the efficiency.



### Robust 6D Object Pose Estimation with Stochastic Congruent Sets
- **Arxiv ID**: http://arxiv.org/abs/1805.06324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06324v1)
- **Published**: 2018-05-16 13:43:00+00:00
- **Updated**: 2018-05-16 13:43:00+00:00
- **Authors**: Chaitanya Mitash, Abdeslam Boularias, Kostas Bekris
- **Comment**: None
- **Journal**: None
- **Summary**: Object pose estimation is frequently achieved by first segmenting an RGB image and then, given depth data, registering the corresponding point cloud segment against the object's 3D model. Despite the progress due to CNNs, semantic segmentation output can be noisy, especially when the CNN is only trained on synthetic data. This causes registration methods to fail in estimating a good object pose. This work proposes a novel stochastic optimization process that treats the segmentation output of CNNs as a confidence probability. The algorithm, called Stochastic Congruent Sets (StoCS), samples pointsets on the point cloud according to the soft segmentation distribution and so as to agree with the object's known geometry. The pointsets are then matched to congruent sets on the 3D object model to generate pose estimates. StoCS is shown to be robust on an APC dataset, despite the fact the CNN is trained only on synthetic data. In the YCB dataset, StoCS outperforms a recent network for 6D pose estimation and alternative pointset matching techniques.



### Multi-task Learning for Macromolecule Classification, Segmentation and Coarse Structural Recovery in Cryo-Tomography
- **Arxiv ID**: http://arxiv.org/abs/1805.06332v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.06332v1)
- **Published**: 2018-05-16 13:58:09+00:00
- **Updated**: 2018-05-16 13:58:09+00:00
- **Authors**: Chang Liu, Xiangrui Zeng, Kaiwen Wang, Qiang Guo, Min Xu
- **Comment**: None
- **Journal**: British Machine Vision Conference (BMVC) 2018
- **Summary**: Cellular Electron Cryo-Tomography (CECT) is a powerful 3D imaging tool for studying the native structure and organization of macromolecules inside single cells. For systematic recognition and recovery of macromolecular structures captured by CECT, methods for several important tasks such as subtomogram classification and semantic segmentation have been developed. However, the recognition and recovery of macromolecular structures are still very difficult due to high molecular structural diversity, crowding molecular environment, and the imaging limitations of CECT. In this paper, we propose a novel multi-task 3D convolutional neural network model for simultaneous classification, segmentation, and coarse structural recovery of macromolecules of interest in subtomograms. In our model, the learned image features of one task are shared and thereby mutually reinforce the learning of other tasks. Evaluated on realistically simulated and experimental CECT data, our multi-task learning model outperformed all single-task learning methods for classification and segmentation. In addition, we demonstrate that our model can generalize to discover, segment and recover novel structures that do not exist in the training data.



### Auxiliary Tasks in Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.06334v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.06334v2)
- **Published**: 2018-05-16 13:59:20+00:00
- **Updated**: 2018-05-17 16:12:16+00:00
- **Authors**: Lukas Liebel, Marco KÃ¶rner
- **Comment**: fixed minor typesetting issue
- **Journal**: None
- **Summary**: Multi-task convolutional neural networks (CNNs) have shown impressive results for certain combinations of tasks, such as single-image depth estimation (SIDE) and semantic segmentation. This is achieved by pushing the network towards learning a robust representation that generalizes well to different atomic tasks. We extend this concept by adding auxiliary tasks, which are of minor relevance for the application, to the set of learned tasks. As a kind of additional regularization, they are expected to boost the performance of the ultimately desired main tasks. To study the proposed approach, we picked vision-based road scene understanding (RSU) as an exemplary application. Since multi-task learning requires specialized datasets, particularly when using extensive sets of tasks, we provide a multi-modal dataset for multi-task RSU, called synMT. More than 2.5 $\cdot$ 10^5 synthetic images, annotated with 21 different labels, were acquired from the video game Grand Theft Auto V (GTA V). Our proposed deep multi-task CNN architecture was trained on various combination of tasks using synMT. The experiments confirmed that auxiliary tasks can indeed boost network performance, both in terms of final results and training time.



### Automatic segmentation of the spinal cord and intramedullary multiple sclerosis lesions with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.06349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06349v2)
- **Published**: 2018-05-16 14:39:23+00:00
- **Updated**: 2018-09-11 22:45:24+00:00
- **Authors**: Charley Gros, Benjamin De Leener, Atef Badji, Josefina Maranzano, Dominique Eden, Sara M. Dupont, Jason Talbott, Ren Zhuoquiong, Yaou Liu, Tobias Granberg, Russell Ouellette, Yasuhiko Tachibana, Masaaki Hori, Kouhei Kamiya, Lydia Chougar, Leszek Stawiarz, Jan Hillert, Elise Bannier, Anne Kerbrat, Gilles Edan, Pierre Labauge, Virginie Callot, Jean Pelletier, Bertrand Audoin, Henitsoa Rasoanandrianina, Jean-Christophe Brisset, Paola Valsasina, Maria A. Rocca, Massimo Filippi, Rohit Bakshi, Shahamat Tauhid, Ferran Prados, Marios Yiannakas, Hugh Kearney, Olga Ciccarelli, Seth Smith, Constantina Andrada Treaba, Caterina Mainero, Jennifer Lefeuvre, Daniel S. Reich, Govind Nair, Vincent Auclair, Donald G. McLaren, Allan R. Martin, Michael G. Fehlings, Shahabeddin Vahdat, Ali Khatibi, Julien Doyon, Timothy Shepherd, Erik Charlson, Sridar Narayanan, Julien Cohen-Adad
- **Comment**: 38 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: The spinal cord is frequently affected by atrophy and/or lesions in multiple sclerosis (MS) patients. Segmentation of the spinal cord and lesions from MRI data provides measures of damage, which are key criteria for the diagnosis, prognosis, and longitudinal monitoring in MS. Automating this operation eliminates inter-rater variability and increases the efficiency of large-throughput analysis pipelines. Robust and reliable segmentation across multi-site spinal cord data is challenging because of the large variability related to acquisition parameters and image artifacts. The goal of this study was to develop a fully-automatic framework, robust to variability in both image parameters and clinical condition, for segmentation of the spinal cord and intramedullary MS lesions from conventional MRI data. Scans of 1,042 subjects (459 healthy controls, 471 MS patients, and 112 with other spinal pathologies) were included in this multi-site study (n=30). Data spanned three contrasts (T1-, T2-, and T2*-weighted) for a total of 1,943 volumes. The proposed cord and lesion automatic segmentation approach is based on a sequence of two Convolutional Neural Networks (CNNs). To deal with the very small proportion of spinal cord and/or lesion voxels compared to the rest of the volume, a first CNN with 2D dilated convolutions detects the spinal cord centerline, followed by a second CNN with 3D convolutions that segments the spinal cord and/or lesions. When compared against manual segmentation, our CNN-based approach showed a median Dice of 95% vs. 88% for PropSeg, a state-of-the-art spinal cord segmentation method. Regarding lesion segmentation on MS data, our framework provided a Dice of 60%, a relative volume difference of -15%, and a lesion-wise detection sensitivity and precision of 83% and 77%, respectively. The proposed framework is open-source and readily available in the Spinal Cord Toolbox.



### Object detection at 200 Frames Per Second
- **Arxiv ID**: http://arxiv.org/abs/1805.06361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06361v1)
- **Published**: 2018-05-16 15:07:09+00:00
- **Updated**: 2018-05-16 15:07:09+00:00
- **Authors**: Rakesh Mehta, Cemalettin Ozturk
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an efficient and fast object detector which can process hundreds of frames per second. To achieve this goal we investigate three main aspects of the object detection framework: network architecture, loss function and training data (labeled and unlabeled). In order to obtain compact network architecture, we introduce various improvements, based on recent work, to develop an architecture which is computationally light-weight and achieves a reasonable performance. To further improve the performance, while keeping the complexity same, we utilize distillation loss function. Using distillation loss we transfer the knowledge of a more accurate teacher network to proposed light-weight student network. We propose various innovations to make distillation efficient for the proposed one stage detector pipeline: objectness scaled distillation loss, feature map non-maximal suppression and a single unified distillation loss function for detection. Finally, building upon the distillation loss, we explore how much can we push the performance by utilizing the unlabeled data. We train our model with unlabeled data using the soft labels of the teacher network. Our final network consists of 10x fewer parameters than the VGG based object detection network and it achieves a speed of more than 200 FPS and proposed changes improve the detection accuracy by 14 mAP over the baseline on Pascal dataset.



### Fast Retinomorphic Event Stream for Video Recognition and Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.06374v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06374v2)
- **Published**: 2018-05-16 15:42:37+00:00
- **Updated**: 2018-05-19 15:10:37+00:00
- **Authors**: Wanjia Liu, Huaijin Chen, Rishab Goel, Yuzhong Huang, Ashok Veeraraghavan, Ankit Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Good temporal representations are crucial for video understanding, and the state-of-the-art video recognition framework is based on two-stream networks. In such framework, besides the regular ConvNets responsible for RGB frame inputs, a second network is introduced to handle the temporal representation, usually the optical flow (OF). However, OF or other task-oriented flow is computationally costly, and is thus typically pre-computed. Critically, this prevents the two-stream approach from being applied to reinforcement learning (RL) applications such as video game playing, where the next state depends on current state and action choices. Inspired by the early vision systems of mammals and insects, we propose a fast event-driven representation (EDR) that models several major properties of early retinal circuits: (1) logarithmic input response, (2) multi-timescale temporal smoothing to filter noise, and (3) bipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the directional information for fast speed (> 9000 fps), EDR en-ables fast real-time inference/learning in video applications that require interaction between an agent and the world such as game-playing, virtual robotics, and domain adaptation. In this vein, we use EDR to demonstrate performance improvements over state-of-the-art reinforcement learning algorithms for Atari games, something that has not been possible with pre-computed OF. Moreover, with UCF-101 video action recognition experiments, we show that EDR performs near state-of-the-art in accuracy while achieving a 1,500x speedup in input representation processing, as compared to optical flow.



### Neural Multi-scale Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1805.06386v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.06386v1)
- **Published**: 2018-05-16 16:00:01+00:00
- **Updated**: 2018-05-16 16:00:01+00:00
- **Authors**: Ken Nakanishi, Shin-ichi Maeda, Takeru Miyato, Daisuke Okanohara
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: This study presents a new lossy image compression method that utilizes the multi-scale features of natural images. Our model consists of two networks: multi-scale lossy autoencoder and parallel multi-scale lossless coder. The multi-scale lossy autoencoder extracts the multi-scale image features to quantized variables and the parallel multi-scale lossless coder enables rapid and accurate lossless coding of the quantized variables via encoding/decoding the variables in parallel. Our proposed model achieves comparable performance to the state-of-the-art model on Kodak and RAISE-1k dataset images, and it encodes a PNG image of size $768 \times 512$ in 70 ms with a single GPU and a single CPU process and decodes it into a high-fidelity image in approximately 200 ms.



### When Regression Meets Manifold Learning for Object Recognition and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.06400v1
- **DOI**: 10.1109/ICRA.2018.8460654
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06400v1)
- **Published**: 2018-05-16 16:11:00+00:00
- **Updated**: 2018-05-16 16:11:00+00:00
- **Authors**: Mai Bui, Sergey Zakharov, Shadi Albarqouni, Slobodan Ilic, Nassir Navab
- **Comment**: None
- **Journal**: 2018 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: In this work, we propose a method for object recognition and pose estimation from depth images using convolutional neural networks. Previous methods addressing this problem rely on manifold learning to learn low dimensional viewpoint descriptors and employ them in a nearest neighbor search on an estimated descriptor space. In comparison we create an efficient multi-task learning framework combining manifold descriptor learning and pose regression. By combining the strengths of manifold learning using triplet loss and pose regression, we could either estimate the pose directly reducing the complexity compared to NN search, or use learned descriptor for the NN descriptor matching. By in depth experimental evaluation of the novel loss function we observed that the view descriptors learned by the network are much more discriminative resulting in almost 30% increase regarding relative pose accuracy compared to related works. On the other hand, regarding directly regressed poses we obtained important improvement compared to simple pose regression. By leveraging the advantages of both manifold learning and regression tasks, we are able to improve the current state-of-the-art for object recognition and pose retrieval that we demonstrate through in depth experimental evaluation.



### Deep Segmentation and Registration in X-Ray Angiography Video
- **Arxiv ID**: http://arxiv.org/abs/1805.06406v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06406v2)
- **Published**: 2018-05-16 16:27:13+00:00
- **Updated**: 2018-08-03 10:20:57+00:00
- **Authors**: Athanasios Vlontzos, Krystian Mikolajczyk
- **Comment**: To appear in BMVC 2018
- **Journal**: None
- **Summary**: In interventional radiology, short video sequences of vein structure in motion are captured in order to help medical personnel identify vascular issues or plan intervention. Semantic segmentation can greatly improve the usefulness of these videos by indicating exact position of vessels and instruments, thus reducing the ambiguity. We propose a real-time segmentation method for these tasks, based on U-Net network trained in a Siamese architecture from automatically generated annotations. We make use of noisy low level binary segmentation and optical flow to generate multi class annotations that are successively improved in a multistage segmentation approach. We significantly improve the performance of a state of the art U-Net at the processing speeds of 90fps.



### Resisting Large Data Variations via Introspective Transformation Network
- **Arxiv ID**: http://arxiv.org/abs/1805.06447v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06447v3)
- **Published**: 2018-05-16 17:53:21+00:00
- **Updated**: 2020-06-26 06:13:13+00:00
- **Authors**: Yunhan Zhao, Ye Tian, Charless Fowlkes, Wei Shen, Alan Yuille
- **Comment**: camera-ready version, WACV 2020
- **Journal**: None
- **Summary**: Training deep networks that generalize to a wide range of variations in test data is essential to building accurate and robust image classifiers. One standard strategy is to apply data augmentation to synthetically enlarge the training set. However, data augmentation is essentially a brute-force method which generates uniform samples from some pre-defined set of transformations. In this paper, we propose a principled approach to train networks with significantly improved resistance to large variations between training and testing data. This is achieved by embedding a learnable transformation module into the introspective network, which is a convolutional neural network (CNN) classifier empowered with generative capabilities. Our approach alternates between synthesizing pseudo-negative samples and transformed positive examples based on the current model, and optimizing model predictions on these synthesized samples. Experimental results verify that our approach significantly improves the ability of deep networks to resist large variations between training and testing data and achieves classification accuracy improvements on several benchmark datasets, including MNIST, affNIST, SVHN, CIFAR-10 and miniImageNet.



### QuaterNet: A Quaternion-based Recurrent Model for Human Motion
- **Arxiv ID**: http://arxiv.org/abs/1805.06485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.06485v2)
- **Published**: 2018-05-16 18:38:37+00:00
- **Updated**: 2018-07-31 20:45:13+00:00
- **Authors**: Dario Pavllo, David Grangier, Michael Auli
- **Comment**: British Machine Vision Conference (BMVC), 2018
- **Journal**: None
- **Summary**: Deep learning for predicting or generating 3D human pose sequences is an active research area. Previous work regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angle or exponential map parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. Our recurrent network, QuaterNet, represents rotations with quaternions and our loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors. On short-term predictions, QuaterNet improves the state-of-the-art quantitatively. For long-term generation, our approach is qualitatively judged as realistic as recent neural strategies from the graphics literature.



### Defoiling Foiled Image Captions
- **Arxiv ID**: http://arxiv.org/abs/1805.06549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1805.06549v1)
- **Published**: 2018-05-16 22:50:17+00:00
- **Updated**: 2018-05-16 22:50:17+00:00
- **Authors**: Pranava Madhyastha, Josiah Wang, Lucia Specia
- **Comment**: In Proceedings of the 2018 Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL 2018)
- **Journal**: None
- **Summary**: We address the task of detecting foiled image captions, i.e. identifying whether a caption contains a word that has been deliberately replaced by a semantically similar word, thus rendering it inaccurate with respect to the image being described. Solving this problem should in principle require a fine-grained understanding of images to detect linguistically valid perturbations in captions. In such contexts, encoding sufficiently descriptive image information becomes a key challenge. In this paper, we demonstrate that it is possible to solve this task using simple, interpretable yet powerful representations based on explicit object information. Our models achieve state-of-the-art performance on a standard dataset, with scores exceeding those achieved by humans on the task. We also measure the upper-bound performance of our models using gold standard annotations. Our analysis reveals that the simpler model performs well even without image information, suggesting that the dataset contains strong linguistic bias.




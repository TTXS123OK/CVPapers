# Arxiv Papers in cs.CV on 2018-05-02
### Fused Deep Neural Networks for Efficient Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.08688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08688v1)
- **Published**: 2018-05-02 00:13:28+00:00
- **Updated**: 2018-05-02 00:13:28+00:00
- **Authors**: Xianzhi Du, Mostafa El-Khamy, Vlad I. Morariu, Jungwon Lee, Larry Davis
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In this paper, we present an efficient pedestrian detection system, designed by fusion of multiple deep neural network (DNN) systems. Pedestrian candidates are first generated by a single shot convolutional multi-box detector at different locations with various scales and aspect ratios. The candidate generator is designed to provide the majority of ground truth pedestrian annotations at the cost of a large number of false positives. Then, a classification system using the idea of ensemble learning is deployed to improve the detection accuracy. The classification system further classifies the generated candidates based on opinions of multiple deep verification networks and a fusion network which utilizes a novel soft-rejection fusion method to adjust the confidence in the detection results. To improve the training of the deep verification networks, a novel soft-label method is devised to assign floating point labels to the generated pedestrian candidates. A deep context aggregation semantic segmentation network also provides pixel-level classification of the scene and its results are softly fused with the detection results by the single shot detector. Our pedestrian detector compared favorably to state-of-art methods on all popular pedestrian detection datasets. For example, our fused DNN has better detection accuracy on the Caltech Pedestrian dataset than all previous state of art methods, while also being the fastest. We significantly improved the log-average miss rate on the Caltech pedestrian dataset to 7.67% and achieved the new state-of-the-art.



### Structure-sensitive Multi-scale Deep Neural Network for Low-Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/1805.00587v3
- **DOI**: 10.1109/ACCESS.2018.2858196
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1805.00587v3)
- **Published**: 2018-05-02 00:37:05+00:00
- **Updated**: 2018-08-10 06:36:06+00:00
- **Authors**: Chenyu You, Qingsong Yang, Hongming Shan, Lars Gjesteby, Guang Li, Shenghong Ju, Zhuiyang Zhang, Zhen Zhao, Yi Zhang, Wenxiang Cong, Ge Wang
- **Comment**: IEEE Access 2018
- **Journal**: None
- **Summary**: Computed tomography (CT) is a popular medical imaging modality in clinical applications. At the same time, the x-ray radiation dose associated with CT scans raises public concerns due to its potential risks to the patients. Over the past years, major efforts have been dedicated to the development of Low-Dose CT (LDCT) methods. However, the radiation dose reduction compromises the signal-to-noise ratio (SNR), leading to strong noise and artifacts that down-grade CT image quality. In this paper, we propose a novel 3D noise reduction method, called Structure-sensitive Multi-scale Generative Adversarial Net (SMGAN), to improve the LDCT image quality. Specifically, we incorporate three-dimensional (3D) volumetric information to improve the image quality. Also, different loss functions for training denoising models are investigated. Experiments show that the proposed method can effectively preserve structural and texture information from normal-dose CT (NDCT) images, and significantly suppress noise and artifacts. Qualitative visual assessments by three experienced radiologists demonstrate that the proposed method retrieves more detailed information, and outperforms competing methods.



### Structured Analysis Dictionary Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1805.00597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00597v1)
- **Published**: 2018-05-02 01:45:26+00:00
- **Updated**: 2018-05-02 01:45:26+00:00
- **Authors**: Wen Tang, Ashkan Panahi, Hamid Krim, Liyi Dai
- **Comment**: This is the final version accepted by ICASSP 2018
- **Journal**: None
- **Summary**: We propose a computationally efficient and high-performance classification algorithm by incorporating class structural information in analysis dictionary learning. To achieve more consistent classification, we associate a class characteristic structure of independent subspaces and impose it on the classification error constrained analysis dictionary learning. Experiments demonstrate that our method achieves a comparable or better performance than the state-of-the-art algorithms in a variety of visual classification tasks. In addition, our method greatly reduces the training and testing computational complexity.



### Bi-directional Graph Structure Information Model for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.00603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00603v2)
- **Published**: 2018-05-02 02:25:28+00:00
- **Updated**: 2018-08-19 11:55:59+00:00
- **Authors**: Jing Wang, Ze Peng, Pei Lv, Junyi Sun, Bing Zhou, Mingliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel multi-stage network architecture with two branches in each stage to estimate multi-person poses in images. The first branch predicts the confidence maps of joints and uses a geometrical transform kernel to propagate information between neighboring joints at the confidence level. The second branch proposes a bi-directional graph structure information model (BGSIM) to encode rich contextual information and to infer the occlusion relationship among different joints. We dynamically determine the joint point with highest response of the confidence maps as base point of passing message in BGSIM. Based on the proposed network structure, we achieve an average precision of 62.9 on the COCO Keypoint Challenge dataset and 77.6 on the MPII (multi-person) dataset. Compared with other state-of-art methods, our method can achieve highly promising results on our selected multi-person dataset without extra training.



### Towards Interpretable Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.00611v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00611v2)
- **Published**: 2018-05-02 03:46:47+00:00
- **Updated**: 2019-08-17 17:02:30+00:00
- **Authors**: Bangjie Yin, Luan Tran, Haoxiang Li, Xiaohui Shen, Xiaoming Liu
- **Comment**: 10 pages, 9 figures, 6 tables, To appear in ICCV 2019 as an oral
  paper
- **Journal**: None
- **Summary**: Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to improve face recognition accuracy with easily interpretable face representations.



### Deep Perm-Set Net: Learn to predict sets with unknown permutation and cardinality using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.00613v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00613v4)
- **Published**: 2018-05-02 03:49:39+00:00
- **Updated**: 2018-10-02 17:05:03+00:00
- **Authors**: S. Hamid Rezatofighi, Roman Kaskman, Farbod T. Motlagh, Qinfeng Shi, Daniel Cremers, Laura Leal-Taixé, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world problems, e.g. object detection, have outputs that are naturally expressed as sets of entities. This creates a challenge for traditional deep neural networks which naturally deal with structured outputs such as vectors, matrices or tensors. We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. Specifically, in our formulation we incorporate the permutation as unobservable variable and estimate its distribution during the learning process using alternating optimization. We demonstrate the validity of this new formulation on two relevant vision problems: object detection, for which our formulation outperforms state-of-the-art detectors such as Faster R-CNN and YOLO, and a complex CAPTCHA test, where we observe that, surprisingly, our set based network acquired the ability of mimicking arithmetics without any rules being coded.



### Multimodal Utterance-level Affect Analysis using Visual, Audio and Text Features
- **Arxiv ID**: http://arxiv.org/abs/1805.00625v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1805.00625v2)
- **Published**: 2018-05-02 05:05:32+00:00
- **Updated**: 2018-05-04 11:24:41+00:00
- **Authors**: Didan Deng, Yuqian Zhou, Jimin Pi, Bertram E. Shi
- **Comment**: 5 pages, 1 figure, subject to the 2018 IJCNN challenge on One-Minute
  Gradual-Emotion Recognition
- **Journal**: None
- **Summary**: The integration of information across multiple modalities and across time is a promising way to enhance the emotion recognition performance of affective systems. Much previous work has focused on instantaneous emotion recognition. The 2018 One-Minute Gradual-Emotion Recognition (OMG-Emotion) challenge, which was held in conjunction with the IEEE World Congress on Computational Intelligence, encouraged participants to address long-term emotion recognition by integrating cues from multiple modalities, including facial expression, audio and language. Intuitively, a multi-modal inference network should be able to leverage information from each modality and their correlations to improve recognition over that achievable by a single modality network. We describe here a multi-modal neural architecture that integrates visual information over time using an LSTM, and combines it with utterance level audio and text cues to recognize human sentiment from multimodal clips. Our model outperforms the unimodal baseline, achieving the concordance correlation coefficients (CCC) of 0.400 on the arousal task, and 0.353 on the valence task.



### Interpretable Fully Convolutional Classification of Intrapapillary Capillary Loops for Real-Time Detection of Early Squamous Neoplasia
- **Arxiv ID**: http://arxiv.org/abs/1805.00632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00632v1)
- **Published**: 2018-05-02 05:28:46+00:00
- **Updated**: 2018-05-02 05:28:46+00:00
- **Authors**: Luis C. Garcia-Peraza-Herrera, Martin Everson, Wenqi Li, Inmanol Luengo, Lorenz Berger, Omer Ahmad, Laurence Lovat, Hsiu-Po Wang, Wen-Lun Wang, Rehan Haidry, Danail Stoyanov, Tom Vercauteren, Sebastien Ourselin
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we have concentrated our efforts on the interpretability of classification results coming from a fully convolutional neural network. Motivated by the classification of oesophageal tissue for real-time detection of early squamous neoplasia, the most frequent kind of oesophageal cancer in Asia, we present a new dataset and a novel deep learning method that by means of deep supervision and a newly introduced concept, the embedded Class Activation Map (eCAM), focuses on the interpretability of results as a design constraint of a convolutional network. We present a new approach to visualise attention that aims to give some insights on those areas of the oesophageal tissue that lead a network to conclude that the images belong to a particular class and compare them with those visual features employed by clinicians to produce a clinical diagnosis. In comparison to a baseline method which does not feature deep supervision but provides attention by grafting Class Activation Maps, we improve the F1-score from 87.3% to 92.7% and provide more detailed attention maps.



### A Deep Network for Arousal-Valence Emotion Prediction with Acoustic-Visual Cues
- **Arxiv ID**: http://arxiv.org/abs/1805.00638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00638v2)
- **Published**: 2018-05-02 06:03:47+00:00
- **Updated**: 2019-06-25 05:11:09+00:00
- **Authors**: Songyou Peng, Le Zhang, Yutong Ban, Meng Fang, Stefan Winkler
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we comprehensively describe the methodology of our submissions to the One-Minute Gradual-Emotion Behavior Challenge 2018.



### MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories and head poses
- **Arxiv ID**: http://arxiv.org/abs/1805.00652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00652v1)
- **Published**: 2018-05-02 07:21:45+00:00
- **Updated**: 2018-05-02 07:21:45+00:00
- **Authors**: Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Alessio Del Bue, Fabio Galasso, Marco Cristani
- **Comment**: 10 pages, 3 figures to appear in CVPR 2018
- **Journal**: None
- **Summary**: Recent approaches on trajectory forecasting use tracklets to predict the future positions of pedestrians exploiting Long Short Term Memory (LSTM) architectures. This paper shows that adding vislets, that is, short sequences of head pose estimations, allows to increase significantly the trajectory forecasting performance. We then propose to use vislets in a novel framework called MX-LSTM, capturing the interplay between tracklets and vislets thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. At the same time, MX-LSTM predicts the future head poses, increasing the standard capabilities of the long-term trajectory forecasting approaches. With standard head pose estimators and an attentional-based social pooling, MX-LSTM scores the new trajectory forecasting state-of-the-art in all the considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic margin when the pedestrians slow down, a case where most of the forecasting approaches struggle to provide an accurate solution.



### Object Detection and Classification in Occupancy Grid Maps using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08689v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.08689v2)
- **Published**: 2018-05-02 07:31:03+00:00
- **Updated**: 2018-12-05 17:28:10+00:00
- **Authors**: Sascha Wirges, Tom Fischer, Jesus Balado Frias, Christoph Stiller
- **Comment**: 6 pages, 4 tables, 4 figures
- **Journal**: 2018 IEEE Intelligent Transportation Systems Conference (ITSC)
- **Summary**: A detailed environment perception is a crucial component of automated vehicles. However, to deal with the amount of perceived information, we also require segmentation strategies. Based on a grid map environment representation, well-suited for sensor fusion, free-space estimation and machine learning, we detect and classify objects using deep convolutional neural networks. As input for our networks we use a multi-layer grid map efficiently encoding 3D range sensor information. The inference output consists of a list of rotated bounding boxes with associated semantic classes. We conduct extensive ablation studies, highlight important design considerations when using grid maps and evaluate our models on the KITTI Bird's Eye View benchmark. Qualitative and quantitative benchmark results show that we achieve robust detection and state of the art accuracy solely using top-view grid maps from range sensor data.



### Convolutional Sequence to Sequence Model for Human Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1805.00655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00655v1)
- **Published**: 2018-05-02 07:42:04+00:00
- **Updated**: 2018-05-02 07:42:04+00:00
- **Authors**: Chen Li, Zhen Zhang, Wee Sun Lee, Gim Hee Lee
- **Comment**: CVPR2018
- **Journal**: None
- **Summary**: Human motion modeling is a classic problem in computer vision and graphics. Challenges in modeling human motion include high dimensional prediction as well as extremely complicated dynamics.We present a novel approach to human motion modeling based on convolutional neural networks (CNN). The hierarchical structure of CNN makes it capable of capturing both spatial and temporal correlations effectively. In our proposed approach,a convolutional long-term encoder is used to encode the whole given motion sequence into a long-term hidden variable, which is used with a decoder to predict the remainder of the sequence. The decoder itself also has an encoder-decoder structure, in which the short-term encoder encodes a shorter sequence to a short-term hidden variable, and the spatial decoder maps the long and short-term hidden variable to motion predictions. By using such a model, we are able to capture both invariant and dynamic information of human motion, which results in more accurate predictions. Experiments show that our algorithm outperforms the state-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our code is available at the project website.



### Text to Image Synthesis Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.00676v1
- **DOI**: 10.13140/RG.2.2.35817.39523
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1805.00676v1)
- **Published**: 2018-05-02 08:47:38+00:00
- **Updated**: 2018-05-02 08:47:38+00:00
- **Authors**: Cristian Bodnar
- **Comment**: None
- **Journal**: None
- **Summary**: Generating images from natural language is one of the primary applications of recent conditional generative models. Besides testing our ability to model conditional, highly dimensional distributions, text to image synthesis has many exciting and practical applications such as photo editing or computer-aided content creation. Recent progress has been made using Generative Adversarial Networks (GANs). This material starts with a gentle introduction to these topics and discusses the existent state of the art models. Moreover, I propose Wasserstein GAN-CLS, a new model for conditional image generation based on the Wasserstein distance which offers guarantees of stability. Then, I show how the novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. In combination with the proposed loss, the model boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics. The only model which performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN which uses word-level visual semantics as well.



### Joint Surgical Gesture and Task Classification with Multi-Task and Multimodal Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.00721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00721v1)
- **Published**: 2018-05-02 10:43:12+00:00
- **Updated**: 2018-05-02 10:43:12+00:00
- **Authors**: Duygu Sarikaya, Khurshid A. Guru, Jason J. Corso
- **Comment**: Keywords Robot-Assisted Surgery, Surgical Gesture Classification,
  Multi-task Learning, Multimodal Learning, Long Short-term Recurrent Neural
  Networks, Convolutional Neural Networks
- **Journal**: None
- **Summary**: We propose a novel multi-modal and multi-task architecture for simultaneous low level gesture and surgical task classification in Robot Assisted Surgery (RAS) videos.Our end-to-end architecture is based on the principles of a long short-term memory network (LSTM) that jointly learns temporal dynamics on rich representations of visual and motion features, while simultaneously classifying activities of low-level gestures and surgical tasks. Our experimental results show that our approach is superior compared to an ar- chitecture that classifies the gestures and surgical tasks separately on visual cues and motion cues respectively. We train our model on a fixed random set of 1200 gesture video segments and use the rest 422 for testing. This results in around 42,000 gesture frames sampled for training and 14,500 for testing. For a 6 split experimentation, while the conventional approach reaches an Average Precision (AP) of only 29% (29.13%), our architecture reaches an AP of 51% (50.83%) for 3 tasks and 14 possible gesture labels, resulting in an improvement of 22% (21.7%). Our architecture learns temporal dynamics on rich representations of visual and motion features that compliment each other for classification of low-level gestures and surgical tasks. Its multi-task learning nature makes use of learned joint re- lationships and combinations of shared and task-specific representations. While benchmark studies focus on recognizing gestures that take place under specific tasks, we focus on recognizing common gestures that reoccur across different tasks and settings and significantly perform better compared to conventional architectures.



### Unsupervised Features for Facial Expression Intensity Estimation over Time
- **Arxiv ID**: http://arxiv.org/abs/1805.00780v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00780v2)
- **Published**: 2018-05-02 13:12:05+00:00
- **Updated**: 2018-05-03 07:15:26+00:00
- **Authors**: Maren Awiszus, Stella Graßhof, Felix Kuhnke, Jörn Ostermann
- **Comment**: Accepted for CVPR 2018 Workshop Track
- **Journal**: None
- **Summary**: The diversity of facial shapes and motions among persons is one of the greatest challenges for automatic analysis of facial expressions. In this paper, we propose a feature describing expression intensity over time, while being invariant to person and the type of performed expression. Our feature is a weighted combination of the dynamics of multiple points adapted to the overall expression trajectory. We evaluate our method on several tasks all related to temporal analysis of facial expression. The proposed feature is compared to a state-of-the-art method for expression intensity estimation, which it outperforms. We use our proposed feature to temporally align multiple sequences of recorded 3D facial expressions. Furthermore, we show how our feature can be used to reveal person-specific differences in performances of facial expressions. Additionally, we apply our feature to identify the local changes in face video sequences based on action unit labels. For all the experiments our feature proves to be robust against noise and outliers, making it applicable to a variety of applications for analysis of facial movements.



### Learnable PINs: Cross-Modal Embeddings for Person Identity
- **Arxiv ID**: http://arxiv.org/abs/1805.00833v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00833v2)
- **Published**: 2018-05-02 14:13:26+00:00
- **Updated**: 2018-07-26 11:54:03+00:00
- **Authors**: Arsha Nagrani, Samuel Albanie, Andrew Zisserman
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: We propose and investigate an identity sensitive joint embedding of face and voice. Such an embedding enables cross-modal retrieval from voice to face and from face to voice. We make the following four contributions: first, we show that the embedding can be learnt from videos of talking faces, without requiring any identity labels, using a form of cross-modal self-supervision; second, we develop a curriculum learning schedule for hard negative mining targeted to this task, that is essential for learning to proceed successfully; third, we demonstrate and evaluate cross-modal retrieval for identities unseen and unheard during training over a number of scenarios and establish a benchmark for this novel task; finally, we show an application of using the joint embedding for automatically retrieving and labelling characters in TV dramas.



### Estimating Gradual-Emotional Behavior in One-Minute Videos with ESNs
- **Arxiv ID**: http://arxiv.org/abs/1805.08690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08690v1)
- **Published**: 2018-05-02 14:34:06+00:00
- **Updated**: 2018-05-02 14:34:06+00:00
- **Authors**: Tianlin Liu, Arvid Kappas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describe our approach for the OMG- Emotion Challenge 2018. The goal is to produce utterance-level valence and arousal estimations for videos of approximately 1 minute length. We tackle this problem by first extracting facial expressions features of videos as time series data, and then using Recurrent Neural Networks of the Echo State Network type to model the correspondence between the time series data and valence-arousal values. Experimentally we show that the proposed approach surpasses the baseline methods provided by the organizers.



### Spectral clustering algorithms for the detection of clusters in block-cyclic and block-acyclic graphs
- **Arxiv ID**: http://arxiv.org/abs/1805.00862v1
- **DOI**: 10.1093/comnet/cny011
- **Categories**: **cs.DS**, cs.CV, cs.DM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00862v1)
- **Published**: 2018-05-02 15:19:49+00:00
- **Updated**: 2018-05-02 15:19:49+00:00
- **Authors**: H. Van Lierde, T. W. S. Chow, J. -C. Delvenne
- **Comment**: This is the unrefereed Author's Original Version of the article. A
  peer-reviewed version has been accepted for publication in the Journal of
  Complex Networks published by Oxford University Press. The present version is
  not the Accepted Manuscript
- **Journal**: Journal of Complex Networks, cny011 (2018)
- **Summary**: We propose two spectral algorithms for partitioning nodes in directed graphs respectively with a cyclic and an acyclic pattern of connection between groups of nodes. Our methods are based on the computation of extremal eigenvalues of the transition matrix associated to the directed graph. The two algorithms outperform state-of-the art methods for directed graph clustering on synthetic datasets, including methods based on blockmodels, bibliometric symmetrization and random walks. Our algorithms have the same space complexity as classical spectral clustering algorithms for undirected graphs and their time complexity is also linear in the number of edges in the graph. One of our methods is applied to a trophic network based on predator-prey relationships. It successfully extracts common categories of preys and predators encountered in food chains. The same method is also applied to highlight the hierarchical structure of a worldwide network of Autonomous Systems depicting business agreements between Internet Service Providers.



### Images & Recipes: Retrieval in the cooking context
- **Arxiv ID**: http://arxiv.org/abs/1805.00900v1
- **DOI**: 10.1109/ICDEW.2018.00035
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1805.00900v1)
- **Published**: 2018-05-02 16:34:01+00:00
- **Updated**: 2018-05-02 16:34:01+00:00
- **Authors**: Micael Carvalho, Rémi Cadène, David Picard, Laure Soulier, Matthieu Cord
- **Comment**: Published at DECOR / ICDE 2018. Extended version accepted at SIGIR
  2018, available here: arXiv:1804.11146
- **Journal**: None
- **Summary**: Recent advances in the machine learning community allowed different use cases to emerge, as its association to domains like cooking which created the computational cuisine. In this paper, we tackle the picture-recipe alignment problem, having as target application the large-scale retrieval task (finding a recipe given a picture, and vice versa). Our approach is validated on the Recipe1M dataset, composed of one million image-recipe pairs and additional class information, for which we achieve state-of-the-art results.



### Altered Fingerprints: Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/1805.00911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00911v2)
- **Published**: 2018-05-02 17:16:18+00:00
- **Updated**: 2018-09-18 18:54:15+00:00
- **Authors**: Elham Tabassi, Tarang Chugh, Debayan Deb, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint alteration, also referred to as obfuscation presentation attack, is to intentionally tamper or damage the real friction ridge patterns to avoid identification by an AFIS. This paper proposes a method for detection and localization of fingerprint alterations. Our main contributions are: (i) design and train CNN models on fingerprint images and minutiae-centered local patches in the image to detect and localize regions of fingerprint alterations, and (ii) train a Generative Adversarial Network (GAN) to synthesize altered fingerprints whose characteristics are similar to true altered fingerprints. A successfully trained GAN can alleviate the limited availability of altered fingerprint images for research. A database of 4,815 altered fingerprints from 270 subjects, and an equal number of rolled fingerprint images are used to train and test our models. The proposed approach achieves a True Detection Rate (TDR) of 99.24% at a False Detection Rate (FDR) of 2%, outperforming published results. The synthetically generated altered fingerprint dataset will be open-sourced.



### Multi-Resolution Multi-Modal Sensor Fusion For Remote Sensing Data With Label Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1805.00930v2
- **DOI**: 10.1109/TGRS.2019.2955320
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00930v2)
- **Published**: 2018-05-02 17:51:13+00:00
- **Updated**: 2019-11-20 19:00:17+00:00
- **Authors**: Xiaoxiao Du, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: In remote sensing, each sensor can provide complementary or reinforcing information. It is valuable to fuse outputs from multiple sensors to boost overall performance. Previous supervised fusion methods often require accurate labels for each pixel in the training data. However, in many remote sensing applications, pixel-level labels are difficult or infeasible to obtain. In addition, outputs from multiple sensors often have different resolution or modalities. For example, rasterized hyperspectral imagery presents data in a pixel grid while airborne Light Detection and Ranging (LiDAR) generates dense three-dimensional (3D) point clouds. It is often difficult to directly fuse such multi-modal, multi-resolution data. To address these challenges, we present a novel Multiple Instance Multi-Resolution Fusion (MIMRF) framework that can fuse multi-resolution and multi-modal sensor outputs while learning from automatically-generated, imprecisely-labeled data. Experiments were conducted on the MUUFL Gulfport hyperspectral and LiDAR data set and a remotely-sensed soybean and weed data set. Results show improved, consistent performance on scene understanding and agricultural applications when compared to traditional fusion methods.



### Exploring the Limits of Weakly Supervised Pretraining
- **Arxiv ID**: http://arxiv.org/abs/1805.00932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.00932v1)
- **Published**: 2018-05-02 17:57:16+00:00
- **Updated**: 2018-05-02 17:57:16+00:00
- **Authors**: Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten
- **Comment**: Technical report
- **Journal**: None
- **Summary**: State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.



### SaaS: Speed as a Supervisor for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1805.00980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.00980v1)
- **Published**: 2018-05-02 18:52:18+00:00
- **Updated**: 2018-05-02 18:52:18+00:00
- **Authors**: Safa Cicek, Alhussein Fawzi, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the SaaS Algorithm for semi-supervised learning, which uses learning speed during stochastic gradient descent in a deep neural network to measure the quality of an iterative estimate of the posterior probability of unknown labels. Training speed in supervised learning correlates strongly with the percentage of correct labels, so we use it as an inference criterion for the unknown labels, without attempting to infer the model parameters at first. Despite its simplicity, SaaS achieves state-of-the-art results in semi-supervised learning benchmarks.



### A Numerical Framework for Efficient Motion Estimation on Evolving Sphere-Like Surfaces based on Brightness and Mass Conservation Laws
- **Arxiv ID**: http://arxiv.org/abs/1805.01006v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, 35A15, 68U10, 92C55, 33C55, 92C37, 53A05, 65N30, 35L65
- **Links**: [PDF](http://arxiv.org/pdf/1805.01006v1)
- **Published**: 2018-05-02 20:30:01+00:00
- **Updated**: 2018-05-02 20:30:01+00:00
- **Authors**: Lukas F. Lang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we consider brightness and mass conservation laws for motion estimation on evolving Riemannian 2-manifolds that allow for a radial parametrisation from the 2-sphere. While conservation of brightness constitutes the foundation for optical flow methods and has been generalised to said scenario, we formulate in this article the principle of mass conservation for time-varying surfaces which are embedded in Euclidean 3-space and derive a generalised continuity equation. The main motivation for this work is efficient cell motion estimation in time-lapse (4D) volumetric fluorescence microscopy images of a living zebrafish embryo. Increasing spatial and temporal resolution of modern microscopes require efficient analysis of such data. With this application in mind we address this need and follow an emerging paradigm in this field: dimensional reduction. In light of the ill-posedness of considered conservation laws we employ Tikhonov regularisation and propose the use of spatially varying regularisation functionals that recover motion only in regions with cells. For the efficient numerical solution we devise a Galerkin method based on compactly supported (tangent) vectorial basis functions. Furthermore, for the fast and accurate estimation of the evolving sphere-like surface from scattered data we utilise surface interpolation with spatio-temporal regularisation. We present numerical results based on aforementioned zebrafish microscopy data featuring fluorescently labelled cells.



### Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model
- **Arxiv ID**: http://arxiv.org/abs/1805.01024v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1805.01024v1)
- **Published**: 2018-05-02 21:08:47+00:00
- **Updated**: 2018-05-02 21:08:47+00:00
- **Authors**: Feng Zhou, Shu Kong, Charless Fowlkes, Tao Chen, Baiying Lei
- **Comment**: code: http://www.ics.uci.edu/~skong2/DimensionalEmotionModel.html
- **Journal**: None
- **Summary**: Automated facial expression analysis has a variety of applications in human-computer interaction. Traditional methods mainly analyze prototypical facial expressions of no more than eight discrete emotions as a classification task. However, in practice, spontaneous facial expressions in naturalistic environment can represent not only a wide range of emotions, but also different intensities within an emotion family. In such situation, these methods are not reliable or adequate. In this paper, we propose to train deep convolutional neural networks (CNNs) to analyze facial expressions explainable in a dimensional emotion model. The proposed method accommodates not only a set of basic emotion expressions, but also a full range of other emotions and subtle emotion intensities that we both feel in ourselves and perceive in others in our daily life. Specifically, we first mapped facial expressions into dimensional measures so that we transformed facial expression analysis from a classification problem to a regression one. We then tested our CNN-based methods for facial expression regression and these methods demonstrated promising performance. Moreover, we improved our method by a bilinear pooling which encodes second-order statistics of features. We showed such bilinear-CNN models significantly outperformed their respective baselines.



### Computing CNN Loss and Gradients for Pose Estimation with Riemannian Geometry
- **Arxiv ID**: http://arxiv.org/abs/1805.01026v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01026v3)
- **Published**: 2018-05-02 21:17:03+00:00
- **Updated**: 2018-07-17 13:38:12+00:00
- **Authors**: Benjamin Hou, Nina Miolane, Bishesh Khanal, Matthew C. H. Lee, Amir Alansary, Steven McDonagh, Jo V. Hajnal, Daniel Rueckert, Ben Glocker, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation, i.e. predicting a 3D rigid transformation with respect to a fixed co-ordinate frame in, SE(3), is an omnipresent problem in medical image analysis with applications such as: image rigid registration, anatomical standard plane detection, tracking and device/camera pose estimation. Deep learning methods often parameterise a pose with a representation that separates rotation and translation. As commonly available frameworks do not provide means to calculate loss on a manifold, regression is usually performed using the L2-norm independently on the rotation's and the translation's parameterisations, which is a metric for linear spaces that does not take into account the Lie group structure of SE(3). In this paper, we propose a general Riemannian formulation of the pose estimation problem. We propose to train the CNN directly on SE(3) equipped with a left-invariant Riemannian metric, coupling the prediction of the translation and rotation defining the pose. At each training step, the ground truth and predicted pose are elements of the manifold, where the loss is calculated as the Riemannian geodesic distance. We then compute the optimisation direction by back-propagating the gradient with respect to the predicted pose on the tangent space of the manifold SE(3) and update the network weights. We thoroughly evaluate the effectiveness of our loss function by comparing its performance with popular and most commonly used existing methods, on tasks such as image-based localisation and intensity-based 2D/3D registration. We also show that hyper-parameters, used in our loss function to weight the contribution between rotations and translations, can be intrinsically calculated from the dataset to achieve greater performance margins.



### Unsupervised Learning using Pretrained CNN and Associative Memory Bank
- **Arxiv ID**: http://arxiv.org/abs/1805.01033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.01033v1)
- **Published**: 2018-05-02 21:32:08+00:00
- **Updated**: 2018-05-02 21:32:08+00:00
- **Authors**: Qun Liu, Supratik Mukhopadhyay
- **Comment**: Paper was accepted at the 2018 International Joint Conference on
  Neural Networks (IJCNN 2018)
- **Journal**: None
- **Summary**: Deep Convolutional features extracted from a comprehensive labeled dataset, contain substantial representations which could be effectively used in a new domain. Despite the fact that generic features achieved good results in many visual tasks, fine-tuning is required for pretrained deep CNN models to be more effective and provide state-of-the-art performance. Fine tuning using the backpropagation algorithm in a supervised setting, is a time and resource consuming process. In this paper, we present a new architecture and an approach for unsupervised object recognition that addresses the above mentioned problem with fine tuning associated with pretrained CNN-based supervised deep learning approaches while allowing automated feature extraction. Unlike existing works, our approach is applicable to general object recognition tasks. It uses a pretrained (on a related domain) CNN model for automated feature extraction pipelined with a Hopfield network based associative memory bank for storing patterns for classification purposes. The use of associative memory bank in our framework allows eliminating backpropagation while providing competitive performance on an unseen dataset.



### EML-NET:An Expandable Multi-Layer NETwork for Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1805.01047v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01047v2)
- **Published**: 2018-05-02 22:32:12+00:00
- **Updated**: 2019-03-11 04:16:04+00:00
- **Authors**: Sen Jia, Neil D. B. Bruce
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency prediction can benefit from training that involves scene understanding that may be tangential to the central task; this may include understanding places, spatial layout, objects or involve different datasets and their bias. One can combine models, but to do this in a sophisticated manner can be complex, and also result in unwieldy networks or produce competing objectives that are hard to balance. In this paper, we propose a scalable system to leverage multiple powerful deep CNN models to better extract visual features for saliency prediction. Our design differs from previous studies in that the whole system is trained in an almost end-to-end piece-wise fashion. The encoder and decoder components are separately trained to deal with complexity tied to the computational paradigm and required space. Furthermore, the encoder can contain more than one CNN model to extract features, and models can have different architectures or be pre-trained on different datasets. This parallel design yields a better computational paradigm overcoming limits to the variety of information or inference that can be combined at the encoder stage towards deeper networks and a more powerful encoding. Our network can be easily expanded almost without any additional cost, and other pre-trained CNN models can be incorporated availing a wider range of visual knowledge. We denote our expandable multi-layer network as EML-NET and our method achieves the state-of-the-art results on the public saliency benchmarks, SALICON, MIT300 and CAT2000.



### Vision-based Structural Inspection using Multiscale Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.01055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.01055v1)
- **Published**: 2018-05-02 23:29:34+00:00
- **Updated**: 2018-05-02 23:29:34+00:00
- **Authors**: Vedhus Hoskere, Yasutaka Narazaki, Tu Hoang, BillieF Spencer Jr
- **Comment**: None
- **Journal**: None
- **Summary**: Current methods of practice for inspection of civil infrastructure typically involve visual assessments conducted manually by trained inspectors. For post-earthquake structural inspections, the number of structures to be inspected often far exceeds the capability of the available inspectors. The labor intensive and time consuming natures of manual inspection have engendered research into development of algorithms for automated damage identification using computer vision techniques. In this paper, a novel damage localization and classification technique based on a state of the art computer vision algorithm is presented to address several key limitations of current computer vision techniques. The proposed algorithm carries out a pixel-wise classification of each image at multiple scales using a deep convolutional neural network and can recognize 6 different types of damage. The resulting output is a segmented image where the portion of the image representing damage is outlined and classified as one of the trained damage categories. The proposed method is evaluated in terms of pixel accuracy and the application of the method to real world images is shown.




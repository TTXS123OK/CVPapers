# Arxiv Papers in cs.CV on 2018-05-23
### Approximate Newton-based statistical inference using only stochastic gradients
- **Arxiv ID**: http://arxiv.org/abs/1805.08920v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1805.08920v2)
- **Published**: 2018-05-23 01:07:47+00:00
- **Updated**: 2019-02-05 15:23:47+00:00
- **Authors**: Tianyang Li, Anastasios Kyrillidis, Liu Liu, Constantine Caramanis
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel statistical inference framework for convex empirical risk minimization, using approximate stochastic Newton steps. The proposed algorithm is based on the notion of finite differences and allows the approximation of a Hessian-vector product from first-order information. In theory, our method efficiently computes the statistical error covariance in $M$-estimation, both for unregularized convex learning problems and high-dimensional LASSO regression, without using exact second order information, or resampling the entire data set. We also present a stochastic gradient sampling scheme for statistical inference in non-i.i.d. time series analysis, where we sample contiguous blocks of indices. In practice, we demonstrate the effectiveness of our framework on large-scale machine learning problems, that go even beyond convexity: as a highlight, our work can be used to detect certain adversarial attacks on neural networks.



### AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference
- **Arxiv ID**: http://arxiv.org/abs/1805.08941v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08941v3)
- **Published**: 2018-05-23 03:05:48+00:00
- **Updated**: 2019-01-17 06:11:48+00:00
- **Authors**: Jian-Hao Luo, Jianxin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Channel pruning is an important family of methods to speed up deep model's inference. Previous filter pruning algorithms regard channel pruning and model fine-tuning as two independent steps. This paper argues that combining them into a single end-to-end trainable system will lead to better results. We propose an efficient channel selection layer, namely AutoPruner, to find less important filters automatically in a joint training manner. Our AutoPruner takes previous activation responses as an input and generates a true binary index code for pruning. Hence, all the filters corresponding to zero index values can be removed safely after training. We empirically demonstrate that the gradient information of this channel selection layer is also helpful for the whole model training. By gradually erasing several weak filters, we can prevent an excessive drop in model accuracy. Compared with previous state-of-the-art pruning algorithms (including training from scratch), AutoPruner achieves significantly better performance. Furthermore, ablation experiments show that the proposed novel mini-batch pooling and binarization operations are vital for the success of filter pruning.



### Building Extraction at Scale using Convolutional Neural Network: Mapping of the United States
- **Arxiv ID**: http://arxiv.org/abs/1805.08946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08946v1)
- **Published**: 2018-05-23 03:28:05+00:00
- **Updated**: 2018-05-23 03:28:05+00:00
- **Authors**: Hsiuhan Lexie Yang, Jiangye Yuan, Dalton Lunga, Melanie Laverdiere, Amy Rose, Budhendra Bhaduri
- **Comment**: Accepted by IEEE Journal of Selected Topics in Applied Earth
  Observations and Remote Sensing
- **Journal**: None
- **Summary**: Establishing up-to-date large scale building maps is essential to understand urban dynamics, such as estimating population, urban planning and many other applications. Although many computer vision tasks has been successfully carried out with deep convolutional neural networks, there is a growing need to understand their large scale impact on building mapping with remote sensing imagery. Taking advantage of the scalability of CNNs and using only few areas with the abundance of building footprints, for the first time we conduct a comparative analysis of four state-of-the-art CNNs for extracting building footprints across the entire continental United States. The four CNN architectures namely: branch-out CNN, fully convolutional neural network (FCN), conditional random field as recurrent neural network (CRFasRNN), and SegNet, support semantic pixel-wise labeling and focus on capturing textural information at multi-scale. We use 1-meter resolution aerial images from National Agriculture Imagery Program (NAIP) as the test-bed, and compare the extraction results across the four methods. In addition, we propose to combine signed-distance labels with SegNet, the preferred CNN architecture identified by our extensive evaluations, to advance building extraction results to instance level. We further demonstrate the usefulness of fusing additional near IR information into the building extraction framework. Large scale experimental evaluations are conducted and reported using metrics that include: precision, recall rate, intersection over union, and the number of buildings extracted. With the improved CNN model and no requirement of further post-processing, we have generated building maps for the United States. The quality of extracted buildings and processing time demonstrated the proposed CNN-based framework fits the need of building extraction at scale.



### ICADx: Interpretable computer aided diagnosis of breast masses
- **Arxiv ID**: http://arxiv.org/abs/1805.08960v1
- **DOI**: 10.1117/12.2293570
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08960v1)
- **Published**: 2018-05-23 04:52:06+00:00
- **Updated**: 2018-05-23 04:52:06+00:00
- **Authors**: Seong Tae Kim, Hakmin Lee, Hak Gu Kim, Yong Man Ro
- **Comment**: This paper was presented at SPIE Medical Imaging 2018, Houston, TX,
  USA
- **Journal**: None
- **Summary**: In this study, a novel computer aided diagnosis (CADx) framework is devised to investigate interpretability for classifying breast masses. Recently, a deep learning technology has been successfully applied to medical image analysis including CADx. Existing deep learning based CADx approaches, however, have a limitation in explaining the diagnostic decision. In real clinical practice, clinical decisions could be made with reasonable explanation. So current deep learning approaches in CADx are limited in real world deployment. In this paper, we investigate interpretability in CADx with the proposed interpretable CADx (ICADx) framework. The proposed framework is devised with a generative adversarial network, which consists of interpretable diagnosis network and synthetic lesion generative network to learn the relationship between malignancy and a standardized description (BI-RADS). The lesion generative network and the interpretable diagnosis network compete in an adversarial learning so that the two networks are improved. The effectiveness of the proposed method was validated on public mammogram database. Experimental results showed that the proposed ICADx framework could provide the interpretability of mass as well as mass classification. It was mainly attributed to the fact that the proposed method was effectively trained to find the relationship between malignancy and interpretations via the adversarial learning. These results imply that the proposed ICADx framework could be a promising approach to develop the CADx system.



### 3D Human Pose Estimation with Relational Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.08961v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08961v2)
- **Published**: 2018-05-23 05:12:36+00:00
- **Updated**: 2018-07-20 13:49:22+00:00
- **Authors**: Sungheon Park, Nojun Kwak
- **Comment**: BMVC 2018, source code: https://github.com/sungheonpark/3D_HPE_RN
- **Journal**: None
- **Summary**: In this paper, we propose a novel 3D human pose estimation algorithm from a single image based on neural networks. We adopted the structure of the relational networks in order to capture the relations among different body parts. In our method, each pair of different body parts generates features, and the average of the features from all the pairs are used for 3D pose estimation. In addition, we propose a dropout method that can be used in relational modules, which inherently imposes robustness to the occlusions. The proposed network achieves state-of-the-art performance for 3D pose estimation in Human 3.6M dataset, and it effectively produces plausible results even in the existence of missing joints.



### Semantic Network Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1805.08969v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.08969v3)
- **Published**: 2018-05-23 05:54:15+00:00
- **Updated**: 2021-11-19 05:53:00+00:00
- **Authors**: Pei Guo, Ryan Farrell
- **Comment**: None
- **Journal**: None
- **Summary**: Network interpretation as an effort to reveal the features learned by a network remains largely visualization-based. In this paper, our goal is to tackle semantic network interpretation at both filter and decision level. For filter-level interpretation, we represent the concepts a filter encodes with a probability distribution of visual attributes. The decision-level interpretation is achieved by textual summarization that generates an explanatory sentence containing clues behind a network's decision. A Bayesian inference algorithm is proposed to automatically associate filters and network decisions with visual attributes. Human study confirms that the semantic interpretation is a beneficial alternative or complement to visualization methods. We demonstrate the crucial role that semantic network interpretation can play in understanding a network's failure patterns. More importantly, semantic network interpretation enables a better understanding of the correlation between a model's performance and its distribution metrics like filter selectivity and concept sparseness.



### Toward a Thinking Microscope: Deep Learning in Optical Microscopy and Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1805.08970v1
- **DOI**: 10.1364/OPN.29.7.000034
- **Categories**: **cs.LG**, cs.CV, physics.optics, stat.ML, 68T01, 68T05, 68U10, 62M45, 78M32, 92C50, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.3; I.3.3; I.4.3; I.4.4; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1805.08970v1)
- **Published**: 2018-05-23 05:54:51+00:00
- **Updated**: 2018-05-23 05:54:51+00:00
- **Authors**: Yair Rivenson, Aydogan Ozcan
- **Comment**: None
- **Journal**: OPN (2018)
- **Summary**: We discuss recently emerging applications of the state-of-art deep learning methods on optical microscopy and microscopic image reconstruction, which enable new transformations among different modes and modalities of microscopic imaging, driven entirely by image data. We believe that deep learning will fundamentally change both the hardware and image reconstruction methods used in optical microscopy in a holistic manner.



### DRPose3D: Depth Ranking in 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1805.08973v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08973v2)
- **Published**: 2018-05-23 06:05:48+00:00
- **Updated**: 2018-05-24 09:15:32+00:00
- **Authors**: Min Wang, Xipeng Chen, Wentao Liu, Chen Qian, Liang Lin, Lizhuang Ma
- **Comment**: Accepted by the 27th International Joint Conference on Artificial
  Intelligence (IJCAI 2018)
- **Journal**: None
- **Summary**: In this paper, we propose a two-stage depth ranking based method (DRPose3D) to tackle the problem of 3D human pose estimation. Instead of accurate 3D positions, the depth ranking can be identified by human intuitively and learned using the deep neural network more easily by solving classification problems. Moreover, depth ranking contains rich 3D information. It prevents the 2D-to-3D pose regression in two-stage methods from being ill-posed. In our method, firstly, we design a Pairwise Ranking Convolutional Neural Network (PRCNN) to extract depth rankings of human joints from images. Secondly, a coarse-to-fine 3D Pose Network(DPNet) is proposed to estimate 3D poses from both depth rankings and 2D human joint locations. Additionally, to improve the generality of our model, we introduce a statistical method to augment depth rankings. Our approach outperforms the state-of-the-art methods in the Human3.6M benchmark for all three testing protocols, indicating that depth ranking is an essential geometric feature which can be learned to improve the 3D pose estimation.



### Do Better ImageNet Models Transfer Better?
- **Arxiv ID**: http://arxiv.org/abs/1805.08974v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.08974v3)
- **Published**: 2018-05-23 06:12:35+00:00
- **Updated**: 2019-06-17 16:25:07+00:00
- **Authors**: Simon Kornblith, Jonathon Shlens, Quoc V. Le
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.



### Particle Filter Networks with Application to Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1805.08975v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.08975v3)
- **Published**: 2018-05-23 06:21:08+00:00
- **Updated**: 2018-10-25 17:23:11+00:00
- **Authors**: Peter Karkus, David Hsu, Wee Sun Lee
- **Comment**: CoRL 2018 camera ready
- **Journal**: None
- **Summary**: Particle filtering is a powerful approach to sequential state estimation and finds application in many domains, including robot localization, object tracking, etc. To apply particle filtering in practice, a critical challenge is to construct probabilistic system models, especially for systems with complex dynamics or rich sensory inputs such as camera images. This paper introduces the Particle Filter Network (PFnet), which encodes both a system model and a particle filter algorithm in a single neural network. The PF-net is fully differentiable and trained end-to-end from data. Instead of learning a generic system model, it learns a model optimized for the particle filter algorithm. We apply the PF-net to a visual localization task, in which a robot must localize in a rich 3-D world, using only a schematic 2-D floor map. In simulation experiments, PF-net consistently outperforms alternative learning architectures, as well as a traditional model-based method, under a variety of sensor inputs. Further, PF-net generalizes well to new, unseen environments.



### RGB-T Object Tracking:Benchmark and Baseline
- **Arxiv ID**: http://arxiv.org/abs/1805.08982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08982v1)
- **Published**: 2018-05-23 07:13:39+00:00
- **Updated**: 2018-05-23 07:13:39+00:00
- **Authors**: Chenglong Li, Xinyan Liang, Yijuan Lu, Nan Zhao, Jin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-Thermal (RGB-T) object tracking receives more and more attention due to the strongly complementary benefits of thermal information to visible data. However, RGB-T research is limited by lacking a comprehensive evaluation platform. In this paper, we propose a large-scale video benchmark dataset for RGB-T tracking.It has three major advantages over existing ones: 1) Its size is sufficiently large for large-scale performance evaluation (total frame number: 234K, maximum frame per sequence: 8K). 2) The alignment between RGB-T sequence pairs is highly accurate, which does not need pre- or post-processing. 3) The occlusion levels are annotated for occlusion-sensitive performance analysis of different tracking algorithms.Moreover, we propose a novel graph-based approach to learn a robust object representation for RGB-T tracking. In particular, the tracked object is represented with a graph with image patches as nodes. This graph including graph structure, node weights and edge weights is dynamically learned in a unified ADMM (alternating direction method of multipliers)-based optimization framework, in which the modality weights are also incorporated for adaptive fusion of multiple source data.Extensive experiments on the large-scale dataset are executed to demonstrate the effectiveness of the proposed tracker against other state-of-the-art tracking methods. We also provide new insights and potential research directions to the field of RGB-T object tracking.



### GPU Accelerated Cascade Hashing Image Matching for Large Scale 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1805.08995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.08995v1)
- **Published**: 2018-05-23 07:57:01+00:00
- **Updated**: 2018-05-23 07:57:01+00:00
- **Authors**: Tao Xu, Kun Sun, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Image feature point matching is a key step in Structure from Motion(SFM). However, it is becoming more and more time consuming because the number of images is getting larger and larger. In this paper, we proposed a GPU accelerated image matching method with improved Cascade Hashing. Firstly, we propose a Disk-Memory-GPU data exchange strategy and optimize the load order of data, so that the proposed method can deal with big data. Next, we parallelize the Cascade Hashing method on GPU. An improved parallel reduction and an improved parallel hashing ranking are proposed to fulfill this task. Finally, extensive experiments show that our image matching is about 20 times faster than SiftGPU on the same graphics card, nearly 100 times faster than the CPU CasHash method and hundreds of times faster than the CPU Kd-Tree based matching method. Further more, we introduce the epipolar constraint to the proposed method, and use the epipolar geometry to guide the feature matching procedure, which further reduces the matching cost.



### Transfer Learning for Illustration Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.02682v1
- **DOI**: 10.2312/ceig.20171213
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02682v1)
- **Published**: 2018-05-23 09:06:16+00:00
- **Updated**: 2018-05-23 09:06:16+00:00
- **Authors**: Manuel Lagunas, Elena Garces
- **Comment**: 9 pages, 8 figures, 4 tables
- **Journal**: 2017 Spanish Computer Graphics Conference (CEIG)
- **Summary**: The field of image classification has shown an outstanding success thanks to the development of deep learning techniques. Despite the great performance obtained, most of the work has focused on natural images ignoring other domains like artistic depictions. In this paper, we use transfer learning techniques to propose a new classification network with better performance in illustration images. Starting from the deep convolutional network VGG19, pre-trained with natural images, we propose two novel models which learn object representations in the new domain. Our optimized network will learn new low-level features of the images (colours, edges, textures) while keeping the knowledge of the objects and shapes that it already learned from the ImageNet dataset. Thus, requiring much less data for the training. We propose a novel dataset of illustration images labelled by content where our optimized architecture achieves $\textbf{86.61\%}$ of top-1 and $\textbf{97.21\%}$ of top-5 precision. We additionally demonstrate that our model is still able to recognize objects in photographs.



### CNN+CNN: Convolutional Decoders for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1805.09019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09019v1)
- **Published**: 2018-05-23 09:16:59+00:00
- **Updated**: 2018-05-23 09:16:59+00:00
- **Authors**: Qingzhong Wang, Antoni B. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is a challenging task that combines the field of computer vision and natural language processing. A variety of approaches have been proposed to achieve the goal of automatically describing an image, and recurrent neural network (RNN) or long-short term memory (LSTM) based models dominate this field. However, RNNs or LSTMs cannot be calculated in parallel and ignore the underlying hierarchical structure of a sentence. In this paper, we propose a framework that only employs convolutional neural networks (CNNs) to generate captions. Owing to parallel computing, our basic model is around 3 times faster than NIC (an LSTM-based model) during training time, while also providing better results. We conduct extensive experiments on MSCOCO and investigate the influence of the model width and depth. Compared with LSTM-based models that apply similar attention mechanisms, our proposed models achieves comparable scores of BLEU-1,2,3,4 and METEOR, and higher scores of CIDEr. We also test our model on the paragraph annotation dataset, and get higher CIDEr score compared with hierarchical LSTMs



### Efficient Relaxations for Dense CRFs with Sparse Higher Order Potentials
- **Arxiv ID**: http://arxiv.org/abs/1805.09028v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09028v2)
- **Published**: 2018-05-23 09:34:51+00:00
- **Updated**: 2018-10-26 11:58:27+00:00
- **Authors**: Thomas Joy, Alban Desmaison, Thalaiyasingam Ajanthan, Rudy Bunel, Mathieu Salzmann, Pushmeet Kohli, Philip H. S. Torr, M. Pawan Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Dense conditional random fields (CRFs) have become a popular framework for modelling several problems in computer vision such as stereo correspondence and multi-class semantic segmentation. By modelling long-range interactions, dense CRFs provide a labelling that captures finer detail than their sparse counterparts. Currently, the state-of-the-art algorithm performs mean-field inference using a filter-based method but fails to provide a strong theoretical guarantee on the quality of the solution. A question naturally arises as to whether it is possible to obtain a maximum a posteriori (MAP) estimate of a dense CRF using a principled method. Within this paper, we show that this is indeed possible. We will show that, by using a filter-based method, continuous relaxations of the MAP problem can be optimised efficiently using state-of-the-art algorithms. Specifically, we will solve a quadratic programming (QP) relaxation using the Frank-Wolfe algorithm and a linear programming (LP) relaxation by developing a proximal minimisation framework. By exploiting labelling consistency in the higher-order potentials and utilising the filter-based method, we are able to formulate the above algorithms such that each iteration has a complexity linear in the number of classes and random variables. The presented algorithms can be applied to any labelling problem using a dense CRF with sparse higher-order potentials. In this paper, we use semantic segmentation as an example application as it demonstrates the ability of the algorithm to scale to dense CRFs with large dimensions. We perform experiments on the Pascal dataset to indicate that the presented algorithms are able to attain lower energies than the mean-field inference method.



### Saliency deep embedding for aurora image search
- **Arxiv ID**: http://arxiv.org/abs/1805.09033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09033v1)
- **Published**: 2018-05-23 09:55:39+00:00
- **Updated**: 2018-05-23 09:55:39+00:00
- **Authors**: Xi Yang, Xinbo Gao, Bin Song, Nannan Wang, Dong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable success in the field of image search. However, the state-of-the-art algorithms are trained and tested for natural images captured with ordinary cameras. In this paper, we aim to explore a new search method for images captured with circular fisheye lens, especially the aurora images. To reduce the interference from uninformative regions and focus on the most interested regions, we propose a saliency proposal network (SPN) to replace the region proposal network (RPN) in the recent Mask R-CNN. In our SPN, the centers of the anchors are not distributed in a rectangular meshing manner, but exhibit spherical distortion. Additionally, the directions of the anchors are along the deformation lines perpendicular to the magnetic meridian, which perfectly accords with the imaging principle of circular fisheye lens. Extensive experiments are performed on the big aurora data, demonstrating the superiority of our method in both search accuracy and efficiency.



### Excitation Dropout: Encouraging Plasticity in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.09092v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09092v3)
- **Published**: 2018-05-23 12:32:41+00:00
- **Updated**: 2021-01-21 18:23:30+00:00
- **Authors**: Andrea Zunino, Sarah Adel Bargal, Pietro Morerio, Jianming Zhang, Stan Sclaroff, Vittorio Murino
- **Comment**: This work is published in the International Journal of Computer
  Vision (IJCV) in 2021
- **Journal**: None
- **Summary**: We propose a guided dropout regularizer for deep networks based on the evidence of a network prediction defined as the firing of neurons in specific paths. In this work, we utilize the evidence at each neuron to determine the probability of dropout, rather than dropping out neurons uniformly at random as in standard dropout. In essence, we dropout with higher probability those neurons which contribute more to decision making at training time. This approach penalizes high saliency neurons that are most relevant for model prediction, i.e. those having stronger evidence. By dropping such high-saliency neurons, the network is forced to learn alternative paths in order to maintain loss minimization, resulting in a plasticity-like behavior, a characteristic of human brains too. We demonstrate better generalization ability, an increased utilization of network neurons, and a higher resilience to network compression using several metrics over four image/video recognition benchmarks.



### Image Restoration by Estimating Frequency Distribution of Local Patches
- **Arxiv ID**: http://arxiv.org/abs/1805.09097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09097v1)
- **Published**: 2018-05-23 12:50:08+00:00
- **Updated**: 2018-05-23 12:50:08+00:00
- **Authors**: Jaeyoung Yoo, Sang-ho Lee, Nojun Kwak
- **Comment**: 9 pages, 5 figures, Accepted as a poster in CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we propose a method to solve the image restoration problem, which tries to restore the details of a corrupted image, especially due to the loss caused by JPEG compression. We have treated an image in the frequency domain to explicitly restore the frequency components lost during image compression. In doing so, the distribution in the frequency domain is learned using the cross entropy loss. Unlike recent approaches, we have reconstructed the details of an image without using the scheme of adversarial training. Rather, the image restoration problem is treated as a classification problem to determine the frequency coefficient for each frequency band in an image patch. In this paper, we show that the proposed method effectively restores a JPEG-compressed image with more detailed high frequency components, making the restored image more vivid.



### Maize Haploid Identification via LSTM-CNN and Hyperspectral Imaging Technology
- **Arxiv ID**: http://arxiv.org/abs/1805.09105v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09105v2)
- **Published**: 2018-05-23 13:01:15+00:00
- **Updated**: 2018-05-24 08:17:39+00:00
- **Authors**: Xuan-Yu Wang, Wen-Xuan Liao, Dong An, Yao-Guang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and fast identification of seed cultivars is crucial to plant breeding, with accelerating breeding of new products and increasing its quality. In our study, the first attempt to design a high-accurate identification model of maize haploid seeds from diploid ones based on optimum waveband selection of the LSTM-CNN algorithm is realized via deep learning and hyperspectral imaging technology, with accuracy reaching 97% in the determining optimum waveband of 1367.6-1526.4nm. The verification of testing another cultivar achieved an accuracy of 93% in the same waveband. The model collected images of 256 wavebands of seeds in the spectral region of 862.9-1704.2nm. The high-noise waveband intervals were found and deleted by the LSTM. The optimum-data waveband intervals were determined by CNN's waveband-based detection. The optimum sample set for network training only accounted for 1/5 of total sample data. The accuracy was significantly higher than the full-waveband modeling or modeling of any other wavebands. Our study demonstrates that the proposed model has outstanding effect on maize haploid identification and it could be generalized to some extent.



### Towards the first adversarially robust neural network model on MNIST
- **Arxiv ID**: http://arxiv.org/abs/1805.09190v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09190v3)
- **Published**: 2018-05-23 14:16:22+00:00
- **Updated**: 2018-09-20 17:49:14+00:00
- **Authors**: Lukas Schott, Jonas Rauber, Matthias Bethge, Wieland Brendel
- **Comment**: None
- **Journal**: None
- **Summary**: Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.



### Attributes in Multiple Facial Images
- **Arxiv ID**: http://arxiv.org/abs/1805.09203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09203v1)
- **Published**: 2018-05-23 14:48:11+00:00
- **Updated**: 2018-05-23 14:48:11+00:00
- **Authors**: Xudong Liu, Guodong Guo
- **Comment**: Accepted by 2018 13th IEEE International Conference on Automatic Face
  & Gesture Recognition (FG 2018 Spotlight)
- **Journal**: None
- **Summary**: Facial attribute recognition is conventionally computed from a single image. In practice, each subject may have multiple face images. Taking the eye size as an example, it should not change, but it may have different estimation in multiple images, which would make a negative impact on face recognition. Thus, how to compute these attributes corresponding to each subject rather than each single image is a profound work. To address this question, we deploy deep training for facial attributes prediction, and we explore the inconsistency issue among the attributes computed from each single image. Then, we develop two approaches to address the inconsistency issue. Experimental results show that the proposed methods can handle facial attribute estimation on either multiple still images or video frames, and can correct the incorrectly annotated labels. The experiments are conducted on two large public databases with annotations of facial attributes.



### SymmSLIC: Symmetry Aware Superpixel Segmentation and its Applications
- **Arxiv ID**: http://arxiv.org/abs/1805.09232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09232v2)
- **Published**: 2018-05-23 15:43:53+00:00
- **Updated**: 2018-08-10 12:35:55+00:00
- **Authors**: Rajendra Nagar, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Over-segmentation of an image into superpixels has become a useful tool for solving various problems in image processing and computer vision. Reflection symmetry is quite prevalent in both natural and man-made objects and is an essential cue in understanding and grouping the objects in natural scenes. Existing algorithms for estimating superpixels do not preserve the reflection symmetry of an object which leads to different sizes and shapes of superpixels across the symmetry axis. In this work, we propose an algorithm to over-segment an image through the propagation of reflection symmetry evident at the pixel level to superpixel boundaries. In order to achieve this goal, we first find the reflection symmetry in the image and represent it by a set of pairs of pixels which are mirror reflections of each other. We partition the image into superpixels while preserving this reflection symmetry through an iterative algorithm. We compare the proposed method with state-of-the-art superpixel generation methods and show the effectiveness in preserving the size and shape of superpixel boundaries across the reflection symmetry axes. We also present two applications, symmetry axes detection and unsupervised symmetric object segmentation, to illustrate the effectiveness of the proposed approach.



### Segmentation of Liver Lesions with Reduced Complexity Deep Models
- **Arxiv ID**: http://arxiv.org/abs/1805.09233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09233v1)
- **Published**: 2018-05-23 15:45:16+00:00
- **Updated**: 2018-05-23 15:45:16+00:00
- **Authors**: Ram Krishna Pandey, Aswin Vasan, A G Ramakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a computationally efficient architecture that learns to segment lesions from CT images of the liver. The proposed architecture uses bilinear interpolation with sub-pixel convolution at the last layer to upscale the course feature in bottle neck architecture. Since bilinear interpolation and sub-pixel convolution do not have any learnable parameter, our overall model is faster and occupies less memory footprint than the traditional U-net. We evaluate our proposed architecture on the highly competitive dataset of 2017 Liver Tumor Segmentation (LiTS) Challenge. Our method achieves competitive results while reducing the number of learnable parameters roughly by a factor of 13.8 compared to the original UNet model.



### Subspace Clustering by Block Diagonal Representation
- **Arxiv ID**: http://arxiv.org/abs/1805.09243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09243v1)
- **Published**: 2018-05-23 15:58:34+00:00
- **Updated**: 2018-05-23 15:58:34+00:00
- **Authors**: Canyi Lu, Jiashi Feng, Zhouchen Lin, Tao Mei, Shuicheng Yan
- **Comment**: None
- **Journal**: IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
  2018
- **Summary**: This paper studies the subspace clustering problem. Given some data points approximately drawn from a union of subspaces, the goal is to group these data points into their underlying subspaces. Many subspace clustering methods have been proposed and among which sparse subspace clustering and low-rank representation are two representative ones. Despite the different motivations, we observe that many existing methods own the common block diagonal property, which possibly leads to correct clustering, yet with their proofs given case by case. In this work, we consider a general formulation and provide a unified theoretical guarantee of the block diagonal property. The block diagonal property of many existing methods falls into our special case. Second, we observe that many existing methods approximate the block diagonal representation matrix by using different structure priors, e.g., sparsity and low-rankness, which are indirect. We propose the first block diagonal matrix induced regularizer for directly pursuing the block diagonal matrix. With this regularizer, we solve the subspace clustering problem by Block Diagonal Representation (BDR), which uses the block diagonal structure prior. The BDR model is nonconvex and we propose an alternating minimization solver and prove its convergence. Experiments on real datasets demonstrate the effectiveness of BDR.



### Learning Illuminant Estimation from Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.09264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09264v1)
- **Published**: 2018-05-23 16:25:26+00:00
- **Updated**: 2018-05-23 16:25:26+00:00
- **Authors**: Marco Buzzelli, Joost van de Weijer, Raimondo Schettini
- **Comment**: Accepted at ICIP 2018
- **Journal**: None
- **Summary**: In this paper we present a deep learning method to estimate the illuminant of an image. Our model is not trained with illuminant annotations, but with the objective of improving performance on an auxiliary task such as object recognition. To the best of our knowledge, this is the first example of a deep learning architecture for illuminant estimation that is trained without ground truth illuminants. We evaluate our solution on standard datasets for color constancy, and compare it with state of the art methods. Our proposal is shown to outperform most deep learning methods in a cross-dataset evaluation setup, and to present competitive results in a comparison with parametric solutions.



### WisenetMD: Motion Detection Using Dynamic Background Region Analysis
- **Arxiv ID**: http://arxiv.org/abs/1805.09277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09277v1)
- **Published**: 2018-05-23 16:48:27+00:00
- **Updated**: 2018-05-23 16:48:27+00:00
- **Authors**: Sang-Ha Lee, Soon-Chul Kwon, Jin-Wook Shim, Jeong-Eun Lim, Jisang Yoo
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Motion detection algorithms that can be applied to surveillance cameras such as CCTV (Closed Circuit Television) have been studied extensively. Motion detection algorithm is mostly based on background subtraction. One main issue in this technique is that false positives of dynamic backgrounds such as wind shaking trees and flowing rivers might occur. In this paper, we proposed a method to search for dynamic background region by analyzing the video and removing false positives by re-checking false positives. The proposed method was evaluated based on CDnet 2012/2014 dataset obtained at "changedetection.net" site. We also compared its processing speed with other algorithms.



### Learning towards Minimum Hyperspherical Energy
- **Arxiv ID**: http://arxiv.org/abs/1805.09298v9
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.09298v9)
- **Published**: 2018-05-23 17:34:47+00:00
- **Updated**: 2020-07-22 15:23:29+00:00
- **Authors**: Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le Song
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.



### SNIPER: Efficient Multi-Scale Training
- **Arxiv ID**: http://arxiv.org/abs/1805.09300v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09300v3)
- **Published**: 2018-05-23 17:38:27+00:00
- **Updated**: 2018-12-13 18:56:14+00:00
- **Authors**: Bharat Singh, Mahyar Najibi, Larry S. Davis
- **Comment**: Presented at the NIPS 2018 conference
- **Journal**: None
- **Summary**: We present SNIPER, an algorithm for performing efficient multi-scale training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on resampled low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47.6% on the COCO dataset for bounding box detection and can process 5 images per second during inference with a single GPU. Code is available at https://github.com/MahyarNajibi/SNIPER/.



### End-to-End Speech-Driven Facial Animation with Temporal GANs
- **Arxiv ID**: http://arxiv.org/abs/1805.09313v4
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1805.09313v4)
- **Published**: 2018-05-23 17:54:32+00:00
- **Updated**: 2018-07-19 12:40:47+00:00
- **Authors**: Konstantinos Vougioukas, Stavros Petridis, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: Speech-driven facial animation is the process which uses speech signals to automatically synthesize a talking character. The majority of work in this domain creates a mapping from audio features to visual features. This often requires post-processing using computer graphics techniques to produce realistic albeit subject dependent results. We present a system for generating videos of a talking head, using a still image of a person and an audio clip containing speech, that doesn't rely on any handcrafted intermediate features. To the best of our knowledge, this is the first method capable of generating subject independent realistic videos directly from raw audio. Our method can generate videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements. We achieve this by using a temporal GAN with 2 discriminators, which are capable of capturing different aspects of the video. The effect of each component in our system is quantified through an ablation study. The generated videos are evaluated based on their sharpness, reconstruction quality, and lip-reading accuracy. Finally, a user study is conducted, confirming that temporal GANs lead to more natural sequences than a static GAN-based approach.



### Anonymizing k-Facial Attributes via Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1805.09380v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09380v2)
- **Published**: 2018-05-23 18:54:28+00:00
- **Updated**: 2018-09-28 07:40:28+00:00
- **Authors**: Saheb Chhabra, Richa Singh, Mayank Vatsa, Gaurav Gupta
- **Comment**: Published in IJCAI-2018
- **Journal**: None
- **Summary**: A face image not only provides details about the identity of a subject but also reveals several attributes such as gender, race, sexual orientation, and age. Advancements in machine learning algorithms and popularity of sharing images on the World Wide Web, including social media websites, have increased the scope of data analytics and information profiling from photo collections. This poses a serious privacy threat for individuals who do not want to be profiled. This research presents a novel algorithm for anonymizing selective attributes which an individual does not want to share without affecting the visual quality of images. Using the proposed algorithm, a user can select single or multiple attributes to be surpassed while preserving identity information and visual content. The proposed adversarial perturbation based algorithm embeds imperceptible noise in an image such that attribute prediction algorithm for the selected attribute yields incorrect classification result, thereby preserving the information according to user's choice. Experiments on three popular databases i.e. MUCT, LFWcrop, and CelebA show that the proposed algorithm not only anonymizes k-attributes, but also preserves image quality and identity information.



### Classifying cooking object's state using a tuned VGG convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1805.09391v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.09391v2)
- **Published**: 2018-05-23 19:22:03+00:00
- **Updated**: 2018-05-30 15:58:50+00:00
- **Authors**: Rahul Paul
- **Comment**: None
- **Journal**: None
- **Summary**: In robotics, knowing the object states and recognizing the desired states are very important. Objects at different states would require different grasping. To achieve different states, different manipulations would be required, as well as different grasping. To analyze the objects at different states, a dataset of cooking objects was created. Cooking consists of various cutting techniques needed for different dishes (e.g. diced, julienne etc.). Identifying each of this state of cooking objects by the human can be difficult sometimes too. In this paper, we have analyzed seven different cooking object states by tuning a convolutional neural network (CNN). For this task, images were downloaded and annotated by students and they are divided into training and a completely different test set. By tuning the vgg-16 CNN 77% accuracy was obtained. The work presented in this paper focuses on classification between various object states rather than task recognition or recipe prediction. This framework can be easily adapted in any other object state classification activity.



### A hybrid approach of interpolations and CNN to obtain super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1805.09400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09400v1)
- **Published**: 2018-05-23 19:43:57+00:00
- **Updated**: 2018-05-23 19:43:57+00:00
- **Authors**: Ram Krishna Pandey, A G Ramakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel architecture that learns an end-to-end mapping function to improve the spatial resolution of the input natural images. The model is unique in forming a nonlinear combination of three traditional interpolation techniques using the convolutional neural network. Another proposed architecture uses a skip connection with nearest neighbor interpolation, achieving almost similar results. The architectures have been carefully designed to ensure that the reconstructed images lie precisely in the manifold of high-resolution images, thereby preserving the high-frequency components with fine details. We have compared with the state of the art and recent deep learning based natural image super-resolution techniques and found that our methods are able to preserve the sharp details in the image, while also obtaining comparable or better PSNR than them. Since our methods use only traditional interpolations and a shallow CNN with less number of smaller filters, the computational cost is kept low. We have reported the results of two proposed architectures on five standard datasets, for an upscale factor of 2. Our methods generalize well in most cases, which is evident from the better results obtained with increasingly complex datasets. For 4-times upscaling, we have designed similar architectures for comparing with other methods.



### Non-convex non-local flows for saliency detection
- **Arxiv ID**: http://arxiv.org/abs/1805.09408v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1805.09408v1)
- **Published**: 2018-05-23 20:03:06+00:00
- **Updated**: 2018-05-23 20:03:06+00:00
- **Authors**: Ivn Ramrez, Gonzalo Galiano, Emanuele Schiavi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and numerically solve a new variational model for automatic saliency detection in digital images. Using a non-local framework we consider a family of edge preserving functions combined with a new quadratic saliency detection term. Such term defines a constrained bilateral obstacle problem for image classification driven by p-Laplacian operators, including the so-called hyper-Laplacian case (0 < p < 1). The related non-convex non-local reactive flows are then considered and applied for glioblastoma segmentation in magnetic resonance fluid-attenuated inversion recovery (MRI-Flair) images. A fast convolutional kernel based approximated solution is computed. The numerical experiments show how the non-convexity related to the hyperLaplacian operators provides monotonically better results in terms of the standard metrics.



### TADAM: Task dependent adaptive metric for improved few-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1805.10123v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.10123v4)
- **Published**: 2018-05-23 20:17:59+00:00
- **Updated**: 2019-01-25 18:47:30+00:00
- **Authors**: Boris N. Oreshkin, Pau Rodriguez, Alexandre Lacoste
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 31, 2018
- **Summary**: Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at https://github.com/ElementAI/TADAM.



### Use of symmetric kernels for convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1805.09421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09421v1)
- **Published**: 2018-05-23 20:57:31+00:00
- **Updated**: 2018-05-23 20:57:31+00:00
- **Authors**: Viacheslav Dudar, Vladimir Semenov
- **Comment**: ICDSIAI 2018
- **Journal**: None
- **Summary**: At this work we introduce horizontally symmetric convolutional kernels for CNNs which make the network output invariant to horizontal flips of the image. We also study other types of symmetric kernels which lead to vertical flip invariance, and approximate rotational invariance. We show that usage of such kernels acts as regularizer, and improves generalization of the convolutional neural networks at the cost of more complicated training process.



### A Two-Stage Subspace Trust Region Approach for Deep Neural Network Training
- **Arxiv ID**: http://arxiv.org/abs/1805.09430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.09430v1)
- **Published**: 2018-05-23 21:12:48+00:00
- **Updated**: 2018-05-23 21:12:48+00:00
- **Authors**: Viacheslav Dudar, Giovanni Chierchia, Emilie Chouzenoux, Jean-Christophe Pesquet, Vladimir Semenov
- **Comment**: EUSIPCO 2017
- **Journal**: None
- **Summary**: In this paper, we develop a novel second-order method for training feed-forward neural nets. At each iteration, we construct a quadratic approximation to the cost function in a low-dimensional subspace. We minimize this approximation inside a trust region through a two-stage procedure: first inside the embedded positive curvature subspace, followed by a gradient descent step. This approach leads to a fast objective function decay, prevents convergence to saddle points, and alleviates the need for manually tuning parameters. We show the good performance of the proposed algorithm on benchmark datasets.



### Implicit Language Model in LSTM for OCR
- **Arxiv ID**: http://arxiv.org/abs/1805.09441v1
- **DOI**: 10.1109/ICDAR.2017.361
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.09441v1)
- **Published**: 2018-05-23 22:01:38+00:00
- **Updated**: 2018-05-23 22:01:38+00:00
- **Authors**: Ekraam Sabir, Stephen Rawls, Prem Natarajan
- **Comment**: None
- **Journal**: 2017 14th IAPR International Conference on Document Analysis and
  Recognition (ICDAR), vol. 7 (2017) pp. 27-31
- **Summary**: Neural networks have become the technique of choice for OCR, but many aspects of how and why they deliver superior performance are still unknown. One key difference between current neural network techniques using LSTMs and the previous state-of-the-art HMM systems is that HMM systems have a strong independence assumption. In comparison LSTMs have no explicit constraints on the amount of context that can be considered during decoding. In this paper we show that they learn an implicit LM and attempt to characterize the strength of the LM in terms of equivalent n-gram context. We show that this implicitly learned language model provides a 2.4\% CER improvement on our synthetic test set when compared against a test set of random characters (i.e. not naturally occurring sequences), and that the LSTM learns to use up to 5 characters of context (which is roughly 88 frames in our configuration). We believe that this is the first ever attempt at characterizing the strength of the implicit LM in LSTM based OCR systems.



### Dyna Planning using a Feature Based Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1805.10129v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.10129v1)
- **Published**: 2018-05-23 23:23:34+00:00
- **Updated**: 2018-05-23 23:23:34+00:00
- **Authors**: Ryan Faulkner, Doina Precup
- **Comment**: 8 pages, 7 figures
- **Journal**: 24th Annual Proceedings of the Advances in Neural Information
  Processing Systems (2010) pp. 1-9
- **Summary**: Dyna-style reinforcement learning is a powerful approach for problems where not much real data is available. The main idea is to supplement real trajectories, or sequences of sampled states over time, with simulated ones sampled from a learned model of the environment. However, in large state spaces, the problem of learning a good generative model of the environment has been open so far. We propose to use deep belief networks to learn an environment model for use in Dyna. We present our approach and validate it empirically on problems where the state observations consist of images. Our results demonstrate that using deep belief networks, which are full generative models, significantly outperforms the use of linear expectation models, proposed in Sutton et al. (2008)




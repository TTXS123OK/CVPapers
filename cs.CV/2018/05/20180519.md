# Arxiv Papers in cs.CV on 2018-05-19
### Fast Disparity Estimation using Dense Networks
- **Arxiv ID**: http://arxiv.org/abs/1805.07499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1805.07499v1)
- **Published**: 2018-05-19 03:15:12+00:00
- **Updated**: 2018-05-19 03:15:12+00:00
- **Authors**: Rowel Atienza
- **Comment**: In Proc. International Conference on Robotics and Automation 2018
  (ICRA2018)
- **Journal**: None
- **Summary**: Disparity estimation is a difficult problem in stereo vision because the correspondence technique fails in images with textureless and repetitive regions. Recent body of work using deep convolutional neural networks (CNN) overcomes this problem with semantics. Most CNN implementations use an autoencoder method; stereo images are encoded, merged and finally decoded to predict the disparity map. In this paper, we present a CNN implementation inspired by dense networks to reduce the number of parameters. Furthermore, our approach takes into account semantic reasoning in disparity estimation. Our proposed network, called DenseMapNet, is compact, fast and can be trained end-to-end. DenseMapNet requires 290k parameters only and runs at 30Hz or faster on color stereo images in full resolution. Experimental results show that DenseMapNet accuracy is comparable with other significantly bigger CNN-based methods.



### Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1805.07509v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07509v7)
- **Published**: 2018-05-19 04:02:57+00:00
- **Updated**: 2020-05-28 18:26:40+00:00
- **Authors**: Jichao Zhang, Yezhi Shu, Songhua Xu, Gongze Cao, Fan Zhong, Meng Liu, Xueying Qin
- **Comment**: Accepted by ACMMM2018
- **Journal**: None
- **Summary**: Recent Image-to-Image Translation algorithms have achieved significant progress in neural style transfer and image attribute manipulation tasks. However, existing approaches require exhaustively labelling training data, which is labor demanding, difficult to scale up, and hard to migrate into new domains. To overcome such a key limitation, we propose Sparsely Grouped Generative Adversarial Networks (SG-GAN) as a novel approach that can translate images on sparsely grouped datasets where only a few samples for training are labelled. Using a novel one-input multi-output architecture, SG-GAN is well-suited for tackling sparsely grouped learning and multi-task learning. The proposed model can translate images among multiple groups using only a single commonly trained model. To experimentally validate advantages of the new model, we apply the proposed method to tackle a series of attribute manipulation tasks for facial images. Experimental results demonstrate that SG-GAN can generate image translation results of comparable quality with baselines methods on adequately labelled datasets and results of superior quality on sparsely grouped datasets. The official implementation is publicly available:https://github.com/zhangqianhui/Sparsely-Grouped-GAN.



### Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.07526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07526v2)
- **Published**: 2018-05-19 06:44:24+00:00
- **Updated**: 2018-10-26 02:22:50+00:00
- **Authors**: Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello, Zhongming Liu
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Inspired by "predictive coding" - a theory in neuroscience, we develop a bi-directional and dynamic neural network with local recurrent processing, namely predictive coding network (PCN). Unlike feedforward-only convolutional neural networks, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to refine representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classification with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also reveal visual saliency or bottom-up attention.



### Two-stage quality adaptive fingerprint image enhancement using Fuzzy c-means clustering based fingerprint quality analysis
- **Arxiv ID**: http://arxiv.org/abs/1805.07527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07527v1)
- **Published**: 2018-05-19 06:49:17+00:00
- **Updated**: 2018-05-19 06:49:17+00:00
- **Authors**: Ram Prakash Sharma, Somnath Dey
- **Comment**: 34 pages, 8 figures, Submitted to Image and Vision Computing
- **Journal**: None
- **Summary**: Fingerprint recognition techniques are immensely dependent on quality of the fingerprint images. To improve the performance of recognition algorithm for poor quality images an efficient enhancement algorithm should be designed. Performance improvement of recognition algorithm will be more if enhancement process is adaptive to the fingerprint quality (wet, dry or normal). In this paper, a quality adaptive fingerprint enhancement algorithm is proposed. The proposed fingerprint quality assessment algorithm clusters the fingerprint images in appropriate quality class of dry, wet, normal dry, normal wet and good quality using fuzzy c-means technique. It considers seven features namely, mean, moisture, variance, uniformity, contrast, ridge valley area uniformity and ridge valley uniformity into account for clustering the fingerprint images in appropriate quality class. Fingerprint images of each quality class undergo through a two-stage fingerprint quality enhancement process. A quality adaptive preprocessing method is used as front-end before enhancing the fingerprint images with Gabor, short term Fourier transform and oriented diffusion filtering based enhancement techniques. Experimental results show improvement in the verification results for FVC2004 datasets. Significant improvement in equal error rate is observed while using quality adaptive preprocessing based approaches in comparison to the current state-of-the-art enhancement techniques.



### End-to-end driving simulation via angle branched network
- **Arxiv ID**: http://arxiv.org/abs/1805.07545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07545v1)
- **Published**: 2018-05-19 08:12:46+00:00
- **Updated**: 2018-05-19 08:12:46+00:00
- **Authors**: Qing Wang, Long Chen, Wei Tian
- **Comment**: 10 pages,6 figures
- **Journal**: None
- **Summary**: Imitation learning for end-to-end autonomous driving has drawn attention from academic communities. Current methods either only use images as the input which is ambiguous when a car approaches an intersection, or use additional command information to navigate the vehicle but not automated enough. Focusing on making the vehicle drive along the given path, we propose a new navigation command that does not require human's participation and a novel model architecture called angle branched network. Both the new navigation command and the angle branched network are easy to understand and effective. Besides, we find that not only segmentation information but also depth information can boost the performance of the driving model. We conduct experiments in a 3D urban simulator and both qualitative and quantitative evaluation results show the effectiveness of our model.



### Learning Pixel-wise Labeling from the Internet without Human Interaction
- **Arxiv ID**: http://arxiv.org/abs/1805.07548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07548v1)
- **Published**: 2018-05-19 08:33:04+00:00
- **Updated**: 2018-05-19 08:33:04+00:00
- **Authors**: Yun Liu, Yujun Shi, JiaWang Bian, Le Zhang, Ming-Ming Cheng, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning stands at the forefront in many computer vision tasks. However, deep neural networks are usually data-hungry and require a huge amount of well-annotated training samples. Collecting sufficient annotated data is very expensive in many applications, especially for pixel-level prediction tasks such as semantic segmentation. To solve this fundamental issue, we consider a new challenging vision task, Internetly supervised semantic segmentation, which only uses Internet data with noisy image-level supervision of corresponding query keywords for segmentation model training. We address this task by proposing the following solution. A class-specific attention model unifying multiscale forward and backward convolutional features is proposed to provide initial segmentation "ground truth". The model trained with such noisy annotations is then improved by an online fine-tuning procedure. It achieves state-of-the-art performance under the weakly-supervised setting on PASCAL VOC2012 dataset. The proposed framework also paves a new way towards learning from the Internet without human interaction and could serve as a strong baseline therein. Code and data will be released upon the paper acceptance.



### Disc-aware Ensemble Network for Glaucoma Screening from Fundus Image
- **Arxiv ID**: http://arxiv.org/abs/1805.07549v1
- **DOI**: 10.1109/TMI.2018.2837012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07549v1)
- **Published**: 2018-05-19 08:51:00+00:00
- **Updated**: 2018-05-19 08:51:00+00:00
- **Authors**: Huazhu Fu, Jun Cheng, Yanwu Xu, Changqing Zhang, Damon Wing Kee Wong, Jiang Liu, Xiaochun Cao
- **Comment**: Project homepage: https://hzfu.github.io/proj_glaucoma_fundus.html ,
  and Accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Glaucoma is a chronic eye disease that leads to irreversible vision loss. Most of the existing automatic screening methods firstly segment the main structure, and subsequently calculate the clinical measurement for detection and screening of glaucoma. However, these measurement-based methods rely heavily on the segmentation accuracy, and ignore various visual features. In this paper, we introduce a deep learning technique to gain additional image-relevant information, and screen glaucoma from the fundus image directly. Specifically, a novel Disc-aware Ensemble Network (DENet) for automatic glaucoma screening is proposed, which integrates the deep hierarchical context of the global fundus image and the local optic disc region. Four deep streams on different levels and modules are respectively considered as global image stream, segmentation-guided network, local disc region stream, and disc polar transformation stream. Finally, the output probabilities of different streams are fused as the final screening result. The experiments on two glaucoma datasets (SCES and new SINDI datasets) show our method outperforms other state-of-the-art algorithms.



### DenseImage Network: Video Spatial-Temporal Evolution Encoding and Understanding
- **Arxiv ID**: http://arxiv.org/abs/1805.07550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07550v1)
- **Published**: 2018-05-19 08:53:44+00:00
- **Updated**: 2018-05-19 08:53:44+00:00
- **Authors**: Xiaokai Chen, Ke Gao
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Many of the leading approaches for video understanding are data-hungry and time-consuming, failing to capture the gist of spatial-temporal evolution in an efficient manner. The latest research shows that CNN network can reason about static relation of entities in images. To further exploit its capacity in dynamic evolution reasoning, we introduce a novel network module called DenseImage Network(DIN) with two main contributions. 1) A novel compact representation of video which distills its significant spatial-temporal evolution into a matrix called DenseImage, primed for efficient video encoding. 2) A simple yet powerful learning strategy based on DenseImage and a temporal-order-preserving CNN network is proposed for video understanding, which contains a local temporal correlation constraint capturing temporal evolution at multiple time scales with different filter widths. Extensive experiments on two recent challenging benchmarks demonstrate that our DenseImage Network can accurately capture the common spatial-temporal evolution between similar actions, even with enormous visual variations or different time scales. Moreover, we obtain the state-of-the-art results in action and gesture recognition with much less time-and-memory cost, indicating its immense potential in video representing and understanding.



### Wildest Faces: Face Detection and Recognition in Violent Settings
- **Arxiv ID**: http://arxiv.org/abs/1805.07566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07566v1)
- **Published**: 2018-05-19 10:46:24+00:00
- **Updated**: 2018-05-19 10:46:24+00:00
- **Authors**: Mehmet Kerim Yucel, Yunus Can Bilge, Oguzhan Oguz, Nazli Ikizler-Cinbis, Pinar Duygulu, Ramazan Gokberk Cinbis
- **Comment**: Submitted to BMVC 2018
- **Journal**: None
- **Summary**: With the introduction of large-scale datasets and deep learning models capable of learning complex representations, impressive advances have emerged in face detection and recognition tasks. Despite such advances, existing datasets do not capture the difficulty of face recognition in the wildest scenarios, such as hostile disputes or fights. Furthermore, existing datasets do not represent completely unconstrained cases of low resolution, high blur and large pose/occlusion variances. To this end, we introduce the Wildest Faces dataset, which focuses on such adverse effects through violent scenes. The dataset consists of an extensive set of violent scenes of celebrities from movies. Our experimental results demonstrate that state-of-the-art techniques are not well-suited for violent scenes, and therefore, Wildest Faces is likely to stir further interest in face detection and recognition research.



### Optimizing the F-measure for Threshold-free Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1805.07567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07567v2)
- **Published**: 2018-05-19 10:54:43+00:00
- **Updated**: 2019-08-13 10:06:43+00:00
- **Authors**: Kai Zhao, Shanghua Gao, Wenguan Wang, Ming-Ming Cheng
- **Comment**: http://kaizhao.net/fmeasure
- **Journal**: ICCV 2019
- **Summary**: Current CNN-based solutions to salient object detection (SOD) mainly rely on the optimization of cross-entropy loss (CELoss). Then the quality of detected saliency maps is often evaluated in terms of F-measure. In this paper, we investigate an interesting issue: can we consistently use the F-measure formulation in both training and evaluation for SOD? By reformulating the standard F-measure we propose the relaxed F-measure which is differentiable w.r.t the posterior and can be easily appended to the back of CNNs as the loss function. Compared to the conventional cross-entropy loss of which the gradients decrease dramatically in the saturated area, our loss function, named FLoss, holds considerable gradients even when the activation approaches the target. Consequently, the FLoss can continuously force the network to produce polarized activations. Comprehensive benchmarks on several popular datasets show that FLoss outperforms the state-of-the-art with a considerable margin. More specifically, due to the polarized predictions, our method is able to obtain high-quality saliency maps without carefully tuning the optimal threshold, showing significant advantages in real-world applications.



### Fast Object Classification in Single-pixel Imaging
- **Arxiv ID**: http://arxiv.org/abs/1805.07582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07582v1)
- **Published**: 2018-05-19 12:41:19+00:00
- **Updated**: 2018-05-19 12:41:19+00:00
- **Authors**: Shuming Jiao
- **Comment**: None
- **Journal**: None
- **Summary**: In single-pixel imaging (SPI), the target object is illuminated with varying patterns sequentially and an intensity sequence is recorded by a single-pixel detector without spatial resolution. A high quality object image can only be computationally reconstructed after a large number of illuminations, with disadvantages of long imaging time and high cost. Conventionally, object classification is performed after a reconstructed object image with good fidelity is available. In this paper, we propose to classify the target object with a small number of illuminations in a fast manner for Fourier SPI. A naive Bayes classifier is employed to classify the target objects based on the single-pixel intensity sequence without any image reconstruction and each sequence element is regarded as an object feature in the classifier. Simulation results demonstrate our proposed scheme can classify the number digit object images with high accuracy (e.g. 80% accuracy using only 13 illuminations, at a sampling ratio of 0.3%).



### Robust Optimization over Multiple Domains
- **Arxiv ID**: http://arxiv.org/abs/1805.07588v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1805.07588v2)
- **Published**: 2018-05-19 13:19:44+00:00
- **Updated**: 2018-11-14 19:13:28+00:00
- **Authors**: Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, Hao Li
- **Comment**: accepted by AAAI'19
- **Journal**: None
- **Summary**: In this work, we study the problem of learning a single model for multiple domains. Unlike the conventional machine learning scenario where each domain can have the corresponding model, multiple domains (i.e., applications/users) may share the same machine learning model due to maintenance loads in cloud computing services. For example, a digit-recognition model should be applicable to hand-written digits, house numbers, car plates, etc. Therefore, an ideal model for cloud computing has to perform well at each applicable domain. To address this new challenge from cloud computing, we develop a framework of robust optimization over multiple domains. In lieu of minimizing the empirical risk, we aim to learn a model optimized to the adversarial distribution over multiple domains. Hence, we propose to learn the model and the adversarial distribution simultaneously with the stochastic algorithm for efficiency. Theoretically, we analyze the convergence rate for convex and non-convex models. To our best knowledge, we first study the convergence rate of learning a robust non-convex model with a practical algorithm. Furthermore, we demonstrate that the robustness of the framework and the convergence rate can be further enhanced by appropriate regularizers over the adversarial distribution. The empirical study on real-world fine-grained visual categorization and digits recognition tasks verifies the effectiveness and efficiency of the proposed framework.



### Generative Creativity: Adversarial Learning for Bionic Design
- **Arxiv ID**: http://arxiv.org/abs/1805.07615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07615v1)
- **Published**: 2018-05-19 15:44:29+00:00
- **Updated**: 2018-05-19 15:44:29+00:00
- **Authors**: Simiao Yu, Hao Dong, Pan Wang, Chao Wu, Yike Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Bionic design refers to an approach of generative creativity in which a target object (e.g. a floor lamp) is designed to contain features of biological source objects (e.g. flowers), resulting in creative biologically-inspired design. In this work, we attempt to model the process of shape-oriented bionic design as follows: given an input image of a design target object, the model generates images that 1) maintain shape features of the input design target image, 2) contain shape features of images from the specified biological source domain, 3) are plausible and diverse. We propose DesignGAN, a novel unsupervised deep generative approach to realising bionic design. Specifically, we employ a conditional Generative Adversarial Networks architecture with several designated losses (an adversarial loss, a regression loss, a cycle loss and a latent loss) that respectively constrict our model to meet the corresponding aforementioned requirements of bionic design modelling. We perform qualitative and quantitative experiments to evaluate our method, and demonstrate that our proposed approach successfully generates creative images of bionic design.



### Do Neural Network Cross-Modal Mappings Really Bridge Modalities?
- **Arxiv ID**: http://arxiv.org/abs/1805.07616v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07616v2)
- **Published**: 2018-05-19 15:51:43+00:00
- **Updated**: 2018-06-04 16:16:37+00:00
- **Authors**: Guillem Collell, Marie-Francine Moens
- **Comment**: To appear at ACL 2018
- **Journal**: None
- **Summary**: Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.



### CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1805.07621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07621v2)
- **Published**: 2018-05-19 16:50:37+00:00
- **Updated**: 2018-10-20 08:07:49+00:00
- **Authors**: Liheng Zhang, Marzieh Edraki, Guo-Jun Qi
- **Comment**: Liheng Zhang, Marzieh Edraki, Guo-Jun Qi. CapProNet: Deep Feature
  Learning via Orthogonal Projections onto Capsule Subspaces, in Proccedings of
  Thirty-second Conference on Neural Information Processing Systems (NIPS
  2018), Palais des Congr\`es de Montr\'eal, Montr\'eal, Canda, December 3-8,
  2018
- **Journal**: None
- **Summary**: In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes. We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class. We will also show that the capsule projection can be viewed as normalizing the multiple columns of the weight matrix simultaneously to form an orthogonal basis, which makes it more effective in incorporating novel components of input features to update capsule representations. In other words, the capsule projection can be viewed as a multi-dimensional weight normalization in capsule subspaces, where the conventional weight normalization is simply a special case of the capsule projection onto 1D lines. Only a small negligible computing overhead is incurred to train the network in low-dimensional capsule subspaces or through an alternative hyper-power iteration to estimate the normalization matrix. Experiment results on image datasets show the presented model can greatly improve the performance of the state-of-the-art ResNet backbones by $10-20\%$ and that of the Densenet by $5-7\%$ respectively at the same level of computing and memory expenses. The CapProNet establishes the competitive state-of-the-art performance for the family of capsule nets by significantly reducing test errors on the benchmark datasets.



### Latent Space Non-Linear Statistics
- **Arxiv ID**: http://arxiv.org/abs/1805.07632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07632v1)
- **Published**: 2018-05-19 18:05:26+00:00
- **Updated**: 2018-05-19 18:05:26+00:00
- **Authors**: Line Kuhnel, Tom Fletcher, Sarang Joshi, Stefan Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: Given data, deep generative models, such as variational autoencoders (VAE) and generative adversarial networks (GAN), train a lower dimensional latent representation of the data space. The linear Euclidean geometry of data space pulls back to a nonlinear Riemannian geometry on the latent space. The latent space thus provides a low-dimensional nonlinear representation of data and classical linear statistical techniques are no longer applicable. In this paper we show how statistics of data in their latent space representation can be performed using techniques from the field of nonlinear manifold statistics. Nonlinear manifold statistics provide generalizations of Euclidean statistical notions including means, principal component analysis, and maximum likelihood fits of parametric probability distributions. We develop new techniques for maximum likelihood inference in latent space, and adress the computational complexity of using geometric algorithms with high-dimensional data by training a separate neural network to approximate the Riemannian metric and cometric tensor capturing the shape of the learned data manifold.



### Learning Sampling Policies for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1805.07641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07641v1)
- **Published**: 2018-05-19 19:09:18+00:00
- **Updated**: 2018-05-19 19:09:18+00:00
- **Authors**: Yash Patel, Kashyap Chitta, Bhavan Jasani
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of semi-supervised domain adaptation of classification algorithms through deep Q-learning. The core idea is to consider the predictions of a source domain network on target domain data as noisy labels, and learn a policy to sample from this data so as to maximize classification accuracy on a small annotated reward partition of the target domain. Our experiments show that learned sampling policies construct labeled sets that improve accuracies of visual classifiers over baselines.



### Capturing human category representations by sampling in deep feature spaces
- **Arxiv ID**: http://arxiv.org/abs/1805.07644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07644v1)
- **Published**: 2018-05-19 19:31:48+00:00
- **Updated**: 2018-05-19 19:31:48+00:00
- **Authors**: Joshua C. Peterson, Jordan W. Suchow, Krisha Aghi, Alexander Y. Ku, Thomas L. Griffiths
- **Comment**: 6 pages, 5 figures, 1 table. Accepted as a paper to the 40th Annual
  Meeting of the Cognitive Science Society (CogSci 2018)
- **Journal**: None
- **Summary**: Understanding how people represent categories is a core problem in cognitive science. Decades of research have yielded a variety of formal theories of categories, but validating them with naturalistic stimuli is difficult. The challenge is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires a workable representation of these stimuli. Deep neural networks have recently been successful in solving a range of computer vision tasks and provide a way to compactly represent image features. Here, we introduce a method to estimate the structure of human categories that combines ideas from cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep image generators. We provide qualitative and quantitative results as a proof-of-concept for the method's feasibility. Samples drawn from human distributions rival those from state-of-the-art generative models in quality and outperform alternative methods for estimating the structure of human categories.



### Long-term face tracking in the wild using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1805.07646v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1805.07646v1)
- **Published**: 2018-05-19 20:00:26+00:00
- **Updated**: 2018-05-19 20:00:26+00:00
- **Authors**: Kunlei Zhang, Elaheh Rashedi, Elaheh Barati, Xue-wen Chen
- **Comment**: KDD Workshop on Large-scale Deep Learning for Data Mining, August
  2016, San Fransisco, CA, USA
- **Journal**: None
- **Summary**: This paper investigates long-term face tracking of a specific person given his/her face image in a single frame as a query in a video stream. Through taking advantage of pre-trained deep learning models on big data, a novel system is developed for accurate video face tracking in the unconstrained environments depicting various people and objects moving in and out of the frame. In the proposed system, we present a detection-verification-tracking method (dubbed as 'DVT') which accomplishes the long-term face tracking task through the collaboration of face detection, face verification, and (short-term) face tracking. An offline trained detector based on cascaded convolutional neural networks localizes all faces appeared in the frames, and an offline trained face verifier based on deep convolutional neural networks and similarity metric learning decides if any face or which face corresponds to the queried person. An online trained tracker follows the face from frame to frame. When validated on a sitcom episode and a TV show, the DVT method outperforms tracking-learning-detection (TLD) and face-TLD in terms of recall and precision. The proposed system is also tested on many other types of videos and shows very promising results.



### Learning Hierarchical Visual Representations in Deep Neural Networks Using Hierarchical Linguistic Labels
- **Arxiv ID**: http://arxiv.org/abs/1805.07647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07647v1)
- **Published**: 2018-05-19 20:03:20+00:00
- **Updated**: 2018-05-19 20:03:20+00:00
- **Authors**: Joshua C. Peterson, Paul Soulos, Aida Nematzadeh, Thomas L. Griffiths
- **Comment**: 6 pages, 4 figures, 1 table. Accepted as a paper to the 40th Annual
  Meeting of the Cognitive Science Society (CogSci 2018)
- **Journal**: None
- **Summary**: Modern convolutional neural networks (CNNs) are able to achieve human-level object classification accuracy on specific tasks, and currently outperform competing models in explaining complex human visual representations. However, the categorization problem is posed differently for these networks than for humans: the accuracy of these networks is evaluated by their ability to identify single labels assigned to each image. These labels often cut arbitrarily across natural psychological taxonomies (e.g., dogs are separated into breeds, but never jointly categorized as "dogs"), and bias the resulting representations. By contrast, it is common for children to hear both "dog" and "Dalmatian" to describe the same stimulus, helping to group perceptually disparate objects (e.g., breeds) into a common mental class. In this work, we train CNN classifiers with multiple labels for each image that correspond to different levels of abstraction, and use this framework to reproduce classic patterns that appear in human generalization behavior.



### On Attention Models for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1805.07648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1805.07648v1)
- **Published**: 2018-05-19 20:13:05+00:00
- **Updated**: 2018-05-19 20:13:05+00:00
- **Authors**: Vishvak S Murahari, Thomas Ploetz
- **Comment**: None
- **Journal**: None
- **Summary**: Most approaches that model time-series data in human activity recognition based on body-worn sensing (HAR) use a fixed size temporal context to represent different activities. This might, however, not be apt for sets of activities with individ- ually varying durations. We introduce attention models into HAR research as a data driven approach for exploring relevant temporal context. Attention models learn a set of weights over input data, which we leverage to weight the temporal context being considered to model each sensor reading. We construct attention models for HAR by adding attention layers to a state- of-the-art deep learning HAR model (DeepConvLSTM) and evaluate our approach on benchmark datasets achieving sig- nificant increase in performance. Finally, we visualize the learned weights to better understand what constitutes relevant temporal context.



### Learning a face space for experiments on human identity
- **Arxiv ID**: http://arxiv.org/abs/1805.07653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07653v1)
- **Published**: 2018-05-19 20:51:49+00:00
- **Updated**: 2018-05-19 20:51:49+00:00
- **Authors**: Jordan W. Suchow, Joshua C. Peterson, Thomas L. Griffiths
- **Comment**: 10 figures. Accepted as a paper to the 40th Annual Meeting of the
  Cognitive Science Society (CogSci 2018). *JWS and JCP contributed equally to
  this submission
- **Journal**: None
- **Summary**: Generative models of human identity and appearance have broad applicability to behavioral science and technology, but the exquisite sensitivity of human face perception means that their utility hinges on the alignment of the model's representation to human psychological representations and the photorealism of the generated images. Meeting these requirements is an exacting task, and existing models of human identity and appearance are often unworkably abstract, artificial, uncanny, or biased. Here, we use a variational autoencoder with an autoregressive decoder to learn a face space from a uniquely diverse dataset of portraits that control much of the variation irrelevant to human identity and appearance. Our method generates photorealistic portraits of fictive identities with a smooth, navigable latent space. We validate our model's alignment with human sensitivities by introducing a psychophysical Turing test for images, which humans mostly fail. Lastly, we demonstrate an initial application of our model to the problem of fast search in mental space to obtain detailed "police sketches" in a small number of trials.



### An Evaluation of Trajectory Prediction Approaches and Notes on the TrajNet Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1805.07663v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1805.07663v6)
- **Published**: 2018-05-19 22:08:54+00:00
- **Updated**: 2018-08-16 09:21:13+00:00
- **Authors**: Stefan Becker, Ronny Hug, Wolfgang HÃ¼bner, Michael Arens
- **Comment**: Accepted at ECCV Workshop on Anticipating Human Behavior under
  adapted title. RED: A simple but effective Baseline Predictor for the TrajNet
  Benchmark
- **Journal**: None
- **Summary**: In recent years, there is a shift from modeling the tracking problem based on Bayesian formulation towards using deep neural networks. Towards this end, in this paper the effectiveness of various deep neural networks for predicting future pedestrian paths are evaluated. The analyzed deep networks solely rely, like in the traditional approaches, on observed tracklets without human-human interaction information. The evaluation is done on the publicly available TrajNet benchmark dataset, which builds up a repository of considerable and popular datasets for trajectory-based activity forecasting. We show that a Recurrent-Encoder with a Dense layer stacked on top, referred to as RED-predictor, is able to achieve sophisticated results compared to elaborated models in such scenarios. Further, we investigate failure cases and give explanations for observed phenomena and give some recommendations for overcoming demonstrated shortcomings.




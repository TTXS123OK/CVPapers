# Arxiv Papers in cs.CV on 2018-12-17
### Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1812.06570v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06570v3)
- **Published**: 2018-12-17 01:13:44+00:00
- **Updated**: 2019-08-01 03:11:26+00:00
- **Authors**: Xiang Li, Shihao Ji
- **Comment**: Published as a workshop paper at MLCS 2019
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been enormously successful across a variety of prediction tasks. However, recent research shows that DNNs are particularly vulnerable to adversarial attacks, which poses a serious threat to their applications in security-sensitive systems. In this paper, we propose a simple yet effective defense algorithm Defense-VAE that uses variational autoencoder (VAE) to purge adversarial perturbations from contaminated images. The proposed method is generic and can defend white-box and black-box attacks without the need of retraining the original CNN classifiers, and can further strengthen the defense by retraining CNN or end-to-end finetuning the whole pipeline. In addition, the proposed method is very efficient compared to the optimization-based alternatives, such as Defense-GAN, since no iterative optimization is needed for online prediction. Extensive experiments on MNIST, Fashion-MNIST, CelebA and CIFAR-10 demonstrate the superior defense accuracy of Defense-VAE compared to Defense-GAN, while being 50x faster than the latter. This makes Defense-VAE widely deployable in real-time security-sensitive systems. Our source code can be found at https://github.com/lxuniverse/defense-vae.



### Latent Dirichlet Allocation in Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.06571v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06571v5)
- **Published**: 2018-12-17 01:21:20+00:00
- **Updated**: 2019-11-06 14:56:54+00:00
- **Authors**: Lili Pan, Shen Cheng, Jian Liu, Yazhou Ren, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of multimodal generative modelling of images based on generative adversarial networks (GANs). Despite the success of existing methods, they often ignore the underlying structure of vision data or its multimodal generation characteristics. To address this problem, we introduce the Dirichlet prior for multimodal image generation, which leads to a new Latent Dirichlet Allocation based GAN (LDAGAN). In detail, for the generative process modelling, LDAGAN defines a generative mode for each sample, determining which generative sub-process it belongs to. For the adversarial training, LDAGAN derives a variational expectation-maximization (VEM) algorithm to estimate model parameters. Experimental results on real-world datasets have demonstrated the outstanding performance of LDAGAN over other existing GANs.



### Learning Incremental Triplet Margin for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1812.06576v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.06576v1)
- **Published**: 2018-12-17 01:48:06+00:00
- **Updated**: 2018-12-17 01:48:06+00:00
- **Authors**: Yingying Zhang, Qiaoyong Zhong, Liang Ma, Di Xie, Shiliang Pu
- **Comment**: accepted by AAAI19 as spotlight
- **Journal**: None
- **Summary**: Person re-identification (ReID) aims to match people across multiple non-overlapping video cameras deployed at different locations. To address this challenging problem, many metric learning approaches have been proposed, among which triplet loss is one of the state-of-the-arts. In this work, we explore the margin between positive and negative pairs of triplets and prove that large margin is beneficial. In particular, we propose a novel multi-stage training strategy which learns incremental triplet margin and improves triplet loss effectively. Multiple levels of feature maps are exploited to make the learned features more discriminative. Besides, we introduce global hard identity searching method to sample hard identities when generating a training batch. Extensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our approach yields a performance boost and outperforms most existing state-of-the-art methods.



### $\ell_0$-Motivated Low-Rank Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1812.06580v1
- **DOI**: 10.1109/TCYB.2018.2883566
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06580v1)
- **Published**: 2018-12-17 02:06:16+00:00
- **Updated**: 2018-12-17 02:06:16+00:00
- **Authors**: Maria Brbić, Ivica Kopriva
- **Comment**: None
- **Journal**: None
- **Summary**: In many applications, high-dimensional data points can be well represented by low-dimensional subspaces. To identify the subspaces, it is important to capture a global and local structure of the data which is achieved by imposing low-rank and sparseness constraints on the data representation matrix. In low-rank sparse subspace clustering (LRSSC), nuclear and $\ell_1$ norms are used to measure rank and sparsity. However, the use of nuclear and $\ell_1$ norms leads to an overpenalized problem and only approximates the original problem. In this paper, we propose two $\ell_0$ quasi-norm based regularizations. First, the paper presents regularization based on multivariate generalization of minimax-concave penalty (GMC-LRSSC), which contains the global minimizers of $\ell_0$ quasi-norm regularized objective. Afterward, we introduce the Schatten-0 ($S_0$) and $\ell_0$ regularized objective and approximate the proximal map of the joint solution using a proximal average method ($S_0/\ell_0$-LRSSC). The resulting nonconvex optimization problems are solved using alternating direction method of multipliers with established convergence conditions of both algorithms. Results obtained on synthetic and four real-world datasets show the effectiveness of GMC-LRSSC and $S_0/\ell_0$-LRSSC when compared to state-of-the-art methods.



### Grounded Video Description
- **Arxiv ID**: http://arxiv.org/abs/1812.06587v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06587v2)
- **Published**: 2018-12-17 02:46:17+00:00
- **Updated**: 2019-05-05 19:52:55+00:00
- **Authors**: Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, Marcus Rohrbach
- **Comment**: CVPR 2019 oral, camera-ready version including appendix
- **Journal**: None
- **Summary**: Video description is one of the most challenging problems in vision and language understanding due to the large variability both on the video and language side. Models, hence, typically shortcut the difficulty in recognition and generate plausible sentences that are based on priors but are not necessarily grounded in the video. In this work, we explicitly link the sentence to the evidence in the video by annotating each noun phrase in a sentence with the corresponding bounding box in one of the frames of a video. Our dataset, ActivityNet-Entities, augments the challenging ActivityNet Captions dataset with 158k bounding box annotations, each grounding a noun phrase. This allows training video description models with this data, and importantly, evaluate how grounded or "true" such model are to the video they describe. To generate grounded captions, we propose a novel video description model which is able to exploit these bounding box annotations. We demonstrate the effectiveness of our model on our dataset, but also show how it can be applied to image description on the Flickr30k Entities dataset. We achieve state-of-the-art performance on video description, video paragraph description, and image description and demonstrate our generated sentences are better grounded in the video.



### Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.06589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06589v2)
- **Published**: 2018-12-17 02:56:09+00:00
- **Updated**: 2020-05-13 13:22:47+00:00
- **Authors**: Hao Zhu, Huaibo Huang, Yi Li, Aihua Zheng, Ran He
- **Comment**: IJCAI-2020
- **Journal**: None
- **Summary**: Talking face generation aims to synthesize a face video with precise lip synchronization as well as a smooth transition of facial motion over the entire video via the given speech clip and facial image. Most existing methods mainly focus on either disentangling the information in a single image or learning temporal information between frames. However, cross-modality coherence between audio and video information has not been well addressed during synthesis. In this paper, we propose a novel arbitrary talking face generation framework by discovering the audio-visual coherence via the proposed Asymmetric Mutual Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA) block by selectively focusing the lip area of the input image during the training stage, to further enhance lip synchronization. Experimental results on benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods on prevalent metrics with robust high-resolution synthesizing on gender and pose variations.



### Learning Student Networks via Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/1812.06597v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06597v1)
- **Published**: 2018-12-17 03:21:20+00:00
- **Updated**: 2018-12-17 03:21:20+00:00
- **Authors**: Hanting Chen, Yunhe Wang, Chang Xu, Chao Xu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student based methods used to rely on additional fully-connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this paper aims to propagate information from teacher to student without introducing new variables which need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features which could inherit intrinsic properties of their corresponding high-dimensional features from teacher network. The resulting portable network thus can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark datasets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.



### A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.06611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06611v1)
- **Published**: 2018-12-17 04:15:41+00:00
- **Updated**: 2018-12-17 04:15:41+00:00
- **Authors**: Weijie Chen, Yuan Zhang, Di Xie, Shiliang Pu
- **Comment**: accepted by AAAI19 as oral
- **Journal**: None
- **Summary**: Neuron pruning is an efficient method to compress the network into a slimmer one for reducing the computational cost and storage overhead. Most of state-of-the-art results are obtained in a layer-by-layer optimization mode. It discards the unimportant input neurons and uses the survived ones to reconstruct the output neurons approaching to the original ones in a layer-by-layer manner. However, an unnoticed problem arises that the information loss is accumulated as layer increases since the survived neurons still do not encode the entire information as before. A better alternative is to propagate the entire useful information to reconstruct the pruned layer instead of directly discarding the less important neurons. To this end, we propose a novel Layer Decomposition-Recomposition Framework (LDRF) for neuron pruning, by which each layer's output information is recovered in an embedding space and then propagated to reconstruct the following pruned layers with useful information preserved. We mainly conduct our experiments on ILSVRC-12 benchmark with VGG-16 and ResNet-50. What should be emphasized is that our results before end-to-end fine-tuning are significantly superior owing to the information-preserving property of our proposed framework.With end-to-end fine-tuning, we achieve state-of-the-art results of 5.13x and 3x speed-up with only 0.5% and 0.65% top-5 accuracy drop respectively, which outperform the existing neuron pruning methods.



### Voiceprint recognition of Parkinson patients based on deep learning
- **Arxiv ID**: http://arxiv.org/abs/1812.06613v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1812.06613v1)
- **Published**: 2018-12-17 04:24:46+00:00
- **Updated**: 2018-12-17 04:24:46+00:00
- **Authors**: Zhijing Xu, Juan Wang, Ying Zhang, Xiangjian He
- **Comment**: 10 pages,4 figures
- **Journal**: None
- **Summary**: More than 90% of the Parkinson Disease (PD) patients suffer from vocal disorders. Speech impairment is already indicator of PD. This study focuses on PD diagnosis through voiceprint features. In this paper, a method based on Deep Neural Network (DNN) recognition and classification combined with Mini-Batch Gradient Descent (MBGD) is proposed to distinguish PD patients from healthy people using voiceprint features. In order to exact the voiceprint features from patients, Weighted Mel Frequency Cepstrum Coefficients (WMFCC) is applied. The proposed method is tested on experimental data obtained by the voice recordings of three sustained vowels /a/, /o/ and /u/ from participants (48 PD and 20 healthy people). The results show that the proposed method achieves a high accuracy of diagnosis of PD patients from healthy people, than the conventional methods like Support Vector Machine (SVM) and other mentioned in this paper. The accuracy achieved is 89.5%. WMFCC approach can solve the problem that the high-order cepstrum coefficients are small and the features component's representation ability to the audio is weak. MBGD reduces the computational loads of the loss function, and increases the training speed of the system. DNN classifier enhances the classification ability of voiceprint features. Therefore, the above approaches can provide a solid solution for the quick auxiliary diagnosis of PD in early stage.



### Feature Fusion Effects of Tensor Product Representation on (De)Compositional Network for Caption Generation for Images
- **Arxiv ID**: http://arxiv.org/abs/1812.06624v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1812.06624v1)
- **Published**: 2018-12-17 06:24:03+00:00
- **Updated**: 2018-12-17 06:24:03+00:00
- **Authors**: Chiranjib Sur
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Progress in image captioning is gradually getting complex as researchers try to generalized the model and define the representation between visual features and natural language processing. This work tried to define such kind of relationship in the form of representation called Tensor Product Representation (TPR) which generalized the scheme of language modeling and structuring the linguistic attributes (related to grammar and parts of speech of language) which will provide a much better structure and grammatically correct sentence. TPR enables better and unique representation and structuring of the feature space and will enable better sentence composition from these representations. A large part of the different ways of defining and improving these TPR are discussed and their performance with respect to the traditional procedures and feature representations are evaluated for image captioning application. The new models achieved considerable improvement than the corresponding previous architectures.



### Semi-supervised mp-MRI Data Synthesis with StitchLayer and Auxiliary Distance Maximization
- **Arxiv ID**: http://arxiv.org/abs/1812.06625v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06625v1)
- **Published**: 2018-12-17 06:27:51+00:00
- **Updated**: 2018-12-17 06:27:51+00:00
- **Authors**: Zhiwei Wang, Yi Lin, Kwang-Ting Cheng, Xin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging (mp-MRI) data, i.e. Apparent Diffusion Coefficients (ADC) and T2-weighted (T2w), containing clinically significant (CS) prostate cancer (PCa) via semi-supervised adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first generating ADC maps from 128-d latent vectors, followed by translating them to the T2w images. The synthesizer is trained in a semisupervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness of synthesizing, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to fuse sub-images together in an interlaced manner into a full-size image. To enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mpMRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship. Compared to the state-of-the-art synthesis methods, our method achieves a significant improvement in terms of both visual and quantitative evaluation metrics.



### Attending Category Disentangled Global Context for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.06663v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06663v5)
- **Published**: 2018-12-17 09:17:45+00:00
- **Updated**: 2022-06-07 15:09:53+00:00
- **Authors**: Keke Tang, Guodong Wei, Runnan Chen, Jie Zhu, Zhaoquan Gu, Wenping Wang
- **Comment**: I have not gotten legal permission to post it on arXiv from the
  corresponding authors
- **Journal**: None
- **Summary**: In this paper, we propose a general framework for image classification using the attention mechanism and global context, which could incorporate with various network architectures to improve their performance. To investigate the capability of the global context, we compare four mathematical models and observe the global context encoded in the category disentangled conditional generative model could give more guidance as "know what is task irrelevant will also know what is relevant". Based on this observation, we define a novel Category Disentangled Global Context (CDGC) and devise a deep network to obtain it. By attending CDGC, the baseline networks could identify the objects of interest more accurately, thus improving the performance. We apply the framework to many different network architectures and compare with the state-of-the-art on four publicly available datasets. Extensive results validate the effectiveness and superiority of our approach. Code will be made public upon paper acceptance.



### Robust Graph Learning from Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/1812.06673v1
- **DOI**: 10.1109/TCYB.2018.2887094
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1812.06673v1)
- **Published**: 2018-12-17 10:01:59+00:00
- **Updated**: 2018-12-17 10:01:59+00:00
- **Authors**: Zhao Kang, Haiqi Pan, Steven C. H. Hoi, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning graphs from data automatically has shown encouraging performance on clustering and semisupervised learning tasks. However, real data are often corrupted, which may cause the learned graph to be inexact or unreliable. In this paper, we propose a novel robust graph learning scheme to learn reliable graphs from real-world noisy data by adaptively removing noise and errors in the raw data. We show that our proposed model can also be viewed as a robust version of manifold regularized robust PCA, where the quality of the graph plays a critical role. The proposed model is able to boost the performance of data clustering, semisupervised classification, and data recovery significantly, primarily due to two key factors: 1) enhanced low-rank recovery by exploiting the graph smoothness assumption, 2) improved graph construction by exploiting clean data recovered by robust PCA. Thus, it boosts the clustering, semi-supervised classification, and data recovery performance overall. Extensive experiments on image/document clustering, object recognition, image shadow removal, and video background subtraction reveal that our model outperforms the previous state-of-the-art methods.



### Floorplan-Jigsaw: Jointly Estimating Scene Layout and Aligning Partial Scans
- **Arxiv ID**: http://arxiv.org/abs/1812.06677v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06677v3)
- **Published**: 2018-12-17 10:24:46+00:00
- **Updated**: 2019-12-03 11:37:29+00:00
- **Authors**: Cheng Lin, Changjian Li, Wenping Wang
- **Comment**: Published at ICCV 2019. Previously on arxiv as "Floorplan Priors for
  Joint Camera Pose and Room Layout Estimation"
- **Journal**: None
- **Summary**: We present a novel approach to align partial 3D reconstructions which may not have substantial overlap. Using floorplan priors, our method jointly predicts a room layout and estimates the transformations from a set of partial 3D data. Unlike the existing methods relying on feature descriptors to establish correspondences, we exploit the 3D "box" structure of a typical room layout that meets the Manhattan World property. We first estimate a local layout for each partial scan separately and then combine these local layouts to form a globally aligned layout with loop closure. Without the requirement of feature matching, the proposed method enables some novel applications ranging from large or featureless scene reconstruction and modeling from sparse input. We validate our method quantitatively and qualitatively on real and synthetic scenes of various sizes and complexities. The evaluations and comparisons show superior effectiveness and accuracy of our method.



### Not Using the Car to See the Sidewalk: Quantifying and Controlling the Effects of Context in Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.06707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06707v1)
- **Published**: 2018-12-17 11:28:05+00:00
- **Updated**: 2018-12-17 11:28:05+00:00
- **Authors**: Rakshith Shetty, Bernt Schiele, Mario Fritz
- **Comment**: 14 pages (12 figures)
- **Journal**: None
- **Summary**: Importance of visual context in scene understanding tasks is well recognized in the computer vision community. However, to what extent the computer vision models for image classification and semantic segmentation are dependent on the context to make their predictions is unclear. A model overly relying on context will fail when encountering objects in context distributions different from training data and hence it is important to identify these dependencies before we can deploy the models in the real-world. We propose a method to quantify the sensitivity of black-box vision models to visual context by editing images to remove selected objects and measuring the response of the target models. We apply this methodology on two tasks, image classification and semantic segmentation, and discover undesirable dependency between objects and context, for example that "sidewalk" segmentation relies heavily on "cars" being present in the image. We propose an object removal based data augmentation solution to mitigate this dependency and increase the robustness of classification and segmentation models to contextual variations. Our experiments show that the proposed data augmentation helps these models improve the performance in out-of-context scenarios, while preserving the performance on regular data.



### Fully-deformable 3D image registration in two seconds
- **Arxiv ID**: http://arxiv.org/abs/1812.06765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06765v1)
- **Published**: 2018-12-17 13:52:09+00:00
- **Updated**: 2018-12-17 13:52:09+00:00
- **Authors**: Daniel Budelmann, Lars König, Nils Papenberg, Jan Lellmann
- **Comment**: Accepted for publication in Bildverarbeitung f\"ur die Medizin (BVM)
  Proceedings 2019
- **Journal**: None
- **Summary**: We present a highly parallel method for accurate and efficient variational deformable 3D image registration on a consumer-grade graphics processing unit (GPU). We build on recent matrix-free variational approaches and specialize the concepts to the massively-parallel manycore architecture provided by the GPU. Compared to a parallel and optimized CPU implementation, this allows us to achieve an average speedup of 32.53 on 986 real-world CT thorax-abdomen follow-up scans. At a resolution of approximately $256^3$ voxels, the average runtime is 1.99 seconds for the full registration. On the publicly available DIR-lab benchmark, our method ranks third with respect to average landmark error at an average runtime of 0.32 seconds.



### Variational Autoencoders Pursue PCA Directions (by Accident)
- **Arxiv ID**: http://arxiv.org/abs/1812.06775v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06775v2)
- **Published**: 2018-12-17 14:06:18+00:00
- **Updated**: 2019-04-16 12:20:39+00:00
- **Authors**: Michal Rolinek, Dominik Zietlow, Georg Martius
- **Comment**: None
- **Journal**: None
- **Summary**: The Variational Autoencoder (VAE) is a powerful architecture capable of representation learning and generative modeling. When it comes to learning interpretable (disentangled) representations, VAE and its variants show unparalleled performance. However, the reasons for this are unclear, since a very particular alignment of the latent embedding is needed but the design of the VAE does not encourage it in any explicit way. We address this matter and offer the following explanation: the diagonal approximation in the encoder together with the inherent stochasticity force local orthogonality of the decoder. The local behavior of promoting both reconstruction and orthogonality matches closely how the PCA embedding is chosen. Alongside providing an intuitive understanding, we justify the statement with full theoretical analysis as well as with experiments.



### Discriminant Patch Representation for RGB-D Face Recognition Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.06829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06829v1)
- **Published**: 2018-12-17 15:19:21+00:00
- **Updated**: 2018-12-17 15:19:21+00:00
- **Authors**: Nesrine Grati, Achraf Ben-Hamadou, Mohamed Hammami
- **Comment**: Preprint accepted for publication in Proceedings of the 14th
  International Joint Conference on Computer Vision, Imaging and Computer
  Graphics Theory and Applications - 2019
- **Journal**: None
- **Summary**: This paper focuses on designing data-driven models to learn a discriminant representation space for face recognition using RGB-D data. Unlike hand-crafted representations, learned models can extract and organize the discriminant information from the data, and can automatically adapt to build new compute vision applications faster. We proposed an effective way to train Convolutional Neural Networks to learn face patch discriminant features. The proposed solution was tested and validated on state-of-the-art RGB-D datasets and showed competitive and promising results relatively to standard hand-crafted feature extractors.



### Convolutional herbal prescription building method from multi-scale facial features
- **Arxiv ID**: http://arxiv.org/abs/1812.06847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.06847v1)
- **Published**: 2018-12-17 15:47:00+00:00
- **Updated**: 2018-12-17 15:47:00+00:00
- **Authors**: Huiqiang Liao, Guihua Wen, Yang Hu, Changjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In Traditional Chinese Medicine (TCM), facial features are important basis for diagnosis and treatment. A doctor of TCM can prescribe according to a patient's physical indicators such as face, tongue, voice, symptoms, pulse. Previous works analyze and generate prescription according to symptoms. However, research work to mine the association between facial features and prescriptions has not been found for the time being. In this work, we try to use deep learning methods to mine the relationship between the patient's face and herbal prescriptions (TCM prescriptions), and propose to construct convolutional neural networks that generate TCM prescriptions according to the patient's face image. It is a novel and challenging job. In order to mine features from different granularities of faces, we design a multi-scale convolutional neural network based on three-grained face, which mines the patient's face information from the organs, local regions, and the entire face. Our experiments show that convolutional neural networks can learn relevant information from face to prescribe, and the multi-scale convolutional neural networks based on three-grained face perform better.



### Winter Road Surface Condition Recognition Using A Pretrained Deep Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1812.06858v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.06858v1)
- **Published**: 2018-12-17 15:55:18+00:00
- **Updated**: 2018-12-17 15:55:18+00:00
- **Authors**: Guangyuan Pan, Liping Fu, Ruifan Yu, Matthew Muresan
- **Comment**: None
- **Journal**: Transportation Research Board 97th Annual Meeting, 2018
- **Summary**: This paper investigates the application of the latest machine learning technique deep neural networks for classifying road surface conditions (RSC) based on images from smartphones. Traditional machine learning techniques such as support vector machine (SVM) and random forests (RF) have been attempted in literature; however, their classification performance has been less than desirable due to challenges associated with image noises caused by sunlight glare and residual salts. A deep learning model based on convolutional neural network (CNN) is proposed and evaluated for its potential to address these challenges for improved classification accuracy. In the proposed approach we introduce the idea of applying an existing CNN model that has been pre-trained using millions of images with proven high recognition accuracy. The model is extended with two additional fully-connected layers of neurons for learning the specific features of the RSC images. The whole model is then trained with a low learning rate for fine-tuning by using a small set of RSC images. Results show that the proposed model has the highest classification performance in comparison to the traditional machine learning techniques. The testing accuracy with different training dataset sizes is also analyzed, showing the potential of achieving much higher accuracy with a larger training dataset.



### Taking a Deeper Look at the Inverse Compositional Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1812.06861v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.06861v2)
- **Published**: 2018-12-17 15:58:10+00:00
- **Updated**: 2019-04-08 04:53:10+00:00
- **Authors**: Zhaoyang Lv, Frank Dellaert, James M. Rehg, Andreas Geiger
- **Comment**: Paper accepted at CVPR 2019, oral presentation. Code is available at
  https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm
- **Journal**: None
- **Summary**: In this paper, we provide a modern synthesis of the classic inverse compositional algorithm for dense image alignment. We first discuss the assumptions made by this well-established technique, and subsequently propose to relax these assumptions by incorporating data-driven priors into this model. More specifically, we unroll a robust version of the inverse compositional algorithm and replace multiple components of this algorithm using more expressive models whose parameters we train in an end-to-end fashion from data. Our experiments on several challenging 3D rigid motion estimation tasks demonstrate the advantages of combining optimization with learning-based techniques, outperforming the classic inverse compositional algorithm as well as data-driven image-to-pose regression approaches.



### BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity
- **Arxiv ID**: http://arxiv.org/abs/1812.06869v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06869v1)
- **Published**: 2018-12-17 16:13:42+00:00
- **Updated**: 2018-12-17 16:13:42+00:00
- **Authors**: Alexey A. Gritsenko, Alex D'Amour, James Atwood, Yoni Halpern, D. Sculley
- **Comment**: 6 pages, 5 figures, NeurIPS Workshop on Ethical, Social and
  Governance Issues in AI
- **Journal**: None
- **Summary**: We introduce the BriarPatch, a pixel-space intervention that obscures sensitive attributes from representations encoded in pre-trained classifiers. The patches encourage internal model representations not to encode sensitive information, which has the effect of pushing downstream predictors towards exhibiting demographic parity with respect to the sensitive information. The net result is that these BriarPatches provide an intervention mechanism available at user level, and complements prior research on fair representations that were previously only applicable by model developers and ML experts.



### TOP-GAN: Label-Free Cancer Cell Classification Using Deep Learning with a Small Training Set
- **Arxiv ID**: http://arxiv.org/abs/1812.11006v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.11006v1)
- **Published**: 2018-12-17 17:02:58+00:00
- **Updated**: 2018-12-17 17:02:58+00:00
- **Authors**: Moran Rubin, Omer Stein, Nir A. Turko, Yoav Nygate, Darina Roitshtain, Lidor Karako, Itay Barnea, Raja Giryes, Natan T. Shaked
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new deep learning approach for medical imaging that copes with the problem of a small training set, the main bottleneck of deep learning, and apply it for classification of healthy and cancer cells acquired by quantitative phase imaging. The proposed method, called transferring of pre-trained generative adversarial network (TOP-GAN), is a hybridization between transfer learning and generative adversarial networks (GANs). Healthy cells and cancer cells of different metastatic potential have been imaged by low-coherence off-axis holography. After the acquisition, the optical path delay maps of the cells have been extracted and directly used as an input to the deep networks. In order to cope with the small number of classified images, we have used GANs to train a large number of unclassified images from another cell type (sperm cells). After this preliminary training, and after transforming the last layer of the network with new ones, we have designed an automatic classifier for the correct cell type (healthy/primary cancer/metastatic cancer) with 90-99% accuracy, although small training sets of down to several images have been used. These results are better in comparison to other classic methods that aim at coping with the same problem of a small training set. We believe that our approach makes the combination of holographic microscopy and deep learning networks more accessible to the medical field by enabling a rapid, automatic and accurate classification in stain-free imaging flow cytometry. Furthermore, our approach is expected to be applicable to many other medical image classification tasks, suffering from a small training set.



### Fast Learning-based Registration of Sparse 3D Clinical Images
- **Arxiv ID**: http://arxiv.org/abs/1812.06932v3
- **DOI**: 10.1145/3368555.3384462
- **Categories**: **cs.CV**, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.06932v3)
- **Published**: 2018-12-17 18:14:24+00:00
- **Updated**: 2020-04-06 13:45:39+00:00
- **Authors**: Kathleen M. Lewis, Natalia S. Rost, John Guttag, Adrian V. Dalca
- **Comment**: This version was accepted to CHIL. It builds on the previous version
  of the paper and includes more experimental results
- **Journal**: None
- **Summary**: We introduce SparseVM, a method that registers clinical-quality 3D MR scans both faster and more accurately than previously possible. Deformable alignment, or registration, of clinical scans is a fundamental task for many clinical neuroscience studies. However, most registration algorithms are designed for high-resolution research-quality scans. In contrast to research-quality scans, clinical scans are often sparse, missing up to 86% of the slices available in research-quality scans. Existing methods for registering these sparse images are either inaccurate or extremely slow. We present a learning-based registration method, SparseVM, that is more accurate and orders of magnitude faster than the most accurate clinical registration methods. To our knowledge, it is the first method to use deep learning specifically tailored to registering clinical images. We demonstrate our method on a clinically-acquired MRI dataset of stroke patients and on a simulated sparse MRI dataset. Our code is available as part of the VoxelMorph package at http://voxelmorph.mit.edu/.



### Three-Dimensional Dose Prediction for Lung IMRT Patients with Deep Neural Networks: Robust Learning from Heterogeneous Beam Configurations
- **Arxiv ID**: http://arxiv.org/abs/1812.06934v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.06934v2)
- **Published**: 2018-12-17 18:17:12+00:00
- **Updated**: 2019-04-11 16:57:31+00:00
- **Authors**: Ana M. Barragan-Montero, Dan Nguyen, Weiguo Lu, Mu-Han Lin, Xavier Geets, Edmond Sterpin, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: The use of neural networks to directly predict three-dimensional dose distributions for automatic planning is becoming popular. However, the existing methods only use patient anatomy as input and assume consistent beam configuration for all patients in the training database. The purpose of this work is to develop a more general model that, in addition to patient anatomy, also considers variable beam configurations, to achieve a more comprehensive automatic planning with a potentially easier clinical implementation, without the need of training specific models for different beam settings.



### 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/1812.07003v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07003v3)
- **Published**: 2018-12-17 19:04:31+00:00
- **Updated**: 2019-04-29 14:18:55+00:00
- **Authors**: Ji Hou, Angela Dai, Matthias Nießner
- **Comment**: video: https://youtu.be/IH9rNLD1-JE
- **Journal**: None
- **Summary**: We introduce 3D-SIS, a novel neural network architecture for 3D semantic instance segmentation in commodity RGB-D scans. The core idea of our method is to jointly learn from both geometric and color signal, thus enabling accurate instance predictions. Rather than operate solely on 2D frames, we observe that most computer vision applications have multi-view RGB-D input available, which we leverage to construct an approach for 3D instance segmentation that effectively fuses together these multi-modal inputs. Our network leverages high-resolution RGB input by associating 2D images with the volumetric grid based on the pose alignment of the 3D reconstruction. For each image, we first extract 2D features for each pixel with a series of 2D convolutions; we then backproject the resulting feature vector to the associated voxel in the 3D grid. This combination of 2D and 3D feature learning allows significantly higher accuracy object detection and instance segmentation than state-of-the-art alternatives. We show results on both synthetic and real-world public benchmarks, achieving an improvement in mAP of over 13 on real-world data.



### Multi Instance Learning For Unbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/1812.07010v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.07010v1)
- **Published**: 2018-12-17 19:28:30+00:00
- **Updated**: 2018-12-17 19:28:30+00:00
- **Authors**: Mark Kozdoba, Edward Moroshko, Lior Shani, Takuya Takagi, Takashi Katoh, Shie Mannor, Koby Crammer
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of Multi Instance Learning, we analyze the Single Instance (SI) learning objective. We show that when the data is unbalanced and the family of classifiers is sufficiently rich, the SI method is a useful learning algorithm. In particular, we show that larger data imbalance, a quality that is typically perceived as negative, in fact implies a better resilience of the algorithm to the statistical dependencies of the objects in bags. In addition, our results shed new light on some known issues with the SI method in the setting of linear classifiers, and we show that these issues are significantly less likely to occur in the setting of neural networks. We demonstrate our results on a synthetic dataset, and on the COCO dataset for the problem of patch classification with weak image level labels derived from captions.



### From FiLM to Video: Multi-turn Question Answering with Multi-modal Context
- **Arxiv ID**: http://arxiv.org/abs/1812.07023v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.07023v1)
- **Published**: 2018-12-17 19:41:37+00:00
- **Updated**: 2018-12-17 19:41:37+00:00
- **Authors**: Dat Tien Nguyen, Shikhar Sharma, Hannes Schulz, Layla El Asri
- **Comment**: Accepted for an Oral presentation at the DSTC7 workshop at AAAI 2019
- **Journal**: None
- **Summary**: Understanding audio-visual content and the ability to have an informative conversation about it have both been challenging areas for intelligent systems. The Audio Visual Scene-aware Dialog (AVSD) challenge, organized as a track of the Dialog System Technology Challenge 7 (DSTC7), proposes a combined task, where a system has to answer questions pertaining to a video given a dialogue with previous question-answer pairs and the video itself. We propose for this task a hierarchical encoder-decoder model which computes a multi-modal embedding of the dialogue context. It first embeds the dialogue history using two LSTMs. We extract video and audio frames at regular intervals and compute semantic features using pre-trained I3D and VGGish models, respectively. Before summarizing both modalities into fixed-length vectors using LSTMs, we use FiLM blocks to condition them on the embeddings of the current question, which allows us to reduce the dimensionality considerably. Finally, we use an LSTM decoder that we train with scheduled sampling and evaluate using beam search. Compared to the modality-fusing baseline model released by the AVSD challenge organizers, our model achieves a relative improvements of more than 16%, scoring 0.36 BLEU-4 and more than 33%, scoring 0.997 CIDEr.



### Fuzzy Controller of Reward of Reinforcement Learning For Handwritten Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.07028v1
- **DOI**: 10.13140/RG.2.2.20222.79688
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.07028v1)
- **Published**: 2018-12-17 19:57:43+00:00
- **Updated**: 2018-12-17 19:57:43+00:00
- **Authors**: Saber Malekzadeh
- **Comment**: Submitted to the 3rd International Conference on applied research in
  Computer Science and Information Technology. in Farsi
- **Journal**: None
- **Summary**: Recognition of human environment with computer systems always was a big deal in artificial intelligence. In this area handwriting recognition and conceptualization of it to computer is an important area in it. In the past years with growth of machine learning in artificial intelligence, efforts to using this technique increased. In this paper is tried to using fuzzy controller, to optimizing amount of reward of reinforcement learning for recognition of handwritten digits. For this aim first a sample of every digit with 10 standard computer fonts, given to actor and then actor is trained. In the next level is tried to test the actor with dataset and then results show improvement of recognition when using fuzzy controller of reinforcement learning.



### Boundary loss for highly unbalanced segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.07032v4
- **DOI**: 10.1016/j.media.2020.101851
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.07032v4)
- **Published**: 2018-12-17 20:06:50+00:00
- **Updated**: 2020-10-17 13:38:05+00:00
- **Authors**: Hoel Kervadec, Jihene Bouchtiba, Christian Desrosiers, Eric Granger, Jose Dolz, Ismail Ben Ayed
- **Comment**: Runner-up for best paper award at MIDL 2019 [PMLR 102:285-296],
  invited for MedIA deep learning special issue (Volume 67, January 2021)
- **Journal**: MIDL 2019, PMLR 102:285-296 -- MedIA Volume 67, January 2021,
  101851
- **Summary**: Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy, are based on integrals over the segmentation regions. Unfortunately, for highly unbalanced segmentations, such regional summations have values that differ by several orders of magnitude across classes, which affects training performance and stability. We propose a boundary loss, which takes the form of a distance metric on the space of contours, not regions. This can mitigate the difficulties of highly unbalanced problems because it uses integrals over the interface between regions instead of unbalanced integrals over the regions. Furthermore, a boundary loss complements regional information. Inspired by graph-based optimization techniques for computing active-contour flows, we express a non-symmetric $L_2$ distance on the space of contours as a regional integral, which avoids completely local differential computations involving contour points. This yields a boundary loss expressed with the regional softmax probability outputs of the network, which can be easily combined with standard regional losses and implemented with any existing deep network architecture for N-D segmentation. We report comprehensive evaluations and comparisons on different unbalanced problems, showing that our boundary loss can yield significant increases in performances while improving training stability. Our code is publicly available: https://github.com/LIVIAETS/surface-loss .



### From Satellite Imagery to Disaster Insights
- **Arxiv ID**: http://arxiv.org/abs/1812.07033v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.07033v1)
- **Published**: 2018-12-17 20:08:23+00:00
- **Updated**: 2018-12-17 20:08:23+00:00
- **Authors**: Jigar Doshi, Saikat Basu, Guan Pang
- **Comment**: NeurIPS 2018 Camera-Ready version; AI for Social Good Workshop
- **Journal**: None
- **Summary**: The use of satellite imagery has become increasingly popular for disaster monitoring and response. After a disaster, it is important to prioritize rescue operations, disaster response and coordinate relief efforts. These have to be carried out in a fast and efficient manner since resources are often limited in disaster-affected areas and it's extremely important to identify the areas of maximum damage. However, most of the existing disaster mapping efforts are manual which is time-consuming and often leads to erroneous results. In order to address these issues, we propose a framework for change detection using Convolutional Neural Networks (CNN) on satellite images which can then be thresholded and clustered together into grids to find areas which have been most severely affected by a disaster. We also present a novel metric called Disaster Impact Index (DII) and use it to quantify the impact of two natural disasters - the Hurricane Harvey flood and the Santa Rosa fire. Our framework achieves a top F1 score of 81.2% on the gridded flood dataset and 83.5% on the gridded fire dataset.



### Probabilistic Attribute Tree in Convolutional Neural Networks for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.07067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.07067v1)
- **Published**: 2018-12-17 21:48:27+00:00
- **Updated**: 2018-12-17 21:48:27+00:00
- **Authors**: Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly, Yan Tong
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we proposed a novel Probabilistic Attribute Tree-CNN (PAT-CNN) to explicitly deal with the large intra-class variations caused by identity-related attributes, e.g., age, race, and gender. Specifically, a novel PAT module with an associated PAT loss was proposed to learn features in a hierarchical tree structure organized according to attributes, where the final features are less affected by the attributes. Then, expression-related features are extracted from leaf nodes. Samples are probabilistically assigned to tree nodes at different levels such that expression-related features can be learned from all samples weighted by probabilities. We further proposed a semi-supervised strategy to learn the PAT-CNN from limited attribute-annotated samples to make the best use of available data. Experimental results on five facial expression datasets have demonstrated that the proposed PAT-CNN outperforms the baseline models by explicitly modeling attributes. More impressively, the PAT-CNN using a single model achieves the best performance for faces in the wild on the SFEW dataset, compared with the state-of-the-art methods using an ensemble of hundreds of CNNs.



### OCTID: Optical Coherence Tomography Image Database
- **Arxiv ID**: http://arxiv.org/abs/1812.07056v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.07056v2)
- **Published**: 2018-12-17 23:26:39+00:00
- **Updated**: 2019-05-27 15:31:35+00:00
- **Authors**: Peyman Gholami, Priyanka Roy, Mohana Kuppuswamy Parthasarathy, Vasudevan Lakshminarayanan
- **Comment**: This paper is linked to an open access Optical Coherence Tomography
  (OCT) image database which is avaiable at:
  https://dataverse.scholarsportal.info/dataverse/OCTID
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is a non-invasive imaging modality which is widely used in clinical ophthalmology. OCT images are capable of visualizing deep retinal layers which is crucial for early diagnosis of retinal diseases. In this paper, we describe a comprehensive open-access database containing more than 500 highresolution images categorized into different pathological conditions. The image classes include Normal (NO), Macular Hole (MH), Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and Diabetic Retinopathy (DR). The images were obtained from a raster scan protocol with a 2mm scan length and 512x1024 pixel resolution. We have also included 25 normal OCT images with their corresponding ground truth delineations which can be used for an accurate evaluation of OCT image segmentation. In addition, we have provided a user-friendly GUI which can be used by clinicians for manual (and semi-automated) segmentation.




# Arxiv Papers in cs.CV on 2018-12-23
### Estimation and Restoration of Compositional Degradation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.09629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09629v1)
- **Published**: 2018-12-23 00:51:10+00:00
- **Updated**: 2018-12-23 00:51:10+00:00
- **Authors**: Kazutaka Uchida, Masayuki Tanaka, Masatoshi Okutomi
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration from a single image degradation type, such as blurring, hazing, random noise, and compression has been investigated for decades. However, image degradations in practice are often a mixture of several types of degradation. Such compositional degradations complicate restoration because they require the differentiation of different degradation types and levels. In this paper, we propose a convolutional neural network (CNN) model for estimating the degradation properties of a given degraded image. Furthermore, we introduce an image restoration CNN model that adopts the estimated degradation properties as its input. Experimental results show that the proposed degradation estimation model can successfully infer the degradation properties of compositionally degraded images. The proposed restoration model can restore degraded images by exploiting the estimated degradation properties and can achieve both blind and nonblind image restorations.



### Chinese Herbal Recognition based on Competitive Attentional Fusion of Multi-hierarchies Pyramid Features
- **Arxiv ID**: http://arxiv.org/abs/1812.09648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09648v1)
- **Published**: 2018-12-23 03:31:08+00:00
- **Updated**: 2018-12-23 03:31:08+00:00
- **Authors**: Yingxue Xu, Guihua Wen, Yang Hu, Mingnan Luo, Dan Dai, Yishan Zhuang
- **Comment**: New Datasets for Chinese Herbs Recognition
- **Journal**: [J]. Pattern Recognition, 2021, 110: 107558
- **Summary**: Convolution neural netwotks (CNNs) are successfully applied in image recognition task. In this study, we explore the approach of automatic herbal recognition with CNNs and build the standard Chinese herbs datasets firstly. According to the characteristics of herbal images, we proposed the competitive attentional fusion pyramid networks to model the features of herbal image, which mdoels the relationship of feature maps from different levels, and re-weights multi-level channels with channel-wise attention mechanism. In this way, we can dynamically adjust the weight of feature maps from various layers, according to the visual characteristics of each herbal image. Moreover, we also introduce the spatial attention to recalibrate the misaligned features caused by sampling in features amalgamation. Extensive experiments are conducted on our proposed datasets and validate the superior performance of our proposed models. The Chinese herbs datasets will be released upon acceptance to facilitate the research of Chinese herbal recognition.



### An Optical Frontend for a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1901.03661v2
- **DOI**: 10.1364/AO.58.003179
- **Categories**: **cs.CV**, cs.ET, cs.LG, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1901.03661v2)
- **Published**: 2018-12-23 06:46:19+00:00
- **Updated**: 2019-01-14 02:16:45+00:00
- **Authors**: Shane Colburn, Yi Chu, Eli Shlizerman, Arka Majumdar
- **Comment**: None
- **Journal**: None
- **Summary**: The parallelism of optics and the miniaturization of optical components using nanophotonic structures, such as metasurfaces present a compelling alternative to electronic implementations of convolutional neural networks. The lack of a low-power optical nonlinearity, however, requires slow and energy-inefficient conversions between the electronic and optical domains. Here, we design an architecture which utilizes a single electrical to optical conversion by designing a free-space optical frontend unit that implements the linear operations of the first layer with the subsequent layers realized electronically. Speed and power analysis of the architecture indicates that the hybrid photonic-electronic architecture outperforms sole electronic architecture for large image sizes and kernels. Benchmarking of the photonic-electronic architecture on a modified version of AlexNet achieves a classification accuracy of 87% on images from the Kaggle Cats and Dogs challenge database.



### Epipolar Geometry based Learning of Multi-view Depth and Ego-Motion from Monocular Sequences
- **Arxiv ID**: http://arxiv.org/abs/1812.11922v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.11922v3)
- **Published**: 2018-12-23 09:26:49+00:00
- **Updated**: 2019-01-07 12:00:24+00:00
- **Authors**: Vignesh Prasad, Dipanjan Das, Brojeshwar Bhowmick
- **Comment**: ICVGIP 2018 Best Paper Award. Extension of our work accepted at WACV
  2019, available at arXiv:1812.08370
- **Journal**: None
- **Summary**: Deep approaches to predict monocular depth and ego-motion have grown in recent years due to their ability to produce dense depth from monocular images. The main idea behind them is to optimize the photometric consistency over image sequences by warping one view into another, similar to direct visual odometry methods. One major drawback is that these methods infer depth from a single view, which might not effectively capture the relation between pixels. Moreover, simply minimizing the photometric loss does not ensure proper pixel correspondences, which is a key factor for accurate depth and pose estimations.   In contrast, we propose a 2-view depth network to infer the scene depth from consecutive frames, thereby learning inter-pixel relationships. To ensure better correspondences, thereby better geometric understanding, we propose incorporating epipolar constraints to make the learning more geometrically sound. We use the Essential matrix obtained using Nist'er's Five Point Algorithm, to enforce meaningful geometric constraints, rather than using it as training labels. This allows us to use lesser no. of trainable parameters compared to state-of-the-art methods. The proposed method results in better depth images and pose estimates, which capture the scene structure and motion in a better way. Such a geometrically constrained learning performs successfully even in cases where simply minimizing the photometric error would fail.



### Learning from past scans: Tomographic reconstruction to detect new structures
- **Arxiv ID**: http://arxiv.org/abs/1812.10998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.10998v1)
- **Published**: 2018-12-23 09:45:15+00:00
- **Updated**: 2018-12-23 09:45:15+00:00
- **Authors**: Preeti Gopal, Sharat Chandran, Imants Svalbe, Ajit Rajwade
- **Comment**: 5 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: The need for tomographic reconstruction from sparse measurements arises when the measurement process is potentially harmful, needs to be rapid, or is uneconomical. In such cases, prior information from previous longitudinal scans of the same or similar objects helps to reconstruct the current object whilst requiring significantly fewer `updating' measurements. However, a significant limitation of all prior-based methods is the possible dominance of the prior over the reconstruction of new localised information that has evolved within the test object. In this paper, we improve the state of the art by (1) detecting potential regions where new changes may have occurred, and (2) effectively reconstructing both the old and new structures by computing regional weights that moderate the local influence of the priors. We have tested the efficacy of our method on synthetic as well as real volume data. The results demonstrate that using weighted priors significantly improves the overall quality of the reconstructed data whilst minimising their impact on regions that contain new information.



### Scene Graph Reasoning with Prior Visual Relationship for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1812.09681v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.09681v2)
- **Published**: 2018-12-23 09:59:49+00:00
- **Updated**: 2019-08-21 16:42:04+00:00
- **Authors**: Zhuoqian Yang, Zengchang Qin, Jing Yu, Yue Hu
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: One of the key issues of Visual Question Answering (VQA) is to reason with semantic clues in the visual content under the guidance of the question, how to model relational semantics still remains as a great challenge. To fully capture visual semantics, we propose to reason over a structured visual representation - scene graph, with embedded objects and inter-object relationships. This shows great benefit over vanilla vector representations and implicit visual relationship learning. Based on existing visual relationship models, we propose a visual relationship encoder that projects visual relationships into a learned deep semantic space constrained by visual context and language priors. Upon the constructed graph, we propose a Scene Graph Convolutional Network (SceneGCN) to jointly reason the object properties and relational semantics for the correct answer. We demonstrate the model's effectiveness and interpretability on the challenging GQA dataset and the classical VQA 2.0 dataset, remarkably achieving state-of-the-art 54.56% accuracy on GQA compared to the existing best model.



### AVRA: Automatic Visual Ratings of Atrophy from MRI images using Recurrent Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.00418v1
- **DOI**: 10.1016/j.nicl.2019.101872
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.00418v1)
- **Published**: 2018-12-23 10:23:50+00:00
- **Updated**: 2018-12-23 10:23:50+00:00
- **Authors**: Gustav MÃ¥rtensson, Daniel Ferreira, Lena Cavallin, J-Sebastian Muehlboeck, Lars-Olof Wahlund, Chunliang Wang, Eric Westman
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Quantifying the degree of atrophy is done clinically by neuroradiologists following established visual rating scales. For these assessments to be reliable the rater requires substantial training and experience, and even then the rating agreement between two radiologists is not perfect. We have developed a model we call AVRA (Automatic Visual Ratings of Atrophy) based on machine learning methods and trained on 2350 visual ratings made by an experienced neuroradiologist. It provides fast and automatic ratings for Scheltens' scale of medial temporal atrophy (MTA), the frontal subscale of Pasquier's Global Cortical Atrophy (GCA-F) scale, and Koedam's scale of Posterior Atrophy (PA). We demonstrate substantial inter-rater agreement between AVRA's and a neuroradiologist ratings with Cohen's weighted kappa values of $\kappa_w$ = 0.74/0.72 (MTA left/right), $\kappa_w$ = 0.62 (GCA-F) and $\kappa_w$ = 0.74 (PA), with an inherent intra-rater agreement of $\kappa_w$ = 1. We conclude that automatic visual ratings of atrophy can potentially have great clinical and scientific value, and aim to present AVRA as a freely available toolbox.



### Image Processing on IOPA Radiographs: A comprehensive case study on Apical Periodontitis
- **Arxiv ID**: http://arxiv.org/abs/1812.09693v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1812.09693v2)
- **Published**: 2018-12-23 11:38:35+00:00
- **Updated**: 2019-03-22 14:02:33+00:00
- **Authors**: Diganta Misra, Vanshika Arora
- **Comment**: 15 pages, 42 figures and Submitted at ICIAP 2019: 21st International
  Conference on Image Analysis and Processing
- **Journal**: None
- **Summary**: With the recent advancements in Image Processing Techniques and development of new robust computer vision algorithms, new areas of research within Medical Diagnosis and Biomedical Engineering are picking up pace. This paper provides a comprehensive in-depth case study of Image Processing, Feature Extraction and Analysis of Apical Periodontitis diagnostic cases in IOPA (Intra Oral Peri-Apical) Radiographs, a common case in oral diagnostic pipeline. This paper provides a detailed analytical approach towards improving the diagnostic procedure with improved and faster results with higher accuracy targeting to eliminate True Negative and False Positive cases.



### Advanced Image Processing for Astronomical Images
- **Arxiv ID**: http://arxiv.org/abs/1812.09702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1812.09702v1)
- **Published**: 2018-12-23 13:18:26+00:00
- **Updated**: 2018-12-23 13:18:26+00:00
- **Authors**: Diganta Misra, Sparsha Mishra, Bhargav Appasani
- **Comment**: 7 pages, 13 figures, accepted at IEEE International Conference on
  Electrical, Communication, Electronics, Instrumentation and Computing
  (ICECEIC)
- **Journal**: None
- **Summary**: Image Processing in Astronomy is a major field of research and involves a lot of techniques pertaining to improve analyzing the properties of the celestial objects or obtaining preliminary inference from the image data. In this paper, we provide a comprehensive case study of advanced image processing techniques applied to Astronomical Galaxy Images for improved analysis, accurate inferences and faster analysis.



### End-to-end Learning for Graph Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1812.09737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09737v1)
- **Published**: 2018-12-23 16:21:08+00:00
- **Updated**: 2018-12-23 16:21:08+00:00
- **Authors**: Jie Song, Bjoern Andres, Michael Black, Otmar Hilliges, Siyu Tang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel end-to-end trainable framework for the graph decomposition problem. The minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels of the initial graph and the hard constraints are introduced in the CRF as high-order potentials. The parameters of a standard Neural Network and the fully differentiable CRF are optimized in an end-to-end manner. Furthermore, our method utilizes the cycle constraints as meta-supervisory signals during the learning of the deep feature representations by taking the dependencies between the output random variables into account. We present analyses of the end-to-end learned representations, showing the impact of the joint training, on the task of clustering images of MNIST. We also validate the effectiveness of our approach both for the feature learning and the final clustering on the challenging task of real-world multi-person pose estimation.



### Leveraging Class Similarity to Improve Deep Neural Network Robustness
- **Arxiv ID**: http://arxiv.org/abs/1812.09744v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.09744v2)
- **Published**: 2018-12-23 17:33:29+00:00
- **Updated**: 2018-12-27 05:40:47+00:00
- **Authors**: Pooran Singh Negi, David chan, Mohammad Mahoor
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally artificial neural networks (ANNs) are trained by minimizing the cross-entropy between a provided groundtruth delta distribution (encoded as one-hot vector) and the ANN's predictive softmax distribution. It seems, however, unacceptable to penalize networks equally for missclassification between classes. Confusing the class "Automobile" with the class "Truck" should be penalized less than confusing the class "Automobile" with the class "Donkey". To avoid such representation issues and learn cleaner classification boundaries in the network, this paper presents a variation of cross-entropy loss which depends not only on the sample class but also on a data-driven prior "class-similarity distribution" across the classes encoded in a matrix form. We explore learning the class-similarity distribution using a datadriven method and then show that by training with our modified similarity-driven loss, we obtain slightly better generalization performance over multiple architectures and datasets as well as improved performance on noisy testing scenarios.



### Deep Learning for Inferring the Surface Solar Irradiance from Sky Imagery
- **Arxiv ID**: http://arxiv.org/abs/1812.09793v1
- **DOI**: 10.1109/IRSEC.2017.8477236
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.09793v1)
- **Published**: 2018-12-23 23:16:54+00:00
- **Updated**: 2018-12-23 23:16:54+00:00
- **Authors**: Mehdi Zakroum, Mounir Ghogho, Mustapha Faqir, Mohamed Aymane Ahajjam
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to perform ground-based estimation and prediction of the surface solar irradiance with the view to predicting photovoltaic energy production. We propose the use of mini-batch k-means clustering to extract features, referred to as per cluster number of pixels (PCNP), from sky images taken by a low-cost fish eye camera. These features are first used to classify the sky as clear or cloudy using a single hidden layer neural network; the classification accuracy achieves 99.7%. If the sky is classified as cloudy, we propose to use a deep neural network having as input features the PCNP to predict intra-hour variability of the solar irradiance. Toward this objective, in this paper, we focus on estimating the deep neural network model relating the PCNP features and the solar irradiance, which is an important step before performing the prediction task. The proposed deep learning-based estimation approach is shown to have an accuracy of 95%.




# Arxiv Papers in cs.CV on 2018-12-12
### An efficient supervised dictionary learning method for audio signal recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.04748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04748v1)
- **Published**: 2018-12-12 00:04:35+00:00
- **Updated**: 2018-12-12 00:04:35+00:00
- **Authors**: Imad Rida, Romain Hérault, Gilles Gasso
- **Comment**: None
- **Journal**: None
- **Summary**: Machine hearing or listening represents an emerging area. Conventional approaches rely on the design of handcrafted features specialized to a specific audio task and that can hardly generalized to other audio fields. For example, Mel-Frequency Cepstral Coefficients (MFCCs) and its variants were successfully applied to computational auditory scene recognition while Chroma vectors are good at music chord recognition. Unfortunately, these predefined features may be of variable discrimination power while extended to other tasks or even within the same task due to different nature of clips. Motivated by this need of a principled framework across domain applications for machine listening, we propose a generic and data-driven representation learning approach. For this sake, a novel and efficient supervised dictionary learning method is presented. The method learns dissimilar dictionaries, one per each class, in order to extract heterogeneous information for classification. In other words, we are seeking to minimize the intra-class homogeneity and maximize class separability. This is made possible by promoting pairwise orthogonality between class specific dictionaries and controlling the sparsity structure of the audio clip's decomposition over these dictionaries. The resulting optimization problem is non-convex and solved using a proximal gradient descent method. Experiments are performed on both computational auditory scene (East Anglia and Rouen) and synthetic music chord recognition datasets. Obtained results show that our method is capable to reach state-of-the-art hand-crafted features for both applications.



### Considering Race a Problem of Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.04751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04751v1)
- **Published**: 2018-12-12 00:30:39+00:00
- **Updated**: 2018-12-12 00:30:39+00:00
- **Authors**: Akbir Khan, Marwa Mahmoud
- **Comment**: Accepted for Oral presentation at DVPBA 2019
- **Journal**: None
- **Summary**: As biometric applications are fielded to serve large population groups, issues of performance differences between individual sub-groups are becoming increasingly important. In this paper we examine cases where we believe race is one such factor. We look in particular at two forms of problem; facial classification and image synthesis. We take the novel approach of considering race as a boundary for transfer learning in both the task (facial classification) and the domain (synthesis over distinct datasets). We demonstrate a series of techniques to improve transfer learning of facial classification; outperforming similar models trained in the target's own domain. We conduct a study to evaluate the performance drop of Generative Adversarial Networks trained to conduct image synthesis, in this process, we produce a new annotation for the Celeb-A dataset by race. These networks are trained solely on one race and tested on another - demonstrating the subsets of the CelebA to be distinct domains for this task.



### Extreme View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1812.04777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04777v2)
- **Published**: 2018-12-12 02:15:27+00:00
- **Updated**: 2019-08-30 01:13:18+00:00
- **Authors**: Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H. Kim, Jan Kautz
- **Comment**: Accepted as an oral presentation at IEEE ICCV 2019
- **Journal**: None
- **Summary**: We present Extreme View Synthesis, a solution for novel view extrapolation that works even when the number of input images is small--as few as two. In this context, occlusions and depth uncertainty are two of the most pressing issues, and worsen as the degree of extrapolation increases. We follow the traditional paradigm of performing depth-based warping and refinement, with a few key improvements. First, we estimate a depth probability volume, rather than just a single depth value for each pixel of the novel view. This allows us to leverage depth uncertainty in challenging regions, such as depth discontinuities. After using it to get an initial estimate of the novel view, we explicitly combine learned image priors and the depth uncertainty to synthesize a refined image with less artifacts. Our method is the first to show visually pleasing results for baseline magnifications of up to 30X.



### Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.04794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04794v1)
- **Published**: 2018-12-12 03:30:41+00:00
- **Updated**: 2018-12-12 03:30:41+00:00
- **Authors**: Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: The task in referring expression comprehension is to localise the object instance in an image described by a referring expression phrased in natural language. As a language-to-vision matching task, the key to this problem is to learn a discriminative object feature that can adapt to the expression used. To avoid ambiguity, the expression normally tends to describe not only the properties of the referent itself, but also its relationships to its neighbourhood. To capture and exploit this important information we propose a graph-based, language-guided attention mechanism. Being composed of node attention component and edge attention component, the proposed graph attention mechanism explicitly represents inter-object relationships, and properties with a flexibility and power impossible with competing approaches. Furthermore, the proposed graph attention mechanism enables the comprehension decision to be visualisable and explainable. Experiments on three referring expression comprehension datasets show the advantage of the proposed approach.



### Strong-Weak Distribution Alignment for Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.04798v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04798v3)
- **Published**: 2018-12-12 04:02:38+00:00
- **Updated**: 2019-04-05 19:26:15+00:00
- **Authors**: Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Kate Saenko
- **Comment**: Accepted to CVPR2019, project page
  http://cs-people.bu.edu/keisaito/research/CVPR2019.html
- **Journal**: None
- **Summary**: We propose an approach for unsupervised adaptation of object detectors from label-rich to label-poor domains which can significantly reduce annotation costs associated with detection. Recently, approaches that align distributions of source and target images using an adversarial loss have been proven effective for adapting object classifiers. However, for object detection, fully matching the entire distributions of source and target images to each other at the global image level may fail, as domains could have distinct scene layouts and different combinations of objects. On the other hand, strong matching of local features such as texture and color makes sense, as it does not change category level semantics. This motivates us to propose a novel method for detector adaptation based on strong local alignment and weak global alignment. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. We empirically verify the effectiveness of our method on four datasets comprising both large and small domain shifts. Our code is available at \url{https://github.com/VisionLearningGroup/DA_Detection}



### Image Segmentation Based on Multiscale Fast Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1812.04816v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.04816v1)
- **Published**: 2018-12-12 05:50:06+00:00
- **Updated**: 2018-12-12 05:50:06+00:00
- **Authors**: Chongyang Zhang, Guofeng Zhu, Minxin Chen, Hong Chen, Chenjian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, spectral clustering has become one of the most popular clustering algorithms for image segmentation. However, it has restricted applicability to large-scale images due to its high computational complexity. In this paper, we first propose a novel algorithm called Fast Spectral Clustering based on quad-tree decomposition. The algorithm focuses on the spectral clustering at superpixel level and its computational complexity is O(nlogn) + O(m) + O(m^(3/2)); its memory cost is O(m), where n and m are the numbers of pixels and the superpixels of a image. Then we propose Multiscale Fast Spectral Clustering by improving Fast Spectral Clustering, which is based on the hierarchical structure of the quad-tree. The computational complexity of Multiscale Fast Spectral Clustering is O(nlogn) and its memory cost is O(m). Extensive experiments on real large-scale images demonstrate that Multiscale Fast Spectral Clustering outperforms Normalized cut in terms of lower computational complexity and memory cost, with comparable clustering accuracy.



### Efficient Super Resolution For Large-Scale Images Using Attentional GAN
- **Arxiv ID**: http://arxiv.org/abs/1812.04821v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04821v4)
- **Published**: 2018-12-12 06:11:32+00:00
- **Updated**: 2019-01-13 07:17:18+00:00
- **Authors**: Harsh Nilesh Pathak, Xinxin Li, Shervin Minaee, Brooke Cowan
- **Comment**: Accepted by IEEE International Conference on Big Data, 2018
- **Journal**: None
- **Summary**: Single Image Super Resolution (SISR) is a well-researched problem with broad commercial relevance. However, most of the SISR literature focuses on small-size images under 500px, whereas business needs can mandate the generation of very high resolution images. At Expedia Group, we were tasked with generating images of at least 2000px for display on the website, four times greater than the sizes typically reported in the literature. This requirement poses a challenge that state-of-the-art models, validated on small images, have not been proven to handle. In this paper, we investigate solutions to the problem of generating high-quality images for large-scale super resolution in a commercial setting. We find that training a generative adversarial network (GAN) with attention from scratch using a large-scale lodging image data set generates images with high PSNR and SSIM scores. We describe a novel attentional SISR model for large-scale images, A-SRGAN, that uses a Flexible Self Attention layer to enable processing of large-scale images. We also describe a distributed algorithm which speeds up training by around a factor of five.



### Iris-GAN: Learning to Generate Realistic Iris Images Using Convolutional GAN
- **Arxiv ID**: http://arxiv.org/abs/1812.04822v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04822v3)
- **Published**: 2018-12-12 06:11:45+00:00
- **Updated**: 2018-12-25 20:44:21+00:00
- **Authors**: Shervin Minaee, Amirali Abdolrashidi
- **Comment**: None
- **Journal**: None
- **Summary**: Generating iris images which look realistic is both an interesting and challenging problem. Most of the classical statistical models are not powerful enough to capture the complicated texture representation in iris images, and therefore fail to generate iris images which look realistic. In this work, we present a machine learning framework based on generative adversarial network (GAN), which is able to generate iris images sampled from a prior distribution (learned from a set of training images). We apply this framework to two popular iris databases, and generate images which look very realistic, and similar to the image distribution in those databases. Through experimental results, we show that the generated iris images have a good diversity, and are able to capture different part of the prior distribution.



### Spatial-Temporal Digital Image Correlation: A Unified Framework
- **Arxiv ID**: http://arxiv.org/abs/1812.04826v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04826v2)
- **Published**: 2018-12-12 06:45:28+00:00
- **Updated**: 2019-01-20 13:49:36+00:00
- **Authors**: Yuxi Chi, Bing Pan
- **Comment**: 20 pages, 7 figures v2:add figures and revise the contents
- **Journal**: None
- **Summary**: A comprehensive and systematic framework for easily extending and implementing the subset-based spatial-temporal digital image correlation (DIC) algorithm is presented. The framework decouples the three main factors (i.e. shape function, correlation criterion, and optimization algorithm) involved in algorithm implementation of DIC and represents different algorithms in a uniform form. One can freely choose and combine the three factors to meet his own need, or freely add more parameters to extract analytic results. Subpixel translation and a simulated image series with different velocity characters are analyzed using different algorithms based on the proposed framework, confirming the merit of noise suppression and velocity compatibility. An application of mitigating air disturbance due to heat haze using spatial-temporal DIC is given to demonstrate the applicability of the framework.



### Weakly Supervised Instance Segmentation Using Hybrid Network
- **Arxiv ID**: http://arxiv.org/abs/1812.04831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04831v1)
- **Published**: 2018-12-12 07:12:50+00:00
- **Updated**: 2018-12-12 07:12:50+00:00
- **Authors**: Shisha Liao, Yongqing Sun, Chenqiang Gao, Pranav Shenoy K P, Song Mu, Jun Shimamura, Atsushi Sagata
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Weakly-supervised instance segmentation, which could greatly save labor and time cost of pixel mask annotation, has attracted increasing attention in recent years. The commonly used pipeline firstly utilizes conventional image segmentation methods to automatically generate initial masks and then use them to train an off-the-shelf segmentation network in an iterative way. However, the initial generated masks usually contains a notable proportion of invalid masks which are mainly caused by small object instances. Directly using these initial masks to train segmentation model is harmful for the performance. To address this problem, we propose a hybrid network in this paper. In our architecture, there is a principle segmentation network which is used to handle the normal samples with valid generated masks. In addition, a complementary branch is added to handle the small and dim objects without valid masks. Experimental results indicate that our method can achieve significantly performance improvement both on the small object instances and large ones, and outperforms all state-of-the-art methods.



### Robust Point Light Source Estimation Using Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/1812.04857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04857v1)
- **Published**: 2018-12-12 09:05:11+00:00
- **Updated**: 2018-12-12 09:05:11+00:00
- **Authors**: Grégoire Nieto, Salma Jiddi, Philippe Robert
- **Comment**: None
- **Journal**: None
- **Summary**: Illumination estimation is often used in mixed reality to re-render a scene from another point of view, to change the color/texture of an object, or to insert a virtual object consistently lit into a real video or photograph. Specifically, the estimation of a point light source is required for the shadows cast by the inserted object to be consistent with the real scene. We tackle the problem of illumination retrieval given an RGBD image of the scene as an inverse problem: we aim to find the illumination that minimizes the photometric error between the rendered image and the observation. In particular we propose a novel differentiable renderer based on the Blinn-Phong model with cast shadows. We compare our differentiable renderer to state-of-the-art methods and demonstrate its robustness to an incorrect reflectance estimation.



### Attentional Road Safety Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.04860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04860v2)
- **Published**: 2018-12-12 09:11:31+00:00
- **Updated**: 2019-01-26 09:21:29+00:00
- **Authors**: Sonu Gupta, Deepak Srivatsav, A. V. Subramanyam, Ponnurangam Kumaraguru
- **Comment**: 8 pages, 5 figures, 7 tables
- **Journal**: None
- **Summary**: Road safety mapping using satellite images is a cost-effective but a challenging problem for smart city planning. The scarcity of labeled data, misalignment and ambiguity makes it hard for supervised deep networks to learn efficient embeddings in order to classify between safe and dangerous road segments. In this paper, we address the challenges using a region guided attention network. In our model, we extract global features from a base network and augment it with local features obtained using the region guided attention network. In addition, we perform domain adaptation for unlabeled target data. In order to bridge the gap between safe samples and dangerous samples from source and target respectively, we propose a loss function based on within and between class covariance matrices. We conduct experiments on a public dataset of London to show that the algorithm achieves significant results with the classification accuracy of 86.21%. We obtain an increase of 4% accuracy for NYC using domain adaptation network. Besides, we perform a user study and demonstrate that our proposed algorithm achieves 23.12% better accuracy compared to subjective analysis.



### Automatic individual pig detection and tracking in surveillance videos
- **Arxiv ID**: http://arxiv.org/abs/1812.04901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04901v1)
- **Published**: 2018-12-12 11:22:27+00:00
- **Updated**: 2018-12-12 11:22:27+00:00
- **Authors**: Lei Zhang, Helen Gray, Xujiong Ye, Lisa Collins, Nigel Allinson
- **Comment**: 19 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: Individual pig detection and tracking is an important requirement in many video-based pig monitoring applications. However, it still remains a challenging task in complex scenes, due to problems of light fluctuation, similar appearances of pigs, shape deformations and occlusions. To tackle these problems, we propose a robust real time multiple pig detection and tracking method which does not require manual marking or physical identification of the pigs, and works under both daylight and infrared light conditions. Our method couples a CNN-based detector and a correlation filter-based tracker via a novel hierarchical data association algorithm. The detector gains the best accuracy/speed trade-off by using the features derived from multiple layers at different scales in a one-stage prediction network. We define a tag-box for each pig as the tracking target, and the multiple object tracking is conducted in a key-points tracking manner using learned correlation filters. Under challenging conditions, the tracking failures are modelled based on the relations between responses of detector and tracker, and the data association algorithm allows the detection hypotheses to be refined, meanwhile the drifted tracks can be corrected by probing the tracking failures followed by the re-initialization of tracking. As a result, the optimal tracklets can sequentially grow with on-line refined detections, and tracking fragments are correctly integrated into respective tracks while keeping the original identifications. Experiments with a dataset captured from a commercial farm show that our method can robustly detect and track multiple pigs under challenging conditions. The promising performance of the proposed method also demonstrates a feasibility of long-term individual pig tracking in a complex environment and thus promises a commercial potential.



### CFUN: Combining Faster R-CNN and U-net Network for Efficient Whole Heart Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.04914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04914v1)
- **Published**: 2018-12-12 12:22:22+00:00
- **Updated**: 2018-12-12 12:22:22+00:00
- **Authors**: Zhanwei Xu, Ziyi Wu, Jianjiang Feng
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel heart segmentation pipeline Combining Faster R-CNN and U-net Network (CFUN). Due to Faster R-CNN's precise localization ability and U-net's powerful segmentation ability, CFUN needs only one-step detection and segmentation inference to get the whole heart segmentation result, obtaining good results with significantly reduced computational cost. Besides, CFUN adopts a new loss function based on edge information named 3D Edge-loss as an auxiliary loss to accelerate the convergence of training and improve the segmentation results. Extensive experiments on the public dataset show that CFUN exhibits competitive segmentation performance in a sharply reduced inference time. Our source code and the model are publicly available at https://github.com/Wuziyi616/CFUN.



### C3: Concentrated-Comprehensive Convolution and its application to semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.04920v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04920v3)
- **Published**: 2018-12-12 12:48:27+00:00
- **Updated**: 2019-07-28 13:56:06+00:00
- **Authors**: Hyojin Park, Youngjoon Yoo, Geonseok Seo, Dongyoon Han, Sangdoo Yun, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: One of the practical choices for making a lightweight semantic segmentation model is to combine a depth-wise separable convolution with a dilated convolution. However, the simple combination of these two methods results in an over-simplified operation which causes severe performance degradation due to loss of information contained in the feature map. To resolve this problem, we propose a new block called Concentrated-Comprehensive Convolution (C3) which applies the asymmetric convolutions before the depth-wise separable dilated convolution to compensate for the information loss due to dilated convolution. The C3 block consists of a concentration stage and a comprehensive convolution stage. The first stage uses two depth-wise asymmetric convolutions for compressed information from the neighboring pixels to alleviate the information loss. The second stage increases the receptive field by using a depth-wise separable dilated convolution from the feature map of the first stage. We applied the C3 block to various segmentation frameworks (ESPNet, DRN, ERFNet, ENet) for proving the beneficial properties of our proposed method. Experimental results show that the proposed method preserves the original accuracies on Cityscapes dataset while reducing the complexity. Furthermore, we modified ESPNet to achieve about 2% better performance while reducing the number of parameters by half and the number of FLOPs by 35% compared with the original ESPNet. Finally, experiments on ImageNet classification task show that C3 block can successfully replace dilated convolutions.



### Separation of water and fat signal in whole-body gradient echo scans using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1812.04922v2
- **DOI**: 10.1002/mrm.27786
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04922v2)
- **Published**: 2018-12-12 12:51:12+00:00
- **Updated**: 2019-04-30 13:08:22+00:00
- **Authors**: Jonathan Andersson, Håkan Ahlström, Joel Kullberg
- **Comment**: 26 pages, 12 figures
- **Journal**: None
- **Summary**: Purpose: To perform and evaluate water-fat signal separation of whole-body gradient echo scans using convolutional neural networks.   Methods: Whole-body gradient echo scans of 240 subjects, each consisting of 5 bipolar echoes, were used. Reference fat fraction maps were created using a conventional method. Convolutional neural networks, more specifically 2D U-nets, were trained using 5-fold cross-validation with 1 or several echoes as input, using the squared difference between the output and the reference fat fraction maps as the loss function. The outputs of the networks were assessed by the loss function, measured liver fat fractions, and visually. Training was performed using a graphics processing unit (GPU). Inference was performed using the GPU as well as a central processing unit (CPU).   Results: The loss curves indicated convergence, and the final loss of the validation data decreased when using more echoes as input. The liver fat fractions could be estimated using only 1 echo, but results were improved by use of more echoes. Visual assessment found the quality of the outputs of the networks to be similar to the reference even when using only 1 echo, with slight improvements when using more echoes. Training a network took at most 28.6 h. Inference time of a whole-body scan took at most 3.7 s using the GPU and 5.8 min using the CPU.   Conclusion: It is possible to perform water-fat signal separation of whole-body gradient echo scans using convolutional neural networks. Separation was possible using only 1 echo, although using more echoes improved the results.



### Semi-Supervised Learning for Face Sketch Synthesis in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1812.04929v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04929v2)
- **Published**: 2018-12-12 13:13:02+00:00
- **Updated**: 2019-05-05 23:20:08+00:00
- **Authors**: Chaofeng Chen, Wei Liu, Xiao Tan, Kwan-Yee K. Wong
- **Comment**: Correct some syntax errors
- **Journal**: None
- **Summary**: Face sketch synthesis has made great progress in the past few years. Recent methods based on deep neural networks are able to generate high quality sketches from face photos. However, due to the lack of training data (photo-sketch pairs), none of such deep learning based methods can be applied successfully to face photos in the wild. In this paper, we propose a semi-supervised deep learning architecture which extends face sketch synthesis to handle face photos in the wild by exploiting additional face photos in training. Instead of supervising the network with ground truth sketches, we first perform patch matching in feature space between the input photo and photos in a small reference set of photo-sketch pairs. We then compose a pseudo sketch feature representation using the corresponding sketch feature patches to supervise our network. With the proposed approach, we can train our networks using a small reference set of photo-sketch pairs together with a large face photo dataset without ground truth sketches. Experiments show that our method achieve state-of-the-art performance both on public benchmarks and face photos in the wild. Codes are available at https://github.com/chaofengc/Face-Sketch-Wild.



### Tree-structured Kronecker Convolutional Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.04945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04945v2)
- **Published**: 2018-12-12 13:57:50+00:00
- **Updated**: 2018-12-15 02:44:57+00:00
- **Authors**: Tianyi Wu, Sheng Tang, Rui Zhang, Juan Cao, Jintao Li
- **Comment**: Code: https://github.com/wutianyiRosun/TKCN
- **Journal**: None
- **Summary**: Most existing semantic segmentation methods employ atrous convolution to enlarge the receptive field of filters, but neglect partial information. To tackle this issue, we firstly propose a novel Kronecker convolution which adopts Kronecker product to expand the standard convolutional kernel for taking into account the partial feature neglected by atrous convolutions. Therefore, it can capture partial information and enlarge the receptive field of filters simultaneously without introducing extra parameters. Secondly, we propose Tree-structured Feature Aggregation (TFA) module which follows a recursive rule to expand and forms a hierarchical structure. Thus, it can naturally learn representations of multi-scale objects and encode hierarchical contextual information in complex scenes. Finally, we design Tree-structured Kronecker Convolutional Networks (TKCN) which employs Kronecker convolution and TFA module. Extensive experiments on three datasets, PASCAL VOC 2012, PASCAL-Context and Cityscapes, verify the effectiveness of our proposed approach. We make the code and the trained model publicly available at https://github.com/wutianyiRosun/TKCN.



### Subjective Annotations for Vision-Based Attention Level Estimation
- **Arxiv ID**: http://arxiv.org/abs/1812.04949v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.04949v2)
- **Published**: 2018-12-12 14:00:46+00:00
- **Updated**: 2019-01-24 11:43:12+00:00
- **Authors**: Andrea Coifman, Péter Rohoska, Miklas S. Kristoffersen, Sven E. Shepstone, Zheng-Hua Tan
- **Comment**: 14th International Conference on Computer Vision Theory and
  Applications
- **Journal**: None
- **Summary**: Attention level estimation systems have a high potential in many use cases, such as human-robot interaction, driver modeling and smart home systems, since being able to measure a person's attention level opens the possibility to natural interaction between humans and computers. The topic of estimating a human's visual focus of attention has been actively addressed recently in the field of HCI. However, most of these previous works do not consider attention as a subjective, cognitive attentive state. New research within the field also faces the problem of the lack of annotated datasets regarding attention level in a certain context. The novelty of our work is two-fold: First, we introduce a new annotation framework that tackles the subjective nature of attention level and use it to annotate more than 100,000 images with three attention levels and second, we introduce a novel method to estimate attention levels, relying purely on extracted geometric features from RGB and depth images, and evaluate it with a deep learning fusion framework. The system achieves an overall accuracy of 80.02%. Our framework and attention level annotations are made publicly available.



### Towards Ophthalmologist Level Accurate Deep Learning System for OCT Screening and Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1812.07105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1812.07105v1)
- **Published**: 2018-12-12 14:42:10+00:00
- **Updated**: 2018-12-12 14:42:10+00:00
- **Authors**: Mrinal Haloi
- **Comment**: JAMA submission
- **Journal**: None
- **Summary**: In this work, we propose an advanced AI based grading system for OCT images. The proposed system is a very deep fully convolutional attentive classification network trained with end to end advanced transfer learning with online random augmentation. It uses quasi random augmentation that outputs confidence values for diseases prevalence during inference. Its a fully automated retinal OCT analysis AI system capable of pathological lesions understanding without any offline preprocessing/postprocessing step or manual feature extraction. We present a state of the art performance on the publicly available Mendeley OCT dataset.



### Real-Time Anomaly Detection With HMOF Feature
- **Arxiv ID**: http://arxiv.org/abs/1812.04980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.04980v1)
- **Published**: 2018-12-12 14:56:14+00:00
- **Updated**: 2018-12-12 14:56:14+00:00
- **Authors**: Huihui Zhu, Bin Liu, Guojun Yin, Yan Lu, Weihai Li, Nenghai Yu
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Anomaly detection is a challenging problem in intelligent video surveillance. Most existing methods are computation consuming, which cannot satisfy the real-time requirement. In this paper, we propose a real-time anomaly detection framework with low computational complexity and high efficiency. A new feature, named Histogram of Magnitude Optical Flow (HMOF), is proposed to capture the motion of video patches. Compared with existing feature descriptors, HMOF is more sensitive to motion magnitude and more efficient to distinguish anomaly information. The HMOF features are computed for foreground patches, and are reconstructed by the auto-encoder for better clustering. Then, we use Gaussian Mixture Model (GMM) Classifiers to distinguish anomalies from normal activities in videos. Experimental results show that our framework outperforms state-of-the-art methods, and can reliably detect anomalies in real-time.



### Long-Term Feature Banks for Detailed Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1812.05038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05038v2)
- **Published**: 2018-12-12 17:13:55+00:00
- **Updated**: 2019-04-17 19:05:30+00:00
- **Authors**: Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krähenbühl, Ross Girshick
- **Comment**: Code and models are available at
  https://github.com/facebookresearch/video-long-term-feature-banks
- **Journal**: None
- **Summary**: To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank---supportive information extracted over the entire span of a video---to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades.



### Learning Semantic Segmentation from Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach
- **Arxiv ID**: http://arxiv.org/abs/1812.05040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05040v2)
- **Published**: 2018-12-12 17:23:24+00:00
- **Updated**: 2019-01-13 23:49:13+00:00
- **Authors**: Yuhua Chen, Wen Li, Xiaoran Chen, Luc Van Gool
- **Comment**: v2: fixed some typos
- **Journal**: None
- **Summary**: Recently, increasing attention has been drawn to training semantic segmentation models using synthetic data and computer-generated annotation. However, domain gap remains a major barrier and prevents models learned from synthetic data from generalizing well to real-world applications. In this work, we take the advantage of additional geometric information from synthetic data, a powerful yet largely neglected cue, to bridge the domain gap. Such geometric information can be generated easily from synthetic data, and is proven to be closely coupled with semantic information. With the geometric information, we propose a model to reduce domain shift on two levels: on the input level, we augment the traditional image translation network with the additional geometric information to translate synthetic images into realistic styles; on the output level, we build a task network which simultaneously performs depth estimation and semantic segmentation on the synthetic data. Meanwhile, we encourage the network to preserve correlation between depth and semantics by adversarial training on the output space. We then validate our method on two pairs of synthetic to real dataset: Virtual KITTI to KITTI, and SYNTHIA to Cityscapes, where we achieve a significant performance gain compared to the non-adapt baseline and methods using only semantic label. This demonstrates the usefulness of geometric information from synthetic data for cross-domain semantic segmentation.



### Fast Online Object Tracking and Segmentation: A Unifying Approach
- **Arxiv ID**: http://arxiv.org/abs/1812.05050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05050v2)
- **Published**: 2018-12-12 17:43:04+00:00
- **Updated**: 2019-05-05 03:49:18+00:00
- **Authors**: Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H. S. Torr
- **Comment**: CVPR 2019 camera ready. Code available at
  https://github.com/foolwood/SiamMask
- **Journal**: None
- **Summary**: In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/~qwang/SiamMask.



### Recent Advances in Autoencoder-Based Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.05069v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.05069v1)
- **Published**: 2018-12-12 18:13:41+00:00
- **Updated**: 2018-12-12 18:13:41+00:00
- **Authors**: Michael Tschannen, Olivier Bachem, Mario Lucic
- **Comment**: Presented at the third workshop on Bayesian Deep Learning (NeurIPS
  2018)
- **Journal**: None
- **Summary**: Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.



### Features Extraction Based on an Origami Representation of 3D Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1812.05082v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.05082v1)
- **Published**: 2018-12-12 18:37:46+00:00
- **Updated**: 2018-12-12 18:37:46+00:00
- **Authors**: Juan Manuel Fernandez Montenegro, Mahdi Maktab Dar Oghaz, Athanasios Gkelias, Georgios Tzimiropoulos, Vasileios Argyriou
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extraction analysis has been widely investigated during the last decades in computer vision community due to the large range of possible applications. Significant work has been done in order to improve the performance of the emotion detection methods. Classification algorithms have been refined, novel preprocessing techniques have been applied and novel representations from images and videos have been introduced. In this paper, we propose a preprocessing method and a novel facial landmarks' representation aiming to improve the facial emotion detection accuracy. We apply our novel methodology on the extended Cohn-Kanade (CK+) dataset and other datasets for affect classification based on Action Units (AU). The performance evaluation demonstrates an improvement on facial emotion classification (accuracy and F1 score) that indicates the superiority of the proposed methodology.



### Adversarial Learning of Semantic Relevance in Text to Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1812.05083v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.05083v2)
- **Published**: 2018-12-12 18:44:23+00:00
- **Updated**: 2019-02-05 21:33:04+00:00
- **Authors**: Miriam Cha, Youngjune L. Gwon, H. T. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a new approach that improves the training of generative adversarial nets (GANs) for synthesizing diverse images from a text input. Our approach is based on the conditional version of GANs and expands on previous work leveraging an auxiliary task in the discriminator. Our generated images are not limited to certain classes and do not suffer from mode collapse while semantically matching the text input. A key to our training methods is how to form positive and negative training examples with respect to the class label of a given image. Instead of selecting random training examples, we perform negative sampling based on the semantic distance from a positive example in the class. We evaluate our approach using the Oxford-102 flower dataset, adopting the inception score and multi-scale structural similarity index (MS-SSIM) metrics to assess discriminability and diversity of the generated images. The empirical results indicate greater diversity in the generated images, especially when we gradually select more negative training examples closer to a positive example in the semantic space.



### DeepTract: A Probabilistic Deep Learning Framework for White Matter Fiber Tractography
- **Arxiv ID**: http://arxiv.org/abs/1812.05129v3
- **DOI**: 10.1007/978-3-030-32248-9_70
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1812.05129v3)
- **Published**: 2018-12-12 19:19:07+00:00
- **Updated**: 2019-10-17 08:35:53+00:00
- **Authors**: Itay Benou, Tammy Riklin-Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We present DeepTract, a deep-learning framework for estimating white matter fibers orientation and streamline tractography. We adopt a data-driven approach for fiber reconstruction from diffusion weighted images (DWI), which does not assume a specific diffusion model. We use a recurrent neural network for mapping sequences of DWI values into probabilistic fiber orientation distributions. Based on these estimations, our model facilitates both deterministic and probabilistic streamline tractography. We quantitatively evaluate our method using the Tractometer tool, demonstrating competitive performance with state-of-the art classical and machine learning based tractography algorithms. We further present qualitative results of bundle-specific probabilistic tractography obtained using our method. The code is publicly available at: https://github.com/itaybenou/DeepTract.git.



### Synthesis of High-Quality Visible Faces from Polarimetric Thermal Faces using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1812.05155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05155v1)
- **Published**: 2018-12-12 21:04:32+00:00
- **Updated**: 2018-12-12 21:04:32+00:00
- **Authors**: He Zhang, Benjamin S. Riggan, Shuowen Hu, Nathaniel J. Short, Vishal M. Patel
- **Comment**: Note that the extended dataset is available upon request. Researchers
  can contact Dr. Sean Hu from ARL at shuowen.hu.civ@mail.mil to obtain the
  dataset
- **Journal**: None
- **Summary**: The large domain discrepancy between faces captured in polarimetric (or conventional) thermal and visible domain makes cross-domain face verification a highly challenging problem for human examiners as well as computer vision algorithms. Previous approaches utilize either a two-step procedure (visible feature estimation and visible image reconstruction) or an input-level fusion technique, where different Stokes images are concatenated and used as a multi-channel input to synthesize the visible image given the corresponding polarimetric signatures. Although these methods have yielded improvements, we argue that input-level fusion alone may not be sufficient to realize the full potential of the available Stokes images. We propose a Generative Adversarial Networks (GAN) based multi-stream feature-level fusion technique to synthesize high-quality visible images from prolarimetric thermal images. The proposed network consists of a generator sub-network, constructed using an encoder-decoder network based on dense residual blocks, and a multi-scale discriminator sub-network. The generator network is trained by optimizing an adversarial loss in addition to a perceptual loss and an identity preserving loss to enable photo realistic generation of visible images while preserving discriminative characteristics. An extended dataset consisting of polarimetric thermal facial signatures of 111 subjects is also introduced. Multiple experiments evaluated on different experimental protocols demonstrate that the proposed method achieves state-of-the-art performance. Code will be made available at https://github.com/hezhangsprinter.



### E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1812.07106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1812.07106v1)
- **Published**: 2018-12-12 22:22:16+00:00
- **Updated**: 2018-12-12 22:22:16+00:00
- **Authors**: Zhe Li, Caiwen Ding, Siyue Wang, Wujie Wen, Youwei Zhuo, Chang Liu, Qinru Qiu, Wenyao Xu, Xue Lin, Xuehai Qian, Yanzhi Wang
- **Comment**: In The 25th International Symposium on High-Performance Computer
  Architecture (HPCA 2019)
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related applications which require efficient and real-time implementations. The two major types are Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. It is a challenging task to have real-time, efficient, and accurate hardware RNN implementations because of the high sensitivity to imprecision accumulation and the requirement of special activation function implementations.   A key limitation of the prior works is the lack of a systematic design optimization framework of RNN model and hardware implementations, especially when the block size (or compression ratio) should be jointly optimized with RNN type, layer size, etc. In this paper, we adopt the block-circulant matrix-based framework, and present the Efficient RNN (E-RNN) framework for FPGA implementations of the Automatic Speech Recognition (ASR) application. The overall goal is to improve performance/energy efficiency under accuracy requirement. We use the alternating direction method of multipliers (ADMM) technique for more accurate block-circulant training, and present two design explorations providing guidance on block size and reducing RNN training trials. Based on the two observations, we decompose E-RNN in two phases: Phase I on determining RNN model to reduce computation and storage subject to accuracy requirement, and Phase II on hardware implementations given RNN model, including processing element design/optimization, quantization, activation implementation, etc. Experimental results on actual FPGA deployments show that E-RNN achieves a maximum energy efficiency improvement of 37.4$\times$ compared with ESE, and more than 2$\times$ compared with C-LSTM, under the same accuracy.



### Power of Tempospatially Unified Spectral Density for Perceptual Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1812.05177v1
- **DOI**: 10.1109/ICME.2017.8019333
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1812.05177v1)
- **Published**: 2018-12-12 22:22:21+00:00
- **Updated**: 2018-12-12 22:22:21+00:00
- **Authors**: Mohammed A. Aabed, Gukyeong Kwon, Ghassan AlRegib
- **Comment**: 6 pages, 4 figures, 3 tables
- **Journal**: M. A. Aabed, G. Kwon, and G. AlRegib, "Power of Tempospatially
  Unified Spectral Density for Perceptual Video Quality Assessment," 2017 IEEE
  International Conference on Multimedia and Expo (ICME), Hong Kong, 2017, pp.
  1476-1481
- **Summary**: We propose a perceptual video quality assessment (PVQA) metric for distorted videos by analyzing the power spectral density (PSD) of a group of pictures. This is an estimation approach that relies on the changes in video dynamic calculated in the frequency domain and are primarily caused by distortion. We obtain a feature map by processing a 3D PSD tensor obtained from a set of distorted frames. This is a full-reference tempospatial approach that considers both temporal and spatial PSD characteristics. This makes it ubiquitously suitable for videos with varying motion patterns and spatial contents. Our technique does not make any assumptions on the coding conditions, streaming conditions or distortion. This approach is also computationally inexpensive which makes it feasible for real-time and practical implementations. We validate our proposed metric by testing it on a variety of distorted sequences from PVQA databases. The results show that our metric estimates the perceptual quality at the sequence level accurately. We report the correlation coefficients with the differential mean opinion scores (DMOS) reported in the databases. The results show high and competitive correlations compared with the state of the art techniques.




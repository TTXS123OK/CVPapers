# Arxiv Papers in cs.CV on 2018-02-23
### Real-Time End-to-End Action Detection with Two-Stream Networks
- **Arxiv ID**: http://arxiv.org/abs/1802.08362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08362v1)
- **Published**: 2018-02-23 02:21:46+00:00
- **Updated**: 2018-02-23 02:21:46+00:00
- **Authors**: Alaaeldin El-Nouby, Graham W. Taylor
- **Comment**: None
- **Journal**: None
- **Summary**: Two-stream networks have been very successful for solving the problem of action detection. However, prior work using two-stream networks train both streams separately, which prevents the network from exploiting regularities between the two streams. Moreover, unlike the visual stream, the dominant forms of optical flow computation typically do not maximally exploit GPU parallelism. We present a real-time end-to-end trainable two-stream network for action detection. First, we integrate the optical flow computation in our framework by using Flownet2. Second, we apply early fusion for the two streams and train the whole pipeline jointly end-to-end. Finally, for better network initialization, we transfer from the task of action recognition to action detection by pre-training our framework using the recently released large-scale Kinetics dataset. Our experimental results show that training the pipeline jointly end-to-end with fine-tuning the optical flow for the objective of action detection improves detection performance significantly. Additionally, we observe an improvement when initializing with parameters pre-trained using Kinetics. Last, we show that by integrating the optical flow computation, our framework is more efficient, running at real-time speeds (up to 31 fps).



### Missing Data Reconstruction in Remote Sensing image with a Unified Spatial-Temporal-Spectral Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1802.08369v1
- **DOI**: 10.1109/TGRS.2018.2810208
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08369v1)
- **Published**: 2018-02-23 02:42:13+00:00
- **Updated**: 2018-02-23 02:42:13+00:00
- **Authors**: Qiang Zhang, Qiangqiang Yuan, Chao Zeng, Xinghua Li, Yancong Wei
- **Comment**: To be published in IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: Because of the internal malfunction of satellite sensors and poor atmospheric conditions such as thick cloud, the acquired remote sensing data often suffer from missing information, i.e., the data usability is greatly reduced. In this paper, a novel method of missing information reconstruction in remote sensing images is proposed. The unified spatial-temporal-spectral framework based on a deep convolutional neural network (STS-CNN) employs a unified deep convolutional neural network combined with spatial-temporal-spectral supplementary information. In addition, to address the fact that most methods can only deal with a single missing information reconstruction task, the proposed approach can solve three typical missing information reconstruction tasks: 1) dead lines in Aqua MODIS band 6; 2) the Landsat ETM+ Scan Line Corrector (SLC)-off problem; and 3) thick cloud removal. It should be noted that the proposed model can use multi-source data (spatial, spectral, and temporal) as the input of the unified framework. The results of both simulated and real-data experiments demonstrate that the proposed model exhibits high effectiveness in the three missing information reconstruction tasks listed above.



### Deep Defense: Training DNNs with Improved Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1803.00404v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.00404v3)
- **Published**: 2018-02-23 05:02:59+00:00
- **Updated**: 2018-12-20 01:53:51+00:00
- **Authors**: Ziang Yan, Yiwen Guo, Changshui Zhang
- **Comment**: Accepted by NeurIPS 2018
- **Journal**: None
- **Summary**: Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named "deep defense". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at https://github.com/ZiangYan/deepdefense.pytorch



### Locally Adaptive Learning Loss for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.08290v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.08290v2)
- **Published**: 2018-02-23 05:18:35+00:00
- **Updated**: 2018-04-16 03:17:34+00:00
- **Authors**: Jinjiang Guo, Pengyuan Ren, Aiguo Gu, Jian Xu, Weixin Wu
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: We propose a novel locally adaptive learning estimator for enhancing the inter- and intra- discriminative capabilities of Deep Neural Networks, which can be used as improved loss layer for semantic image segmentation tasks. Most loss layers compute pixel-wise cost between feature maps and ground truths, ignoring spatial layouts and interactions between neighboring pixels with same object category, and thus networks cannot be effectively sensitive to intra-class connections. Stride by stride, our method firstly conducts adaptive pooling filter operating over predicted feature maps, aiming to merge predicted distributions over a small group of neighboring pixels with same category, and then it computes cost between the merged distribution vector and their category label. Such design can make groups of neighboring predictions from same category involved into estimations on predicting correctness with respect to their category, and hence train networks to be more sensitive to regional connections between adjacent pixels based on their categories. In the experiments on Pascal VOC 2012 segmentation datasets, the consistently improved results show that our proposed approach achieves better segmentation masks against previous counterparts.



### Left ventricle segmentation By modelling uncertainty in prediction of deep convolutional neural networks and adaptive thresholding inference
- **Arxiv ID**: http://arxiv.org/abs/1803.00406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00406v1)
- **Published**: 2018-02-23 06:11:37+00:00
- **Updated**: 2018-02-23 06:11:37+00:00
- **Authors**: Alireza Norouzi, Ali Emami, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Deep neural networks have shown great achievements in solving complex problems. However, there are fundamental problems that limit their real world applications. Lack of measurable criteria for estimating uncertainty in the network outputs is one of these problems. In this paper, we address this limitation by introducing deformation to the network input and measuring the level of stability in the network's output. We calculate simple random transformations to estimate the prediction uncertainty of deep convolutional neural networks. For a real use-case, we apply this method to left ventricle segmentation in MRI cardiac images. We also propose an adaptive thresholding method to consider the deep neural network uncertainty. Experimental results demonstrate state-of-the-art performance and highlight the capabilities of simple methods in conjunction with deep neural networks.



### Adaptive specular reflection detection and inpainting in colonoscopy video frames
- **Arxiv ID**: http://arxiv.org/abs/1802.08402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08402v1)
- **Published**: 2018-02-23 06:25:21+00:00
- **Updated**: 2018-02-23 06:25:21+00:00
- **Authors**: Mojtaba Akbari, Majid Mohrekesh, S. M. Reza Soroushmehr, Nader Karimi, Shadrokh Samavi, Kayvan Najarian
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Colonoscopy video frames might be contaminated by bright spots with unsaturated values known as specular reflection. Detection and removal of such reflections could enhance the quality of colonoscopy images and facilitate diagnosis procedure. In this paper we propose a novel two-phase method for this purpose, consisting of detection and removal phases. In the detection phase, we employ both HSV and RGB color space information for segmentation of specular reflections. We first train a non-linear SVM for selecting a color space based on image statistical features extracted from each channel of the color spaces. Then, a cost function for detection of specular reflections is introduced. In the removal phase, we propose a two-step inpainting method which consists of appropriate replacement patch selection and removal of the blockiness effects. The proposed method is evaluated by testing on an available colonoscopy image database where accuracy and Dice score of 99.68% and 71.79% are achieved respectively.



### Closed-form solution to cooperative visual-inertial structure from motion
- **Arxiv ID**: http://arxiv.org/abs/1802.08515v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08515v1)
- **Published**: 2018-02-23 13:11:36+00:00
- **Updated**: 2018-02-23 13:11:36+00:00
- **Authors**: Agostino Martinelli
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of visual-inertial sensor fusion in the cooperative case and it provides new theoretical contributions, which regard its observability and its resolvability in closed form. The case of two agents is investigated. Each agent is equipped with inertial sensors (accelerometer and gyroscope) and with a monocular camera. By using the monocular camera, each agent can observe the other agent. No additional camera observations (e.g., of external point features in the environment) are considered. All the inertial sensors are assumed to be affected by a bias. First, the entire observable state is analytically derived. This state includes the absolute scale, the relative velocity between the two agents, the three Euler angles that express the rotation between the two agent frames and all the accelerometer and gyroscope biases. Second, the paper provides the extension of the closed-form solution given in [19] (which holds for a single agent) to the aforementioned cooperative case. The impact of the presence of the bias on the performance of this closed-form solution is investigated. As in the case of a single agent, this performance is significantly sensitive to the presence of a bias on the gyroscope, while, the presence of a bias on the accelerometer is negligible. Finally, a simple and effective method to obtain the gyroscope bias is proposed. Extensive simulations clearly show that the proposed method is successful. It is amazing that, it is possible to automatically retrieve the absolute scale and simultaneously calibrate the gyroscopes not only without any prior knowledge (as in [13]), but also without external point features in the environment.



### 6D Pose Estimation using an Improved Method based on Point Pair Features
- **Arxiv ID**: http://arxiv.org/abs/1802.08516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08516v1)
- **Published**: 2018-02-23 13:11:39+00:00
- **Updated**: 2018-02-23 13:11:39+00:00
- **Authors**: Joel Vidal, Chyi-Yeu Lin, Robert Mart√≠
- **Comment**: None
- **Journal**: None
- **Summary**: The Point Pair Feature (Drost et al. 2010) has been one of the most successful 6D pose estimation method among model-based approaches as an efficient, integrated and compromise alternative to the traditional local and global pipelines. During the last years, several variations of the algorithm have been proposed. Among these extensions, the solution introduced by Hinterstoisser et al. (2016) is a major contribution. This work presents a variation of this PPF method applied to the SIXD Challenge datasets presented at the 3rd International Workshop on Recovering 6D Object Pose held at the ICCV 2017. We report an average recall of 0.77 for all datasets and overall recall of 0.82, 0.67, 0.85, 0.37, 0.97 and 0.96 for hinterstoisser, tless, tudlight, rutgers, tejani and doumanoglou datasets, respectively.



### Training wide residual networks for deployment using a single bit for each weight
- **Arxiv ID**: http://arxiv.org/abs/1802.08530v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.08530v1)
- **Published**: 2018-02-23 13:54:23+00:00
- **Updated**: 2018-02-23 13:54:23+00:00
- **Authors**: Mark D. McDonnell
- **Comment**: Published as a conference paper at ICLR 2018
- **Journal**: ICLR 2018 - International Conference on Learning Representations,
  Apr 2018, Vancouver, Canada. 2018
- **Summary**: For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .



### Indic Handwritten Script Identification using Offline-Online Multimodal Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1802.08568v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08568v3)
- **Published**: 2018-02-23 14:49:28+00:00
- **Updated**: 2019-10-15 22:49:04+00:00
- **Authors**: Ayan Kumar Bhunia, Subham Mukherjee, Aneeshan Sain, Ankan Kumar Bhunia, Partha Pratim Roy, Umapada Pal
- **Comment**: Accepted in Information Fusion, Elsevier
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach of word-level Indic script identification using only character-level data in training stage. The advantages of using character level data for training have been outlined in section I. Our method uses a multimodal deep network which takes both offline and online modality of the data as input in order to explore the information from both the modalities jointly for script identification task. We take handwritten data in either modality as input and the opposite modality is generated through intermodality conversion. Thereafter, we feed this offline-online modality pair to our network. Hence, along with the advantage of utilizing information from both the modalities, it can work as a single framework for both offline and online script identification simultaneously which alleviates the need for designing two separate script identification modules for individual modality. One more major contribution is that we propose a novel conditional multimodal fusion scheme to combine the information from offline and online modality which takes into account the real origin of the data being fed to our network and thus it combines adaptively. An exhaustive experiment has been done on a data set consisting of English and six Indic scripts. Our proposed framework clearly outperforms different frameworks based on traditional classifiers along with handcrafted features and deep learning based methods with a clear margin. Extensive experiments show that using only character level training data can achieve state-of-art performance similar to that obtained with traditional training using word level data in our framework.



### Adversarial vulnerability for any classifier
- **Arxiv ID**: http://arxiv.org/abs/1802.08686v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.08686v2)
- **Published**: 2018-02-23 15:46:05+00:00
- **Updated**: 2018-11-30 15:44:52+00:00
- **Authors**: Alhussein Fawzi, Hamza Fawzi, Omar Fawzi
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.



### An Approach to Vehicle Trajectory Prediction Using Automatically Generated Traffic Maps
- **Arxiv ID**: http://arxiv.org/abs/1802.08632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08632v2)
- **Published**: 2018-02-23 16:48:35+00:00
- **Updated**: 2018-06-14 08:25:46+00:00
- **Authors**: Jannik Quehl, Haohao Hu, Sascha Wirges, Martin Lauer
- **Comment**: 6 Pages, 9 figures, submitted to IV 2018
- **Journal**: None
- **Summary**: Trajectory and intention prediction of traffic participants is an important task in automated driving and crucial for safe interaction with the environment. In this paper, we present a new approach to vehicle trajectory prediction based on automatically generated maps containing statistical information about the behavior of traffic participants in a given area. These maps are generated based on trajectory observations using image processing and map matching techniques and contain all typical vehicle movements and probabilities in the considered area. Our prediction approach matches an observed trajectory to a behavior contained in the map and uses this information to generate a prediction. We evaluated our approach on a dataset containing over 14000 trajectories and found that it produces significantly more precise mid-term predictions compared to motion model-based prediction approaches.



### Interactive Image Manipulation with Natural Language Instruction Commands
- **Arxiv ID**: http://arxiv.org/abs/1802.08645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08645v1)
- **Published**: 2018-02-23 17:20:13+00:00
- **Updated**: 2018-02-23 17:20:13+00:00
- **Authors**: Seitaro Shinagawa, Koichiro Yoshino, Sakriani Sakti, Yu Suzuki, Satoshi Nakamura
- **Comment**: accepted at NIPS 2017 ViGIL workshop
  (https://nips2017vigil.github.io/)
- **Journal**: None
- **Summary**: We propose an interactive image-manipulation system with natural language instruction, which can generate a target image from a source image and an instruction that describes the difference between the source and the target image. The system makes it possible to modify a generated image interactively and make natural language conditioned image generation more controllable. We construct a neural network that handles image vectors in latent space to transform the source vector to the target vector by using the vector of instruction. The experimental results indicate that the proposed framework successfully generates the target image by using a source image and an instruction on manipulation in our dataset.



### Comparative Analysis of Unsupervised Algorithms for Breast MRI Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.08655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08655v1)
- **Published**: 2018-02-23 17:40:40+00:00
- **Updated**: 2018-02-23 17:40:40+00:00
- **Authors**: Sulaiman Vesal, Nishant Ravikumar, Stephan Ellman, Andreas Maier
- **Comment**: 6 pages, submitted to Bildverarbeitung in der Medizin 2018
- **Journal**: None
- **Summary**: Accurate segmentation of breast lesions is a crucial step in evaluating the characteristics of tumors. However, this is a challenging task, since breast lesions have sophisticated shape, topological structure, and variation in the intensity distribution. In this paper, we evaluated the performance of three unsupervised algorithms for the task of breast Magnetic Resonance (MRI) lesion segmentation, namely, Gaussian Mixture Model clustering, K-means clustering and a marker-controlled Watershed transformation based method. All methods were applied on breast MRI slices following selection of regions of interest (ROIs) by an expert radiologist and evaluated on 106 subjects' images, which include 59 malignant and 47 benign lesions. Segmentation accuracy was evaluated by comparing our results with ground truth masks, using the Dice similarity coefficient (DSC), Jaccard index (JI), Hausdorff distance and precision-recall metrics. The results indicate that the marker-controlled Watershed transformation outperformed all other algorithms investigated.



### Machine learning based hyperspectral image analysis: A survey
- **Arxiv ID**: http://arxiv.org/abs/1802.08701v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08701v2)
- **Published**: 2018-02-23 19:11:25+00:00
- **Updated**: 2019-02-10 07:31:43+00:00
- **Authors**: Utsav B. Gewali, Sildomar T. Monteiro, Eli Saber
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral sensors enable the study of the chemical properties of scene materials remotely for the purpose of identification, detection, and chemical composition analysis of objects in the environment. Hence, hyperspectral images captured from earth observing satellites and aircraft have been increasingly important in agriculture, environmental monitoring, urban planning, mining, and defense. Machine learning algorithms due to their outstanding predictive power have become a key tool for modern hyperspectral image analysis. Therefore, a solid understanding of machine learning techniques have become essential for remote sensing researchers and practitioners. This paper reviews and compares recent machine learning-based hyperspectral image analysis methods published in literature. We organize the methods by the image analysis task and by the type of machine learning algorithm, and present a two-way mapping between the image analysis tasks and the types of machine learning algorithms that can be applied to them. The paper is comprehensive in coverage of both hyperspectral image analysis tasks and machine learning algorithms. The image analysis tasks considered are land cover classification, target detection, unmixing, and physical parameter estimation. The machine learning algorithms covered are Gaussian models, linear regression, logistic regression, support vector machines, Gaussian mixture model, latent linear models, sparse linear models, Gaussian mixture models, ensemble learning, directed graphical models, undirected graphical models, clustering, Gaussian processes, Dirichlet processes, and deep learning. We also discuss the open challenges in the field of hyperspectral image analysis and explore possible future directions.



### A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1802.08722v4
- **DOI**: 10.1109/CVPR.2018.00253
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08722v4)
- **Published**: 2018-02-23 20:19:59+00:00
- **Updated**: 2019-04-04 13:07:47+00:00
- **Authors**: Michel Melo Silva, Washington Luis Souza Ramos, Joao Klock Ferreira, Felipe Cadar Chamone, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento
- **Comment**: Accepted for publication in the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR) 2018. Link to the project wesite:
  https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/
- **Journal**: None
- **Summary**: Thanks to the advances in the technology of low-cost digital cameras and the popularity of the self-recording culture, the amount of visual data on the Internet is going to the opposite side of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched in a computer folder or website. In this work, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem, which combined with a smoothing frame transition method accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. The experiments show that our method is able to fast-forward videos to retain as much relevant information and smoothness as the state-of-the-art techniques in less time. We also present a new 80-hour multimodal (RGB-D, IMU, and GPS) dataset of first-person videos with annotations for recorder profile, frame scene, activities, interaction, and attention.



### Longitudinal Face Aging in the Wild - Recent Deep Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1802.08726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08726v1)
- **Published**: 2018-02-23 20:29:50+00:00
- **Updated**: 2018-02-23 20:29:50+00:00
- **Authors**: Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui
- **Comment**: None
- **Journal**: None
- **Summary**: Face Aging has raised considerable attentions and interest from the computer vision community in recent years. Numerous approaches ranging from purely image processing techniques to deep learning structures have been proposed in literature. In this paper, we aim to give a review of recent developments of modern deep learning based approaches, i.e. Deep Generative Models, for Face Aging task. Their structures, formulation, learning algorithms as well as synthesized results are also provided with systematic discussions. Moreover, the aging databases used in most methods to learn the aging process are also reviewed.



### A DIRT-T Approach to Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1802.08735v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.08735v2)
- **Published**: 2018-02-23 20:57:28+00:00
- **Updated**: 2018-03-19 13:45:46+00:00
- **Authors**: Rui Shu, Hung H. Bui, Hirokazu Narui, Stefano Ermon
- **Comment**: ICLR 2018
- **Journal**: None
- **Summary**: Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.



### Edge-Based Recognition of Novel Objects for Robotic Grasping
- **Arxiv ID**: http://arxiv.org/abs/1802.08753v1
- **DOI**: 10.3390/robotics8030063
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.08753v1)
- **Published**: 2018-02-23 22:38:55+00:00
- **Updated**: 2018-02-23 22:38:55+00:00
- **Authors**: Amirhossein Jabalameli, Nabil Ettehadi, Aman Behal
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of grasping novel objects in unstructured environments. To address this problem, consideration of the object geometry, reachability and force closure analysis are required. We propose a framework for grasping unknown objects by localizing contact regions on the contours formed by a set of depth edges in a single view 2D depth image. According to the edge geometric features obtained from analyzing the data of the depth map, the contact regions are determined. Finally,We validate the performance of the approach by applying it to the scenes with both single and multiple objects, using Baxter manipulator.



### No Blind Spots: Full-Surround Multi-Object Tracking for Autonomous Vehicles using Cameras & LiDARs
- **Arxiv ID**: http://arxiv.org/abs/1802.08755v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.08755v4)
- **Published**: 2018-02-23 22:48:29+00:00
- **Updated**: 2019-02-19 18:55:42+00:00
- **Authors**: Akshay Rangesh, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Online multi-object tracking (MOT) is extremely important for high-level spatial reasoning and path planning for autonomous and highly-automated vehicles. In this paper, we present a modular framework for tracking multiple objects (vehicles), capable of accepting object proposals from different sensor modalities (vision and range) and a variable number of sensors, to produce continuous object tracks. This work is a generalization of the MDP framework for MOT, with some key extensions - First, we track objects across multiple cameras and across different sensor modalities. This is done by fusing object proposals across sensors accurately and efficiently. Second, the objects of interest (targets) are tracked directly in the real world. This is a departure from traditional techniques where objects are simply tracked in the image plane. Doing so allows the tracks to be readily used by an autonomous agent for navigation and related tasks.   To verify the effectiveness of our approach, we test it on real world highway data collected from a heavily sensorized testbed capable of capturing full-surround information. We demonstrate that our framework is well-suited to track objects through entire maneuvers around the ego-vehicle, some of which take more than a few minutes to complete. We also leverage the modularity of our approach by comparing the effects of including/excluding different sensors, changing the total number of sensors, and the quality of object proposals on the final tracking result.




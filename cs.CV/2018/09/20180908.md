# Arxiv Papers in cs.CV on 2018-09-08
### Joint Autoregressive and Hierarchical Priors for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1809.02736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02736v1)
- **Published**: 2018-09-08 01:51:28+00:00
- **Updated**: 2018-09-08 01:51:28+00:00
- **Authors**: David Minnen, Johannes Ballé, George Toderici
- **Comment**: Accepted at the 32nd Conference on Neural Information Processing
  Systems (NIPS 2018)
- **Journal**: None
- **Summary**: Recent models for learned image compression are based on autoencoders, learning approximately invertible mappings from pixels to a quantized latent representation. These are combined with an entropy model, a prior on the latent representation that can be used with standard arithmetic coding algorithms to yield a compressed bitstream. Recently, hierarchical entropy models have been introduced as a way to exploit more structure in the latents than simple fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, as well as combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models come with a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and, together, exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate--distortion performance, providing a 15.8% average reduction in file size over the previous state-of-the-art method based on deep learning, which corresponds to a 59.8% size reduction over JPEG, more than 35% reduction compared to WebP and JPEG2000, and bitstreams 8.4% smaller than BPG, the current state-of-the-art image codec. To the best of our knowledge, our model is the first learning-based method to outperform BPG on both PSNR and MS-SSIM distortion metrics.



### RealPoint3D: Point Cloud Generation from a Single Image with Complex Background
- **Arxiv ID**: http://arxiv.org/abs/1809.02743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02743v1)
- **Published**: 2018-09-08 02:24:19+00:00
- **Updated**: 2018-09-08 02:24:19+00:00
- **Authors**: Yan Xia, Yang Zhang, Dingfu Zhou, Xinyu Huang, Cheng Wang, Ruigang Yang
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: 3D point cloud generation by the deep neural network from a single image has been attracting more and more researchers' attention. However, recently-proposed methods require the objects be captured with relatively clean backgrounds, fixed viewpoint, while this highly limits its application in the real environment. To overcome these drawbacks, we proposed to integrate the prior 3D shape knowledge into the network to guide the 3D generation. By taking additional 3D information, the proposed network can handle the 3D object generation from a single real image captured from any viewpoint and complex background. Specifically, giving a query image, we retrieve the nearest shape model from a pre-prepared 3D model database. Then, the image together with the retrieved shape model is fed into the proposed network to generate the fine-grained 3D point cloud. The effectiveness of our proposed framework has been verified on different kinds of datasets. Experimental results show that the proposed framework achieves state-of-the-art accuracy compared to other volumetric-based and point set generation methods. Furthermore, the proposed framework works well for real images in complex backgrounds with various view angles.



### CNNs for Surveillance Footage Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.02766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02766v1)
- **Published**: 2018-09-08 06:45:22+00:00
- **Updated**: 2018-09-08 06:45:22+00:00
- **Authors**: Utkarsh Contractor, Chinmayi Dixit, Deepti Mahajan
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we adapt high-performing CNN architectures to differentiate between scenes with and without abandoned luggage. Using frames from two video datasets, we compare the results of training different architectures on each dataset as well as on combining the datasets. We additionally use network visualization techniques to gain insight into what the neural network sees, and the basis of the classification decision. We intend that our results benefit further work in applying CNNs in surveillance and security-related tasks.



### Instance-based Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.02776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02776v2)
- **Published**: 2018-09-08 08:34:14+00:00
- **Updated**: 2018-11-24 06:27:53+00:00
- **Authors**: Tianyang Wang, Jun Huan, Michelle Zhu
- **Comment**: Accepted to WACV 2019. This is a preprint version
- **Journal**: None
- **Summary**: Deep transfer learning recently has acquired significant research interest. It makes use of pre-trained models that are learned from a source domain, and utilizes these models for the tasks in a target domain. Model-based deep transfer learning is probably the most frequently used method. However, very little research work has been devoted to enhancing deep transfer learning by focusing on the influence of data. In this paper, we propose an instance-based approach to improve deep transfer learning in a target domain. Specifically, we choose a pre-trained model from a source domain and apply this model to estimate the influence of training samples in a target domain. Then we optimize the training data of the target domain by removing the training samples that will lower the performance of the pre-trained model. We later either fine-tune the pre-trained model with the optimized training data in the target domain, or build a new model which is initialized partially based on the pre-trained model, and fine-tune it with the optimized training data in the target domain. Using this approach, transfer learning can help deep learning models to capture more useful features. Extensive experiments demonstrate the effectiveness of our approach on boosting the quality of deep learning models for some common computer vision tasks, such as image classification.



### Structure-Preserving Transformation: Generating Diverse and Transferable Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1809.02786v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.02786v3)
- **Published**: 2018-09-08 10:26:50+00:00
- **Updated**: 2018-12-22 09:07:32+00:00
- **Authors**: Dan Peng, Zizhan Zheng, Xiaofeng Zhang
- **Comment**: The AAAI-2019 Workshop on Artificial Intelligence for Cyber Security
  (AICS)
- **Journal**: None
- **Summary**: Adversarial examples are perturbed inputs designed to fool machine learning models. Most recent works on adversarial examples for image classification focus on directly modifying pixels with minor perturbations. A common requirement in all these works is that the malicious perturbations should be small enough (measured by an L_p norm for some p) so that they are imperceptible to humans. However, small perturbations can be unnecessarily restrictive and limit the diversity of adversarial examples generated. Further, an L_p norm based distance metric ignores important structure patterns hidden in images that are important to human perception. Consequently, even the minor perturbation introduced in recent works often makes the adversarial examples less natural to humans. More importantly, they often do not transfer well and are therefore less effective when attacking black-box models especially for those protected by a defense mechanism. In this paper, we propose a structure-preserving transformation (SPT) for generating natural and diverse adversarial examples with extremely high transferability. The key idea of our approach is to allow perceptible deviation in adversarial examples while keeping structure patterns that are central to a human classifier. Empirical results on the MNIST and the fashion-MNIST datasets show that adversarial examples generated by our approach can easily bypass strong adversarial training. Further, they transfer well to other target models with no loss or little loss of successful attack rate.



### Adversarial Learning for Image Forensics Deep Matching with Atrous Convolution
- **Arxiv ID**: http://arxiv.org/abs/1809.02791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02791v1)
- **Published**: 2018-09-08 11:59:36+00:00
- **Updated**: 2018-09-08 11:59:36+00:00
- **Authors**: Yaqi Liu, Xianfeng Zhao, Xiaobin Zhu, Yun Cao
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Constrained image splicing detection and localization (CISDL) is a newly proposed challenging task for image forensics, which investigates two input suspected images and identifies whether one image has suspected regions pasted from the other. In this paper, we propose a novel adversarial learning framework to train the deep matching network for CISDL. Our framework mainly consists of three building blocks: 1) the deep matching network based on atrous convolution (DMAC) aims to generate two high-quality candidate masks which indicate the suspected regions of the two input images, 2) the detection network is designed to rectify inconsistencies between the two corresponding candidate masks, 3) the discriminative network drives the DMAC network to produce masks that are hard to distinguish from ground-truth ones. In DMAC, atrous convolution is adopted to extract features with rich spatial information, the correlation layer based on the skip architecture is proposed to capture hierarchical features, and atrous spatial pyramid pooling is constructed to localize tampered regions at multiple scales. The detection network and the discriminative network act as the losses with auxiliary parameters to supervise the training of DMAC in an adversarial way. Extensive experiments, conducted on 21 generated testing sets and two public datasets, demonstrate the effectiveness of the proposed framework and the superior performance of DMAC.



### Video Smoke Detection Based on Deep Saliency Network
- **Arxiv ID**: http://arxiv.org/abs/1809.02802v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02802v2)
- **Published**: 2018-09-08 13:44:28+00:00
- **Updated**: 2019-01-14 01:21:32+00:00
- **Authors**: Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Zhong Wang, Yang Jia, Jinjun Wang
- **Comment**: 21 pages, 12 figures
- **Journal**: None
- **Summary**: Video smoke detection is a promising fire detection method especially in open or large spaces and outdoor environments. Traditional video smoke detection methods usually consist of candidate region extraction and classification, but lack powerful characterization for smoke. In this paper, we propose a novel video smoke detection method based on deep saliency network. Visual saliency detection aims to highlight the most important object regions in an image. The pixel-level and object-level salient convolutional neural networks are combined to extract the informative smoke saliency map. An end-to-end framework for salient smoke detection and existence prediction of smoke is proposed for application in video smoke detection. The deep feature map is combined with the saliency map to predict the existence of smoke in an image. Initial and augmented dataset are built to measure the performance of frameworks with different design strategies. Qualitative and quantitative analysis at frame-level and pixel-level demonstrate the excellent performance of the ultimate framework.



### Faithful Multimodal Explanation for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1809.02805v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.02805v2)
- **Published**: 2018-09-08 14:14:03+00:00
- **Updated**: 2019-06-03 19:54:15+00:00
- **Authors**: Jialin Wu, Raymond J. Mooney
- **Comment**: In ACL 2019 BlackboxNLP workshop
- **Journal**: None
- **Summary**: AI systems' ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods with both automatic evaluation metrics and human evaluation metrics.



### Rate-Adaptive Neural Networks for Spatial Multiplexers
- **Arxiv ID**: http://arxiv.org/abs/1809.02850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02850v1)
- **Published**: 2018-09-08 18:21:31+00:00
- **Updated**: 2018-09-08 18:21:31+00:00
- **Authors**: Suhas Lohit, Rajhans Singh, Kuldeep Kulkarni, Pavan Turaga
- **Comment**: None
- **Journal**: None
- **Summary**: In resource-constrained environments, one can employ spatial multiplexing cameras to acquire a small number of measurements of a scene, and perform effective reconstruction or high-level inference using purely data-driven neural networks. However, once trained, the measurement matrix and the network are valid only for a single measurement rate (MR) chosen at training time. To overcome this drawback, we answer the following question: How can we jointly design the measurement operator and the reconstruction/inference network so that the system can operate over a \textit{range} of MRs? To this end, we present a novel training algorithm, for learning \textbf{\textit{rate-adaptive}} networks. Using standard datasets, we demonstrate that, when tested over a range of MRs, a rate-adaptive network can provide high quality reconstruction over a the entire range, resulting in up to about 15 dB improvement over previous methods, where the network is valid for only one MR. We demonstrate the effectiveness of our approach for sample-efficient object tracking where video frames are acquired at dynamically varying MRs. We also extend this algorithm to learn the measurement operator in conjunction with image recognition networks. Experiments on MNIST and CIFAR-10 confirm the applicability of our algorithm to different tasks.



### Online Mutual Foreground Segmentation for Multispectral Stereo Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.02851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02851v2)
- **Published**: 2018-09-08 18:21:50+00:00
- **Updated**: 2018-12-21 23:10:50+00:00
- **Authors**: Pierre-Luc St-Charles, Guillaume-Alexandre Bilodeau, Robert Bergevin
- **Comment**: Preprint accepted for publication in IJCV (December 2018)
- **Journal**: None
- **Summary**: The segmentation of video sequences into foreground and background regions is a low-level process commonly used in video content analysis and smart surveillance applications. Using a multispectral camera setup can improve this process by providing more diverse data to help identify objects despite adverse imaging conditions. The registration of several data sources is however not trivial if the appearance of objects produced by each sensor differs substantially. This problem is further complicated when parallax effects cannot be ignored when using close-range stereo pairs. In this work, we present a new method to simultaneously tackle multispectral segmentation and stereo registration. Using an iterative procedure, we estimate the labeling result for one problem using the provisional result of the other. Our approach is based on the alternating minimization of two energy functions that are linked through the use of dynamic priors. We rely on the integration of shape and appearance cues to find proper multispectral correspondences, and to properly segment objects in low contrast regions. We also formulate our model as a frame processing pipeline using higher order terms to improve the temporal coherence of our results. Our method is evaluated under different configurations on multiple multispectral datasets, and our implementation is available online.



### Learning Sports Camera Selection from Internet Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.02854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02854v1)
- **Published**: 2018-09-08 19:16:04+00:00
- **Updated**: 2018-09-08 19:16:04+00:00
- **Authors**: Jianhui Chen, Keyu Lu, Sijia Tian, James J. Little
- **Comment**: 8 + 2 pages, WACV2019 accepted
- **Journal**: None
- **Summary**: This work addresses camera selection, the task of predicting which camera should be "on air" from multiple candidate cameras for soccer broadcast. The task is challenging because of the scarcity of learning data with all candidate views. Meanwhile, broadcast videos are freely available on the Internet (e.g. Youtube). However, these videos only record the selected camera views, omitting the other candidate views. To overcome this problem, we first introduce a random survival forest (RSF) method to impute the incomplete data effectively. Then, we propose a spatial-appearance heatmap to describe foreground objects (e.g. players and balls) in an image. To evaluate the performance of our system, we collect the largest-ever dataset for soccer broadcasting camera selection. It has one main game which has all candidate views and twelve auxiliary games which only have the broadcast view. Our method significantly outperforms state-of-the-art methods on this challenging dataset. Further analysis suggests that the improvement in performance is indeed from the extra information from auxiliary games.



### Unsupervised Person Re-identification by Deep Learning Tracklet Association
- **Arxiv ID**: http://arxiv.org/abs/1809.02874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02874v1)
- **Published**: 2018-09-08 20:49:01+00:00
- **Updated**: 2018-09-08 20:49:01+00:00
- **Authors**: Minxian Li, Xiatian Zhu, Shaogang Gong
- **Comment**: ECCV 2018 Oral
- **Journal**: None
- **Summary**: Mostexistingpersonre-identification(re-id)methods relyon supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation re- id methods using six person re-id benchmarking datasets.



### A Supervised Learning Methodology for Real-Time Disguised Face Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1809.02875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02875v1)
- **Published**: 2018-09-08 20:53:19+00:00
- **Updated**: 2018-09-08 20:53:19+00:00
- **Authors**: Saumya Kumaar, Abhinandan Dogra, Abrar Majeedi, Hanan Gani, Ravi M. Vishwanath, S N Omkar
- **Comment**: Accepted at 2018 International Conference on Robotics and Computer
  Vision
- **Journal**: None
- **Summary**: Facial recognition has always been a challeng- ing task for computer vision scientists and experts. Despite complexities arising due to variations in camera parameters, illumination and face orientations, significant progress has been made in the field with deep learning algorithms now competing with human-level accuracy. But in contrast to the recent advances in face recognition techniques, Disguised Facial Identification continues to be a tougher challenge in the field of computer vision. The modern day scenario, where security is of prime concern, regular face identification techniques do not perform as required when the faces are disguised, which calls for a different approach to handle situations where intruders have their faces masked. Along the same lines, we propose a deep learning architecture for disguised facial recognition (DFR). The algorithm put forward in this paper detects 20 facial key-points in the first stage, using a 14-layered convolutional neural network (CNN). These facial key-points are later utilized by a support vector machine (SVM) for classifying the disguised faces based on the euclidean distance ratios and angles between different facial key-points. This overall architecture imparts a basic intelligence to our system. Our key-point feature prediction accuracy is 65% while the classification rate is 72.4%. Moreover, the architecture works at 19 FPS, thereby performing in almost real-time. The efficiency of our approach is also compared with the state-of-the-art Disguised Facial Identification methods.



### Cost-Sensitive Active Learning for Intracranial Hemorrhage Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.02882v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.02882v1)
- **Published**: 2018-09-08 21:43:18+00:00
- **Updated**: 2018-09-08 21:43:18+00:00
- **Authors**: Weicheng Kuo, Christian Häne, Esther Yuh, Pratik Mukherjee, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning for clinical applications is subject to stringent performance requirements, which raises a need for large labeled datasets. However, the enormous cost of labeling medical data makes this challenging. In this paper, we build a cost-sensitive active learning system for the problem of intracranial hemorrhage detection and segmentation on head computed tomography (CT). We show that our ensemble method compares favorably with the state-of-the-art, while running faster and using less memory. Moreover, our experiments are done using a substantially larger dataset than earlier papers on this topic. Since the labeling time could vary tremendously across examples, we model the labeling time and optimize the return on investment. We validate this idea by core-set selection on our large labeled dataset and by growing it with data from the wild.



### Fine-Tuning VGG Neural Network For Fine-grained State Recognition of Food Images
- **Arxiv ID**: http://arxiv.org/abs/1809.09529v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.09529v1)
- **Published**: 2018-09-08 22:38:33+00:00
- **Updated**: 2018-09-08 22:38:33+00:00
- **Authors**: Kaoutar Ben Ahmed, Ahmad Babaeian Jelodar
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: State recognition of food images can be considered as one of the promising applications of object recognition and fine-grained image classification in computer vision. In this paper, evidence is provided for the power of convolutional neural network (CNN) for food state recognition, even with a small data set. In this study, we fine-tuned a CNN initially trained on a large natural image recognition dataset (Imagenet ILSVRC) and transferred the learned feature representations to the food state recognition task. A small-scale dataset consisting of 5978 images of seven categories was constructed and annotated manually. Data augmentation was applied to increase the size of the data.




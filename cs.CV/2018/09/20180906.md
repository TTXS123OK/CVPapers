# Arxiv Papers in cs.CV on 2018-09-06
### MDCN: Multi-Scale, Deep Inception Convolutional Neural Networks for Efficient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.01791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01791v1)
- **Published**: 2018-09-06 02:24:44+00:00
- **Updated**: 2018-09-06 02:24:44+00:00
- **Authors**: Wenchi Ma, Yuanwei Wu, Zongbo Wang, Guanghui Wang
- **Comment**: None
- **Journal**: IEEE ICPR2018
- **Summary**: Object detection in challenging situations such as scale variation, occlusion, and truncation depends not only on feature details but also on contextual information. Most previous networks emphasize too much on detailed feature extraction through deeper and wider networks, which may enhance the accuracy of object detection to certain extent. However, the feature details are easily being changed or washed out after passing through complicated filtering structures. To better handle these challenges, the paper proposes a novel framework, multi-scale, deep inception convolutional neural network (MDCN), which focuses on wider and broader object regions by activating feature maps produced in the deep part of the network. Instead of incepting inner layers in the shallow part of the network, multi-scale inceptions are introduced in the deep layers. The proposed framework integrates the contextual information into the learning process through a single-shot network structure. It is computational efficient and avoids the hard training problem of previous macro feature extraction network designed for shallow layers. Extensive experiments demonstrate the effectiveness and superior performance of MDCN over the state-of-the-art models.



### Interpretable Visual Question Answering by Reasoning on Dependency Trees
- **Arxiv ID**: http://arxiv.org/abs/1809.01810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01810v2)
- **Published**: 2018-09-06 04:09:28+00:00
- **Updated**: 2019-12-25 04:32:51+00:00
- **Authors**: Qingxing Cao, Bailin Li, Xiaodan Liang, Liang Lin
- **Comment**: 14 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1804.00105
- **Journal**: None
- **Summary**: Collaborative reasoning for understanding image-question pairs is a very critical but underexplored topic in interpretable visual question answering systems. Although very recent studies have attempted to use explicit compositional processes to assemble multiple subtasks embedded in questions, their models heavily rely on annotations or handcrafted rules to obtain valid reasoning processes, which leads to either heavy workloads or poor performance on compositional reasoning. In this paper, to better align image and language domains in diverse and unrestricted cases, we propose a novel neural network model that performs global reasoning on a dependency tree parsed from the question; thus, our model is called a parse-tree-guided reasoning network (PTGRN). This network consists of three collaborative modules: i) an attention module that exploits the local visual evidence of each word parsed from the question, ii) a gated residual composition module that composes the previously mined evidence, and iii) a parse-tree-guided propagation module that passes the mined evidence along the parse tree. Thus, PTGRN is capable of building an interpretable visual question answering (VQA) system that gradually derives image cues following question-driven parse-tree reasoning. Experiments on relational datasets demonstrate the superiority of PTGRN over current state-of-the-art VQA methods, and the visualization results highlight the explainable capability of our reasoning system.



### YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1809.03327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.03327v1)
- **Published**: 2018-09-06 04:19:45+00:00
- **Updated**: 2018-09-06 04:19:45+00:00
- **Authors**: Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, Thomas Huang
- **Comment**: Dataset Report. arXiv admin note: substantial text overlap with
  arXiv:1809.00461
- **Journal**: None
- **Summary**: Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatialtemporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 4,453 YouTube video clips and 94 object categories. This is by far the largest video object segmentation dataset to our knowledge and has been released at http://youtube-vos.org. We further evaluate several existing state-of-the-art video object segmentation algorithms on this dataset which aims to establish baselines for the development of new algorithms in the future.



### Visual Coreference Resolution in Visual Dialog using Neural Module Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.01816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1809.01816v1)
- **Published**: 2018-09-06 04:36:22+00:00
- **Updated**: 2018-09-06 04:36:22+00:00
- **Authors**: Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra, Marcus Rohrbach
- **Comment**: ECCV 2018 + results on VisDial v1.0 dataset
- **Journal**: None
- **Summary**: Visual dialog entails answering a series of questions grounded in an image, using dialog history as context. In addition to the challenges found in visual question answering (VQA), which can be seen as one-round dialog, visual dialog encompasses several more. We focus on one such problem called visual coreference resolution that involves determining which words, typically noun phrases and pronouns, co-refer to the same entity/object instance in an image. This is crucial, especially for pronouns (e.g., `it'), as the dialog agent must first link it to a previous coreference (e.g., `boat'), and only then can rely on the visual grounding of the coreference `boat' to reason about the pronoun `it'. Prior work (in visual dialog) models visual coreference resolution either (a) implicitly via a memory network over history, or (b) at a coarse level for the entire question; and not explicitly at a phrase level of granularity. In this work, we propose a neural module network architecture for visual dialog by introducing two novel modules - Refer and Exclude - that perform explicit, grounded, coreference resolution at a finer word level. We demonstrate the effectiveness of our model on MNIST Dialog, a visually simple yet coreference-wise complex dataset, by achieving near perfect accuracy, and on VisDial, a large and challenging visual dialog dataset on real images, where our model outperforms other approaches, and is more interpretable, grounded, and consistent qualitatively.



### Driving Experience Transfer Method for End-to-End Control of Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1809.01822v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.01822v2)
- **Published**: 2018-09-06 05:00:10+00:00
- **Updated**: 2018-09-07 06:03:50+00:00
- **Authors**: Dooseop Choi, Taeg-Hyun An, Kyounghwan Ahn, Jeongdan Choi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.



### Connecting Image Denoising and High-Level Vision Tasks via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.01826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01826v1)
- **Published**: 2018-09-06 05:13:22+00:00
- **Updated**: 2018-09-06 05:13:22+00:00
- **Authors**: Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, Thomas S. Huang
- **Comment**: arXiv admin note: text overlap with arXiv:1706.04284
- **Journal**: None
- **Summary**: Image denoising and high-level vision tasks are usually handled independently in the conventional practice of computer vision, and their connection is fragile. In this paper, we cope with the two jointly and explore the mutual influence between them with the focus on two questions, namely (1) how image denoising can help improving high-level vision tasks, and (2) how the semantic information from high-level vision tasks can be used to guide image denoising. First for image denoising we propose a convolutional neural network in which convolutions are conducted in various spatial resolutions via downsampling and upsampling operations in order to fuse and exploit contextual information on different scales. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We experimentally show that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network produces more visually appealing results. Extensive experiments demonstrate the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online: https://github.com/Ding-Liu/DeepDenoising



### Unsupervised Learning of View-invariant Action Representations
- **Arxiv ID**: http://arxiv.org/abs/1809.01844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01844v1)
- **Published**: 2018-09-06 06:41:03+00:00
- **Updated**: 2018-09-06 06:41:03+00:00
- **Authors**: Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.



### Guiding the Creation of Deep Learning-based Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1809.03322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03322v1)
- **Published**: 2018-09-06 07:07:12+00:00
- **Updated**: 2018-09-06 07:07:12+00:00
- **Authors**: Ángela Casado, Jónathan Heras
- **Comment**: To be published in I Workshop en Deep Learning of the CAEPIA
  Conference
- **Journal**: None
- **Summary**: Object detection is a computer vision field that has applications in several contexts ranging from biomedicine and agriculture to security. In the last years, several deep learning techniques have greatly improved object detection models. Among those techniques, we can highlight the YOLO approach, that allows the construction of accurate models that can be employed in real-time applications. However, as most deep learning techniques, YOLO has a steep learning curve and creating models using this approach might be challenging for non-expert users. In this work, we tackle this problem by constructing a suite of Jupyter notebooks that democratizes the construction of object detection models using YOLO. The suitability of our approach has been proven with a dataset of stomata images where we have achieved a mAP of 90.91%.



### On the Importance of Visual Context for Data Augmentation in Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1809.02492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02492v3)
- **Published**: 2018-09-06 08:37:15+00:00
- **Updated**: 2019-09-19 09:41:11+00:00
- **Authors**: Nikita Dvornik, Julien Mairal, Cordelia Schmid
- **Comment**: Updated the experimental section. arXiv admin note: substantial text
  overlap with arXiv:1807.07428
- **Journal**: None
- **Summary**: Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artificially increasing the number of training examples, it helps reducing overfitting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-specific prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with significant gains in a limited annotation scenario, i.e. when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation.



### Full-body High-resolution Anime Generation with Progressive Structure-conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.01890v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.01890v1)
- **Published**: 2018-09-06 09:09:40+00:00
- **Updated**: 2018-09-06 09:09:40+00:00
- **Authors**: Koichi Hamada, Kentaro Tachibana, Tianqi Li, Hiroto Honda, Yusuke Uchida
- **Comment**: Accepted to ECCV 2018 Workshop: Computer Vision for Fashion, Art and
  Design. Project page is at https://dena.com/intl/anime-generation
- **Journal**: None
- **Summary**: We propose Progressive Structure-conditional Generative Adversarial Networks (PSGAN), a new framework that can generate full-body and high-resolution character images based on structural information. Recent progress in generative adversarial networks with progressive training has made it possible to generate high-resolution images. However, existing approaches have limitations in achieving both high image quality and structural consistency at the same time. Our method tackles the limitations by progressively increasing the resolution of both generated images and structural conditions during training. In this paper, we empirically demonstrate the effectiveness of this method by showing the comparison with existing approaches and video generation results of diverse anime characters at 1024x1024 based on target pose sequences. We also create a novel dataset containing full-body 1024x1024 high-resolution images and exact 2D pose keypoints using Unity 3D Avatar models.



### A Stable Minutia Descriptor based on Gabor Wavelet and Linear Discriminant Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.03326v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1809.03326v1)
- **Published**: 2018-09-06 10:46:19+00:00
- **Updated**: 2018-09-06 10:46:19+00:00
- **Authors**: Gwang-Il Ri, Mun-Chol Kim, Su-Rim Ji
- **Comment**: None
- **Journal**: None
- **Summary**: The minutia descriptor which describes characteristics of minutia, plays a major role in fingerprint recognition. Typically, fingerprint recognition systems employ minutia descriptors to find potential correspondence between minutiae, and they use similarity between two minutia descriptors to calculate overall similarity between two fingerprint images. A good minutia descriptor can improve recognition accuracy of fingerprint recognition system and largely reduce comparing time. A good minutia descriptor should have high ability to distinguish between different minutiae and at the same time should be robust in difficult conditions including poor quality image and small size image. It also should be effective in computational cost of similarity among descriptors. In this paper, a robust minutia descriptor is constructed using Gabor wavelet and linear discriminant analysis. This minutia descriptor has high distinguishing ability, stability and simple comparing method. Experimental results on FVC2004 and FVC2006 databases show that the proposed minutia descriptor is very effective in fingerprint recognition.



### Dynamic Block Matching to assess the longitudinal component of the dense motion field of the carotid artery wall in B-mode ultrasound sequences -- Association with coronary artery disease
- **Arxiv ID**: http://arxiv.org/abs/1809.01924v3
- **DOI**: 10.1002/mp.13186
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01924v3)
- **Published**: 2018-09-06 11:24:45+00:00
- **Updated**: 2020-05-18 20:37:17+00:00
- **Authors**: Guillaume Zahnd, Kozue Saito, Kazuyuki Nagatsuka, Yoshito Otake, Yoshinobu Sato
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: The motion of the common carotid artery tissue layers along the vessel axis during the cardiac cycle, observed in ultrasound imaging, is associated with the presence of established cardiovascular risk factors. However, the vast majority of the methods are based on the tracking of a single point, thus failing to capture the overall motion of the entire arterial wall. The aim of this work is to introduce a motion tracking framework able to simultaneously extract the trajectory of a large collection of points spanning the entire exploitable width of the image.   Method: The longitudinal motion, which is the main focus of the present work, is determined in two steps. First, a series of independent block matching operations are carried out for all the tracked points. Then, an original dynamic-programming approach is exploited to regularize the collection of similarity maps and estimate the globally optimal motion over the entire vessel wall. Sixty-two atherosclerotic participants at high cardiovascular risk were involved in this study.   Results: A dense displacement field, describing the longitudinal motion of the carotid far wall over time, was extracted. For each cine-loop, the method was evaluated against manual reference tracings performed on three local points, with an average absolute error of 150+/-163 um. A strong correlation was found between motion inhomogeneity and the presence of coronary artery disease (beta-coefficient=0.586, p=0.003).   Conclusions: To the best of our knowledge, this is the first time that a method is specifically proposed to assess the dense motion field of the carotid far wall. This approach has potential to evaluate the (in)homogeneity of the wall dynamics. The proposed method has promising performances to improve the analysis of arterial longitudinal motion and the understanding of the underlying patho-physiological parameters.



### Disentangled Variational Representation for Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.01936v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01936v3)
- **Published**: 2018-09-06 12:15:59+00:00
- **Updated**: 2019-01-23 07:25:42+00:00
- **Authors**: Xiang Wu, Huaibo Huang, Vishal M. Patel, Ran He, Zhenan Sun
- **Comment**: AAAI 2019
- **Journal**: None
- **Summary**: Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.



### Cascaded Mutual Modulation for Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1809.01943v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01943v1)
- **Published**: 2018-09-06 12:26:24+00:00
- **Updated**: 2018-09-06 12:26:24+00:00
- **Authors**: Yiqun Yao, Jiaming Xu, Feng Wang, Bo Xu
- **Comment**: to appear in EMNLP 2018
- **Journal**: None
- **Summary**: Visual reasoning is a special visual question answering problem that is multi-step and compositional by nature, and also requires intensive text-vision interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end visual reasoning model. CMM includes a multi-step comprehension process for both question and image. In each step, we use a Feature-wise Linear Modulation (FiLM) technique to enable textual/visual pipeline to mutually control each other. Experiments show that CMM significantly outperforms most related models, and reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR, collected from both synthetic and natural languages. Ablation studies confirm that both our multistep framework and our visual-guided language modulation are critical to the task. Our code is available at https://github.com/FlamingHorizon/CMM-VR.



### Pore detection in high-resolution fingerprint images using Deep Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1809.01986v2
- **DOI**: 10.1117/1.JEI.28.2.020502
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01986v2)
- **Published**: 2018-09-06 13:38:16+00:00
- **Updated**: 2018-09-20 09:41:17+00:00
- **Authors**: Vijay Anand, Vivek kanhangad
- **Comment**: 9 pages, 1 figure, 4 Tables
- **Journal**: None
- **Summary**: This letter presents a residual learning-based convolutional neural network, referred to as DeepResPore, for detection of pores in high-resolution fingerprint images. Specifically, the proposed DeepResPore model generates a pore intensity map from the input fingerprint image. Subsequently, the local maxima filter is operated on the pore intensity map to identify the pore coordinates. The results of our experiments indicate that the proposed approach is effective in extracting pores with a true detection rate of 94:49% on Test set I and 93:78% on Test set II of the publicly available PolyU HRF dataset. Most importantly, the proposed approach achieves state-of-the-art performance on both test sets.



### Multi-Expert Gender Classification on Age Group by Integrating Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.01990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01990v2)
- **Published**: 2018-09-06 13:47:19+00:00
- **Updated**: 2018-09-07 15:05:03+00:00
- **Authors**: Jun Beom Kho
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Generally, facial age variations affect gender classification accuracy significantly, because facial shape and skin texture change as they grow old. This requires re-examination on the gender classification system to consider facial age information. In this paper, we propose Multi-expert Gender Classification on Age Group (MGA), an end-to-end multi-task learning schemes of age estimation and gender classification. First, two types of deep neural networks are utilized; Convolutional Appearance Network (CAN) for facial appearance feature and Deep Geometry Network (DGN) for facial geometric feature. Then, CAN and DGN are integrated by the proposed model integration strategy and fine-tuned in order to improve age and gender classification accuracy. The facial images are categorized into one of three age groups (young, adult and elder group) based on their estimated age, and the system makes a gender prediction according to average fusion strategy of three gender classification experts, which are trained to fit gender characteristics of each age group. Rigorous experimental results conducted on the challenging databases suggest that the proposed MGA outperforms several state-of-art researches with smaller computational cost.



### Dense Pose Transfer
- **Arxiv ID**: http://arxiv.org/abs/1809.01995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01995v1)
- **Published**: 2018-09-06 13:53:00+00:00
- **Updated**: 2018-09-06 13:53:00+00:00
- **Authors**: Natalia Neverova, Riza Alp Guler, Iasonas Kokkinos
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: In this work we integrate ideas from surface-based modeling with neural synthesis: we propose a combination of surface-based pose estimation and deep generative models that allows us to perform accurate pose transfer, i.e. synthesize a new image of a person based on a single image of that person and the image of a pose donor. We use a dense pose estimation system that maps pixels from both images to a common surface-based coordinate system, allowing the two images to be brought in correspondence with each other. We inpaint and refine the source image intensities in the surface coordinate system, prior to warping them onto the target pose. These predictions are fused with those of a convolutional predictive module through a neural synthesis module allowing for training the whole pipeline jointly end-to-end, optimizing a combination of adversarial and perceptual losses. We show that dense pose estimation is a substantially more powerful conditioning input than landmark-, or mask-based alternatives, and report systematic improvements over state of the art generators on DeepFashion and MVC datasets.



### 3D Surface Reconstruction by Pointillism
- **Arxiv ID**: http://arxiv.org/abs/1809.02002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02002v2)
- **Published**: 2018-09-06 14:11:50+00:00
- **Updated**: 2018-10-04 06:56:42+00:00
- **Authors**: Olivia Wiles, Andrew Zisserman
- **Comment**: ECCV workshop on Geometry meets Deep Learning
- **Journal**: None
- **Summary**: The objective of this work is to infer the 3D shape of an object from a single image. We use sculptures as our training and test bed, as these have great variety in shape and appearance.   To achieve this we build on the success of multiple view geometry (MVG) which is able to accurately provide correspondences between images of 3D objects under varying viewpoint and illumination conditions, and make the following contributions: first, we introduce a new loss function that can harness image-to-image correspondences to provide a supervisory signal to train a deep network to infer a depth map. The network is trained end-to-end by differentiating through the camera. Second, we develop a processing pipeline to automatically generate a large scale multi-view set of correspondences for training the network. Finally, we demonstrate that we can indeed obtain a depth map of a novel object from a single image for a variety of sculptures with varying shape/texture, and that the network generalises at test time to new domains (e.g. synthetic images).



### On-Orbit Smart Camera System to Observe Illuminated and Unilluminated Space Objects
- **Arxiv ID**: http://arxiv.org/abs/1809.02042v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM
- **Links**: [PDF](http://arxiv.org/pdf/1809.02042v1)
- **Published**: 2018-09-06 15:22:42+00:00
- **Updated**: 2018-09-06 15:22:42+00:00
- **Authors**: Steve Morad, Ravi Teja Nallapu, Himangshu Kalita, Byon Kwon, Vishnu Reddy, Roberto Furfaro, Erik Asphaug, Jekan Thangavelautham
- **Comment**: 12 pages, 11 figures, appears at Advanced Maui Optical and Space
  Surveillance Technologies Conference 2018
- **Journal**: None
- **Summary**: The wide availability of Commercial Off-The-Shelf (COTS) electronics that can withstand Low Earth Orbit conditions has opened avenue for wide deployment of CubeSats and small-satellites. CubeSats thanks to their low developmental and launch costs offer new opportunities for rapidly demonstrating on-orbit surveillance capabilities. In our earlier work, we proposed development of SWIMSat (Space based Wide-angle Imaging of Meteors) a 3U CubeSat demonstrator that is designed to observe illuminated objects entering the Earth's atmosphere. The spacecraft would operate autonomously using a smart camera with vision algorithms to detect, track and report of objects. Several CubeSats can track an object in a coordinated fashion to pinpoint an object's trajectory. An extension of this smart camera capability is to track unilluminated objects utilizing capabilities we have been developing to track and navigate to Near Earth Objects (NEOs). This extension enables detecting and tracking objects that can't readily be detected by humans. The system maintains a dense star map of the night sky and performs round the clock observations. Standard optical flow algorithms are used to obtain trajectories of all moving objects in the camera field of view. Through a process of elimination, certain stars maybe occluded by a transiting unilluminated object which is then used to first detect and obtain a trajectory of the object. Using multiple cameras observing the event from different points of view, it may be possible then to triangulate the position of the object in space and obtain its orbital trajectory. In this work, the performance of our space object detection algorithm coupled with a spacecraft guidance, navigation, and control system is demonstrated.



### Oblique Stripe Removal in Remote Sensing Images via Oriented Variation
- **Arxiv ID**: http://arxiv.org/abs/1809.02043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02043v1)
- **Published**: 2018-09-06 15:24:52+00:00
- **Updated**: 2018-09-06 15:24:52+00:00
- **Authors**: Xinxin Liu, Xiliang Lu, Huanfeng Shen, Qiangqiang Yuan, Liangpei Zhang
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: Destriping is a classical problem in remote sensing image processing. Although considerable effort has been made to remove stripes, few of the existing methods can eliminate stripe noise with arbitrary orientations. This situation makes the removal of oblique stripes in the higher-level remote sensing products become an unfinished and urgent issue. To overcome the challenging problem, we propose a novel destriping model which is self-adjusted to different orientations of stripe noise. First of all, the oriented variation model is designed to accomplish the stripe orientation approximation. In this model, the stripe direction is automatically estimated and then imbedded into the constraint term to depict the along-stripe smoothness of the stripe component. Mainly based on the oriented variation model, a whole destriping framework is proposed by jointly employing an L1-norm constraint and a TV regularization to separately capture the global distribution property of stripe component and the piecewise smoothness of the clean image. The qualitative and quantitative experimental results of both orientation and destriping aspects confirm the effectiveness and stability of the proposed method.



### Surface Light Field Fusion
- **Arxiv ID**: http://arxiv.org/abs/1809.02057v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1809.02057v1)
- **Published**: 2018-09-06 15:45:05+00:00
- **Updated**: 2018-09-06 15:45:05+00:00
- **Authors**: Jeong Joon Park, Richard Newcombe, Steve Seitz
- **Comment**: Project Website: http://grail.cs.washington.edu/projects/slfusion/
- **Journal**: 3DV 2018
- **Summary**: We present an approach for interactively scanning highly reflective objects with a commodity RGBD sensor. In addition to shape, our approach models the surface light field, encoding scene appearance from all directions. By factoring the surface light field into view-independent and wavelength-independent components, we arrive at a representation that can be robustly estimated with IR-equipped commodity depth sensors, and achieves high quality results.



### Memory Replay GANs: learning to generate images from new categories without forgetting
- **Arxiv ID**: http://arxiv.org/abs/1809.02058v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02058v3)
- **Published**: 2018-09-06 15:45:36+00:00
- **Updated**: 2019-09-23 09:59:38+00:00
- **Authors**: Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de Weijer, Bogdan Raducanu
- **Comment**: Appear in NeurIPS 2018
- **Journal**: None
- **Summary**: Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.



### Multiple Object Tracking in Urban Traffic Scenes with a Multiclass Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1809.02073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02073v1)
- **Published**: 2018-09-06 16:17:10+00:00
- **Updated**: 2018-09-06 16:17:10+00:00
- **Authors**: Hui-Lee Ooi, Guillaume-Alexandre Bilodeau, Nicolas Saunier, David-Alexandre Beaupré
- **Comment**: 13th International Symposium on Visual Computing (ISVC)
- **Journal**: None
- **Summary**: Multiple object tracking (MOT) in urban traffic aims to produce the trajectories of the different road users that move across the field of view with different directions and speeds and that can have varying appearances and sizes. Occlusions and interactions among the different objects are expected and common due to the nature of urban road traffic. In this work, a tracking framework employing classification label information from a deep learning detection approach is used for associating the different objects, in addition to object position and appearances. We want to investigate the performance of a modern multiclass object detector for the MOT task in traffic scenes. Results show that the object labels improve tracking performance, but that the output of object detectors are not always reliable.



### Are adversarial examples inevitable?
- **Arxiv ID**: http://arxiv.org/abs/1809.02104v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.02104v3)
- **Published**: 2018-09-06 17:26:58+00:00
- **Updated**: 2020-02-03 21:18:27+00:00
- **Authors**: Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, Tom Goldstein
- **Comment**: None
- **Journal**: International Conference on Learning Representations, 2019.
  https://openreview.net/forum?id=r1lWUoA9FQ
- **Summary**: A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier's robustness against adversarial examples.



### Deep Audio-Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.02108v2
- **DOI**: 10.1109/TPAMI.2018.2889052
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02108v2)
- **Published**: 2018-09-06 17:34:27+00:00
- **Updated**: 2018-12-22 06:14:27+00:00
- **Authors**: Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman
- **Comment**: Accepted for publication by IEEE Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: None
- **Summary**: The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) we compare two models for lip reading, one using a CTC loss, and the other using a sequence-to-sequence loss. Both models are built on top of the transformer self-attention architecture; (2) we investigate to what extent lip reading is complementary to audio speech recognition, especially when the audio signal is noisy; (3) we introduce and publicly release a new dataset for audio-visual speech recognition, LRS2-BBC, consisting of thousands of natural sentences from British television. The models that we train surpass the performance of all previous work on a lip reading benchmark dataset by a significant margin.



### Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1809.02110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02110v2)
- **Published**: 2018-09-06 17:35:39+00:00
- **Updated**: 2019-02-07 16:10:41+00:00
- **Authors**: Daan de Geus, Panagiotis Meletis, Gijs Dubbelman
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present a single network method for panoptic segmentation. This method combines the predictions from a jointly trained semantic and instance segmentation network using heuristics. Joint training is the first step towards an end-to-end panoptic segmentation network and is faster and more memory efficient than training and predicting with two networks, as done in previous work. The architecture consists of a ResNet-50 feature extractor shared by the semantic segmentation and instance segmentation branch. For instance segmentation, a Mask R-CNN type of architecture is used, while the semantic segmentation branch is augmented with a Pyramid Pooling Module. Results for this method are submitted to the COCO and Mapillary Joint Recognition Challenge 2018. Our approach achieves a PQ score of 17.6 on the Mapillary Vistas validation set and 27.2 on the COCO test-dev set.



### Labeling Panoramas with Spherical Hourglass Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.02123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02123v1)
- **Published**: 2018-09-06 17:55:11+00:00
- **Updated**: 2018-09-06 17:55:11+00:00
- **Authors**: Carlos Esteves, Kostas Daniilidis, Ameesh Makadia
- **Comment**: Accepted to the 360{\deg} Perception and Interaction Workshop at ECCV
  2018
- **Journal**: None
- **Summary**: With the recent proliferation of consumer-grade 360{\deg} cameras, it is worth revisiting visual perception challenges with spherical cameras given the potential benefit of their global field of view. To this end we introduce a spherical convolutional hourglass network (SCHN) for the dense labeling on the sphere. The SCHN is invariant to camera orientation (lifting the usual requirement for `upright' panoramic images), and its design is scalable for larger practical datasets. Initial experiments show promising results on a spherical semantic segmentation task.



### Structural Consistency and Controllability for Diverse Colorization
- **Arxiv ID**: http://arxiv.org/abs/1809.02129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.02129v1)
- **Published**: 2018-09-06 17:59:57+00:00
- **Updated**: 2018-09-06 17:59:57+00:00
- **Authors**: Safa Messaoud, David Forsyth, Alexander G. Schwing
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Colorizing a given gray-level image is an important task in the media and advertising industry. Due to the ambiguity inherent to colorization (many shades are often plausible), recent approaches started to explicitly model diversity. However, one of the most obvious artifacts, structural inconsistency, is rarely considered by existing methods which predict chrominance independently for every pixel. To address this issue, we develop a conditional random field based variational auto-encoder formulation which is able to achieve diversity while taking into account structural consistency. Moreover, we introduce a controllability mecha- nism that can incorporate external constraints from diverse sources in- cluding a user interface. Compared to existing baselines, we demonstrate that our method obtains more diverse and globally consistent coloriza- tions on the LFW, LSUN-Church and ILSVRC-2015 datasets.



### Object Hallucination in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1809.02156v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.02156v2)
- **Published**: 2018-09-06 18:25:18+00:00
- **Updated**: 2019-03-29 23:48:52+00:00
- **Authors**: Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko
- **Comment**: Rohrbach and Hendricks contributed equally; accepted to EMNLP 2018
- **Journal**: None
- **Summary**: Despite continuously improving performance, contemporary image captioning models are prone to "hallucinating" objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.



### Deep Learning for Generic Object Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1809.02165v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02165v4)
- **Published**: 2018-09-06 18:42:04+00:00
- **Updated**: 2019-08-22 04:32:38+00:00
- **Authors**: Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikäinen
- **Comment**: IJCV Minor
- **Journal**: None
- **Summary**: Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.



### Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1809.02169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02169v2)
- **Published**: 2018-09-06 18:44:56+00:00
- **Updated**: 2018-09-27 12:07:54+00:00
- **Authors**: Mohsan Alvi, Andrew Zisserman, Christoffer Nellaker
- **Comment**: Will appear in Workshop on Bias Estimation in Face Analytics, ECCV
  2018
- **Journal**: None
- **Summary**: Neural networks achieve the state-of-the-art in image classification tasks. However, they can encode spurious variations or biases that may be present in the training data. For example, training an age predictor on a dataset that is not balanced for gender can lead to gender biased predicitons (e.g. wrongly predicting that males are older if only elderly males are in the training set). We present two distinct contributions: 1) An algorithm that can remove multiple sources of variation from the feature representation of a network. We demonstrate that this algorithm can be used to remove biases from the feature representation, and thereby improve classification accuracies, when training networks on extremely biased datasets. 2) An ancestral origin database of 14,000 images of individuals from East Asia, the Indian subcontinent, sub-Saharan Africa, and Western Europe. We demonstrate on this dataset, for a number of facial attribute classification tasks, that we are able to remove racial biases from the network feature representation.



### 2PFPCE: Two-Phase Filter Pruning Based on Conditional Entropy
- **Arxiv ID**: http://arxiv.org/abs/1809.02220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02220v1)
- **Published**: 2018-09-06 21:13:00+00:00
- **Updated**: 2018-09-06 21:13:00+00:00
- **Authors**: Chuhan Min, Aosen Wang, Yiran Chen, Wenyao Xu, Xin Chen
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks~(CNNs) offer remarkable performance of classifications and regressions in many high-dimensional problems and have been widely utilized in real-word cognitive applications. However, high computational cost of CNNs greatly hinder their deployment in resource-constrained applications, real-time systems and edge computing platforms. To overcome this challenge, we propose a novel filter-pruning framework, two-phase filter pruning based on conditional entropy, namely \textit{2PFPCE}, to compress the CNN models and reduce the inference time with marginal performance degradation. In our proposed method, we formulate filter pruning process as an optimization problem and propose a novel filter selection criteria measured by conditional entropy. Based on the assumption that the representation of neurons shall be evenly distributed, we also develop a maximum-entropy filter freeze technique that can reduce over fitting. Two filter pruning strategies -- global and layer-wise strategies, are compared. Our experiment result shows that combining these two strategies can achieve a higher neural network compression ratio than applying only one of them under the same accuracy drop threshold. Two-phase pruning, that is, combining both global and layer-wise strategies, achieves 10 X FLOPs reduction and 46% inference time reduction on VGG-16, with 2% accuracy drop.



### Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images
- **Arxiv ID**: http://arxiv.org/abs/1809.02226v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02226v3)
- **Published**: 2018-09-06 21:31:48+00:00
- **Updated**: 2020-09-28 10:03:56+00:00
- **Authors**: Vedrana Andersen Dahl, Monica Jane Emerson, Camilla Himmelstrup Trinderup, Anders Bjorholm Dahl
- **Comment**: 9 pages, 7 figures, PDFLaTeX
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, June 2020
- **Summary**: Efficient and easy segmentation of images and volumes is of great practical importance. Segmentation problems that motivate our approach originate from microscopy imaging commonly used in materials science, medicine, and biology. We formulate image segmentation as a probabilistic pixel classification problem, and we apply segmentation as a step towards characterising image content. Our method allows the user to define structures of interest by interactively marking a subset of pixels. Thanks to the real-time feedback, the user can place new markings strategically, depending on the current outcome. The final pixel classification may be obtained from a very modest user input. An important ingredient of our method is a graph that encodes image content. This graph is built in an unsupervised manner during initialisation and is based on clustering of image features. Since we combine a limited amount of user-labelled data with the clustering information obtained from the unlabelled parts of the image, our method fits in the general framework of semi-supervised learning. We demonstrate how this can be a very efficient approach to segmentation through pixel classification.



### Obstacle Detection Quality as a Problem-Oriented Approach to Stereo Vision Algorithms Estimation in Road Situation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.02228v1
- **DOI**: 10.1088/1742-6596/1096/1/012035
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02228v1)
- **Published**: 2018-09-06 21:36:56+00:00
- **Updated**: 2018-09-06 21:36:56+00:00
- **Authors**: A. A. Smagina, D. A. Shepelev, E. I. Ershov, A. S. Grigoryev
- **Comment**: None
- **Journal**: IOP Conf. Series: Journal of Physics: Conf. Series 1096 (2018)
  012035
- **Summary**: In this work we present a method for performance evaluation of stereo vision based obstacle detection techniques that takes into account the specifics of road situation analysis to minimize the effort required to prepare a test dataset. This approach has been designed to be implemented in systems such as self-driving cars or driver assistance and can also be used as problem-oriented quality criterion for evaluation of stereo vision algorithms.



### Player Experience Extraction from Gameplay Video
- **Arxiv ID**: http://arxiv.org/abs/1809.06201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.06201v1)
- **Published**: 2018-09-06 22:00:46+00:00
- **Updated**: 2018-09-06 22:00:46+00:00
- **Authors**: Zijin Luo, Matthew Guzdial, Nicholas Liao, Mark Riedl
- **Comment**: 8 pages, 6 figures, AIIDE 2018
- **Journal**: None
- **Summary**: The ability to extract the sequence of game events for a given player's play-through has traditionally required access to the game's engine or source code. This serves as a barrier to researchers, developers, and hobbyists who might otherwise benefit from these game logs. In this paper we present two approaches to derive game logs from game video via convolutional neural networks and transfer learning. We evaluate the approaches in a Super Mario Bros. clone, Mega Man and Skyrim. Our results demonstrate our approach outperforms random forest and other transfer baselines.




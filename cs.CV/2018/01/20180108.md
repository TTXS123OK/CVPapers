# Arxiv Papers in cs.CV on 2018-01-08
### Identity-preserving Face Recovery from Portraits
- **Arxiv ID**: http://arxiv.org/abs/1801.02279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02279v2)
- **Published**: 2018-01-08 00:25:35+00:00
- **Updated**: 2018-02-05 23:17:49+00:00
- **Authors**: Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, Piotr Koniusz
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering the latent photorealistic faces from their artistic portraits aids human perception and facial analysis. However, a recovery process that can preserve identity is challenging because the fine details of real faces can be distorted or lost in stylized images. In this paper, we present a new Identity-preserving Face Recovery from Portraits (IFRP) to recover latent photorealistic faces from unaligned stylized portraits. Our IFRP method consists of two components: Style Removal Network (SRN) and Discriminative Network (DN). The SRN is designed to transfer feature maps of stylized images to the feature maps of the corresponding photorealistic faces. By embedding spatial transformer networks into the SRN, our method can compensate for misalignments of stylized faces automatically and output aligned realistic face images. The role of the DN is to enforce recovered faces to be similar to authentic faces. To ensure the identity preservation, we promote the recovered and ground-truth faces to share similar visual features via a distance measure which compares features of recovered and ground-truth faces extracted from a pre-trained VGG network. We evaluate our method on a large-scale synthesized dataset of real and stylized face pairs and attain state of the art results. In addition, our method can recover photorealistic faces from previously unseen stylized portraits, original paintings and human-drawn sketches.



### Long-term Multi-granularity Deep Framework for Driver Drowsiness Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.02325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02325v1)
- **Published**: 2018-01-08 07:21:46+00:00
- **Updated**: 2018-01-08 07:21:46+00:00
- **Authors**: Jie Lyu, Zejian Yuan, Dapeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: For real-world driver drowsiness detection from videos, the variation of head pose is so large that the existing methods on global face is not capable of extracting effective features, such as looking aside and lowering head. Temporal dependencies with variable length are also rarely considered by the previous approaches, e.g., yawning and speaking. In this paper, we propose a Long-term Multi-granularity Deep Framework to detect driver drowsiness in driving videos containing the frontal faces. The framework includes two key components: (1) Multi-granularity Convolutional Neural Network (MCNN), a novel network utilizes a group of parallel CNN extractors on well-aligned facial patches of different granularities, and extracts facial representations effectively for large variation of head pose, furthermore, it can flexibly fuse both detailed appearance clues of the main parts and local to global spatial constraints; (2) a deep Long Short Term Memory network is applied on facial representations to explore long-term relationships with variable length over sequential frames, which is capable to distinguish the states with temporal dependencies, such as blinking and closing eyes. Our approach achieves 90.05% accuracy and about 37 fps speed on the evaluation set of the public NTHU-DDD dataset, which is the state-of-the-art method on driver drowsiness detection. Moreover, we build a new dataset named FI-DDD, which is of higher precision of drowsy locations in temporal dimension.



### Synthetic Data Augmentation using GAN for Improved Liver Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.02385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02385v1)
- **Published**: 2018-01-08 11:20:36+00:00
- **Updated**: 2018-01-08 11:20:36+00:00
- **Authors**: Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan
- **Comment**: To be presented at IEEE International Symposium on Biomedical Imaging
  (ISBI), 2018
- **Journal**: None
- **Summary**: In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity.



### DeepStyle: Multimodal Search Engine for Fashion and Interior Design
- **Arxiv ID**: http://arxiv.org/abs/1801.03002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03002v2)
- **Published**: 2018-01-08 14:08:55+00:00
- **Updated**: 2019-02-20 12:44:10+00:00
- **Authors**: Ivona Tautkute, Tomasz Trzcinski, Aleksander Skorupa, Lukasz Brocki, Krzysztof Marasek
- **Comment**: Copyright held by IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: IEEE Access 2019
- **Summary**: In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21% on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.



### Deep Crisp Boundaries: From Boundaries to Higher-level Tasks
- **Arxiv ID**: http://arxiv.org/abs/1801.02439v3
- **DOI**: 10.1109/TIP.2018.2874279
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02439v3)
- **Published**: 2018-01-08 14:30:21+00:00
- **Updated**: 2018-10-09 03:51:08+00:00
- **Authors**: Yupei Wang, Xin Zhao, Yin Li, Kaiqi Huang
- **Comment**: Accepted by IEEE Trans. on Image Processing
- **Journal**: None
- **Summary**: Edge detection has made significant progress with the help of deep Convolutional Networks (ConvNet). These ConvNet based edge detectors have approached human level performance on standard benchmarks. We provide a systematical study of these detectors' outputs. We show that the detection results did not accurately localize edge pixels, which can be adversarial for tasks that require crisp edge inputs. As a remedy, we propose a novel refinement architecture to address the challenging problem of learning a crisp edge detector using ConvNet. Our method leverages a top-down backward refinement pathway, and progressively increases the resolution of feature maps to generate crisp edges. Our results achieve superior performance, surpassing human accuracy when using standard criteria on BSDS500, and largely outperforming state-of-the-art methods when using more strict criteria. More importantly, we demonstrate the benefit of crisp edge maps for several important applications in computer vision, including optical flow estimation, object proposal generation and semantic segmentation.



### Ensemble One-dimensional Convolution Neural Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.02475v2
- **DOI**: 10.1109/LSP.2018.2841649
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02475v2)
- **Published**: 2018-01-08 15:12:54+00:00
- **Updated**: 2018-01-13 06:22:55+00:00
- **Authors**: Yangyang Xu, Lei Wang
- **Comment**: the title of Table 3 has something wrong and the expermient is not
  enough
- **Journal**: None
- **Summary**: In this paper, we proposed a effective but extensible residual one-dimensional convolution neural network as base network, based on the this network, we proposed four subnets to explore the features of skeleton sequences from each aspect. Given a skeleton sequences, the spatial information are encoded into the skeleton joints coordinate in a frame and the temporal information are present by multiple frames. Limited by the skeleton sequence representations, two-dimensional convolution neural network cannot be used directly, we chose one-dimensional convolution layer as the basic layer. Each sub network could extract discriminative features from different aspects. Our first subnet is a two-stream network which could explore both temporal and spatial information. The second is a body-parted network, which could gain micro spatial features and macro temporal features. The third one is an attention network, the main contribution of which is to focus the key frames and feature channels which high related with the action classes in a skeleton sequence. One frame-difference network, as the last subnet, mainly processes the joints changes between the consecutive frames. Four subnets ensemble together by late fusion, the key problem of ensemble method is each subnet should have a certain performance and between the subnets, there are diversity existing. Each subnet shares a wellperformance basenet and differences between subnets guaranteed the diversity. Experimental results show that the ensemble network gets a state-of-the-art performance on three widely used datasets.



### Bridging the Gap: Simultaneous Fine Tuning for Data Re-Balancing
- **Arxiv ID**: http://arxiv.org/abs/1801.02548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02548v1)
- **Published**: 2018-01-08 16:44:38+00:00
- **Updated**: 2018-01-08 16:44:38+00:00
- **Authors**: John McKay, Isaac Gerg, Vishal Monga
- **Comment**: Submitted to IGARSS 2018, 4 Pages, 8 Figures
- **Journal**: None
- **Summary**: There are many real-world classification problems wherein the issue of data imbalance (the case when a data set contains substantially more samples for one/many classes than the rest) is unavoidable. While under-sampling the problematic classes is a common solution, this is not a compelling option when the large data class is itself diverse and/or the limited data class is especially small. We suggest a strategy based on recent work concerning limited data problems which utilizes a supplemental set of images with similar properties to the limited data class to aid in the training of a neural network. We show results for our model against other typical methods on a real-world synthetic aperture sonar data set. Code can be found at github.com/JohnMcKay/dataImbalance.



### Unsupervised Discovery of Toxoplasma gondii Motility Phenotypes
- **Arxiv ID**: http://arxiv.org/abs/1801.02591v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1801.02591v2)
- **Published**: 2018-01-08 18:04:00+00:00
- **Updated**: 2018-01-11 19:04:18+00:00
- **Authors**: Mojtaba S. Fazli, Stephen A. Vella, Silvia N. J. Moreno, Shannon Quinn
- **Comment**: 4 pages, Accepted to 2018 IEEE International Symposium on Biomedical
  Imaging
- **Journal**: None
- **Summary**: Toxoplasma gondii is a parasitic protozoan that causes dis- seminated toxoplasmosis, a disease that afflicts roughly a third of the worlds population. Its virulence is predicated on its motility and ability to enter and exit nucleated cells; therefore, studies elucidating its mechanism of motility and in particular, its motility patterns in the context of its lytic cycle, are critical to the eventual development of therapeutic strate- gies. Here, we present an end-to-end computational pipeline for identifying T. gondii motility phenotypes in a completely unsupervised, data-driven way. We track the parasites before and after addition of extracellular Ca2+ to study its effects on the parasite motility patterns and use this information to parameterize the motion and group it according to similarity of spatiotemporal dynamics.



### LaVAN: Localized and Visible Adversarial Noise
- **Arxiv ID**: http://arxiv.org/abs/1801.02608v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.02608v2)
- **Published**: 2018-01-08 18:44:23+00:00
- **Updated**: 2018-03-01 12:49:11+00:00
- **Authors**: Danny Karmon, Daniel Zoran, Yoav Goldberg
- **Comment**: None
- **Journal**: None
- **Summary**: Most works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates.



### Generating Adversarial Examples with Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.02610v5
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02610v5)
- **Published**: 2018-01-08 18:50:13+00:00
- **Updated**: 2019-02-14 15:53:22+00:00
- **Authors**: Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song
- **Comment**: Accepted to IJCAI2018
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.



### Spatially Transformed Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1801.02612v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02612v2)
- **Published**: 2018-01-08 18:51:59+00:00
- **Updated**: 2018-01-09 19:03:32+00:00
- **Authors**: Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies show that widely used deep neural networks (DNNs) are vulnerable to carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\mathcal{L}_p$ distance for penalizing perturbations. Researchers have explored different defense methods to defend against such adversarial attacks. While the effectiveness of $\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted.



### Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality
- **Arxiv ID**: http://arxiv.org/abs/1801.02613v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.02613v3)
- **Published**: 2018-01-08 18:54:40+00:00
- **Updated**: 2018-03-14 07:24:10+00:00
- **Authors**: Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E. Houle, James Bailey
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the distinction of adversarial examples generated using state-of-the-art attacks. As a proof-of-concept, we show that a potential application of LID is to distinguish adversarial examples, and the preliminary results show that it can outperform several state-of-the-art detection measures by large margins for five attack strategies considered in this paper across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.



### Boundary Optimizing Network (BON)
- **Arxiv ID**: http://arxiv.org/abs/1801.02642v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02642v3)
- **Published**: 2018-01-08 19:02:44+00:00
- **Updated**: 2018-01-23 10:24:09+00:00
- **Authors**: Marco Singh, Akshay Pai
- **Comment**: None
- **Journal**: None
- **Summary**: Despite all the success that deep neural networks have seen in classifying certain datasets, the challenge of finding optimal solutions that generalize still remains. In this paper, we propose the Boundary Optimizing Network (BON), a new approach to generalization for deep neural networks when used for supervised learning. Given a classification network, we propose to use a collaborative generative network that produces new synthetic data points in the form of perturbations of original data points. In this way, we create a data support around each original data point which prevents decision boundaries from passing too close to the original data points, i.e. prevents overfitting. We show that BON improves convergence on CIFAR-10 using the state-of-the-art Densenet. We do however observe that the generative network suffers from catastrophic forgetting during training, and we therefore propose to use a variation of Memory Aware Synapses to optimize the generative network (called BON++). On the Iris dataset, we visualize the effect of BON++ when the generator does not suffer from catastrophic forgetting and conclude that the approach has the potential to create better boundaries in a higher dimensional space.



### Generative Sensing: Transforming Unreliable Sensor Data for Reliable Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.02684v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1801.02684v1)
- **Published**: 2018-01-08 20:57:34+00:00
- **Updated**: 2018-01-08 20:57:34+00:00
- **Authors**: Lina Karam, Tejas Borkar, Yu Cao, Junseok Chae
- **Comment**: 5 pages, Submitted to IEEE MIPR 2018
- **Journal**: None
- **Summary**: This paper introduces a deep learning enabled generative sensing framework which integrates low-end sensors with computational intelligence to attain a high recognition accuracy on par with that attained with high-end sensors. The proposed generative sensing framework aims at transforming low-end, low-quality sensor data into higher quality sensor data in terms of achieved classification accuracy. The low-end data can be transformed into higher quality data of the same modality or into data of another modality. Different from existing methods for image generation, the proposed framework is based on discriminative models and targets to maximize the recognition accuracy rather than a similarity measure. This is achieved through the introduction of selective feature regeneration in a deep neural network (DNN). The proposed generative sensing will essentially transform low-quality sensor data into high-quality information for robust perception. Results are presented to illustrate the performance of the proposed framework.



### Towards Multi-Object Detection and Tracking in Urban Scenario under Uncertainties
- **Arxiv ID**: http://arxiv.org/abs/1801.02686v2
- **DOI**: 10.5220/0006706101560167
- **Categories**: **cs.CV**, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1801.02686v2)
- **Published**: 2018-01-08 21:05:10+00:00
- **Updated**: 2018-02-03 17:51:59+00:00
- **Authors**: Achim Kampker, Mohsen Sefati, Arya Abdul Rachman, Kai Kreisk√∂ther, Pascual Campoy
- **Comment**: Some significant editorial/editing issues are found upon review.
  Paper will undergo language re-proofing before resubmitted
- **Journal**: 4th.VEHITS.Proc. 109 (2018) 156-167
- **Summary**: Urban-oriented autonomous vehicles require a reliable perception technology to tackle the high amount of uncertainties. The recently introduced compact 3D LIDAR sensor offers a surround spatial information that can be exploited to enhance the vehicle perception. We present a real-time integrated framework of multi-target object detection and tracking using 3D LIDAR geared toward urban use. Our approach combines sensor occlusion-aware detection method with computationally efficient heuristics rule-based filtering and adaptive probabilistic tracking to handle uncertainties arising from sensing limitation of 3D LIDAR and complexity of the target object movement. The evaluation results using real-world pre-recorded 3D LIDAR data and comparison with state-of-the-art works shows that our framework is capable of achieving promising tracking performance in the urban situation.



### Deep Supervision with Intermediate Concepts
- **Arxiv ID**: http://arxiv.org/abs/1801.03399v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03399v2)
- **Published**: 2018-01-08 22:00:48+00:00
- **Updated**: 2018-07-20 04:10:35+00:00
- **Authors**: Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D. Hager, Manmohan Chandraker
- **Comment**: Submitted to TPAMI, first revision. arXiv admin note: text overlap
  with arXiv:1612.02699
- **Journal**: None
- **Summary**: Recent data-driven approaches to scene interpretation predominantly pose inference as an end-to-end black-box mapping, commonly performed by a Convolutional Neural Network (CNN). However, decades of work on perceptual organization in both human and machine vision suggests that there are often intermediate representations that are intrinsic to an inference task, and which provide essential structure to improve generalization. In this work, we explore an approach for injecting prior domain structure into neural network training by supervising hidden layers of a CNN with intermediate concepts that normally are not observed in practice. We formulate a probabilistic framework which formalizes these notions and predicts improved generalization via this deep supervision method. One advantage of this approach is that we are able to train only from synthetic CAD renderings of cluttered scenes, where concept values can be extracted, but apply the results to real images. Our implementation achieves the state-of-the-art performance of 2D/3D keypoint localization and image classification on real image benchmarks, including KITTI, PASCAL VOC, PASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach outperforms alternative forms of supervision, such as multi-task networks.



### End-to-end detection-segmentation network with ROI convolution
- **Arxiv ID**: http://arxiv.org/abs/1801.02722v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02722v2)
- **Published**: 2018-01-08 23:34:45+00:00
- **Updated**: 2019-12-02 22:46:51+00:00
- **Authors**: Zichen Zhang, Min Tang, Dana Cobzas, Dornoosh Zonoobi, Martin Jagersand, Jacob L. Jaremko
- **Comment**: ISBI 2018
- **Journal**: None
- **Summary**: We propose an end-to-end neural network that improves the segmentation accuracy of fully convolutional networks by incorporating a localization unit. This network performs object localization first, which is then used as a cue to guide the training of the segmentation network. We test the proposed method on a segmentation task of small objects on a clinical dataset of ultrasound images. We show that by jointly learning for detection and segmentation, the proposed network is able to improve the segmentation accuracy compared to only learning for segmentation. Code is publicly available at https://github.com/vincentzhang/roi-fcn.



### Brain MRI Super Resolution Using 3D Deep Densely Connected Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.02728v1
- **DOI**: 10.1109/ISBI.2018.8363679
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1801.02728v1)
- **Published**: 2018-01-08 23:56:32+00:00
- **Updated**: 2018-01-08 23:56:32+00:00
- **Authors**: Yuhua Chen, Yibin Xie, Zhengwei Zhou, Feng Shi, Anthony G. Christodoulou, Debiao Li
- **Comment**: Accepted by ISBI'18
- **Journal**: None
- **Summary**: Magnetic resonance image (MRI) in high spatial resolution provides detailed anatomical information and is often necessary for accurate quantitative analysis. However, high spatial resolution typically comes at the expense of longer scan time, less spatial coverage, and lower signal to noise ratio (SNR). Single Image Super-Resolution (SISR), a technique aimed to restore high-resolution (HR) details from one single low-resolution (LR) input image, has been improved dramatically by recent breakthroughs in deep learning. In this paper, we introduce a new neural network architecture, 3D Densely Connected Super-Resolution Networks (DCSRN) to restore HR features of structural brain MR images. Through experiments on a dataset with 1,113 subjects, we demonstrate that our network outperforms bicubic interpolation as well as other deep learning methods in restoring 4x resolution-reduced images.




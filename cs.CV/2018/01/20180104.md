# Arxiv Papers in cs.CV on 2018-01-04
### Facial Attributes: Accuracy and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1801.02480v2
- **DOI**: 10.1016/j.patrec.2017.10.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02480v2)
- **Published**: 2018-01-04 00:53:16+00:00
- **Updated**: 2018-04-20 16:11:40+00:00
- **Authors**: Andras Rozsa, Manuel Günther, Ethan M. Rudd, Terrance E. Boult
- **Comment**: arXiv admin note: text overlap with arXiv:1605.05411
- **Journal**: Pattern Recognition Letters, 2017, ISSN 0167-8655
- **Summary**: Facial attributes, emerging soft biometrics, must be automatically and reliably extracted from images in order to be usable in stand-alone systems. While recent methods extract facial attributes using deep neural networks (DNNs) trained on labeled facial attribute data, the robustness of deep attribute representations has not been evaluated. In this paper, we examine the representational stability of several approaches that recently advanced the state of the art on the CelebA benchmark by generating adversarial examples formed by adding small, non-random perturbations to inputs yielding altered classifications. We show that our fast flipping attribute (FFA) technique generates more adversarial examples than traditional algorithms, and that the adversarial robustness of DNNs varies highly between facial attributes. We also test the correlation of facial attributes and find that only for related attributes do the formed adversarial perturbations change the classification of others. Finally, we introduce the concept of natural adversarial samples, i.e., misclassified images where predictions can be corrected via small perturbations. We demonstrate that natural adversarial samples commonly occur and show that many of these images remain misclassified even with additional training epochs, even though their correct classification may require only a small adjustment to network parameters.



### Depth Not Needed - An Evaluation of RGB-D Feature Encodings for Off-Road Scene Understanding by Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1801.01235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01235v1)
- **Published**: 2018-01-04 03:03:45+00:00
- **Updated**: 2018-01-04 03:03:45+00:00
- **Authors**: Christopher J. Holder, Toby P. Breckon, Xiong Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Scene understanding for autonomous vehicles is a challenging computer vision task, with recent advances in convolutional neural networks (CNNs) achieving results that notably surpass prior traditional feature driven approaches. However, limited work investigates the application of such methods either within the highly unstructured off-road environment or to RGBD input data. In this work, we take an existing CNN architecture designed to perform semantic segmentation of RGB images of urban road scenes, then adapt and retrain it to perform the same task with multichannel RGBD images obtained under a range of challenging off-road conditions. We compare two different stereo matching algorithms and five different methods of encoding depth information, including disparity, local normal orientation and HHA (horizontal disparity, height above ground plane, angle with gravity), to create a total of ten experimental variations of our dataset, each of which is used to train and test a CNN so that classification performance can be evaluated against a CNN trained using standard RGB input.



### Crossing Generative Adversarial Networks for Cross-View Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1801.01760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01760v1)
- **Published**: 2018-01-04 03:52:15+00:00
- **Updated**: 2018-01-04 03:52:15+00:00
- **Authors**: Chengyuan Zhang, Lin Wu, Yang Wang
- **Comment**: 12 pages. arXiv admin note: text overlap with arXiv:1702.03431 by
  other authors
- **Journal**: None
- **Summary**: Person re-identification (\textit{re-id}) refers to matching pedestrians across disjoint yet non-overlapping camera views. The most effective way to match these pedestrians undertaking significant visual variations is to seek reliably invariant features that can describe the person of interest faithfully. Most of existing methods are presented in a supervised manner to produce discriminative features by relying on labeled paired images in correspondence. However, annotating pair-wise images is prohibitively expensive in labors, and thus not practical in large-scale networked cameras. Moreover, seeking comparable representations across camera views demands a flexible model to address the complex distributions of images. In this work, we study the co-occurrence statistic patterns between pairs of images, and propose to crossing Generative Adversarial Network (Cross-GAN) for learning a joint distribution for cross-image representations in a unsupervised manner. Given a pair of person images, the proposed model consists of the variational auto-encoder to encode the pair into respective latent variables, a proposed cross-view alignment to reduce the view disparity, and an adversarial layer to seek the joint distribution of latent representations. The learned latent representations are well-aligned to reflect the co-occurrence patterns of paired images. We empirically evaluate the proposed model against challenging datasets, and our results show the importance of joint invariant features in improving matching rates of person re-id with comparison to semi/unsupervised state-of-the-arts.



### Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner
- **Arxiv ID**: http://arxiv.org/abs/1801.01258v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.01258v1)
- **Published**: 2018-01-04 06:35:53+00:00
- **Updated**: 2018-01-04 06:35:53+00:00
- **Authors**: Yoseob Han, Jingu Kang, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: For homeland and transportation security applications, 2D X-ray explosive detection system (EDS) have been widely used, but they have limitations in recognizing 3D shape of the hidden objects. Among various types of 3D computed tomography (CT) systems to address this issue, this paper is interested in a stationary CT using fixed X-ray sources and detectors. However, due to the limited number of projection views, analytic reconstruction algorithms produce severe streaking artifacts. Inspired by recent success of deep learning approach for sparse view CT reconstruction, here we propose a novel image and sinogram domain deep learning architecture for 3D reconstruction from very sparse view measurement. The algorithm has been tested with the real data from a prototype 9-view dual energy stationary CT EDS carry-on baggage scanner developed by GEMSS Medical Systems, Korea, which confirms the superior reconstruction performance over the existing approaches.



### Cross-domain Human Parsing via Adversarial Feature and Label Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1801.01260v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1801.01260v2)
- **Published**: 2018-01-04 06:55:59+00:00
- **Updated**: 2018-01-08 03:25:07+00:00
- **Authors**: Si Liu, Yao Sun, Defa Zhu, Guanghui Ren, Yu Chen, Jiashi Feng, Jizhong Han
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: Human parsing has been extensively studied recently due to its wide applications in many important scenarios. Mainstream fashion parsing models focus on parsing the high-resolution and clean images. However, directly applying the parsers trained on benchmarks to a particular application scenario in the wild, e.g., a canteen, airport or workplace, often gives non-satisfactory performance due to domain shift. In this paper, we explore a new and challenging cross-domain human parsing problem: taking the benchmark dataset with extensive pixel-wise labeling as the source domain, how to obtain a satisfactory parser on a new target domain without requiring any additional manual labeling? To this end, we propose a novel and efficient cross-domain human parsing model to bridge the cross-domain differences in terms of visual appearance and environment conditions and fully exploit commonalities across domains. Our proposed model explicitly learns a feature compensation network, which is specialized for mitigating the cross-domain differences. A discriminative feature adversarial network is introduced to supervise the feature compensation to effectively reduce the discrepancy between feature distributions of two domains. Besides, our model also introduces a structured label adversarial network to guide the parsing results of the target domain to follow the high-order relationships of the structured labels shared across domains. The proposed framework is end-to-end trainable, practical and scalable in real applications. Extensive experiments are conducted where LIP dataset is the source domain and 4 different datasets including surveillance videos, movies and runway shows are evaluated as target domains. The results consistently confirm data efficiency and performance advantages of the proposed method for the cross-domain human parsing problem.



### ICFVR 2017: 3rd International Competition on Finger Vein Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.01262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01262v1)
- **Published**: 2018-01-04 07:14:32+00:00
- **Updated**: 2018-01-04 07:14:32+00:00
- **Authors**: Yi Zhang, Houjun Huang, Haifeng Zhang, Liao Ni, Wei Xu, Nasir Uddin Ahmed, Md. Shakil Ahmed, Yilun Jin, Yingjie Chen, Jingxuan Wen, Wenxin Li
- **Comment**: 8 pages, 15 figures
- **Journal**: None
- **Summary**: In recent years, finger vein recognition has become an important sub-field in biometrics and been applied to real-world applications. The development of finger vein recognition algorithms heavily depends on large-scale real-world data sets. In order to motivate research on finger vein recognition, we released the largest finger vein data set up to now and hold finger vein recognition competitions based on our data set every year. In 2017, International Competition on Finger Vein Recognition(ICFVR) is held jointly with IJCB 2017. 11 teams registered and 10 of them joined the final evaluation. The winner of this year dramatically improved the EER from 2.64% to 0.483% compared to the winner of last year. In this paper, we introduce the process and results of ICFVR 2017 and give insights on development of state-of-art finger vein recognition algorithms.



### Object segmentation in depth maps with one user click and a synthetically trained fully convolutional network
- **Arxiv ID**: http://arxiv.org/abs/1801.01281v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.01281v2)
- **Published**: 2018-01-04 09:13:20+00:00
- **Updated**: 2018-09-24 09:06:37+00:00
- **Authors**: Matthieu Grard, Romain Brégier, Florian Sella, Emmanuel Dellandréa, Liming Chen
- **Comment**: This is a pre-print of an article published in Human Friendly
  Robotics, 10th International Workshop, Springer Proceedings in Advanced
  Robotics, vol 7. The final authenticated version is available online at:
  https://doi.org/10.1007/978-3-319-89327-3\_16, Springer Proceedings in
  Advanced Robotics, Siciliano Bruno, Khatib Oussama, In press, Human Friendly
  Robotics, 10th International Workshop, 7
- **Journal**: None
- **Summary**: With more and more household objects built on planned obsolescence and consumed by a fast-growing population, hazardous waste recycling has become a critical challenge. Given the large variability of household waste, current recycling platforms mostly rely on human operators to analyze the scene, typically composed of many object instances piled up in bulk. Helping them by robotizing the unitary extraction is a key challenge to speed up this tedious process. Whereas supervised deep learning has proven very efficient for such object-level scene understanding, e.g., generic object detection and segmentation in everyday scenes, it however requires large sets of per-pixel labeled images, that are hardly available for numerous application contexts, including industrial robotics. We thus propose a step towards a practical interactive application for generating an object-oriented robotic grasp, requiring as inputs only one depth map of the scene and one user click on the next object to extract. More precisely, we address in this paper the middle issue of object seg-mentation in top views of piles of bulk objects given a pixel location, namely seed, provided interactively by a human operator. We propose a twofold framework for generating edge-driven instance segments. First, we repurpose a state-of-the-art fully convolutional object contour detector for seed-based instance segmentation by introducing the notion of edge-mask duality with a novel patch-free and contour-oriented loss function. Second, we train one model using only synthetic scenes, instead of manually labeled training data. Our experimental results show that considering edge-mask duality for training an encoder-decoder network, as we suggest, outperforms a state-of-the-art patch-based network in the present application context.



### PixelLink: Detecting Scene Text via Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.01315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01315v1)
- **Published**: 2018-01-04 11:48:21+00:00
- **Updated**: 2018-01-04 11:48:21+00:00
- **Authors**: Dan Deng, Haifeng Liu, Xuelong Li, Deng Cai
- **Comment**: AAAI-2018
- **Journal**: None
- **Summary**: Most state-of-the-art scene text detection algorithms are deep learning based methods that depend on bounding box regression and perform at least two kinds of predictions: text/non-text classification and location regression. Regression plays a key role in the acquisition of bounding boxes in these methods, but it is not indispensable because text/non-text prediction can also be considered as a kind of semantic segmentation that contains full location information in itself. However, text instances in scene images often lie very close to each other, making them very difficult to separate via semantic segmentation. Therefore, instance segmentation is needed to address this problem. In this paper, PixelLink, a novel scene text detection algorithm based on instance segmentation, is proposed. Text instances are first segmented out by linking pixels within the same instance together. Text bounding boxes are then extracted directly from the segmentation result without location regression. Experiments show that, compared with regression-based methods, PixelLink can achieve better or comparable performance on several benchmarks, while requiring many fewer training iterations and less training data.



### Text Extraction and Retrieval from Smartphone Screenshots: Building a Repository for Life in Media
- **Arxiv ID**: http://arxiv.org/abs/1801.01316v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.DL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1801.01316v1)
- **Published**: 2018-01-04 11:51:26+00:00
- **Updated**: 2018-01-04 11:51:26+00:00
- **Authors**: Agnese Chiatti, Mu Jung Cho, Anupriya Gagneja, Xiao Yang, Miriam Brinberg, Katie Roehrick, Sagnik Ray Choudhury, Nilam Ram, Byron Reeves, C. Lee Giles
- **Comment**: None
- **Journal**: None
- **Summary**: Daily engagement in life experiences is increasingly interwoven with mobile device use. Screen capture at the scale of seconds is being used in behavioral studies and to implement "just-in-time" health interventions. The increasing psychological breadth of digital information will continue to make the actual screens that people view a preferred if not required source of data about life experiences. Effective and efficient Information Extraction and Retrieval from digital screenshots is a crucial prerequisite to successful use of screen data. In this paper, we present the experimental workflow we exploited to: (i) pre-process a unique collection of screen captures, (ii) extract unstructured text embedded in the images, (iii) organize image text and metadata based on a structured schema, (iv) index the resulting document collection, and (v) allow for Image Retrieval through a dedicated vertical search engine application. The adopted procedure integrates different open source libraries for traditional image processing, Optical Character Recognition (OCR), and Image Retrieval. Our aim is to assess whether and how state-of-the-art methodologies can be applied to this novel data set. We show how combining OpenCV-based pre-processing modules with a Long short-term memory (LSTM) based release of Tesseract OCR, without ad hoc training, led to a 74% character-level accuracy of the extracted text. Further, we used the processed repository as baseline for a dedicated Image Retrieval system, for the immediate use and application for behavioral and prevention scientists. We discuss issues of Text Information Extraction and Retrieval that are particular to the screenshot image case and suggest important future work.



### Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions
- **Arxiv ID**: http://arxiv.org/abs/1801.01317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01317v1)
- **Published**: 2018-01-04 11:55:55+00:00
- **Updated**: 2018-01-04 11:55:55+00:00
- **Authors**: Tao Yang, Yan Wu, Junqiao Zhao, Linting Guan
- **Comment**: 16 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Semantic image segmentation is one of the most challenged tasks in computer vision. In this paper, we propose a highly fused convolutional network, which consists of three parts: feature downsampling, combined feature upsampling and multiple predictions. We adopt a strategy of multiple steps of upsampling and combined feature maps in pooling layers with its corresponding unpooling layers. Then we bring out multiple pre-outputs, each pre-output is generated from an unpooling layer by one-step upsampling. Finally, we concatenate these pre-outputs to get the final output. As a result, our proposed network makes highly use of the feature information by fusing and reusing feature maps. In addition, when training our model, we add multiple soft cost functions on pre-outputs and final outputs. In this way, we can reduce the loss reduction when the loss is back propagated. We evaluate our model on three major segmentation datasets: CamVid, PASCAL VOC and ADE20K. We achieve a state-of-the-art performance on CamVid dataset, as well as considerable improvements on PASCAL VOC dataset and ADE20K dataset



### A fully automated framework for lung tumour detection, segmentation and analysis
- **Arxiv ID**: http://arxiv.org/abs/1801.01402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01402v1)
- **Published**: 2018-01-04 15:26:24+00:00
- **Updated**: 2018-01-04 15:26:24+00:00
- **Authors**: Devesh Walawalkar
- **Comment**: None
- **Journal**: None
- **Summary**: Early and correct diagnosis is a very important aspect of cancer treatment. Detection of tumour in Computed Tomography scan is a tedious and tricky task which requires expert knowledge and a lot of human working hours. As small human error is present in any work he does, it is possible that a CT scan could be misdiagnosed causing the patient to become terminal. This paper introduces a novel fully automated framework which helps to detect and segment tumour, if present in a lung CT scan series. It also provides useful analysis of the detected tumour such as its approximate volume, centre location and more. The framework provides a single click solution which analyses all CT images of a single patient series in one go. It helps to reduce the work of manually going through each CT slice and provides quicker and more accurate tumour diagnosis. It makes use of customized image processing and image segmentation methods, to detect and segment the prospective tumour region from the CT scan. It then uses a trained ensemble classifier to correctly classify the segmented region as being tumour or not. Tumour analysis further computed can then be used to determine malignity of the tumour. With an accuracy of 98.14%, the implemented framework can be used in various practical scenarios, capable of eliminating need of any expert pathologist intervention.



### What have we learned from deep representations for action recognition?
- **Arxiv ID**: http://arxiv.org/abs/1801.01415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01415v1)
- **Published**: 2018-01-04 15:47:47+00:00
- **Updated**: 2018-01-04 15:47:47+00:00
- **Authors**: Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes, Andrew Zisserman
- **Comment**: This document is best viewed in Adobe Reader where figures play on
  click. Supplementary material can be downloaded at
  http://feichtenhofer.github.io/action_vis.pdf
- **Journal**: None
- **Summary**: As the success of deep models has led to their deployment in all areas of computer vision, it is increasingly important to understand how these representations work and what they are capturing. In this paper, we shed light on deep spatiotemporal representations by visualizing what two-stream models have learned in order to recognize actions in video. We show that local detectors for appearance and motion objects arise to form distributed representations for recognizing human actions. Key observations include the following. First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features. Second, the networks can learn local representations that are highly class specific, but also generic representations that can serve a range of classes. Third, throughout the hierarchy of the network, features become more abstract and show increasing invariance to aspects of the data that are unimportant to desired distinctions (e.g. motion patterns across various speeds). Fourth, visualizations can be used not only to shed light on learned representations, but also to reveal idiosyncracies of training data and to explain failure cases of the system.



### SmartTennisTV: Automatic indexing of tennis videos
- **Arxiv ID**: http://arxiv.org/abs/1801.01430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1801.01430v1)
- **Published**: 2018-01-04 16:38:55+00:00
- **Updated**: 2018-01-04 16:38:55+00:00
- **Authors**: Anurag Ghosh, C. V. Jawahar
- **Comment**: 10 pages, 4 figures, NCVPRIPG 2017 Accepted Paper (Best Paper Award
  Winner)
- **Journal**: None
- **Summary**: In this paper, we demonstrate a score based indexing approach for tennis videos. Given a broadcast tennis video (BTV), we index all the video segments with their scores to create a navigable and searchable match. Our approach temporally segments the rallies in the video and then recognizes the scores from each of the segments, before refining the scores using the knowledge of the tennis scoring system. We finally build an interface to effortlessly retrieve and view the relevant video segments by also automatically tagging the segmented rallies with human accessible tags such as 'fault' and 'deuce'. The efficiency of our approach is demonstrated on BTV's from two major tennis tournaments.



### A Large Dataset for Improving Patch Matching
- **Arxiv ID**: http://arxiv.org/abs/1801.01466v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01466v3)
- **Published**: 2018-01-04 17:37:45+00:00
- **Updated**: 2018-04-17 14:31:04+00:00
- **Authors**: Rahul Mitra, Nehal Doiphode, Utkarsh Gautam, Sanath Narayan, Shuaib Ahmed, Sharat Chandran, Arjun Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new dataset for learning local image descriptors which can be used for significantly improved patch matching. Our proposed dataset consists of an order of magnitude more number of scenes, images, and positive and negative correspondences compared to the currently available Multi-View Stereo (MVS) dataset from Brown et al. The new dataset also has better coverage of the overall viewpoint, scale, and lighting changes in comparison to the MVS dataset. Our dataset also provides supplementary information like RGB patches with scale and rotations values, and intrinsic and extrinsic camera parameters which as shown later can be used to customize training data as per application. We train an existing state-of-the-art model on our dataset and evaluate on publicly available benchmarks such as HPatches dataset and Strecha et al.\cite{strecha} to quantify the image descriptor performance. Experimental evaluations show that the descriptors trained using our proposed dataset outperform the current state-of-the-art descriptors trained on MVS by 8%, 4% and 10% on matching, verification and retrieval tasks respectively on the HPatches dataset. Similarly on the Strecha dataset, we see an improvement of 3-5% for the matching task in non-planar scenes.



### Deep Cross Polarimetric Thermal-to-visible Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.01486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01486v1)
- **Published**: 2018-01-04 18:41:27+00:00
- **Updated**: 2018-01-04 18:41:27+00:00
- **Authors**: Seyed Mehdi Iranmanesh, Ali Dabouei, Hadi Kazemi, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a deep coupled learning frame- work to address the problem of matching polarimetric ther- mal face photos against a gallery of visible faces. Polariza- tion state information of thermal faces provides the miss- ing textural and geometrics details in the thermal face im- agery which exist in visible spectrum. we propose a coupled deep neural network architecture which leverages relatively large visible and thermal datasets to overcome the problem of overfitting and eventually we train it by a polarimetric thermal face dataset which is the first of its kind. The pro- posed architecture is able to make full use of the polari- metric thermal information to train a deep model compared to the conventional shallow thermal-to-visible face recogni- tion methods. Proposed coupled deep neural network also finds global discriminative features in a nonlinear embed- ding space to relate the polarimetric thermal faces to their corresponding visible faces. The results show the superior- ity of our method compared to the state-of-the-art models in cross thermal-to-visible face recognition algorithms.



### Plan in 2D, execute in 3D: An augmented reality solution for cup placement in total hip arthroplasty
- **Arxiv ID**: http://arxiv.org/abs/1801.01557v1
- **DOI**: 10.1117/1.JMI.5.2.021205
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01557v1)
- **Published**: 2018-01-04 22:00:20+00:00
- **Updated**: 2018-01-04 22:00:20+00:00
- **Authors**: Javad Fotouhi, Clayton P. Alexander, Mathias Unberath, Giacomo Taylor, Sing Chun Lee, Bernhard Fuerst, Alex Johnson, Greg Osgood, Russell H. Taylor, Harpal Khanuja, Mehran Armand, Nassir Navab
- **Comment**: None
- **Journal**: J. Med. Imag. 5(2), 021205 (2018)
- **Summary**: Reproducibly achieving proper implant alignment is a critical step in total hip arthroplasty (THA) procedures that has been shown to substantially affect patient outcome. In current practice, correct alignment of the acetabular cup is verified in C-arm X-ray images that are acquired in an anterior-posterior (AP) view. Favorable surgical outcome is, therefore, heavily dependent on the surgeon's experience in understanding the 3D orientation of a hemispheric implant from 2D AP projection images. This work proposes an easy to use intra-operative component planning system based on two C-arm X-ray images that is combined with 3D augmented reality (AR) visualization that simplifies impactor and cup placement according to the planning by providing a real-time RGBD data overlay. We evaluate the feasibility of our system in a user study comprising four orthopedic surgeons at the Johns Hopkins Hospital, and also report errors in translation, anteversion, and abduction as low as 1.98 mm, 1.10 degrees, and 0.53 degrees, respectively. The promising performance of this AR solution shows that deploying this system could eliminate the need for excessive radiation, simplify the intervention, and enable reproducibly accurate placement of acetabular implants.



### On-the-fly Augmented Reality for Orthopaedic Surgery Using a Multi-Modal Fiducial
- **Arxiv ID**: http://arxiv.org/abs/1801.01560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01560v1)
- **Published**: 2018-01-04 22:02:33+00:00
- **Updated**: 2018-01-04 22:02:33+00:00
- **Authors**: Sebastian Andress, Alex Johnson, Mathias Unberath, Alexander Winkler, Kevin Yu, Javad Fotouhi, Simon Weidert, Greg Osgood, Nassir Navab
- **Comment**: S. Andress, A. Johnson, M. Unberath, and A. Winkler have contributed
  equally and are listed in alphabetical order
- **Journal**: J. Med. Imag. 5(2), 2018
- **Summary**: Fluoroscopic X-ray guidance is a cornerstone for percutaneous orthopaedic surgical procedures. However, two-dimensional observations of the three-dimensional anatomy suffer from the effects of projective simplification. Consequently, many X-ray images from various orientations need to be acquired for the surgeon to accurately assess the spatial relations between the patient's anatomy and the surgical tools. In this paper, we present an on-the-fly surgical support system that provides guidance using augmented reality and can be used in quasi-unprepared operating rooms. The proposed system builds upon a multi-modality marker and simultaneous localization and mapping technique to co-calibrate an optical see-through head mounted display to a C-arm fluoroscopy system. Then, annotations on the 2D X-ray images can be rendered as virtual objects in 3D providing surgical guidance. We quantitatively evaluate the components of the proposed system, and finally, design a feasibility study on a semi-anthropomorphic phantom. The accuracy of our system was comparable to the traditional image-guided technique while substantially reducing the number of acquired X-ray images as well as procedure time. Our promising results encourage further research on the interaction between virtual and real objects, that we believe will directly benefit the proposed method. Further, we would like to explore the capabilities of our on-the-fly augmented reality support system in a larger study directed towards common orthopaedic interventions.



### LoopSmart: Smart Visual SLAM Through Surface Loop Closure
- **Arxiv ID**: http://arxiv.org/abs/1801.01572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01572v1)
- **Published**: 2018-01-04 22:53:07+00:00
- **Updated**: 2018-01-04 22:53:07+00:00
- **Authors**: Guoxiang Zhang, YangQuan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a visual simultaneous localization and mapping (SLAM) framework of closing surface loops. It combines both sparse feature matching and dense surface alignment. Sparse feature matching is used for visual odometry and globally camera pose fine-tuning when dense loops are detected, while dense surface alignment is the way of closing large loops and solving surface mismatching problem. To achieve smart dense surface loop closure, a highly efficient CUDA-based global point cloud registration method and a map content dependent loop verification method are proposed. We run extensive experiments on different datasets, our method outperforms state-of-the-art ones in terms of both camera trajectory and surface reconstruction accuracy.



### Object Referring in Videos with Language and Human Gaze
- **Arxiv ID**: http://arxiv.org/abs/1801.01582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01582v2)
- **Published**: 2018-01-04 23:31:20+00:00
- **Updated**: 2018-04-04 15:38:07+00:00
- **Authors**: Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool
- **Comment**: Accepted to CVPR 2018, 10 pages, 6 figures
- **Journal**: None
- **Summary**: We investigate the problem of object referring (OR) i.e. to localize a target object in a visual scene coming with a language description. Humans perceive the world more as continued video snippets than as static images, and describe objects not only by their appearance, but also by their spatio-temporal context and motion features. Humans also gaze at the object when they issue a referring expression. Existing works for OR mostly focus on static images only, which fall short in providing many such cues. This paper addresses OR in videos with language and human gaze. To that end, we present a new video dataset for OR, with 30, 000 objects over 5, 000 stereo video sequences annotated for their descriptions and gaze. We further propose a novel network model for OR in videos, by integrating appearance, motion, gaze, and spatio-temporal context into one network. Experimental results show that our method effectively utilizes motion cues, human gaze, and spatio-temporal context. Our method outperforms previousOR methods. For dataset and code, please refer https://people.ee.ethz.ch/~arunv/ORGaze.html.




# Arxiv Papers in cs.CV on 2018-07-18
### Defend Deep Neural Networks Against Adversarial Examples via Fixed and Dynamic Quantized Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/1807.06714v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.06714v2)
- **Published**: 2018-07-18 00:21:12+00:00
- **Updated**: 2019-12-18 23:54:10+00:00
- **Authors**: Adnan Siraj Rakin, Jinfeng Yi, Boqing Gong, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks. To this end, many defense approaches that attempt to improve the robustness of DNNs have been proposed. In a separate and yet related area, recent works have explored to quantize neural network weights and activation functions into low bit-width to compress model size and reduce computational complexity. In this work, we find that these two different tracks, namely the pursuit of network compactness and robustness, can be merged into one and give rise to networks of both advantages. To the best of our knowledge, this is the first work that uses quantization of activation functions to defend against adversarial examples. We also propose to train robust neural networks by using adaptive quantization techniques for the activation functions. Our proposed Dynamic Quantized Activation (DQA) is verified through a wide range of experiments with the MNIST and CIFAR-10 datasets under different white-box attack methods, including FGSM, PGD, and C & W attacks. Furthermore, Zeroth Order Optimization and substitute model-based black-box attacks are also considered in this work. The experimental results clearly show that the robustness of DNNs could be greatly improved using the proposed DQA.



### Finding any Waldo: zero-shot invariant and efficient visual search
- **Arxiv ID**: http://arxiv.org/abs/1807.10587v1
- **DOI**: 10.1038/s41467-018-06217-x
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1807.10587v1)
- **Published**: 2018-07-18 01:17:34+00:00
- **Updated**: 2018-07-18 01:17:34+00:00
- **Authors**: Mengmi Zhang, Jiashi Feng, Keng Teck Ma, Joo Hwee Lim, Qi Zhao, Gabriel Kreiman
- **Comment**: Number of figures: 6 Number of supplementary figures: 14
- **Journal**: None
- **Summary**: Searching for a target object in a cluttered scene constitutes a fundamental challenge in daily vision. Visual search must be selective enough to discriminate the target from distractors, invariant to changes in the appearance of the target, efficient to avoid exhaustive exploration of the image, and must generalize to locate novel target objects with zero-shot training. Previous work has focused on searching for perfect matches of a target after extensive category-specific training. Here we show for the first time that humans can efficiently and invariantly search for natural objects in complex scenes. To gain insight into the mechanisms that guide visual search, we propose a biologically inspired computational model that can locate targets without exhaustive sampling and generalize to novel objects. The model provides an approximation to the mechanisms integrating bottom-up and top-down signals during search in natural scenes.



### 3D Global Convolutional Adversarial Network\\ for Prostate MR Volume Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.06742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06742v1)
- **Published**: 2018-07-18 02:27:53+00:00
- **Updated**: 2018-07-18 02:27:53+00:00
- **Authors**: Haozhe Jia, Yang Song, Donghao Zhang, Heng Huang, Dagan Feng, Michael Fulham, Yong Xia, Weidong Cai
- **Comment**: 9 pages, 3 figures, 2 tables, 16 references
- **Journal**: None
- **Summary**: Advanced deep learning methods have been developed to conduct prostate MR volume segmentation in either a 2D or 3D fully convolutional manner. However, 2D methods tend to have limited segmentation performance, since large amounts of spatial information of prostate volumes are discarded during the slice-by-slice segmentation process; and 3D methods also have room for improvement, since they use isotropic kernels to perform 3D convolutions whereas most prostate MR volumes have anisotropic spatial resolution. Besides, the fully convolutional structural methods achieve good performance for localization issues but neglect the per-voxel classification for segmentation tasks. In this paper, we propose a 3D Global Convolutional Adversarial Network (3D GCA-Net) to address efficient prostate MR volume segmentation. We first design a 3D ResNet encoder to extract 3D features from prostate scans, and then develop the decoder, which is composed of a multi-scale 3D global convolutional block and a 3D boundary refinement block, to address the classification and localization issues simultaneously for volumetric segmentation. Additionally, we combine the encoder-decoder segmentation network with an adversarial network in the training phrase to enforce the contiguity of long-range spatial predictions. Throughout the proposed model, we use anisotropic convolutional processing for better feature learning on prostate MR scans. We evaluated our 3D GCA-Net model on two public prostate MR datasets and achieved state-of-the-art performances.



### Signal Alignment for Humanoid Skeletons via the Globally Optimal Reparameterization Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1807.07432v2
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1807.07432v2)
- **Published**: 2018-07-18 03:16:39+00:00
- **Updated**: 2018-07-20 15:14:55+00:00
- **Authors**: Thomas W. Mitchel, Sipu Ruan, Gregory S. Chirikjian
- **Comment**: Humanoids 2018 initial submission; companion paper to
  arXiv:1807.05485
- **Journal**: None
- **Summary**: The general ability to analyze and classify the 3D kinematics of the human form is an essential step in the development of socially adept humanoid robots. A variety of different types of signals can be used by machines to represent and characterize actions such as RGB videos, infrared maps, and optical flow. In particular, skeleton sequences provide a natural 3D kinematic description of human motions and can be acquired in real time using RGB+D cameras. Moreover, skeleton sequences are generalizable to characterize the motions of both humans and humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA) is a novel, recently proposed algorithm for signal alignment in which signals are reparameterized to a globally optimal universal standard timescale (UST). Here, we introduce a variant of GORA for humanoid action recognition with skeleton sequences, which we call GORA-S. We briefly review the algorithm's mathematical foundations and contextualize them in the problem of action recognition with skeleton sequences. Subsequently, we introduce GORA-S and discuss parameters and numerical techniques for its effective implementation. We then compare its performance with that of the DTW and FastDTW algorithms, in terms of computational efficiency and accuracy in matching skeletons. Our results show that GORA-S attains a complexity that is significantly less than that of any tested DTW method. In addition, it displays a favorable balance between speed and accuracy that remains invariant under changes in skeleton sampling frequency, lending it a degree of versatility that could make it well-suited for a variety of action recognition tasks.



### On Evaluation of Embodied Navigation Agents
- **Arxiv ID**: http://arxiv.org/abs/1807.06757v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.06757v1)
- **Published**: 2018-07-18 03:28:02+00:00
- **Updated**: 2018-07-18 03:28:02+00:00
- **Authors**: Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir
- **Comment**: Report of a working group on empirical methodology in navigation
  research. Authors are listed in alphabetical order
- **Journal**: None
- **Summary**: Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.



### UNet++: A Nested U-Net Architecture for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.10165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10165v1)
- **Published**: 2018-07-18 04:08:21+00:00
- **Updated**: 2018-07-18 04:08:21+00:00
- **Authors**: Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming Liang
- **Comment**: 8 pages, 3 figures, 3 tables, accepted by 4th Deep Learning in
  Medical Image Analysis (DLMIA) Workshop
- **Journal**: None
- **Summary**: In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.



### Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.06772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06772v1)
- **Published**: 2018-07-18 04:29:20+00:00
- **Updated**: 2018-07-18 04:29:20+00:00
- **Authors**: Ranju Mandal, Partha Pratim Roy, Umapada Pal, Michael Blumenstein
- **Comment**: None
- **Journal**: None
- **Summary**: An end-to-end architecture for multi-script document retrieval using handwritten signatures is proposed in this paper. The user supplies a query signature sample and the system exclusively returns a set of documents that contain the query signature. In the first stage, a component-wise classification technique separates the potential signature components from all other components. A bag-of-visual-words powered by SIFT descriptors in a patch-based framework is proposed to compute the features and a Support Vector Machine (SVM)-based classifier was used to separate signatures from the documents. In the second stage, features from the foreground (i.e. signature strokes) and the background spatial information (i.e. background loops, reservoirs etc.) were combined to characterize the signature object to match with the query signature. Finally, three distance measures were used to match a query signature with the signature present in target documents for retrieval. The `Tobacco' document database and an Indian script database containing 560 documents of Devanagari (Hindi) and Bangla scripts were used for the performance evaluation. The proposed system was also tested on noisy documents and promising results were obtained. A comparative study shows that the proposed method outperforms the state-of-the-art approaches.



### Visual Affordance and Function Understanding: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1807.06775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.06775v1)
- **Published**: 2018-07-18 05:21:30+00:00
- **Updated**: 2018-07-18 05:21:30+00:00
- **Authors**: Mohammed Hassanin, Salman Khan, Murat Tahtali
- **Comment**: 26 pages, 22 images
- **Journal**: None
- **Summary**: Nowadays, robots are dominating the manufacturing, entertainment and healthcare industries. Robot vision aims to equip robots with the ability to discover information, understand it and interact with the environment. These capabilities require an agent to effectively understand object affordances and functionalities in complex visual domains. In this literature survey, we first focus on Visual affordances and summarize the state of the art as well as open problems and research gaps. Specifically, we discuss sub-problems such as affordance detection, categorization, segmentation and high-level reasoning. Furthermore, we cover functional scene understanding and the prevalent functional descriptors used in the literature. The survey also provides necessary background to the problem, sheds light on its significance and highlights the existing challenges for affordance and functionality learning.



### An Attention-Based Approach for Single Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1807.06779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06779v1)
- **Published**: 2018-07-18 05:34:49+00:00
- **Updated**: 2018-07-18 05:34:49+00:00
- **Authors**: Yuan Liu, Yuancheng Wang, Nan Li, Xu Cheng, Yifeng Zhang, Yongming Huang, Guojun Lu
- **Comment**: None
- **Journal**: None
- **Summary**: The main challenge of single image super resolution (SISR) is the recovery of high frequency details such as tiny textures. However, most of the state-of-the-art methods lack specific modules to identify high frequency areas, causing the output image to be blurred. We propose an attention-based approach to give a discrimination between texture areas and smooth areas. After the positions of high frequency details are located, high frequency compensation is carried out. This approach can incorporate with previously proposed SISR networks. By providing high frequency enhancement, better performance and visual effect are achieved. We also propose our own SISR network composed of DenseRes blocks. The block provides an effective way to combine the low level features and high level features. Extensive benchmark evaluation shows that our proposed method achieves significant improvement over the state-of-the-art works in SISR.



### DroNet: Efficient convolutional neural network detector for real-time UAV applications
- **Arxiv ID**: http://arxiv.org/abs/1807.06789v1
- **DOI**: 10.23919/DATE.2018.8342149
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06789v1)
- **Published**: 2018-07-18 06:30:54+00:00
- **Updated**: 2018-07-18 06:30:54+00:00
- **Authors**: Christos Kyrkou, George Plastiras, Stylianos Venieris, Theocharis Theocharides, Christos-Savvas Bouganis
- **Comment**: C. Kyrkou, G. Plastiras, T. Theocharides, S. I. Venieris and C. S.
  Bouganis, "DroNet: Efficient convolutional neural network detector for
  real-time UAV applications," 2018 Design, Automation & Test in Europe
  Conference & Exhibition (DATE), Dresden, 2018, pp. 967-972. Keywords:
  Convolutional neural networks, Machine learning, autonomous aerial vehicles,
  computer vision, embedded systems
- **Journal**: 2018 Design, Automation & Test in Europe Conference & Exhibition
  (DATE)
- **Summary**: Unmanned Aerial Vehicles (drones) are emerging as a promising technology for both environmental and infrastructure monitoring, with broad use in a plethora of applications. Many such applications require the use of computer vision algorithms in order to analyse the information captured from an on-board camera. Such applications include detecting vehicles for emergency response and traffic monitoring. This paper therefore, explores the trade-offs involved in the development of a single-shot object detector based on deep convolutional neural networks (CNNs) that can enable UAVs to perform vehicle detection under a resource constrained environment such as in a UAV. The paper presents a holistic approach for designing such systems; the data collection and training stages, the CNN architecture, and the optimizations necessary to efficiently map such a CNN on a lightweight embedded processing platform suitable for deployment on UAVs. Through the analysis we propose a CNN architecture that is capable of detecting vehicles from aerial UAV images and can operate between 5-18 frames-per-second for a variety of platforms with an overall accuracy of ~95%. Overall, the proposed architecture is suitable for UAV applications, utilizing low-power embedded processors that can be deployed on commercial UAVs.



### Real-Time Stereo Vision for Road Surface 3-D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.07433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07433v2)
- **Published**: 2018-07-18 06:44:07+00:00
- **Updated**: 2018-08-29 02:14:20+00:00
- **Authors**: Rui Fan, Yanan Liu, Xingrui Yang, Mohammud Junaid Bocus, Naim Dahnoun, Scott Tancock
- **Comment**: 6 pages, 4 figures, IEEE International Conference on Imaging System
  and Techniques (IST) 2018. arXiv admin note: substantial text overlap with
  arXiv:1807.02044
- **Journal**: None
- **Summary**: Stereo vision techniques have been widely used in civil engineering to acquire 3-D road data. The two important factors of stereo vision are accuracy and speed. However, it is very challenging to achieve both of them simultaneously and therefore the main aim of developing a stereo vision system is to improve the trade-off between these two factors. In this paper, we present a real-time stereo vision system used for road surface 3-D reconstruction. The proposed system is developed from our previously published 3-D reconstruction algorithm where the perspective view of the target image is first transformed into the reference view, which not only increases the disparity accuracy but also improves the processing speed. Then, the correlation cost between each pair of blocks is computed and stored in two 3-D cost volumes. To adaptively aggregate the matching costs from neighbourhood systems, bilateral filtering is performed on the cost volumes. This greatly reduces the ambiguities during stereo matching and further improves the precision of the estimated disparities. Finally, the subpixel resolution is achieved by conducting a parabola interpolation and the subpixel disparity map is used to reconstruct the 3-D road surface. The proposed algorithm is implemented on an NVIDIA GTX 1080 GPU for the real-time purpose. The experimental results illustrate that the reconstruction accuracy is around 3 mm.



### Determining ellipses from low-resolution images with a comprehensive image formation model
- **Arxiv ID**: http://arxiv.org/abs/1807.06814v3
- **DOI**: 10.1364/JOSAA.36.000212
- **Categories**: **cs.CV**, 68U10, 62M40
- **Links**: [PDF](http://arxiv.org/pdf/1807.06814v3)
- **Published**: 2018-07-18 08:23:34+00:00
- **Updated**: 2019-01-24 03:57:24+00:00
- **Authors**: Wojciech Chojnacki, Zygmunt L. Szpak
- **Comment**: 20 pages, 22 figures
- **Journal**: Journal of the Optical Society of America A, vol. 36, 2019, pp.
  212 - 233
- **Summary**: When determining the parameters of a parametric planar shape based on a single low-resolution image, common estimation paradigms lead to inaccurate parameter estimates. The reason behind poor estimation results is that standard estimation frameworks fail to model the image formation process at a sufficiently detailed level of analysis. We propose a new method for estimating the parameters of a planar elliptic shape based on a single photon-limited, low-resolution image. Our technique incorporates the effects of several elements - point-spread function, discretisation step, quantisation step, and photon noise - into a single cohesive and manageable statistical model. While we concentrate on the particular task of estimating the parameters of elliptic shapes, our ideas and methods have a much broader scope and can be used to address the problem of estimating the parameters of an arbitrary parametrically representable planar shape. Comprehensive experimental results on simulated and real imagery demonstrate that our approach yields parameter estimates with unprecedented accuracy. Furthermore, our method supplies a parameter covariance matrix as a measure of uncertainty for the estimated parameters, as well as a planar confidence region as a means for visualising the parameter uncertainty. The mathematical model developed in this paper may prove useful in a variety of disciplines which operate with imagery at the limits of resolution.



### Self-supervised Knowledge Distillation Using Singular Value Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1807.06819v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06819v1)
- **Published**: 2018-07-18 08:52:05+00:00
- **Updated**: 2018-07-18 08:52:05+00:00
- **Authors**: Seung Hyun Lee, Dae Ha Kim, Byung Cheol Song
- **Comment**: accepted to ECCV 2018
- **Journal**: None
- **Summary**: To solve deep neural network (DNN)'s huge training dataset and its high computation issue, so-called teacher-student (T-S) DNN which transfers the knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN has limited range of use, and the knowledge of T-DNN is insufficiently transferred to S-DNN. To improve the quality of the transferred knowledge from T-DNN, we propose a new knowledge distillation using singular value decomposition (SVD). In addition, we define a knowledge transfer as a self-supervised task and suggest a way to continuously receive information from T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of the T-DNN can be up to 1.1\% better than the T-DNN in terms of classification accuracy. Also assuming the same computational cost, our S-DNN outperforms the S-DNN driven by the state-of-the-art distillation with a performance advantage of 1.79\%. code is available on https://github.com/sseung0703/SSKD\_SVD.



### Computed Tomography Image Enhancement using 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.06821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06821v1)
- **Published**: 2018-07-18 08:52:27+00:00
- **Updated**: 2018-07-18 08:52:27+00:00
- **Authors**: Meng Li, Shiwen Shen, Wen Gao, William Hsu, Jason Cong
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) is increasingly being used for cancer screening, such as early detection of lung cancer. However, CT studies have varying pixel spacing due to differences in acquisition parameters. Thick slice CTs have lower resolution, hindering tasks such as nodule characterization during computer-aided detection due to partial volume effect. In this study, we propose a novel 3D enhancement convolutional neural network (3DECNN) to improve the spatial resolution of CT studies that were acquired using lower resolution/slice thicknesses to higher resolutions. Using a subset of the LIDC dataset consisting of 20,672 CT slices from 100 scans, we simulated lower resolution/thick section scans then attempted to reconstruct the original images using our 3DECNN network. A significant improvement in PSNR (29.3087dB vs. 28.8769dB, p-value < 2.2e-16) and SSIM (0.8529dB vs. 0.8449dB, p-value < 2.2e-16) compared to other state-of-art deep learning methods is observed.



### Metric Embedding Autoencoders for Unsupervised Cross-Dataset Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.10591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10591v1)
- **Published**: 2018-07-18 09:59:34+00:00
- **Updated**: 2018-07-18 09:59:34+00:00
- **Authors**: Alexey Potapov, Sergey Rodionov, Hugo Latapie, Enzo Fenoglio
- **Comment**: ICANN 2018 (The 27th International Conference on Artificial Neural
  Networks) proceeding
- **Journal**: None
- **Summary**: Cross-dataset transfer learning is an important problem in person re-identification (Re-ID). Unfortunately, not too many deep transfer Re-ID models exist for realistic settings of practical Re-ID systems. We propose a purely deep transfer Re-ID model consisting of a deep convolutional neural network and an autoencoder. The latent code is divided into metric embedding and nuisance variables. We then utilize an unsupervised training method that does not rely on co-training with non-deep models. Our experiments show improvements over both the baseline and competitors' transfer learning models.



### Harmonic Adversarial Attack Method
- **Arxiv ID**: http://arxiv.org/abs/1807.10590v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10590v2)
- **Published**: 2018-07-18 10:09:37+00:00
- **Updated**: 2018-08-08 03:58:43+00:00
- **Authors**: Wen Heng, Shuchang Zhou, Tingting Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks find perturbations that can fool models into misclassifying images. Previous works had successes in generating noisy/edge-rich adversarial perturbations, at the cost of degradation of image quality. Such perturbations, even when they are small in scale, are usually easily spottable by human vision. In contrast, we propose Harmonic Adversar- ial Attack Methods (HAAM), that generates edge-free perturbations by using harmonic functions. The property of edge-free guarantees that the generated adversarial images can still preserve visual quality, even when perturbations are of large magnitudes. Experiments also show that adversaries generated by HAAM often have higher rates of success when transferring between models. In addition, we find harmonic perturbations can simulate natural phenomena like natural lighting and shadows. It would then be possible to help find corner cases for given models, as a first step to improving them.



### Learning Interpretable Anatomical Features Through Deep Generative Models: Application to Cardiac Remodeling
- **Arxiv ID**: http://arxiv.org/abs/1807.06843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06843v1)
- **Published**: 2018-07-18 10:15:23+00:00
- **Updated**: 2018-07-18 10:15:23+00:00
- **Authors**: Carlo Biffi, Ozan Oktay, Giacomo Tarroni, Wenjia Bai, Antonio De Marvao, Georgia Doumou, Martin Rajchl, Reem Bedair, Sanjay Prasad, Stuart Cook, Declan O'Regan, Daniel Rueckert
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Alterations in the geometry and function of the heart define well-established causes of cardiovascular disease. However, current approaches to the diagnosis of cardiovascular diseases often rely on subjective human assessment as well as manual analysis of medical images. Both factors limit the sensitivity in quantifying complex structural and functional phenotypes. Deep learning approaches have recently achieved success for tasks such as classification or segmentation of medical images, but lack interpretability in the feature extraction and decision processes, limiting their value in clinical diagnosis. In this work, we propose a 3D convolutional generative model for automatic classification of images from patients with cardiac diseases associated with structural remodeling. The model leverages interpretable task-specific anatomic patterns learned from 3D segmentations. It further allows to visualise and quantify the learned pathology-specific remodeling patterns in the original input space of the images. This approach yields high accuracy in the categorization of healthy and hypertrophic cardiomyopathy subjects when tested on unseen MR images from our own multi-centre dataset (100%) as well on the ACDC MICCAI 2017 dataset (90%). We believe that the proposed deep learning approach is a promising step towards the development of interpretable classifiers for the medical imaging domain, which may help clinicians to improve diagnostic accuracy and enhance patient risk-stratification.



### Melanoma Recognition with an Ensemble of Techniques for Segmentation and a Structural Analysis for Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.06905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06905v1)
- **Published**: 2018-07-18 13:10:56+00:00
- **Updated**: 2018-07-18 13:10:56+00:00
- **Authors**: Christoph Rasche
- **Comment**: Participation in ISIC 2018 competition. 4 pages
- **Journal**: None
- **Summary**: An approach to lesion recognition is described that for lesion localization uses an ensemble of segmentation techniques and for lesion classification an exhaustive structural analysis. For localization, candidate regions are obtained from global thresholding of the chromatic maps and from applying the K-Means algorithm to the RGB image; the candidate regions are then integrated. For classification, a relatively exhaustive structural analysis of contours and regions is carried out.



### Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search
- **Arxiv ID**: http://arxiv.org/abs/1807.06906v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06906v1)
- **Published**: 2018-07-18 13:11:08+00:00
- **Updated**: 2018-07-18 13:11:08+00:00
- **Authors**: Arber Zela, Aaron Klein, Stefan Falkner, Frank Hutter
- **Comment**: 11 pages, 3 figures, 3 tables, ICML 2018 AutoML Workshop
- **Journal**: ICML 2018 AutoML Workshop
- **Summary**: While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.



### Learning Hybrid Sparsity Prior for Image Restoration: Where Deep Learning Meets Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1807.06920v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.06920v2)
- **Published**: 2018-07-18 13:33:02+00:00
- **Updated**: 2018-11-28 12:36:01+00:00
- **Authors**: Fangfang Wu, Weisheng Dong, Guangming Shi, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art approaches toward image restoration can be classified into model-based and learning-based. The former - best represented by sparse coding techniques - strive to exploit intrinsic prior knowledge about the unknown high-resolution images; while the latter - popularized by recently developed deep learning techniques - leverage external image prior from some training dataset. It is natural to explore their middle ground and pursue a hybrid image prior capable of achieving the best in both worlds. In this paper, we propose a systematic approach of achieving this goal called Structured Analysis Sparse Coding (SASC). Specifically, a structured sparse prior is learned from extrinsic training data via a deep convolutional neural network (in a similar way to previous learning-based approaches); meantime another structured sparse prior is internally estimated from the input observation image (similar to previous model-based approaches). Two structured sparse priors will then be combined to produce a hybrid prior incorporating the knowledge from both domains. To manage the computational complexity, we have developed a novel framework of implementing hybrid structured sparse coding processes by deep convolutional neural networks. Experimental results show that the proposed hybrid image restoration method performs comparably with and often better than the current state-of-the-art techniques.



### Method for motion artifact reduction using a convolutional neural network for dynamic contrast enhanced MRI of the liver
- **Arxiv ID**: http://arxiv.org/abs/1807.06956v2
- **DOI**: 10.2463/mrms.mp.2018-0156
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.06956v2)
- **Published**: 2018-07-18 14:16:31+00:00
- **Updated**: 2018-10-03 06:09:25+00:00
- **Authors**: Daiki Tamada, Marie-Luise Kromrey, Hiroshi Onishi, Utaroh Motosugi
- **Comment**: 11 pages, 6 figures
- **Journal**: Magnetic Resonance in Medical Sciences 19.1 (2020): 64-76
- **Summary**: Purpose: To improve the quality of images obtained via dynamic contrast-enhanced MRI (DCE-MRI) that include motion artifacts and blurring using a deep learning approach. Methods: A multi-channel convolutional neural network (MARC) based method is proposed for reducing the motion artifacts and blurring caused by respiratory motion in images obtained via DCE-MRI of the liver. The training datasets for the neural network included images with and without respiration-induced motion artifacts or blurring, and the distortions were generated by simulating the phase error in k-space. Patient studies were conducted using a multi-phase T1-weighted spoiled gradient echo sequence for the liver containing breath-hold failures during data acquisition. The trained network was applied to the acquired images to analyze the filtering performance, and the intensities and contrast ratios before and after denoising were compared via Bland-Altman plots. Results: The proposed network was found to significantly reduce the magnitude of the artifacts and blurring induced by respiratory motion, and the contrast ratios of the images after processing via the network were consistent with those of the unprocessed images. Conclusion: A deep learning based method for removing motion artifacts in images obtained via DCE-MRI in the liver was demonstrated and validated.



### Active Learning for Segmentation by Optimizing Content Information for Maximal Entropy
- **Arxiv ID**: http://arxiv.org/abs/1807.06962v1
- **DOI**: 10.1007/978-3-030-00889-5_21
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06962v1)
- **Published**: 2018-07-18 14:24:39+00:00
- **Updated**: 2018-07-18 14:24:39+00:00
- **Authors**: Firat Ozdemir, Zixuan Peng, Christine Tanner, Philipp Fuernstahl, Orcun Goksel
- **Comment**: 8 pages, 4 figures, Accepted to MICCAI 2018 Workshop: Deep Learning
  in Medical Image Analysis (DLMIA)
- **Journal**: None
- **Summary**: Segmentation is essential for medical image analysis tasks such as intervention planning, therapy guidance, diagnosis, treatment decisions. Deep learning is becoming increasingly prominent for segmentation, where the lack of annotations, however, often becomes the main limitation. Due to privacy concerns and ethical considerations, most medical datasets are created, curated, and allow access only locally. Furthermore, current deep learning methods are often suboptimal in translating anatomical knowledge between different medical imaging modalities. Active learning can be used to select an informed set of image samples to request for manual annotation, in order to best utilize the limited annotation time of clinical experts for optimal outcomes, which we focus on in this work. Our contributions herein are two fold: (1) we enforce domain-representativeness of selected samples using a proposed penalization scheme to maximize information at the network abstraction layer, and (2) we propose a Borda-count based sample querying scheme for selecting samples for segmentation. Comparative experiments with baseline approaches show that the samples queried with our proposed method, where both above contributions are combined, result in significantly improved segmentation performance for this active learning task.



### Video Time: Properties, Encoders and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1807.06980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06980v1)
- **Published**: 2018-07-18 14:47:36+00:00
- **Updated**: 2018-07-18 14:47:36+00:00
- **Authors**: Amir Ghodrati, Efstratios Gavves, Cees G. M. Snoek
- **Comment**: 14 pages, BMVC 2018
- **Journal**: None
- **Summary**: Time-aware encoding of frame sequences in a video is a fundamental problem in video understanding. While many attempted to model time in videos, an explicit study on quantifying video time is missing. To fill this lacuna, we aim to evaluate video time explicitly. We describe three properties of video time, namely a) temporal asymmetry, b)temporal continuity and c) temporal causality. Based on each we formulate a task able to quantify the associated property. This allows assessing the effectiveness of modern video encoders, like C3D and LSTM, in their ability to model time. Our analysis provides insights about existing encoders while also leading us to propose a new video time encoder, which is better suited for the video time recognition tasks than C3D and LSTM. We believe the proposed meta-analysis can provide a reasonable baseline to assess video time encoders on equal grounds on a set of temporal-aware tasks.



### Skin Lesion Segmentation and Classification for ISIC 2018 Using Traditional Classifiers with Hand-Crafted Features
- **Arxiv ID**: http://arxiv.org/abs/1807.07001v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07001v1)
- **Published**: 2018-07-18 15:42:29+00:00
- **Updated**: 2018-07-18 15:42:29+00:00
- **Authors**: Russell C. Hardie, Redha Ali, Manawaduge Supun De Silva, Temesguen Messay Kebede
- **Comment**: ISIC 2018 https://challenge2018.isic-archive.com/
- **Journal**: None
- **Summary**: This paper provides the required description of the methods used to obtain submitted results for Task1 and Task 3 of ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection. The results have been created by a team of researchers at the University of Dayton Signal and Image Processing Lab. In this submission, traditional classifiers with hand-crafted features are utilized for Task 1 and Task 3. Our team is providing additional separate submissions using deep learning methods for comparison.



### Skeletal Movement to Color Map: A Novel Representation for 3D Action Recognition with Inception Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.07033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07033v1)
- **Published**: 2018-07-18 16:36:39+00:00
- **Updated**: 2018-07-18 16:36:39+00:00
- **Authors**: Huy Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio A. Velastin
- **Comment**: This article corresponds to our accepted version at the 2018 IEEE
  International Conference on Image Processing (ICIP). We will link the Digital
  Object Identifier (DOI) as soon as it is available
- **Journal**: None
- **Summary**: We propose a novel skeleton-based representation for 3D action recognition in videos using Deep Convolutional Neural Networks (D-CNNs). Two key issues have been addressed: First, how to construct a robust representation that easily captures the spatial-temporal evolutions of motions from skeleton sequences. Second, how to design D-CNNs capable of learning discriminative features from the new representation in a effective manner. To address these tasks, a skeletonbased representation, namely, SPMF (Skeleton Pose-Motion Feature) is proposed. The SPMFs are built from two of the most important properties of a human action: postures and their motions. Therefore, they are able to effectively represent complex actions. For learning and recognition tasks, we design and optimize new D-CNNs based on the idea of Inception Residual networks to predict actions from SPMFs. Our method is evaluated on two challenging datasets including MSR Action3D and NTU-RGB+D. Experimental results indicated that the proposed method surpasses state-of-the-art methods whilst requiring less computation.



### Location Augmentation for CNN
- **Arxiv ID**: http://arxiv.org/abs/1807.07044v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07044v3)
- **Published**: 2018-07-18 17:16:42+00:00
- **Updated**: 2018-10-14 23:57:51+00:00
- **Authors**: Zhenyi Wang, Olga Veksler
- **Comment**: None
- **Journal**: None
- **Summary**: CNNs have made a tremendous impact on the field of computer vision in the last several years. The main component of any CNN architecture is the convolution operation, which is translation invariant by design. However, location in itself can be an important cue. For example, a salient object is more likely to be closer to the center of the image, the sky in the top part of an image, etc. To include the location cue for feature learning, we propose to augment the color image, the usual input to CNNs, with one or more channels that carry location information. We test two approaches for adding location information. In the first approach, we incorporate location directly, by including the row and column indexes as two additional channels to the input image. In the second approach, we add location less directly by adding distance transform from the center pixel as an additional channel to the input image. We perform experiments with both direct and indirect ways to encode location. We show the advantage of augmenting the standard color input with location related channels on the tasks of salient object segmentation, semantic segmentation, and scene parsing.



### Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias
- **Arxiv ID**: http://arxiv.org/abs/1807.07049v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07049v1)
- **Published**: 2018-07-18 17:25:28+00:00
- **Updated**: 2018-07-18 17:25:28+00:00
- **Authors**: Abhinav Gupta, Adithyavairavan Murali, Dhiraj Gandhi, Lerrel Pinto
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people's homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots.



### A Modality-Adaptive Method for Segmenting Brain Tumors and Organs-at-Risk in Radiation Therapy Planning
- **Arxiv ID**: http://arxiv.org/abs/1807.10588v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10588v2)
- **Published**: 2018-07-18 20:16:00+00:00
- **Updated**: 2018-08-15 19:41:38+00:00
- **Authors**: Mikael Agn, Per Munck af RosenschÃ¶ld, Oula Puonti, Michael J. Lundemann, Laura Mancini, Anastasia Papadaki, Steffi Thust, John Ashburner, Ian Law, Koen Van Leemput
- **Comment**: corrected one reference
- **Journal**: None
- **Summary**: In this paper we present a method for simultaneously segmenting brain tumors and an extensive set of organs-at-risk for radiation therapy planning of glioblastomas. The method combines a contrast-adaptive generative model for whole-brain segmentation with a new spatial regularization model of tumor shape using convolutional restricted Boltzmann machines. We demonstrate experimentally that the method is able to adapt to image acquisitions that differ substantially from any available training data, ensuring its applicability across treatment sites; that its tumor segmentation accuracy is comparable to that of the current state of the art; and that it captures most organs-at-risk sufficiently well for radiation therapy planning purposes. The proposed method may be a valuable step towards automating the delineation of brain tumors and organs-at-risk in glioblastoma patients undergoing radiation therapy.



### CT Image Enhancement Using Stacked Generative Adversarial Networks and Transfer Learning for Lesion Segmentation Improvement
- **Arxiv ID**: http://arxiv.org/abs/1807.07144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07144v1)
- **Published**: 2018-07-18 21:01:37+00:00
- **Updated**: 2018-07-18 21:01:37+00:00
- **Authors**: Youbao Tang, Jinzheng Cai, Le Lu, Adam P. Harrison, Ke Yan, Jing Xiao, Lin Yang, Ronald M. Summers
- **Comment**: Accepted by MLMI 2018
- **Journal**: None
- **Summary**: Automated lesion segmentation from computed tomography (CT) is an important and challenging task in medical image analysis. While many advancements have been made, there is room for continued improvements. One hurdle is that CT images can exhibit high noise and low contrast, particularly in lower dosages. To address this, we focus on a preprocessing method for CT images that uses stacked generative adversarial networks (SGAN) approach. The first GAN reduces the noise in the CT image and the second GAN generates a higher resolution image with enhanced boundaries and high contrast. To make up for the absence of high quality CT images, we detail how to synthesize a large number of low- and high-quality natural images and use transfer learning with progressively larger amounts of CT images. We apply both the classic GrabCut method and the modern holistically nested network (HNN) to lesion segmentation, testing whether SGAN can yield improved lesion segmentation. Experimental results on the DeepLesion dataset demonstrate that the SGAN enhancements alone can push GrabCut performance over HNN trained on original images. We also demonstrate that HNN + SGAN performs best compared against four other enhancement methods, including when using only a single GAN.



### Take a Look Around: Using Street View and Satellite Images to Estimate House Prices
- **Arxiv ID**: http://arxiv.org/abs/1807.07155v2
- **DOI**: 10.1145/3342240
- **Categories**: **econ.EM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07155v2)
- **Published**: 2018-07-18 21:10:08+00:00
- **Updated**: 2019-10-21 09:49:59+00:00
- **Authors**: Stephen Law, Brooks Paige, Chris Russell
- **Comment**: published in ACM Transactions on Intelligent Systems and Technology
  (TIST) Volume 10 Issue 5, October 2019 Article No. 54
- **Journal**: None
- **Summary**: When an individual purchases a home, they simultaneously purchase its structural features, its accessibility to work, and the neighborhood amenities. Some amenities, such as air quality, are measurable while others, such as the prestige or the visual impression of a neighborhood, are difficult to quantify. Despite the well-known impacts intangible housing features have on house prices, limited attention has been given to systematically quantifying these difficult to measure amenities. Two issues have led to this neglect. Not only do few quantitative methods exist that can measure the urban environment, but that the collection of such data is both costly and subjective.   We show that street image and satellite image data can capture these urban qualities and improve the estimation of house prices. We propose a pipeline that uses a deep neural network model to automatically extract visual features from images to estimate house prices in London, UK. We make use of traditional housing features such as age, size, and accessibility as well as visual features from Google Street View images and Bing aerial images in estimating the house price model. We find encouraging results where learning to characterize the urban quality of a neighborhood improves house price prediction, even when generalizing to previously unseen London boroughs.   We explore the use of non-linear vs. linear methods to fuse these cues with conventional models of house pricing, and show how the interpretability of linear models allows us to directly extract proxy variables for visual desirability of neighborhoods that are both of interest in their own right, and could be used as inputs to other econometric methods. This is particularly valuable as once the network has been trained with the training data, it can be applied elsewhere, allowing us to generate vivid dense maps of the visual appeal of London streets.




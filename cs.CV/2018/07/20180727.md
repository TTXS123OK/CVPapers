# Arxiv Papers in cs.CV on 2018-07-27
### Adapting control policies from simulation to reality using a pairwise loss
- **Arxiv ID**: http://arxiv.org/abs/1807.10413v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.10413v2)
- **Published**: 2018-07-27 01:54:08+00:00
- **Updated**: 2018-10-26 19:42:23+00:00
- **Authors**: Ulrich Viereck, Xingchao Peng, Kate Saenko, Robert Platt
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an approach to domain transfer based on a pairwise loss function that helps transfer control policies learned in simulation onto a real robot. We explore the idea in the context of a 'category level' manipulation task where a control policy is learned that enables a robot to perform a mating task involving novel objects. We explore the case where depth images are used as the main form of sensor input. Our experimental results demonstrate that proposed method consistently outperforms baseline methods that train only in simulation or that combine real and simulated data in a naive way.



### Characters Detection on Namecard with faster RCNN
- **Arxiv ID**: http://arxiv.org/abs/1807.10417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10417v1)
- **Published**: 2018-07-27 02:17:58+00:00
- **Updated**: 2018-07-27 02:17:58+00:00
- **Authors**: Weitong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We apply Faster R-CNN to the detection of characters in namecard, in order to solve the problem of a small amount of data and the inbalance between different class, we designed the data augmentation and the 'fake' data generalizer to generate more data for the training of network. Without using data augmentation, the average IoU in correct samples could be no less than 80% and the mAP result of 80% was also achieved with Faster R-CNN. By applying the data augmentation, the variance of mAP is decreased and both of the IoU and mAP score has increased a little.



### W-TALC: Weakly-supervised Temporal Activity Localization and Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.10418v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10418v3)
- **Published**: 2018-07-27 02:31:49+00:00
- **Updated**: 2018-12-15 17:02:27+00:00
- **Authors**: Sujoy Paul, Sourya Roy, Amit K Roy-Chowdhury
- **Comment**: Accepted at European Conference on Computer Vision (ECCV), 2018
- **Journal**: None
- **Summary**: Most activity localization methods in the literature suffer from the burden of frame-wise annotation requirement. Learning from weak labels may be a potential solution towards reducing such manual labeling effort. Recent years have witnessed a substantial influx of tagged videos on the Internet, which can serve as a rich source of weakly-supervised training data. Specifically, the correlations between videos with similar tags can be utilized to temporally localize the activities. Towards this goal, we present W-TALC, a Weakly-supervised Temporal Activity Localization and Classification framework using only video-level labels. The proposed network can be divided into two sub-networks, namely the Two-Stream based feature extractor network and a weakly-supervised module, which we learn by optimizing two complimentary loss functions. Qualitative and quantitative results on two challenging datasets - Thumos14 and ActivityNet1.2, demonstrate that the proposed method is able to detect activities at a fine granularity and achieve better performance than current state-of-the-art methods.



### Fusion Network for Face-based Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.10421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10421v1)
- **Published**: 2018-07-27 03:22:10+00:00
- **Updated**: 2018-07-27 03:22:10+00:00
- **Authors**: Haoyi Wang, Xingjie Wei, Victor Sanchez, Chang-Tsun Li
- **Comment**: ICIP 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been applied to age-related research as the core framework. Although faces are composed of numerous facial attributes, most works with CNNs still consider a face as a typical object and do not pay enough attention to facial regions that carry age-specific feature for this particular task. In this paper, we propose a novel CNN architecture called Fusion Network (FusionNet) to tackle the age estimation problem. Apart from the whole face image, the FusionNet successively takes several age-specific facial patches as part of the input to emphasize the age-specific features. Through experiments, we show that the FusionNet significantly outperforms other state-of-the-art models on the MORPH II benchmark.



### Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency
- **Arxiv ID**: http://arxiv.org/abs/1807.10437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10437v1)
- **Published**: 2018-07-27 05:25:52+00:00
- **Updated**: 2018-07-27 05:25:52+00:00
- **Authors**: Eunji Chong, Nataniel Ruiz, Yongxin Wang, Yun Zhang, Agata Rozga, James Rehg
- **Comment**: Appears in: European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: This paper addresses the challenging problem of estimating the general visual attention of people in images. Our proposed method is designed to work across multiple naturalistic social scenarios and provides a full picture of the subject's attention and gaze. In contrast, earlier works on gaze and attention estimation have focused on constrained problems in more specific contexts. In particular, our model explicitly represents the gaze direction and handles out-of-frame gaze targets. We leverage three different datasets using a multi-task learning approach. We evaluate our method on widely used benchmarks for single-tasks such as gaze angle estimation and attention-within-an-image, as well as on the new challenging task of generalized visual attention prediction. In addition, we have created extended annotations for the MMDB and GazeFollow datasets which are used in our experiments, which we will publicly release.



### Synthetically Trained Icon Proposals for Parsing and Summarizing Infographics
- **Arxiv ID**: http://arxiv.org/abs/1807.10441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10441v1)
- **Published**: 2018-07-27 05:33:09+00:00
- **Updated**: 2018-07-27 05:33:09+00:00
- **Authors**: Spandan Madan, Zoya Bylinskii, Matthew Tancik, Adrià Recasens, Kimberli Zhong, Sami Alsheikh, Hanspeter Pfister, Aude Oliva, Fredo Durand
- **Comment**: None
- **Journal**: None
- **Summary**: Widely used in news, business, and educational media, infographics are handcrafted to effectively communicate messages about complex and often abstract topics including `ways to conserve the environment' and `understanding the financial crisis'. Composed of stylistically and semantically diverse visual and textual elements, infographics pose new challenges for computer vision. While automatic text extraction works well on infographics, computer vision approaches trained on natural images fail to identify the stand-alone visual elements in infographics, or `icons'. To bridge this representation gap, we propose a synthetic data generation strategy: we augment background patches in infographics from our Visually29K dataset with Internet-scraped icons which we use as training data for an icon proposal mechanism. On a test set of 1K annotated infographics, icons are located with 38% precision and 34% recall (the best model trained with natural images achieves 14% precision and 7% recall). Combining our icon proposals with icon classification and text extraction, we present a multi-modal summarization application. Our application takes an infographic as input and automatically produces text tags and visual hashtags that are textually and visually representative of the infographic's topics respectively.



### A multi-contrast MRI approach to thalamus segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.10757v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1807.10757v1)
- **Published**: 2018-07-27 06:13:04+00:00
- **Updated**: 2018-07-27 06:13:04+00:00
- **Authors**: Veronica Corona, Jan Lellmann, Peter Nestor, Carola-Bibiane Schoenlieb, Julio Acosta-Cabronero
- **Comment**: None
- **Journal**: None
- **Summary**: Thalamic alterations are relevant to many neurological disorders including Alzheimer's disease, Parkinson's disease and multiple sclerosis. Routine interventions to improve symptom severity in movement disorders, for example, often consist of surgery or deep brain stimulation to diencephalic nuclei. Therefore, accurate delineation of grey matter thalamic subregions is of the upmost clinical importance. MRI is highly appropriate for structural segmentation as it provides different views of the anatomy from a single scanning session. Though with several contrasts potentially available, it is also of increasing importance to develop new image segmentation techniques that can operate multi-spectrally. We hereby propose a new segmentation method for use with multi-modality data, which we evaluated for automated segmentation of major thalamic subnuclear groups using T1-, T2*-weighted and quantitative susceptibility mapping (QSM) information. The proposed method consists of four steps: highly iterative image co-registration, manual segmentation on the average training-data template, supervised learning for pattern recognition, and a final convex optimisation step imposing further spatial constraints to refine the solution. This led to solutions in greater agreement with manual segmentation than the standard Morel atlas based approach. Furthermore, we show that the multi-contrast approach boosts segmentation performances. We then investigated whether prior knowledge using the training-template contours could further improve convex segmentation accuracy and robustness, which led to highly precise multi-contrast segmentations in single subjects. This approach can be extended to most 3D imaging data types and any region of interest discernible in single scans or multi-subject templates.



### Semi-supervised Deep Generative Modelling of Incomplete Multi-Modality Emotional Data
- **Arxiv ID**: http://arxiv.org/abs/1808.02096v1
- **DOI**: 10.1145/3240508.3240528
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.02096v1)
- **Published**: 2018-07-27 07:07:36+00:00
- **Updated**: 2018-07-27 07:07:36+00:00
- **Authors**: Changde Du, Changying Du, Hao Wang, Jinpeng Li, Wei-Long Zheng, Bao-Liang Lu, Huiguang He
- **Comment**: arXiv admin note: text overlap with arXiv:1704.07548, 2018 ACM
  Multimedia Conference (MM'18)
- **Journal**: None
- **Summary**: There are threefold challenges in emotion recognition. First, it is difficult to recognize human's emotional states only considering a single modality. Second, it is expensive to manually annotate the emotional data. Third, emotional data often suffers from missing modalities due to unforeseeable sensor malfunction or configuration issues. In this paper, we address all these problems under a novel multi-view deep generative framework. Specifically, we propose to model the statistical relationships of multi-modality emotional data using multiple modality-specific generative networks with a shared latent space. By imposing a Gaussian mixture assumption on the posterior approximation of the shared latent variables, our framework can learn the joint deep representation from multiple modalities and evaluate the importance of each modality simultaneously. To solve the labeled-data-scarcity problem, we extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task. To address the missing-modality problem, we further extend our semi-supervised multi-view model to deal with incomplete data, where a missing view is treated as a latent variable and integrated out during inference. This way, the proposed overall framework can utilize all available (both labeled and unlabeled, as well as both complete and incomplete) data to improve its generalization ability. The experiments conducted on two real multi-modal emotion datasets demonstrated the superiority of our framework.



### A Deep Learning Framework for Automatic Diagnosis in Lung Cancer
- **Arxiv ID**: http://arxiv.org/abs/1807.10466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10466v1)
- **Published**: 2018-07-27 07:32:46+00:00
- **Updated**: 2018-07-27 07:32:46+00:00
- **Authors**: Nikolay Burlutskiy, Feng Gu, Lena Kajland Wilen, Max Backman, Patrick Micke
- **Comment**: Presented as a poster at Medical Imaging with Deep Learning (MIDL) in
  Amsterdam, 4-6th July 2018 (http://midl.amsterdam/)
- **Journal**: None
- **Summary**: We developed a deep learning framework that helps to automatically identify and segment lung cancer areas in patients' tissue specimens. The study was based on a cohort of lung cancer patients operated at the Uppsala University Hospital. The tissues were reviewed by lung pathologists and then the cores were compiled to tissue micro-arrays (TMAs). For experiments, hematoxylin-eosin stained slides from 712 patients were scanned and then manually annotated. Then these scans and annotations were used to train segmentation models of the developed framework. The performance of the developed deep learning framework was evaluated on fully annotated TMA cores from 178 patients reaching pixel-wise precision of 0.80 and recall of 0.86. Finally, publicly available Stanford TMA cores were used to demonstrate high performance of the framework qualitatively.



### Adversarial Open-World Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1807.10482v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10482v3)
- **Published**: 2018-07-27 08:15:48+00:00
- **Updated**: 2018-10-09 05:15:05+00:00
- **Authors**: Xiang Li, Ancong Wu, Wei-Shi Zheng
- **Comment**: 17 pages, 3 figures, Accepted by European Conference on Computer
  Vision 2018
- **Journal**: None
- **Summary**: In a typical real-world application of re-id, a watch-list (gallery set) of a handful of target people (e.g. suspects) to track around a large volume of non-target people are demanded across camera views, and this is called the open-world person re-id. Different from conventional (closed-world) person re-id, a large portion of probe samples are not from target people in the open-world setting. And, it always happens that a non-target person would look similar to a target one and therefore would seriously challenge a re-id system. In this work, we introduce a deep open-world group-based person re-id model based on adversarial learning to alleviate the attack problem caused by similar non-target people. The main idea is learning to attack feature extractor on the target people by using GAN to generate very target-like images (imposters), and in the meantime the model will make the feature extractor learn to tolerate the attack by discriminative learning so as to realize group-based verification. The framework we proposed is called the adversarial open-world person re-identification, and this is realized by our Adversarial PersonNet (APN) that jointly learns a generator, a person discriminator, a target discriminator and a feature extractor, where the feature extractor and target discriminator share the same weights so as to makes the feature extractor learn to tolerate the attack by imposters for better group-based verification. While open-world person re-id is challenging, we show for the first time that the adversarial-based approach helps stabilize person re-id system under imposter attack more effectively.



### Pull Message Passing for Nonparametric Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/1807.10487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10487v1)
- **Published**: 2018-07-27 08:24:55+00:00
- **Updated**: 2018-07-27 08:24:55+00:00
- **Authors**: Karthik Desingh, Anthony Opipari, Odest Chadwicke Jenkins
- **Comment**: None
- **Journal**: None
- **Summary**: We present a "pull" approach to approximate products of Gaussian mixtures within message updates for Nonparametric Belief Propagation (NBP) inference. Existing NBP methods often represent messages between continuous-valued latent variables as Gaussian mixture models. To avoid computational intractability in loopy graphs, NBP necessitates an approximation of the product of such mixtures. Sampling-based product approximations have shown effectiveness for NBP inference. However, such approximations used within the traditional "push" message update procedures quickly become computationally prohibitive for multi-modal distributions over high-dimensional variables. In contrast, we propose a "pull" method, as the Pull Message Passing for Nonparametric Belief propagation (PMPNBP) algorithm, and demonstrate its viability for efficient inference. We report results using an experiment from an existing NBP method, PAMPAS, for inferring the pose of an articulated structure in clutter. Results from this illustrative problem found PMPNBP has a greater ability to efficiently scale the number of components in its mixtures and, consequently, improve inference accuracy.



### Person Search in Videos with One Portrait Through Visual and Temporal Links
- **Arxiv ID**: http://arxiv.org/abs/1807.10510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10510v1)
- **Published**: 2018-07-27 09:39:28+00:00
- **Updated**: 2018-07-27 09:39:28+00:00
- **Authors**: Qingqiu Huang, Wentao Liu, Dahua Lin
- **Comment**: European Conference on Computer Vision (ECCV), 2018
- **Journal**: None
- **Summary**: In real-world applications, e.g. law enforcement and video retrieval, one often needs to search a certain person in long videos with just one portrait. This is much more challenging than the conventional settings for person re-identification, as the search may need to be carried out in the environments different from where the portrait was taken. In this paper, we aim to tackle this challenge and propose a novel framework, which takes into account the identity invariance along a tracklet, thus allowing person identities to be propagated via both the visual and the temporal links. We also develop a novel scheme called Progressive Propagation via Competitive Consensus, which significantly improves the reliability of the propagation process. To promote the study of person search, we construct a large-scale benchmark, which contains 127K manually annotated tracklets from 192 movies. Experiments show that our approach remarkably outperforms mainstream person re-id methods, raising the mAP from 42.16% to 62.27%.



### FARM: Functional Automatic Registration Method for 3D Human Bodies
- **Arxiv ID**: http://arxiv.org/abs/1807.10517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1807.10517v1)
- **Published**: 2018-07-27 09:53:45+00:00
- **Updated**: 2018-07-27 09:53:45+00:00
- **Authors**: Riccardo Marin, Simone Melzi, Emanuele Rodolà, Umberto Castellani
- **Comment**: Under submission to CGF
- **Journal**: None
- **Summary**: We introduce a new method for non-rigid registration of 3D human shapes. Our proposed pipeline builds upon a given parametric model of the human, and makes use of the functional map representation for encoding and inferring shape maps throughout the registration process. This combination endows our method with robustness to a large variety of nuisances observed in practical settings, including non-isometric transformations, downsampling, topological noise, and occlusions; further, the pipeline can be applied invariably across different shape representations (e.g. meshes and point clouds), and in the presence of (even dramatic) missing parts such as those arising in real-world depth sensing applications. We showcase our method on a selection of challenging tasks, demonstrating results in line with, or even surpassing, state-of-the-art methods in the respective areas.



### ESCaF: Pupil Centre Localization Algorithm with Candidate Filtering
- **Arxiv ID**: http://arxiv.org/abs/1807.10520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10520v1)
- **Published**: 2018-07-27 10:08:11+00:00
- **Updated**: 2018-07-27 10:08:11+00:00
- **Authors**: Anjith George, Aurobinda Routray
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Algorithms for accurate localization of pupil centre is essential for gaze tracking in real world conditions. Most of the algorithms fail in real world conditions like illumination variations, contact lenses, glasses, eye makeup, motion blur, noise, etc. We propose a new algorithm which improves the detection rate in real world conditions. The proposed algorithm uses both edges as well as intensity information along with a candidate filtering approach to identify the best pupil candidate. A simple tracking scheme has also been added which improves the processing speed. The algorithm has been evaluated in Labelled Pupil in the Wild (LPW) dataset, largest in its class which contains real world conditions. The proposed algorithm outperformed the state of the art algorithms while achieving real-time performance.



### CrossNet: An End-to-end Reference-based Super Resolution Network using Cross-scale Warping
- **Arxiv ID**: http://arxiv.org/abs/1807.10547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10547v1)
- **Published**: 2018-07-27 12:15:40+00:00
- **Updated**: 2018-07-27 12:15:40+00:00
- **Authors**: Haitian Zheng, Mengqi Ji, Haoqian Wang, Yebin Liu, Lu Fang
- **Comment**: To be appeared in ECCV 2018
- **Journal**: None
- **Summary**: The Reference-based Super-resolution (RefSR) super-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image, where the reference image and LR image share similar viewpoint but with significant resolution gap x8. Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions, leading to the inter-patch misalignment, grid effect and inefficient optimization. To resolve these issues, we present CrossNet, an end-to-end and fully-convolutional deep neural network using cross-scale warping. Our network contains image encoders, cross-scale warping layers, and fusion decoder: the encoder serves to extract multi-scale features from both the LR and the reference images; the cross-scale warping layers spatially aligns the reference feature map with the LR feature map; the decoder finally aggregates feature maps from both domains to synthesize the HR output. Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes both in precision (around 2dB-4dB) and efficiency (more than 100 times faster).



### X2Face: A network for controlling face generation by using images, audio, and pose codes
- **Arxiv ID**: http://arxiv.org/abs/1807.10550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10550v1)
- **Published**: 2018-07-27 12:31:16+00:00
- **Updated**: 2018-07-27 12:31:16+00:00
- **Authors**: Olivia Wiles, A. Sophia Koepke, Andrew Zisserman
- **Comment**: To appear in ECCV 2018. Accompanying video:
  http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html
- **Journal**: None
- **Summary**: The objective of this paper is a neural network model that controls the pose and expression of a given face, using another face or modality (e.g. audio). This model can then be used for lightweight, sophisticated video and image editing.   We make the following three contributions. First, we introduce a network, X2Face, that can control a source face (specified by one or more frames) using another face in a driving frame to produce a generated frame with the identity of the source frame but the pose and expression of the face in the driving frame. Second, we propose a method for training the network fully self-supervised using a large collection of video data. Third, we show that the generation process can be driven by other modalities, such as audio or pose codes, without any further training of the network.   The generation results for driving a face with another face are compared to state-of-the-art self-supervised/supervised methods. We show that our approach is more robust than other methods, as it makes fewer assumptions about the input data. We also show examples of using our framework for video face editing.



### Improving High Resolution Histology Image Classification with Deep Spatial Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/1807.10552v1
- **DOI**: 10.1007/978-3-030-00949-6_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10552v1)
- **Published**: 2018-07-27 12:34:34+00:00
- **Updated**: 2018-07-27 12:34:34+00:00
- **Authors**: Yongxiang Huang, Albert Chi-shing Chung
- **Comment**: 8 pages, MICCAI workshop preceedings
- **Journal**: Computational Pathology and Ophthalmic Medical Image Analysis.
  Springer, Cham, 2018. 19-26
- **Summary**: Histology imaging is an essential diagnosis method to finalize the grade and stage of cancer of different tissues, especially for breast cancer diagnosis. Specialists often disagree on the final diagnosis on biopsy tissue due to the complex morphological variety. Although convolutional neural networks (CNN) have advantages in extracting discriminative features in image classification, directly training a CNN on high resolution histology images is computationally infeasible currently. Besides, inconsistent discriminative features often distribute over the whole histology image, which incurs challenges in patch-based CNN classification method. In this paper, we propose a novel architecture for automatic classification of high resolution histology images. First, an adapted residual network is employed to explore hierarchical features without attenuation. Second, we develop a robust deep fusion network to utilize the spatial relationship between patches and learn to correct the prediction bias generated from inconsistent discriminative feature distribution. The proposed method is evaluated using 10-fold cross-validation on 400 high resolution breast histology images with balanced labels and reports 95% accuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class classification (carcinoma and non-carcinoma), which substantially outperforms previous methods and close to pathologist performance.



### Towards an Embodied Semantic Fovea: Semantic 3D scene reconstruction from ego-centric eye-tracker videos
- **Arxiv ID**: http://arxiv.org/abs/1807.10561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.10561v1)
- **Published**: 2018-07-27 12:51:36+00:00
- **Updated**: 2018-07-27 12:51:36+00:00
- **Authors**: Mickey Li, Noyan Songur, Pavel Orlov, Stefan Leutenegger, A Aldo Faisal
- **Comment**: None
- **Journal**: None
- **Summary**: Incorporating the physical environment is essential for a complete understanding of human behavior in unconstrained every-day tasks. This is especially important in ego-centric tasks where obtaining 3 dimensional information is both limiting and challenging with the current 2D video analysis methods proving insufficient. Here we demonstrate a proof-of-concept system which provides real-time 3D mapping and semantic labeling of the local environment from an ego-centric RGB-D video-stream with 3D gaze point estimation from head mounted eye tracking glasses. We augment existing work in Semantic Simultaneous Localization And Mapping (Semantic SLAM) with collected gaze vectors. Our system can then find and track objects both inside and outside the user field-of-view in 3D from multiple perspectives with reasonable accuracy. We validate our concept by producing a semantic map from images of the NYUv2 dataset while simultaneously estimating gaze position and gaze classes from recorded gaze data of the dataset images.



### Diverse feature visualizations reveal invariances in early layers of deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10589v1)
- **Published**: 2018-07-27 13:15:08+00:00
- **Updated**: 2018-07-27 13:15:08+00:00
- **Authors**: Santiago A. Cadena, Marissa A. Weis, Leon A. Gatys, Matthias Bethge, Alexander S. Ecker
- **Comment**: Accepted for ECCV 2018
- **Journal**: None
- **Summary**: Visualizing features in deep neural networks (DNNs) can help understanding their computations. Many previous studies aimed to visualize the selectivity of individual units by finding meaningful images that maximize their activation. However, comparably little attention has been paid to visualizing to what image transformations units in DNNs are invariant. Here we propose a method to discover invariances in the responses of hidden layer units of deep neural networks. Our approach is based on simultaneously searching for a batch of images that strongly activate a unit while at the same time being as distinct from each other as possible. We find that even early convolutional layers in VGG-19 exhibit various forms of response invariance: near-perfect phase invariance in some units and invariance to local diffeomorphic transformations in others. At the same time, we uncover representational differences with ResNet-50 in its corresponding layers. We conclude that invariance transformations are a major computational component learned by DNNs and we provide a systematic method to study them.



### Learning associations between clinical information and motion-based descriptors using a large scale MR-derived cardiac motion atlas
- **Arxiv ID**: http://arxiv.org/abs/1807.10653v1
- **DOI**: 10.1007/978-3-030-12029-0_11
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1807.10653v1)
- **Published**: 2018-07-27 14:33:27+00:00
- **Updated**: 2018-07-27 14:33:27+00:00
- **Authors**: Esther Puyol-Anton, Bram Ruijsink, Helene Langet, Mathieu De Craene, Paolo Piro, Julia A. Schnabel, Andrew P. King
- **Comment**: 2018 International Workshop on Statistical Atlases and Computational
  Modeling of the Heart
- **Journal**: None
- **Summary**: The availability of large scale databases containing imaging and non-imaging data, such as the UK Biobank, represents an opportunity to improve our understanding of healthy and diseased bodily function. Cardiac motion atlases provide a space of reference in which the motion fields of a cohort of subjects can be directly compared. In this work, a cardiac motion atlas is built from cine MR data from the UK Biobank (~ 6000 subjects). Two automated quality control strategies are proposed to reject subjects with insufficient image quality. Based on the atlas, three dimensionality reduction algorithms are evaluated to learn data-driven cardiac motion descriptors, and statistical methods used to study the association between these descriptors and non-imaging data. Results show a positive correlation between the atlas motion descriptors and body fat percentage, basal metabolic rate, hypertension, smoking status and alcohol intake frequency. The proposed method outperforms the ability to identify changes in cardiac function due to these known cardiovascular risk factors compared to ejection fraction, the most commonly used descriptor of cardiac function. In conclusion, this work represents a framework for further investigation of the factors influencing cardiac health.



### Influence of Image Classification Accuracy on Saliency Map Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.10657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10657v1)
- **Published**: 2018-07-27 14:36:58+00:00
- **Updated**: 2018-07-27 14:36:58+00:00
- **Authors**: Taiki Oyama, Takao Yamanaka
- **Comment**: CAAI Transactions on Intelligence Technology, accepted in 2018
- **Journal**: None
- **Summary**: Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pretrained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this paper, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. We also investigated the effective architecture based on multi scale images and the upsampling layers to refine the saliency-map resolution. Our model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT Saliency Benchmark, our model achieved the best performance in some metrics and competitive results in the other metrics.



### Diagnosing Error in Temporal Action Detectors
- **Arxiv ID**: http://arxiv.org/abs/1807.10706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10706v1)
- **Published**: 2018-07-27 16:13:22+00:00
- **Updated**: 2018-07-27 16:13:22+00:00
- **Authors**: Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Despite the recent progress in video understanding and the continuous rate of improvement in temporal action localization throughout the years, it is still unclear how far (or close?) we are to solving the problem. To this end, we introduce a new diagnostic tool to analyze the performance of temporal action detectors in videos and compare different methods beyond a single scalar metric. We exemplify the use of our tool by analyzing the performance of the top rewarded entries in the latest ActivityNet action localization challenge. Our analysis shows that the most impactful areas to work on are: strategies to better handle temporal context around the instances, improving the robustness w.r.t. the instance absolute and relative size, and strategies to reduce the localization errors. Moreover, our experimental analysis finds the lack of agreement among annotator is not a major roadblock to attain progress in the field. Our diagnostic tool is publicly available to keep fueling the minds of other researchers with additional insights about their algorithms.



### Deep Learning Methods and Applications for Region of Interest Detection in Dermoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1807.10711v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10711v3)
- **Published**: 2018-07-27 16:29:11+00:00
- **Updated**: 2022-03-01 20:11:46+00:00
- **Authors**: Manu Goyal, Moi Hoon Yap, Saeed Hassanpour
- **Comment**: Natural Augmentation
- **Journal**: None
- **Summary**: Rapid growth in the development of medical imaging analysis technology has been propelled by the great interest in improving computer-aided diagnosis and detection (CAD) systems for three popular image visualization tasks: classification, segmentation, and Region of Interest (ROI) detection. However, a limited number of datasets with ground truth annotations are available for developing segmentation and ROI detection of lesions, as expert annotations are laborious and expensive. Detecting the ROI is vital to locate lesions accurately. In this paper, we propose the use of two deep object detection meta-architectures (Faster R-CNN Inception-V2 and SSD Inception-V2) to develop robust ROI detection of skin lesions in dermoscopic datasets (2017 ISIC Challenge, PH2, and HAM10000), and compared the performance with state-of-the-art segmentation algorithm (DeeplabV3+). To further demonstrate the potential of our work, we built a smartphone application for real-time automated detection of skin lesions based on this methodology. In addition, we developed an automated natural data-augmentation method from ROI detection to produce augmented copies of dermoscopic images, as a pre-processing step in the segmentation of skin lesions to further improve the performance of the current state-of-the-art deep learning algorithm. Our proposed ROI detection has the potential to more appropriately streamline dermatology referrals and reduce unnecessary biopsies in the diagnosis of skin cancer.



### Semi-convolutional Operators for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.10712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10712v1)
- **Published**: 2018-07-27 16:29:53+00:00
- **Updated**: 2018-07-27 16:29:53+00:00
- **Authors**: David Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi
- **Comment**: Accepted as a conference paper at ECCV 2018
- **Journal**: None
- **Summary**: Object detection and instance segmentation are dominated by region-based methods such as Mask RCNN. However, there is a growing interest in reducing these problems to pixel labeling tasks, as the latter could be more efficient, could be integrated seamlessly in image-to-image network architectures as used in many other tasks, and could be more accurate for objects that are not well approximated by bounding boxes. In this paper we show theoretically and empirically that constructing dense pixel embeddings that can separate object instances cannot be easily achieved using convolutional operators. At the same time, we show that simple modifications, which we call semi-convolutional, have a much better chance of succeeding at this task. We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network. We demonstrate that these operators can also be used to improve approaches such as Mask RCNN, demonstrating better segmentation of complex biological shapes and PASCAL VOC categories than achievable by Mask RCNN alone.



### Few Shot Learning with Simplex
- **Arxiv ID**: http://arxiv.org/abs/1807.10726v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10726v2)
- **Published**: 2018-07-27 16:52:57+00:00
- **Updated**: 2018-10-30 00:59:36+00:00
- **Authors**: Bowen Zhang, Xifan Zhang, Fan Cheng, Deli Zhao
- **Comment**: There is still room for model improvement
- **Journal**: None
- **Summary**: Deep learning has made remarkable achievement in many fields. However, learning the parameters of neural networks usually demands a large amount of labeled data. The algorithms of deep learning, therefore, encounter difficulties when applied to supervised learning where only little data are available. This specific task is called few-shot learning. To address it, we propose a novel algorithm for few-shot learning using discrete geometry, in the sense that the samples in a class are modeled as a reduced simplex. The volume of the simplex is used for the measurement of class scatter. During testing, combined with the test sample and the points in the class, a new simplex is formed. Then the similarity between the test sample and the class can be quantized with the ratio of volumes of the new simplex to the original class simplex. Moreover, we present an approach to constructing simplices using local regions of feature maps yielded by convolutional neural networks. Experiments on Omniglot and miniImageNet verify the effectiveness of our simplex algorithm on few-shot learning.



### An Algorithm for Learning Shape and Appearance Models without Annotations
- **Arxiv ID**: http://arxiv.org/abs/1807.10731v1
- **DOI**: 10.1016/j.media.2019.04.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10731v1)
- **Published**: 2018-07-27 16:59:22+00:00
- **Updated**: 2018-07-27 16:59:22+00:00
- **Authors**: John Ashburner, Mikael Brudfors, Kevin Bronik, Yael Balbastre
- **Comment**: 61 pages, 16 figures (some downsampled by a factor of 4), submitted
  to MedIA
- **Journal**: None
- **Summary**: This paper presents a framework for automatically learning shape and appearance models for medical (and certain other) images. It is based on the idea that having a more accurate shape and appearance model leads to more accurate image registration, which in turn leads to a more accurate shape and appearance model. This leads naturally to an iterative scheme, which is based on a probabilistic generative model that is fit using Gauss-Newton updates within an EM-like framework. It was developed with the aim of enabling distributed privacy-preserving analysis of brain image data, such that shared information (shape and appearance basis functions) may be passed across sites, whereas latent variables that encode individual images remain secure within each site. These latent variables are proposed as features for privacy-preserving data mining applications.   The approach is demonstrated qualitatively on the KDEF dataset of 2D face images, showing that it can align images that traditionally require shape and appearance models trained using manually annotated data (manually defined landmarks etc.). It is applied to MNIST dataset of handwritten digits to show its potential for machine learning applications, particularly when training data is limited. The model is able to handle ``missing data'', which allows it to be cross-validated according to how well it can predict left-out voxels. The suitability of the derived features for classifying individuals into patient groups was assessed by applying it to a dataset of over 1,900 segmented T1-weighted MR images, which included images from the COBRE and ABIDE datasets.



### Attention-based Active Visual Search for Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1807.10744v1
- **DOI**: 10.1007/s10514-019-09882-z
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.10744v1)
- **Published**: 2018-07-27 17:48:02+00:00
- **Updated**: 2018-07-27 17:48:02+00:00
- **Authors**: Amir Rasouli, Pablo Lanillos, Gordon Cheng, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: We present an active visual search model for finding objects in unknown environments. The proposed algorithm guides the robot towards the sought object using the relevant stimuli provided by the visual sensors. Existing search strategies are either purely reactive or use simplified sensor models that do not exploit all the visual information available. In this paper, we propose a new model that actively extracts visual information via visual attention techniques and, in conjunction with a non-myopic decision-making algorithm, leads the robot to search more relevant areas of the environment. The attention module couples both top-down and bottom-up attention models enabling the robot to search regions with higher importance first.   The proposed algorithm is evaluated on a mobile robot platform in a 3D simulated environment. The results indicate that the use of visual attention significantly improves search, but the degree of improvement depends on the nature of the task and the complexity of the environment. In our experiments, we found that performance enhancements of up to 42\% in structured and 38\% in highly unstructured cluttered environments can be achieved using visual attention mechanisms.



### Gated Fusion Network for Joint Image Deblurring and Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1807.10806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10806v1)
- **Published**: 2018-07-27 19:17:02+00:00
- **Updated**: 2018-07-27 19:17:02+00:00
- **Authors**: Xinyi Zhang, Hang Dong, Zhe Hu, Wei-Sheng Lai, Fei Wang, Ming-Hsuan Yang
- **Comment**: Accepted as an oral presentation at BMVC 2018
- **Journal**: None
- **Summary**: Single-image super-resolution is a fundamental task for vision applications to enhance the image quality with respect to spatial resolution. If the input image contains degraded pixels, the artifacts caused by the degradation could be amplified by super-resolution methods. Image blur is a common degradation source. Images captured by moving or still cameras are inevitably affected by motion blur due to relative movements between sensors and objects. In this work, we focus on the super-resolution task with the presence of motion blur. We propose a deep gated fusion convolution neural network to generate a clear high-resolution frame from a single natural image with severe blur. By decomposing the feature extraction step into two task-independent streams, the dual-branch design can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. Extensive experiments demonstrate that our method generates sharper super-resolved images from low-resolution inputs with high computational efficiency.



### CASED: Curriculum Adaptive Sampling for Extreme Data Imbalance
- **Arxiv ID**: http://arxiv.org/abs/1807.10819v1
- **DOI**: 10.1007/978-3-319-66179-7_73
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10819v1)
- **Published**: 2018-07-27 20:10:11+00:00
- **Updated**: 2018-07-27 20:10:11+00:00
- **Authors**: Andrew Jesson, Nicolas Guizard, Sina Hamidi Ghalehjegh, Damien Goblot, Florian Soudan, Nicolas Chapados
- **Comment**: 20th International Conference on Medical Image Computing and Computer
  Assisted Intervention 2017
- **Journal**: None
- **Summary**: We introduce CASED, a novel curriculum sampling algorithm that facilitates the optimization of deep learning segmentation or detection models on data sets with extreme class imbalance. We evaluate the CASED learning framework on the task of lung nodule detection in chest CT. In contrast to two-stage solutions, wherein nodule candidates are first proposed by a segmentation model and refined by a second detection stage, CASED improves the training of deep nodule segmentation models (e.g. UNet) to the point where state of the art results are achieved using only a trivial detection stage. CASED improves the optimization of deep segmentation models by allowing them to first learn how to distinguish nodules from their immediate surroundings, while continuously adding a greater proportion of difficult-to-classify global context, until uniformly sampling from the empirical data distribution. Using CASED during training yields a minimalist proposal to the lung nodule detection problem that tops the LUNA16 nodule detection benchmark with an average sensitivity score of 88.35%. Furthermore, we find that models trained using CASED are robust to nodule annotation quality by showing that comparable results can be achieved when only a point and radius for each ground truth nodule are provided during training. Finally, the CASED learning framework makes no assumptions with regard to imaging modality or segmentation target and should generalize to other medical imaging problems where class imbalance is a persistent problem.



### TBI Contusion Segmentation from MRI using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10839v1
- **DOI**: 10.1109/ISBI.2018.8363545
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10839v1)
- **Published**: 2018-07-27 21:56:05+00:00
- **Updated**: 2018-07-27 21:56:05+00:00
- **Authors**: Snehashis Roy, John A. Butman, Leighton Chan, Dzung L. Pham
- **Comment**: https://ieeexplore.ieee.org/abstract/document/8363545/, IEEE 15th
  International Symposium on Biomedical Imaging (ISBI 2018)
- **Journal**: None
- **Summary**: Traumatic brain injury (TBI) is caused by a sudden trauma to the head that may result in hematomas and contusions and can lead to stroke or chronic disability. An accurate quantification of the lesion volumes and their locations is essential to understand the pathophysiology of TBI and its progression. In this paper, we propose a fully convolutional neural network (CNN) model to segment contusions and lesions from brain magnetic resonance (MR) images of patients with TBI. The CNN architecture proposed here was based on a state of the art CNN architecture from Google, called Inception. Using a 3-layer Inception network, lesions are segmented from multi-contrast MR images. When compared with two recent TBI lesion segmentation methods, one based on CNN (called DeepMedic) and another based on random forests, the proposed algorithm showed improved segmentation accuracy on images of 18 patients with mild to severe TBI. Using a leave-one-out cross validation, the proposed model achieved a median Dice of 0.75, which was significantly better (p<0.01) than the two competing methods.



### Deep nested level sets: Fully automated segmentation of cardiac MR images in patients with pulmonary hypertension
- **Arxiv ID**: http://arxiv.org/abs/1807.10760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.10760v1)
- **Published**: 2018-07-27 22:38:12+00:00
- **Updated**: 2018-07-27 22:38:12+00:00
- **Authors**: Jinming Duan, Jo Schlemper, Wenjia Bai, Timothy J W Dawes, Ghalib Bello, Georgia Doumou, Antonio De Marvao, Declan P O'Regan, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a novel and accurate optimisation method for segmentation of cardiac MR (CMR) images in patients with pulmonary hypertension (PH). The proposed method explicitly takes into account the image features learned from a deep neural network. To this end, we estimate simultaneous probability maps over region and edge locations in CMR images using a fully convolutional network. Due to the distinct morphology of the heart in patients with PH, these probability maps can then be incorporated in a single nested level set optimisation framework to achieve multi-region segmentation with high efficiency. The proposed method uses an automatic way for level set initialisation and thus the whole optimisation is fully automated. We demonstrate that the proposed deep nested level set (DNLS) method outperforms existing state-of-the-art methods for CMR segmentation in PH patients.



### Synthesizing CT from Ultrashort Echo-Time MR Images via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10850v1
- **DOI**: 10.1007/978-3-319-68127-6_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10850v1)
- **Published**: 2018-07-27 22:43:12+00:00
- **Updated**: 2018-07-27 22:43:12+00:00
- **Authors**: Snehashis Roy, John A. Butman, Dzung L. Pham
- **Comment**: https://link.springer.com/chapter/10.1007/978-3-319-68127-6_3,
  International Workshop on Simulation and Synthesis in Medical Imaging
  (SASHIMI 2017)
- **Journal**: None
- **Summary**: With the increasing popularity of PET-MR scanners in clinical applications, synthesis of CT images from MR has been an important research topic. Accurate PET image reconstruction requires attenuation correction, which is based on the electron density of tissues and can be obtained from CT images. While CT measures electron density information for x-ray photons, MR images convey information about the magnetic properties of tissues. Therefore, with the advent of PET-MR systems, the attenuation coefficients need to be indirectly estimated from MR images. In this paper, we propose a fully convolutional neural network (CNN) based method to synthesize head CT from ultra-short echo-time (UTE) dual-echo MR images. Unlike traditional $T_1$-w images which do not have any bone signal, UTE images show some signal for bone, which makes it a good candidate for MR to CT synthesis. A notable advantage of our approach is that accurate results were achieved with a small training data set. Using an atlas of a single CT and dual-echo UTE pair, we train a deep neural network model to learn the transform of MR intensities to CT using patches. We compared our CNN based model with a state-of-the-art registration based as well as a Bayesian model based CT synthesis method, and showed that the proposed CNN model outperforms both of them. We also compared the proposed model when only $T_1$-w images are available instead of UTE, and show that UTE images produce better synthesis than using just $T_1$-w images.




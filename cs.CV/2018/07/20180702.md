# Arxiv Papers in cs.CV on 2018-07-02
### Accurate Weakly-Supervised Deep Lesion Segmentation using Large-Scale Clinical Annotations: Slice-Propagated 3D Mask Generation from 2D RECIST
- **Arxiv ID**: http://arxiv.org/abs/1807.01172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01172v1)
- **Published**: 2018-07-02 00:17:42+00:00
- **Updated**: 2018-07-02 00:17:42+00:00
- **Authors**: Jinzheng Cai, Youbao Tang, Le Lu, Adam P. Harrison, Ke Yan, Jing Xiao, Lin Yang, Ronald M. Summers
- **Comment**: 9 pages, 3 figures, accepted to MICCAI 2018. arXiv admin note:
  substantial text overlap with arXiv:1801.08614
- **Journal**: None
- **Summary**: Volumetric lesion segmentation from computed tomography (CT) images is a powerful means to precisely assess multiple time-point lesion/tumor changes. However, because manual 3D segmentation is prohibitively time consuming, current practices rely on an imprecise surrogate called response evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST markers are commonly found in current hospital picture and archiving systems (PACS), meaning they can provide a potentially powerful, yet extraordinarily challenging, source of weak supervision for full 3D segmentation. Toward this end, we introduce a convolutional neural network (CNN) based weakly supervised slice-propagated segmentation (WSSS) method to 1) generate the initial lesion segmentation on the axial RECIST-slice; 2) learn the data distribution on RECIST-slices; 3) extrapolate to segment the whole lesion slice by slice to finally obtain a volumetric segmentation. To validate the proposed method, we first test its performance on a fully annotated lymph node dataset, where WSSS performs comparably to its fully supervised counterparts. We then test on a comprehensive lesion dataset with 32,735 RECIST marks, where we report a mean Dice score of 92% on RECIST-marked slices and 76% on the entire 3D volumes.



### Confounding variables can degrade generalization performance of radiological deep learning models
- **Arxiv ID**: http://arxiv.org/abs/1807.00431v2
- **DOI**: 10.1371/journal.pmed.1002683
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00431v2)
- **Published**: 2018-07-02 01:57:38+00:00
- **Updated**: 2018-07-13 01:07:41+00:00
- **Authors**: John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, Eric K. Oermann
- **Comment**: None
- **Journal**: PLoS Med 15(11):e1002683 (2019)
- **Summary**: Early results in using convolutional neural networks (CNNs) on x-rays to diagnose disease have been promising, but it has not yet been shown that models trained on x-rays from one hospital or one group of hospitals will work equally well at different hospitals. Before these tools are used for computer-aided diagnosis in real-world clinical settings, we must verify their ability to generalize across a variety of hospital systems. A cross-sectional design was used to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays from NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904 patients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural comparisons, performance on chest x-rays from outside hospitals was significantly lower than on held-out x-rays from the original hospital systems. CNNs were able to detect where an x-ray was acquired (hospital system, hospital department) with extremely high accuracy and calibrate predictions accordingly. The performance of CNNs in diagnosing diseases on x-rays may reflect not only their ability to identify disease-specific imaging findings on x-rays, but also their ability to exploit confounding information. Estimates of CNN performance based on test data from hospital systems used for model training may overstate their likely real-world performance.



### Liver Lesion Detection from Weakly-labeled Multi-phase CT Volumes with a Grouped Single Shot MultiBox Detector
- **Arxiv ID**: http://arxiv.org/abs/1807.00436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00436v1)
- **Published**: 2018-07-02 02:31:33+00:00
- **Updated**: 2018-07-02 02:31:33+00:00
- **Authors**: Sang-gil Lee, Jae Seok Bae, Hyunjae Kim, Jung Hoon Kim, Sungroh Yoon
- **Comment**: Accepted at MICCAI 2018. 8 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: We present a focal liver lesion detection model leveraged by custom-designed multi-phase computed tomography (CT) volumes, which reflects real-world clinical lesion detection practice using a Single Shot MultiBox Detector (SSD). We show that grouped convolutions effectively harness richer information of the multi-phase data for the object detection model, while a naive application of SSD suffers from a generalization gap. We trained and evaluated the modified SSD model and recently proposed variants with our CT dataset of 64 subjects by five-fold cross validation. Our model achieved a 53.3% average precision score and ran in under three seconds per volume, outperforming the original model and state-of-the-art variants. Results show that the one-stage object detection model is a practical solution, which runs in near real-time and can learn an unbiased feature representation from a large-volume real-world detection dataset, which requires less tedious and time consuming construction of the weak phase-level bounding box labels.



### A non-convex approach to low-rank and sparse matrix decomposition
- **Arxiv ID**: http://arxiv.org/abs/1807.01276v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.01276v2)
- **Published**: 2018-07-02 03:25:42+00:00
- **Updated**: 2019-05-11 07:40:01+00:00
- **Authors**: Angang Cui, Meng Wen, Haiyang Li, Jigen Peng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a nonconvex approach to the problem of low-rank and sparse matrix decomposition. In our nonconvex method, we replace the rank function and the $l_{0}$-norm of a given matrix with a non-convex fraction function on the singular values and the elements of the matrix respectively. An alternative direction method of multipliers algorithm is utilized to solve our proposed nonconvex problem with the nonconvex fraction function penalty. Numerical experiments on some low-rank and sparse matrix decomposition problems show that our method performs very well in recovering low-rank matrices which are heavily corrupted by large sparse errors.



### Elastic Neural Networks: A Scalable Framework for Embedded Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1807.00453v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00453v2)
- **Published**: 2018-07-02 03:51:53+00:00
- **Updated**: 2018-10-02 09:50:04+00:00
- **Authors**: Yue Bai, Shuvra S. Bhattacharyya, Antti P. Happonen, Heikki Huttunen
- **Comment**: EUSIPCO 2018
- **Journal**: None
- **Summary**: We propose a new framework for image classification with deep neural networks. The framework introduces intermediate outputs to the computational graph of a network. This enables flexible control of the computational load and balances the tradeoff between accuracy and execution time.   Moreover, we present an interesting finding that the intermediate outputs can act as a regularizer at training time, improving the prediction accuracy. In the experimental section we demonstrate the performance of our proposed framework with various commonly used pretrained deep networks in the use case of apparent age estimation.



### Evenly Cascaded Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.00456v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1807.00456v2)
- **Published**: 2018-07-02 04:12:16+00:00
- **Updated**: 2018-07-27 07:49:01+00:00
- **Authors**: Chengxi Ye, Chinmaya Devaraj, Michael Maynord, Cornelia Fermüller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Evenly Cascaded convolutional Network (ECN), a neural network taking inspiration from the cascade algorithm of wavelet analysis. ECN employs two feature streams - a low-level and high-level steam. At each layer these streams interact, such that low-level features are modulated using advanced perspectives from the high-level stream. ECN is evenly structured through resizing feature map dimensions by a consistent ratio, which removes the burden of ad-hoc specification of feature map dimensions. ECN produces easily interpretable features maps, a result whose intuition can be understood in the context of scale-space theory. We demonstrate that ECN's design facilitates the training process through providing easily trainable shortcuts. We report new state-of-the-art results for small networks, without the need for additional treatment such as pruning or compression - a consequence of ECN's simple structure and direct training. A 6-layered ECN design with under 500k parameters achieves 95.24% and 78.99% accuracy on CIFAR-10 and CIFAR-100 datasets, respectively, outperforming the current state-of-the-art on small parameter networks, and a 3 million parameter ECN produces results competitive to the state-of-the-art.



### Adversarial Perturbations Against Real-Time Video Classification Systems
- **Arxiv ID**: http://arxiv.org/abs/1807.00458v1
- **DOI**: 10.14722/ndss.2019.23202
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00458v1)
- **Published**: 2018-07-02 04:25:46+00:00
- **Updated**: 2018-07-02 04:25:46+00:00
- **Authors**: Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy Chowdhury, Ananthram Swami
- **Comment**: None
- **Journal**: Network and Distributed Systems Security (NDSS) Symposium 2019
  24-27 February 2019, San Diego, CA, USA
- **Summary**: Recent research has demonstrated the brittleness of machine learning systems to adversarial perturbations. However, the studies have been mostly limited to perturbations on images and more generally, classification that does not deal with temporally varying inputs. In this paper we ask "Are adversarial perturbations possible in real-time video classification systems and if so, what properties must they satisfy?" Such systems find application in surveillance applications, smart vehicles, and smart elderly care and thus, misclassification could be particularly harmful (e.g., a mishap at an elderly care facility may be missed). We show that accounting for temporal structure is key to generating adversarial examples in such systems. We exploit recent advances in generative adversarial network (GAN) architectures to account for temporal correlations and generate adversarial samples that can cause misclassification rates of over 80% for targeted activities. More importantly, the samples also leave other activities largely unaffected making them extremely stealthy. Finally, we also surprisingly find that in many scenarios, the same perturbation can be applied to every frame in a video clip that makes the adversary's ability to achieve misclassification relatively easy.



### Estimation of Large Motion in Lung CT by Integrating Regularized Keypoint Correspondences into Dense Deformable Registration
- **Arxiv ID**: http://arxiv.org/abs/1807.00467v1
- **DOI**: 10.1109/TMI.2017.2691259
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00467v1)
- **Published**: 2018-07-02 05:27:27+00:00
- **Updated**: 2018-07-02 05:27:27+00:00
- **Authors**: Jan Rühaak, Thomas Polzin, Stefan Heldmann, Ivor J. A. Simpson, Heinz Handels, Jan Modersitzki, Mattias P. Heinrich
- **Comment**: 12 pages, 7 figures, \c{opyright} 2017 IEEE. Personal use is
  permitted, but republication/redistribution requires IEEE permission
- **Journal**: IEEE Transactions on Medical Imaging, 2018, Vol. 36 (8), pp.
  1746-1757
- **Summary**: We present a novel algorithm for the registration of pulmonary CT scans. Our method is designed for large respiratory motion by integrating sparse keypoint correspondences into a dense continuous optimization framework. The detection of keypoint correspondences enables robustness against large deformations by jointly optimizing over a large number of potential discrete displacements, whereas the dense continuous registration achieves subvoxel alignment with smooth transformations. Both steps are driven by the same normalized gradient fields data term. We employ curvature regularization and a volume change control mechanism to prevent foldings of the deformation grid and restrict the determinant of the Jacobian to physiologically meaningful values. Keypoint correspondences are integrated into the dense registration by a quadratic penalty with adaptively determined weight. Using a parallel matrix-free derivative calculation scheme, a runtime of about 5 min was realized on a standard PC. The proposed algorithm ranks first in the EMPIRE10 challenge on pulmonary image registration. Moreover, it achieves an average landmark distance of 0.82 mm on the DIR-Lab COPD database, thereby improving upon the state of the art in accuracy by 15%. Our algorithm is the first to reach the inter-observer variability in landmark annotation on this dataset.



### An initial study on estimating area of a leaf using image processing
- **Arxiv ID**: http://arxiv.org/abs/1807.00487v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00487v1)
- **Published**: 2018-07-02 06:47:47+00:00
- **Updated**: 2018-07-02 06:47:47+00:00
- **Authors**: G. D. Illeperuma
- **Comment**: None
- **Journal**: None
- **Summary**: Calculating leaf area is very important. Computer aided image processing can make this faster and more accurate. This include scanning the leaf , converting it to binary image and calculation of number of pixels covered. Later this is converted to mm2.



### Active Testing: An Efficient and Robust Framework for Estimating Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1807.00493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00493v1)
- **Published**: 2018-07-02 07:18:44+00:00
- **Updated**: 2018-07-02 07:18:44+00:00
- **Authors**: Phuc Nguyen, Deva Ramanan, Charless Fowlkes
- **Comment**: accepted to ICML 2018
- **Journal**: None
- **Summary**: Much recent work on visual recognition aims to scale up learning to massive, noisily-annotated datasets. We address the problem of scaling- up the evaluation of such models to large-scale datasets with noisy labels. Current protocols for doing so require a human user to either vet (re-annotate) a small fraction of the test set and ignore the rest, or else correct errors in annotation as they are found through manual inspection of results. In this work, we re-formulate the problem as one of active testing, and examine strategies for efficiently querying a user so as to obtain an accu- rate performance estimate with minimal vetting. We demonstrate the effectiveness of our proposed active testing framework on estimating two performance metrics, Precision@K and mean Average Precision, for two popular computer vision tasks, multi-label classification and instance segmentation. We further show that our approach is able to save significant human annotation effort and is more robust than alternative evaluation protocols.



### Estimating Phenotypic Traits From UAV Based RGB Imagery
- **Arxiv ID**: http://arxiv.org/abs/1807.00498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00498v1)
- **Published**: 2018-07-02 07:32:46+00:00
- **Updated**: 2018-07-02 07:32:46+00:00
- **Authors**: Javier Ribera, Fangning He, Yuhao Chen, Ayman F. Habib, Edward J. Delp
- **Comment**: 8 pages, double-column
- **Journal**: None
- **Summary**: In many agricultural applications one wants to characterize physical properties of plants and use the measurements to predict, for example biomass and environmental influence. This process is known as phenotyping. Traditional collection of phenotypic information is labor-intensive and time-consuming. Use of imagery is becoming popular for phenotyping. In this paper, we present methods to estimate traits of sorghum plants from RBG cameras on board of an unmanned aerial vehicle (UAV). The position and orientation of the imagery together with the coordinates of sparse points along the area of interest are derived through a new triangulation method. A rectified orthophoto mosaic is then generated from the imagery. The number of leaves is estimated and a model-based method to analyze the leaf morphology for leaf segmentation is proposed. We present a statistical model to find the location of each individual sorghum plant.



### Leveraging Uncertainty Estimates for Predicting Segmentation Quality
- **Arxiv ID**: http://arxiv.org/abs/1807.00502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00502v1)
- **Published**: 2018-07-02 07:42:51+00:00
- **Updated**: 2018-07-02 07:42:51+00:00
- **Authors**: Terrance DeVries, Graham W. Taylor
- **Comment**: None
- **Journal**: None
- **Summary**: The use of deep learning for medical imaging has seen tremendous growth in the research community. One reason for the slow uptake of these systems in the clinical setting is that they are complex, opaque and tend to fail silently. Outside of the medical imaging domain, the machine learning community has recently proposed several techniques for quantifying model uncertainty (i.e.~a model knowing when it has failed). This is important in practical settings, as we can refer such cases to manual inspection or correction by humans. In this paper, we aim to bring these recent results on estimating uncertainty to bear on two important outputs in deep learning-based segmentation. The first is producing spatial uncertainty maps, from which a clinician can observe where and why a system thinks it is failing. The second is quantifying an image-level prediction of failure, which is useful for isolating specific cases and removing them from automated pipelines. We also show that reasoning about spatial uncertainty, the first output, is a useful intermediate representation for generating segmentation quality predictions, the second output. We propose a two-stage architecture for producing these measures of uncertainty, which can accommodate any deep learning-based medical segmentation pipeline.



### Deep Reasoning with Knowledge Graph for Social Relationship Understanding
- **Arxiv ID**: http://arxiv.org/abs/1807.00504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00504v1)
- **Published**: 2018-07-02 07:48:50+00:00
- **Updated**: 2018-07-02 07:48:50+00:00
- **Authors**: Zhouxia Wang, Tianshui Chen, Jimmy Ren, Weihao Yu, Hui Cheng, Liang Lin
- **Comment**: Accepted at IJCAI 2018. The first work that integrates high-level
  knowledge graph to reason about social relationships between person pair of
  interest in still image
- **Journal**: None
- **Summary**: Social relationships (e.g., friends, couple etc.) form the basis of the social network in our daily life. Automatically interpreting such relationships bears a great potential for the intelligent systems to understand human behavior in depth and to better interact with people at a social level. Human beings interpret the social relationships within a group not only based on the people alone, and the interplay between such social relationships and the contextual information around the people also plays a significant role. However, these additional cues are largely overlooked by the previous studies. We found that the interplay between these two factors can be effectively modeled by a novel structured knowledge graph with proper message propagation and attention. And this structured knowledge can be efficiently integrated into the deep neural network architecture to promote social relationship understanding by an end-to-end trainable Graph Reasoning Model (GRM), in which a propagation mechanism is learned to propagate node message through the graph to explore the interaction between persons of interest and the contextual objects. Meanwhile, a graph attentional mechanism is introduced to explicitly reason about the discriminative objects to promote recognition. Extensive experiments on the public benchmarks demonstrate the superiority of our method over the existing leading competitors.



### Knowledge-Embedded Representation Learning for Fine-Grained Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.00505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00505v1)
- **Published**: 2018-07-02 07:49:06+00:00
- **Updated**: 2018-07-02 07:49:06+00:00
- **Authors**: Tianshui Chen, Liang Lin, Riquan Chen, Yang Wu, Xiaonan Luo
- **Comment**: Accepted at IJCAI 2018. The first work that introduces high-level
  knowledge to enhance representation learning for fine-grained image
  classification
- **Journal**: None
- **Summary**: Humans can naturally understand an image in depth with the aid of rich knowledge accumulated from daily lives or professions. For example, to achieve fine-grained image recognition (e.g., categorizing hundreds of subordinate categories of birds) usually requires a comprehensive visual concept organization including category labels and part-level attributes. In this work, we investigate how to unify rich professional knowledge with deep neural network architectures and propose a Knowledge-Embedded Representation Learning (KERL) framework for handling the problem of fine-grained image recognition. Specifically, we organize the rich visual concepts in the form of knowledge graph and employ a Gated Graph Neural Network to propagate node message through the graph for generating the knowledge representation. By introducing a novel gated mechanism, our KERL framework incorporates this knowledge representation into the discriminative image feature learning, i.e., implicitly associating the specific attributes with the feature maps. Compared with existing methods of fine-grained image classification, our KERL framework has several appealing properties: i) The embedded high-level knowledge enhances the feature representation, thus facilitating distinguishing the subtle differences among subordinate categories. ii) Our framework can learn feature maps with a meaningful configuration that the highlighted regions finely accord with the nodes (specific attributes) of the knowledge graph. Extensive experiments on the widely used Caltech-UCSD bird dataset demonstrate the superiority of our KERL framework over existing state-of-the-art methods.



### COSMO: Contextualized Scene Modeling with Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1807.00511v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.00511v2)
- **Published**: 2018-07-02 08:07:36+00:00
- **Updated**: 2018-12-19 15:20:39+00:00
- **Authors**: Ilker Bozcan, Sinan Kalkan
- **Comment**: 40 pages, 15 figures, 9 tables, accepted to the Robotics and
  Autonomous Systems (RAS) special issue on Semantic Policy and Action
  Representations for Autonomous Robots (SPAR)
- **Journal**: None
- **Summary**: Scene modeling is very crucial for robots that need to perceive, reason about and manipulate the objects in their environments. In this paper, we adapt and extend Boltzmann Machines (BMs) for contextualized scene modeling. Although there are many models on the subject, ours is the first to bring together objects, relations, and affordances in a highly-capable generative model. For this end, we introduce a hybrid version of BMs where relations and affordances are introduced with shared, tri-way connections into the model. Moreover, we contribute a dataset for relation estimation and modeling studies. We evaluate our method in comparison with several baselines on object estimation, out-of-context object detection, relation estimation, and affordance estimation tasks. Moreover, to illustrate the generative capability of the model, we show several example scenes that the model is able to generate.



### Women also Snowboard: Overcoming Bias in Captioning Models (Extended Abstract)
- **Arxiv ID**: http://arxiv.org/abs/1807.00517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00517v1)
- **Published**: 2018-07-02 08:15:11+00:00
- **Updated**: 2018-07-02 08:15:11+00:00
- **Authors**: Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, Anna Rohrbach
- **Comment**: Burns and Hendricks contributed equally. 2018 ICML Workshop on
  Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2018)
- **Journal**: None
- **Summary**: Most machine learning methods are known to capture and exploit biases of the training data. While some biases are beneficial for learning, others are harmful. Specifically, image captioning models tend to exaggerate biases present in training data. This can lead to incorrect captions in domains where unbiased captions are desired, or required, due to over reliance on the learned prior and image context. We investigate generation of gender specific caption words (e.g. man, woman) based on the person's appearance or the image context. We introduce a new Equalizer model that ensures equal gender probability when gender evidence is occluded in a scene and confident predictions when gender evidence is present. The resulting model is forced to look at a person rather than use contextual cues to make a gender specific prediction. The losses that comprise our model, the Appearance Confusion Loss and the Confident Loss, are general, and can be added to any description model in order to mitigate impacts of unwanted bias in a description dataset. Our proposed model has lower error than prior work when describing images with people and mentioning their gender and more closely matches the ground truth ratio of sentences including women to sentences including men.



### SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1807.00537v1
- **DOI**: 10.1016/j.jvcir.2019.01.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00537v1)
- **Published**: 2018-07-02 08:53:01+00:00
- **Updated**: 2018-07-02 08:53:01+00:00
- **Authors**: Xing Fan, Wei Jiang, Hao Luo, Mengjuan Fei
- **Comment**: Contribute to Journal of Visual Communication and Image
  Representation
- **Journal**: None
- **Summary**: Many current successful Person Re-Identification(ReID) methods train a model with the softmax loss function to classify images of different persons and obtain the feature vectors at the same time. However, the underlying feature embedding space is ignored. In this paper, we use a modified softmax function, termed Sphere Softmax, to solve the classification problem and learn a hypersphere manifold embedding simultaneously. A balanced sampling strategy is also introduced. Finally, we propose a convolutional neural network called SphereReID adopting Sphere Softmax and training a single model end-to-end with a new warming-up learning rate schedule on four challenging datasets including Market-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results demonstrate that this single model outperforms the state-of-the-art methods on all four datasets without fine-tuning or re-ranking. For example, it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID. The code and trained weights of our model will be released.



### Studio2Shop: from studio photo shoots to fashion articles
- **Arxiv ID**: http://arxiv.org/abs/1807.00556v1
- **DOI**: 10.5220/0006544500370048
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00556v1)
- **Published**: 2018-07-02 09:26:58+00:00
- **Updated**: 2018-07-02 09:26:58+00:00
- **Authors**: Julia Lasserre, Katharina Rasch, Roland Vollgraf
- **Comment**: 12 pages, 9 figures (Figure 1 has 5 subfigures, Figure 2 has 3
  subfigures), 7 tables
- **Journal**: Proceedings of the 7th International Conference on Pattern
  Recognition Applications and Methods (January 16-18, 2018, in Funchal,
  Madeira, Portugal), Vol. 1 (ISBN 978-989-758-276-9), P. 37-48
- **Summary**: Fashion is an increasingly important topic in computer vision, in particular the so-called street-to-shop task of matching street images with shop images containing similar fashion items. Solving this problem promises new means of making fashion searchable and helping shoppers find the articles they are looking for. This paper focuses on finding pieces of clothing worn by a person in full-body or half-body images with neutral backgrounds. Such images are ubiquitous on the web and in fashion blogs, and are typically studio photos, we refer to this setting as studio-to-shop. Recent advances in computational fashion include the development of domain-specific numerical representations. Our model Studio2Shop builds on top of such representations and uses a deep convolutional network trained to match a query image to the numerical feature vectors of all the articles annotated in this image. Top-$k$ retrieval evaluation on test query images shows that the correct items are most often found within a range that is sufficiently small for building realistic visual search engines for the studio-to-shop setting.



### Classifying neuromorphic data using a deep learning framework for image classification
- **Arxiv ID**: http://arxiv.org/abs/1807.00578v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1807.00578v1)
- **Published**: 2018-07-02 10:18:37+00:00
- **Updated**: 2018-07-02 10:18:37+00:00
- **Authors**: Roshan Gopalakrishnan, Yansong Chua, Laxmi R Iyer
- **Comment**: 4 pages, 3 figures, submitted to ICARCV 2018
- **Journal**: None
- **Summary**: In the field of artificial intelligence, neuromorphic computing has been around for several decades. Deep learning has however made much recent progress such that it consistently outperforms neuromorphic learning algorithms in classification tasks in terms of accuracy. Specifically in the field of image classification, neuromorphic computing has been traditionally using either the temporal or rate code for encoding static images in datasets into spike trains. It is only till recently, that neuromorphic vision sensors are widely used by the neuromorphic research community, and provides an alternative to such encoding methods. Since then, several neuromorphic datasets as obtained by applying such sensors on image datasets (e.g. the neuromorphic CALTECH 101) have been introduced. These data are encoded in spike trains and hence seem ideal for benchmarking of neuromorphic learning algorithms. Specifically, we train a deep learning framework used for image classification on the CALTECH 101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We obtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic CALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to the best reported accuracy, while for neuromorphic CALTECH 101, it outperforms the last best reported accuracy by over 10%. This raises the question of the suitability of such datasets as benchmarks for neuromorphic learning algorithms.



### Sample Efficient Semantic Segmentation using Rotation Equivariant Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.00583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00583v1)
- **Published**: 2018-07-02 10:31:05+00:00
- **Updated**: 2018-07-02 10:31:05+00:00
- **Authors**: Jasper Linmans, Jim Winkens, Bastiaan S. Veeling, Taco S. Cohen, Max Welling
- **Comment**: Presented at the ICML workshop: Towards learning with limited labels:
  Equivariance, Invariance, and Beyond, 2018
- **Journal**: None
- **Summary**: We propose a semantic segmentation model that exploits rotation and reflection symmetries. We demonstrate significant gains in sample efficiency due to increased weight sharing, as well as improvements in robustness to symmetry transformations. The group equivariant CNN framework is extended for segmentation by introducing a new equivariant (G->Z2)-convolution that transforms feature maps on a group to planar feature maps. Also, equivariant transposed convolution is formulated for up-sampling in an encoder-decoder network. To demonstrate improvements in sample efficiency we evaluate on multiple data regimes of a rotation-equivariant segmentation task: cancer metastases detection in histopathology images. We further show the effectiveness of exploiting more symmetries by varying the size of the group.



### A Pulmonary Nodule Detection Model Based on Progressive Resolution and Hierarchical Saliency
- **Arxiv ID**: http://arxiv.org/abs/1807.00598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00598v1)
- **Published**: 2018-07-02 11:05:30+00:00
- **Updated**: 2018-07-02 11:05:30+00:00
- **Authors**: Junjie Zhang, Yong Xia, Yanning Zhang
- **Comment**: 8 pages,4 figures,1 table
- **Journal**: None
- **Summary**: Detection of pulmonary nodules on chest CT is an essential step in the early diagnosis of lung cancer, which is critical for best patient care. Although a number of computer-aided nodule detection methods have been published in the literature, these methods still have two major drawbacks: missing out true nodules during the detection of nodule candidates and less-accurate identification of nodules from non-nodule. In this paper, we propose an automated pulmonary nodule detection algorithm that jointly combines progressive resolution and hierarchical saliency. Specifically, we design a 3D progressive resolution-based densely dilated FCN, namely the progressive resolution network (PRN), to detect nodule candidates inside the lung, and construct a densely dilated 3D CNN with hierarchical saliency, namely the hierarchical saliency network (HSN), to simultaneously identify genuine nodules from those candidates and estimate the diameters of nodules. We evaluated our algorithm on the benchmark LUng Nodule Analysis 2016 (LUNA16) dataset and achieved a state-of-the-art detection score. Our results suggest that the proposed algorithm can effectively detect pulmonary nodules on chest CT and accurately estimate their diameters.



### Crowd Counting using Deep Recurrent Spatial-Aware Network
- **Arxiv ID**: http://arxiv.org/abs/1807.00601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00601v1)
- **Published**: 2018-07-02 11:21:27+00:00
- **Updated**: 2018-07-02 11:21:27+00:00
- **Authors**: Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, Liang Lin
- **Comment**: Accepted to IJCAI 2018
- **Journal**: None
- **Summary**: Crowd counting from unconstrained scene images is a crucial task in many real-world applications like urban surveillance and management, but it is greatly challenged by the camera's perspective that causes huge appearance variations in people's scales and rotations. Conventional methods address such challenges by resorting to fixed multi-scale architectures that are often unable to cover the largely varied scales while ignoring the rotation variations. In this paper, we propose a unified neural network framework, named Deep Recurrent Spatial-Aware Network, which adaptively addresses the two issues in a learnable spatial transform module with a region-wise refinement process. Specifically, our framework incorporates a Recurrent Spatial-Aware Refinement (RSAR) module iteratively conducting two components: i) a Spatial Transformer Network that dynamically locates an attentional region from the crowd density map and transforms it to the suitable scale and rotation for optimal crowd estimation; ii) a Local Refinement Network that refines the density map of the attended region with residual learning. Extensive experiments on four challenging benchmarks show the effectiveness of our approach. Specifically, comparing with the existing best-performing methods, we achieve an improvement of 12% on the largest dataset WorldExpo'10 and 22.8% on the most challenging dataset UCF_CC_50.



### Multi-modal Egocentric Activity Recognition using Audio-Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1807.00612v3
- **DOI**: 10.1007/s11042-020-08789-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00612v3)
- **Published**: 2018-07-02 12:04:24+00:00
- **Updated**: 2020-04-30 08:31:52+00:00
- **Authors**: Mehmet Ali Arabacı, Fatih Özkan, Elif Surer, Peter Jančovič, Alptekin Temizel
- **Comment**: None
- **Journal**: Multimedia Tools and Applications (2020)
- **Summary**: Egocentric activity recognition in first-person videos has an increasing importance with a variety of applications such as lifelogging, summarization, assisted-living and activity tracking. Existing methods for this task are based on interpretation of various sensor information using pre-determined weights for each feature. In this work, we propose a new framework for egocentric activity recognition problem based on combining audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that purpose, firstly grid optical-flow, virtual-inertia feature, log-covariance, cuboid are extracted from the video. The audio signal is characterized using a "supervector", obtained based on Gaussian mixture modelling of frame-level features, followed by a maximum a-posteriori adaptation. Then, the extracted multi-modal features are adaptively fused by MKL classifiers in which both the feature and kernel selection/weighing and recognition tasks are performed together. The proposed framework was evaluated on a number of egocentric datasets. The results showed that using multi-modal features with MKL outperforms the existing methods.



### Mammography Dual View Mass Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1807.00637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00637v1)
- **Published**: 2018-07-02 12:52:24+00:00
- **Updated**: 2018-07-02 12:52:24+00:00
- **Authors**: Shaked Perek, Alon Hazan, Ella Barkan, Ayelet Akselrod-Ballin
- **Comment**: None
- **Journal**: None
- **Summary**: Standard breast cancer screening involves the acquisition of two mammography X-ray projections for each breast. Typically, a comparison of both views supports the challenging task of tumor detection and localization. We introduce a deep learning, patch-based Siamese network for lesion matching in dual-view mammography. Our locally-fitted approach generates a joint patch pair representation and comparison with a shared configuration between the two views. We performed a comprehensive set of experiments with the network on standard datasets, among them the large Digital Database for Screening Mammography (DDSM). We analyzed the effect of transfer learning with the network between different types of datasets and compared the network-based matching to using Euclidean distance by template matching. Finally, we evaluated the contribution of the matching network in a full detection pipeline. Experimental results demonstrate the promise of improved detection accuracy using our approach.



### PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.00652v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00652v2)
- **Published**: 2018-07-02 13:29:47+00:00
- **Updated**: 2018-11-24 03:41:48+00:00
- **Authors**: Mingyang Jiang, Yiran Wu, Tianqi Zhao, Zelin Zhao, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, 3D understanding research sheds light on extracting features from point cloud directly, which requires effective shape pattern description of point clouds. Inspired by the outstanding 2D shape descriptor SIFT, we design a module called PointSIFT that encodes information of different orientations and is adaptive to scale of shape. Specifically, an orientation-encoding unit is designed to describe eight crucial orientations, and multi-scale representation is achieved by stacking several orientation-encoding units. PointSIFT module can be integrated into various PointNet-based architecture to improve the representation ability. Extensive experiments show our PointSIFT-based framework outperforms state-of-the-art method on standard benchmark datasets. The code and trained model will be published accompanied by this paper.



### Learning to Personalize in Appearance-Based Gaze Tracking
- **Arxiv ID**: http://arxiv.org/abs/1807.00664v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00664v3)
- **Published**: 2018-07-02 13:59:51+00:00
- **Updated**: 2019-09-02 17:37:12+00:00
- **Authors**: Erik Lindén, Jonas Sjöstrand, Alexandre Proutiere
- **Comment**: None
- **Journal**: None
- **Summary**: Personal variations severely limit the performance of appearance-based gaze tracking. Adapting to these variations using standard neural network model adaptation methods is difficult. The problems range from overfitting, due to small amounts of training data, to underfitting, due to restrictive model architectures. We tackle these problems by introducing the SPatial Adaptive GaZe Estimator (SPAZE). By modeling personal variations as a low-dimensional latent parameter space, SPAZE provides just enough adaptability to capture the range of personal variations without being prone to overfitting. Calibrating SPAZE for a new person reduces to solving a small optimization problem. SPAZE achieves an error of 2.70 degrees with 9 calibration samples on MPIIGaze, improving on the state-of-the-art by 14 %. We contribute to gaze tracking research by empirically showing that personal variations are well-modeled as a 3-dimensional latent parameter space for each eye. We show that this low-dimensionality is expected by examining model-based approaches to gaze tracking. We also show that accurate head pose-free gaze tracking is possible.



### Introducing the Simulated Flying Shapes and Simulated Planar Manipulator Datasets
- **Arxiv ID**: http://arxiv.org/abs/1807.00703v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.00703v1)
- **Published**: 2018-07-02 14:20:24+00:00
- **Updated**: 2018-07-02 14:20:24+00:00
- **Authors**: Fabio Ferreira, Jonas Rothfuss, Eren Erdal Aksoy, You Zhou, Tamim Asfour
- **Comment**: technical documentation, 2 figures, links to repositories
- **Journal**: None
- **Summary**: We release two artificial datasets, Simulated Flying Shapes and Simulated Planar Manipulator that allow to test the learning ability of video processing systems. In particular, the dataset is meant as a tool which allows to easily assess the sanity of deep neural network models that aim to encode, reconstruct or predict video frame sequences. The datasets each consist of 90000 videos. The Simulated Flying Shapes dataset comprises scenes showing two objects of equal shape (rectangle, triangle and circle) and size in which one object approaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF planar manipulator that executes a pick-and-place task in which it has to place a size-varying circle on a squared platform. Different from other widely used datasets such as moving MNIST [1], [2], the two presented datasets involve goal-oriented tasks (e.g. the manipulator grasping an object and placing it on a platform), rather than showing random movements. This makes our datasets more suitable for testing prediction capabilities and the learning of sophisticated motions by a machine learning model. This technical document aims at providing an introduction into the usage of both datasets.



### Synthetic contrast enhancement in cardiac CT with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.01779v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1807.01779v1)
- **Published**: 2018-07-02 15:26:14+00:00
- **Updated**: 2018-07-02 15:26:14+00:00
- **Authors**: Gianmarco Santini, Lorena M. Zumbo, Nicola Martini, Gabriele Valvano, Andrea Leo, Andrea Ripoli, Francesco Avogliero, Dante Chiappino, Daniele Della Latta
- **Comment**: 8 pages,3 figures
- **Journal**: None
- **Summary**: In Europe the 20% of the CT scans cover the thoracic region. The acquired images contain information about the cardiovascular system that often remains latent due to the lack of contrast in the cardiac area. On the other hand, the contrast enhanced computed tomography (CECT) represents an imaging technique that allows to easily assess the cardiac chambers volumes and the contrast dynamics. With this work we aim to face the problem of extraction and presentation of these latent information, using a deep learning approach with convolutional neural networks. Starting from the extraction of relevant features from the image without contrast medium, we try to re-map them on features typical of CECT, to synthesize an image characterized by an attenuation in the cardiac chambers as if a virtually iodine contrast medium was injected. The purposes are to guarantee an estimation of the left cardiac chambers volume and to perform an evaluation of the contrast dynamics. Our approach is based on a deconvolutional network trained on a set of 120 patients who underwent both CT acquisitions in the same contrastographic arterial phase and the same cardiac phase. To ensure a reliable predicted CECT image, in terms of values and morphology, a custom loss function is defined by combining an error function to find a pixel-wise correspondence, which takes into account the similarity in term of Hounsfield units between the input and output images and by a cross-entropy computed on the binarized versions of the synthesized and of the real CECT image. The proposed method is finally tested on 20 subjects.



### Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1807.00751v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00751v6)
- **Published**: 2018-07-02 15:41:34+00:00
- **Updated**: 2018-12-23 15:09:29+00:00
- **Authors**: Zhiming Zhou, Yuxuan Song, Lantao Yu, Hongwei Wang, Jiadong Liang, Weinan Zhang, Zhihua Zhang, Yong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the underlying factor that leads to failure and success in the training of GANs. We study the property of the optimal discriminative function and show that in many GANs, the gradient from the optimal discriminative function is not reliable, which turns out to be the fundamental cause of failure in training of GANs. We further demonstrate that a well-defined distance metric does not necessarily guarantee the convergence of GANs. Finally, we prove in this paper that Lipschitz-continuity condition is a general solution to make the gradient of the optimal discriminative function reliable, and characterized the necessary condition where Lipschitz-continuity ensures the convergence, which leads to a broad family of valid GAN objectives under Lipschitz-continuity condition, where Wasserstein distance is one special case. We experiment with several new objectives, which are sound according to our theorems, and we found that, compared with Wasserstein distance, the outputs of the discriminator with new objectives are more stable and the final qualities of generated samples are also consistently higher than those produced by Wasserstein distance.



### Ambient Hidden Space of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.00780v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00780v1)
- **Published**: 2018-07-02 16:51:27+00:00
- **Updated**: 2018-07-02 16:51:27+00:00
- **Authors**: Xinhan Di, Pengqian Yu, Meng Tian
- **Comment**: Accepted for publication in Uncertainty in Deep Learning Workshop at
  Uncertainty in Artificial Intelligence (UAI) 2018
- **Journal**: Uncertainty in Deep Learning Workshop at Uncertainty in Artificial
  Intelligence (UAI) 2018
- **Summary**: Generative adversarial models are powerful tools to model structure in complex distributions for a variety of tasks. Current techniques for learning generative models require an access to samples which have high quality, and advanced generative models are applied to generate samples from noisy training data through ambient modules. However, the modules are only practical for the output space of the generator, and their application in the hidden space is not well studied. In this paper, we extend the ambient module to the hidden space of the generator, and provide the uniqueness condition and the corresponding strategy for the ambient hidden generator in the adversarial training process. We report the practicality of the proposed method on the benchmark dataset.



### PRED18: Dataset and Further Experiments with DAVIS Event Camera in Predator-Prey Robot Chasing
- **Arxiv ID**: http://arxiv.org/abs/1807.03128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03128v1)
- **Published**: 2018-07-02 18:07:18+00:00
- **Updated**: 2018-07-02 18:07:18+00:00
- **Authors**: Diederik Paul Moeys, Daniel Neil, Federico Corradi, Emmett Kerr, Philip Vance, Gautham Das, Sonya A. Coleman, Thomas M. McGinnity, Dermot Kerr, Tobi Delbruck
- **Comment**: 8 pages
- **Journal**: IEEE EBCCSP 2018
- **Summary**: Machine vision systems using convolutional neural networks (CNNs) for robotic applications are increasingly being developed. Conventional vision CNNs are driven by camera frames at constant sample rate, thus achieving a fixed latency and power consumption tradeoff. This paper describes further work on the first experiments of a closed-loop robotic system integrating a CNN together with a Dynamic and Active Pixel Vision Sensor (DAVIS) in a predator/prey scenario. The DAVIS, mounted on the predator Summit XL robot, produces frames at a fixed 15 Hz frame-rate and Dynamic Vision Sensor (DVS) histograms containing 5k ON and OFF events at a variable frame-rate ranging from 15-500 Hz depending on the robot speeds. In contrast to conventional frame-based systems, the latency and processing cost depends on the rate of change of the image. The CNN is trained offline on the 1.25h labeled dataset to recognize the position and size of the prey robot, in the field of view of the predator. During inference, combining the ten output classes of the CNN allows extracting the analog position vector of the prey relative to the predator with a mean 8.7% error in angular estimation. The system is compatible with conventional deep learning technology, but achieves a variable latency-power tradeoff that adapts automatically to the dynamics. Finally, investigations on the robustness of the algorithm, a human performance comparison and a deconvolution analysis are also explored.



### Make (Nearly) Every Neural Network Better: Generating Neural Network Ensembles by Weight Parameter Resampling
- **Arxiv ID**: http://arxiv.org/abs/1807.00847v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.00847v1)
- **Published**: 2018-07-02 18:12:32+00:00
- **Updated**: 2018-07-02 18:12:32+00:00
- **Authors**: Jiayi Liu, Samarth Tripathi, Unmesh Kurup, Mohak Shah
- **Comment**: Accepted at UAI Workshop on Uncertainty in Deep Learning
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have become increasingly popular in computer vision, natural language processing, and other areas. However, training and fine-tuning a deep learning model is computationally intensive and time-consuming. We propose a new method to improve the performance of nearly every model including pre-trained models. The proposed method uses an ensemble approach where the networks in the ensemble are constructed by reassigning model parameter values based on the probabilistic distribution of these parameters, calculated towards the end of the training process. For pre-trained models, this approach results in an additional training step (usually less than one epoch). We perform a variety of analysis using the MNIST dataset and validate the approach with a number of DNN models using pre-trained models on the ImageNet dataset.



### Client-Specific Anomaly Detection for Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.00848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00848v1)
- **Published**: 2018-07-02 18:19:03+00:00
- **Updated**: 2018-07-02 18:19:03+00:00
- **Authors**: Shervin Rahimzadeh Arashloo, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: The one-class anomaly detection approach has previously been found to be effective in face presentation attack detection, especially in an \textit{unseen} attack scenario, where the system is exposed to novel types of attacks. This work follows the same anomaly-based formulation of the problem and analyses the merits of deploying \textit{client-specific} information for face spoofing detection. We propose training one-class client-specific classifiers (both generative and discriminative) using representations obtained from pre-trained deep convolutional neural networks. Next, based on subject-specific score distributions, a distinct threshold is set for each client, which is then used for decision making regarding a test query. Through extensive experiments using different one-class systems, it is shown that the use of client-specific information in a one-class anomaly detection formulation (both in model construction as well as decision threshold tuning) improves the performance significantly. In addition, it is demonstrated that the same set of deep convolutional features used for the recognition purposes is effective for face presentation attack detection in the class-specific one-class anomaly detection paradigm.



### Semi-supervised Learning: Fusion of Self-supervised, Supervised Learning, and Multimodal Cues for Tactical Driver Behavior Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.00864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00864v1)
- **Published**: 2018-07-02 19:24:19+00:00
- **Updated**: 2018-07-02 19:24:19+00:00
- **Authors**: Athma Narayanan, Yi-Ting Chen, Srikanth Malla
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we presented a preliminary study for tactical driver behavior detection from untrimmed naturalistic driving recordings. While supervised learning based detection is a common approach, it suffers when labeled data is scarce. Manual annotation is both time-consuming and expensive. To emphasize this problem, we experimented on a 104-hour real-world naturalistic driving dataset with a set of predefined driving behaviors annotated. There are three challenges in the dataset. First, predefined driving behaviors are sparse in a naturalistic driving setting. Second, the distribution of driving behaviors is long-tail. Third, a huge intra-class variation is observed. To address these issues, recent self-supervised and supervised learning and fusion of multimodal cues are leveraged into our architecture design. Preliminary experiments and discussions are reported.



### Model-based Hand Pose Estimation for Generalized Hand Shape with Appearance Normalization
- **Arxiv ID**: http://arxiv.org/abs/1807.00898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00898v1)
- **Published**: 2018-07-02 21:27:08+00:00
- **Updated**: 2018-07-02 21:27:08+00:00
- **Authors**: Jan Wöhlke, Shile Li, Dongheui Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Since the emergence of large annotated datasets, state-of-the-art hand pose estimation methods have been mostly based on discriminative learning. Recently, a hybrid approach has embedded a kinematic layer into the deep learning structure in such a way that the pose estimates obey the physical constraints of human hand kinematics. However, the existing approach relies on a single person's hand shape parameters, which are fixed constants. Therefore, the existing hybrid method has problems to generalize to new, unseen hands. In this work, we extend the kinematic layer to make the hand shape parameters learnable. In this way, the learnt network can generalize towards arbitrary hand shapes. Furthermore, inspired by the idea of Spatial Transformer Networks, we apply a cascade of appearance normalization networks to decrease the variance in the input data. The input images are shifted, rotated, and globally scaled to a similar appearance. The effectiveness and limitations of our proposed approach are extensively evaluated on the Hands 2017 challenge dataset and the NYU dataset.



### Semantic Segmentation with Scarce Data
- **Arxiv ID**: http://arxiv.org/abs/1807.00911v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.00911v2)
- **Published**: 2018-07-02 22:06:11+00:00
- **Updated**: 2018-08-02 03:23:04+00:00
- **Authors**: Isay Katsman, Rohun Tripathi, Andreas Veit, Serge Belongie
- **Comment**: ICML 2018 Workshop, camera-ready version
- **Journal**: None
- **Summary**: Semantic segmentation is a challenging vision problem that usually necessitates the collection of large amounts of finely annotated data, which is often quite expensive to obtain. Coarsely annotated data provides an interesting alternative as it is usually substantially more cheap. In this work, we present a method to leverage coarsely annotated data along with fine supervision to produce better segmentation results than would be obtained when training using only the fine data. We validate our approach by simulating a scarce data setting with less than 200 low resolution images from the Cityscapes dataset and show that our method substantially outperforms solely training on the fine annotation data by an average of 15.52% mIoU and outperforms the coarse mask by an average of 5.28% mIoU.



### Recurrent-OctoMap: Learning State-based Map Refinement for Long-Term Semantic Mapping with 3D-Lidar Data
- **Arxiv ID**: http://arxiv.org/abs/1807.00925v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.00925v2)
- **Published**: 2018-07-02 23:28:37+00:00
- **Updated**: 2018-07-29 01:37:51+00:00
- **Authors**: Li Sun, Zhi Yan, Anestis Zaganidis, Cheng Zhao, Tom Duckett
- **Comment**: Accepted by IEEE Robotics and Automation Letters RA-L, 2018
- **Journal**: None
- **Summary**: This paper presents a novel semantic mapping approach, Recurrent-OctoMap, learned from long-term 3D Lidar data. Most existing semantic mapping approaches focus on improving semantic understanding of single frames, rather than 3D refinement of semantic maps (i.e. fusing semantic observations). The most widely-used approach for 3D semantic map refinement is a Bayesian update, which fuses the consecutive predictive probabilities following a Markov-Chain model. Instead, we propose a learning approach to fuse the semantic features, rather than simply fusing predictions from a classifier. In our approach, we represent and maintain our 3D map as an OctoMap, and model each cell as a recurrent neural network (RNN), to obtain a Recurrent-OctoMap. In this case, the semantic mapping process can be formulated as a sequence-to-sequence encoding-decoding problem. Moreover, in order to extend the duration of observations in our Recurrent-OctoMap, we developed a robust 3D localization and mapping system for successively mapping a dynamic environment using more than two weeks of data, and the system can be trained and deployed with arbitrary memory length. We validate our approach on the ETH long-term 3D Lidar dataset [1]. The experimental results show that our proposed approach outperforms the conventional "Bayesian update" approach.




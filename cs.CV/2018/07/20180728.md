# Arxiv Papers in cs.CV on 2018-07-28
### MaskConnect: Connectivity Learning by Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/1807.11473v1
- **DOI**: 10.1007/978-3-030-01228-1_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11473v1)
- **Published**: 2018-07-28 00:41:53+00:00
- **Updated**: 2018-07-28 00:41:53+00:00
- **Authors**: Karim Ahmed, Lorenzo Torresani
- **Comment**: ECCV 2018. arXiv admin note: substantial text overlap with
  arXiv:1709.09582
- **Journal**: ECCV 2018
- **Summary**: Although deep networks have recently emerged as the model of choice for many computer vision problems, in order to yield good results they often require time-consuming architecture search. To combat the complexity of design choices, prior work has adopted the principle of modularized design which consists in defining the network in terms of a composition of topologically identical or similar building blocks (a.k.a. modules). This reduces architecture search to the problem of determining the number of modules to compose and how to connect such modules. Again, for reasons of design complexity and training cost, previous approaches have relied on simple rules of connectivity, e.g., connecting each module to only the immediately preceding module or perhaps to all of the previous ones. Such simple connectivity rules are unlikely to yield the optimal architecture for the given problem.   In this work we remove these predefined choices and propose an algorithm to learn the connections between modules in the network. Instead of being chosen a priori by the human designer, the connectivity is learned simultaneously with the weights of the network by optimizing the loss function of the end task using a modified version of gradient descent. We demonstrate our connectivity learning method on the problem of multi-class image classification using two popular architectures: ResNet and ResNeXt. Experiments on four different datasets show that connectivity learning using our approach yields consistently higher accuracy compared to relying on traditional predefined rules of connectivity. Furthermore, in certain settings it leads to significant savings in number of parameters.



### Pairwise Body-Part Attention for Recognizing Human-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/1807.10889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10889v1)
- **Published**: 2018-07-28 04:37:18+00:00
- **Updated**: 2018-07-28 04:37:18+00:00
- **Authors**: Hao-Shu Fang, Jinkun Cao, Yu-Wing Tai, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In human-object interactions (HOI) recognition, conventional methods consider the human body as a whole and pay a uniform attention to the entire body region. They ignore the fact that normally, human interacts with an object by using some parts of the body. In this paper, we argue that different body parts should be paid with different attention in HOI recognition, and the correlations between different body parts should be further considered. This is because our body parts always work collaboratively. We propose a new pairwise body-part attention model which can learn to focus on crucial parts, and their correlations for HOI recognition. A novel attention based feature selection method and a feature representation scheme that can capture pairwise correlations between body parts are introduced in the model. Our proposed approach achieved 4% improvement over the state-of-the-art results in HOI recognition on the HICO dataset. We will make our model and source codes publicly available.



### Maximum Margin Metric Learning Over Discriminative Nullspace for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1807.10908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10908v1)
- **Published**: 2018-07-28 07:53:39+00:00
- **Updated**: 2018-07-28 07:53:39+00:00
- **Authors**: T M Feroz Ali, Subhasis Chaudhuri
- **Comment**: Accepted for ECCV 2018
- **Journal**: None
- **Summary**: In this paper we propose a novel metric learning framework called Nullspace Kernel Maximum Margin Metric Learning (NK3ML) which efficiently addresses the small sample size (SSS) problem inherent in person re-identification and offers a significant performance gain over existing state-of-the-art methods. Taking advantage of the very high dimensionality of the feature space, the metric is learned using a maximum margin criterion (MMC) over a discriminative nullspace where all training sample points of a given class map onto a single point, minimizing the within class scatter. A kernel version of MMC is used to obtain a better between class separation. Extensive experiments on four challenging benchmark datasets for person re-identification demonstrate that the proposed algorithm outperforms all existing methods. We obtain 99.8% rank-1 accuracy on the most widely accepted and challenging dataset VIPeR, compared to the previous state of the art being only 63.92%.



### Unsupervised Adversarial Depth Estimation using Cycled Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10915v1)
- **Published**: 2018-07-28 09:49:41+00:00
- **Updated**: 2018-07-28 09:49:41+00:00
- **Authors**: Andrea Pilzer, Dan Xu, Mihai Marian Puscas, Elisa Ricci, Nicu Sebe
- **Comment**: To appear in 3DV 2018. Code is available on GitHub
- **Journal**: None
- **Summary**: While recent deep monocular depth estimation approaches based on supervised regression have achieved remarkable performance, costly ground truth annotations are required during training. To cope with this issue, in this paper we present a novel unsupervised deep learning approach for predicting depth maps and show that the depth estimation task can be effectively tackled within an adversarial learning framework. Specifically, we propose a deep generative network that learns to predict the correspondence field i.e. the disparity map between two image views in a calibrated stereo camera setting. The proposed architecture consists of two generative sub-networks jointly trained with adversarial learning for reconstructing the disparity map and organized in a cycle such as to provide mutual constraints and supervision to each other. Extensive experiments on the publicly available datasets KITTI and Cityscapes demonstrate the effectiveness of the proposed model and competitive results with state of the art methods. The code and trained model are available on https://github.com/andrea-pilzer/unsup-stereo-depthGAN.



### Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data
- **Arxiv ID**: http://arxiv.org/abs/1807.10916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10916v1)
- **Published**: 2018-07-28 09:54:54+00:00
- **Updated**: 2018-07-28 09:54:54+00:00
- **Authors**: Yabin Zhang, Hui Tang, Kui Jia
- **Comment**: Accepted to ECCV 2018, Source code available at:
  https://github.com/YabinZhang1994/MetaFGNet, European Conference on Computer
  Vision (ECCV), 2018
- **Journal**: None
- **Summary**: Fine-grained visual categorization (FGVC) is challenging due in part to the fact that it is often difficult to acquire an enough number of training samples. To employ large models for FGVC without suffering from overfitting, existing methods usually adopt a strategy of pre-training the models using a rich set of auxiliary data, followed by fine-tuning on the target FGVC task. However, the objective of pre-training does not take the target task into account, and consequently such obtained models are suboptimal for fine-tuning. To address this issue, we propose in this paper a new deep FGVC model termed MetaFGNet. Training of MetaFGNet is based on a novel regularized meta-learning objective, which aims to guide the learning of network parameters so that they are optimal for adapting to the target FGVC task. Based on MetaFGNet, we also propose a simple yet effective scheme for selecting more useful samples from the auxiliary data. Experiments on benchmark FGVC datasets show the efficacy of our proposed method.



### Deep Leaf Segmentation Using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1807.10931v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10931v3)
- **Published**: 2018-07-28 13:05:04+00:00
- **Updated**: 2019-03-21 23:50:41+00:00
- **Authors**: Daniel Ward, Peyman Moghadam, Nicolas Hudson
- **Comment**: British Machine Vision Conference (BMVC) 2018 Proceedings. CVPPP
  Workshop at BMVC 2018. Dataset available for download at:
  https://research.csiro.au/robotics/databases/synthetic-arabidopsis-dataset/
- **Journal**: None
- **Summary**: Automated segmentation of individual leaves of a plant in an image is a prerequisite to measure more complex phenotypic traits in high-throughput phenotyping. Applying state-of-the-art machine learning approaches to tackle leaf instance segmentation requires a large amount of manually annotated training data. Currently, the benchmark datasets for leaf segmentation contain only a few hundred labeled training images. In this paper, we propose a framework for leaf instance segmentation by augmenting real plant datasets with generated synthetic images of plants inspired by domain randomisation. We train a state-of-the-art deep learning segmentation architecture (Mask-RCNN) with a combination of real and synthetic images of Arabidopsis plants. Our proposed approach achieves 90% leaf segmentation score on the A1 test set outperforming the-state-of-the-art approaches for the CVPPP Leaf Segmentation Challenge (LSC). Our approach also achieves 81% mean performance over all five test datasets.



### Unsupervised Learning of a Hierarchical Spiking Neural Network for Optical Flow Estimation: From Events to Global Motion Perception
- **Arxiv ID**: http://arxiv.org/abs/1807.10936v2
- **DOI**: 10.1109/TPAMI.2019.2903179
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10936v2)
- **Published**: 2018-07-28 13:37:41+00:00
- **Updated**: 2019-03-28 09:06:11+00:00
- **Authors**: Federico Paredes-Vall√©s, Kirk Y. W. Scheper, Guido C. H. E. de Croon
- **Comment**: 14 pages, 14 figures
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2019
- **Summary**: The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.



### PROPEL: Probabilistic Parametric Regression Loss for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10937v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10937v2)
- **Published**: 2018-07-28 13:41:44+00:00
- **Updated**: 2020-08-08 21:09:41+00:00
- **Authors**: Muhammad Asad, Rilwan Basaru, S M Masudur Rahman Al Arif, Greg Slabaugh
- **Comment**: 11 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: In recent years, Convolutional Neural Networks (CNNs) have enabled significant advancements to the state-of-the-art in computer vision. For classification tasks, CNNs have widely employed probabilistic output and have shown the significance of providing additional confidence for predictions. However, such probabilistic methodologies are not widely applicable for addressing regression problems using CNNs, as regression involves learning unconstrained continuous and, in many cases, multi-variate target variables. We propose a PRObabilistic Parametric rEgression Loss (PROPEL) that facilitates CNNs to learn parameters of probability distributions for addressing probabilistic regression problems. PROPEL is fully differentiable and, hence, can be easily incorporated for end-to-end training of existing CNN regression architectures using existing optimization algorithms. The proposed method is flexible as it enables learning complex unconstrained probabilities while being generalizable to higher dimensional multi-variate regression problems. We utilize a PROPEL-based CNN to address the problem of learning hand and head orientation from uncalibrated color images. Our experimental validation and comparison with existing CNN regression loss functions show that PROPEL improves the accuracy of a CNN by enabling probabilistic regression, while significantly reducing required model parameters by $10 \times$, resulting in improved generalization as compared to the existing state-of-the-art.



### RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours
- **Arxiv ID**: http://arxiv.org/abs/1807.10972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10972v1)
- **Published**: 2018-07-28 19:36:46+00:00
- **Updated**: 2018-07-28 19:36:46+00:00
- **Authors**: Raghav Mehta, Tal Arbel
- **Comment**: Accepted at Workshop on Simulation and Synthesis in Medical Imaging -
  SASHIMI2018 held in conjunction with the 21st International Conference on
  Medical Image Computing and Computer Assisted Intervention (MICCAI 2018)
- **Journal**: None
- **Summary**: Accurate synthesis of a full 3D MR image containing tumours from available MRI (e.g. to replace an image that is currently unavailable or corrupted) would provide a clinician as well as downstream inference methods with important complementary information for disease analysis. In this paper, we present an end-to-end 3D convolution neural network that takes a set of acquired MR image sequences (e.g. T1, T2, T1ce) as input and concurrently performs (1) regression of the missing full resolution 3D MRI (e.g. FLAIR) and (2) segmentation of the tumour into subtypes (e.g. enhancement, core). The hypothesis is that this would focus the network to perform accurate synthesis in the area of the tumour. Experiments on the BraTS 2015 and 2017 datasets [1] show that: (1) the proposed method gives better performance than state-of-the-art methods in terms of established global evaluation metrics (e.g. PSNR), (2) replacing real MR volumes with the synthesized MRI does not lead to significant degradation in tumour and sub-structure segmentation accuracy. The system further provides uncertainty estimates based on Monte Carlo (MC) dropout [11] for the synthesized volume at each voxel, permitting quantification of the system's confidence in the output at each location.



### Actor-Centric Relation Network
- **Arxiv ID**: http://arxiv.org/abs/1807.10982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10982v1)
- **Published**: 2018-07-28 22:48:27+00:00
- **Updated**: 2018-07-28 22:48:27+00:00
- **Authors**: Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy, Rahul Sukthankar, Cordelia Schmid
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level and model temporal context with 3D ConvNets. Here, we go one step further and model spatio-temporal relations to capture the interactions between human actors, relevant objects and scene elements essential to differentiate similar human actions. Our approach is weakly supervised and mines the relevant elements automatically with an actor-centric relational network (ACRN). ACRN computes and accumulates pair-wise relation information from actor and global scene features, and generates relation features for action classification. It is implemented as neural networks and can be trained jointly with an existing action detection system. We show that ACRN outperforms alternative approaches which capture relation information, and that the proposed framework improves upon the state-of-the-art performance on JHMDB and AVA. A visualization of the learned relation features confirms that our approach is able to attend to the relevant relations for each action.




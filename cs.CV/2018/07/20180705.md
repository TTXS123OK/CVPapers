# Arxiv Papers in cs.CV on 2018-07-05
### PortraitGAN for Flexible Portrait Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1807.01826v2
- **DOI**: 10.1017/ATSIP.2020.20
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01826v2)
- **Published**: 2018-07-05 01:52:15+00:00
- **Updated**: 2018-12-18 19:22:43+00:00
- **Authors**: Jiali Duan, Xiaoyuan Guo, Yuhang Song, Chao Yang, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: APSIPA Transactions on Signal and Information Processing 9 (2020)
  e22
- **Summary**: Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality portrait manipulation with photo-realistic effects.



### Detecting Tiny Moving Vehicles in Satellite Videos
- **Arxiv ID**: http://arxiv.org/abs/1807.01864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01864v1)
- **Published**: 2018-07-05 06:46:31+00:00
- **Updated**: 2018-07-05 06:46:31+00:00
- **Authors**: Wei Ao, Yanwei Fu, Feng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the satellite videos have been captured by a moving satellite platform. In contrast to consumer, movie, and common surveillance videos, satellite video can record the snapshot of the city-scale scene. In a broad field-of-view of satellite videos, each moving target would be very tiny and usually composed of several pixels in frames. Even worse, the noise signals also existed in the video frames, since the background of the video frame has the subpixel-level and uneven moving thanks to the motion of satellites. We argue that this is a new type of computer vision task since previous technologies are unable to detect such tiny vehicles efficiently. This paper proposes a novel framework that can identify the small moving vehicles in satellite videos. In particular, we offer a novel detecting algorithm based on the local noise modeling. We differentiate the potential vehicle targets from noise patterns by an exponential probability distribution. Subsequently, a multi-morphological-cue based discrimination strategy is designed to distinguish correct vehicle targets from a few existing noises further. Another significant contribution is to introduce a series of evaluation protocols to measure the performance of tiny moving vehicle detection systematically. We annotate a satellite video manually and use it to test our algorithms under different evaluation criterion. The proposed algorithm is also compared with the state-of-the-art baselines, and demonstrates the advantages of our framework over the benchmarks.



### Road surface 3d reconstruction based on dense subpixel disparity map estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.01874v2
- **DOI**: 10.1109/TIP.2018.2808770
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01874v2)
- **Published**: 2018-07-05 07:13:42+00:00
- **Updated**: 2022-05-22 03:24:39+00:00
- **Authors**: Rui Fan, Xiao Ai, Naim Dahnoun
- **Comment**: 11 pages, 16 figures, IEEE Transactions on Image Processing
- **Journal**: [J]. IEEE Transactions on Image Processing, 2018, 27(6): 3025-3035
- **Summary**: Various 3D reconstruction methods have enabled civil engineers to detect damage on a road surface. To achieve the millimetre accuracy required for road condition assessment, a disparity map with subpixel resolution needs to be used. However, none of the existing stereo matching algorithms are specially suitable for the reconstruction of the road surface. Hence in this paper, we propose a novel dense subpixel disparity estimation algorithm with high computational efficiency and robustness. This is achieved by first transforming the perspective view of the target frame into the reference view, which not only increases the accuracy of the block matching for the road surface but also improves the processing speed. The disparities are then estimated iteratively using our previously published algorithm where the search range is propagated from three estimated neighbouring disparities. Since the search range is obtained from the previous iteration, errors may occur when the propagated search range is not sufficient. Therefore, a correlation maxima verification is performed to rectify this issue, and the subpixel resolution is achieved by conducting a parabola interpolation enhancement. Furthermore, a novel disparity global refinement approach developed from the Markov Random Fields and Fast Bilateral Stereo is introduced to further improve the accuracy of the estimated disparity map, where disparities are updated iteratively by minimising the energy function that is related to their interpolated correlation polynomials. The algorithm is implemented in C language with a near real-time performance. The experimental results illustrate that the absolute error of the reconstruction varies from 0.1 mm to 3 mm.



### A Single Shot Text Detector with Scale-adaptive Anchors
- **Arxiv ID**: http://arxiv.org/abs/1807.01884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01884v1)
- **Published**: 2018-07-05 07:48:18+00:00
- **Updated**: 2018-07-05 07:48:18+00:00
- **Authors**: Qi Yuan, Bingwang Zhang, Haojie Li, Zhihui Wang, Zhongxuan Luo
- **Comment**: 8 pages, 6figures
- **Journal**: None
- **Summary**: Currently, most top-performing text detection networks tend to employ fixed-size anchor boxes to guide the search for text instances. They usually rely on a large amount of anchors with different scales to discover texts in scene images, thus leading to high computational cost. In this paper, we propose an end-to-end box-based text detector with scale-adaptive anchors, which can dynamically adjust the scales of anchors according to the sizes of underlying texts by introducing an additional scale regression layer. The proposed scale-adaptive anchors allow us to use a few number of anchors to handle multi-scale texts and therefore significantly improve the computational efficiency. Moreover, compared to discrete scales used in previous methods, the learned continuous scales are more reliable, especially for small texts detection. Additionally, we propose Anchor convolution to better exploit necessary feature information by dynamically adjusting the sizes of receptive fields according to the learned scales. Extensive experiments demonstrate that the proposed detector is fast, taking only $0.28$ second per image, while outperforming most state-of-the-art methods in accuracy.



### Volumetric performance capture from minimal camera viewpoints
- **Arxiv ID**: http://arxiv.org/abs/1807.01950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01950v2)
- **Published**: 2018-07-05 11:51:50+00:00
- **Updated**: 2018-07-10 11:25:21+00:00
- **Authors**: Andrew Gilbert, Marco Volino, John Collomosse, Adrian Hilton
- **Comment**: None
- **Journal**: None
- **Summary**: We present a convolutional autoencoder that enables high fidelity volumetric reconstructions of human performance to be captured from multi-view video comprising only a small set of camera views. Our method yields similar end-to-end reconstruction error to that of a probabilistic visual hull computed using significantly more (double or more) viewpoints. We use a deep prior implicitly learned by the autoencoder trained over a dataset of view-ablated multi-view video footage of a wide range of subjects and actions. This opens up the possibility of high-end volumetric performance capture in on-set and prosumer scenarios where time or cost prohibit a high witness camera count.



### Subpixel-Precise Tracking of Rigid Objects in Real-time
- **Arxiv ID**: http://arxiv.org/abs/1807.01952v1
- **DOI**: 10.1007/978-3-319-59126-1_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01952v1)
- **Published**: 2018-07-05 11:54:41+00:00
- **Updated**: 2018-07-05 11:54:41+00:00
- **Authors**: Tobias Böttger, Markus Ulrich, Carsten Steger
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel object tracking scheme that can track rigid objects in real time. The approach uses subpixel-precise image edges to track objects with high accuracy. It can determine the object position, scale, and rotation with subpixel-precision at around 80fps. The tracker returns a reliable score for each frame and is capable of self diagnosing a tracking failure. Furthermore, the choice of the similarity measure makes the approach inherently robust against occlusion, clutter, and nonlinear illumination changes. We evaluate the method on sequences from rigid objects from the OTB-2015 and VOT2016 dataset and discuss its performance. The evaluation shows that the tracker is more accurate than state-of-the-art real-time trackers while being equally robust.



### Jigsaw Puzzle Solving Using Local Feature Co-Occurrences in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.03155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03155v1)
- **Published**: 2018-07-05 12:19:09+00:00
- **Updated**: 2018-07-05 12:19:09+00:00
- **Authors**: Marie-Morgane Paumard, David Picard, Hedi Tabia
- **Comment**: ICIP 2018
- **Journal**: None
- **Summary**: Archaeologists are in dire need of automated object reconstruction methods. Fragments reassembly is close to puzzle problems, which may be solved by computer vision algorithms. As they are often beaten on most image related tasks by deep learning algorithms, we study a classification method that can solve jigsaw puzzles. In this paper, we focus on classifying the relative position: given a couple of fragments, we compute their local relation (e.g. on top). We propose several enhancements over the state of the art in this domain, which is outperformed by our method by 25\%. We propose an original dataset composed of pictures from the Metropolitan Museum of Art. We propose a greedy reconstruction method based on the predicted relative positions.



### Model-free Consensus Maximization for Non-Rigid Shapes
- **Arxiv ID**: http://arxiv.org/abs/1807.01963v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01963v2)
- **Published**: 2018-07-05 12:34:40+00:00
- **Updated**: 2018-08-13 13:10:07+00:00
- **Authors**: Thomas Probst, Ajad Chhatkuli, Danda Pani Paudel, Luc Van Gool
- **Comment**: ECCV18
- **Journal**: None
- **Summary**: Many computer vision methods use consensus maximization to relate measurements containing outliers with the correct transformation model. In the context of rigid shapes, this is typically done using Random Sampling and Consensus (RANSAC) by estimating an analytical model that agrees with the largest number of measurements (inliers). However, small parameter models may not be always available. In this paper, we formulate the model-free consensus maximization as an Integer Program in a graph using `rules' on measurements. We then provide a method to solve it optimally using the Branch and Bound (BnB) paradigm. We focus its application on non-rigid shapes, where we apply the method to remove outlier 3D correspondences and achieve performance superior to the state of the art. Our method works with outlier ratio as high as 80\%. We further derive a similar formulation for 3D template to image matching, achieving similar or better performance compared to the state of the art.



### Open Logo Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/1807.01964v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01964v3)
- **Published**: 2018-07-05 12:40:39+00:00
- **Updated**: 2018-09-27 10:58:01+00:00
- **Authors**: Hang Su, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted by BMVC 2018. The QMUL-OpenLogo benchmark is publicly
  available at: qmul-openlogo.github.io
- **Journal**: None
- **Summary**: Existing logo detection benchmarks consider artificial deployment scenarios by assuming that large training data with fine-grained bounding box annotations for each class are available for model training. Such assumptions are often invalid in realistic logo detection scenarios where new logo classes come progressively and require to be detected with little or none budget for exhaustively labelling fine-grained training data for every new class. Existing benchmarks are thus unable to evaluate the true performance of a logo detection method in realistic and open deployments. In this work, we introduce a more realistic and challenging logo detection setting, called Open Logo Detection. Specifically, this new setting assumes fine-grained labelling only on a small proportion of logo classes whilst the remaining classes have no labelled training data to simulate the open deployment. We further create an open logo detection benchmark, called OpenLogo,to promote the investigation of this new challenge. OpenLogo contains 27,083 images from 352 logo classes, built by aggregating/refining 7 existing datasets and establishing an open logo detection evaluation protocol. To address this challenge, we propose a Context Adversarial Learning (CAL) approach to synthesising training data with coherent logo instance appearance against diverse background context for enabling more effective optimisation of contemporary deep learning detection models. Experiments show the performance advantage of CAL over existing state-of-the-art alternative methods on the more realistic and challenging OpenLogo benchmark.



### Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.01972v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.01972v2)
- **Published**: 2018-07-05 12:52:07+00:00
- **Updated**: 2018-09-20 15:18:34+00:00
- **Authors**: Aram Ter-Sarkisov, Robert Ross, John Kelleher, Bernadette Earley, Michael Keane
- **Comment**: accepted at BMVC 2018
- **Journal**: None
- **Summary**: We present an instance segmentation algorithm trained and applied to a CCTV recording of beef cattle during a winter finishing period. A fully convolutional network was transformed into an instance segmentation network that learns to label each instance of an animal separately. We introduce a conceptually simple framework that the network uses to output a single prediction for every animal. These results are a contribution towards behaviour analysis in winter finishing beef cattle for early detection of animal welfare-related problems.



### Sparse Representation and Non-Negative Matrix Factorization for image denoise
- **Arxiv ID**: http://arxiv.org/abs/1807.03694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03694v1)
- **Published**: 2018-07-05 13:02:46+00:00
- **Updated**: 2018-07-05 13:02:46+00:00
- **Authors**: R. M. Farouk, M. E. Abd El-aziz, A. M. Adam
- **Comment**: None
- **Journal**: Journal of computer science approaches; Vol.4, Issue 2 Pages
  20-27;2017
- **Summary**: Recently, the problem of blind image separation has been widely investigated, especially the medical image denoise which is the main step in medical diag-nosis. Removing the noise without affecting relevant features of the image is the main goal. Sparse decomposition over redundant dictionaries become of the most used approaches to solve this problem. NMF codes naturally favor sparse, parts-based representations. In sparse representation, signals represented as a linear combination of a redundant dictionary atoms. In this paper, we propose an algorithm based on sparse representation over the redundant dictionary and Non-Negative Matrix Factorization (N-NMF). The algorithm initializes a dic-tionary based on training samples constructed from noised image, then it searches for the best representation for the source by using the approximate matching pursuit (AMP). The proposed N-NMF gives a better reconstruction of an image from denoised one. We have compared our numerical results with different image denoising techniques and we have found the performance of the proposed technique is promising. Keywords: Image denoising, sparse representation, dictionary learning, matching pursuit, non-negative matrix factorization.



### Revisiting Perspective Information for Efficient Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1807.01989v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01989v3)
- **Published**: 2018-07-05 13:33:28+00:00
- **Updated**: 2019-04-01 15:29:29+00:00
- **Authors**: Miaojing Shi, Zhaohui Yang, Chao Xu, Qijun Chen
- **Comment**: CVPR2019 camera-ready
- **Journal**: None
- **Summary**: Crowd counting is the task of estimating people numbers in crowd images. Modern crowd counting methods employ deep neural networks to estimate crowd counts via crowd density regressions. A major challenge of this task lies in the perspective distortion, which results in drastic person scale change in an image. Density regression on the small person area is in general very hard. In this work, we propose a perspective-aware convolutional neural network (PACNN) for efficient crowd counting, which integrates the perspective information into density regression to provide additional knowledge of the person scale change in an image. Ground truth perspective maps are firstly generated for training; PACNN is then specifically designed to predict multi-scale perspective maps, and encode them as perspective-aware weighting layers in the network to adaptively combine the outputs of multi-scale density maps. The weights are learned at every pixel of the maps such that the final density combination is robust to the perspective distortion. We conduct extensive experiments on the ShanghaiTech, WorldExpo'10, UCF_CC_50, and UCSD datasets, and demonstrate the effectiveness and efficiency of PACNN over the state-of-the-art.



### Acquire, Augment, Segment & Enjoy: Weakly Supervised Instance Segmentation of Supermarket Products
- **Arxiv ID**: http://arxiv.org/abs/1807.02001v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02001v2)
- **Published**: 2018-07-05 13:38:55+00:00
- **Updated**: 2018-07-06 07:22:22+00:00
- **Authors**: Patrick Follmann, Bertram Drost, Tobias Böttger
- **Comment**: None
- **Journal**: None
- **Summary**: Grocery stores have thousands of products that are usually identified using barcodes with a human in the loop. For automated checkout systems, it is necessary to count and classify the groceries efficiently and robustly. One possibility is to use a deep learning algorithm for instance-aware semantic segmentation. Such methods achieve high accuracies but require a large amount of annotated training data.   We propose a system to generate the training annotations in a weakly supervised manner, drastically reducing the labeling effort. We assume that for each training image, only the object class is known. The system automatically segments the corresponding object from the background. The obtained training data is augmented to simulate variations similar to those seen in real-world setups.



### Calamari - A High-Performance Tensorflow-based Deep Learning Package for Optical Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.02004v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02004v3)
- **Published**: 2018-07-05 13:46:37+00:00
- **Updated**: 2018-08-06 07:52:56+00:00
- **Authors**: Christoph Wick, Christian Reul, Frank Puppe
- **Comment**: 11 pages, 3 figures
- **Journal**: Digital Humanities Quarterly 14 (2), 2020
- **Summary**: Optical Character Recognition (OCR) on contemporary and historical data is still in the focus of many researchers. Especially historical prints require book specific trained OCR models to achieve applicable results (Springmann and L\"udeling, 2016, Reul et al., 2017a). To reduce the human effort for manually annotating ground truth (GT) various techniques such as voting and pretraining have shown to be very efficient (Reul et al., 2018a, Reul et al., 2018b). Calamari is a new open source OCR line recognition software that both uses state-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and giving native support for techniques such as pretraining and voting. The customizable network architectures constructed of Convolutional Neural Networks (CNNS) and Long-ShortTerm-Memory (LSTM) layers are trained by the so-called Connectionist Temporal Classification (CTC) algorithm of Graves et al. (2006). Optional usage of a GPU drastically reduces the computation times for both training and prediction. We use two different datasets to compare the performance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches a Character Error Rate (CER) of 0.11% on the UW3 dataset written in modern English and 0.18% on the DTA19 dataset written in German Fraktur, which considerably outperforms the results of the existing softwares.



### Improving Unsupervised Defect Segmentation by Applying Structural Similarity to Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1807.02011v3
- **DOI**: 10.5220/0007364503720380
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.02011v3)
- **Published**: 2018-07-05 14:07:23+00:00
- **Updated**: 2019-02-01 16:16:28+00:00
- **Authors**: Paul Bergmann, Sindy Löwe, Michael Fauser, David Sattlegger, Carsten Steger
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional autoencoders have emerged as popular methods for unsupervised defect segmentation on image data. Most commonly, this task is performed by thresholding a pixel-wise reconstruction error based on an $\ell^p$ distance. This procedure, however, leads to large residuals whenever the reconstruction encompasses slight localization inaccuracies around edges. It also fails to reveal defective regions that have been visually altered when intensity values stay roughly consistent. We show that these problems prevent these approaches from being applied to complex real-world scenarios and that it cannot be easily avoided by employing more elaborate architectures such as variational or feature matching autoencoders. We propose to use a perceptual loss function based on structural similarity which examines inter-dependencies between local image regions, taking into account luminance, contrast and structural information, instead of simply comparing single pixel values. It achieves significant performance gains on a challenging real-world dataset of nanofibrous materials and a novel dataset of two woven fabrics over the state of the art approaches for unsupervised defect segmentation that use pixel-wise reconstruction error metrics.



### Detection and Analysis of Content Creator Collaborations in YouTube Videos using Face- and Speaker-Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.02020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02020v1)
- **Published**: 2018-07-05 14:27:02+00:00
- **Updated**: 2018-07-05 14:27:02+00:00
- **Authors**: Moritz Lode, Michael Örtl, Christian Koch, Amr Rizk, Ralf Steinmetz
- **Comment**: None
- **Journal**: None
- **Summary**: This work discusses and implements the application of speaker recognition for the detection of collaborations in YouTube videos. CATANA, an existing framework for detection and analysis of YouTube collaborations, is utilizing face recognition for the detection of collaborators, which naturally performs poor on video-content without appearing faces. This work proposes an extension of CATANA using active speaker detection and speaker recognition to improve the detection accuracy.



### Reflection Analysis for Face Morphing Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.02030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1807.02030v1)
- **Published**: 2018-07-05 14:46:19+00:00
- **Updated**: 2018-07-05 14:46:19+00:00
- **Authors**: Clemens Seibold, Anna Hilsmann, Peter Eisert
- **Comment**: 5 pages, 6 figures, accepted at EUSIPCO 2018
- **Journal**: None
- **Summary**: A facial morph is a synthetically created image of a face that looks similar to two different individuals and can even trick biometric facial recognition systems into recognizing both individuals. This attack is known as face morphing attack. The process of creating such a facial morph is well documented and a lot of tutorials and software to create them are freely available. Therefore, it is mandatory to be able to detect this kind of fraud to ensure the integrity of the face as reliable biometric feature. In this work, we study the effects of face morphing on the physically correctness of the illumination. We estimate the direction to the light sources based on specular highlights in the eyes and use them to generate a synthetic map for highlights on the skin. This map is compared with the highlights in the image that is suspected to be a fraud. Morphing faces with different geometries, a bad alignment of the source images or using images with different illuminations, can lead to inconsistencies in reflections that indicate the existence of a morphing attack.



### Learning a Representation Map for Robot Navigation using Deep Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1807.02401v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.02401v2)
- **Published**: 2018-07-05 14:46:32+00:00
- **Updated**: 2018-09-13 19:22:04+00:00
- **Authors**: Kaixin Hu, Peter O'Connor
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this work is to use Variational Autoencoder (VAE) to learn a representation of an indoor environment that can be used for robot navigation. We use images extracted from a video, in which a camera takes a tour around a house, for training the VAE model with a 4 dimensional latent space. After the model is trained, each real frame has a corresponding representation point on manifold in the latent space, and each representation point has corresponding reconstructed image. For the navigation problem, we map the starting image and destination image to the latent space, then optimize a path on the learned manifold connecting the two points, and finally map the path back through decoder to a sequence of images. The ideal sequence of images should correspond to a route that is spatially continuous - i.e. neighbor images in the route should correspond to neighbor locations in physical space. Such a route could be used for navigation with computer vision techniques, i.e. a robot could follow the image sequence from starting location to destination in the environment step by step. We implement this algorithm, but find in our experimental results that the resulting route is not satisfactory. The route consist of several discontinuous image frames along the ideal routes, so that the route could not be followed by a robot with computer vision techniques in practice. In our evaluation, we propose two reasons for our failure to automatically find continuous routes: (1) The VAE tends to capture global structures, but discard the details; (2) the Euclidean similarity metric used for measuring continuity between house images is sub-optimal. For further work, we propose: trying other generative models like VAE-GANs which may be better at reconstructing the details to learn the representation map, and adjusting the similarity metric in the path selecting algorithm.



### Consistent Generative Query Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.02033v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.02033v3)
- **Published**: 2018-07-05 14:51:51+00:00
- **Updated**: 2019-04-22 00:51:13+00:00
- **Authors**: Ananya Kumar, S. M. Ali Eslami, Danilo J. Rezende, Marta Garnelo, Fabio Viola, Edward Lockhart, Murray Shanahan
- **Comment**: None
- **Journal**: None
- **Summary**: Stochastic video prediction models take in a sequence of image frames, and generate a sequence of consecutive future image frames. These models typically generate future frames in an autoregressive fashion, which is slow and requires the input and output frames to be consecutive. We introduce a model that overcomes these drawbacks by generating a latent representation from an arbitrary set of frames that can then be used to simultaneously and efficiently sample temporally consistent frames at arbitrary time-points. For example, our model can "jump" and directly sample frames at the end of the video, without sampling intermediate frames. Synthetic video evaluations confirm substantial gains in speed and functionality without loss in fidelity. We also apply our framework to a 3D scene reconstruction dataset. Here, our model is conditioned on camera location and can sample consistent sets of images for what an occluded region of a 3D scene might look like, even if there are multiple possibilities for what that region might contain. Reconstructions and videos are available at https://bit.ly/2O4Pc4R.



### Real-Time Subpixel Fast Bilateral Stereo
- **Arxiv ID**: http://arxiv.org/abs/1807.02044v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02044v3)
- **Published**: 2018-07-05 15:06:35+00:00
- **Updated**: 2018-08-15 13:49:41+00:00
- **Authors**: Rui Fan, Yanan Liu, Mohammud Junaid Bocus, Lujia Wang, Ming Liu
- **Comment**: 8 pages, 7 figures, International Conference on Information and
  automation
- **Journal**: None
- **Summary**: Stereo vision technique has been widely used in robotic systems to acquire 3-D information. In recent years, many researchers have applied bilateral filtering in stereo vision to adaptively aggregate the matching costs. This has greatly improved the accuracy of the estimated disparity maps. However, the process of filtering the whole cost volume is very time consuming and therefore the researchers have to resort to some powerful hardware for the real-time purpose. This paper presents the implementation of fast bilateral stereo on a state-of-the-art GPU. By highly exploiting the parallel computing architecture of the GPU, the fast bilateral stereo performs in real time when processing the Middlebury stereo datasets.



### Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1807.02062v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02062v3)
- **Published**: 2018-07-05 15:52:09+00:00
- **Updated**: 2018-11-29 11:10:42+00:00
- **Authors**: Peiliang Li, Tong Qin, Shaojie Shen
- **Comment**: 14 pages, 9 figures, eccv2018
- **Journal**: None
- **Summary**: We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a light-weight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-the-art solutions.



### Combining Background Subtraction Algorithms with Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.02080v2
- **DOI**: 10.1117/1.JEI.28.1.013011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02080v2)
- **Published**: 2018-07-05 16:39:50+00:00
- **Updated**: 2018-07-09 19:30:56+00:00
- **Authors**: Dongdong Zeng, Ming Zhu, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and fast extraction of foreground object is a key prerequisite for a wide range of computer vision applications such as object tracking and recognition. Thus, enormous background subtraction methods for foreground object detection have been proposed in recent decades. However, it is still regarded as a tough problem due to a variety of challenges such as illumination variations, camera jitter, dynamic backgrounds, shadows, and so on. Currently, there is no single method that can handle all the challenges in a robust way. In this letter, we try to solve this problem from a new perspective by combining different state-of-the-art background subtraction algorithms to create a more robust and more advanced foreground detection algorithm. More specifically, an encoder-decoder fully convolutional neural network architecture is trained to automatically learn how to leverage the characteristics of different algorithms to fuse the results produced by different background subtraction algorithms and output a more precise result. Comprehensive experiments evaluated on the CDnet 2014 dataset demonstrate that the proposed method outperforms all the considered single background subtraction algorithm. And we show that our solution is more efficient than other combination strategies.



### A Region-based Gauss-Newton Approach to Real-Time Monocular Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1807.02087v2
- **DOI**: 10.1109/TPAMI.2018.2884990
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02087v2)
- **Published**: 2018-07-05 16:56:54+00:00
- **Updated**: 2018-12-19 07:51:46+00:00
- **Authors**: Henning Tjaden, Ulrich Schwanecke, Elmar Schömer, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an algorithm for real-time 6DOF pose tracking of rigid 3D objects using a monocular RGB camera. The key idea is to derive a region-based cost function using temporally consistent local color histograms. While such region-based cost functions are commonly optimized using first-order gradient descent techniques, we systematically derive a Gauss-Newton optimization scheme which gives rise to drastically faster convergence and highly accurate and robust tracking performance. We furthermore propose a novel complex dataset dedicated for the task of monocular object pose tracking and make it publicly available to the community. To our knowledge, it is the first to address the common and important scenario in which both the camera as well as the objects are moving simultaneously in cluttered scenes. In numerous experiments - including our own proposed dataset - we demonstrate that the proposed Gauss-Newton approach outperforms existing approaches, in particular in the presence of cluttered backgrounds, heterogeneous objects and partial occlusions.



### MAT-CNN-SOPC: Motionless Analysis of Traffic Using Convolutional Neural Networks on System-On-a-Programmable-Chip
- **Arxiv ID**: http://arxiv.org/abs/1807.02098v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.2.1; C.1.4
- **Links**: [PDF](http://arxiv.org/pdf/1807.02098v2)
- **Published**: 2018-07-05 17:35:33+00:00
- **Updated**: 2018-08-14 23:31:16+00:00
- **Authors**: Somdip Dey, Grigorios Kalliatakis, Sangeet Saha, Amit Kumar Singh, Shoaib Ehsan, Klaus McDonald-Maier
- **Comment**: 6 pages, 3 figures, 2 tables
- **Journal**: 2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS
  2018)
- **Summary**: Intelligent Transportation Systems (ITS) have become an important pillar in modern "smart city" framework which demands intelligent involvement of machines. Traffic load recognition can be categorized as an important and challenging issue for such systems. Recently, Convolutional Neural Network (CNN) models have drawn considerable amount of interest in many areas such as weather classification, human rights violation detection through images, due to its accurate prediction capabilities. This work tackles real-life traffic load recognition problem on System-On-a-Programmable-Chip (SOPC) platform and coin it as MAT-CNN- SOPC, which uses an intelligent re-training mechanism of the CNN with known environments. The proposed methodology is capable of enhancing the efficacy of the approach by 2.44x in comparison to the state-of-art and proven through experimental analysis. We have also introduced a mathematical equation, which is capable of quantifying the suitability of using different CNN models over the other for a particular application based implementation.



### Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1807.03146v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03146v2)
- **Published**: 2018-07-05 17:41:49+00:00
- **Updated**: 2018-11-23 10:21:09+00:00
- **Authors**: Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad Norouzi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at http://keypointnet.github.io/.



### 3D Human Action Recognition with Siamese-LSTM Based Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.02131v1
- **DOI**: 10.18178/joig.6.1.21-26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02131v1)
- **Published**: 2018-07-05 18:13:07+00:00
- **Updated**: 2018-07-05 18:13:07+00:00
- **Authors**: Seyma Yucer, Yusuf Sinan Akgul
- **Comment**: None
- **Journal**: Journal of Image and Graphics,2018
- **Summary**: This paper proposes a new 3D Human Action Recognition system as a two-phase system: (1) Deep Metric Learning Module which learns a similarity metric between two 3D joint sequences using Siamese-LSTM networks; (2) A Multiclass Classification Module that uses the output of the first module to produce the final recognition output. This model has several advantages: the first module is trained with a larger set of data because it uses many combinations of sequence pairs.Our deep metric learning module can also be trained independently of the datasets, which makes our system modular and generalizable. We tested the proposed system on standard and newly introduced datasets that showed us that initial results are promising. We will continue developing this system by adding more sophisticated LSTM blocks and by cross-training between different datasets.



### Face Recognition Using Map Discriminant on YCbCr Color Space
- **Arxiv ID**: http://arxiv.org/abs/1807.02135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02135v1)
- **Published**: 2018-07-05 18:19:30+00:00
- **Updated**: 2018-07-05 18:19:30+00:00
- **Authors**: I Gede Pasek Suta Wijaya
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents face recognition using maximum a posteriori (MAP) discriminant on YCbCr color space. The YCbCr color space is considered in order to cover the skin information of face image on the recognition process. The proposed method is employed to improve the recognition rate and equal error rate (EER) of the gray scale based face recognition. In this case, the face features vector consisting of small part of dominant frequency elements which is extracted by non-blocking DCT is implemented as dimensional reduction of the raw face images. The matching process between the query face features and the trained face features is performed using maximum a posteriori (MAP) discriminant. From the experimental results on data from four face databases containing 2268 images with 196 classes show that the face recognition YCbCr color space provide better recognition rate and lesser EER than those of gray scale based face recognition which improve the first rank of grayscale based method result by about 4%. However, it requires three times more computation time than that of grayscale based method.



### Detecting Visual Relationships Using Box Attention
- **Arxiv ID**: http://arxiv.org/abs/1807.02136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02136v2)
- **Published**: 2018-07-05 18:24:56+00:00
- **Updated**: 2019-05-02 15:23:12+00:00
- **Authors**: Alexander Kolesnikov, Alina Kuznetsova, Christoph H. Lampert, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new model for detecting visual relationships, such as "person riding motorcycle" or "bottle on table". This task is an important step towards comprehensive structured image understanding, going beyond detecting individual objects. Our main novelty is a Box Attention mechanism that allows to model pairwise interactions between objects using standard object detection pipelines. The resulting model is conceptually clean, expressive and relies on well-justified training and prediction procedures. Moreover, unlike previously proposed approaches, our model does not introduce any additional complex components or hyperparameters on top of those already required by the underlying detection model. We conduct an experimental evaluation on three challenging datasets, V-COCO, Visual Relationships and Open Images, demonstrating strong quantitative and qualitative results.



### Spatiotemporal KSVD Dictionary Learning for Online Multi-target Tracking
- **Arxiv ID**: http://arxiv.org/abs/1807.02143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02143v1)
- **Published**: 2018-07-05 18:36:24+00:00
- **Updated**: 2018-07-05 18:36:24+00:00
- **Authors**: Huynh Manh, Gita Alaghband
- **Comment**: To appear in Proceedings of 15th Conference on Computer and Robot
  Vision 2018 (Oral)
- **Journal**: None
- **Summary**: In this paper, we present a new spatial discriminative KSVD dictionary algorithm (STKSVD) for learning target appearance in online multi-target tracking. Different from other classification/recognition tasks (e.g. face, image recognition), learning target's appearance in online multi-target tracking is impacted by factors such as posture/articulation changes, partial occlusion by background scene or other targets, background changes (human detection bounding box covers human parts and part of the scene), etc. However, we observe that these variations occur gradually relative to spatial and temporal dynamics. We characterize the spatial and temporal information between target's samples through a new STKSVD appearance learning algorithm to better discriminate sparse code, linear classifier parameters and minimize reconstruction error in a single optimization system. Our appearance learning algorithm and tracking framework employ two different methods of calculating appearance similarity score in each stage of a two-stage association: a linear classifier in the first stage, and minimum residual errors in the second stage. The results tested using 2DMOT2015 dataset and its public Aggregated Channel features (ACF) human detection for all comparisons show that our method outperforms the existing related learning methods.



### Automatic deep learning-based normalization of breast dynamic contrast-enhanced magnetic resonance images
- **Arxiv ID**: http://arxiv.org/abs/1807.02152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02152v1)
- **Published**: 2018-07-05 18:56:14+00:00
- **Updated**: 2018-07-05 18:56:14+00:00
- **Authors**: Jun Zhang, Ashirbani Saha, Brian J. Soher, Maciej A. Mazurowski
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: To develop an automatic image normalization algorithm for intensity correction of images from breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) acquired by different MRI scanners with various imaging parameters, using only image information. Methods: DCE-MR images of 460 subjects with breast cancer acquired by different scanners were used in this study. Each subject had one T1-weighted pre-contrast image and three T1-weighted post-contrast images available. Our normalization algorithm operated under the assumption that the same type of tissue in different patients should be represented by the same voxel value. We used four tissue/material types as the anchors for the normalization: 1) air, 2) fat tissue, 3) dense tissue, and 4) heart. The algorithm proceeded in the following two steps: First, a state-of-the-art deep learning-based algorithm was applied to perform tissue segmentation accurately and efficiently. Then, based on the segmentation results, a subject-specific piecewise linear mapping function was applied between the anchor points to normalize the same type of tissue in different patients into the same intensity ranges. We evaluated the algorithm with 300 subjects used for training and the rest used for testing. Results: The application of our algorithm to images with different scanning parameters resulted in highly improved consistency in pixel values and extracted radiomics features. Conclusion: The proposed image normalization strategy based on tissue segmentation can perform intensity correction fully automatically, without the knowledge of the scanner parameters. Significance: We have thoroughly tested our algorithm and showed that it successfully normalizes the intensity of DCE-MR images. We made our software publicly available for others to apply in their analyses.



### A new ultrasound despeckling method through adaptive threshold
- **Arxiv ID**: http://arxiv.org/abs/1807.03160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03160v1)
- **Published**: 2018-07-05 20:39:14+00:00
- **Updated**: 2018-07-05 20:39:14+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 7 pages, 2 figures, conference
- **Journal**: None
- **Summary**: An efficient despeckling method using a quantum-inspired adaptive threshold function is presented for reducing noise of ultrasound images. In the first step, the ultrasound image is decorrelated by an spectrum equalization procedure due to the fact that speckle noise is neither Gaussian nor white. In fact, a linear filter is exploited to flatten the power spectral density (PSD) of the ultrasound image. Then, the proposed method shrinks complex wavelet coefficients based on the quantum-inspired adaptive threshold function. The proposed approach has been used to denoise both real and simulated data sets and compare with other widely adopted techniques. Experimental results demonstrate that the proposed method has a competitive performance to remove speckle noise and can preserve details and textures of medical ultrasound images.



### Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1807.02188v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.02188v4)
- **Published**: 2018-07-05 21:52:36+00:00
- **Updated**: 2019-05-31 19:09:09+00:00
- **Authors**: Priyadarshini Panda, Kaushik Roy
- **Comment**: Preliminary version of this work accepted at ICML 2019 (Workshop on
  Uncertainty and Robustness in Deep Learning)
- **Journal**: None
- **Summary**: We introduce a Noise-based prior Learning (NoL) approach for training neural networks that are intrinsically robust to adversarial attacks. We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness. We evaluate our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis. Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components. We show that models learnt with our approach perform remarkably well against a wide-range of attacks. Furthermore, combining NoL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.



### Affective EEG-Based Person Identification Using the Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1807.03147v3
- **DOI**: 10.1109/TCDS.2019.2924648
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.03147v3)
- **Published**: 2018-07-05 22:43:01+00:00
- **Updated**: 2019-04-29 11:22:02+00:00
- **Authors**: Theerawit Wilaiprasitporn, Apiwat Ditthapron, Karis Matchaparn, Tanaboon Tongbuasirilai, Nannapas Banluesombatkul, Ekapol Chuangsuwanich
- **Comment**: 10 pages
- **Journal**: IEEE Transactions on Cognitive and Developmental System (2019)
- **Summary**: Electroencephalography (EEG) is another mode for performing Person Identification (PI). Due to the nature of the EEG signals, EEG-based PI is typically done while the person is performing some kind of mental task, such as motor control. However, few works have considered EEG-based PI while the person is in different mental states (affective EEG). The aim of this paper is to improve the performance of affective EEG-based PI using a deep learning approach. \textcolor{red}{We proposed a cascade of deep learning using a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)}. CNNs are used to handle the spatial information from the EEG while RNNs extract the temporal information. \textcolor{red}{We evaluated two types of RNNs, namely, Long Short-Term Memory (CNN-LSTM) and Gated Recurrent Unit (CNN-GRU). } The proposed method is evaluated on the state-of-the-art affective dataset DEAP. The results indicate that CNN-GRU and CNN-LSTM can perform PI from different affective states and reach up to 99.90--100\% mean Correct Recognition Rate (CRR), significantly outperforming a support vector machine (SVM) baseline system that uses power spectral density (PSD) features. Notably, the 100\% mean \emph{CRR} comes from only 40 subjects in DEAP dataset. To reduce the number of EEG electrodes from thirty-two to five for more practical applications, the frontal region gives the best results reaching up to 99.17\% CRR (from CNN-GRU). Amongst the two deep learning models, we find CNN-GRU to slightly outperform CNN-LSTM, while having faster training time. \textcolor{red}{Furthermore, CNN-GRU overcomes the influence of affective states in EEG-Based PI reported in the previous works.




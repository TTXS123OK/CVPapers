# Arxiv Papers in cs.CV on 2017-05-08
### Automatic Recognition of Mammal Genera on Camera-Trap Images using Multi-Layer Robust Principal Component Analysis and Mixture Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1705.02727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02727v1)
- **Published**: 2017-05-08 02:50:06+00:00
- **Updated**: 2017-05-08 02:50:06+00:00
- **Authors**: Jhony-Heriberto Giraldo-Zuluaga, Augusto Salazar, Alexander Gomez, Angélica Diaz-Pulido
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation and classification of animals from camera-trap images is due to the conditions under which the images are taken, a difficult task. This work presents a method for classifying and segmenting mammal genera from camera-trap images. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA) for segmenting, Convolutional Neural Networks (CNNs) for extracting features, Least Absolute Shrinkage and Selection Operator (LASSO) for selecting features, and Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for classifying mammal genera present in the Colombian forest. We evaluated our method with the camera-trap images from the Alexander von Humboldt Biological Resources Research Institute. We obtained an accuracy of 92.65% classifying 8 mammal genera and a False Positive (FP) class, using automatic-segmented images. On the other hand, we reached 90.32% of accuracy classifying 10 mammal genera, using ground-truth images only. Unlike almost all previous works, we confront the animal segmentation and genera classification in the camera-trap recognition. This method shows a new approach toward a fully-automatic detection of animals from camera-trap images.



### ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition
- **Arxiv ID**: http://arxiv.org/abs/1705.02743v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02743v3)
- **Published**: 2017-05-08 05:16:51+00:00
- **Updated**: 2017-10-15 17:58:08+00:00
- **Authors**: Xin Chen, Yu Zhu, Hua Zhou, Liang Diao, Dongyan Wang
- **Comment**: 8 pages, 5 figure, 2 tables
- **Journal**: None
- **Summary**: In this paper, we introduce a new and challenging large-scale food image dataset called "ChineseFoodNet", which aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In our dataset, images of each food category of our dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. We present our efforts to build this large-scale image dataset, including food category selection, data collection, and data clean and label, in particular how to use machine learning methods to reduce manual labeling work that is an expensive process. We share a detailed benchmark of several state-of-the-art deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose a novel two-step data fusion approach referred as "TastyNet", which combines prediction results from different CNNs with voting method. Our proposed approach achieves top-1 accuracies of 81.43% on the validation set and 81.55% on the test set, respectively. The latest dataset is public available for research and can be achieved at https://sites.google.com/view/chinesefoodnet.



### High-Level Concepts for Affective Understanding of Images
- **Arxiv ID**: http://arxiv.org/abs/1705.02751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02751v1)
- **Published**: 2017-05-08 05:58:05+00:00
- **Updated**: 2017-05-08 05:58:05+00:00
- **Authors**: Afsheen Rafaqat Ali, Usman Shahid, Mohsen Ali, Jeffrey Ho
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to bridge the affective gap between image content and the emotional response of the viewer it elicits by using High-Level Concepts (HLCs). In contrast to previous work that relied solely on low-level features or used convolutional neural network (CNN) as a black-box, we use HLCs generated by pretrained CNNs in an explicit way to investigate the relations/associations between these HLCs and a (small) set of Ekman's emotional classes. As a proof-of-concept, we first propose a linear admixture model for modeling these relations, and the resulting computational framework allows us to determine the associations between each emotion class and certain HLCs (objects and places). This linear model is further extended to a nonlinear model using support vector regression (SVR) that aims to predict the viewer's emotional response using both low-level image features and HLCs extracted from images. These class-specific regressors are then assembled into a regressor ensemble that provide a flexible and effective predictor for predicting viewer's emotional responses from images. Experimental results have demonstrated that our results are comparable to existing methods, with a clear view of the association between HLCs and emotional classes that is ostensibly missing in most existing work.



### What Can Help Pedestrian Detection?
- **Arxiv ID**: http://arxiv.org/abs/1705.02757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02757v1)
- **Published**: 2017-05-08 06:43:20+00:00
- **Updated**: 2017-05-08 06:43:20+00:00
- **Authors**: Jiayuan Mao, Tete Xiao, Yuning Jiang, Zhimin Cao
- **Comment**: Accepted to IEEE International Conference on Computer Vision and
  Pattern Recognition (CVPR) 2017
- **Journal**: None
- **Summary**: Aggregating extra features has been considered as an effective approach to boost traditional pedestrian detection methods. However, there is still a lack of studies on whether and how CNN-based pedestrian detectors can benefit from these extra features. The first contribution of this paper is exploring this issue by aggregating extra features into CNN-based pedestrian detection framework. Through extensive experiments, we evaluate the effects of different kinds of extra features quantitatively. Moreover, we propose a novel network architecture, namely HyperLearner, to jointly learn pedestrian detection as well as the given extra feature. By multi-task training, HyperLearner is able to utilize the information of given features and improve detection performance without extra inputs in inference. The experimental results on multiple pedestrian benchmarks validate the effectiveness of the proposed HyperLearner.



### Deep Descriptor Transforming for Image Co-Localization
- **Arxiv ID**: http://arxiv.org/abs/1705.02758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.02758v1)
- **Published**: 2017-05-08 06:52:44+00:00
- **Updated**: 2017-05-08 06:52:44+00:00
- **Authors**: Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie, Jianxin Wu, Chunhua Shen, Zhi-Hua Zhou
- **Comment**: Accepted by IJCAI 2017
- **Journal**: None
- **Summary**: Reusable model design becomes desirable with the rapid expansion of machine learning applications. In this paper, we focus on the reusability of pre-trained deep convolutional models. Specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., the convolutional activations could act as a detector for the common object in the image co-localization problem. We propose a simple but effective method, named Deep Descriptor Transforming (DDT), for evaluating the correlations of descriptors and then obtaining the category-consistent regions, which can accurately locate the common object in a set of images. Empirical studies validate the effectiveness of the proposed DDT method. On benchmark image co-localization datasets, DDT consistently outperforms existing state-of-the-art methods by a large margin. Moreover, DDT also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data.



### Scene Text Eraser
- **Arxiv ID**: http://arxiv.org/abs/1705.02772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1705.02772v1)
- **Published**: 2017-05-08 08:28:34+00:00
- **Updated**: 2017-05-08 08:28:34+00:00
- **Authors**: Toshiki Nakamura, Anna Zhu, Keiji Yanai, Seiichi Uchida
- **Comment**: None
- **Journal**: None
- **Summary**: The character information in natural scene images contains various personal information, such as telephone numbers, home addresses, etc. It is a high risk of leakage the information if they are published. In this paper, we proposed a scene text erasing method to properly hide the information via an inpainting convolutional neural network (CNN) model. The input is a scene text image, and the output is expected to be text erased image with all the character regions filled up the colors of the surrounding background pixels. This work is accomplished by a CNN model through convolution to deconvolution with interconnection process. The training samples and the corresponding inpainting images are considered as teaching signals for training. To evaluate the text erasing performance, the output images are detected by a novel scene text detection method. Subsequently, the same measurement on text detection is utilized for testing the images in benchmark dataset ICDAR2013. Compared with direct text detection way, the scene text erasing process demonstrates a drastically decrease on the precision, recall and f-score. That proves the effectiveness of proposed method for erasing the text in natural scene images.



### Face Recognition Machine Vision System Using Eigenfaces
- **Arxiv ID**: http://arxiv.org/abs/1705.02782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02782v1)
- **Published**: 2017-05-08 08:53:30+00:00
- **Updated**: 2017-05-08 08:53:30+00:00
- **Authors**: Fares Jalled
- **Comment**: 7 pages, 11 figures
- **Journal**: None
- **Summary**: Face Recognition is a common problem in Machine Learning. This technology has already been widely used in our lives. For example, Facebook can automatically tag people's faces in images, and also some mobile devices use face recognition to protect private security. Face images comes with different background, variant illumination, different facial expression and occlusion. There are a large number of approaches for the face recognition. Different approaches for face recognition have been experimented with specific databases which consist of single type, format and composition of image. Doing so, these approaches don't suit with different face databases. One of the basic face recognition techniques is eigenface which is quite simple, efficient, and yields generally good results in controlled circumstances. So, this paper presents an experimental performance comparison of face recognition using Principal Component Analysis (PCA) and Normalized Principal Component Analysis (NPCA). The experiments are carried out on the ORL (ATT) and Indian face database (IFD) which contain variability in expression, pose, and facial details. The results obtained for the two methods have been compared by varying the number of training images. MATLAB is used for implementing algorithms also.



### Video Processing for Barycenter Trajectory Identification in Diving
- **Arxiv ID**: http://arxiv.org/abs/1705.02854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02854v1)
- **Published**: 2017-05-08 13:09:09+00:00
- **Updated**: 2017-05-08 13:09:09+00:00
- **Authors**: Stefano Frassinelli, Alessandro Niccolai, Riccardo E. Zich
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this paper is to show a procedure for identify the barycentre of a diver by means of video processing. This procedure is aimed to introduce quantitative analysis tools and diving performance measurement and therefore in diving training. Sport performance analysis is a trend that is growing exponentially for all level athletes: it has been applied extensively in some sports such as cycling. Sport performance analysis has been applied mainly for high level athletes; in order to be used also for middle or low level athletes the proposed technique has to be flexible and low cost. Video processing is suitable to fulfil both these requirements. In diving, the first analysis that has to be done is the barycentre trajectory tracking.



### A Dual-Source Approach for 3D Human Pose Estimation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1705.02883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02883v2)
- **Published**: 2017-05-08 14:03:48+00:00
- **Updated**: 2017-09-06 13:24:52+00:00
- **Authors**: Umar Iqbal, Andreas Doering, Hashim Yasin, Björn Krüger, Andreas Weber, Juergen Gall
- **Comment**: under consideration at Computer Vision and Image Understanding.
  Extended version of CVPR-2016 paper, arXiv:1509.06720
- **Journal**: None
- **Summary**: In this work we address the challenging problem of 3D human pose estimation from single images. Recent approaches learn deep neural networks to regress 3D pose directly from images. One major challenge for such methods, however, is the collection of training data. Specifically, collecting large amounts of training data containing unconstrained images annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of accurate 3D motion capture data, and the second source consists of unconstrained images with annotated 2D poses. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient 3D pose retrieval. To this end, we first convert the motion capture data into a normalized 2D pose space, and separately learn a 2D pose estimation model from the image data. During inference, we estimate the 2D pose and efficiently retrieve the nearest 3D poses. We then jointly estimate a mapping from the 3D pose space to the image and reconstruct the 3D pose. We provide a comprehensive evaluation of the proposed method and experimentally demonstrate the effectiveness of our approach, even when the skeleton structures of the two sources differ substantially.



### Generative Cooperative Net for Image Generation and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1705.02887v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02887v3)
- **Published**: 2017-05-08 14:10:07+00:00
- **Updated**: 2018-02-08 15:29:28+00:00
- **Authors**: Qiangeng Xu, Zengchang Qin, Tao Wan
- **Comment**: 12 pages, 8 figures
- **Journal**: The International Symposium on Integrated Uncertainty in Knowledge
  Modelling and Decision Making (IUKM) 2019
- **Summary**: How to build a good model for image generation given an abstract concept is a fundamental problem in computer vision. In this paper, we explore a generative model for the task of generating unseen images with desired features. We propose the Generative Cooperative Net (GCN) for image generation. The idea is similar to generative adversarial networks except that the generators and discriminators are trained to work accordingly. Our experiments on hand-written digit generation and facial expression generation show that GCN's two cooperative counterparts (the generator and the classifier) can work together nicely and achieve promising results. We also discovered a usage of such generative model as an data-augmentation tool. Our experiment of applying this method on a recognition task shows that it is very effective comparing to other existing methods. It is easy to set up and could help generate a very large synthesized dataset.



### Multi Resolution LSTM For Long Term Prediction In Neural Activity Video
- **Arxiv ID**: http://arxiv.org/abs/1705.02893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02893v2)
- **Published**: 2017-05-08 14:32:22+00:00
- **Updated**: 2018-07-03 02:50:09+00:00
- **Authors**: Yilin Song, Jonathan Viventi, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Epileptic seizures are caused by abnormal, overly syn- chronized, electrical activity in the brain. The abnor- mal electrical activity manifests as waves, propagating across the brain. Accurate prediction of the propagation velocity and direction of these waves could enable real- time responsive brain stimulation to suppress or prevent the seizures entirely. However, this problem is very chal- lenging because the algorithm must be able to predict the neural signals in a sufficiently long time horizon to allow enough time for medical intervention. We consider how to accomplish long term prediction using a LSTM network. To alleviate the vanishing gradient problem, we propose two encoder-decoder-predictor structures, both using multi-resolution representation. The novel LSTM structure with multi-resolution layers could significantly outperform the single-resolution benchmark with similar number of parameters. To overcome the blurring effect associated with video prediction in the pixel domain using standard mean square error (MSE) loss, we use energy- based adversarial training to improve the long-term pre- diction. We demonstrate and analyze how a discriminative model with an encoder-decoder structure using 3D CNN model improves long term prediction.



### Geometric GAN
- **Arxiv ID**: http://arxiv.org/abs/1705.02894v2
- **DOI**: None
- **Categories**: **stat.ML**, cond-mat.dis-nn, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1705.02894v2)
- **Published**: 2017-05-08 14:32:33+00:00
- **Updated**: 2017-05-09 01:12:28+00:00
- **Authors**: Jae Hyun Lim, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.



### Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression
- **Arxiv ID**: http://arxiv.org/abs/1705.02900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1705.02900v1)
- **Published**: 2017-05-08 14:55:32+00:00
- **Updated**: 2017-05-08 14:55:32+00:00
- **Authors**: Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Li Chen, Michael E. Kounavis, Duen Horng Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.



### Cross-label Suppression: A Discriminative and Fast Dictionary Learning with Group Regularization
- **Arxiv ID**: http://arxiv.org/abs/1705.02928v1
- **DOI**: 10.1109/TIP.2017.2703101
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1705.02928v1)
- **Published**: 2017-05-08 15:49:43+00:00
- **Updated**: 2017-05-08 15:49:43+00:00
- **Authors**: Xiudong Wang, Yuantao Gu
- **Comment**: 36 pages, 12 figures, 11 tables
- **Journal**: None
- **Summary**: This paper addresses image classification through learning a compact and discriminative dictionary efficiently. Given a structured dictionary with each atom (columns in the dictionary matrix) related to some label, we propose cross-label suppression constraint to enlarge the difference among representations for different classes. Meanwhile, we introduce group regularization to enforce representations to preserve label properties of original samples, meaning the representations for the same class are encouraged to be similar. Upon the cross-label suppression, we don't resort to frequently-used $\ell_0$-norm or $\ell_1$-norm for coding, and obtain computational efficiency without losing the discriminative power for categorization. Moreover, two simple classification schemes are also developed to take full advantage of the learnt dictionary. Extensive experiments on six data sets including face recognition, object categorization, scene classification, texture recognition and sport action categorization are conducted, and the results show that the proposed approach can outperform lots of recently presented dictionary algorithms on both recognition accuracy and computational efficiency.



### Learning non-maximum suppression
- **Arxiv ID**: http://arxiv.org/abs/1705.02950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02950v2)
- **Published**: 2017-05-08 16:16:28+00:00
- **Updated**: 2017-05-09 12:52:04+00:00
- **Authors**: Jan Hosang, Rodrigo Benenson, Bernt Schiele
- **Comment**: Added "Supplementary material" title
- **Journal**: None
- **Summary**: Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and -- being based on greedy clustering with a fixed distance threshold -- forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.



### Temporal Segment Networks for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/1705.02953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02953v1)
- **Published**: 2017-05-08 16:21:26+00:00
- **Updated**: 2017-05-08 16:21:26+00:00
- **Authors**: Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool
- **Comment**: 14 pages. An extension of submission at
  https://arxiv.org/abs/1608.00859
- **Journal**: None
- **Summary**: Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%), THUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.



### You said that?
- **Arxiv ID**: http://arxiv.org/abs/1705.02966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.02966v2)
- **Published**: 2017-05-08 16:44:46+00:00
- **Updated**: 2017-07-18 14:58:55+00:00
- **Authors**: Joon Son Chung, Amir Jamaludin, Andrew Zisserman
- **Comment**: https://youtu.be/LeufDSb15Kc British Machine Vision Conference
  (BMVC), 2017
- **Journal**: None
- **Summary**: We present a method for generating a video of a talking face. The method takes as inputs: (i) still images of the target face, and (ii) an audio speech segment; and outputs a video of the target face lip synched with the audio. The method runs in real time and is applicable to faces and audio not seen at training time.   To achieve this we propose an encoder-decoder CNN model that uses a joint embedding of the face and audio to generate synthesised talking face video frames. The model is trained on tens of hours of unlabelled videos.   We also show results of re-dubbing videos using speech from a different person.



### Robust tracking of respiratory rate in high-dynamic range scenes using mobile thermal imaging
- **Arxiv ID**: http://arxiv.org/abs/1705.06628v2
- **DOI**: 10.1364/BOE.8.004480
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1705.06628v2)
- **Published**: 2017-05-08 17:49:03+00:00
- **Updated**: 2017-09-20 21:06:05+00:00
- **Authors**: Youngjun Cho, Simon J. Julier, Nicolai Marquardt, Nadia Bianchi-Berthouze
- **Comment**: Vol. 8, No. 10, 1 Oct 2017, Biomedical Optics Express 4480 - Full
  abstract can be found in this journal article (due to limited word counts of
  'arXiv abstract')
- **Journal**: Biomedical Optics Express, 2017
- **Summary**: The ability to monitor respiratory rate is extremely important for medical treatment, healthcare and fitness sectors. In many situations, mobile methods, which allow users to undertake every day activities, are required. However, current monitoring systems can be obtrusive, requiring users to wear respiration belts or nasal probes. Recent advances in thermographic systems have shrunk their size, weight and cost, to the point where it is possible to create smart-phone based respiration rate monitoring devices that are not affected by lighting conditions. However, mobile thermal imaging is challenged in scenes with high thermal dynamic ranges. This challenge is further amplified by general problems such as motion artifacts and low spatial resolution, leading to unreliable breathing signals. In this paper, we propose a novel and robust approach for respiration tracking which compensates for the negative effects of variations in the ambient temperature and motion artifacts and can accurately extract breathing rates in highly dynamic thermal scenes. It has three main contributions. The first is a novel Optimal Quantization technique which adaptively constructs a color mapping of absolute temperature to improve segmentation, classification and tracking. The second is the Thermal Gradient Flow method that computes thermal gradient magnitude maps to enhance accuracy of the nostril region tracking. Finally, we introduce the Thermal Voxel method to increase the reliability of the captured respiration signals compared to the traditional averaging method. We demonstrate the extreme robustness of our system to track the nostril-region and measure the respiratory rate in high dynamic range scenes.



### Light Field Video Capture Using a Learning-Based Hybrid Imaging System
- **Arxiv ID**: http://arxiv.org/abs/1705.02997v1
- **DOI**: 10.1145/3072959.3073614
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.02997v1)
- **Published**: 2017-05-08 17:56:44+00:00
- **Updated**: 2017-05-08 17:56:44+00:00
- **Authors**: Ting-Chun Wang, Jun-Yan Zhu, Nima Khademi Kalantari, Alexei A. Efros, Ravi Ramamoorthi
- **Comment**: ACM Transactions on Graphics (Proceedings of SIGGRAPH 2017)
- **Journal**: None
- **Summary**: Light field cameras have many advantages over traditional cameras, as they allow the user to change various camera settings after capture. However, capturing light fields requires a huge bandwidth to record the data: a modern light field camera can only take three images per second. This prevents current consumer light field cameras from capturing light field videos. Temporal interpolation at such extreme scale (10x, from 3 fps to 30 fps) is infeasible as too much information will be entirely missing between adjacent frames. Instead, we develop a hybrid imaging system, adding another standard video camera to capture the temporal information. Given a 3 fps light field sequence and a standard 30 fps 2D video, our system can then generate a full light field video at 30 fps. We adopt a learning-based approach, which can be decomposed into two steps: spatio-temporal flow estimation and appearance estimation. The flow estimation propagates the angular information from the light field sequence to the 2D video, so we can warp input images to the target view. The appearance estimation then combines these warped images to output the final pixels. The whole process is trained end-to-end using convolutional neural networks. Experimental results demonstrate that our algorithm outperforms current video interpolation methods, enabling consumer light field videography, and making applications such as refocusing and parallax view generation achievable on videos for the first time.



### Real-Time User-Guided Image Colorization with Learned Deep Priors
- **Arxiv ID**: http://arxiv.org/abs/1705.02999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1705.02999v1)
- **Published**: 2017-05-08 17:58:11+00:00
- **Updated**: 2017-05-08 17:58:11+00:00
- **Authors**: Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S. Lin, Tianhe Yu, Alexei A. Efros
- **Comment**: Accepted to SIGGRAPH 2017. Project page:
  https://richzhang.github.io/ideepcolor
- **Journal**: None
- **Summary**: We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user "hints" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user "hints" to the desired colorization, showing an application to color histogram transfer. Our code and models are available at https://richzhang.github.io/ideepcolor.



### A simple yet effective baseline for 3d human pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1705.03098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03098v2)
- **Published**: 2017-05-08 21:48:37+00:00
- **Updated**: 2017-08-04 18:36:24+00:00
- **Authors**: Julieta Martinez, Rayat Hossain, Javier Romero, James J. Little
- **Comment**: Accepted to ICCV 17
- **Journal**: None
- **Summary**: Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3-dimensional positions. With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, "lifting" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feed-forward network outperforms the best reported result by about 30\% on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (\ie, using images as input) yields state of the art results -- this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.



### CAD Priors for Accurate and Flexible Instance Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1705.03111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.03111v2)
- **Published**: 2017-05-08 22:39:17+00:00
- **Updated**: 2017-08-16 21:38:53+00:00
- **Authors**: Tolga Birdal, Slobodan Ilic
- **Comment**: Published at International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: We present an efficient and automatic approach for accurate reconstruction of instances of big 3D objects from multiple, unorganized and unstructured point clouds, in presence of dynamic clutter and occlusions. In contrast to conventional scanning, where the background is assumed to be rather static, we aim at handling dynamic clutter where background drastically changes during the object scanning. Currently, it is tedious to solve this with available methods unless the object of interest is first segmented out from the rest of the scene. We address the problem by assuming the availability of a prior CAD model, roughly resembling the object to be reconstructed. This assumption almost always holds in applications such as industrial inspection or reverse engineering. With aid of this prior acting as a proxy, we propose a fully enhanced pipeline, capable of automatically detecting and segmenting the object of interest from scenes and creating a pose graph, online, with linear complexity. This allows initial scan alignment to the CAD model space, which is then refined without the CAD constraint to fully recover a high fidelity 3D reconstruction, accurate up to the sensor noise level. We also contribute a novel object detection method, local implicit shape models (LISM) and give a fast verification scheme. We evaluate our method on multiple datasets, demonstrating the ability to accurately reconstruct objects from small sizes up to $125m^3$.




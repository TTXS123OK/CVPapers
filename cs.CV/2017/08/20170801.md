# Arxiv Papers in cs.CV on 2017-08-01
### Material Editing Using a Physically Based Rendering Network
- **Arxiv ID**: http://arxiv.org/abs/1708.00106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00106v2)
- **Published**: 2017-08-01 00:02:03+00:00
- **Updated**: 2017-08-09 17:28:54+00:00
- **Authors**: Guilin Liu, Duygu Ceylan, Ersin Yumer, Jimei Yang, Jyh-Ming Lien
- **Comment**: 14 pages, ICCV 2017
- **Journal**: None
- **Summary**: The ability to edit materials of objects in images is desirable by many content creators. However, this is an extremely challenging task as it requires to disentangle intrinsic physical properties of an image. We propose an end-to-end network architecture that replicates the forward image formation process to accomplish this task. Specifically, given a single image, the network first predicts intrinsic properties, i.e. shape, illumination, and material, which are then provided to a rendering layer. This layer performs in-network image synthesis, thereby enabling the network to understand the physics behind the image formation process. The proposed rendering layer is fully differentiable, supports both diffuse and specular materials, and thus can be applicable in a variety of problem settings. We demonstrate a rich set of visually plausible material editing examples and provide an extensive comparative study.



### Deep Generative Adversarial Neural Networks for Realistic Prostate Lesion MRI Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1708.00129v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.10; I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/1708.00129v1)
- **Published**: 2017-08-01 02:09:12+00:00
- **Updated**: 2017-08-01 02:09:12+00:00
- **Authors**: Andy Kitchen, Jarrel Seah
- **Comment**: 8 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Generative Adversarial Neural Networks (GANs) are applied to the synthetic generation of prostate lesion MRI images. GANs have been applied to a variety of natural images, is shown show that the same techniques can be used in the medical domain to create realistic looking synthetic lesion images. 16mm x 16mm patches are extracted from 330 MRI scans from the SPIE ProstateX Challenge 2016 and used to train a Deep Convolutional Generative Adversarial Neural Network (DCGAN) utilizing cutting edge techniques. Synthetic outputs are compared to real images and the implicit latent representations induced by the GAN are explored. Training techniques and successful neural network architectures are explained in detail.



### Computational Motility Tracking of Calcium Dynamics in Toxoplasma gondii
- **Arxiv ID**: http://arxiv.org/abs/1708.01871v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/1708.01871v2)
- **Published**: 2017-08-01 04:00:16+00:00
- **Updated**: 2017-08-18 01:40:37+00:00
- **Authors**: Mojtaba Sedigh Fazli, Stephen Andrew Vella, Silvia N. J. Moreno, Shannon Quinn
- **Comment**: 7 pages, 13 figures, KDDBigDas Workshop
- **Journal**: None
- **Summary**: Toxoplasma gondii is the causative agent responsible for toxoplasmosis and serves as one of the most common parasites in the world. For a successful lytic cycle, T. gondii must traverse biological barriers in order to invade host cells, and as such, motility is critical for its virulence. Calcium signaling, governed by fluctuations in cytosolic calcium (Ca2+) concentrations, is utilized universally across life and regulates many cellular processes, including the stimulation of T. gondii virulence factors such as motility. Therefore, increases in cytosolic calcium, called calcium oscillations, serve as a means to link and quantify the intracellular signaling processes that lead to T. gondii motility and invasion. Here, we describe our work extracting, quantifying and modeling motility patterns of T. gondii before and after the addition of pharmacological drugs and/or extracellular calcium. We demonstrate a computational pipeline including a robust tracking system using optical flow and dense trajectory features to extract T. gondii motility patterns. Using this pipeline, we were able to track changes in T.gondii motility in response to cytosolic Ca2+ fluxes in extracellular parasites. This allows us to study how Ca2+ signaling via release from intracellular Ca2+ stores and/or from extracellular Ca2+ entry relates to motility patterns, a crucial first step in developing countermeasures for T. gondii virulence.



### Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1708.00153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00153v1)
- **Published**: 2017-08-01 04:16:34+00:00
- **Updated**: 2017-08-01 04:16:34+00:00
- **Authors**: Heng Fan, Haibin Ling
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Being intensively studied, visual tracking has seen great recent advances in either speed (e.g., with correlation filters) or accuracy (e.g., with deep features). Real-time and high accuracy tracking algorithms, however, remain scarce. In this paper we study the problem from a new perspective and present a novel parallel tracking and verifying (PTAV) framework, by taking advantage of the ubiquity of multi-thread techniques and borrowing from the success of parallel tracking and mapping in visual SLAM. Our PTAV framework typically consists of two components, a tracker T and a verifier V, working in parallel on two separate threads. The tracker T aims to provide a super real-time tracking inference and is expected to perform well most of the time; by contrast, the verifier V checks the tracking results and corrects T when needed. The key innovation is that, V does not work on every frame but only upon the requests from T; on the other end, T may adjust the tracking according to the feedback from V. With such collaboration, PTAV enjoys both the high efficiency provided by T and the strong discriminative power by V. In our extensive experiments on popular benchmarks including OTB2013, OTB2015, TC128 and UAV20L, PTAV achieves the best tracking accuracy among all real-time trackers, and in fact performs even better than many deep learning based solutions. Moreover, as a general framework, PTAV is very flexible and has great rooms for improvement and generalization.



### Image Denoising via CNNs: An Adversarial Approach
- **Arxiv ID**: http://arxiv.org/abs/1708.00159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00159v1)
- **Published**: 2017-08-01 05:04:22+00:00
- **Updated**: 2017-08-01 05:04:22+00:00
- **Authors**: Nithish Divakar, R. Venkatesh Babu
- **Comment**: None
- **Journal**: None
- **Summary**: Is it possible to recover an image from its noisy version using convolutional neural networks? This is an interesting problem as convolutional layers are generally used as feature detectors for tasks like classification, segmentation and object detection. We present a new CNN architecture for blind image denoising which synergically combines three architecture components, a multi-scale feature extraction layer which helps in reducing the effect of noise on feature maps, an l_p regularizer which helps in selecting only the appropriate feature maps for the task of reconstruction, and finally a three step training approach which leverages adversarial training to give the final performance boost to the model. The proposed model shows competitive denoising performance when compared to the state-of-the-art approaches.



### Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance
- **Arxiv ID**: http://arxiv.org/abs/1708.00163v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00163v3)
- **Published**: 2017-08-01 05:21:08+00:00
- **Updated**: 2018-04-24 05:06:13+00:00
- **Authors**: Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, Zelun Luo, Alisha Rege, Jeffrey Jopling, Lance Downing, William Beninati, Amit Singh, Terry Platchek, Arnold Milstein, Li Fei-Fei
- **Comment**: Machine Learning for Healthcare Conference (MLHC)
- **Journal**: PMLR 68:75-87, 2017
- **Summary**: One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people's activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method's interpretability. This work is a step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections.



### A Locally Weighted Fixation Density-Based Metric for Assessing the Quality of Visual Saliency Predictions
- **Arxiv ID**: http://arxiv.org/abs/1708.00169v1
- **DOI**: 10.1109/TIP.2016.2577498
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00169v1)
- **Published**: 2017-08-01 05:39:08+00:00
- **Updated**: 2017-08-01 05:39:08+00:00
- **Authors**: Milind S. Gide, Lina J. Karam
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 25, no. 8, pp.
  3852-3861, Aug. 2016
- **Summary**: With the increased focus on visual attention (VA) in the last decade, a large number of computational visual saliency methods have been developed over the past few years. These models are traditionally evaluated by using performance evaluation metrics that quantify the match between predicted saliency and fixation data obtained from eye-tracking experiments on human observers. Though a considerable number of such metrics have been proposed in the literature, there are notable problems in them. In this work, we discuss shortcomings in existing metrics through illustrative examples and propose a new metric that uses local weights based on fixation density which overcomes these flaws. To compare the performance of our proposed metric at assessing the quality of saliency prediction with other existing metrics, we construct a ground-truth subjective database in which saliency maps obtained from 17 different VA models are evaluated by 16 human observers on a 5-point categorical scale in terms of their visual resemblance with corresponding ground-truth fixation density maps obtained from eye-tracking data. The metrics are evaluated by correlating metric scores with the human subjective ratings. The correlation results show that the proposed evaluation metric outperforms all other popular existing metrics. Additionally, the constructed database and corresponding subjective ratings provide an insight into which of the existing metrics and future metrics are better at estimating the quality of saliency prediction and can be used as a benchmark.



### PROBE-GK: Predictive Robust Estimation using Generalized Kernels
- **Arxiv ID**: http://arxiv.org/abs/1708.00171v2
- **DOI**: 10.1109/ICRA.2016.7487212
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00171v2)
- **Published**: 2017-08-01 05:45:20+00:00
- **Updated**: 2017-08-02 21:40:50+00:00
- **Authors**: Valentin Peretroukhin, William Vega-Brown, Nicholas Roy, Jonathan Kelly
- **Comment**: In Proceedings of the IEEE International Conference on Robotics and
  Automation (ICRA'16), Stockholm, Sweden, May 16-21, 2016
- **Journal**: None
- **Summary**: Many algorithms in computer vision and robotics make strong assumptions about uncertainty, and rely on the validity of these assumptions to produce accurate and consistent state estimates. In practice, dynamic environments may degrade sensor performance in predictable ways that cannot be captured with static uncertainty parameters. In this paper, we employ fast nonparametric Bayesian inference techniques to more accurately model sensor uncertainty. By setting a prior on observation uncertainty, we derive a predictive robust estimator, and show how our model can be learned from sample images, both with and without knowledge of the motion used to generate the data. We validate our approach through Monte Carlo simulations, and report significant improvements in localization accuracy relative to a fixed noise model in several settings, including on synthetic data, the KITTI dataset, and our own experimental platform.



### PROBE: Predictive Robust Estimation for Visual-Inertial Navigation
- **Arxiv ID**: http://arxiv.org/abs/1708.00174v3
- **DOI**: 10.1109/IROS.2015.7353890
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.00174v3)
- **Published**: 2017-08-01 06:03:09+00:00
- **Updated**: 2019-08-05 11:19:20+00:00
- **Authors**: Valentin Peretroukhin, Lee Clement, Matthew Giamou, Jonathan Kelly
- **Comment**: In Proceedings of the IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS'15), Hamburg, Germany, Sep. 28-Oct. 2,
  2015
- **Journal**: None
- **Summary**: Navigation in unknown, chaotic environments continues to present a significant challenge for the robotics community. Lighting changes, self-similar textures, motion blur, and moving objects are all considerable stumbling blocks for state-of-the-art vision-based navigation algorithms. In this paper we present a novel technique for improving localization accuracy within a visual-inertial navigation system (VINS). We make use of training data to learn a model for the quality of visual features with respect to localization error in a given environment. This model maps each visual observation from a predefined prediction space of visual-inertial predictors onto a scalar weight, which is then used to scale the observation covariance matrix. In this way, our model can adjust the influence of each observation according to its quality. We discuss our choice of predictors and report substantial reductions in localization error on 4 km of data from the KITTI dataset, as well as on experimental datasets consisting of 700 m of indoor and outdoor driving on a small ground rover equipped with a Skybotix VI-Sensor.



### Model-based learning of local image features for unsupervised texture segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.00180v1
- **DOI**: 10.1109/TIP.2018.2792904
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00180v1)
- **Published**: 2017-08-01 06:35:46+00:00
- **Updated**: 2017-08-01 06:35:46+00:00
- **Authors**: Martin Kiechle, Martin Storath, Andreas Weinmann, Martin Kleinsteuber
- **Comment**: None
- **Journal**: None
- **Summary**: Features that capture well the textural patterns of a certain class of images are crucial for the performance of texture segmentation methods. The manual selection of features or designing new ones can be a tedious task. Therefore, it is desirable to automatically adapt the features to a certain image or class of images. Typically, this requires a large set of training images with similar textures and ground truth segmentation. In this work, we propose a framework to learn features for texture segmentation when no such training data is available. The cost function for our learning process is constructed to match a commonly used segmentation model, the piecewise constant Mumford-Shah model. This means that the features are learned such that they provide an approximately piecewise constant feature image with a small jump set. Based on this idea, we develop a two-stage algorithm which first learns suitable convolutional features and then performs a segmentation. We note that the features can be learned from a small set of images, from a single image, or even from image patches. The proposed method achieves a competitive rank in the Prague texture segmentation benchmark, and it is effective for segmenting histological images.



### Tensorial Recurrent Neural Networks for Longitudinal Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/1708.00185v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.00185v1)
- **Published**: 2017-08-01 07:14:36+00:00
- **Updated**: 2017-08-01 07:14:36+00:00
- **Authors**: Mingyuan Bai, Boyan Zhang, Junbin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional Recurrent Neural Networks assume vectorized data as inputs. However many data from modern science and technology come in certain structures such as tensorial time series data. To apply the recurrent neural networks for this type of data, a vectorisation process is necessary, while such a vectorisation leads to the loss of the precise information of the spatial or longitudinal dimensions. In addition, such a vectorized data is not an optimum solution for learning the representation of the longitudinal data. In this paper, we propose a new variant of tensorial neural networks which directly take tensorial time series data as inputs. We call this new variant as Tensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor Tucker decomposition.



### Real-time Deep Video Deinterlacing
- **Arxiv ID**: http://arxiv.org/abs/1708.00187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00187v1)
- **Published**: 2017-08-01 07:23:53+00:00
- **Updated**: 2017-08-01 07:23:53+00:00
- **Authors**: Haichao Zhu, Xueting Liu, Xiangyu Mao, Tien-Tsin Wong
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: Interlacing is a widely used technique, for television broadcast and video recording, to double the perceived frame rate without increasing the bandwidth. But it presents annoying visual artifacts, such as flickering and silhouette "serration," during the playback. Existing state-of-the-art deinterlacing methods either ignore the temporal information to provide real-time performance but lower visual quality, or estimate the motion for better deinterlacing but with a trade-off of higher computational cost. In this paper, we present the first and novel deep convolutional neural networks (DCNNs) based method to deinterlace with high visual quality and real-time performance. Unlike existing models for super-resolution problems which relies on the translation-invariant assumption, our proposed DCNN model utilizes the temporal information from both the odd and even half frames to reconstruct only the missing scanlines, and retains the given odd and even scanlines for producing the full deinterlaced frames. By further introducing a layer-sharable architecture, our system can achieve real-time performance on a single GPU. Experiments shows that our method outperforms all existing methods, in terms of reconstruction accuracy and computational performance.



### Video Object Segmentation with Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1708.00197v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.00197v1)
- **Published**: 2017-08-01 08:17:37+00:00
- **Updated**: 2017-08-01 08:17:37+00:00
- **Authors**: Xiaoxiao Li, Yuankai Qi, Zhe Wang, Kai Chen, Ziwei Liu, Jianping Shi, Ping Luo, Xiaoou Tang, Chen Change Loy
- **Comment**: Published in CVPR 2017 Workshop, DAVIS Challenge on Video Object
  Segmentation 2017 (Winning Entry)
- **Journal**: None
- **Summary**: Conventional video segmentation methods often rely on temporal continuity to propagate masks. Such an assumption suffers from issues like drifting and inability to handle large displacement. To overcome these issues, we formulate an effective mechanism to prevent the target from being lost via adaptive object re-identification. Specifically, our Video Object Segmentation with Re-identification (VS-ReID) model includes a mask propagation module and a ReID module. The former module produces an initial probability map by flow warping while the latter module retrieves missing instances by adaptive matching. With these two modules iteratively applied, our VS-ReID records a global mean (Region Jaccard and Boundary F measure) of 0.699, the best performance in 2017 DAVIS Challenge.



### Switching Convolutional Neural Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1708.00199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00199v2)
- **Published**: 2017-08-01 08:30:28+00:00
- **Updated**: 2017-08-03 11:02:02+00:00
- **Authors**: Deepak Babu Sam, Shiv Surya, R. Venkatesh Babu
- **Comment**: Version 2: Added references. Corrected Typos. Published at CVPR 2017,
  Honolulu, Hawaii. The first two authors contributed equally. For Project Page
  see http://val.serc.iisc.ernet.in/crowdcnn/
- **Journal**: None
- **Summary**: We propose a novel crowd counting model that maps a given crowd scene to its density. Crowd analysis is compounded by myriad of factors like inter-occlusion between people due to extreme crowding, high similarity of appearance between people and background elements, and large variability of camera view-points. Current state-of-the art approaches tackle these factors by using multi-scale CNN architectures, recurrent networks and late fusion of features from multi-column CNN with different receptive fields. We propose switching convolutional neural network that leverages variation of crowd density within an image to improve the accuracy and localization of the predicted crowd count. Patches from a grid within a crowd scene are relayed to independent CNN regressors based on crowd count prediction quality of the CNN established during training. The independent CNN regressors are designed to have different receptive fields and a switch classifier is trained to relay the crowd scene patch to the best CNN regressor. We perform extensive experiments on all major crowd counting datasets and evidence better performance compared to current state-of-the-art methods. We provide interpretable representations of the multichotomy of space of crowd scene patches inferred from the switch. It is observed that the switch relays an image patch to a particular CNN column based on density of crowd.



### Learning to Hallucinate Face Images via Component Generation and Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1708.00223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.00223v1)
- **Published**: 2017-08-01 09:46:18+00:00
- **Updated**: 2017-08-01 09:46:18+00:00
- **Authors**: Yibing Song, Jiawei Zhang, Shengfeng He, Linchao Bao, Qingxiong Yang
- **Comment**: IJCAI 2017. Project page:
  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sr/index.html
- **Journal**: None
- **Summary**: We propose a two-stage method for face hallucination. First, we generate facial components of the input image using CNNs. These components represent the basic facial structures. Second, we synthesize fine-grained facial structures from high resolution training images. The details of these structures are transferred into facial components for enhancement. Therefore, we generate facial components to approximate ground truth global appearance in the first stage and enhance them through recovering details in the second stage. The experiments demonstrate that our method performs favorably against state-of-the-art methods



### Fast Preprocessing for Robust Face Sketch Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1708.00224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.00224v1)
- **Published**: 2017-08-01 09:46:54+00:00
- **Updated**: 2017-08-01 09:46:54+00:00
- **Authors**: Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang
- **Comment**: IJCAI 2017. Project page:
  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sketch/index.html
- **Journal**: None
- **Summary**: Exemplar-based face sketch synthesis methods usually meet the challenging problem that input photos are captured in different lighting conditions from training photos. The critical step causing the failure is the search of similar patch candidates for an input photo patch. Conventional illumination invariant patch distances are adopted rather than directly relying on pixel intensity difference, but they will fail when local contrast within a patch changes. In this paper, we propose a fast preprocessing method named Bidirectional Luminance Remapping (BLR), which interactively adjust the lighting of training and input photos. Our method can be directly integrated into state-of-the-art exemplar-based methods to improve their robustness with ignorable computational cost.



### CREST: Convolutional Residual Learning for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1708.00225v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.00225v1)
- **Published**: 2017-08-01 09:47:20+00:00
- **Updated**: 2017-08-01 09:47:20+00:00
- **Authors**: Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson Lau, Ming-Hsuan Yang
- **Comment**: ICCV 2017. Project page:
  http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html
- **Journal**: None
- **Summary**: Discriminative correlation filters (DCFs) have been shown to perform superiorly in visual tracking. They only need a small set of training samples from the initial frame to generate an appearance model. However, existing DCFs learn the filters separately from feature extraction, and update these filters using a moving average operation with an empirical weight. These DCF trackers hardly benefit from the end-to-end training. In this paper, we propose the CREST algorithm to reformulate DCFs as a one-layer convolutional neural network. Our method integrates feature extraction, response map generation as well as model update into the neural networks for an end-to-end training. To reduce model degradation during online update, we apply residual learning to take appearance changes into account. Extensive experiments on the benchmark datasets demonstrate that our CREST tracker performs favorably against state-of-the-art trackers.



### HMM-based Indic Handwritten Word Recognition using Zone Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1708.00227v1
- **DOI**: 10.1016/j.patcog.2016.04.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00227v1)
- **Published**: 2017-08-01 09:52:03+00:00
- **Updated**: 2017-08-01 09:52:03+00:00
- **Authors**: Partha Pratim Roy, Ayan Kumar Bhunia, Ayan Das, Prasenjit Dey, Umapada Pal
- **Comment**: Published in Pattern Recognition(2016)
- **Journal**: Pattern Recognition, Volume 60, December 2016, Pages 1057-1075
- **Summary**: This paper presents a novel approach towards Indic handwritten word recognition using zone-wise information. Because of complex nature due to compound characters, modifiers, overlapping and touching, etc., character segmentation and recognition is a tedious job in Indic scripts (e.g. Devanagari, Bangla, Gurumukhi, and other similar scripts). To avoid character segmentation in such scripts, HMM-based sequence modeling has been used earlier in holistic way. This paper proposes an efficient word recognition framework by segmenting the handwritten word images horizontally into three zones (upper, middle and lower) and recognize the corresponding zones. The main aim of this zone segmentation approach is to reduce the number of distinct component classes compared to the total number of classes in Indic scripts. As a result, use of this zone segmentation approach enhances the recognition performance of the system. The components in middle zone where characters are mostly touching are recognized using HMM. After the recognition of middle zone, HMM based Viterbi forced alignment is applied to mark the left and right boundaries of the characters. Next, the residue components, if any, in upper and lower zones in their respective boundary are combined to achieve the final word level recognition. Water reservoir feature has been integrated in this framework to improve the zone segmentation and character alignment defects while segmentation. A novel sliding window-based feature, called Pyramid Histogram of Oriented Gradient (PHOG) is proposed for middle zone recognition. An exhaustive experiment is performed on two Indic scripts namely, Bangla and Devanagari for the performance evaluation. From the experiment, it has been noted that proposed zone-wise recognition improves accuracy with respect to the traditional way of Indic word recognition.



### CNN Cascades for Segmenting Whole Slide Images of the Kidney
- **Arxiv ID**: http://arxiv.org/abs/1708.00251v1
- **DOI**: 10.1016/j.compmedimag.2018.11.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00251v1)
- **Published**: 2017-08-01 11:13:04+00:00
- **Updated**: 2017-08-01 11:13:04+00:00
- **Authors**: Michael Gadermayr, Ann-Kathrin Dombrowski, Barbara Mara Klinkhammer, Peter Boor, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the increasing availability of whole slide scanners facilitating digitization of histopathological tissue, there is a strong demand for the development of computer based image analysis systems. In this work, the focus is on the segmentation of the glomeruli constituting a highly relevant structure in renal histopathology, which has not been investigated before in combination with CNNs. We propose two different CNN cascades for segmentation applications with sparse objects. These approaches are applied to the problem of glomerulus segmentation and compared with conventional fully-convolutional networks. Overall, with the best performing cascade approach, single CNNs are outperformed and a pixel-level Dice similarity coefficient of 0.90 is obtained. Combined with qualitative and further object-level analyses the obtained results are assessed as excellent also compared to recent approaches. In conclusion, we can state that especially one of the proposed cascade networks proved to be a highly powerful tool for segmenting the renal glomeruli providing best segmentation accuracies and also keeping the computing time at a low level.



### Learning Deep Convolutional Embeddings for Face Representation Using Joint Sample- and Set-based Supervision
- **Arxiv ID**: http://arxiv.org/abs/1708.00277v3
- **DOI**: 10.1109/ICCVW.2017.195
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00277v3)
- **Published**: 2017-08-01 12:22:21+00:00
- **Updated**: 2018-01-12 19:03:06+00:00
- **Authors**: Baris Gecer, Vassileios Balntas, Tae-Kyun Kim
- **Comment**: 8 pages, 5 figures, 2 tables, workshop paper
- **Journal**: The IEEE International Conference on Computer Vision (ICCV) 2017,
  p.1665-1672
- **Summary**: In this work, we investigate several methods and strategies to learn deep embeddings for face recognition, using joint sample- and set-based optimization. We explain our framework that expands traditional learning with set-based supervision together with the strategies used to maintain set characteristics. We, then, briefly review the related set-based loss functions, and subsequently propose a novel Max-Margin Loss which maximizes maximum possible inter-class margin with assistance of Support Vector Machines (SVMs). It implicitly pushes all the samples towards correct side of the margin with a vector perpendicular to the hyperplane and a strength exponentially growing towards to negative side of the hyperplane. We show that the introduced loss outperform the previous sample-based and set-based ones in terms verification of faces on two commonly used benchmarks.



### Dual Motion GAN for Future-Flow Embedded Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1708.00284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00284v2)
- **Published**: 2017-08-01 12:38:58+00:00
- **Updated**: 2017-08-03 04:23:30+00:00
- **Authors**: Xiaodan Liang, Lisa Lee, Wei Dai, Eric P. Xing
- **Comment**: ICCV 17 camera ready
- **Journal**: None
- **Summary**: Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.



### Best Viewpoint Tracking for Camera Mounted on Robotic Arm with Dynamic Obstacles
- **Arxiv ID**: http://arxiv.org/abs/1708.00300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00300v2)
- **Published**: 2017-08-01 13:18:23+00:00
- **Updated**: 2017-10-17 13:34:24+00:00
- **Authors**: Christos Maniatis, Marcelo Saval-Calvo, Radim Tylecek, Robert B. Fisher
- **Comment**: 10 pages, 6 figures, poster in 3DV conference
- **Journal**: None
- **Summary**: The problem of finding a next best viewpoint for 3D modeling or scene mapping has been explored in computer vision over the last decade. This paper tackles a similar problem, but with different characteristics. It proposes a method for dynamic next best viewpoint recovery of a target point while avoiding possible occlusions. Since the environment can change, the method has to iteratively find the next best view with a global understanding of the free and occupied parts.   We model the problem as a set of possible viewpoints which correspond to the centers of the facets of a virtual tessellated hemisphere covering the scene. Taking into account occlusions, distances between current and future viewpoints, quality of the viewpoint and joint constraints (robot arm joint distances or limits), we evaluate the next best viewpoint. The proposal has been evaluated on 8 different scenarios with different occlusions and a short 3D video sequence to validate its dynamic performance.



### Generative Semantic Manipulation with Contrasting GAN
- **Arxiv ID**: http://arxiv.org/abs/1708.00315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00315v1)
- **Published**: 2017-08-01 13:46:32+00:00
- **Updated**: 2017-08-01 13:46:32+00:00
- **Authors**: Xiaodan Liang, Hao Zhang, Eric P. Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently achieved significant improvement on paired/unpaired image-to-image translation, such as photo$\rightarrow$ sketch and artist painting style transfer. However, existing models can only be capable of transferring the low-level information (e.g. color or texture changes), but fail to edit high-level semantic meanings (e.g., geometric structure or content) of objects. On the other hand, while some researches can synthesize compelling real-world images given a class label or caption, they cannot condition on arbitrary shapes or structures, which largely limits their application scenarios and interpretive capability of model results. In this work, we focus on a more challenging semantic manipulation task, which aims to modify the semantic meaning of an object while preserving its own characteristics (e.g. viewpoints and shapes), such as cow$\rightarrow$sheep, motor$\rightarrow$ bicycle, cat$\rightarrow$dog. To tackle such large semantic changes, we introduce a contrasting GAN (contrast-GAN) with a novel adversarial contrasting objective. Instead of directly making the synthesized samples close to target data as previous GANs did, our adversarial contrasting objective optimizes over the distance comparisons between samples, that is, enforcing the manipulated data be semantically closer to the real data with target category than the input data. Equipped with the new contrasting objective, a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes. Experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset show considerable performance gain by our contrast-GAN over other conditional GANs. Quantitative results further demonstrate the superiority of our model on generating manipulated results with high visual fidelity and reasonable object semantics.



### Self-Supervised Learning for Spinal MRIs
- **Arxiv ID**: http://arxiv.org/abs/1708.00367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00367v1)
- **Published**: 2017-08-01 14:44:48+00:00
- **Updated**: 2017-08-01 14:44:48+00:00
- **Authors**: Amir Jamaludin, Timor Kadir, Andrew Zisserman
- **Comment**: 3rd Workshop on Deep Learning in Medical Image Analysis
- **Journal**: None
- **Summary**: A significant proportion of patients scanned in a clinical setting have follow-up scans. We show in this work that such longitudinal scans alone can be used as a form of 'free' self-supervision for training a deep network. We demonstrate this self-supervised learning for the case of T2-weighted sagittal lumbar Magnetic Resonance Images (MRIs). A Siamese convolutional neural network (CNN) is trained using two losses: (i) a contrastive loss on whether the scan is of the same person (i.e. longitudinal) or not, together with (ii) a classification loss on predicting the level of vertebral bodies. The performance of this pre-trained network is then assessed on a grading classification task. We experiment on a dataset of 1016 subjects, 423 possessing follow-up scans, with the end goal of learning the disc degeneration radiological gradings attached to the intervertebral discs. We show that the performance of the pre-trained CNN on the supervised classification task is (i) superior to that of a network trained from scratch; and (ii) requires far fewer annotated training samples to reach an equivalent performance to that of the network trained from scratch.



### Hand2Face: Automatic Synthesis and Recognition of Hand Over Face Occlusions
- **Arxiv ID**: http://arxiv.org/abs/1708.00370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00370v2)
- **Published**: 2017-08-01 14:46:09+00:00
- **Updated**: 2017-08-17 02:06:44+00:00
- **Authors**: Behnaz Nojavanasghari, Charles. E. Hughes, Tadas Baltrusaitis, Louis-philippe Morency
- **Comment**: Accepted to International Conference on Affective Computing and
  Intelligent Interaction (ACII), 2017
- **Journal**: None
- **Summary**: A person's face discloses important information about their affective state. Although there has been extensive research on recognition of facial expressions, the performance of existing approaches is challenged by facial occlusions. Facial occlusions are often treated as noise and discarded in recognition of affective states. However, hand over face occlusions can provide additional information for recognition of some affective states such as curiosity, frustration and boredom. One of the reasons that this problem has not gained attention is the lack of naturalistic occluded faces that contain hand over face occlusions as well as other types of occlusions. Traditional approaches for obtaining affective data are time demanding and expensive, which limits researchers in affective computing to work on small datasets. This limitation affects the generalizability of models and deprives researchers from taking advantage of recent advances in deep learning that have shown great success in many fields but require large volumes of data. In this paper, we first introduce a novel framework for synthesizing naturalistic facial occlusions from an initial dataset of non-occluded faces and separate images of hands, reducing the costly process of data collection and annotation. We then propose a model for facial occlusion type recognition to differentiate between hand over face occlusions and other types of occlusions such as scarves, hair, glasses and objects. Finally, we present a model to localize hand over face occlusions and identify the occluded regions of the face.



### Segmentation of Glioma Tumors in Brain Using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1708.00377v1
- **DOI**: 10.1016/j.neucom.2017.12.032
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00377v1)
- **Published**: 2017-08-01 15:06:07+00:00
- **Updated**: 2017-08-01 15:06:07+00:00
- **Authors**: Saddam Hussain, Syed Muhammad Anwar, Muhammad Majid
- **Comment**: Submitted to Neurocomputing
- **Journal**: Neurocomputing 2018
- **Summary**: Detection of brain tumor using a segmentation based approach is critical in cases, where survival of a subject depends on an accurate and timely clinical diagnosis. Gliomas are the most commonly found tumors having irregular shape and ambiguous boundaries, making them one of the hardest tumors to detect. The automation of brain tumor segmentation remains a challenging problem mainly due to significant variations in its structure. An automated brain tumor segmentation algorithm using deep convolutional neural network (DCNN) is presented in this paper. A patch based approach along with an inception module is used for training the deep network by extracting two co-centric patches of different sizes from the input images. Recent developments in deep neural networks such as drop-out, batch normalization, non-linear activation and inception module are used to build a new ILinear nexus architecture. The module overcomes the over-fitting problem arising due to scarcity of data using drop-out regularizer. Images are normalized and bias field corrected in the pre-processing step and then extracted patches are passed through a DCNN, which assigns an output label to the central pixel of each patch. Morphological operators are used for post-processing to remove small false positives around the edges. A two-phase weighted training method is introduced and evaluated using BRATS 2013 and BRATS 2015 datasets, where it improves the performance parameters of state-of-the-art techniques under similar settings.



### Momo: Monocular Motion Estimation on Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1708.00397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00397v1)
- **Published**: 2017-08-01 15:51:32+00:00
- **Updated**: 2017-08-01 15:51:32+00:00
- **Authors**: Johannes Graeter, Tobias Strauss, Martin Lauer
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge about the location of a vehicle is indispensable for autonomous driving. In order to apply global localisation methods, a pose prior must be known which can be obtained from visual odometry. The quality and robustness of that prior determine the success of localisation. Momo is a monocular frame-to-frame motion estimation methodology providing a high quality visual odometry for that purpose. By taking into account the motion model of the vehicle, reliability and accuracy of the pose prior are significantly improved. We show that especially in low-structure environments Momo outperforms the state of the art. Moreover, the method is designed so that multiple cameras with or without overlap can be integrated. The evaluation on the KITTI-dataset and on a proper multi-camera dataset shows that even with only 100--300 feature matches the prior is estimated with high accuracy and in real-time.



### Depth Super-Resolution Meets Uncalibrated Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/1708.00411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00411v2)
- **Published**: 2017-08-01 16:39:56+00:00
- **Updated**: 2017-08-24 16:24:55+00:00
- **Authors**: Songyou Peng, Bjoern Haefner, Yvain Qu√©au, Daniel Cremers
- **Comment**: International Conference on Computer Vision (ICCV) Workshop, 2017
- **Journal**: None
- **Summary**: A novel depth super-resolution approach for RGB-D sensors is presented. It disambiguates depth super-resolution through high-resolution photometric clues and, symmetrically, it disambiguates uncalibrated photometric stereo through low-resolution depth cues. To this end, an RGB-D sequence is acquired from the same viewing angle, while illuminating the scene from various uncalibrated directions. This sequence is handled by a variational framework which fits high-resolution shape and reflectance, as well as lighting, to both the low-resolution depth measurements and the high-resolution RGB ones. The key novelty consists in a new PDE-based photometric stereo regularizer which implicitly ensures surface regularity. This allows to carry out depth super-resolution in a purely data-driven manner, without the need for any ad-hoc prior or material calibration. Real-world experiments are carried out using an out-of-the-box RGB-D sensor and a hand-held LED light source.



### Improved Speech Reconstruction from Silent Video
- **Arxiv ID**: http://arxiv.org/abs/1708.01204v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1708.01204v3)
- **Published**: 2017-08-01 18:01:08+00:00
- **Updated**: 2017-08-29 19:45:23+00:00
- **Authors**: Ariel Ephrat, Tavi Halperin, Shmuel Peleg
- **Comment**: Accepted to ICCV 2017 Workshop on Computer Vision for Audio-Visual
  Media. Supplementary video: https://www.youtube.com/watch?v=Xjbn7h7tpg0.
  arXiv admin note: text overlap with arXiv:1701.00495
- **Journal**: None
- **Summary**: Speechreading is the task of inferring phonetic information from visually observed articulatory facial movements, and is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible and natural-sounding acoustic speech signal from silent video frames of a speaking person. We train our model on speakers from the GRID and TCD-TIMIT datasets, and evaluate the quality and intelligibility of reconstructed speech using common objective measurements. We show that speech predictions from the proposed model attain scores which indicate significantly improved quality over existing models. In addition, we show promising results towards reconstructing speech from an unconstrained dictionary.



### Active Learning for Convolutional Neural Networks: A Core-Set Approach
- **Arxiv ID**: http://arxiv.org/abs/1708.00489v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.00489v4)
- **Published**: 2017-08-01 19:50:53+00:00
- **Updated**: 2018-06-01 10:17:23+00:00
- **Authors**: Ozan Sener, Silvio Savarese
- **Comment**: ICLR 2018 Paper
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning).   Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.



### Dense Piecewise Planar RGB-D SLAM for Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1708.00514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.00514v1)
- **Published**: 2017-08-01 21:07:14+00:00
- **Updated**: 2017-08-01 21:07:14+00:00
- **Authors**: Phi-Hung Le, Jana Kosecka
- **Comment**: International Conference on Intelligent Robots and Systems (IROS)
  2017
- **Journal**: None
- **Summary**: The paper exploits weak Manhattan constraints to parse the structure of indoor environments from RGB-D video sequences in an online setting. We extend the previous approach for single view parsing of indoor scenes to video sequences and formulate the problem of recovering the floor plan of the environment as an optimal labeling problem solved using dynamic programming. The temporal continuity is enforced in a recursive setting, where labeling from previous frames is used as a prior term in the objective function. In addition to recovery of piecewise planar weak Manhattan structure of the extended environment, the orthogonality constraints are also exploited by visual odometry and pose graph optimization. This yields reliable estimates in the presence of large motions and absence of distinctive features to track. We evaluate our method on several challenging indoors sequences demonstrating accurate SLAM and dense mapping of low texture environments. On existing TUM benchmark we achieve competitive results with the alternative approaches which fail in our environments.




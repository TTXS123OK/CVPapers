# Arxiv Papers in cs.CV on 2017-08-07
### PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN
- **Arxiv ID**: http://arxiv.org/abs/1708.01956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01956v1)
- **Published**: 2017-08-07 01:07:20+00:00
- **Updated**: 2017-08-07 01:07:20+00:00
- **Authors**: Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, Shih-Fu Chang
- **Comment**: To appear in International Conference on Computer Vision (ICCV) 2017,
  Venice, Italy
- **Journal**: None
- **Summary**: We aim to tackle a novel vision task called Weakly Supervised Visual Relation Detection (WSVRD) to detect "subject-predicate-object" relations in an image with object relation groundtruths available only at the image level. This is motivated by the fact that it is extremely expensive to label the combinatorial relations between objects at the instance level. Compared to the extensively studied problem, Weakly Supervised Object Detection (WSOD), WSVRD is more challenging as it needs to examine a large set of regions pairs, which is computationally prohibitive and more likely stuck in a local optimal solution such as those involving wrong spatial context. To this end, we present a Parallel, Pairwise Region-based, Fully Convolutional Network (PPR-FCN) for WSVRD. It uses a parallel FCN architecture that simultaneously performs pair selection and classification of single regions and region pairs for object and relation detection, while sharing almost all computation shared over the entire image. In particular, we propose a novel position-role-sensitive score map with pairwise RoI pooling to efficiently capture the crucial context associated with a pair of objects. We demonstrate the superiority of PPR-FCN over all baselines in solving the WSVRD challenge by using results of extensive experiments over two visual relation benchmarks.



### Accurate Light Field Depth Estimation with Superpixel Regularization over Partially Occluded Regions
- **Arxiv ID**: http://arxiv.org/abs/1708.01964v1
- **DOI**: 10.1109/TIP.2018.2839524
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01964v1)
- **Published**: 2017-08-07 02:05:36+00:00
- **Updated**: 2017-08-07 02:05:36+00:00
- **Authors**: Jie Chen, Junhui Hou, Yun Ni, Lap-Pui Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is a fundamental problem for light field photography applications. Numerous methods have been proposed in recent years, which either focus on crafting cost terms for more robust matching, or on analyzing the geometry of scene structures embedded in the epipolar-plane images. Significant improvements have been made in terms of overall depth estimation error; however, current state-of-the-art methods still show limitations in handling intricate occluding structures and complex scenes with multiple occlusions. To address these challenging issues, we propose a very effective depth estimation framework which focuses on regularizing the initial label confidence map and edge strength weights. Specifically, we first detect partially occluded boundary regions (POBR) via superpixel based regularization. Series of shrinkage/reinforcement operations are then applied on the label confidence map and edge strength weights over the POBR. We show that after weight manipulations, even a low-complexity weighted least squares model can produce much better depth estimation than state-of-the-art methods in terms of average disparity error rate, occlusion boundary precision-recall rate, and the preservation of intricate visual features.



### Identifying 3 moss species by deep learning, using the "chopped picture" method
- **Arxiv ID**: http://arxiv.org/abs/1708.01986v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.01986v2)
- **Published**: 2017-08-07 04:37:23+00:00
- **Updated**: 2017-08-08 01:38:37+00:00
- **Authors**: Takeshi Ise, Mari Minagawa, Masanori Onishi
- **Comment**: 7 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: In general, object identification tends not to work well on ambiguous, amorphous objects such as vegetation. In this study, we developed a simple but effective approach to identify ambiguous objects and applied the method to several moss species. As a result, the model correctly classified test images with accuracy more than 90%. Using this approach will help progress in computer vision studies.



### Identity-Aware Textual-Visual Matching with Latent Co-attention
- **Arxiv ID**: http://arxiv.org/abs/1708.01988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01988v1)
- **Published**: 2017-08-07 04:57:45+00:00
- **Updated**: 2017-08-07 04:57:45+00:00
- **Authors**: Shuang Li, Tong Xiao, Hongsheng Li, Wei Yang, Xiaogang Wang
- **Comment**: Accepted to ICCV 2017
- **Journal**: None
- **Summary**: Textual-visual matching aims at measuring similarities between sentence descriptions and images. Most existing methods tackle this problem without effectively utilizing identity-level annotations. In this paper, we propose an identity-aware two-stage framework for the textual-visual matching problem. Our stage-1 CNN-LSTM network learns to embed cross-modal features with a novel Cross-Modal Cross-Entropy (CMCE) loss. The stage-1 network is able to efficiently screen easy incorrect matchings and also provide initial training point for the stage-2 training. The stage-2 CNN-LSTM network refines the matching results with a latent co-attention mechanism. The spatial attention relates each word with corresponding image regions while the latent semantic attention aligns different sentence structures to make the matching results more robust to sentence structure variations. Extensive experiments on three datasets with identity-level annotations show that our framework outperforms state-of-the-art approaches by large margins.



### Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.02001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02001v1)
- **Published**: 2017-08-07 06:29:38+00:00
- **Updated**: 2017-08-07 06:29:38+00:00
- **Authors**: Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, Xiang Ruan
- **Comment**: Accepted as a poster in ICCV 2017, including 10 pages, 5 figures and
  2 tables
- **Journal**: None
- **Summary**: Fully convolutional neural networks (FCNs) have shown outstanding performance in many dense labeling problems. One key pillar of these successes is mining relevant information from features in convolutional layers. However, how to better aggregate multi-level convolutional feature maps for salient object detection is underexplored. In this work, we present Amulet, a generic aggregating multi-level convolutional feature framework for salient object detection. Our framework first integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and fine details. Then it adaptively learns to combine these feature maps at each resolution and predict saliency maps with the combined features. Finally, the predicted results are efficiently fused to generate the final saliency map. In addition, to achieve accurate boundary inference and semantic enhancement, edge-aware feature maps in low-level layers and the predicted results of low resolution features are recursively embedded into the learning framework. By aggregating multi-level convolutional features in this efficient and flexible manner, the proposed saliency model provides accurate salient object labeling. Comprehensive experiments demonstrate that our method performs favorably against state-of-the art approaches in terms of near all compared evaluation metrics.



### Focal Loss for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.02002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02002v2)
- **Published**: 2017-08-07 06:32:42+00:00
- **Updated**: 2018-02-07 18:44:44+00:00
- **Authors**: Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár
- **Comment**: None
- **Journal**: None
- **Summary**: The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.



### Learning Uncertain Convolutional Features for Accurate Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.02031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02031v1)
- **Published**: 2017-08-07 08:18:04+00:00
- **Updated**: 2017-08-07 08:18:04+00:00
- **Authors**: Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, Baocai Yin
- **Comment**: Accepted as a poster in ICCV 2017,including 10 pages, 7 figures and 3
  tables
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have delivered superior performance in many computer vision tasks. In this paper, we propose a novel deep fully convolutional network model for accurate salient object detection. The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection. We achieve this via introducing a reformulated dropout (R-dropout) after specific convolutional layers to construct an uncertain ensemble of internal feature units. In addition, we propose an effective hybrid upsampling method to reduce the checkerboard artifacts of deconvolution operators in our decoder network. The proposed methods can also be applied to other deep convolutional networks. Compared with existing saliency detection methods, the proposed UCF model is able to incorporate uncertainties for more accurate object boundary inference. Extensive experiments demonstrate that our proposed saliency model performs favorably against state-of-the-art approaches. The uncertain feature learning mechanism as well as the upsampling method can significantly improve performance on other pixel-wise vision tasks.



### A Solution for Crime Scene Reconstruction using Time-of-Flight Cameras
- **Arxiv ID**: http://arxiv.org/abs/1708.02033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02033v1)
- **Published**: 2017-08-07 08:29:19+00:00
- **Updated**: 2017-08-07 08:29:19+00:00
- **Authors**: Silvio Giancola, Daniele Piron, Pasquale Poppa, Remo Sala
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a method for three-dimensional (3D) reconstruction of wide crime scene, based on a Simultaneous Localization and Mapping (SLAM) approach. We used a Kinect V2 Time-of-Flight (TOF) RGB-D camera to provide colored dense point clouds at a 30 Hz frequency. This device is moved freely (6 degrees of freedom) during the scene exploration. The implemented SLAM solution aligns successive point clouds using an 3D keypoints description and matching approach. This type of approach exploits both colorimetric and geometrical information, and permits reconstruction under poor illumination conditions. Our solution has been tested for indoor crime scene and outdoor archaeological site reconstruction, returning a mean error around one centimeter. It is less precise than environmental laser scanner solution, but more practical and portable as well as less cumbersome. Also, the hardware is definitively cheaper.



### What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?
- **Arxiv ID**: http://arxiv.org/abs/1708.02043v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1708.02043v2)
- **Published**: 2017-08-07 09:01:35+00:00
- **Updated**: 2017-08-25 15:40:00+00:00
- **Authors**: Marc Tanti, Albert Gatt, Kenneth P. Camilleri
- **Comment**: Appears in: Proceedings of the 10th International Conference on
  Natural Language Generation (INLG'17)
- **Journal**: None
- **Summary**: In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage. This paper compares these two architectures. We find that, in general, late merging outperforms injection, suggesting that RNNs are better viewed as encoders, rather than generators.



### Unconstrained Fashion Landmark Detection via Hierarchical Recurrent Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.02044v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.02044v1)
- **Published**: 2017-08-07 09:02:52+00:00
- **Updated**: 2017-08-07 09:02:52+00:00
- **Authors**: Sijie Yan, Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang
- **Comment**: To appear in ACM Multimedia (ACM MM) 2017 as a full research paper.
  More details at the project page:
  http://personal.ie.cuhk.edu.hk/~lz013/projects/UnconstrainedLandmarks.html
- **Journal**: None
- **Summary**: Fashion landmarks are functional key points defined on clothes, such as corners of neckline, hemline, and cuff. They have been recently introduced as an effective visual representation for fashion image understanding. However, detecting fashion landmarks are challenging due to background clutters, human poses, and scales. To remove the above variations, previous works usually assumed bounding boxes of clothes are provided in training and test as additional annotations, which are expensive to obtain and inapplicable in practice. This work addresses unconstrained fashion landmark detection, where clothing bounding boxes are not provided in both training and test. To this end, we present a novel Deep LAndmark Network (DLAN), where bounding boxes and landmarks are jointly estimated and trained iteratively in an end-to-end manner. DLAN contains two dedicated modules, including a Selective Dilated Convolution for handling scale discrepancies, and a Hierarchical Recurrent Spatial Transformer for handling background clutters. To evaluate DLAN, we present a large-scale fashion landmark dataset, namely Unconstrained Landmark Database (ULD), consisting of 30K images. Statistics show that ULD is more challenging than existing datasets in terms of image scales, background clutters, and human poses. Extensive experiments demonstrate the effectiveness of DLAN over the state-of-the-art methods. DLAN also exhibits excellent generalization across different clothing categories and modalities, making it extremely suitable for real-world fashion analysis.



### Structured Attentions for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1708.02071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02071v1)
- **Published**: 2017-08-07 11:14:11+00:00
- **Updated**: 2017-08-07 11:14:11+00:00
- **Authors**: Chen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, Yi Ma
- **Comment**: ICCV2017
- **Journal**: None
- **Summary**: Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models. Although the questions may involve complex relations among multiple regions, few attention models can effectively encode such cross-region relations. In this paper, we demonstrate the importance of encoding such relations by showing the limited effective receptive field of ResNet on two datasets, and propose to model the visual attention as a multivariate distribution over a grid-structured Conditional Random Field on image regions. We demonstrate how to convert the iterative inference algorithms, Mean Field and Loopy Belief Propagation, as recurrent layers of an end-to-end neural network. We empirically evaluated our model on 3 datasets, in which it surpasses the best baseline model of the newly released CLEVR dataset by 9.5%, and the best published model on the VQA dataset by 1.25%. Source code is available at https: //github.com/zhuchen03/vqa-sva.



### Measuring Catastrophic Forgetting in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.02072v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.02072v4)
- **Published**: 2017-08-07 11:18:43+00:00
- **Updated**: 2017-11-09 14:53:07+00:00
- **Authors**: Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, Christopher Kanan
- **Comment**: To appear in AAAI 2018
- **Journal**: None
- **Summary**: Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.



### Learning for Active 3D Mapping
- **Arxiv ID**: http://arxiv.org/abs/1708.02074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02074v1)
- **Published**: 2017-08-07 11:21:57+00:00
- **Updated**: 2017-08-07 11:21:57+00:00
- **Authors**: Karel Zimmermann, Tomas Petricek, Vojtech Salansky, Tomas Svoboda
- **Comment**: ICCV 2017 (oral). See video:
  https://www.youtube.com/watch?v=KNex0zjeGYE
- **Journal**: None
- **Summary**: We propose an active 3D mapping method for depth sensors, which allow individual control of depth-measuring rays, such as the newly emerging solid-state lidars. The method simultaneously (i) learns to reconstruct a dense 3D occupancy map from sparse depth measurements, and (ii) optimizes the reactive control of depth-measuring rays. To make the first step towards the online control optimization, we propose a fast prioritized greedy algorithm, which needs to update its cost function in only a small fraction of pos- sible rays. The approximation ratio of the greedy algorithm is derived. An experimental evaluation on the subset of the KITTI dataset demonstrates significant improve- ment in the 3D map accuracy when learning-to-reconstruct from sparse measurements is coupled with the optimization of depth-measuring rays.



### Extraction of Airways with Probabilistic State-space Models and Bayesian Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1708.02096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02096v1)
- **Published**: 2017-08-07 12:43:26+00:00
- **Updated**: 2017-08-07 12:43:26+00:00
- **Authors**: Raghavendra Selvan, Jens Petersen, Jesper H. Pedersen, Marleen de Bruijne
- **Comment**: 10 pages. Pre-print of the paper accepted at Workshop on Graphs in
  Biomedical Image Analysis. MICCAI 2017. Quebec City
- **Journal**: None
- **Summary**: Segmenting tree structures is common in several image processing applications. In medical image analysis, reliable segmentations of airways, vessels, neurons and other tree structures can enable important clinical applications. We present a framework for tracking tree structures comprising of elongated branches using probabilistic state-space models and Bayesian smoothing. Unlike most existing methods that proceed with sequential tracking of branches, we present an exploratory method, that is less sensitive to local anomalies in the data due to acquisition noise and/or interfering structures. The evolution of individual branches is modelled using a process model and the observed data is incorporated into the update step of the Bayesian smoother using a measurement model that is based on a multi-scale blob detector. Bayesian smoothing is performed using the RTS (Rauch-Tung-Striebel) smoother, which provides Gaussian density estimates of branch states at each tracking step. We select likely branch seed points automatically based on the response of the blob detection and track from all such seed points using the RTS smoother. We use covariance of the marginal posterior density estimated for each branch to discriminate false positive and true positive branches. The method is evaluated on 3D chest CT scans to track airways. We show that the presented method results in additional branches compared to a baseline method based on region growing on probability images.



### Two-Phase Learning for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1708.02108v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02108v3)
- **Published**: 2017-08-07 13:20:50+00:00
- **Updated**: 2017-08-16 17:51:55+00:00
- **Authors**: Dahun Kim, Donghyeon Cho, Donggeun Yoo, In So Kweon
- **Comment**: Accepted at ICCV 2017
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation and localiza- tion have a problem of focusing only on the most important parts of an image since they use only image-level annota- tions. In this paper, we solve this problem fundamentally via two-phase learning. Our networks are trained in two steps. In the first step, a conventional fully convolutional network (FCN) is trained to find the most discriminative parts of an image. In the second step, the activations on the most salient parts are suppressed by inference conditional feedback, and then the second learning is performed to find the area of the next most important parts. By combining the activations of both phases, the entire portion of the tar- get object can be captured. Our proposed training scheme is novel and can be utilized in well-designed techniques for weakly supervised semantic segmentation, salient region detection, and object location prediction. Detailed experi- ments demonstrate the effectiveness of our two-phase learn- ing in each task.



### MonoPerfCap: Human Performance Capture from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1708.02136v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1708.02136v2)
- **Published**: 2017-08-07 14:43:57+00:00
- **Updated**: 2018-02-23 12:40:25+00:00
- **Authors**: Weipeng Xu, Avishek Chatterjee, Michael Zollhöfer, Helge Rhodin, Dushyant Mehta, Hans-Peter Seidel, Christian Theobalt
- **Comment**: Accepted to ACM TOG 2018, to be presented on SIGGRAPH 2018
- **Journal**: None
- **Summary**: We present the first marker-less approach for temporally coherent 3D performance capture of a human with general clothing from monocular video. Our approach reconstructs articulated human skeleton motion as well as medium-scale non-rigid surface deformations in general scenes. Human performance capture is a challenging problem due to the large range of articulation, potentially fast motion, and considerable non-rigid deformations, even from multi-view data. Reconstruction from monocular video alone is drastically more challenging, since strong occlusions and the inherent depth ambiguity lead to a highly ill-posed reconstruction problem. We tackle these challenges by a novel approach that employs sparse 2D and 3D human pose detections from a convolutional neural network using a batch-based pose estimation strategy. Joint recovery of per-batch motion allows to resolve the ambiguities of the monocular reconstruction problem based on a low dimensional trajectory subspace. In addition, we propose refinement of the surface geometry based on fully automatically extracted silhouettes to enable medium-scale non-rigid alignment. We demonstrate state-of-the-art performance capture results that enable exciting applications such as video editing and free viewpoint video, previously infeasible from monocular video. Our qualitative and quantitative evaluation demonstrates that our approach significantly outperforms previous monocular methods in terms of accuracy, robustness and scene complexity that can be handled.



### Learning to segment on tiny datasets: a new shape model
- **Arxiv ID**: http://arxiv.org/abs/1708.02165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02165v1)
- **Published**: 2017-08-07 15:26:11+00:00
- **Updated**: 2017-08-07 15:26:11+00:00
- **Authors**: Maxime Tremblay, André Zaccarin
- **Comment**: 5 pages, 2 figures, ICIP2017
- **Journal**: None
- **Summary**: Current object segmentation algorithms are based on the hypothesis that one has access to a very large amount of data. In this paper, we aim to segment objects using only tiny datasets. To this extent, we propose a new automatic part-based object segmentation algorithm for non-deformable and semi-deformable objects in natural backgrounds. We have developed a novel shape descriptor which models the local boundaries of an object's part. This shape descriptor is used in a bag-of-words approach for object detection. Once the detection process is performed, we use the background and foreground likelihood given by our trained shape model, and the information from the image content, to define a dense CRF model. We use a mean field approximation to solve it and thus segment the object of interest. Performance evaluated on different datasets shows that our approach can sometimes achieve results near state-of-the-art techniques based on big data while requiring only a tiny training set.



### Self-supervised Learning of Pose Embeddings from Spatiotemporal Relations in Videos
- **Arxiv ID**: http://arxiv.org/abs/1708.02179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02179v1)
- **Published**: 2017-08-07 15:57:32+00:00
- **Updated**: 2017-08-07 15:57:32+00:00
- **Authors**: Ömer Sümer, Tobias Dencker, Björn Ommer
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: Human pose analysis is presently dominated by deep convolutional networks trained with extensive manual annotations of joint locations and beyond. To avoid the need for expensive labeling, we exploit spatiotemporal relations in training videos for self-supervised learning of pose embeddings. The key idea is to combine temporal ordering and spatial placement estimation as auxiliary tasks for learning pose similarities in a Siamese convolutional network. Since the self-supervised sampling of both tasks from natural videos can result in ambiguous and incorrect training labels, our method employs a curriculum learning idea that starts training with the most reliable data samples and gradually increases the difficulty. To further refine the training process we mine repetitive poses in individual videos which provide reliable labels while removing inconsistencies. Our pose embeddings capture visual characteristics of human pose that can boost existing supervised representations in human pose estimation and retrieval. We report quantitative and qualitative results on these tasks in Olympic Sports, Leeds Pose Sports and MPII Human Pose datasets.



### Unsupervised Domain Adaptation for Face Recognition in Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/1708.02191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1708.02191v1)
- **Published**: 2017-08-07 16:36:54+00:00
- **Updated**: 2017-08-07 16:36:54+00:00
- **Authors**: Kihyuk Sohn, Sifei Liu, Guangyu Zhong, Xiang Yu, Ming-Hsuan Yang, Manmohan Chandraker
- **Comment**: accepted for publication at International Conference on Computer
  Vision (ICCV) 2017
- **Journal**: None
- **Summary**: Despite rapid advances in face recognition, there remains a clear gap between the performance of still image-based face recognition and video-based face recognition, due to the vast difference in visual quality between the domains and the difficulty of curating diverse large-scale video datasets. This paper addresses both of those challenges, through an image to video feature-level domain adaptation approach, to learn discriminative video frame representations. The framework utilizes large-scale unlabeled video data to reduce the gap between different domains while transferring discriminative knowledge from large-scale labeled still images. Given a face recognition network that is pretrained in the image domain, the adaptation is achieved by (i) distilling knowledge from the network to a video adaptation network through feature matching, (ii) performing feature restoration through synthetic data augmentation and (iii) learning a domain-invariant feature through a domain adversarial discriminator. We further improve performance through a discriminator-guided feature fusion that boosts high-quality frames while eliminating those degraded by video domain-specific factors. Experiments on the YouTube Faces and IJB-A datasets demonstrate that each module contributes to our feature-level domain adaptation framework and substantially improves video face recognition performance to achieve state-of-the-art accuracy. We demonstrate qualitatively that the network learns to suppress diverse artifacts in videos such as pose, illumination or occlusion without being explicitly trained for them.



### MemNet: A Persistent Memory Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1708.02209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02209v1)
- **Published**: 2017-08-07 17:20:58+00:00
- **Updated**: 2017-08-07 17:20:58+00:00
- **Authors**: Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu
- **Comment**: Accepted by ICCV 2017 (Spotlight presentation)
- **Journal**: None
- **Summary**: Recently, very deep convolutional neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the long-term dependency problem is rarely realized for these very deep models, which results in the prior states/layers having little influence on the subsequent ones. Motivated by the fact that human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a gate unit, to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the gate unit, which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i.e., image denosing, super-resolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at https://github.com/tyshiwo/MemNet.



### Training Deep Networks to be Spatially Sensitive
- **Arxiv ID**: http://arxiv.org/abs/1708.02212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02212v1)
- **Published**: 2017-08-07 17:26:13+00:00
- **Updated**: 2017-08-07 17:26:13+00:00
- **Authors**: Nicholas Kolkin, Gregory Shakhnarovich, Eli Shechtman
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: In many computer vision tasks, for example saliency prediction or semantic segmentation, the desired output is a foreground map that predicts pixels where some criteria is satisfied. Despite the inherently spatial nature of this task commonly used learning objectives do not incorporate the spatial relationships between misclassified pixels and the underlying ground truth. The Weighted F-measure, a recently proposed evaluation metric, does reweight errors spatially, and has been shown to closely correlate with human evaluation of quality, and stably rank predictions with respect to noisy ground truths (such as a sloppy human annotator might generate). However it suffers from computational complexity which makes it intractable as an optimization objective for gradient descent, which must be evaluated thousands or millions of times while learning a model's parameters. We propose a differentiable and efficient approximation of this metric. By incorporating spatial information into the objective we can use a simpler model than competing methods without sacrificing accuracy, resulting in faster inference speeds and alleviating the need for pre/post-processing. We match (or improve) performance on several tasks compared to prior state of the art by traditional metrics, and in many cases significantly improve performance by the weighted F-measure.



### Reinforced Video Captioning with Entailment Rewards
- **Arxiv ID**: http://arxiv.org/abs/1708.02300v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.02300v1)
- **Published**: 2017-08-07 20:50:24+00:00
- **Updated**: 2017-08-07 20:50:24+00:00
- **Authors**: Ramakanth Pasunuru, Mohit Bansal
- **Comment**: EMNLP 2017 (9 pages)
- **Journal**: None
- **Summary**: Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.



### GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled Images
- **Arxiv ID**: http://arxiv.org/abs/1708.02313v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1708.02313v1)
- **Published**: 2017-08-07 21:34:59+00:00
- **Updated**: 2017-08-07 21:34:59+00:00
- **Authors**: Avi Singh, Larry Yang, Sergey Levine
- **Comment**: ICCV 2017. Also accepted at ICML 2017 Workshop on Lifelong Learning:
  A Reinforcement Learning Approach. Webpage:
  https://people.eecs.berkeley.edu/~avisingh/iccv17/
- **Journal**: None
- **Summary**: We tackle the problem of learning robotic sensorimotor control policies that can generalize to visually diverse and unseen environments. Achieving broad generalization typically requires large datasets, which are difficult to obtain for task-specific interactive processes such as reinforcement learning or learning from demonstration. However, much of the visual diversity in the world can be captured through passively collected datasets of images or videos. In our method, which we refer to as GPLAC (Generalized Policy Learning with Attentional Classifier), we use both interaction data and weakly labeled image data to augment the generalization capacity of sensorimotor policies. Our method combines multitask learning on action selection and an auxiliary binary classification objective, together with a convolutional neural network architecture that uses an attentional mechanism to avoid distractors. We show that pairing interaction data from just a single environment with a diverse dataset of weakly labeled data results in greatly improved generalization to unseen environments, and show that this generalization depends on both the auxiliary objective and the attentional architecture that we propose. We demonstrate our results in both simulation and on a real robotic manipulator, and demonstrate substantial improvement over standard convolutional architectures and domain adaptation methods.



### Multibiometric Secure System Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.02314v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1708.02314v1)
- **Published**: 2017-08-07 21:35:26+00:00
- **Updated**: 2017-08-07 21:35:26+00:00
- **Authors**: Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi
- **Comment**: To be published in Proc. IEEE Global SIP 2017
- **Journal**: None
- **Summary**: In this paper, we propose a secure multibiometric system that uses deep neural networks and error-correction coding. We present a feature-level fusion framework to generate a secure multibiometric template from each user's multiple biometrics. Two fusion architectures, fully connected architecture and bilinear architecture, are implemented to develop a robust multibiometric shared representation. The shared representation is used to generate a cancelable biometric template that involves the selection of a different set of reliable and discriminative features for each user. This cancelable template is a binary vector and is passed through an appropriate error-correcting decoder to find a closest codeword and this codeword is hashed to generate the final secure template. The efficacy of the proposed approach is shown using a multimodal database where we achieve state-of-the-art matching performance, along with cancelability and security.



### What Makes a Place? Building Bespoke Place Dependent Object Detectors for Robotics
- **Arxiv ID**: http://arxiv.org/abs/1708.02330v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1708.02330v1)
- **Published**: 2017-08-07 23:06:20+00:00
- **Updated**: 2017-08-07 23:06:20+00:00
- **Authors**: Jeffrey Hawke, Alex Bewley, Ingmar Posner
- **Comment**: IROS 2017
- **Journal**: None
- **Summary**: This paper is about enabling robots to improve their perceptual performance through repeated use in their operating environment, creating local expert detectors fitted to the places through which a robot moves. We leverage the concept of 'experiences' in visual perception for robotics, accounting for bias in the data a robot sees by fitting object detector models to a particular place. The key question we seek to answer in this paper is simply: how do we define a place? We build bespoke pedestrian detector models for autonomous driving, highlighting the necessary trade off between generalisation and model capacity as we vary the extent of the place we fit to. We demonstrate a sizeable performance gain over a current state-of-the-art detector when using computationally lightweight bespoke place-fitted detector models.




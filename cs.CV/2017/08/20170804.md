# Arxiv Papers in cs.CV on 2017-08-04
### MemexQA: Visual Memex Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1708.01336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1708.01336v1)
- **Published**: 2017-08-04 00:17:48+00:00
- **Updated**: 2017-08-04 00:17:48+00:00
- **Authors**: Lu Jiang, Junwei Liang, Liangliang Cao, Yannis Kalantidis, Sachin Farfade, Alexander Hauptmann
- **Comment**: https://memexqa.cs.cmu.edu/
- **Journal**: None
- **Summary**: This paper proposes a new task, MemexQA: given a collection of photos or videos from a user, the goal is to automatically answer questions that help users recover their memory about events captured in the collection. Towards solving the task, we 1) present the MemexQA dataset, a large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers, 2) propose MemexNet, a unified, end-to-end trainable network architecture for image, text and video question answering. Experimental results on the MemexQA dataset demonstrate that MemexNet outperforms strong baselines and yields the state-of-the-art on this novel and challenging task. The promising results on TextQA and VideoQA suggest MemexNet's efficacy and scalability across various QA tasks.



### CASSL: Curriculum Accelerated Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.01354v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1708.01354v2)
- **Published**: 2017-08-04 02:13:50+00:00
- **Updated**: 2018-02-12 23:24:52+00:00
- **Authors**: Adithyavairavan Murali, Lerrel Pinto, Dhiraj Gandhi, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Recent self-supervised learning approaches focus on using a few thousand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for high-dimensional control require either scaling up the data collection efforts or using a clever sampling strategy for training. We present a novel approach - Curriculum Accelerated Self-Supervised Learning (CASSL) - to train policies that map visual information to high-level, higher- dimensional action spaces. CASSL orders the sampling of training data based on control dimensions: the learning and sampling are focused on few control parameters before other parameters. The right curriculum for learning is suggested by variance-based global sensitivity analysis of the control space. We apply our CASSL framework to learning how to grasp using an adaptive, underactuated multi-fingered gripper, a challenging system to control. Our experimental results indicate that CASSL provides significant improvement and generalization compared to baseline methods such as staged curriculum learning (8% increase) and complete end-to-end learning with random exploration (14% improvement) tested on a set of novel objects.



### Beyond Low-Rank Representations: Orthogonal Clustering Basis Reconstruction with Optimized Graph Structure for Multi-view Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1708.02288v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.02288v4)
- **Published**: 2017-08-04 03:36:26+00:00
- **Updated**: 2018-03-22 01:08:03+00:00
- **Authors**: Yang Wang, Lin Wu
- **Comment**: Accepted to appear in Neural Networks, Elsevier, on 9th March 2018
- **Journal**: None
- **Summary**: Low-Rank Representation (LRR) is arguably one of the most powerful paradigms for Multi-view spectral clustering, which elegantly encodes the multi-view local graph/manifold structures into an intrinsic low-rank self-expressive data similarity embedded in high-dimensional space, to yield a better graph partition than their single-view counterparts. In this paper we revisit it with a fundamentally different perspective by discovering LRR as essentially a latent clustered orthogonal projection based representation winged with an optimized local graph structure for spectral clustering; each column of the representation is fundamentally a cluster basis orthogonal to others to indicate its members, which intuitively projects the view-specific feature representation to be the one spanned by all orthogonal basis to characterize the cluster structures. Upon this finding, we propose our technique with the followings: (1) We decompose LRR into latent clustered orthogonal representation via low-rank matrix factorization, to encode the more flexible cluster structures than LRR over primal data objects; (2) We convert the problem of LRR into that of simultaneously learning orthogonal clustered representation and optimized local graph structure for each view; (3) The learned orthogonal clustered representations and local graph structures enjoy the same magnitude for multi-view, so that the ideal multi-view consensus can be readily achieved. The experiments over multi-view datasets validate its superiority.



### μ-MAR: Multiplane 3D Marker based Registration for Depth-sensing Cameras
- **Arxiv ID**: http://arxiv.org/abs/1708.01405v1
- **DOI**: 10.1016/j.eswa.2015.08.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01405v1)
- **Published**: 2017-08-04 07:35:22+00:00
- **Updated**: 2017-08-04 07:35:22+00:00
- **Authors**: Marcelo Saval-Calvo, Jorge Azorin-Lopez, Andres Fuster-Guillo, Higinio Mora-Mora
- **Comment**: None
- **Journal**: Expert Systems with Applications, Volume 42, Issue 23, Pages
  9353-9365 (2015)
- **Summary**: Many applications including object reconstruction, robot guidance, and scene mapping require the registration of multiple views from a scene to generate a complete geometric and appearance model of it. In real situations, transformations between views are unknown an it is necessary to apply expert inference to estimate them. In the last few years, the emergence of low-cost depth-sensing cameras has strengthened the research on this topic, motivating a plethora of new applications. Although they have enough resolution and accuracy for many applications, some situations may not be solved with general state-of-the-art registration methods due to the Signal-to-Noise ratio (SNR) and the resolution of the data provided. The problem of working with low SNR data, in general terms, may appear in any 3D system, then it is necessary to propose novel solutions in this aspect. In this paper, we propose a method, {\mu}-MAR, able to both coarse and fine register sets of 3D points provided by low-cost depth-sensing cameras, despite it is not restricted to these sensors, into a common coordinate system. The method is able to overcome the noisy data problem by means of using a model-based solution of multiplane registration. Specifically, it iteratively registers 3D markers composed by multiple planes extracted from points of multiple views of the scene. As the markers and the object of interest are static in the scenario, the transformations obtained for the markers are applied to the object in order to reconstruct it. Experiments have been performed using synthetic and real data. The synthetic data allows a qualitative and quantitative evaluation by means of visual inspection and Hausdorff distance respectively. The real data experiments show the performance of the proposal using data acquired by a Primesense Carmine RGB-D sensor. The method has been compared to several state-of-the-art methods. The ...



### On the Selective and Invariant Representation of DCNN for High-Resolution Remote Sensing Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.01420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01420v1)
- **Published**: 2017-08-04 08:33:12+00:00
- **Updated**: 2017-08-04 08:33:12+00:00
- **Authors**: Jie Chen, Chao Yuan, Min Deng, Chao Tao, Jian Peng, Haifeng Li
- **Comment**: 68 pages, 13 pages
- **Journal**: None
- **Summary**: Human vision possesses strong invariance in image recognition. The cognitive capability of deep convolutional neural network (DCNN) is close to the human visual level because of hierarchical coding directly from raw image. Owing to its superiority in feature representation, DCNN has exhibited remarkable performance in scene recognition of high-resolution remote sensing (HRRS) images and classification of hyper-spectral remote sensing images. In-depth investigation is still essential for understanding why DCNN can accurately identify diverse ground objects via its effective feature representation. Thus, we train the deep neural network called AlexNet on our large scale remote sensing image recognition benchmark. At the neuron level in each convolution layer, we analyze the general properties of DCNN in HRRS image recognition by use of a framework of visual stimulation-characteristic response combined with feature coding-classification decoding. Specifically, we use histogram statistics, representational dissimilarity matrix, and class activation mapping to observe the selective and invariance representations of DCNN in HRRS image recognition. We argue that selective and invariance representations play important roles in remote sensing images tasks, such as classification, detection, and segment. Also selective and invariance representations are significant to design new DCNN liked models for analyzing and understanding remote sensing images.



### Video Salient Object Detection Using Spatiotemporal Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1708.01447v3
- **DOI**: 10.1109/TIP.2018.2849860
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01447v3)
- **Published**: 2017-08-04 11:05:14+00:00
- **Updated**: 2018-06-18 01:49:28+00:00
- **Authors**: Trung-Nghia Le, Akihiro Sugimoto
- **Comment**: accepted at TIP
- **Journal**: None
- **Summary**: This paper presents a method for detecting salient objects in videos where temporal information in addition to spatial information is fully taken into account. Following recent reports on the advantage of deep features over conventional hand-crafted features, we propose a new set of SpatioTemporal Deep (STD) features that utilize local and global contexts over frames. We also propose new SpatioTemporal Conditional Random Field (STCRF) to compute saliency from STD features. STCRF is our extension of CRF to the temporal domain and describes the relationships among neighboring regions both in a frame and over frames. STCRF leads to temporally consistent saliency maps over frames, contributing to the accurate detection of salient objects' boundaries and noise reduction during detection. Our proposed method first segments an input video into multiple scales and then computes a saliency map at each scale level using STD features with STCRF. The final saliency map is computed by fusing saliency maps at different scale levels. Our experiments, using publicly available benchmark datasets, confirm that the proposed method significantly outperforms state-of-the-art methods. We also applied our saliency computation to the video object segmentation task, showing that our method outperforms existing video object segmentation methods.



### Correlation and Class Based Block Formation for Improved Structured Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.01448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01448v2)
- **Published**: 2017-08-04 11:06:42+00:00
- **Updated**: 2017-08-07 03:26:50+00:00
- **Authors**: Nagendra Kumar, Rohit Sinha
- **Comment**: 9 pages, Submitted to IEEE Transactions on Signal Processing
- **Journal**: None
- **Summary**: In recent years, the creation of block-structured dictionary has attracted a lot of interest. Learning such dictionaries involve two step process: block formation and dictionary update. Both these steps are important in producing an effective dictionary. The existing works mostly assume that the block structure is known a priori while learning the dictionary. For finding the unknown block structure given a dictionary commonly sparse agglomerative clustering (SAC) is used. It groups atoms based on their consistency in sparse coding with respect to the unstructured dictionary. This paper explores two innovations towards improving the reconstruction as well as the classification ability achieved with the block-structured dictionary. First, we propose a novel block structuring approach that makes use of the correlation among dictionary atoms. Unlike the SAC approach, which groups diverse atoms, in the proposed approach the blocks are formed by grouping the top most correlated atoms in the dictionary. The proposed block clustering approach is noted to yield significant reductions in redundancy as well as provides a direct control on the block size when compared with the existing SAC-based block structuring. Later, motivated by works using supervised \emph{a priori} known block structure, we also explore the incorporation of class information in the proposed block formation approach to further enhance the classification ability of the block dictionary. For assessment of the reconstruction ability with proposed innovations is done on synthetic data while the classification ability has been evaluated in large variability speaker verification task.



### Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1708.01471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01471v1)
- **Published**: 2017-08-04 12:17:49+00:00
- **Updated**: 2017-08-04 12:17:49+00:00
- **Authors**: Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multi-modal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multi-modal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a co-attention mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-the-art performance on the real-world VQA dataset. Code available at https://github.com/yuzcccc/mfb.



### Hierarchical Metric Learning for Optical Remote Sensing Scene Categorization
- **Arxiv ID**: http://arxiv.org/abs/1708.01494v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01494v3)
- **Published**: 2017-08-04 13:38:46+00:00
- **Updated**: 2018-08-01 18:42:33+00:00
- **Authors**: Akashdeep Goel, Biplab Banerjee, Aleksandra Pizurica
- **Comment**: Undergoing revision in GRSL
- **Journal**: None
- **Summary**: We address the problem of scene classification from optical remote sensing (RS) images based on the paradigm of hierarchical metric learning. Ideally, supervised metric learning strategies learn a projection from a set of training data points so as to minimize intra-class variance while maximizing inter-class separability to the class label space. However, standard metric learning techniques do not incorporate the class interaction information in learning the transformation matrix, which is often considered to be a bottleneck while dealing with fine-grained visual categories. As a remedy, we propose to organize the classes in a hierarchical fashion by exploring their visual similarities and subsequently learn separate distance metric transformations for the classes present at the non-leaf nodes of the tree. We employ an iterative max-margin clustering strategy to obtain the hierarchical organization of the classes. Experiment results obtained on the large-scale NWPU-RESISC45 and the popular UC-Merced datasets demonstrate the efficacy of the proposed hierarchical metric learning based RS scene recognition strategy in comparison to the standard approaches.



### A Latent Variable Model for Two-Dimensional Canonical Correlation Analysis and its Variational Inference
- **Arxiv ID**: http://arxiv.org/abs/1708.01519v1
- **DOI**: 10.1007/s00500-020-04906-8
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.01519v1)
- **Published**: 2017-08-04 14:16:25+00:00
- **Updated**: 2017-08-04 14:16:25+00:00
- **Authors**: Mehran Safayani, Saeid Momenzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Describing the dimension reduction (DR) techniques by means of probabilistic models has recently been given special attention. Probabilistic models, in addition to a better interpretability of the DR methods, provide a framework for further extensions of such algorithms. One of the new approaches to the probabilistic DR methods is to preserving the internal structure of data. It is meant that it is not necessary that the data first be converted from the matrix or tensor format to the vector format in the process of dimensionality reduction. In this paper, a latent variable model for matrix-variate data for canonical correlation analysis (CCA) is proposed. Since in general there is not any analytical maximum likelihood solution for this model, we present two approaches for learning the parameters. The proposed methods are evaluated using the synthetic data in terms of convergence and quality of mappings. Also, real data set is employed for assessing the proposed methods with several probabilistic and none-probabilistic CCA based approaches. The results confirm the superiority of the proposed methods with respect to the competing algorithms. Moreover, this model can be considered as a framework for further extensions.



### Associations among Image Assessments as Cost Functions in Linear Decomposition: MSE, SSIM, and Correlation Coefficient
- **Arxiv ID**: http://arxiv.org/abs/1708.01541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01541v1)
- **Published**: 2017-08-04 15:03:21+00:00
- **Updated**: 2017-08-04 15:03:21+00:00
- **Authors**: Jianji Wang, Nanning Zheng, Badong Chen, Jose C. Principe
- **Comment**: 11 pages, 0 figures
- **Journal**: None
- **Summary**: The traditional methods of image assessment, such as mean squared error (MSE), signal-to-noise ratio (SNR), and Peak signal-to-noise ratio (PSNR), are all based on the absolute error of images. Pearson's inner-product correlation coefficient (PCC) is also usually used to measure the similarity between images. Structural similarity (SSIM) index is another important measurement which has been shown to be more effective in the human vision system (HVS). Although there are many essential differences among these image assessments, some important associations among them as cost functions in linear decomposition are discussed in this paper. Firstly, the selected bases from a basis set for a target vector are the same in the linear decomposition schemes with different cost functions MSE, SSIM, and PCC. Moreover, for a target vector, the ratio of the corresponding affine parameters in the MSE-based linear decomposition scheme and the SSIM-based scheme is a constant, which is just the value of PCC between the target vector and its estimated vector.



### Improving Speaker-Independent Lipreading with Domain-Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1708.01565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1708.01565v1)
- **Published**: 2017-08-04 15:57:38+00:00
- **Updated**: 2017-08-04 15:57:38+00:00
- **Authors**: Michael Wand, Juergen Schmidhuber
- **Comment**: Accepted at Interspeech 2017
- **Journal**: None
- **Summary**: We present a Lipreading system, i.e. a speech recognition system using only visual features, which uses domain-adversarial training for speaker independence. Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM (Long Short-Term Memory) recurrent neural networks, yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker. On pairs of different source and target speakers, we achieve a relative accuracy improvement of around 40% with only 15 to 20 seconds of untranscribed target speech data. On multi-speaker training setups, the accuracy improvements are smaller but still substantial.



### Augmented Reality Meets Computer Vision : Efficient Data Generation for Urban Driving Scenes
- **Arxiv ID**: http://arxiv.org/abs/1708.01566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01566v1)
- **Published**: 2017-08-04 16:03:52+00:00
- **Updated**: 2017-08-04 16:03:52+00:00
- **Authors**: Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas Geiger, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep learning in computer vision is based on availability of large annotated datasets. To lower the need for hand labeled images, virtually rendered 3D worlds have recently gained popularity. Creating realistic 3D content is challenging on its own and requires significant human effort. In this work, we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation and object detection models. Exploiting the fact that not all aspects of the scene are equally important for this task, we propose to augment real-world imagery with virtual objects of the target category. Capturing real-world images at large scale is easy and cheap, and directly provides real background appearances without the need for creating complex 3D models of the environment. We present an efficient procedure to augment real images with virtual objects. This allows us to create realistic composite images which exhibit both realistic background appearance and a large number of complex object arrangements. In contrast to modeling complete 3D environments, our augmentation approach requires only a few user interactions in combination with 3D shapes of the target object. Through extensive experimentation, we conclude the right set of parameters to produce augmented data which can maximally enhance the performance of instance segmentation models. Further, we demonstrate the utility of our approach on training standard deep models for semantic instance segmentation and object detection of cars in outdoor driving scenes. We test the models trained on our augmented data on the KITTI 2015 dataset, which we have annotated with pixel-accurate ground truth, and on Cityscapes dataset. Our experiments demonstrate that models trained on augmented imagery generalize better than those trained on synthetic data or models trained on limited amount of annotated real data.



### Sensing Urban Land-Use Patterns By Integrating Google Tensorflow And Scene-Classification Models
- **Arxiv ID**: http://arxiv.org/abs/1708.01580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01580v1)
- **Published**: 2017-08-04 16:39:22+00:00
- **Updated**: 2017-08-04 16:39:22+00:00
- **Authors**: Yao Yao, Haolin Liang, Xia Li, Jinbao Zhang, Jialv He
- **Comment**: 8 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: With the rapid progress of China's urbanization, research on the automatic detection of land-use patterns in Chinese cities is of substantial importance. Deep learning is an effective method to extract image features. To take advantage of the deep-learning method in detecting urban land-use patterns, we applied a transfer-learning-based remote-sensing image approach to extract and classify features. Using the Google Tensorflow framework, a powerful convolution neural network (CNN) library was created. First, the transferred model was previously trained on ImageNet, one of the largest object-image data sets, to fully develop the model's ability to generate feature vectors of standard remote-sensing land-cover data sets (UC Merced and WHU-SIRI). Then, a random-forest-based classifier was constructed and trained on these generated vectors to classify the actual urban land-use pattern on the scale of traffic analysis zones (TAZs). To avoid the multi-scale effect of remote-sensing imagery, a large random patch (LRP) method was used. The proposed method could efficiently obtain acceptable accuracy (OA = 0.794, Kappa = 0.737) for the study area. In addition, the results show that the proposed method can effectively overcome the multi-scale effect that occurs in urban land-use classification at the irregular land-parcel level. The proposed method can help planners monitor dynamic urban land use and evaluate the impact of urban-planning schemes.



### Region-Based Multiscale Spatiotemporal Saliency for Video
- **Arxiv ID**: http://arxiv.org/abs/1708.01589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01589v1)
- **Published**: 2017-08-04 17:01:41+00:00
- **Updated**: 2017-08-04 17:01:41+00:00
- **Authors**: Trung-Nghia Le, Akihiro Sugimoto
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting salient objects from a video requires exploiting both spatial and temporal knowledge included in the video. We propose a novel region-based multiscale spatiotemporal saliency detection method for videos, where static features and dynamic features computed from the low and middle levels are combined together. Our method utilizes such combined features spatially over each frame and, at the same time, temporally across frames using consistency between consecutive frames. Saliency cues in our method are analyzed through a multiscale segmentation model, and fused across scale levels, yielding to exploring regions efficiently. An adaptive temporal window using motion information is also developed to combine saliency values of consecutive frames in order to keep temporal consistency across frames. Performance evaluation on several popular benchmark datasets validates that our method outperforms existing state-of-the-arts.



### Localizing Moments in Video with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1708.01641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01641v1)
- **Published**: 2017-08-04 18:57:52+00:00
- **Updated**: 2017-08-04 18:57:52+00:00
- **Authors**: Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, Bryan Russell
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.



### Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.01642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01642v1)
- **Published**: 2017-08-04 18:58:03+00:00
- **Updated**: 2017-08-04 18:58:03+00:00
- **Authors**: Debidatta Dwibedi, Ishan Misra, Martial Hebert
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data.



### 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.01648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.01648v1)
- **Published**: 2017-08-04 19:30:13+00:00
- **Updated**: 2017-08-04 19:30:13+00:00
- **Authors**: Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, Derek Hoiem
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: The success of various applications including robotics, digital content creation, and visualization demand a structured and abstract representation of the 3D world from limited sensor data. Inspired by the nature of human perception of 3D shapes as a collection of simple parts, we explore such an abstract shape representation based on primitives. Given a single depth image of an object, we present 3D-PRNN, a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives. Our generative model encodes symmetry characteristics of common man-made objects, preserves long-range structural coherence, and describes objects of varying complexity with a compact representation. We also propose a method based on Gaussian Fields to generate a large scale dataset of primitive-based shape representations to train our network. We evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space.



### Better Together: Joint Reasoning for Non-rigid 3D Reconstruction with Specularities and Shading
- **Arxiv ID**: http://arxiv.org/abs/1708.01654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01654v1)
- **Published**: 2017-08-04 20:15:33+00:00
- **Updated**: 2017-08-04 20:15:33+00:00
- **Authors**: Qi Liu-Yin, Rui Yu, Lourdes Agapito, Andrew Fitzgibbon, Chris Russell
- **Comment**: Submitted to IJCV
- **Journal**: None
- **Summary**: We demonstrate the use of shape-from-shading (SfS) to improve both the quality and the robustness of 3D reconstruction of dynamic objects captured by a single camera. Unlike previous approaches that made use of SfS as a post-processing step, we offer a principled integrated approach that solves dynamic object tracking and reconstruction and SfS as a single unified cost function. Moving beyond Lambertian S f S , we propose a general approach that models both specularities and shading while simultaneously tracking and reconstructing general dynamic objects. Solving these problems jointly prevents the kinds of tracking failures which can not be recovered from by pipeline approaches. We show state-of-the-art results both qualitatively and quantitatively.



### Accelerated Image Reconstruction for Nonlinear Diffractive Imaging
- **Arxiv ID**: http://arxiv.org/abs/1708.01663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01663v2)
- **Published**: 2017-08-04 20:52:44+00:00
- **Updated**: 2017-12-14 15:43:23+00:00
- **Authors**: Yanting Ma, Hassan Mansour, Dehong Liu, Petros T. Boufounos, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of reconstructing an object from the measurements of the light it scatters is common in numerous imaging applications. While the most popular formulations of the problem are based on linearizing the object-light relationship, there is an increased interest in considering nonlinear formulations that can account for multiple light scattering. In this paper, we propose an image reconstruction method, called CISOR, for nonlinear diffractive imaging, based on a nonconvex optimization formulation with total variation (TV) regularization. The nonconvex solver used in CISOR is our new variant of fast iterative shrinkage/thresholding algorithm (FISTA). We provide fast and memory-efficient implementation of the new FISTA variant and prove that it reliably converges for our nonconvex optimization problem. In addition, we systematically compare our method with other state-of-the-art methods on simulated as well as experimentally measured data in both 2D and 3D settings.



### Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry Optimization with Spatially-Varying Lighting
- **Arxiv ID**: http://arxiv.org/abs/1708.01670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01670v1)
- **Published**: 2017-08-04 21:34:46+00:00
- **Updated**: 2017-08-04 21:34:46+00:00
- **Authors**: Robert Maier, Kihwan Kim, Daniel Cremers, Jan Kautz, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automatically-selected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shape-from-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.



### Query-guided Regression Network with Context Policy for Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/1708.01676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01676v1)
- **Published**: 2017-08-04 22:27:08+00:00
- **Updated**: 2017-08-04 22:27:08+00:00
- **Authors**: Kan Chen, Rama Kovvuri, Ram Nevatia
- **Comment**: Spotlight in ICCV 2017
- **Journal**: None
- **Summary**: Given a textual description of an image, phrase grounding localizes objects in the image referred by query phrases in the description. State-of-the-art methods address the problem by ranking a set of proposals based on the relevance to each query, which are limited by the performance of independent proposal generation systems and ignore useful cues from context in the description. In this paper, we adopt a spatial regression method to break the performance limit, and introduce reinforcement learning techniques to further leverage semantic context information. We propose a novel Query-guided Regression network with Context policy (QRC Net) which jointly learns a Proposal Generation Network (PGN), a Query-guided Regression Network (QRN) and a Context Policy Network (CPN). Experiments show QRC Net provides a significant improvement in accuracy on two popular datasets: Flickr30K Entities and Referit Game, with 14.25% and 17.14% increase over the state-of-the-arts respectively.



### Deep Metric Learning with Angular Loss
- **Arxiv ID**: http://arxiv.org/abs/1708.01682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.01682v1)
- **Published**: 2017-08-04 23:17:29+00:00
- **Updated**: 2017-08-04 23:17:29+00:00
- **Authors**: Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, Yuanqing Lin
- **Comment**: International Conference on Computer Vision 2017
- **Journal**: None
- **Summary**: The modern image search system requires semantic understanding of image, and a key yet under-addressed problem is to learn a good metric for measuring the similarity between images. While deep metric learning has yielded impressive performance gains by extracting high level abstractions from image data, a proper objective loss function becomes the central issue to boost the performance. In this paper, we propose a novel angular loss, which takes angle relationship into account, for learning better similarity metric. Whereas previous metric learning methods focus on optimizing the similarity (contrastive loss) or relative similarity (triplet loss) of image pairs, our proposed method aims at constraining the angle at the negative point of triplet triangles. Several favorable properties are observed when compared with conventional methods. First, scale invariance is introduced, improving the robustness of objective against feature variance. Second, a third-order geometric constraint is inherently imposed, capturing additional local structure of triplet triangles than contrastive loss or triplet loss. Third, better convergence has been demonstrated by experiments on three publicly available datasets.




# Arxiv Papers in cs.CV on 2017-08-25
### Hierarchical Multi-scale Attention Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.07590v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07590v2)
- **Published**: 2017-08-25 01:08:10+00:00
- **Updated**: 2017-08-28 05:23:58+00:00
- **Authors**: Shiyang Yan, Jeremy S. Smith, Wenjin Lu, Bailing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent Neural Networks (RNNs) have been widely used in natural language processing and computer vision. Among them, the Hierarchical Multi-scale RNN (HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn the hierarchical temporal structure from data automatically. In this paper, we extend the work to solve the computer vision task of action recognition. However, in sequence-to-sequence models like RNN, it is normally very hard to discover the relationships between inputs and outputs given static inputs. As a solution, attention mechanism could be applied to extract the relevant information from input thus facilitating the modeling of input-output relationships. Based on these considerations, we propose a novel attention network, namely Hierarchical Multi-scale Attention Network (HM-AN), by combining the HM-RNN and the attention mechanism and apply it to action recognition. A newly proposed gradient estimation method for stochastic neurons, namely Gumbel-softmax, is exploited to implement the temporal boundary detectors and the stochastic hard attention mechanism. To amealiate the negative effect of sensitive temperature of the Gumbel-softmax, an adaptive temperature training method is applied to better the system performance. The experimental results demonstrate the improved effect of HM-AN over LSTM with attention on the vision task. Through visualization of what have been learnt by the networks, it can be observed that both the attention regions of images and the hierarchical temporal structure can be captured by HM-AN.



### A wavelet frame coefficient total variational model for image restoration
- **Arxiv ID**: http://arxiv.org/abs/1708.07601v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07601v2)
- **Published**: 2017-08-25 02:22:23+00:00
- **Updated**: 2018-02-10 05:36:20+00:00
- **Authors**: Wei Wang, Xiang-Gen Xia, Shengli Zhang, Chuanjiang He
- **Comment**: 19 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we propose a vector total variation (VTV) of feature image model for image restoration. The VTV imposes different smoothing powers on different features (e.g. edges and cartoons) based on choosing various regularization parameters. Thus, the model can simultaneously preserve edges and remove noises. Next, the existence of solution for the model is proved and the split Bregman algorithm is used to solve the model. At last, we use the wavelet filter banks to explicitly define the feature operator and present some experimental results to show its advantage over the related methods in both quality and efficiency.



### Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.07632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07632v1)
- **Published**: 2017-08-25 07:05:49+00:00
- **Updated**: 2017-08-25 07:05:49+00:00
- **Authors**: Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh
- **Comment**: To appear in ICCV 2017 Workshop (Chalearn)
- **Journal**: None
- **Summary**: Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have an ability to directly extract spatio-temporal features from videos for action recognition. Although the 3D kernels tend to overfit because of a large number of their parameters, the 3D CNNs are greatly improved by using recent huge video databases. However, the architecture of 3D CNNs is relatively shallow against to the success of very deep neural networks in 2D-based CNNs, such as residual networks (ResNets). In this paper, we propose a 3D CNNs based on ResNets toward a better action representation. We describe the training procedure of our 3D ResNets in details. We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the Kinetics did not suffer from overfitting despite the large number of parameters of the model, and achieved better performance than relatively shallow networks, such as C3D. Our code and pretrained models (e.g. Kinetics and ActivityNet) are publicly available at https://github.com/kenshohara/3D-ResNets.



### Understanding and Comparing Deep Neural Networks for Age and Gender Classification
- **Arxiv ID**: http://arxiv.org/abs/1708.07689v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.IR, cs.LG, 68
- **Links**: [PDF](http://arxiv.org/pdf/1708.07689v1)
- **Published**: 2017-08-25 11:08:38+00:00
- **Updated**: 2017-08-25 11:08:38+00:00
- **Authors**: Sebastian Lapuschkin, Alexander Binder, Klaus-Robert Müller, Wojciech Samek
- **Comment**: 8 pages, 5 figures, 5 tables. Presented at ICCV 2017 Workshop: 7th
  IEEE International Workshop on Analysis and Modeling of Faces and Gestures
- **Journal**: None
- **Summary**: Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects.   In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition.



### Linear Differential Constraints for Photo-polarimetric Height Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.07718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1708.07718v1)
- **Published**: 2017-08-25 13:01:05+00:00
- **Updated**: 2017-08-25 13:01:05+00:00
- **Authors**: Silvia Tozza, William A. P. Smith, Dizhong Zhu, Ravi Ramamoorthi, Edwin R. Hancock
- **Comment**: To appear at International Conference on Computer Vision (ICCV),
  Venice, Italy, October 22-29, 2017
- **Journal**: None
- **Summary**: In this paper we present a differential approach to photo-polarimetric shape estimation. We propose several alternative differential constraints based on polarisation and photometric shading information and show how to express them in a unified partial differential system. Our method uses the image ratios technique to combine shading and polarisation information in order to directly reconstruct surface height, without first computing surface normal vectors. Moreover, we are able to remove the non-linearities so that the problem reduces to solving a linear differential problem. We also introduce a new method for estimating a polarisation image from multichannel data and, finally, we show it is possible to estimate the illumination directions in a two source setup, extending the method into an uncalibrated scenario. From a numerical point of view, we use a least-squares formulation of the discrete version of the problem. To the best of our knowledge, this is the first work to consider a unified differential approach to solve photo-polarimetric shape estimation directly for height. Numerical results on synthetic and real-world data confirm the effectiveness of our proposed method.



### Deep Structure for end-to-end inverse rendering
- **Arxiv ID**: http://arxiv.org/abs/1708.08998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.08998v1)
- **Published**: 2017-08-25 14:00:15+00:00
- **Updated**: 2017-08-25 14:00:15+00:00
- **Authors**: Shima Kamyab, Ali Ghodsi, S. Zohreh Azimifar
- **Comment**: None
- **Journal**: None
- **Summary**: Inverse rendering in a 3D format denoted to recovering the 3D properties of a scene given 2D input image(s) and is typically done using 3D Morphable Model (3DMM) based methods from single view images. These models formulate each face as a weighted combination of some basis vectors extracted from the training data. In this paper a deep framework is proposed in which the coefficients and basis vectors are computed by training an autoencoder network and a Convolutional Neural Network (CNN) simultaneously. The idea is to find a common cause which can be mapped to both the 3D structure and corresponding 2D image using deep networks. The empirical results verify the power of deep framework in finding accurate 3D shapes of human faces from their corresponding 2D images on synthetic datasets of human faces.



### Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1708.07747v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1708.07747v2)
- **Published**: 2017-08-25 14:01:29+00:00
- **Updated**: 2017-09-15 21:29:49+00:00
- **Authors**: Han Xiao, Kashif Rasul, Roland Vollgraf
- **Comment**: Dataset is freely available at
  https://github.com/zalandoresearch/fashion-mnist Benchmark is available at
  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/
- **Journal**: None
- **Summary**: We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist



### Evaluation of Deep Learning on an Abstract Image Classification Dataset
- **Arxiv ID**: http://arxiv.org/abs/1708.07770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07770v1)
- **Published**: 2017-08-25 15:10:22+00:00
- **Updated**: 2017-08-25 15:10:22+00:00
- **Authors**: Sebastian Stabinger, Antonio Rodriguez-Sanchez
- **Comment**: Copyright IEEE. To be published in the proceedings of MBCC at
  ICCV2017
- **Journal**: None
- **Summary**: Convolutional Neural Networks have become state of the art methods for image classification over the last couple of years. By now they perform better than human subjects on many of the image classification datasets. Most of these datasets are based on the notion of concrete classes (i.e. images are classified by the type of object in the image). In this paper we present a novel image classification dataset, using abstract classes, which should be easy to solve for humans, but variations of it are challenging for CNNs. The classification performance of popular CNN architectures is evaluated on this dataset and variations of the dataset that might be interesting for further research are identified.



### Integral Curvature Representation and Matching Algorithms for Identification of Dolphins and Whales
- **Arxiv ID**: http://arxiv.org/abs/1708.07785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07785v1)
- **Published**: 2017-08-25 15:44:46+00:00
- **Updated**: 2017-08-25 15:44:46+00:00
- **Authors**: Hendrik J. Weideman, Zachary M. Jablons, Jason Holmberg, Kiirsten Flynn, John Calambokidis, Reny B. Tyson, Jason B. Allen, Randall S. Wells, Krista Hupman, Kim Urian, Charles V. Stewart
- **Comment**: To appear in ICCV 2017 First Workshop on Visual Wildlife Monitoring
- **Journal**: None
- **Summary**: We address the problem of identifying individual cetaceans from images showing the trailing edge of their fins. Given the trailing edge from an unknown individual, we produce a ranking of known individuals from a database. The nicks and notches along the trailing edge define an individual's unique signature. We define a representation based on integral curvature that is robust to changes in viewpoint and pose, and captures the pattern of nicks and notches in a local neighborhood at multiple scales. We explore two ranking methods that use this representation. The first uses a dynamic programming time-warping algorithm to align two representations, and interprets the alignment cost as a measure of similarity. This algorithm also exploits learned spatial weights to downweight matches from regions of unstable curvature. The second interprets the representation as a feature descriptor. Feature keypoints are defined at the local extrema of the representation. Descriptors for the set of known individuals are stored in a tree structure, which allows us to perform queries given the descriptors from an unknown trailing edge. We evaluate the top-k accuracy on two real-world datasets to demonstrate the effectiveness of the curvature representation, achieving top-1 accuracy scores of approximately 95% and 80% for bottlenose dolphins and humpback whales, respectively.



### Shape Registration with Directional Data
- **Arxiv ID**: http://arxiv.org/abs/1708.07791v2
- **DOI**: 10.1016/j.patcog.2018.02.021
- **Categories**: **cs.CV**, I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1708.07791v2)
- **Published**: 2017-08-25 15:57:14+00:00
- **Updated**: 2017-08-29 15:08:51+00:00
- **Authors**: Mairéad Grogan, Rozenn Dahyot
- **Comment**: v2: Updated v1 by adding supplementary material
- **Journal**: Pattern Recognition 79 (2018) 452-466
- **Summary**: We propose several cost functions for registration of shapes encoded with Euclidean and/or non-Euclidean information (unit vectors). Our framework is assessed for estimation of both rigid and non-rigid transformations between the target and model shapes corresponding to 2D contours and 3D surfaces. The experimental results obtained confirm that using the combination of a point's position and unit normal vector in a cost function can enhance the registration results compared to state of the art methods.



### Accelerated Reconstruction of Perfusion-Weighted MRI Enforcing Jointly Local and Nonlocal Spatio-temporal Constraints
- **Arxiv ID**: http://arxiv.org/abs/1708.07808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07808v1)
- **Published**: 2017-08-25 16:52:04+00:00
- **Updated**: 2017-08-25 16:52:04+00:00
- **Authors**: Cagdas Ulas, Christine Preibisch, Jonathan Sperl, Thomas Pyka, Jayashree Kalpathy-Cramer, Bjoern Menze
- **Comment**: Submission to IEEE Transactions on Medical Imaging (August 2017)
- **Journal**: None
- **Summary**: Perfusion-weighted magnetic resonance imaging (MRI) is an imaging technique that allows one to measure tissue perfusion in an organ of interest through the injection of an intravascular paramagnetic contrast agent (CA). Due to a preference for high temporal and spatial resolution in many applications, this modality could significantly benefit from accelerated data acquisitions. In this paper, we specifically address the problem of reconstructing perfusion MR image series from a subset of k-space data. Our proposed approach is motivated by the observation that temporal variations (dynamics) in perfusion imaging often exhibit correlation across different spatial scales. Hence, we propose a model that jointly penalizes the voxel-wise deviations in temporal gradient images obtained based on a baseline, and the patch-wise dissimilarities between the spatio-temporal neighborhoods of entire image sequence. We validate our method on dynamic susceptibility contrast (DSC)-MRI and dynamic contrast-enhanced (DCE)-MRI brain perfusion datasets acquired from 10 tumor patients in total. We provide extensive analysis of reconstruction performance and perfusion parameter estimation in comparison to state-of-the-art reconstruction methods. Experimental results on clinical datasets demonstrate that our reconstruction model can potentially achieve up to 8-fold acceleration by enabling accurate estimation of perfusion parameters while preserving spatial image details and reconstructing the complete perfusion time-intensity curves (TICs).



### Identifying Mirror Symmetry Density with Delay in Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.02684v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.02684v1)
- **Published**: 2017-08-25 17:18:16+00:00
- **Updated**: 2017-08-25 17:18:16+00:00
- **Authors**: Jonathan K. George, Cesare Soci, Volker J. Sorger
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: The ability to rapidly identify symmetry and anti-symmetry is an essential attribute of intelligence. Symmetry perception is a central process in human vision and may be key to human 3D visualization. While previous work in understanding neuron symmetry perception has concentrated on the neuron as an integrator, here we show how the coincidence detecting property of the spiking neuron can be used to reveal symmetry density in spatial data. We develop a method for synchronizing symmetry-identifying spiking artificial neural networks to enable layering and feedback in the network. We show a method for building a network capable of identifying symmetry density between sets of data and present a digital logic implementation demonstrating an 8x8 leaky-integrate-and-fire symmetry detector in a field programmable gate array. Our results show that the efficiencies of spiking neural networks can be harnessed to rapidly identify symmetry in spatial data with applications in image processing, 3D computer vision, and robotics.



### Semantic Foggy Scene Understanding with Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1708.07819v3
- **DOI**: 10.1007/s11263-018-1072-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07819v3)
- **Published**: 2017-08-25 17:36:02+00:00
- **Updated**: 2019-05-17 18:55:04+00:00
- **Authors**: Christos Sakaridis, Dengxin Dai, Luc Van Gool
- **Comment**: Accepted manuscript, 21 pages. The datasets, models and code have
  been made publicly available
- **Journal**: International Journal of Computer Vision, 126(9):973-992, 2018
- **Summary**: This work addresses the problem of semantic foggy scene understanding (SFSU). Although extensive research has been performed on image dehazing and on semantic scene understanding with clear-weather images, little attention has been paid to SFSU. Due to the difficulty of collecting and annotating foggy images, we choose to generate synthetic fog on real images that depict clear-weather outdoor scenes, and then leverage these partially synthetic data for SFSU by employing state-of-the-art convolutional neural networks (CNN). In particular, a complete pipeline to add synthetic fog to real, clear-weather images using incomplete depth information is developed. We apply our fog synthesis on the Cityscapes dataset and generate Foggy Cityscapes with 20550 images. SFSU is tackled in two ways: 1) with typical supervised learning, and 2) with a novel type of semi-supervised learning, which combines 1) with an unsupervised supervision transfer from clear-weather images to their synthetic foggy counterparts. In addition, we carefully study the usefulness of image dehazing for SFSU. For evaluation, we present Foggy Driving, a dataset with 101 real-world images depicting foggy driving scenes, which come with ground truth annotations for semantic segmentation and object detection. Extensive experiments show that 1) supervised learning with our synthetic data significantly improves the performance of state-of-the-art CNN for SFSU on Foggy Driving; 2) our semi-supervised learning strategy further improves performance; and 3) image dehazing marginally advances SFSU with our learning strategy. The datasets, models and code are made publicly available.



### Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms, and Applications
- **Arxiv ID**: http://arxiv.org/abs/1708.07850v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1708.07850v1)
- **Published**: 2017-08-25 18:14:44+00:00
- **Updated**: 2017-08-25 18:14:44+00:00
- **Authors**: Benjamin D. Haeffele, Rene Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convex formulations of low-rank matrix factorization problems have received considerable attention in machine learning. However, such formulations often require solving for a matrix of the size of the data matrix, making it challenging to apply them to large scale datasets. Moreover, in many applications the data can display structures beyond simply being low-rank, e.g., images and videos present complex spatio-temporal structures that are largely ignored by standard low-rank methods. In this paper we study a matrix factorization technique that is suitable for large datasets and captures additional structure in the factors by using a particular form of regularization that includes well-known regularizers such as total variation and the nuclear norm as particular cases. Although the resulting optimization problem is non-convex, we show that if the size of the factors is large enough, under certain conditions, any local minimizer for the factors yields a global minimizer. A few practical algorithms are also provided to solve the matrix factorization problem, and bounds on the distance from a given approximate solution of the optimization problem to the global optimum are derived. Examples in neural calcium imaging video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets.



### The Parallel Algorithm for the 2-D Discrete Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/1708.07853v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07853v3)
- **Published**: 2017-08-25 18:19:08+00:00
- **Updated**: 2017-09-26 07:19:38+00:00
- **Authors**: David Barina, Pavel Najman, Petr Kleparnik, Michal Kula, Pavel Zemcik
- **Comment**: accepted for publication at ICGIP 2017
- **Journal**: None
- **Summary**: The discrete wavelet transform can be found at the heart of many image-processing algorithms. Until now, the transform on general-purpose processors (CPUs) was mostly computed using a separable lifting scheme. As the lifting scheme consists of a small number of operations, it is preferred for processing using single-core CPUs. However, considering a parallel processing using multi-core processors, this scheme is inappropriate due to a large number of steps. On such architectures, the number of steps corresponds to the number of points that represent the exchange of data. Consequently, these points often form a performance bottleneck. Our approach appropriately rearranges calculations inside the transform, and thereby reduces the number of steps. In other words, we propose a new scheme that is friendly to parallel environments. When evaluating on multi-core CPUs, we consistently overcome the original lifting scheme. The evaluation was performed on 61-core Intel Xeon Phi and 8-core Intel Xeon processors.



### Multi-task Self-Supervised Visual Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.07860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07860v1)
- **Published**: 2017-08-25 18:52:17+00:00
- **Updated**: 2017-08-25 18:52:17+00:00
- **Authors**: Carl Doersch, Andrew Zisserman
- **Comment**: Published at ICCV 2017
- **Journal**: None
- **Summary**: We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for "harmonizing" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.



### Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras
- **Arxiv ID**: http://arxiv.org/abs/1708.07878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07878v1)
- **Published**: 2017-08-25 20:50:54+00:00
- **Updated**: 2017-08-25 20:50:54+00:00
- **Authors**: Rui Wang, Martin Schwörer, Daniel Cremers
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for highly accurate real-time visual odometry estimation of large-scale environments from stereo cameras. It jointly optimizes for all the model parameters within the active window, including the intrinsic/extrinsic camera parameters of all keyframes and the depth values of all selected pixels. In particular, we propose a novel approach to integrate constraints from static stereo into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is realized by sampling pixels uniformly from image regions with sufficient intensity gradient. Fixed-baseline stereo resolves scale drift. It also reduces the sensitivities to large optical flow and to rolling shutter effect which are known shortcomings of direct image alignment methods. Quantitative evaluation demonstrates that the proposed Stereo DSO outperforms existing state-of-the-art visual odometry methods both in terms of tracking accuracy and robustness. Moreover, our method delivers a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches while providing a higher reconstruction density than feature-based methods.



### RaspiReader: An Open Source Fingerprint Reader Facilitating Spoof Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.07887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07887v1)
- **Published**: 2017-08-25 21:11:28+00:00
- **Updated**: 2017-08-25 21:11:28+00:00
- **Authors**: Joshua J. Engelsma, Kai Cao, Anil K. Jain
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: We present the design and prototype of an open source, optical fingerprint reader, called RaspiReader, using ubiquitous components. RaspiReader, a low-cost and easy to assemble reader, provides the fingerprint research community a seamless and simple method for gaining more control over the sensing component of fingerprint recognition systems. In particular, we posit that this versatile fingerprint reader will encourage researchers to explore novel spoof detection methods that integrate both hardware and software. RaspiReader's hardware is customized with two cameras for fingerprint acquisition with one camera providing high contrast, frustrated total internal reflection (FTIR) images, and the other camera outputting direct images. Using both of these image streams, we extract complementary information which, when fused together, results in highly discriminative features for fingerprint spoof (presentation attack) detection. Our experimental results demonstrate a marked improvement over previous spoof detection methods which rely only on FTIR images provided by COTS optical readers. Finally, fingerprint matching experiments between images acquired from the FTIR output of the RaspiReader and images acquired from a COTS fingerprint reader verify the interoperability of the RaspiReader with existing COTS optical readers.



### Batch-Based Activity Recognition from Egocentric Photo-Streams
- **Arxiv ID**: http://arxiv.org/abs/1708.07889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.07889v1)
- **Published**: 2017-08-25 21:12:42+00:00
- **Updated**: 2017-08-25 21:12:42+00:00
- **Authors**: Alejandro Cartas, Mariella Dimiccoli, Petia Radeva
- **Comment**: 8 pages, 7 figures, 1 table. To appear at the ICCV 2017 workshop on
  Egocentric Perception, Interaction and Computing
- **Journal**: None
- **Summary**: Activity recognition from long unstructured egocentric photo-streams has several applications in assistive technology such as health monitoring and frailty detection, just to name a few. However, one of its main technical challenges is to deal with the low frame rate of wearable photo-cameras, which causes abrupt appearance changes between consecutive frames. In consequence, important discriminatory low-level features from motion such as optical flow cannot be estimated. In this paper, we present a batch-driven approach for training a deep learning architecture that strongly rely on Long short-term units to tackle this problem. We propose two different implementations of the same approach that process a photo-stream sequence using batches of fixed size with the goal of capturing the temporal evolution of high-level features. The main difference between these implementations is that one explicitly models consecutive batches by overlapping them. Experimental results over a public dataset acquired by three users demonstrate the validity of the proposed architectures to exploit the temporal evolution of convolutional features over time without relying on event boundaries.




# Arxiv Papers in cs.CV on 2017-08-31
### Action Classification and Highlighting in Videos
- **Arxiv ID**: http://arxiv.org/abs/1708.09522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09522v1)
- **Published**: 2017-08-31 01:19:57+00:00
- **Updated**: 2017-08-31 01:19:57+00:00
- **Authors**: Atousa Torabi, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by recent advances in neural machine translation, that jointly align and translate using encoder-decoder networks equipped with attention, we propose an attentionbased LSTM model for human activity recognition. Our model jointly learns to classify actions and highlight frames associated with the action, by attending to salient visual information through a jointly learned soft-attention networks. We explore attention informed by various forms of visual semantic features, including those encoding actions, objects and scenes. We qualitatively show that soft-attention can learn to effectively attend to important objects and scene information correlated with specific human actions. Further, we show that, quantitatively, our attention-based LSTM outperforms the vanilla LSTM and CNN models used by stateof-the-art methods. On a large-scale youtube video dataset, ActivityNet, our model outperforms competing methods in action classification.



### Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork
- **Arxiv ID**: http://arxiv.org/abs/1708.09533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09533v2)
- **Published**: 2017-08-31 02:13:57+00:00
- **Updated**: 2018-08-23 15:10:59+00:00
- **Authors**: Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, Kiyoshi Tanaka
- **Comment**: 16 pages, 11 figures, accepted version at IEEE Transactions on Image
  Processing (T-IP)
- **Journal**: None
- **Summary**: This paper proposes a series of new approaches to improve Generative Adversarial Network (GAN) for conditional image synthesis and we name the proposed model as ArtGAN. One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images on Oxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN



### Video Summarization with Attention-Based Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1708.09545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09545v2)
- **Published**: 2017-08-31 03:04:17+00:00
- **Updated**: 2018-04-16 00:30:42+00:00
- **Authors**: Zhong Ji, Kailin Xiong, Yanwei Pang, Xuelong Li
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: This paper addresses the problem of supervised video summarization by formulating it as a sequence-to-sequence learning problem, where the input is a sequence of original video frames, the output is a keyshot sequence. Our key idea is to learn a deep summarization network with attention mechanism to mimic the way of selecting the keyshots of human. To this end, we propose a novel video summarization framework named Attentive encoder-decoder networks for Video Summarization (AVS), in which the encoder uses a Bidirectional Long Short-Term Memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, two attention-based LSTM networks are explored by using additive and multiplicative objective functions, respectively. Extensive experiments are conducted on three video summarization benchmark datasets, i.e., SumMe, and TVSum. The results demonstrate the superiority of the proposed AVS-based approaches against the state-of-the-art approaches,with remarkable improvements from 0.8% to 3% on two datasets,respectively..



### Fast Landmark Localization with 3D Component Reconstruction and CNN for Cross-Pose Recognition
- **Arxiv ID**: http://arxiv.org/abs/1708.09580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09580v1)
- **Published**: 2017-08-31 06:26:42+00:00
- **Updated**: 2017-08-31 06:26:42+00:00
- **Authors**: Gee-Sern, Hsu, Hung-Cheng Shie, Cheng-Hua Hsieh
- **Comment**: 14 pages, 12 figures, 4 tables
- **Journal**: None
- **Summary**: Two approaches are proposed for cross-pose face recognition, one is based on the 3D reconstruction of facial components and the other is based on the deep Convolutional Neural Network (CNN). Unlike most 3D approaches that consider holistic faces, the proposed approach considers 3D facial components. It segments a 2D gallery face into components, reconstructs the 3D surface for each component, and recognizes a probe face by component features. The segmentation is based on the landmarks located by a hierarchical algorithm that combines the Faster R-CNN for face detection and the Reduced Tree Structured Model for landmark localization. The core part of the CNN-based approach is a revised VGG network. We study the performances with different settings on the training set, including the synthesized data from 3D reconstruction, the real-life data from an in-the-wild database, and both types of data combined. We investigate the performances of the network when it is employed as a classifier or designed as a feature extractor. The two recognition approaches and the fast landmark localization are evaluated in extensive experiments, and compared to stateof-the-art methods to demonstrate their efficacy.



### ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)
- **Arxiv ID**: http://arxiv.org/abs/1708.09585v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09585v3)
- **Published**: 2017-08-31 06:49:06+00:00
- **Updated**: 2018-09-26 16:36:20+00:00
- **Authors**: Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, Xiang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Chinese is the most widely used language in the world. Algorithms that read Chinese text in natural images facilitate applications of various kinds. Despite the large potential value, datasets and competitions in the past primarily focus on English, which bares very different characteristics than Chinese. This report introduces RCTW, a new competition that focuses on Chinese text reading. The competition features a large-scale dataset with 12,263 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams. This report includes dataset description, task definitions, evaluation protocols, and results summaries and analysis. Through this competition, we call for more future research on the Chinese text reading problem. The official website for the competition is http://rctw.vlrlab.net



### ALCN: Meta-Learning for Contrast Normalization Applied to Robust 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1708.09633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09633v1)
- **Published**: 2017-08-31 09:28:23+00:00
- **Updated**: 2017-08-31 09:28:23+00:00
- **Authors**: Mahdi Rad, Peter M. Roth, Vincent Lepetit
- **Comment**: BMVC' 17
- **Journal**: None
- **Summary**: To be robust to illumination changes when detecting objects in images, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is very cumbersome, or sometimes even impossible, for some applications such as 3D pose estimation of specific objects, which is the application we focus on in this paper. We therefore propose a novel illumination normalization method that lets us learn to detect objects and estimate their 3D pose under challenging illumination conditions from very few training samples. Our key insight is that normalization parameters should adapt to the input image. In particular, we realized this via a Convolutional Neural Network trained to predict the parameters of a generalization of the Difference-of-Gaussians method. We show that our method significantly outperforms standard normalization methods and demonstrate it on two challenging 3D detection and pose estimation problems.



### Automatic Semantic Style Transfer using Deep Convolutional Neural Networks and Soft Masks
- **Arxiv ID**: http://arxiv.org/abs/1708.09641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09641v1)
- **Published**: 2017-08-31 09:48:00+00:00
- **Updated**: 2017-08-31 09:48:00+00:00
- **Authors**: Huihuang Zhao, Paul L. Rosin, Yu-Kun Lai
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: This paper presents an automatic image synthesis method to transfer the style of an example image to a content image. When standard neural style transfer approaches are used, the textures and colours in different semantic regions of the style image are often applied inappropriately to the content image, ignoring its semantic layout, and ruining the transfer result. In order to reduce or avoid such effects, we propose a novel method based on automatically segmenting the objects and extracting their soft semantic masks from the style and content images, in order to preserve the structure of the content image while having the style transferred. Each soft mask of the style image represents a specific part of the style image, corresponding to the soft mask of the content image with the same semantics. Both the soft masks and source images are provided as multichannel input to an augmented deep CNN framework for style transfer which incorporates a generative Markov random field (MRF) model. Results on various images show that our method outperforms the most recent techniques.



### Neural Class-Specific Regression for face verification
- **Arxiv ID**: http://arxiv.org/abs/1708.09642v1
- **DOI**: 10.1049/iet-bmt.2017.0081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09642v1)
- **Published**: 2017-08-31 09:48:37+00:00
- **Updated**: 2017-08-31 09:48:37+00:00
- **Authors**: Guanqun Cao, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: 9 pages, 4 figures
- **Journal**: IET Biometrics, vol. 7, no. 1, pp. 63-70, 2018
- **Summary**: Face verification is a problem approached in the literature mainly using nonlinear class-specific subspace learning techniques. While it has been shown that kernel-based Class-Specific Discriminant Analysis is able to provide excellent performance in small- and medium-scale face verification problems, its application in today's large-scale problems is difficult due to its training space and computational requirements. In this paper, generalizing our previous work on kernel-based class-specific discriminant analysis, we show that class-specific subspace learning can be cast as a regression problem. This allows us to derive linear, (reduced) kernel and neural network-based class-specific discriminant analysis methods using efficient batch and/or iterative training schemes, suited for large-scale learning problems. We test the performance of these methods in two datasets describing medium- and large-scale face verification problems.



### Abnormal Event Detection in Videos using Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1708.09644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1708.09644v1)
- **Published**: 2017-08-31 09:56:02+00:00
- **Updated**: 2017-08-31 09:56:02+00:00
- **Authors**: Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, Nicu Sebe
- **Comment**: Best Paper / Student Paper Award Finalist, IEEE International
  Conference on Image Processing (ICIP), 2017
- **Journal**: None
- **Summary**: In this paper we address the abnormality detection problem in crowded scenes. We propose to use Generative Adversarial Nets (GANs), which are trained using normal frames and corresponding optical-flow images in order to learn an internal representation of the scene normality. Since our GANs are trained with only normal data, they are not able to generate abnormal events. At testing time the real data are compared with both the appearance and the motion representations reconstructed by our GANs and abnormal areas are detected by computing local differences. Experimental results on challenging abnormality detection datasets show the superiority of the proposed method compared to the state of the art in both frame-level and pixel-level abnormality detection tasks.



### Generating Video Descriptions with Topic Guidance
- **Arxiv ID**: http://arxiv.org/abs/1708.09666v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1708.09666v2)
- **Published**: 2017-08-31 11:17:53+00:00
- **Updated**: 2017-09-04 11:38:38+00:00
- **Authors**: Shizhe Chen, Jia Chen, Qin Jin
- **Comment**: Appeared at ICMR 2017
- **Journal**: None
- **Summary**: Generating video descriptions in natural language (a.k.a. video captioning) is a more challenging task than image captioning as the videos are intrinsically more complicated than images in two aspects. First, videos cover a broader range of topics, such as news, music, sports and so on. Second, multiple topics could coexist in the same video. In this paper, we propose a novel caption model, topic-guided model (TGM), to generate topic-oriented descriptions for videos in the wild via exploiting topic information. In addition to predefined topics, i.e., category tags crawled from the web, we also mine topics in a data-driven way based on training captions by an unsupervised topic mining model. We show that data-driven topics reflect a better topic schema than the predefined topics. As for testing video topic prediction, we treat the topic mining model as teacher to train the student, the topic prediction model, by utilizing the full multi-modalities in the video especially the speech modality. We propose a series of caption models to exploit topic guidance, including implicitly using the topics as input features to generate words related to the topic and explicitly modifying the weights in the decoder with topics to function as an ensemble of topic-aware language decoders. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the effectiveness of our topic-guided model, which significantly surpasses the winning performance in the 2016 MSR video to language challenge.



### Video Captioning with Guidance of Multimodal Latent Topics
- **Arxiv ID**: http://arxiv.org/abs/1708.09667v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1708.09667v3)
- **Published**: 2017-08-31 11:18:28+00:00
- **Updated**: 2023-02-14 17:11:54+00:00
- **Authors**: Shizhe Chen, Jia Chen, Qin Jin, Alexander Hauptmann
- **Comment**: ACM Multimedia 2017
- **Journal**: None
- **Summary**: The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. The topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model. M&M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves better generalization ability.



### Gravitational Clustering: A Simple, Robust and Adaptive Approach for Distributed Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.02287v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.02287v1)
- **Published**: 2017-08-31 12:05:17+00:00
- **Updated**: 2017-08-31 12:05:17+00:00
- **Authors**: Patricia Binder, Michael Muma, Abdelhak M. Zoubir
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Distributed signal processing for wireless sensor networks enables that different devices cooperate to solve different signal processing tasks. A crucial first step is to answer the question: who observes what? Recently, several distributed algorithms have been proposed, which frame the signal/object labelling problem in terms of cluster analysis after extracting source-specific features, however, the number of clusters is assumed to be known. We propose a new method called Gravitational Clustering (GC) to adaptively estimate the time-varying number of clusters based on a set of feature vectors. The key idea is to exploit the physical principle of gravitational force between mass units: streaming-in feature vectors are considered as mass units of fixed position in the feature space, around which mobile mass units are injected at each time instant. The cluster enumeration exploits the fact that the highest attraction on the mobile mass units is exerted by regions with a high density of feature vectors, i.e., gravitational clusters. By sharing estimates among neighboring nodes via a diffusion-adaptation scheme, cooperative and distributed cluster enumeration is achieved. Numerical experiments concerning robustness against outliers, convergence and computational complexity are conducted. The application in a distributed cooperative multi-view camera network illustrates the applicability to real-world problems.



### Boosting with Lexicographic Programming: Addressing Class Imbalance without Cost Tuning
- **Arxiv ID**: http://arxiv.org/abs/1708.09684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09684v2)
- **Published**: 2017-08-31 12:41:29+00:00
- **Updated**: 2019-02-04 01:08:07+00:00
- **Authors**: Shounak Datta, Sayak Nag, Swagatam Das
- **Comment**: This work has been submitted to the IEEE for publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: A large amount of research effort has been dedicated to adapting boosting for imbalanced classification. However, boosting methods are yet to be satisfactorily immune to class imbalance, especially for multi-class problems. This is because most of the existing solutions for handling class imbalance rely on expensive cost set tuning for determining the proper level of compensation. We show that the assignment of weights to the component classifiers of a boosted ensemble can be thought of as a game of Tug of War between the classes in the margin space. We then demonstrate how this insight can be used to attain a good compromise between the rare and abundant classes without having to resort to cost set tuning, which has long been the norm for imbalanced classification. The solution is based on a lexicographic linear programming framework which requires two stages. Initially, class-specific component weight combinations are found so as to minimize a hinge loss individually for each of the classes. Subsequently, the final component weights are assigned so that the maximum deviation from the class-specific minimum loss values (obtained in the previous stage) is minimized. Hence, the proposal is not only restricted to two-class situations, but is also readily applicable to multi-class problems. Additionally,we also derive the dual formulation corresponding to the proposed framework. Experiments conducted on artificial and real-world imbalanced datasets as well as on challenging applications such as hyperspectral image classification and ImageNet classification establish the efficacy of the proposal.



### Fine-grained Visual-textual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.00340v4
- **DOI**: 10.1109/TCSVT.2019.2892802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00340v4)
- **Published**: 2017-08-31 12:41:55+00:00
- **Updated**: 2019-02-20 13:37:21+00:00
- **Authors**: Xiangteng He, Yuxin Peng
- **Comment**: 12 pages, accepted by IEEE Transactions on Circuits and Systems for
  Video Technology (TCSVT)
- **Journal**: None
- **Summary**: Fine-grained visual categorization is to recognize hundreds of subcategories belonging to the same basic-level category, which is a highly challenging task due to the quite subtle and local visual distinctions among similar subcategories. Most existing methods generally learn part detectors to discover discriminative regions for better categorization performance. However, not all parts are beneficial and indispensable for visual categorization, and the setting of part detector number heavily relies on prior knowledge as well as experimental validation. As is known to all, when we describe the object of an image via textual descriptions, we mainly focus on the pivotal characteristics, and rarely pay attention to common characteristics as well as the background areas. This is an involuntary transfer from human visual attention to textual attention, which leads to the fact that textual attention tells us how many and which parts are discriminative and significant to categorization. So textual attention could help us to discover visual attention in image. Inspired by this, we propose a fine-grained visual-textual representation learning (VTRL) approach, and its main contributions are: (1) Fine-grained visual-textual pattern mining devotes to discovering discriminative visual-textual pairwise information for boosting categorization performance through jointly modeling vision and text with generative adversarial networks (GANs), which automatically and adaptively discovers discriminative parts. (2) Visual-textual representation learning jointly combines visual and textual information, which preserves the intra-modality and inter-modality information to generate complementary fine-grained representation, as well as further improves categorization performance.



### Quantifying Facial Age by Posterior of Age Comparisons
- **Arxiv ID**: http://arxiv.org/abs/1708.09687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09687v2)
- **Published**: 2017-08-31 12:49:05+00:00
- **Updated**: 2017-10-13 00:50:36+00:00
- **Authors**: Yunxuan Zhang, Li Liu, Cheng Li, Chen change Loy
- **Comment**: To appear on BMVC 2017 (oral) revised version
- **Journal**: None
- **Summary**: We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person's actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed `MegaAge', which consists of 41,941 images. Data can be downloaded from our project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and github.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-the-art results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.



### Inferring Human Activities Using Robust Privileged Probabilistic Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.09825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09825v1)
- **Published**: 2017-08-31 17:19:56+00:00
- **Updated**: 2017-08-31 17:19:56+00:00
- **Authors**: Michalis Vrigkas, Evangelos Kazakos, Christophoros Nikou, Ioannis A. Kakadiaris
- **Comment**: To appear in ICCV Workshops 2017 (TASK-CV)
- **Journal**: None
- **Summary**: Classification models may often suffer from "structure imbalance" between training and testing data that may occur due to the deficient data collection process. This imbalance can be represented by the learning using privileged information (LUPI) paradigm. In this paper, we present a supervised probabilistic classification approach that integrates LUPI into a hidden conditional random field (HCRF) model. The proposed model is called LUPI-HCRF and is able to cope with additional information that is only available during training. Moreover, the proposed method employes Student's t-distribution to provide robustness to outliers by modeling the conditional distribution of the privileged information. Experimental results in three publicly available datasets demonstrate the effectiveness of the proposed approach and improve the state-of-the-art in the LUPI framework for recognizing human activities.



### Model based learning for accelerated, limited-view 3D photoacoustic tomography
- **Arxiv ID**: http://arxiv.org/abs/1708.09832v3
- **DOI**: 10.1109/TMI.2018.2820382
- **Categories**: **cs.CV**, cs.NE, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1708.09832v3)
- **Published**: 2017-08-31 17:32:29+00:00
- **Updated**: 2018-03-26 14:12:38+00:00
- **Authors**: Andreas Hauptmann, Felix Lucka, Marta Betcke, Nam Huynh, Jonas Adler, Ben Cox, Paul Beard, Sebastien Ourselin, Simon Arridge
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning for tomographic reconstructions have shown great potential to create accurate and high quality images with a considerable speed-up. In this work we present a deep neural network that is specifically designed to provide high resolution 3D images from restricted photoacoustic measurements. The network is designed to represent an iterative scheme and incorporates gradient information of the data fit to compensate for limited view artefacts. Due to the high complexity of the photoacoustic forward operator, we separate training and computation of the gradient information. A suitable prior for the desired image structures is learned as part of the training. The resulting network is trained and tested on a set of segmented vessels from lung CT scans and then applied to in-vivo photoacoustic measurement data.



### 3D Visual Perception for Self-Driving Cars using a Multi-Camera System: Calibration, Mapping, Localization, and Obstacle Detection
- **Arxiv ID**: http://arxiv.org/abs/1708.09839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09839v1)
- **Published**: 2017-08-31 17:45:42+00:00
- **Updated**: 2017-08-31 17:45:42+00:00
- **Authors**: Christian HÃ¤ne, Lionel Heng, Gim Hee Lee, Friedrich Fraundorfer, Paul Furgale, Torsten Sattler, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: Cameras are a crucial exteroceptive sensor for self-driving cars as they are low-cost and small, provide appearance information about the environment, and work in various weather conditions. They can be used for multiple purposes such as visual navigation and obstacle detection. We can use a surround multi-camera system to cover the full 360-degree field-of-view around the car. In this way, we avoid blind spots which can otherwise lead to accidents. To minimize the number of cameras needed for surround perception, we utilize fisheye cameras. Consequently, standard vision pipelines for 3D mapping, visual localization, obstacle detection, etc. need to be adapted to take full advantage of the availability of multiple cameras rather than treat each camera individually. In addition, processing of fisheye images has to be supported. In this paper, we describe the camera calibration and subsequent processing pipeline for multi-fisheye-camera systems developed as part of the V-Charge project. This project seeks to enable automated valet parking for self-driving cars. Our pipeline is able to precisely calibrate multi-camera systems, build sparse 3D maps for visual navigation, visually localize the car with respect to these maps, generate accurate dense maps, as well as detect obstacles based on real-time depth map extraction.



### Predicting Cardiovascular Risk Factors from Retinal Fundus Photographs using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1708.09843v2
- **DOI**: 10.1038/s41551-018-0195-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1708.09843v2)
- **Published**: 2017-08-31 17:49:15+00:00
- **Updated**: 2017-09-21 22:48:06+00:00
- **Authors**: Ryan Poplin, Avinash V. Varadarajan, Katy Blumer, Yun Liu, Michael V. McConnell, Greg S. Corrado, Lily Peng, Dale R. Webster
- **Comment**: None
- **Journal**: Nature Biomedical Engineering (2018)
- **Summary**: Traditionally, medical discoveries are made by observing associations and then designing experiments to test these hypotheses. However, observing and quantifying associations in images can be difficult because of the wide variety of features, patterns, colors, values, shapes in real data. In this paper, we use deep learning, a machine learning technique that learns its own features, to discover new knowledge from retinal fundus images. Using models trained on data from 284,335 patients, and validated on two independent datasets of 12,026 and 999 patients, we predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as such as age (within 3.26 years), gender (0.97 AUC), smoking status (0.71 AUC), HbA1c (within 1.39%), systolic blood pressure (within 11.23mmHg) as well as major adverse cardiac events (0.70 AUC). We further show that our models used distinct aspects of the anatomy to generate each prediction, such as the optic disc or blood vessels, opening avenues of further research.



### EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.00029v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.00029v2)
- **Published**: 2017-08-31 18:19:10+00:00
- **Updated**: 2019-02-01 09:51:18+00:00
- **Authors**: Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the challenge of land use and land cover classification using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible provided in the Earth observation program Copernicus. We present a novel dataset based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images. We provide benchmarks for this novel dataset with its spectral bands using state-of-the-art deep Convolutional Neural Network (CNNs). With the proposed novel dataset, we achieved an overall classification accuracy of 98.57%. The resulting classification system opens a gate towards a number of Earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.



### Multi-task Dictionary Learning based Convolutional Neural Network for Computer aided Diagnosis with Longitudinal Images
- **Arxiv ID**: http://arxiv.org/abs/1709.00042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00042v1)
- **Published**: 2017-08-31 18:58:59+00:00
- **Updated**: 2017-08-31 18:58:59+00:00
- **Authors**: Jie Zhang, Qingyang Li, Richard J. Caselli, Jieping Ye, Yalin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases on longitudinal data has drawn great interest from computer vision researchers. The current state-of-the-art models for many image classification tasks are based on the Convolutional Neural Networks (CNN). However, a key challenge in applying CNN to biological problems is that the available labeled training samples are very limited. Another issue for CNN to be applied in computer aided diagnosis applications is that to achieve better diagnosis and prognosis accuracy, one usually has to deal with the longitudinal dataset, i.e., the dataset of images scanned at different time points. Here we argue that an enhanced CNN model with transfer learning for the joint analysis of tasks from multiple time points or regions of interests may have a potential to improve the accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN based deep learning multi-task dictionary learning framework to address the above challenges. Firstly, we pre-train CNN on the ImageNet dataset and transfer the knowledge from the pre-trained model to the medical imaging progression representation, generating the features for different tasks. Then, we propose a novel unsupervised learning method, termed Multi-task Stochastic Coordinate Coding (MSCC), for learning different tasks by using shared and individual dictionaries and generating the sparse features required to predict the future cognitive clinical scores. We apply our new model in a publicly available neuroimaging cohort to predict clinical measures with two different feature sets and compare them with seven other state-of-the-art methods. The experimental results show our proposed method achieved superior results.



### Learning Inference Models for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1709.00069v1
- **DOI**: 10.15496/publikation-17888
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00069v1)
- **Published**: 2017-08-31 20:33:06+00:00
- **Updated**: 2017-08-31 20:33:06+00:00
- **Authors**: Varun Jampani
- **Comment**: PhD Dissertation
- **Journal**: None
- **Summary**: Computer vision can be understood as the ability to perform inference on image data. Breakthroughs in computer vision technology are often marked by advances in inference techniques. This thesis proposes novel inference schemes and demonstrates applications in computer vision. We propose inference techniques for both generative and discriminative vision models. The use of generative models in vision is often hampered by the difficulty of posterior inference. We propose techniques for improving inference in MCMC sampling and message-passing inference. Our inference strategy is to learn separate discriminative models that assist Bayesian inference in a generative model. Experiments on a range of generative models show that the proposed techniques accelerate the inference process and/or converge to better solutions. A main complication in the design of discriminative models is the inclusion of prior knowledge. We concentrate on CNN models and propose a generalization of standard spatial convolutions to bilateral convolutions. We generalize the existing use of bilateral filters and then propose new neural network architectures with learnable bilateral filters, which we call `Bilateral Neural Networks'. Experiments demonstrate the use of the bilateral networks on a wide range of image and video tasks and datasets. In summary, we propose techniques for better inference in several vision models ranging from inverse graphics to freely parameterized neural networks. In generative models, our inference techniques alleviate some of the crucial hurdles in Bayesian posterior inference, paving new ways for the use of model based machine learning in vision. In discriminative CNN models, the proposed filter generalizations aid in the design of new neural network architectures that can handle sparse high-dimensional data as well as provide a way to incorporate prior knowledge into CNNs.



### Exact Blur Measure Outperforms Conventional Learned Features for Depth Finding
- **Arxiv ID**: http://arxiv.org/abs/1709.00072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.00072v1)
- **Published**: 2017-08-31 20:41:19+00:00
- **Updated**: 2017-08-31 20:41:19+00:00
- **Authors**: Akbar Saadat
- **Comment**: 5 pages, Submitted to SSPD 2017
- **Journal**: None
- **Summary**: Image analysis methods that are based on exact blur values are faced with the computational complexities due to blur measurement error. This atmosphere encourages scholars to look for handcrafted and learned features for finding depth from a single image. This paper introduces a novel exact realization for blur measures on digital images and implements it on a new measure of defocus Gaussian blur at edge points in Depth From Defocus (DFD) methods with the potential to change this atmosphere. The experiments on real images indicate superiority of the proposed measure in error performance over conventional learned features in the state-of the-art single image based depth estimation methods.



### First and Second Order Methods for Online Convolutional Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.00106v3
- **DOI**: 10.1137/17M1145689
- **Categories**: **cs.LG**, cs.CV, eess.IV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.00106v3)
- **Published**: 2017-08-31 23:19:02+00:00
- **Updated**: 2018-06-16 19:21:10+00:00
- **Authors**: Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, Wotao Yin
- **Comment**: None
- **Journal**: SIAM J. Imaging Sci., 11(2), 1589-1628, 2018
- **Summary**: Convolutional sparse representations are a form of sparse representation with a structured, translation invariant dictionary. Most convolutional dictionary learning algorithms to date operate in batch mode, requiring simultaneous access to all training images during the learning process, which results in very high memory usage and severely limits the training data that can be used. Very recently, however, a number of authors have considered the design of online convolutional dictionary learning algorithms that offer far better scaling of memory and computational cost with training set size than batch methods. This paper extends our prior work, improving a number of aspects of our previous algorithm; proposing an entirely new one, with better performance, and that supports the inclusion of a spatial mask for learning from incomplete data; and providing a rigorous theoretical analysis of these methods.




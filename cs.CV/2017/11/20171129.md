# Arxiv Papers in cs.CV on 2017-11-29
### PSIque: Next Sequence Prediction of Satellite Images using a Convolutional Sequence-to-Sequence Network
- **Arxiv ID**: http://arxiv.org/abs/1711.10644v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.10644v2)
- **Published**: 2017-11-29 02:02:13+00:00
- **Updated**: 2017-11-30 21:25:33+00:00
- **Authors**: Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song
- **Comment**: Workshop on Deep Learning for Physical Sciences (DLPS 2017), NIPS
  2017, Long Beach, CA, USA
- **Journal**: None
- **Summary**: Predicting unseen weather phenomena is an important issue for disaster management. In this paper, we suggest a model for a convolutional sequence-to-sequence autoencoder for predicting undiscovered weather situations from previous satellite images. We also propose a symmetric skip connection between encoder and decoder modules to produce more comprehensive image predictions. To examine our model performance, we conducted experiments for each suggested model to predict future satellite images from historical satellite images. A specific combination of skip connection and sequence-to-sequence autoencoder was able to generate closest prediction from the ground truth image.



### Deep-Person: Learning Discriminative Deep Features for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1711.10658v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10658v4)
- **Published**: 2017-11-29 03:15:07+00:00
- **Updated**: 2019-09-09 03:34:10+00:00
- **Authors**: Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, Yongchao Xu
- **Comment**: Accepted to Pattern Recognition. The code is released:
  https://github.com/zydou/Deep-Person
- **Journal**: None
- **Summary**: Recently, many methods of person re-identification (Re-ID) rely on part-based feature representation to learn a discriminative pedestrian descriptor. However, the spatial context between these parts is ignored for the independent extractor to each separate part. In this paper, we propose to apply Long Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as a sequence of body parts from head to foot. Integrating the contextual information strengthens the discriminative ability of local representation. We also leverage the complementary information between local and global feature. Furthermore, we integrate both identification task and ranking task in one network, where a discriminative embedding and a similarity measurement are learned concurrently. This results in a novel three-branch framework named Deep-Person, which learns highly discriminative features for person Re-ID. Experimental results demonstrate that Deep-Person outperforms the state-of-the-art methods by a large margin on three challenging datasets including Market-1501, CUHK03, and DukeMTMC-reID. Specifically, combining with a re-ranking approach, we achieve a 90.84% mAP on Market-1501 under single query setting.



### An Adaptive Fuzzy-Based System to Simulate, Quantify and Compensate Color Blindness
- **Arxiv ID**: http://arxiv.org/abs/1711.10662v1
- **DOI**: 10.3233/ICA-2011-0356
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10662v1)
- **Published**: 2017-11-29 03:31:14+00:00
- **Updated**: 2017-11-29 03:31:14+00:00
- **Authors**: Jinmi Lee, Wellington Pinheiro dos Santos
- **Comment**: None
- **Journal**: Integrated Computer-Aided Engineering, v. 18, p. 29-40, 2011
- **Summary**: About 8% of the male population of the world are affected by a determined type of color vision disturbance, which varies from the partial to complete reduction of the ability to distinguish certain colors. A considerable amount of color blind people are able to live all life long without knowing they have color vision disabilities and abnormalities. Nowadays the evolution of information technology and computer science, specifically image processing techniques and computer graphics, can be fundamental to aid at the development of adaptive color blindness correction tools. This paper presents a software tool based on Fuzzy Logic to evaluate the type and the degree of color blindness a person suffer from. In order to model several degrees of color blindness, herein this work we modified the classical linear transform-based simulation method by the use of fuzzy parameters. We also proposed four new methods to correct color blindness based on a fuzzy approach: Methods A and B, with and without histogram equalization. All the methods are based on combinations of linear transforms and histogram operations. In order to evaluate the results we implemented a web-based survey to get the best results according to optimize to distinguish different elements in an image. Results obtained from 40 volunteers proved that the Method B with histogram equalization got the best results for about 47% of volunteers.



### Image2Mesh: A Learning Framework for Single Image 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1711.10669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10669v1)
- **Published**: 2017-11-29 03:57:32+00:00
- **Updated**: 2017-11-29 03:57:32+00:00
- **Authors**: Jhony K. Pontes, Chen Kong, Sridha Sridharan, Simon Lucey, Anders Eriksson, Clinton Fookes
- **Comment**: 9 pages, 4 figures
- **Journal**: Asian Conference on Computer Vision (ACCV) 2018
- **Summary**: One challenge that remains open in 3D deep learning is how to efficiently represent 3D data to feed deep networks. Recent works have relied on volumetric or point cloud representations, but such approaches suffer from a number of issues such as computational complexity, unordered data, and lack of finer geometry. This paper demonstrates that a mesh representation (i.e. vertices and faces to form polygonal surfaces) is able to capture fine-grained geometry for 3D reconstruction tasks. A mesh however is also unstructured data similar to point clouds. We address this problem by proposing a learning framework to infer the parameters of a compact mesh representation rather than learning from the mesh itself. This compact representation encodes a mesh using free-form deformation and a sparse linear combination of models allowing us to reconstruct 3D meshes from single images. In contrast to prior work, we do not rely on silhouettes and landmarks to perform 3D reconstruction. We evaluate our method on synthetic and real-world datasets with very promising results. Our framework efficiently reconstructs 3D objects in a low-dimensional way while preserving its important geometrical aspects.



### AttGAN: Facial Attribute Editing by Only Changing What You Want
- **Arxiv ID**: http://arxiv.org/abs/1711.10678v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10678v3)
- **Published**: 2017-11-29 04:50:31+00:00
- **Updated**: 2018-07-25 10:08:00+00:00
- **Authors**: Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, Xilin Chen
- **Comment**: Submitted to IEEE Transactions on Image Processing, Code:
  https://github.com/LynnHo/AttGAN-Tensorflow
- **Journal**: None
- **Summary**: Facial attribute editing aims to manipulate single or multiple attributes of a face image, i.e., to generate a new face with desired attributes while preserving other details. Recently, generative adversarial net (GAN) and encoder-decoder architecture are usually incorporated to handle this task with promising results. Based on the encoder-decoder architecture, facial attribute editing is achieved by decoding the latent representation of the given face conditioned on the desired attributes. Some existing methods attempt to establish an attribute-independent latent representation for further attribute editing. However, such attribute-independent constraint on the latent representation is excessive because it restricts the capacity of the latent representation and may result in information loss, leading to over-smooth and distorted generation. Instead of imposing constraints on the latent representation, in this work we apply an attribute classification constraint to the generated image to just guarantee the correct change of desired attributes, i.e., to "change what you want". Meanwhile, the reconstruction learning is introduced to preserve attribute-excluding details, in other words, to "only change what you want". Besides, the adversarial learning is employed for visually realistic editing. These three components cooperate with each other forming an effective framework for high quality facial attribute editing, referred as AttGAN. Furthermore, our method is also directly applicable for attribute intensity control and can be naturally extended for attribute style manipulation. Experiments on CelebA dataset show that our method outperforms the state-of-the-arts on realistic attribute editing with facial details well preserved.



### Patch Correspondences for Interpreting Pixel-level CNNs
- **Arxiv ID**: http://arxiv.org/abs/1711.10683v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10683v4)
- **Published**: 2017-11-29 05:13:32+00:00
- **Updated**: 2018-09-04 01:35:17+00:00
- **Authors**: Victor Fragoso, Chunhui Liu, Aayush Bansal, Deva Ramanan
- **Comment**: None
- **Journal**: None
- **Summary**: We present compositional nearest neighbors (CompNN), a simple approach to visually interpreting distributed representations learned by a convolutional neural network (CNN) for pixel-level tasks (e.g., image synthesis and segmentation). It does so by reconstructing both a CNN's input and output image by copy-pasting corresponding patches from the training set with similar feature embeddings. To do so efficiently, it makes of a patch-match-based algorithm that exploits the fact that the patch representations learned by a CNN for pixel level tasks vary smoothly. Finally, we show that CompNN can be used to establish semantic correspondences between two images and control properties of the output image by modifying the images contained in the training set. We present qualitative and quantitative experiments for semantic segmentation and image-to-image translation that demonstrate that CompNN is a good tool for interpreting the embeddings learned by pixel-level CNNs.



### Road Extraction by Deep Residual U-Net
- **Arxiv ID**: http://arxiv.org/abs/1711.10684v1
- **DOI**: 10.1109/LGRS.2018.2802944
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10684v1)
- **Published**: 2017-11-29 05:16:14+00:00
- **Updated**: 2017-11-29 05:16:14+00:00
- **Authors**: Zhengxin Zhang, Qingjie Liu, Yunhong Wang
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network which combines the strengths of residual learning and U-Net is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model is two-fold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters however better performance. We test our network on a public road dataset and compare it with U-Net and other two state of the art deep learning based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts.



### Facial Dynamics Interpreter Network: What are the Important Relations between Local Dynamics for Facial Trait Estimation?
- **Arxiv ID**: http://arxiv.org/abs/1711.10688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10688v2)
- **Published**: 2017-11-29 05:44:18+00:00
- **Updated**: 2018-07-24 05:25:40+00:00
- **Authors**: Seong Tae Kim, Yong Man Ro
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: Human face analysis is an important task in computer vision. According to cognitive-psychological studies, facial dynamics could provide crucial cues for face analysis. The motion of a facial local region in facial expression is related to the motion of other facial local regions. In this paper, a novel deep learning approach, named facial dynamics interpreter network, has been proposed to interpret the important relations between local dynamics for estimating facial traits from expression sequence. The facial dynamics interpreter network is designed to be able to encode a relational importance, which is used for interpreting the relation between facial local dynamics and estimating facial traits. By comparative experiments, the effectiveness of the proposed method has been verified. The important relations between facial local dynamics are investigated by the proposed facial dynamics interpreter network in gender classification and age estimation. Moreover, experimental results show that the proposed method outperforms the state-of-the-art methods in gender classification and age estimation.



### Small Drone Field Experiment: Data Collection & Processing
- **Arxiv ID**: http://arxiv.org/abs/1711.10693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10693v1)
- **Published**: 2017-11-29 06:08:16+00:00
- **Updated**: 2017-11-29 06:08:16+00:00
- **Authors**: Dalton Rosario, Christoph Borel, Damon Conover, Ryan McAlinden, Anthony Ortiz, Sarah Shiver, Blair Simon
- **Comment**: None
- **Journal**: None
- **Summary**: Following an initiative formalized in April 2016 formally known as ARL West between the U.S. Army Research Laboratory (ARL) and University of Southern California's Institute for Creative Technologies (USC ICT), a field experiment was coordinated and executed in the summer of 2016 by ARL, USC ICT, and Headwall Photonics. The purpose was to image part of the USC main campus in Los Angeles, USA, using two portable COTS (commercial off the shelf) aerial drone solutions for data acquisition, for photogrammetry (3D reconstruction from images), and fusion of hyperspectral data with the recovered set of 3D point clouds representing the target area. The research aims for determining the viability of having a machine capable of segmenting the target area into key material classes (e.g., manmade structures, live vegetation, water) for use in multiple purposes, to include providing the user with a more accurate scene understanding and enabling the unsupervised automatic sampling of meaningful material classes from the target area for adaptive semi-supervised machine learning. In the latter, a target set library may be used for automatic machine training with data of local material classes, as an example, to increase the prediction chances of machines recognizing targets. The field experiment and associated data post processing approach to correct for reflectance, geo-rectify, recover the area's dense point clouds from images, register spectral with elevation properties of scene surfaces from the independently collected datasets, and generate the desired scene segmented maps are discussed. Lessons learned from the experience are also highlighted throughout the paper.



### BLADE: Filter Learning for General Purpose Computational Photography
- **Arxiv ID**: http://arxiv.org/abs/1711.10700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10700v2)
- **Published**: 2017-11-29 06:38:41+00:00
- **Updated**: 2017-12-07 23:26:05+00:00
- **Authors**: Pascal Getreuer, Ignacio Garcia-Dorado, John Isidoro, Sungjoon Choi, Frank Ong, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: The Rapid and Accurate Image Super Resolution (RAISR) method of Romano, Isidoro, and Milanfar is a computationally efficient image upscaling method using a trained set of filters. We describe a generalization of RAISR, which we name Best Linear Adaptive Enhancement (BLADE). This approach is a trainable edge-adaptive filtering framework that is general, simple, computationally efficient, and useful for a wide range of problems in computational photography. We show applications to operations which may appear in a camera pipeline including denoising, demosaicing, and stylization.



### FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors
- **Arxiv ID**: http://arxiv.org/abs/1711.10703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10703v1)
- **Published**: 2017-11-29 06:47:04+00:00
- **Updated**: 2017-11-29 06:47:04+00:00
- **Authors**: Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang
- **Comment**: Chen and Tai contributed equally to this paper
- **Journal**: None
- **Summary**: Face Super-Resolution (SR) is a domain-specific super-resolution problem. The specific facial prior knowledge could be leveraged for better super-resolving face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes full use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To further generate realistic faces, we propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Moreover, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive benchmark experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively. Code will be made available upon publication.



### Deep Eyes: Binocular Depth-from-Focus on Focal Stack Pairs
- **Arxiv ID**: http://arxiv.org/abs/1711.10729v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10729v4)
- **Published**: 2017-11-29 08:52:17+00:00
- **Updated**: 2020-08-10 16:03:37+00:00
- **Authors**: Xinqing Guo, Zhang Chen, Siyuan Li, Yang Yang, Jingyi Yu
- **Comment**: None
- **Journal**: The Chinese Conference on Pattern Recognition and Computer Vision
  (PRCV), 2019
- **Summary**: Human visual system relies on both binocular stereo cues and monocular focusness cues to gain effective 3D perception. In computer vision, the two problems are traditionally solved in separate tracks. In this paper, we present a unified learning-based technique that simultaneously uses both types of cues for depth inference. Specifically, we use a pair of focal stacks as input to emulate human perception. We first construct a comprehensive focal stack training dataset synthesized by depth-guided light field rendering. We then construct three individual networks: a Focus-Net to extract depth from a single focal stack, a EDoF-Net to obtain the extended depth of field (EDoF) image from the focal stack, and a Stereo-Net to conduct stereo matching. We show how to integrate them into a unified BDfF-Net to obtain high-quality depth maps. Comprehensive experiments show that our approach outperforms the state-of-the-art in both accuracy and speed and effectively emulates human vision systems.



### DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems
- **Arxiv ID**: http://arxiv.org/abs/1711.10733v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10733v2)
- **Published**: 2017-11-29 09:04:27+00:00
- **Updated**: 2018-07-31 07:22:55+00:00
- **Authors**: Florian Bernard, Christian Theobalt, Michael Moeller
- **Comment**: Published at CVPR 2018
- **Journal**: None
- **Summary**: In this work we study convex relaxations of quadratic optimisation problems over permutation matrices. While existing semidefinite programming approaches can achieve remarkably tight relaxations, they have the strong disadvantage that they lift the original $n {\times} n$-dimensional variable to an $n^2 {\times} n^2$-dimensional variable, which limits their practical applicability. In contrast, here we present a lifting-free convex relaxation that is provably at least as tight as existing (lifting-free) convex relaxations. We demonstrate experimentally that our approach is superior to existing convex and non-convex methods for various problems, including image arrangement and multi-graph matching.



### Unpaired Photo-to-Caricature Translation on Faces in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1711.10735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10735v2)
- **Published**: 2017-11-29 09:12:50+00:00
- **Updated**: 2018-07-25 01:01:15+00:00
- **Authors**: Ziqiang Zheng, Wang Chao, Zhibin Yu, Nan Wang, Haiyong Zheng, Bing Zheng
- **Comment**: 28 pages, 11 figures
- **Journal**: None
- **Summary**: Recently, image-to-image translation has been made much progress owing to the success of conditional Generative Adversarial Networks (cGANs). And some unpaired methods based on cycle consistency loss such as DualGAN, CycleGAN and DiscoGAN are really popular. However, it's still very challenging for translation tasks with the requirement of high-level visual information conversion, such as photo-to-caricature translation that requires satire, exaggeration, lifelikeness and artistry. We present an approach for learning to translate faces in the wild from the source photo domain to the target caricature domain with different styles, which can also be used for other high-level image-to-image translation tasks. In order to capture global structure with local statistics while translation, we design a dual pathway model with one coarse discriminator and one fine discriminator. For generator, we provide one extra perceptual loss in association with adversarial loss and cycle consistency loss to achieve representation learning for two different domains. Also the style can be learned by the auxiliary noise input. Experiments on photo-to-caricature translation of faces in the wild show considerable performance gain of our proposed method over state-of-the-art translation methods as well as its potential real applications.



### Pipeline Generative Adversarial Networks for Facial Images Generation with Multiple Attributes
- **Arxiv ID**: http://arxiv.org/abs/1711.10742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10742v1)
- **Published**: 2017-11-29 09:25:36+00:00
- **Updated**: 2017-11-29 09:25:36+00:00
- **Authors**: Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Chao Wang, Nan Wang
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Generative Adversarial Networks are proved to be efficient on various kinds of image generation tasks. However, it is still a challenge if we want to generate images precisely. Many researchers focus on how to generate images with one attribute. But image generation under multiple attributes is still a tough work. In this paper, we try to generate a variety of face images under multiple constraints using a pipeline process. The Pip-GAN (Pipeline Generative Adversarial Network) we present employs a pipeline network structure which can generate a complex facial image step by step using a neutral face image. We applied our method on two face image databases and demonstrate its ability to generate convincing novel images of unseen identities under multiple conditions previously.



### Convolutional Neural Networks for Breast Cancer Screening: Transfer Learning with Exponential Decay
- **Arxiv ID**: http://arxiv.org/abs/1711.10752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10752v1)
- **Published**: 2017-11-29 10:08:46+00:00
- **Updated**: 2017-11-29 10:08:46+00:00
- **Authors**: Hiba Chougrad, Hamid Zouaki, Omar Alheyane
- **Comment**: 6 pages, 2 figures, NIPS ML4H 2017: Machine Learning for Health
  Workshop at NIPS 2017, Long Beach, CA, United States, December 8, 2017
- **Journal**: None
- **Summary**: In this paper, we propose a Computer Assisted Diagnosis (CAD) system based on a deep Convolutional Neural Network (CNN) model, to build an end-to-end learning process that classifies breast mass lesions. We investigate the impact that has transfer learning when large data is scarce, and explore the proper way to fine-tune the layers to learn features that are more specific to the new data. The proposed approach showed better performance compared to other proposals that classified the same dataset.



### Transfer Learning with Binary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1711.10761v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.10761v1)
- **Published**: 2017-11-29 10:28:02+00:00
- **Updated**: 2017-11-29 10:28:02+00:00
- **Authors**: Sam Leroux, Steven Bohez, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt
- **Comment**: Machine Learning on the Phone and other Consumer Devices, NIPS2017
  Workshop
- **Journal**: None
- **Summary**: Previous work has shown that it is possible to train deep neural networks with low precision weights and activations. In the extreme case it is even possible to constrain the network to binary values. The costly floating point multiplications are then reduced to fast logical operations. High end smart phones such as Google's Pixel 2 and Apple's iPhone X are already equipped with specialised hardware for image processing and it is very likely that other future consumer hardware will also have dedicated accelerators for deep neural networks. Binary neural networks are attractive in this case because the logical operations are very fast and efficient when implemented in hardware. We propose a transfer learning based architecture where we first train a binary network on Imagenet and then retrain part of the network for different tasks while keeping most of the network fixed. The fixed binary part could be implemented in a hardware accelerator while the last layers of the network are evaluated in software. We show that a single binary neural network trained on the Imagenet dataset can indeed be used as a feature extractor for other datasets.



### Online Product Quantization
- **Arxiv ID**: http://arxiv.org/abs/1711.10775v2
- **DOI**: 10.1109/TKDE.2018.2817526
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10775v2)
- **Published**: 2017-11-29 11:17:04+00:00
- **Updated**: 2018-03-24 12:03:46+00:00
- **Authors**: Donna Xu, Ivor W. Tsang, Ying Zhang
- **Comment**: To appear in IEEE Transactions on Knowledge and Data Engineering
  (DOI: 10.1109/TKDE.2018.2817526)
- **Journal**: None
- **Summary**: Approximate nearest neighbor (ANN) search has achieved great success in many tasks. However, existing popular methods for ANN search, such as hashing and quantization methods, are designed for static databases only. They cannot handle well the database with data distribution evolving dynamically, due to the high computational effort for retraining the model based on the new database. In this paper, we address the problem by developing an online product quantization (online PQ) model and incrementally updating the quantization codebook that accommodates to the incoming streaming data. Moreover, to further alleviate the issue of large scale computation for the online PQ update, we design two budget constraints for the model to update partial PQ codebook instead of all. We derive a loss bound which guarantees the performance of our online PQ model. Furthermore, we develop an online PQ model over a sliding window with both data insertion and deletion supported, to reflect the real-time behaviour of the data. The experiments demonstrate that our online PQ model is both time-efficient and effective for ANN search in dynamic large scale databases compared with baseline methods and the idea of partial PQ codebook update further reduces the update cost.



### Blind estimation of white Gaussian noise variance in highly textured images
- **Arxiv ID**: http://arxiv.org/abs/1711.10792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10792v1)
- **Published**: 2017-11-29 11:43:26+00:00
- **Updated**: 2017-11-29 11:43:26+00:00
- **Authors**: Mykola Ponomarenko, Nikolay Gapon, Viacheslav Voronin, Karen Egiazarian
- **Comment**: IS&T International Symposium on Electronic Imaging (EI 2018), Image
  Processing: Algorithms and Systems XVI
- **Journal**: None
- **Summary**: In the paper, a new method of blind estimation of noise variance in a single highly textured image is proposed. An input image is divided into 8x8 blocks and discrete cosine transform (DCT) is performed for each block. A part of 64 DCT coefficients with lowest energy calculated through all blocks is selected for further analysis. For the DCT coefficients, a robust estimate of noise variance is calculated. Corresponding to the obtained estimate, a part of blocks having very large values of local variance calculated only for the selected DCT coefficients are excluded from the further analysis. These two steps (estimation of noise variance and exclusion of blocks) are iteratively repeated three times. For the verification of the proposed method, a new noise-free test image database TAMPERE17 consisting of many highly textured images is designed. It is shown for this database and different values of noise variance from the set {25, 49, 100, 225}, that the proposed method provides approximately two times lower estimation root mean square error than other methods.



### Saliency Weighted Convolutional Features for Instance Search
- **Arxiv ID**: http://arxiv.org/abs/1711.10795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1711.10795v1)
- **Published**: 2017-11-29 11:46:56+00:00
- **Updated**: 2017-11-29 11:46:56+00:00
- **Authors**: Eva Mohedano, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor
- **Comment**: None
- **Journal**: None
- **Summary**: This work explores attention models to weight the contribution of local convolutional representations for the instance search task. We present a retrieval framework based on bags of local convolutional features (BLCF) that benefits from saliency weighting to build an efficient image representation. The use of human visual attention models (saliency) allows significant improvements in retrieval performance without the need to conduct region analysis or spatial verification, and without requiring any feature fine tuning. We investigate the impact of different saliency models, finding that higher performance on saliency benchmarks does not necessarily equate to improved performance when used in instance search tasks. The proposed approach outperforms the state-of-the-art on the challenging INSTRE benchmark by a large margin, and provides similar performance on the Oxford and Paris benchmarks compared to more complex methods that use off-the-shelf representations. The source code used in this project is available at https://imatge-upc.github.io/salbow/



### DeepSkeleton: Skeleton Map for 3D Human Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/1711.10796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10796v1)
- **Published**: 2017-11-29 11:50:01+00:00
- **Updated**: 2017-11-29 11:50:01+00:00
- **Authors**: Qingfu Wan, Wei Zhang, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent success on 2D human pose estimation, 3D human pose estimation still remains an open problem. A key challenge is the ill-posed depth ambiguity nature. This paper presents a novel intermediate feature representation named skeleton map for regression. It distills structural context from irrelavant properties of RGB image e.g. illumination and texture. It is simple, clean and can be easily generated via deconvolution network. For the first time, we show that training regression network from skeleton map alone is capable of meeting the performance of state-of-theart 3D human pose estimation works. We further exploit the power of multiple 3D hypothesis generation to obtain reasonbale 3D pose in consistent with 2D pose detection. The effectiveness of our approach is validated on challenging in-the-wild dataset MPII and indoor dataset Human3.6M.



### Compression for Smooth Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1711.10824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1711.10824v1)
- **Published**: 2017-11-29 12:46:08+00:00
- **Updated**: 2017-11-29 12:46:08+00:00
- **Authors**: V. Estellers, F. R. Schmidt, D. Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Most 3D shape analysis methods use triangular meshes to discretize both the shape and functions on it as piecewise linear functions. With this representation, shape analysis requires fine meshes to represent smooth shapes and geometric operators like normals, curvatures, or Laplace-Beltrami eigenfunctions at large computational and memory costs.   We avoid this bottleneck with a compression technique that represents a smooth shape as subdivision surfaces and exploits the subdivision scheme to parametrize smooth functions on that shape with a few control parameters. This compression does not affect the accuracy of the Laplace-Beltrami operator and its eigenfunctions and allow us to compute shape descriptors and shape matchings at an accuracy comparable to triangular meshes but a fraction of the computational cost.   Our framework can also compress surfaces represented by point clouds to do shape analysis of 3D scanning data.



### Sparse Photometric 3D Face Reconstruction Guided by Morphable Models
- **Arxiv ID**: http://arxiv.org/abs/1711.10870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10870v1)
- **Published**: 2017-11-29 14:24:58+00:00
- **Updated**: 2017-11-29 14:24:58+00:00
- **Authors**: Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Cen Wang, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and latest advances on face registration/modeling from a single image. We observe that 3D morphable faces approach provides a reasonable geometry proxy for light position calibration. Specifically, we develop a robust optimization technique that can calibrate per-pixel lighting direction and illumination at a very high precision without assuming uniform surface albedos. Next, we apply semantic segmentation on input images and the geometry proxy to refine hairy vs. bare skin regions using tailored filters. Experiments on synthetic and real data show that by using a very small set of images, our technique is able to reconstruct fine geometric details such as wrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing movie quality productions.



### PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.10871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10871v2)
- **Published**: 2017-11-29 14:25:13+00:00
- **Updated**: 2018-08-25 22:22:44+00:00
- **Authors**: Danfei Xu, Dragomir Anguelov, Ashesh Jain
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.



### Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density Network
- **Arxiv ID**: http://arxiv.org/abs/1711.10872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10872v2)
- **Published**: 2017-11-29 14:25:18+00:00
- **Updated**: 2018-05-21 18:01:43+00:00
- **Authors**: Qi Ye, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Learning and predicting the pose parameters of a 3D hand model given an image, such as locations of hand joints, is challenging due to large viewpoint changes and articulations, and severe self-occlusions exhibited particularly in egocentric views. Both feature learning and prediction modeling have been investigated to tackle the problem. Though effective, most existing discriminative methods yield a single deterministic estimation of target poses. Due to their single-value mapping intrinsic, they fail to adequately handle self-occlusion problems, where occluded joints present multiple modes. In this paper, we tackle the self-occlusion issue and provide a complete description of observed poses given an input depth image by a novel method called hierarchical mixture density networks (HMDN). The proposed method leverages the state-of-the-art hand pose estimators based on Convolutional Neural Networks to facilitate feature learning, while it models the multiple modes in a two-level hierarchy to reconcile single-valued and multi-valued mapping in its output. The whole framework with a mixture of two differentiable density functions is naturally end-to-end trainable. In the experiments, HMDN produces interpretable and diverse candidate samples, and significantly outperforms the state-of-the-art methods on two benchmarks with occlusions, and performs comparably on another benchmark free of occlusions.



### Learning Spatio-temporal Features with Partial Expression Sequences for on-the-Fly Prediction
- **Arxiv ID**: http://arxiv.org/abs/1711.10914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10914v1)
- **Published**: 2017-11-29 15:27:22+00:00
- **Updated**: 2017-11-29 15:27:22+00:00
- **Authors**: Wissam J. Baddar, Yong Man Ro
- **Comment**: Accepted at AAAI 2018
- **Journal**: None
- **Summary**: Spatio-temporal feature encoding is essential for encoding facial expression dynamics in video sequences. At test time, most spatio-temporal encoding methods assume that a temporally segmented sequence is fed to a learned model, which could require the prediction to wait until the full sequence is available to an auxiliary task that performs the temporal segmentation. This causes a delay in predicting the expression. In an interactive setting, such as affective interactive agents, such delay in the prediction could not be tolerated. Therefore, training a model that can accurately predict the facial expression "on-the-fly" (as they are fed to the system) is essential. In this paper, we propose a new spatio-temporal feature learning method, which would allow prediction with partial sequences. As such, the prediction could be performed on-the-fly. The proposed method utilizes an estimated expression intensity to generate dense labels, which are used to regulate the prediction model training with a novel objective function. As results, the learned spatio-temporal features can robustly predict the expression with partial (incomplete) expression sequences, on-the-fly. Experimental results showed that the proposed method achieved higher recognition rates compared to the state-of-the-art methods on both datasets. More importantly, the results verified that the proposed method improved the prediction frames with partial expression sequence inputs.



### Joint Blind Motion Deblurring and Depth Estimation of Light Field
- **Arxiv ID**: http://arxiv.org/abs/1711.10918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10918v2)
- **Published**: 2017-11-29 15:31:55+00:00
- **Updated**: 2018-06-14 06:40:34+00:00
- **Authors**: Dongwoo Lee, Haesol Park, In Kyu Park, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Removing camera motion blur from a single light field is a challenging task since it is highly ill-posed inverse problem. The problem becomes even worse when blur kernel varies spatially due to scene depth variation and high-order camera motion. In this paper, we propose a novel algorithm to estimate all blur model variables jointly, including latent sub-aperture image, camera motion, and scene depth from the blurred 4D light field. Exploiting multi-view nature of a light field relieves the inverse property of the optimization by utilizing strong depth cues and multi-view blur observation. The proposed joint estimation achieves high quality light field deblurring and depth estimation simultaneously under arbitrary 6-DOF camera motion and unconstrained scene depth. Intensive experiment on real and synthetic blurred light field confirms that the proposed algorithm outperforms the state-of-the-art light field deblurring and depth estimation methods.



### Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1711.10925v4
- **DOI**: 10.1007/s11263-020-01303-4
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.10925v4)
- **Published**: 2017-11-29 15:50:05+00:00
- **Updated**: 2020-05-17 10:57:04+00:00
- **Authors**: Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.   Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep_image_prior .



### Automatic Spine Segmentation using Convolutional Neural Network via Redundant Generation of Class Labels for 3D Spine Modeling
- **Arxiv ID**: http://arxiv.org/abs/1712.01640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01640v1)
- **Published**: 2017-11-29 16:19:07+00:00
- **Updated**: 2017-11-29 16:19:07+00:00
- **Authors**: Malinda Vania, Dawit Mureja, Deukhee Lee
- **Comment**: 18 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: There has been a significant increase from 2010 to 2016 in the number of people suffering from spine problems. The automatic image segmentation of the spine obtained from a computed tomography (CT) image is important for diagnosing spine conditions and for performing surgery with computer-assisted surgery systems. The spine has a complex anatomy that consists of 33 vertebrae, 23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the spinal surgeon is faced with the challenge of needing a robust algorithm to segment and create a model of the spine. In this study, we developed an automatic segmentation method to segment the spine, and we compared our segmentation results with reference segmentations obtained by experts. We developed a fully automatic approach for spine segmentation from CT based on a hybrid method. This method combines the convolutional neural network (CNN) and fully convolutional network (FCN), and utilizes class redundancy as a soft constraint to greatly improve the segmentation results. The proposed method was found to significantly enhance the accuracy of the segmentation results and the system processing time. Our comparison was based on 12 measurements: the Dice coefficient (94%), Jaccard index (93%), volumetric similarity (96%), sensitivity (97%), specificity (99%), precision (over segmentation; 8.3 and under segmentation 2.6), accuracy (99%), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We experimented with CT images from 32 patients, and the experimental results demonstrated the efficiency of the proposed method.



### Automatic Generation of Constrained Furniture Layouts
- **Arxiv ID**: http://arxiv.org/abs/1711.10939v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1711.10939v3)
- **Published**: 2017-11-29 16:21:32+00:00
- **Updated**: 2019-01-24 20:58:23+00:00
- **Authors**: Paul Henderson, Kartic Subr, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient authoring of vast virtual environments hinges on algorithms that are able to automatically generate content while also being controllable. We propose a method to automatically generate furniture layouts for indoor environments. Our method is simple, efficient, human-interpretable and amenable to a wide variety of constraints. We model the composition of rooms into classes of objects and learn joint (co-occurrence) statistics from a database of training layouts. We generate new layouts by performing a sequence of conditional sampling steps, exploiting the statistics learned from the database. The generated layouts are specified as 3D object models, along with their positions and orientations. We show they are of equivalent perceived quality to the training layouts, and compare favorably to a state-of-the-art method. We incorporate constraints using a general mechanism -- rejection sampling -- which provides great flexibility at the cost of extra computation. We demonstrate the versatility of our method by applying a wide variety of constraints relevant to real-world applications.



### Saccade Sequence Prediction: Beyond Static Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1711.10959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10959v1)
- **Published**: 2017-11-29 16:48:28+00:00
- **Updated**: 2017-11-29 16:48:28+00:00
- **Authors**: Calden Wloka, Iuliia Kotseruba, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention is a field with a considerable history, with eye movement control and prediction forming an important subfield. Fixation modeling in the past decades has been largely dominated computationally by a number of highly influential bottom-up saliency models, such as the Itti-Koch-Niebur model. The accuracy of such models has dramatically increased recently due to deep learning. However, on static images the emphasis of these models has largely been based on non-ordered prediction of fixations through a saliency map. Very few implemented models can generate temporally ordered human-like sequences of saccades beyond an initial fixation point. Towards addressing these shortcomings we present STAR-FC, a novel multi-saccade generator based on a central/peripheral integration of deep learning-based saliency and lower-level feature-based saliency. We have evaluated our model using the CAT2000 database, successfully predicting human patterns of fixation with equivalent accuracy and quality compared to what can be achieved by using one human sequence to predict another. This is a significant improvement over fixation sequences predicted by state-of-the-art saliency algorithms.



### Colour Constancy: Biologically-inspired Contrast Variant Pooling Mechanism
- **Arxiv ID**: http://arxiv.org/abs/1711.10968v1
- **DOI**: 10.5244/C.31.77
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.10968v1)
- **Published**: 2017-11-29 17:14:50+00:00
- **Updated**: 2017-11-29 17:14:50+00:00
- **Authors**: Arash Akbarinia, Raquel Gil Rodr√≠guez, C. Alejandro Parraga
- **Comment**: None
- **Journal**: Proceedings of the British machine Vision Conference (BMVC) 2017
- **Summary**: Pooling is a ubiquitous operation in image processing algorithms that allows for higher-level processes to collect relevant low-level features from a region of interest. Currently, max-pooling is one of the most commonly used operators in the computational literature. However, it can lack robustness to outliers due to the fact that it relies merely on the peak of a function. Pooling mechanisms are also present in the primate visual cortex where neurons of higher cortical areas pool signals from lower ones. The receptive fields of these neurons have been shown to vary according to the contrast by aggregating signals over a larger region in the presence of low contrast stimuli. We hypothesise that this contrast-variant-pooling mechanism can address some of the shortcomings of max-pooling. We modelled this contrast variation through a histogram clipping in which the percentage of pooled signal is inversely proportional to the local contrast of an image. We tested our hypothesis by applying it to the phenomenon of colour constancy where a number of popular algorithms utilise a max-pooling step (e.g. White-Patch, Grey-Edge and Double-Opponency). For each of these methods, we investigated the consequences of replacing their original max-pooling by the proposed contrast-variant-pooling. Our experiments on three colour constancy benchmark datasets suggest that previous results can significantly improve by adopting a contrast-variant-pooling mechanism.



### HoME: a Household Multimodal Environment
- **Arxiv ID**: http://arxiv.org/abs/1711.11017v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1711.11017v1)
- **Published**: 2017-11-29 18:45:59+00:00
- **Updated**: 2017-11-29 18:45:59+00:00
- **Authors**: Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, Aaron Courville
- **Comment**: Presented at NIPS 2017's Visually-Grounded Interaction and Language
  Workshop
- **Journal**: None
- **Summary**: We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.



### Detection-aided liver lesion segmentation using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1711.11069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11069v1)
- **Published**: 2017-11-29 19:27:40+00:00
- **Updated**: 2017-11-29 19:27:40+00:00
- **Authors**: Miriam Bellver, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Xavier Giro-i-Nieto, Jordi Torres, Luc Van Gool
- **Comment**: NIPS 2017 Workshop on Machine Learning for Health (ML4H)
- **Journal**: None
- **Summary**: A fully automatic technique for segmenting the liver and localizing its unhealthy tissues is a convenient tool in order to diagnose hepatic diseases and assess the response to the according treatments. In this work we propose a method to segment the liver and its lesions from Computed Tomography (CT) scans using Convolutional Neural Networks (CNNs), that have proven good results in a variety of computer vision tasks, including medical imaging. The network that segments the lesions consists of a cascaded architecture, which first focuses on the region of the liver in order to segment the lesions on it. Moreover, we train a detector to localize the lesions, and mask the results of the segmentation network with the positive detections. The segmentation architecture is based on DRIU, a Fully Convolutional Network (FCN) with side outputs that work on feature maps of different resolutions, to finally benefit from the multi-scale information learned by different stages of the network. The main contribution of this work is the use of a detector to localize the lesions, which we show to be beneficial to remove false positives triggered by the segmentation network. Source code and models are available at https://imatge-upc.github.io/liverseg-2017-nipsws/ .



### A fast nonconvex Compressed Sensing algorithm for highly low-sampled MR images reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1711.11075v1
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1711.11075v1)
- **Published**: 2017-11-29 19:39:08+00:00
- **Updated**: 2017-11-29 19:39:08+00:00
- **Authors**: Damiana Lazzaro, Elena Loli Piccolomini, Fabiana Zama
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a fast and efficient method for the reconstruction of Magnetic Resonance Images (MRI) from severely under-sampled data. From the Compressed Sensing theory we have mathematically modeled the problem as a constrained minimization problem with a family of non-convex regularizing objective functions depending on a parameter and a least squares data fit constraint. We propose a fast and efficient algorithm, named Fast NonConvex Reweighting (FNCR) algorithm, based on an iterative scheme where the non-convex problem is approximated by its convex linearization and the penalization parameter is automatically updated. The convex problem is solved by a Forward-Backward procedure, where the Backward step is performed by a Split Bregman strategy. Moreover, we propose a new efficient iterative solver for the arising linear systems. We prove the convergence of the proposed FNCR method. The results on synthetic phantoms and real images show that the algorithm is very well performing and computationally efficient, even when compared to the best performing methods proposed in the literature.



### Optimizing colormaps with consideration for color vision deficiency to enable accurate interpretation of scientific data
- **Arxiv ID**: http://arxiv.org/abs/1712.01662v3
- **DOI**: 10.1371/journal.pone.0199239
- **Categories**: **cs.CV**, q-bio.OT
- **Links**: [PDF](http://arxiv.org/pdf/1712.01662v3)
- **Published**: 2017-11-29 20:24:15+00:00
- **Updated**: 2018-08-01 19:17:41+00:00
- **Authors**: Jamie R. Nu√±ez, Christopher R. Anderton, Ryan S. Renslow
- **Comment**: None
- **Journal**: J. R. Nu\~nez, C. R. Anderton, and R. S. Renslow, "Optimizing
  colormaps with consideration for color vision deficiency to enable accurate
  interpretation of scientific data," PLOS ONE, 2018. 13(7): p. e0199239
- **Summary**: Color vision deficiency (CVD) affects more than 4% of the population and leads to a different visual perception of colors. Though this has been known for decades, colormaps with many colors across the visual spectra are often used to represent data, leading to the potential for misinterpretation or difficulty with interpretation by someone with this deficiency. Until the creation of the module presented here, there were no colormaps mathematically optimized for CVD using modern color appearance models. While there have been some attempts to make aesthetically pleasing or subjectively tolerable colormaps for those with CVD, our goal was to make optimized colormaps for the most accurate perception of scientific data by as many viewers as possible. We developed a Python module, cmaputil, to create CVD-optimized colormaps, which imports colormaps and modifies them to be perceptually uniform in CVD-safe colorspace while linearizing and maximizing the brightness range. The module is made available to the science community to enable others to easily create their own CVDoptimized colormaps. Here, we present an example CVD-optimized colormap created with this module that is optimized for viewing by those without a CVD as well as those with redgreen colorblindness. This colormap, cividis, enables nearly-identical visual-data interpretation to both groups, is perceptually uniform in hue and brightness, and increases in brightness linearly.



### Deep Learning for identifying radiogenomic associations in breast cancer
- **Arxiv ID**: http://arxiv.org/abs/1711.11097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11097v1)
- **Published**: 2017-11-29 20:41:05+00:00
- **Updated**: 2017-11-29 20:41:05+00:00
- **Authors**: Zhe Zhu, Ehab Albadawy, Ashirbani Saha, Jun Zhang, Michael R. Harowicz, Maciej A. Mazurowski
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To determine whether deep learning models can distinguish between breast cancer molecular subtypes based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Materials and methods: In this institutional review board-approved single-center study, we analyzed DCE-MR images of 270 patients at our institution. Lesions of interest were identified by radiologists. The task was to automatically determine whether the tumor is of the Luminal A subtype or of another subtype based on the MR image patches representing the tumor. Three different deep learning approaches were used to classify the tumor according to their molecular subtypes: learning from scratch where only tumor patches were used for training, transfer learning where networks pre-trained on natural images were fine-tuned using tumor patches, and off-the-shelf deep features where the features extracted by neural networks trained on natural images were used for classification with a support vector machine. Network architectures utilized in our experiments were GoogleNet, VGG, and CIFAR. We used 10-fold crossvalidation method for validation and area under the receiver operating characteristic (AUC) as the measure of performance. Results: The best AUC performance for distinguishing molecular subtypes was 0.65 (95% CI:[0.57,0.71]) and was achieved by the off-the-shelf deep features approach. The highest AUC performance for training from scratch was 0.58 (95% CI:[0.51,0.64]) and the best AUC performance for transfer learning was 0.60 (95% CI:[0.52,0.65]) respectively. For the off-the-shelf approach, the features extracted from the fully connected layer performed the best. Conclusion: Deep learning may play a role in discovering radiogenomic associations in breast cancer.



### Towards Alzheimer's Disease Classification through Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.11117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11117v1)
- **Published**: 2017-11-29 21:40:36+00:00
- **Updated**: 2017-11-29 21:40:36+00:00
- **Authors**: Marcia Hon, Naimul Khan
- **Comment**: Presented at IEEE BIBM 2017
- **Journal**: None
- **Summary**: Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI through machine learning have been a subject of intense research in recent years. Recent success of deep learning in computer vision have progressed such research further. However, common limitations with such algorithms are reliance on a large number of training images, and requirement of careful optimization of the architecture of deep networks. In this paper, we attempt solving these issues with transfer learning, where state-of-the-art architectures such as VGG and Inception are initialized with pre-trained weights from large benchmark datasets consisting of natural images, and the fully-connected layer is re-trained with only a small number of MRI images. We employ image entropy to select the most informative slices for training. Through experimentation on the OASIS MRI dataset, we show that with training size almost 10 times smaller than the state-of-the-art, we reach comparable or even better performance than current deep-learning based methods.



### Video Captioning via Hierarchical Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1711.11135v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.11135v3)
- **Published**: 2017-11-29 22:23:59+00:00
- **Updated**: 2018-03-29 07:06:47+00:00
- **Authors**: Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang
- **Comment**: CVPR 2018, with supplementary material
- **Journal**: None
- **Summary**: Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.



### Structured learning and detailed interpretation of minimal object images
- **Arxiv ID**: http://arxiv.org/abs/1711.11151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11151v1)
- **Published**: 2017-11-29 23:26:52+00:00
- **Updated**: 2017-11-29 23:26:52+00:00
- **Authors**: Guy Ben-Yosef, Liav Assif, Shimon Ullman
- **Comment**: Accepted to Workshop on Mutual Benefits of Cognitive and Computer
  Vision, at the International Conference on Computer Vision. Venice, Italy,
  2017
- **Journal**: None
- **Summary**: We model the process of human full interpretation of object images, namely the ability to identify and localize all semantic features and parts that are recognized by human observers. The task is approached by dividing the interpretation of the complete object to the interpretation of multiple reduced but interpretable local regions. We model interpretation by a structured learning framework, in which there are primitive components and relations that play a useful role in local interpretation by humans. To identify useful components and relations used in the interpretation process, we consider the interpretation of minimal configurations, namely reduced local regions that are minimal in the sense that further reduction will turn them unrecognizable and uninterpretable. We show experimental results of our model, and results of predicting and testing relations that were useful to the model via transformed minimal images.



### Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1711.11152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11152v2)
- **Published**: 2017-11-29 23:29:02+00:00
- **Updated**: 2018-07-07 11:18:56+00:00
- **Authors**: Shuyang Sun, Zhanghui Kuang, Wanli Ouyang, Lu Sheng, Wei Zhang
- **Comment**: CVPR 2018. code available at
  https://github.com/kevin-ssy/Optical-Flow-Guided-Feature
- **Journal**: None
- **Summary**: Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatiotemporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96:0% and 74:2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at https://github.com/kevin-ssy/Optical-Flow-Guided-Feature.



### Predicting Depression Severity by Multi-Modal Feature Engineering and Fusion
- **Arxiv ID**: http://arxiv.org/abs/1711.11155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.11155v1)
- **Published**: 2017-11-29 23:39:43+00:00
- **Updated**: 2017-11-29 23:39:43+00:00
- **Authors**: Aven Samareh, Yan Jin, Zhangyang Wang, Xiangyu Chang, Shuai Huang
- **Comment**: Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)
- **Journal**: None
- **Summary**: We present our preliminary work to determine if patient's vocal acoustic, linguistic, and facial patterns could predict clinical ratings of depression severity, namely Patient Health Questionnaire depression scale (PHQ-8). We proposed a multi modal fusion model that combines three different modalities: audio, video , and text features. By training over AVEC 2017 data set, our proposed model outperforms each single modality prediction model, and surpasses the data set baseline with ice margin.




# Arxiv Papers in cs.CV on 2017-11-18
### Excitation Backprop for RNNs
- **Arxiv ID**: http://arxiv.org/abs/1711.06778v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06778v3)
- **Published**: 2017-11-18 00:22:17+00:00
- **Updated**: 2018-03-08 16:49:13+00:00
- **Authors**: Sarah Adel Bargal, Andrea Zunino, Donghyun Kim, Jianming Zhang, Vittorio Murino, Stan Sclaroff
- **Comment**: CVPR 2018 Camera Ready Version
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2018
- **Summary**: Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.



### Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1711.06787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06787v2)
- **Published**: 2017-11-18 01:37:04+00:00
- **Updated**: 2018-07-31 14:43:16+00:00
- **Authors**: Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan Luo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Single image dehazing is an important low-level vision task with many applications. Early researches have investigated different kinds of visual priors to address this problem. However, they may fail when their assumptions are not valid on specific images. Recent deep networks also achieve relatively good performance in this task. But unfortunately, due to the disappreciation of rich physical rules in hazes, large amounts of data are required for their training. More importantly, they may still fail when there exist completely different haze distributions in testing images. By considering the collaborations of these two perspectives, this paper designs a novel residual architecture to aggregate both prior (i.e., domain knowledge) and data (i.e., haze distribution) information to propagate transmissions for scene radiance estimation. We further present a variational energy based perspective to investigate the intrinsic propagation behavior of our aggregated deep model. In this way, we actually bridge the gap between prior driven models and data driven networks and leverage advantages but avoid limitations of previous dehazing approaches. A lightweight learning framework is proposed to train our propagation network. Finally, by introducing a taskaware image separation formulation with a flexible optimization scheme, we extend the proposed model for more challenging vision tasks, such as underwater image enhancement and single image rain removal. Experiments on both synthetic and realworld images demonstrate the effectiveness and efficiency of the proposed framework.



### Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1711.06794v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1711.06794v2)
- **Published**: 2017-11-18 02:07:34+00:00
- **Updated**: 2017-12-12 08:34:43+00:00
- **Authors**: Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang
- **Comment**: To appear in AAAI 2018
- **Journal**: None
- **Summary**: Recently, the Visual Question Answering (VQA) task has gained increasing attention in artificial intelligence. Existing VQA methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering. The free-form region based and the detection-based visual attention mechanisms are mostly investigated, with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions. We argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the VQA problem. In this paper, we propose a novel deep neural network for VQA that integrates both attention mechanisms. Our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering. The proposed method is extensively evaluated on two publicly available datasets, COCO-QA and VQA, and outperforms state-of-the-art approaches. Source code is available at https://github.com/lupantech/dual-mfa-vqa.



### A Genetic Algorithm Approach for ImageRepresentation Learning through Color Quantization
- **Arxiv ID**: http://arxiv.org/abs/1711.06809v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06809v3)
- **Published**: 2017-11-18 04:08:34+00:00
- **Updated**: 2020-11-20 14:03:43+00:00
- **Authors**: Érico M. Pereira, Ricardo da S. Torres, Jefersson A. dos Santos
- **Comment**: Submitted to Multimedia Tools and Applications
- **Journal**: None
- **Summary**: Over the last decades, hand-crafted feature extractors have been used to encode image visual properties into feature vectors. Recently, data-driven feature learning approaches have been successfully explored as alternatives for producing more representative visual features. In this work, we combine both research venues, focusing on the color quantization problem. We propose two data-driven approaches to learn image representations through the search for optimized quantization schemes, which lead to more effective feature extraction algorithms and compact representations. Our strategy employs Genetic Algorithm, a soft-computing apparatus successfully utilized in Information-retrieval-related optimization problems. We hypothesize that changing the quantization affects the quality of image description approaches, leading to effective and efficient representations. We evaluate our approaches in content-based image retrieval tasks, considering eight well-known datasets with different visual properties. Results indicate that the approach focused on representation effectiveness outperformed baselines in all tested scenarios. The other approach, which also considers the size of created representations, produced competitive results keeping or even reducing the dimensionality of feature vectors up to 25%.



### Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates
- **Arxiv ID**: http://arxiv.org/abs/1711.06821v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1711.06821v3)
- **Published**: 2017-11-18 07:00:44+00:00
- **Updated**: 2020-03-06 15:23:13+00:00
- **Authors**: Guillem Collell, Luc Van Gool, Marie-Francine Moens
- **Comment**: To appear at AAAI 2018 Conference
- **Journal**: None
- **Summary**: Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., "on", "below", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., "glass on table"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., "man riding horse"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output ("where is the man w.r.t. a horse when the man is walking the horse?"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,"man walking dog") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., "dog"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.



### Transferable Semi-supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1711.06828v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06828v2)
- **Published**: 2017-11-18 08:56:06+00:00
- **Updated**: 2018-05-09 01:53:52+00:00
- **Authors**: Huaxin Xiao, Yunchao Wei, Yu Liu, Maojun Zhang, Jiashi Feng
- **Comment**: Minor update of arXiv:1711.06828
- **Journal**: None
- **Summary**: The performance of deep learning based semantic segmentation models heavily depends on sufficient data with careful annotations. However, even the largest public datasets only provide samples with pixel-level annotations for rather limited semantic categories. Such data scarcity critically limits scalability and applicability of semantic segmentation models in real applications. In this paper, we propose a novel transferable semi-supervised semantic segmentation model that can transfer the learned segmentation knowledge from a few strong categories with pixel-level annotations to unseen weak categories with only image-level annotations, significantly broadening the applicable territory of deep segmentation models. In particular, the proposed model consists of two complementary and learnable components: a Label transfer Network (L-Net) and a Prediction transfer Network (P-Net). The L-Net learns to transfer the segmentation knowledge from strong categories to the images in the weak categories and produces coarse pixel-level semantic maps, by effectively exploiting the similar appearance shared across categories. Meanwhile, the P-Net tailors the transferred knowledge through a carefully designed adversarial learning strategy and produces refined segmentation results with better details. Integrating the L-Net and P-Net achieves 96.5% and 89.4% performance of the fully-supervised baseline using 50% and 0% categories with pixel-level annotations respectively on PASCAL VOC 2012. With such a novel transfer mechanism, our proposed model is easily generalizable to a variety of new categories, only requiring image-level annotations, and offers appealing scalability in real applications.



### Neural Network Based Reinforcement Learning for Audio-Visual Gaze Control in Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/1711.06834v2
- **DOI**: 10.1016/j.patrec.2018.05.023
- **Categories**: **cs.RO**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1711.06834v2)
- **Published**: 2017-11-18 09:19:47+00:00
- **Updated**: 2018-04-23 15:07:23+00:00
- **Authors**: Stéphane Lathuilière, Benoit Massé, Pablo Mesejo, Radu Horaud
- **Comment**: Paper submitted to Pattern Recognition Letters
- **Journal**: Pattern Recognition Letters, vol. 118, 2019, 61-71
- **Summary**: This paper introduces a novel neural network-based reinforcement learning approach for robot gaze control. Our approach enables a robot to learn and to adapt its gaze control strategy for human-robot interaction neither with the use of external sensors nor with human supervision. The robot learns to focus its attention onto groups of people from its own audio-visual experiences, independently of the number of people, of their positions and of their physical appearances. In particular, we use a recurrent neural network architecture in combination with Q-learning to find an optimal action-selection policy; we pre-train the network using a simulated environment that mimics realistic scenarios that involve speaking/silent participants, thus avoiding the need of tedious sessions of a robot interacting with people. Our experimental evaluation suggests that the proposed method is robust against parameter estimation, i.e. the parameter values yielded by the method do not have a decisive impact on the performance. The best results are obtained when both audio and visual information is jointly used. Experiments with the Nao robot indicate that our framework is a step forward towards the autonomous learning of socially acceptable gaze behavior.



### DLTK: State of the Art Reference Implementations for Deep Learning on Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1711.06853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1711.06853v1)
- **Published**: 2017-11-18 12:31:10+00:00
- **Updated**: 2017-11-18 12:31:10+00:00
- **Authors**: Nick Pawlowski, Sofia Ira Ktena, Matthew C. H. Lee, Bernhard Kainz, Daniel Rueckert, Ben Glocker, Martin Rajchl
- **Comment**: Submitted to Medical Imaging Meets NIPS 2017, Code at
  https://github.com/DLTK/DLTK
- **Journal**: None
- **Summary**: We present DLTK, a toolkit providing baseline implementations for efficient experimentation with deep learning methods on biomedical images. It builds on top of TensorFlow and its high modularity and easy-to-use examples allow for a low-threshold access to state-of-the-art implementations for typical medical imaging problems. A comparison of DLTK's reference implementations of popular network architectures for image segmentation demonstrates new top performance on the publicly available challenge data "Multi-Atlas Labeling Beyond the Cranial Vault". The average test Dice similarity coefficient of $81.5$ exceeds the previously best performing CNN ($75.7$) and the accuracy of the challenge winning method ($79.0$).



### Single-Shot Refinement Neural Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1711.06897v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06897v3)
- **Published**: 2017-11-18 17:05:34+00:00
- **Updated**: 2018-01-03 13:01:54+00:00
- **Authors**: Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, Stan Z. Li
- **Comment**: 14 pages, 7 figures, 7 tables
- **Journal**: None
- **Summary**: For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multi-task loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet



### Gazing into the Abyss: Real-time Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1711.06918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1711.06918v1)
- **Published**: 2017-11-18 18:53:40+00:00
- **Updated**: 2017-11-18 18:53:40+00:00
- **Authors**: George He, Sami Oueida, Tucker Ward
- **Comment**: 9 pages, computer vision, source code available
- **Journal**: None
- **Summary**: Gaze and face tracking algorithms have traditionally battled a compromise between computational complexity and accuracy; the most accurate neural net algorithms cannot be implemented in real time, but less complex real-time algorithms suffer from higher error. This project seeks to better bridge that gap by improving on real-time eye and facial recognition algorithms in order to develop accurate, real-time gaze estimation with an emphasis on minimizing training data and computational complexity. Our goal is to use eye and facial recognition techniques to enable users to perform limited tasks based on gaze and facial input using only a standard, low-quality web cam found in most modern laptops and smart phones and the limited computational power and training data typical of those scenarios. We therefore identified seven promising, fundamentally different algorithms based on different user features and developed one outstanding, one workable, and one honorable mention gaze tracking pipelines that match the performance of modern gaze trackers while using no training data.




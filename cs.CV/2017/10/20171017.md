# Arxiv Papers in cs.CV on 2017-10-17
### Face Transfer with Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1710.06090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06090v1)
- **Published**: 2017-10-17 04:31:39+00:00
- **Updated**: 2017-10-17 04:31:39+00:00
- **Authors**: Runze Xu, Zhiming Zhou, Weinan Zhang, Yong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Face transfer animates the facial performances of the character in the target video by a source actor. Traditional methods are typically based on face modeling. We propose an end-to-end face transfer method based on Generative Adversarial Network. Specifically, we leverage CycleGAN to generate the face image of the target character with the corresponding head pose and facial expression of the source. In order to improve the quality of generated videos, we adopt PatchGAN and explore the effect of different receptive field sizes on generated images.



### Spontaneous Symmetry Breaking in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.06096v1
- **DOI**: None
- **Categories**: **stat.CO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.06096v1)
- **Published**: 2017-10-17 04:55:14+00:00
- **Updated**: 2017-10-17 04:55:14+00:00
- **Authors**: Ricky Fok, Aijun An, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework to understand the unprecedented performance and robustness of deep neural networks using field theory. Correlations between the weights within the same layer can be described by symmetries in that layer, and networks generalize better if such symmetries are broken to reduce the redundancies of the weights. Using a two parameter field theory, we find that the network can break such symmetries itself towards the end of training in a process commonly known in physics as spontaneous symmetry breaking. This corresponds to a network generalizing itself without any user input layers to break the symmetry, but by communication with adjacent layers. In the layer decoupling limit applicable to residual networks (He et al., 2015), we show that the remnant symmetries that survive the non-linear layers are spontaneously broken. The Lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar. Using results from quantum field theory we show that our framework is able to explain many experimentally observed phenomena,such as training on random labels with zero error (Zhang et al., 2017), the information bottleneck, the phase transition out of it and gradient variance explosion (Shwartz-Ziv & Tishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.



### Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55
- **Arxiv ID**: http://arxiv.org/abs/1710.06104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06104v2)
- **Published**: 2017-10-17 05:34:22+00:00
- **Updated**: 2017-10-27 22:26:43+00:00
- **Authors**: Li Yi, Lin Shao, Manolis Savva, Haibin Huang, Yang Zhou, Qirui Wang, Benjamin Graham, Martin Engelcke, Roman Klokov, Victor Lempitsky, Yuan Gan, Pengyu Wang, Kun Liu, Fenggen Yu, Panpan Shui, Bingyang Hu, Yan Zhang, Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Minki Jeong, Jaehoon Choi, Changick Kim, Angom Geetchandra, Narasimha Murthy, Bhargava Ramu, Bharadwaj Manda, M Ramanathan, Gautam Kumar, P Preetham, Siddharth Srivastava, Swati Bhugra, Brejesh Lall, Christian Haene, Shubham Tulsiani, Jitendra Malik, Jared Lafer, Ramsey Jones, Siyuan Li, Jie Lu, Shi Jin, Jingyi Yu, Qixing Huang, Evangelos Kalogerakis, Silvio Savarese, Pat Hanrahan, Thomas Funkhouser, Hao Su, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a large-scale 3D shape understanding benchmark using data and annotation from ShapeNet 3D object database. The benchmark consists of two tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view images. Ten teams have participated in the challenge and the best performing teams have outperformed state-of-the-art approaches on both tasks. A few novel deep learning architectures have been proposed on various 3D representations on both tasks. We report the techniques used by each team and the corresponding performances. In addition, we summarize the major discoveries from the reported results and possible trends for the future work in the field.



### Scalable Dense Monocular Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1710.06130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06130v1)
- **Published**: 2017-10-17 07:17:01+00:00
- **Updated**: 2017-10-17 07:17:01+00:00
- **Authors**: Mohammad Dawud Ansari, Vladislav Golyanik, Didier Stricker
- **Comment**: International Conference on 3D Vision (3DV), Qingdao, China, October
  2017
- **Journal**: None
- **Summary**: This paper reports on a novel template-free monocular non-rigid surface reconstruction approach. Existing techniques using motion and deformation cues rely on multiple prior assumptions, are often computationally expensive and do not perform equally well across the variety of data sets. In contrast, the proposed Scalable Monocular Surface Reconstruction (SMSR) combines strengths of several algorithms, i.e., it is scalable with the number of points, can handle sparse and dense settings as well as different types of motions and deformations. We estimate camera pose by singular value thresholding and proximal gradient. Our formulation adopts alternating direction method of multipliers which converges in linear time for large point track matrices. In the proposed SMSR, trajectory space constraints are integrated by smoothing of the measurement matrix. In the extensive experiments, SMSR is demonstrated to consistently achieve state-of-the-art accuracy on a wide variety of data sets.



### Combining LiDAR Space Clustering and Convolutional Neural Networks for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1710.06160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06160v1)
- **Published**: 2017-10-17 08:40:16+00:00
- **Updated**: 2017-10-17 08:40:16+00:00
- **Authors**: Damien Matti, Hazım Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection is an important component for safety of autonomous vehicles, as well as for traffic and street surveillance. There are extensive benchmarks on this topic and it has been shown to be a challenging problem when applied on real use-case scenarios. In purely image-based pedestrian detection approaches, the state-of-the-art results have been achieved with convolutional neural networks (CNN) and surprisingly few detection frameworks have been built upon multi-cue approaches. In this work, we develop a new pedestrian detector for autonomous vehicles that exploits LiDAR data, in addition to visual information. In the proposed approach, LiDAR data is utilized to generate region proposals by processing the three dimensional point cloud that it provides. These candidate regions are then further processed by a state-of-the-art CNN classifier that we have fine-tuned for pedestrian detection. We have extensively evaluated the proposed detection process on the KITTI dataset. The experimental results show that the proposed LiDAR space clustering approach provides a very efficient way of generating region proposals leading to higher recall rates and fewer misses for pedestrian detection. This indicates that LiDAR data can provide auxiliary information for CNN-based approaches.



### Learning to Learn Image Classifiers with Visual Analogy
- **Arxiv ID**: http://arxiv.org/abs/1710.06177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06177v2)
- **Published**: 2017-10-17 09:17:33+00:00
- **Updated**: 2019-04-16 12:11:30+00:00
- **Authors**: Linjun Zhou, Peng Cui, Shiqiang Yang, Wenwu Zhu, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are far better learners who can learn a new concept very fast with only a few samples compared with machines. The plausible mystery making the difference is two fundamental learning mechanisms: learning to learn and learning by analogy. In this paper, we attempt to investigate a new human-like learning method by organically combining these two mechanisms. In particular, we study how to generalize the classification parameters from previously learned concepts to a new concept. we first propose a novel Visual Analogy Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional embedding space and a linear mapping function from the embedding space to classification parameters for base classes. We then propose an out-of-sample embedding method to learn the embedding of a new class represented by a few samples through its visual analogy with base classes and derive the classification parameters for the new class. We conduct extensive experiments on ImageNet dataset and the results show that our method could consistently and significantly outperform state-of-the-art baselines.



### A New Coherence-Penalized Minimal Path Model with Application to Retinal Vessel Centerline Delineation
- **Arxiv ID**: http://arxiv.org/abs/1710.06194v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.06194v1)
- **Published**: 2017-10-17 10:23:57+00:00
- **Updated**: 2017-10-17 10:23:57+00:00
- **Authors**: Da Chen, Laurent D. Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new minimal path model for minimally interactive retinal vessel centerline extraction. The main contribution lies at the construction of a novel coherence-penalized Riemannian metric in a lifted space, dependently of the local geometry of tubularity and an external scalar-valued reference feature map. The globally minimizing curves associated to the proposed metric favour to pass through a set of retinal vessel segments with low variations of the feature map, thus can avoid the short branches combination problem and shortcut problem, commonly suffered by the existing minimal path models in the application of retinal imaging. We validate our model on a series of retinal vessel patches obtained from the DRIVE and IOSTAR datasets, showing that our model indeed get promising results.



### Projective reconstruction in algebraic vision
- **Arxiv ID**: http://arxiv.org/abs/1710.06205v3
- **DOI**: 10.4153/S0008439519000687
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.06205v3)
- **Published**: 2017-10-17 11:03:00+00:00
- **Updated**: 2019-11-11 14:54:58+00:00
- **Authors**: Atsushi Ito, Makoto Miura, Kazushi Ueda
- **Comment**: 15 pages
- **Journal**: Can. Math. Bull. 63 (2020) 592-609
- **Summary**: We discuss the geometry of rational maps from a projective space of an arbitrary dimension to the product of projective spaces of lower dimensions induced by linear projections. In particular, we give an algebro-geometric variant of the projective reconstruction theorem by Hartley and Schaffalitzky [HS09].



### Robust Fusion of LiDAR and Wide-Angle Camera Data for Autonomous Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1710.06230v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.06230v3)
- **Published**: 2017-10-17 12:01:19+00:00
- **Updated**: 2018-08-23 04:05:50+00:00
- **Authors**: Varuna De Silva, Jamie Roche, Ahmet Kondoz
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous robots that assist humans in day to day living tasks are becoming increasingly popular. Autonomous mobile robots operate by sensing and perceiving their surrounding environment to make accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of autonomous vehicles. These heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor for free space detection. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression-based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a uncertainty aware free space detection algorithm



### 3D Object Discovery and Modeling Using Single RGB-D Images Containing Multiple Object Instances
- **Arxiv ID**: http://arxiv.org/abs/1710.06231v1
- **DOI**: 10.1109/3dv.2017.00056
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.06231v1)
- **Published**: 2017-10-17 12:03:34+00:00
- **Updated**: 2017-10-17 12:03:34+00:00
- **Authors**: Wim Abbeloos, Esra Ataer-Cansizoglu, Sergio Caccamo, Yuichi Taguchi, Yukiyasu Domae
- **Comment**: None
- **Journal**: Proceedings International Conference on 3D Vision 2017 (pp.
  431-439)
- **Summary**: Unsupervised object modeling is important in robotics, especially for handling a large set of objects. We present a method for unsupervised 3D object discovery, reconstruction, and localization that exploits multiple instances of an identical object contained in a single RGB-D image. The proposed method does not rely on segmentation, scene knowledge, or user input, and thus is easily scalable. Our method aims to find recurrent patterns in a single RGB-D image by utilizing appearance and geometry of the salient regions. We extract keypoints and match them in pairs based on their descriptors. We then generate triplets of the keypoints matching with each other using several geometric criteria to minimize false matches. The relative poses of the matched triplets are computed and clustered to discover sets of triplet pairs with similar relative poses. Triplets belonging to the same set are likely to belong to the same object and are used to construct an initial object model. Detection of remaining instances with the initial object model using RANSAC allows to further expand and refine the model. The automatically generated object models are both compact and descriptive. We show quantitative and qualitative results on RGB-D images with various objects including some from the Amazon Picking Challenge. We also demonstrate the use of our method in an object picking scenario with a robotic arm.



### Analysis of feature detector and descriptor combinations with a localization experiment for various performance metrics
- **Arxiv ID**: http://arxiv.org/abs/1710.06232v1
- **DOI**: 10.3906/elk-1602-225
- **Categories**: **cs.CV**, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1710.06232v1)
- **Published**: 2017-10-17 12:10:37+00:00
- **Updated**: 2017-10-17 12:10:37+00:00
- **Authors**: Ertugrul Bayraktar, Pinar Boyraz
- **Comment**: 11 pages, 3 figures, 1 table
- **Journal**: Turkish Journal of Electrical Engineering & Computer Sciences,
  (2017) 25: 2444 - 2454
- **Summary**: The purpose of this study is to provide a detailed performance comparison of feature detector/descriptor methods, particularly when their various combinations are used for image-matching. The localization experiments of a mobile robot in an indoor environment are presented as a case study. In these experiments, 3090 query images and 127 dataset images were used. This study includes five methods for feature detectors (features from accelerated segment test (FAST), oriented FAST and rotated binary robust independent elementary features (BRIEF) (ORB), speeded-up robust features (SURF), scale invariant feature transform (SIFT), and binary robust invariant scalable keypoints (BRISK)) and five other methods for feature descriptors (BRIEF, BRISK, SIFT, SURF, and ORB). These methods were used in 23 different combinations and it was possible to obtain meaningful and consistent comparison results using the performance criteria defined in this study. All of these methods were used independently and separately from each other as either feature detector or descriptor. The performance analysis shows the discriminative power of various combinations of detector and descriptor methods. The analysis is completed using five parameters: (i) accuracy, (ii) time, (iii) angle difference between keypoints, (iv) number of correct matches, and (v) distance between correctly matched keypoints. In a range of 60{\deg}, covering five rotational pose points for our system, the FAST-SURF combination had the lowest distance and angle difference values and the highest number of matched keypoints. SIFT-SURF was the most accurate combination with a 98.41% correct classification rate. The fastest algorithm was ORB-BRIEF, with a total running time of 21,303.30 s to match 560 images captured during motion with 127 dataset images.



### Real-time marker-less multi-person 3D pose estimation in RGB-Depth camera networks
- **Arxiv ID**: http://arxiv.org/abs/1710.06235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.06235v1)
- **Published**: 2017-10-17 12:27:23+00:00
- **Updated**: 2017-10-17 12:27:23+00:00
- **Authors**: Marco Carraro, Matteo Munaro, Jeff Burke, Emanuele Menegatti
- **Comment**: Submitted to the 2018 IEEE International Conference on Robotics and
  Automation
- **Journal**: None
- **Summary**: This paper proposes a novel system to estimate and track the 3D poses of multiple persons in calibrated RGB-Depth camera networks. The multi-view 3D pose of each person is computed by a central node which receives the single-view outcomes from each camera of the network. Each single-view outcome is computed by using a CNN for 2D pose estimation and extending the resulting skeletons to 3D by means of the sensor depth. The proposed system is marker-less, multi-person, independent of background and does not make any assumption on people appearance and initial pose. The system provides real-time outcomes, thus being perfectly suited for applications requiring user interaction. Experimental results show the effectiveness of this work with respect to a baseline multi-view approach in different scenarios. To foster research and applications based on this work, we released the source code in OpenPTrack, an open source project for RGB-D people tracking.



### Single Shot Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1710.06236v1
- **DOI**: 10.1145/3123266.3123343
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06236v1)
- **Published**: 2017-10-17 12:41:17+00:00
- **Updated**: 2017-10-17 12:41:17+00:00
- **Authors**: Tianwei Lin, Xu Zhao, Zheng Shou
- **Comment**: ACM Multimedia 2017
- **Journal**: None
- **Summary**: Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the "detection by classification" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.



### Procedural Modeling and Physically Based Rendering for Synthetic Data Generation in Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/1710.06270v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1710.06270v2)
- **Published**: 2017-10-17 13:38:16+00:00
- **Updated**: 2017-10-18 06:46:05+00:00
- **Authors**: Apostolia Tsirikoglou, Joel Kronander, Magnus Wrenninge, Jonas Unger
- **Comment**: The project web page at
  http://vcl.itn.liu.se/publications/2017/TKWU17/ contains a version of the
  paper with high-resolution images as well as additional material
- **Journal**: None
- **Summary**: We present an overview and evaluation of a new, systematic approach for generation of highly realistic, annotated synthetic data for training of deep neural networks in computer vision tasks. The main contribution is a procedural world modeling approach enabling high variability coupled with physically accurate image synthesis, and is a departure from the hand-modeled virtual worlds and approximate image synthesis methods used in real-time applications. The benefits of our approach include flexible, physically accurate and scalable image synthesis, implicit wide coverage of classes and features, and complete data introspection for annotations, which all contribute to quality and cost efficiency. To evaluate our approach and the efficacy of the resulting data, we use semantic segmentation for autonomous vehicles and robotic navigation as the main application, and we train multiple deep learning architectures using synthetic data with and without fine tuning on organic (i.e. real-world) data. The evaluation shows that our approach improves the neural network's performance and that even modest implementation efforts produce state-of-the-art results.



### Precision Learning: Reconstruction Filter Kernel Discretization
- **Arxiv ID**: http://arxiv.org/abs/1710.06287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06287v2)
- **Published**: 2017-10-17 13:57:00+00:00
- **Updated**: 2018-07-09 13:15:14+00:00
- **Authors**: Christopher Syben, Bernhard Stimpel, Katharina Breininger, Tobias Würfl, Rebecca Fahrig, Arnd Dörfler, Andreas Maier
- **Comment**: Accepted at The Fifth International Conference on Image Formation in
  X-Ray Computed Tomography
- **Journal**: None
- **Summary**: In this paper, we present substantial evidence that a deep neural network will intrinsically learn the appropriate way to discretize the ideal continuous reconstruction filter. Currently, the Ram-Lak filter or heuristic filters which impose different noise assumptions are used for filtered back-projection. All of these, however, inhibit a fully data-driven reconstruction deep learning approach. In addition, the heuristic filters are not chosen in an optimal sense. To tackle this issue, we propose a formulation to directly learn the reconstruction filter. The filter is initialized with the ideal Ramp filter as a strong pre-training and learned in frequency domain. We compare the learned filter with the Ram-Lak and the Ramp filter on a numerical phantom as well as on a real CT dataset. The results show that the network properly discretizes the continuous Ramp filter and converges towards the Ram-Lak solution. In our view these observations are interesting to gain a better understanding of deep learning techniques and traditional analytic techniques such as Wiener filtering and discretization theory. Furthermore, this will allow fully trainable data-driven reconstruction deep learning approaches.



### VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.06288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06288v1)
- **Published**: 2017-10-17 13:57:29+00:00
- **Updated**: 2017-10-17 13:57:29+00:00
- **Authors**: Seokju Lee, Junsik Kim, Jae Shin Yoon, Seunghak Shin, Oleksandr Bailo, Namil Kim, Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, In So Kweon
- **Comment**: To appear on ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we propose a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition that is guided by a vanishing point under adverse weather conditions. We tackle rainy and low illumination conditions, which have not been extensively studied until now due to clear challenges. For example, images taken under rainy days are subject to low illumination, while wet roads cause light reflection and distort the appearance of lane and road markings. At night, color distortion occurs under limited illumination. As a result, no benchmark dataset exists and only a few developed algorithms work under poor weather conditions. To address this shortcoming, we build up a lane and road marking benchmark which consists of about 20,000 images with 17 lane and road marking classes under four different scenarios: no rain, rain, heavy rain, and night. We train and evaluate several versions of the proposed multi-task network and validate the importance of each task. The resulting approach, VPGNet, can detect and classify lanes and road markings, and predict a vanishing point with a single forward pass. Experimental results show that our approach achieves high accuracy and robustness under various conditions in real-time (20 fps). The benchmark and the VPGNet model will be publicly available.



### Describing Natural Images Containing Novel Objects with Knowledge Guided Assitance
- **Arxiv ID**: http://arxiv.org/abs/1710.06303v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1710.06303v1)
- **Published**: 2017-10-17 14:11:37+00:00
- **Updated**: 2017-10-17 14:11:37+00:00
- **Authors**: Aditya Mogadala, Umanga Bista, Lexing Xie, Achim Rettinger
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Images in the wild encapsulate rich knowledge about varied abstract concepts and cannot be sufficiently described with models built only using image-caption pairs containing selected objects. We propose to handle such a task with the guidance of a knowledge base that incorporate many abstract concepts. Our method is a two-step process where we first build a multi-entity-label image recognition model to predict abstract concepts as image labels and then leverage them in the second step as an external semantic attention and constrained inference in the caption generation model for describing images that depict unseen/novel objects. Evaluations show that our models outperform most of the prior work for out-of-domain captioning on MSCOCO and are useful for integration of knowledge and vision in general.



### Towards CT-quality Ultrasound Imaging using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.06304v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1710.06304v1)
- **Published**: 2017-10-17 14:11:57+00:00
- **Updated**: 2017-10-17 14:11:57+00:00
- **Authors**: Sanketh Vedula, Ortal Senouf, Alex M. Bronstein, Oleg V. Michailovich, Michael Zibulevsky
- **Comment**: None
- **Journal**: None
- **Summary**: The cost-effectiveness and practical harmlessness of ultrasound imaging have made it one of the most widespread tools for medical diagnosis. Unfortunately, the beam-forming based image formation produces granular speckle noise, blurring, shading and other artifacts. To overcome these effects, the ultimate goal would be to reconstruct the tissue acoustic properties by solving a full wave propagation inverse problem. In this work, we make a step towards this goal, using Multi-Resolution Convolutional Neural Networks (CNN). As a result, we are able to reconstruct CT-quality images from the reflected ultrasound radio-frequency(RF) data obtained by simulation from real CT scans of a human body. We also show that CNN is able to imitate existing computationally heavy despeckling methods, thereby saving orders of magnitude in computations and making them amenable to real-time applications.



### Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.06319v2
- **DOI**: 10.22489/CinC.2017.363-223
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1710.06319v2)
- **Published**: 2017-10-17 14:39:17+00:00
- **Updated**: 2017-10-24 09:51:04+00:00
- **Authors**: Patrick Schwab, Gaetano Scebba, Jia Zhang, Marco Delai, Walter Karlen
- **Comment**: Accepted at Computing in Cardiology (CinC) 2017
- **Journal**: None
- **Summary**: With tens of thousands of electrocardiogram (ECG) records processed by mobile cardiac event recorders every day, heart rhythm classification algorithms are an important tool for the continuous monitoring of patients at risk. We utilise an annotated dataset of 12,186 single-lead ECG recordings to build a diverse ensemble of recurrent neural networks (RNNs) that is able to distinguish between normal sinus rhythms, atrial fibrillation, other types of arrhythmia and signals that are too noisy to interpret. In order to ease learning over the temporal dimension, we introduce a novel task formulation that harnesses the natural segmentation of ECG signals into heartbeats to drastically reduce the number of time steps per sequence. Additionally, we extend our RNNs with an attention mechanism that enables us to reason about which heartbeats our RNNs focus on to make their decisions. Through the use of attention, our model maintains a high degree of interpretability, while also achieving state-of-the-art classification performance with an average F1 score of 0.79 on an unseen test set (n=3,658).



### Embedded Spectral Descriptors: Learning the point-wise correspondence metric via Siamese neural networks
- **Arxiv ID**: http://arxiv.org/abs/1710.06368v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.06368v3)
- **Published**: 2017-10-17 16:26:04+00:00
- **Updated**: 2019-10-18 07:37:30+00:00
- **Authors**: Zhiyu Sun, Yusen He, Andrey Gritsenko, Amaury Lendasse, Stephen Baek
- **Comment**: None
- **Journal**: None
- **Summary**: A robust and informative local shape descriptor plays an important role in mesh registration. In this regard, spectral descriptors that are based on the spectrum of the Laplace-Beltrami operator have been a popular subject of research for the last decade due to their advantageous properties, such as isometry invariance. Despite such, however, spectral descriptors often fail to give a correct similarity measure for non-isometric cases where the metric distortion between the models is large. Hence, they are not reliable for correspondence matching problems when the models are not isometric. In this paper, it is proposed a method to improve the similarity metric of spectral descriptors for correspondence matching problems. We embed a spectral shape descriptor into a different metric space where the Euclidean distance between the elements directly indicates the geometric dissimilarity. We design and train a Siamese neural network to find such an embedding, where the embedded descriptors are promoted to rearrange based on the geometric similarity. We demonstrate our approach can significantly enhance the performance of the conventional spectral descriptors by the simple augmentation achieved via the Siamese neural network in comparison to other state-of-the-art methods.



### Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation
- **Arxiv ID**: http://arxiv.org/abs/1710.06422v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1710.06422v2)
- **Published**: 2017-10-17 17:54:50+00:00
- **Updated**: 2018-03-04 04:08:58+00:00
- **Authors**: Kuan Fang, Yunfei Bai, Stefan Hinterstoisser, Silvio Savarese, Mrinal Kalakrishnan
- **Comment**: ICRA 2018
- **Journal**: None
- **Summary**: Learning-based approaches to robotic manipulation are limited by the scalability of data collection and accessibility of labels. In this paper, we present a multi-task domain adaptation framework for instance grasping in cluttered scenes by utilizing simulated robot experiments. Our neural network takes monocular RGB images and the instance segmentation mask of a specified target object as inputs, and predicts the probability of successfully grasping the specified object for each candidate motor command. The proposed transfer learning framework trains a model for instance grasping in simulation and uses a domain-adversarial loss to transfer the trained model to real robots using indiscriminate grasping data, which is available both in simulation and the real world. We evaluate our model in real-world robot experiments, comparing it with alternative model architectures as well as an indiscriminate grasping baseline.



### Superpixels Based Marker Tracking Vs. Hue Thresholding In Rodent Biomechanics Application
- **Arxiv ID**: http://arxiv.org/abs/1710.06473v4
- **DOI**: 10.1109/ACSSC.2017.8335168
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06473v4)
- **Published**: 2017-10-17 19:08:29+00:00
- **Updated**: 2018-05-28 15:48:19+00:00
- **Authors**: Omid Haji Maghsoudi, Annie Vahedipour Tabrizi, Benjamin Robertson, Andrew Spence
- **Comment**: This paper has been accepted for 2017 Asilomar conference, IEEE
- **Journal**: None
- **Summary**: Examining locomotion has improved our basic understanding of motor control and aided in treating motor impairment. Mice and rats are premier models of human disease and increasingly the model systems of choice for basic neuroscience. High frame rates (250 Hz) are needed to quantify the kinematics of these running rodents. Manual tracking, especially for multiple markers, becomes time-consuming and impossible for large sample sizes. Therefore, the need for automatic segmentation of these markers has grown in recent years. We propose two methods to segment and track these markers: first, using SLIC superpixels segmentation with a tracker based on position, speed, shape, and color information of the segmented region in the previous frame; second, using a thresholding on hue channel following up with the same tracker. The comparison showed that the SLIC superpixels method was superior because the segmentation was more reliable and based on both color and spatial information.



### A Line-Point Unified Solution to Relative Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1710.06495v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.06495v1)
- **Published**: 2017-10-17 20:27:51+00:00
- **Updated**: 2017-10-17 20:27:51+00:00
- **Authors**: Ashraf Qadir, Jeremiah Neubert
- **Comment**: Submitted to ICRA 2018
- **Journal**: None
- **Summary**: In this work we present a unified method of relative camera pose estimation from points and lines correspondences. Given a set of 2D points and lines correspondences in three views, of which two are known, a method has been developed for estimating the camera pose of the third view. Novelty of this algorithm is to combine both points and lines correspondences in the camera pose estimation which enables us to compute relative camera pose with a small number of feature correspondences. Our central idea is to exploit the tri-linear relationship between three views and generate a set of linear equations from the points and lines correspondences in the three views. The desired solution to the system of equations are expressed as a linear combination of the singular vectors and the coefficients are computed by solving a small set of quadratic equations generated by imposing orthonormality constraints for general camera motion. The advantages of the proposed method are demonstrated by experimenting on publicly available data set. Results show the robustness and efficiency of the method in relative camera pose estimation for both small and large camera motion with a small set of points and line features.



### Do Convolutional Neural Networks Learn Class Hierarchy?
- **Arxiv ID**: http://arxiv.org/abs/1710.06501v1
- **DOI**: 10.1109/TVCG.2017.2744683
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1710.06501v1)
- **Published**: 2017-10-17 21:02:59+00:00
- **Updated**: 2017-10-17 21:02:59+00:00
- **Authors**: Bilal Alsallakh, Amin Jourabloo, Mao Ye, Xiaoming Liu, Liu Ren
- **Comment**: Video demo at https://vimeo.com/228263798
- **Journal**: IEEE Transactions on Visualization and Computer Graphics, Volume:
  24, Issue: 1 (2018)
- **Summary**: Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.



### Scene Parsing with Global Context Embedding
- **Arxiv ID**: http://arxiv.org/abs/1710.06507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06507v2)
- **Published**: 2017-10-17 21:36:03+00:00
- **Updated**: 2017-10-20 22:04:47+00:00
- **Authors**: Wei-Chih Hung, Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, Ming-Hsuan Yang
- **Comment**: Accepted in ICCV'17. Code available at
  https://github.com/hfslyc/GCPNet
- **Journal**: None
- **Summary**: We present a scene parsing method that utilizes global context information based on both the parametric and non- parametric models. Compared to previous methods that only exploit the local relationship between objects, we train a context network based on scene similarities to generate feature representations for global contexts. In addition, these learned features are utilized to generate global and spatial priors for explicit classes inference. We then design modules to embed the feature representations and the priors into the segmentation network as additional global context cues. We show that the proposed method can eliminate false positives that are not compatible with the global context representations. Experiments on both the MIT ADE20K and PASCAL Context datasets show that the proposed method performs favorably against existing methods.



### Multi-focus image fusion using VOL and EOL in DCT domain
- **Arxiv ID**: http://arxiv.org/abs/1710.06511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06511v2)
- **Published**: 2017-10-17 21:53:47+00:00
- **Updated**: 2017-10-24 20:24:36+00:00
- **Authors**: Mostafa Amin-Naji, Ali Aghagolzadeh
- **Comment**: 2016 1st International Conference on New Research Achievements in
  Electrical and Computer Engineering (ICNRAECE), pp. 728-733, 2016
- **Journal**: None
- **Summary**: The purpose of multi-focus image fusion is gathering the essential information and the focused parts from the input multi-focus images into a single image. These multi-focused images are captured with different depths of focus of cameras. Multi-focus image fusion is very time-saving and appropriate in discrete cosine transform (DCT) domain, especially when JPEG images are used in visual sensor networks (VSN). The previous works in DCT domain have some errors in selection of the suitable divided blocks according to their criterion for measurement of the block contrast. In this paper, we used variance of Laplacian (VOL) and energy of Laplacian (EOL) as criterion to measure the contrast of image. Also in this paper, the EOL and VOL calculations directly in DCT domain are prepared using vector processing. We developed four matrices which calculate the Laplacian of block easily in DCT domain. Our works greatly reduce error due to unsuitable block selection. The results of the proposed algorithms are compared with the previous algorithms in order to demonstrate the superiority of the output image quality in the proposed methods. The several JPEG multi-focus images are used in experiments and their fused image by our proposed methods and the other algorithms are compared with different measurement criteria.



### Pose-based Deep Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/1710.06512v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06512v3)
- **Published**: 2017-10-17 21:58:02+00:00
- **Updated**: 2018-02-08 13:04:40+00:00
- **Authors**: Anna Sokolova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Human gait or walking manner is a biometric feature that allows identification of a person when other biometric features such as the face or iris are not visible. In this paper, we present a new pose-based convolutional neural network model for gait recognition. Unlike many methods that consider the full-height silhouette of a moving person, we consider the motion of points in the areas around human joints. To extract motion information, we estimate the optical flow between consecutive frames. We propose a deep convolutional model that computes pose-based gait descriptors. We compare different network architectures and aggregation methods and experimentally assess various sets of body parts to determine which are the most important for gait recognition. In addition, we investigate the generalization ability of the developed algorithms by transferring them between datasets. The results of these experiments show that our approach outperforms state-of-the-art methods.



### Learning Pose Grammar to Encode Human Body Configuration for 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1710.06513v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1710.06513v6)
- **Published**: 2017-10-17 22:05:19+00:00
- **Updated**: 2018-01-04 22:50:45+00:00
- **Authors**: Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: In this paper, we propose a pose grammar to tackle the problem of 3D human pose estimation. Our model directly takes 2D pose as input and learns a generalized 2D-3D mapping function. The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNN) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a pose sample simulator to augment training samples in virtual camera views, which further improves our model generalizability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.



### Sistema de Navegação Autônomo Baseado em Visão Computacional
- **Arxiv ID**: http://arxiv.org/abs/1710.06518v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1710.06518v1)
- **Published**: 2017-10-17 22:37:07+00:00
- **Updated**: 2017-10-17 22:37:07+00:00
- **Authors**: Michel Conrado Cardoso Meneses
- **Comment**: in Portuguese. Thesis presented to the Federal University of Sergipe,
  at Sergipe, Brazil in partial fulfillment of the requirement for the degree
  of Bachelor of Science in Computer Engineering. A demonstration of this
  project can be watched by this link: https://youtu.be/hzyKAGhQExg Advisors:
  Dr. Leonardo Nogueira Matos, Dr. Bruno Otavio Piedade Prado
- **Journal**: None
- **Summary**: Autonomous robots are used as the tool to solve many kinds of problems, such as environmental mapping and monitoring. Either for adverse conditions related to the human presence or even for the need to reduce costs, it is certain that many efforts have been made to develop robots with an increasingly high level of autonomy. They must be capable of locomotion through dynamic environments, without human operators or assistant systems' help. It is noted, thus, that the form of perception and modeling of the environment becomes significantly relevant to navigation. Among the main sensing methods are those based on vision. Through this, it is possible to create highly-detailed models about the environment, since many characteristics can be measured, such as texture, color, and illumination. However, the most accurate vision-based navigation techniques are computationally expensive to run on low-cost mobile platforms. Therefore, the goal of this work was to develop a low-cost robot, controlled by a Raspberry Pi, whose navigation system is based on vision. For this purpose, the strategy used consisted in identifying obstacles via optical flow pattern recognition. Through this signal, it is possible to infer the relative displacement between the robot and other elements in the environment. Its estimation was done using the Lucas-Kanade algorithm, which can be executed by the Raspberry Pi without harming its performance. Finally, an SVM based classifier was used to identify patterns of this signal associated with obstacles movement. The developed system was evaluated considering its execution over an optical flow pattern dataset extracted from a real navigation environment. In the end, it was verified that the processing frequency of the system was superior to the others. Furthermore, its accuracy and acquisition cost were, respectively, higher and lower than most of the cited works.



### Material Classification using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1710.06854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.06854v1)
- **Published**: 2017-10-17 23:24:39+00:00
- **Updated**: 2017-10-17 23:24:39+00:00
- **Authors**: Anca Sticlaru
- **Comment**: 45 pages, BSc thesis
- **Journal**: None
- **Summary**: The recognition and classification of the diversity of materials that exist in the environment around us are a key visual competence that computer vision systems focus on in recent years. Understanding the identification of materials in distinct images involves a deep process that has made usage of the recent progress in neural networks which has brought the potential to train architectures to extract features for this challenging task. This project uses state-of-the-art Convolutional Neural Network (CNN) techniques and Support Vector Machine (SVM) classifiers in order to classify materials and analyze the results. Building on various widely used material databases collected, a selection of CNN architectures is evaluated to understand which is the best approach to extract features in order to achieve outstanding results for the task. The results gathered over four material datasets and nine CNNs outline that the best overall performance of a CNN using a linear SVM can achieve up to ~92.5% mean average precision, while applying a new relevant direction in computer vision, transfer learning. By limiting the amount of information extracted from the layer before the last fully connected layer, transfer learning aims at analyzing the contribution of shading information and reflectance to identify which main characteristics decide the material category the image belongs to. In addition to the main topic of my project, the evaluation of the nine different CNN architectures, it is questioned if, by using the transfer learning instead of extracting the information from the last convolutional layer, the total accuracy of the system created improves. The results of the comparison emphasize the fact that the accuracy and performance of the system improve, especially in the datasets which consist of a large number of images.




# Arxiv Papers in cs.CV on 2017-10-05
### Real-Time Illegal Parking Detection System Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1710.02546v1
- **DOI**: 10.1145/3094243.3094261
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1710.02546v1)
- **Published**: 2017-10-05 07:57:29+00:00
- **Updated**: 2017-10-05 07:57:29+00:00
- **Authors**: Xuemei Xie, Chenye Wang, Shu Chen, Guangming Shi, Zhifu Zhao
- **Comment**: 5pages,6figures
- **Journal**: None
- **Summary**: The increasing illegal parking has become more and more serious. Nowadays the methods of detecting illegally parked vehicles are based on background segmentation. However, this method is weakly robust and sensitive to environment. Benefitting from deep learning, this paper proposes a novel illegal vehicle parking detection system. Illegal vehicles captured by camera are firstly located and classified by the famous Single Shot MultiBox Detector (SSD) algorithm. To improve the performance, we propose to optimize SSD by adjusting the aspect ratio of default box to accommodate with our dataset better. After that, a tracking and analysis of movement is adopted to judge the illegal vehicles in the region of interest (ROI). Experiments show that the system can achieve a 99% accuracy and real-time (25FPS) detection with strong robustness in complex environments.



### A self-organizing neural network architecture for learning human-object interactions
- **Arxiv ID**: http://arxiv.org/abs/1710.01916v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1710.01916v2)
- **Published**: 2017-10-05 08:40:24+00:00
- **Updated**: 2018-03-02 09:00:41+00:00
- **Authors**: Luiza Mici, German I. Parisi, Stefan Wermter
- **Comment**: None
- **Journal**: None
- **Summary**: The visual recognition of transitive actions comprising human-object interactions is a key component for artificial systems operating in natural environments. This challenging task requires jointly the recognition of articulated body actions as well as the extraction of semantic elements from the scene such as the identity of the manipulated objects. In this paper, we present a self-organizing neural network for the recognition of human-object interactions from RGB-D videos. Our model consists of a hierarchy of Grow-When-Required (GWR) networks that learn prototypical representations of body motion patterns and objects, accounting for the development of action-object mappings in an unsupervised fashion. We report experimental results on a dataset of daily activities collected for the purpose of this study as well as on a publicly available benchmark dataset. In line with neurophysiological studies, our self-organizing architecture exhibits higher neural activation for congruent action-object pairs learned during training sessions with respect to synthetically created incongruent ones. We show that our unsupervised model shows competitive classification results on the benchmark dataset with respect to strictly supervised approaches.



### Plane-extraction from depth-data using a Gaussian mixture regression model
- **Arxiv ID**: http://arxiv.org/abs/1710.01925v4
- **DOI**: 10.1016/j.patrec.2018.03.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01925v4)
- **Published**: 2017-10-05 09:09:18+00:00
- **Updated**: 2018-03-30 08:42:12+00:00
- **Authors**: Richard T. Marriott, Alexander Paschevich, Radu Horaud
- **Comment**: 11 pages, 2 figures, 1 table
- **Journal**: Pattern Recognition Letters, 2018, 110, pp 44-50
- **Summary**: We propose a novel algorithm for unsupervised extraction of piecewise planar models from depth-data. Among other applications, such models are a good way of enabling autonomous agents (robots, cars, drones, etc.) to effectively perceive their surroundings and to navigate in three dimensions. We propose to do this by fitting the data with a piecewise-linear Gaussian mixture regression model whose components are skewed over planes, making them flat in appearance rather than being ellipsoidal, by embedding an outlier-trimming process that is formally incorporated into the proposed expectation-maximization algorithm, and by selectively fusing contiguous, coplanar components. Part of our motivation is an attempt to estimate more accurate plane-extraction by allowing each model component to make use of all available data through probabilistic clustering. The algorithm is thoroughly evaluated against a standard benchmark and is shown to rank among the best of the existing state-of-the-art methods.



### Semantic speech retrieval with a visually grounded model of untranscribed speech
- **Arxiv ID**: http://arxiv.org/abs/1710.01949v2
- **DOI**: 10.1109/TASLP.2018.2872106
- **Categories**: **cs.CL**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1710.01949v2)
- **Published**: 2017-10-05 10:24:46+00:00
- **Updated**: 2018-10-31 13:58:31+00:00
- **Authors**: Herman Kamper, Gregory Shakhnarovich, Karen Livescu
- **Comment**: 10 pages, 3 figures, 5 tables; accepted to the IEEE/ACM Transactions
  on Audio, Speech and Language Processing
- **Journal**: IEEE/ACM Transactions on Audio, Speech and Language Processing 27
  (2019) 89-98
- **Summary**: There is growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to (semantic) keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.



### Integrating Boundary and Center Correlation Filters for Visual Tracking with Aspect Ratio Variation
- **Arxiv ID**: http://arxiv.org/abs/1710.02039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02039v1)
- **Published**: 2017-10-05 14:19:50+00:00
- **Updated**: 2017-10-05 14:19:50+00:00
- **Authors**: Feng Li, Yingjie Yao, Peihua Li, David Zhang, Wangmeng Zuo, Ming-Hsuan Yang
- **Comment**: Accepted by ICCV 2017 workshop
- **Journal**: None
- **Summary**: The aspect ratio variation frequently appears in visual tracking and has a severe influence on performance. Although many correlation filter (CF)-based trackers have also been suggested for scale adaptive tracking, few studies have been given to handle the aspect ratio variation for CF trackers. In this paper, we make the first attempt to address this issue by introducing a family of 1D boundary CFs to localize the left, right, top, and bottom boundaries in videos. This allows us cope with the aspect ratio variation flexibly during tracking. Specifically, we present a novel tracking model to integrate 1D Boundary and 2D Center CFs (IBCCF) where boundary and center filters are enforced by a near-orthogonality regularization term. To optimize our IBCCF model, we develop an alternating direction method of multipliers. Experiments on several datasets show that IBCCF can effectively handle aspect ratio variation, and achieves state-of-the-art performance in terms of accuracy and robustness.



### Online Photometric Calibration for Auto Exposure Video for Realtime Visual Odometry and SLAM
- **Arxiv ID**: http://arxiv.org/abs/1710.02081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02081v1)
- **Published**: 2017-10-05 15:49:13+00:00
- **Updated**: 2017-10-05 15:49:13+00:00
- **Authors**: Paul Bergmann, Rui Wang, Daniel Cremers
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Recent direct visual odometry and SLAM algorithms have demonstrated impressive levels of precision. However, they require a photometric camera calibration in order to achieve competitive results. Hence, the respective algorithm cannot be directly applied to an off-the-shelf-camera or to a video sequence acquired with an unknown camera. In this work we propose a method for online photometric calibration which enables to process auto exposure videos with visual odometry precisions that are on par with those of photometrically calibrated videos. Our algorithm recovers the exposure times of consecutive frames, the camera response function, and the attenuation factors of the sensor irradiance due to vignetting. Gain robust KLT feature tracks are used to obtain scene point correspondences as input to a nonlinear optimization framework. We show that our approach can reliably calibrate arbitrary video sequences by evaluating it on datasets for which full photometric ground truth is available. We further show that our calibration can improve the performance of a state-of-the-art direct visual odometry method that works solely on pixel intensities, calibrating for photometric parameters in an online fashion in realtime.



### Anatomical Pattern Analysis for decoding visual stimuli in human brains
- **Arxiv ID**: http://arxiv.org/abs/1710.02113v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1710.02113v1)
- **Published**: 2017-10-05 16:57:21+00:00
- **Updated**: 2017-10-05 16:57:21+00:00
- **Authors**: Muhammad Yousefnezhad, Daoqiang Zhang
- **Comment**: Published in Cognitive Computation
- **Journal**: None
- **Summary**: Background: A universal unanswered question in neuroscience and machine learning is whether computers can decode the patterns of the human brain. Multi-Voxels Pattern Analysis (MVPA) is a critical tool for addressing this question. However, there are two challenges in the previous MVPA methods, which include decreasing sparsity and noise in the extracted features and increasing the performance of prediction.   Methods: In overcoming mentioned challenges, this paper proposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the human brain. This framework develops a novel anatomical feature extraction method and a new imbalance AdaBoost algorithm for binary classification. Further, it utilizes an Error-Correcting Output Codes (ECOC) method for multiclass prediction. APA can automatically detect active regions for each category of the visual stimuli. Moreover, it enables us to combine homogeneous datasets for applying advanced classification.   Results and Conclusions: Experimental studies on 4 visual categories (words, consonants, objects and scrambled photos) demonstrate that the proposed approach achieves superior performance to state-of-the-art methods.



### Multiframe Scene Flow with Piecewise Rigid Motion
- **Arxiv ID**: http://arxiv.org/abs/1710.02124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02124v1)
- **Published**: 2017-10-05 17:28:44+00:00
- **Updated**: 2017-10-05 17:28:44+00:00
- **Authors**: Vladislav Golyanik, Kihwan Kim, Robert Maier, Matthias Nie√üner, Didier Stricker, Jan Kautz
- **Comment**: International Conference on 3D Vision (3DV), Qingdao, China, October
  2017
- **Journal**: None
- **Summary**: We introduce a novel multiframe scene flow approach that jointly optimizes the consistency of the patch appearances and their local rigid motions from RGB-D image sequences. In contrast to the competing methods, we take advantage of an oversegmentation of the reference frame and robust optimization techniques. We formulate scene flow recovery as a global non-linear least squares problem which is iteratively solved by a damped Gauss-Newton approach. As a result, we obtain a qualitatively new level of accuracy in RGB-D based scene flow estimation which can potentially run in real-time. Our method can handle challenging cases with rigid, piecewise rigid, articulated and moderate non-rigid motion, and does not rely on prior knowledge about the types of motions and deformations. Extensive experiments on synthetic and real data show that our method outperforms state-of-the-art.



### DiffuserCam: Lensless Single-exposure 3D Imaging
- **Arxiv ID**: http://arxiv.org/abs/1710.02134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02134v1)
- **Published**: 2017-10-05 17:48:57+00:00
- **Updated**: 2017-10-05 17:48:57+00:00
- **Authors**: Nick Antipa, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan, Ren Ng, Laura Waller
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: We demonstrate a compact and easy-to-build computational camera for single-shot 3D imaging. Our lensless system consists solely of a diffuser placed in front of a standard image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D voxel grid is chosen to match the experimentally measured two-point optical resolution across the field-of-view, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide new theory for analyzing resolution in such systems.



### Tracking Persons-of-Interest via Unsupervised Representation Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1710.02139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02139v1)
- **Published**: 2017-10-05 17:58:28+00:00
- **Updated**: 2017-10-05 17:58:28+00:00
- **Authors**: Shun Zhang, Jia-Bin Huang, Jongwoo Lim, Yihong Gong, Jinjun Wang, Narendra Ahuja, Ming-Hsuan Yang
- **Comment**: Project page: http://vllab1.ucmerced.edu/~szhang/FaceTracking/
- **Journal**: None
- **Summary**: Multi-face tracking in unconstrained videos is a challenging problem as faces of one person often appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Existing multi-target tracking methods often use low-level features which are not sufficiently discriminative for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face representations using convolutional neural networks (CNNs). Unlike existing CNN-based approaches which are only trained on large-scale face image datasets offline, we use the contextual constraints to generate a large number of training samples for a given video, and further adapt the pre-trained face CNN to specific videos using discovered training samples. Using these training samples, we optimize the embedding space so that the Euclidean distances correspond to a measure of semantic face similarity via minimizing a triplet loss function. With the learned discriminative features, we apply the hierarchical clustering algorithm to link tracklets across multiple shots to generate trajectories. We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques.



### Video Denoising and Enhancement via Dynamic Video Layering
- **Arxiv ID**: http://arxiv.org/abs/1710.02213v1
- **DOI**: 10.1109/LSP.2018.2833429
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.02213v1)
- **Published**: 2017-10-05 20:54:56+00:00
- **Updated**: 2017-10-05 20:54:56+00:00
- **Authors**: Han Guo, Namrata Vaswani
- **Comment**: Shorter version with title "Video Denoising via Online Sparse and
  Low-rank Matrix Decomposition" appeared in Statistical Signal Processing
  Workshop (SSP) 2016
- **Journal**: None
- **Summary**: Video denoising refers to the problem of removing "noise" from a video sequence. Here the term "noise" is used in a broad sense to refer to any corruption or outlier or interference that is not the quantity of interest. In this work, we develop a novel approach to video denoising that is based on the idea that many noisy or corrupted videos can be split into three parts - the "low-rank layer", the "sparse layer", and a small residual (which is small and bounded). We show, using extensive experiments, that our denoising approach outperforms the state-of-the-art denoising algorithms.



### How Much Chemistry Does a Deep Neural Network Need to Know to Make Accurate Predictions?
- **Arxiv ID**: http://arxiv.org/abs/1710.02238v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1710.02238v2)
- **Published**: 2017-10-05 23:53:59+00:00
- **Updated**: 2018-03-18 14:03:12+00:00
- **Authors**: Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas, Nathan Baker
- **Comment**: In Proceedings of 2018 IEEE Winter Conference on Applications of
  Computer Vision (WACV)
- **Journal**: None
- **Summary**: The meteoric rise of deep learning models in computer vision research, having achieved human-level accuracy in image recognition tasks is firm evidence of the impact of representation learning of deep neural networks. In the chemistry domain, recent advances have also led to the development of similar CNN models, such as Chemception, that is trained to predict chemical properties using images of molecular drawings. In this work, we investigate the effects of systematically removing and adding localized domain-specific information to the image channels of the training data. By augmenting images with only 3 additional basic information, and without introducing any architectural changes, we demonstrate that an augmented Chemception (AugChemception) outperforms the original model in the prediction of toxicity, activity, and solvation free energy. Then, by altering the information content in the images, and examining the resulting model's performance, we also identify two distinct learning patterns in predicting toxicity/activity as compared to solvation free energy. These patterns suggest that Chemception is learning about its tasks in the manner that is consistent with established knowledge. Thus, our work demonstrates that advanced chemical knowledge is not a pre-requisite for deep learning models to accurately predict complex chemical properties.




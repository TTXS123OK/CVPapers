# Arxiv Papers in cs.CV on 2017-01-08
### Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies
- **Arxiv ID**: http://arxiv.org/abs/1701.01909v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01909v2)
- **Published**: 2017-01-08 03:29:26+00:00
- **Updated**: 2017-04-03 21:42:58+00:00
- **Authors**: Amir Sadeghian, Alexandre Alahi, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of existing solutions to the Multi-Target Tracking (MTT) problem do not combine cues in a coherent end-to-end fashion over a long period of time. However, we present an online method that encodes long-term temporal dependencies across multiple cues. One key challenge of tracking methods is to accurately track occluded targets or those which share similar appearance properties with surrounding objects. To address this challenge, we present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window. We are able to correct many data association errors and recover observations from an occluded state. We demonstrate the robustness of our data-driven approach by tracking multiple targets using their appearance, motion, and even interactions. Our method outperforms previous works on multiple publicly available datasets including the challenging MOT benchmark.



### Random Sampling for Fast Face Sketch Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1701.01911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01911v2)
- **Published**: 2017-01-08 03:47:59+00:00
- **Updated**: 2017-08-11 02:43:48+00:00
- **Authors**: Nannan Wang, Xinbo Gao, Jie Li
- **Comment**: None
- **Journal**: None
- **Summary**: Exemplar-based face sketch synthesis plays an important role in both digital entertainment and law enforcement. It generally consists of two parts: neighbor selection and reconstruction weight representation. The most time-consuming or main computation complexity for exemplar-based face sketch synthesis methods lies in the neighbor selection process. State-of-the-art face sketch synthesis methods perform neighbor selection online in a data-driven manner by $K$ nearest neighbor ($K$-NN) searching. Actually, the online search increases the time consuming for synthesis. Moreover, since these methods need to traverse the whole training dataset for neighbor selection, the computational complexity increases with the scale of the training database and hence these methods have limited scalability. In this paper, we proposed a simple but effective offline random sampling in place of online $K$-NN search to improve the synthesis efficiency. Extensive experiments on public face sketch databases demonstrate the superiority of the proposed method in comparison to state-of-the-art methods, in terms of both synthesis quality and time consumption. The proposed method could be extended to other heterogeneous face image transformation problems such as face hallucination. We release the source codes of our proposed methods and the evaluation metrics for future study online: http://www.ihitworld.com/RSLCR.html.



### On Classification of Distorted Images with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1701.01924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01924v1)
- **Published**: 2017-01-08 08:03:00+00:00
- **Updated**: 2017-01-08 08:03:00+00:00
- **Authors**: Yiren Zhou, Sibo Song, Ngai-Man Cheung
- **Comment**: 5 pages, 8 figures, ICASSP 2017
- **Journal**: None
- **Summary**: Image blur and image noise are common distortions during image acquisition. In this paper, we systematically study the effect of image distortions on the deep neural network (DNN) image classifiers. First, we examine the DNN classifier performance under four types of distortions. Second, we propose two approaches to alleviate the effect of image distortion: re-training and fine-tuning with noisy images. Our results suggest that, under certain conditions, fine-tuning with noisy images can alleviate much effect due to distorted inputs, and is more practical than re-training.



### Stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for Earth observation Level 2 product generation, Part 1 Theory
- **Arxiv ID**: http://arxiv.org/abs/1701.01930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01930v1)
- **Published**: 2017-01-08 09:26:49+00:00
- **Updated**: 2017-01-08 09:26:49+00:00
- **Authors**: Andrea Baraldi, Michael Laurence Humber, Dirk Tiede, Stefan Lang
- **Comment**: None
- **Journal**: None
- **Summary**: The European Space Agency (ESA) defines an Earth Observation (EO) Level 2 product as a multispectral (MS) image corrected for geometric, atmospheric, adjacency and topographic effects, stacked with its scene classification map (SCM), whose legend includes quality layers such as cloud and cloud-shadow. No ESA EO Level 2 product has ever been systematically generated at the ground segment. To contribute toward filling an information gap from EO big data to the ESA EO Level 2 product, an original Stage 4 validation (Val) of the Satellite Image Automatic Mapper (SIAM) lightweight computer program was conducted by independent means on an annual Web-Enabled Landsat Data (WELD) image composite time-series of the conterminous U.S. The core of SIAM is a one pass prior knowledge based decision tree for MS reflectance space hyperpolyhedralization into static color names presented in literature in recent years. For the sake of readability this paper is split into two. The present Part 1 Theory provides the multidisciplinary background of a priori color naming in cognitive science, from linguistics to computer vision. To cope with dictionaries of MS color names and land cover class names that do not coincide and must be harmonized, an original hybrid guideline is proposed to identify a categorical variable pair relationship. An original quantitative measure of categorical variable pair association is also proposed. The subsequent Part 2 Validation discusses Stage 4 Val results collected by an original protocol for wall-to-wall thematic map quality assessment without sampling where the test and reference map legends can differ. Conclusions are that the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the 4 class taxonomy of the FAO Land Cover Classification System at the Dichotomous Phase Level 1 vegetation/nonvegetation and Level 2 terrestrial/aquatic.



### Stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for Earth observation Level 2 product generation, Part 2 Validation
- **Arxiv ID**: http://arxiv.org/abs/1701.01932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01932v1)
- **Published**: 2017-01-08 09:35:30+00:00
- **Updated**: 2017-01-08 09:35:30+00:00
- **Authors**: Andrea Baraldi, Michael Laurence Humber, Dirk Tiede, Stefan Lang
- **Comment**: None
- **Journal**: None
- **Summary**: The European Space Agency (ESA) defines an Earth Observation (EO) Level 2 product as a multispectral (MS) image corrected for geometric, atmospheric, adjacency and topographic effects, stacked with its scene classification map (SCM) whose legend includes quality layers such as cloud and cloud-shadow. No ESA EO Level 2 product has ever been systematically generated at the ground segment. To contribute toward filling an information gap from EO big sensory data to the ESA EO Level 2 product, a Stage 4 validation (Val) of an off the shelf Satellite Image Automatic Mapper (SIAM) lightweight computer program for prior knowledge based MS color naming was conducted by independent means. A time-series of annual Web Enabled Landsat Data (WELD) image composites of the conterminous U.S. (CONUS) was selected as input dataset. The annual SIAM WELD maps of the CONUS were validated in comparison with the U.S. National Land Cover Data (NLCD) 2006 map. These test and reference maps share the same spatial resolution and spatial extent, but their map legends are not the same and must be harmonized. For the sake of readability this paper is split into two. The previous Part 1 Theory provided the multidisciplinary background of a priori color naming. The present Part 2 Validation presents and discusses Stage 4 Val results collected from the test SIAM WELD map time series and the reference NLCD map by an original protocol for wall to wall thematic map quality assessment without sampling, where the test and reference map legends can differ in agreement with the Part 1. Conclusions are that the SIAM-WELD maps instantiate a Level 2 SCM product whose legend is the FAO Land Cover Classification System (LCCS) taxonomy at the Dichotomous Phase (DP) Level 1 vegetation/nonvegetation, Level 2 terrestrial/aquatic or superior LCCS level.



### Automated Linear-Time Detection and Quality Assessment of Superpixels in Uncalibrated True- or False-Color RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1701.01940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01940v1)
- **Published**: 2017-01-08 11:09:59+00:00
- **Updated**: 2017-01-08 11:09:59+00:00
- **Authors**: Andrea Baraldi, Dirk Tiede, Stefan Lang
- **Comment**: None
- **Journal**: None
- **Summary**: Capable of automated near real time superpixel detection and quality assessment in an uncalibrated monitor typical red green blue (RGB) image, depicted in either true or false colors, an original low level computer vision (CV) lightweight computer program, called RGB Image Automatic Mapper (RGBIAM), is designed and implemented. Constrained by the Calibration Validation (CalVal) requirements of the Quality Assurance Framework for Earth Observation (QA4EO) guidelines, RGBIAM requires as mandatory an uncalibrated RGB image pre processing first stage, consisting of an automated statistical model based color constancy algorithm. The RGBIAM hybrid inference pipeline comprises: (I) a direct quantitative to nominal (QN) RGB variable transform, where RGB pixel values are mapped onto a prior dictionary of color names, equivalent to a static polyhedralization of the RGB cube. Prior color naming is the deductive counterpart of inductive vector quantization (VQ), whose typical VQ error function to minimize is a root mean square error (RMSE). In the output multi level color map domain, superpixels are automatically detected in linear time as connected sets of pixels featuring the same color label. (II) An inverse nominal to quantitative (NQ) RGB variable transform, where a superpixelwise constant RGB image approximation is generated in linear time to assess a VQ error image. The hybrid direct and inverse RGBIAM QNQ transform is: (i) general purpose, data and application independent. (ii) Automated, i.e., it requires no user machine interaction. (iii) Near real time, with a computational complexity increasing linearly with the image size. (iv) Implemented in tile streaming mode, to cope with massive images. Collected outcome and process quality indicators, including degree of automation, computational efficiency, VQ rate and VQ error, are consistent with theoretical expectations.



### Multi-Objective Software Suite of Two-Dimensional Shape Descriptors for Object-Based Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1701.01941v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01941v2)
- **Published**: 2017-01-08 11:16:43+00:00
- **Updated**: 2017-02-02 08:12:20+00:00
- **Authors**: Andrea Baraldi, Jo√£o V. B. Soares
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years two sets of planar (2D) shape attributes, provided with an intuitive physical meaning, were proposed to the remote sensing community by, respectively, Nagao & Matsuyama and Shackelford & Davis in their seminal works on the increasingly popular geographic object based image analysis (GEOBIA) paradigm. These two published sets of intuitive geometric features were selected as initial conditions by the present R&D software project, whose multi-objective goal was to accomplish: (i) a minimally dependent and maximally informative design (knowledge/information representation) of a general purpose, user and application independent dictionary of 2D shape terms provided with a physical meaning intuitive to understand by human end users and (ii) an effective (accurate, scale invariant, easy to use) and efficient implementation of 2D shape descriptors. To comply with the Quality Assurance Framework for Earth Observation guidelines, the proposed suite of geometric functions is validated by means of a novel quantitative quality assurance policy, centered on inter feature dependence (causality) assessment. This innovative multivariate feature validation strategy is alternative to traditional feature selection procedures based on either inductive data learning classification accuracy estimation, which is inherently case specific, or cross correlation estimation, because statistical cross correlation does not imply causation. The project deliverable is an original general purpose software suite of seven validated off the shelf 2D shape descriptors intuitive to use. Alternative to existing commercial or open source software libraries of tens of planar shape functions whose informativeness remains unknown, it is eligible for use in (GE)OBIA systems in operating mode, expected to mimic human reasoning based on a convergence of evidence approach.



### Multi-spectral Image Panchromatic Sharpening, Outcome and Process Quality Assessment Protocol
- **Arxiv ID**: http://arxiv.org/abs/1701.01942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1701.01942v1)
- **Published**: 2017-01-08 11:22:59+00:00
- **Updated**: 2017-01-08 11:22:59+00:00
- **Authors**: Andrea Baraldi, Francesca Despini, Sergio Teggi
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral (MS) image panchromatic (PAN) sharpening algorithms proposed to the remote sensing community are ever increasing in number and variety. Their aim is to sharpen a coarse spatial resolution MS image with a fine spatial resolution PAN image acquired simultaneously by a spaceborne or airborne Earth observation (EO) optical imaging sensor pair. Unfortunately, to date, no standard evaluation procedure for MS image PAN sharpening outcome and process is community agreed upon, in contrast with the Quality Assurance Framework for Earth Observation (QA4EO) guidelines proposed by the intergovernmental Group on Earth Observations (GEO). In general, process is easier to measure, outcome is more important. The original contribution of the present study is fourfold. First, existing procedures for quantitative quality assessment (Q2A) of the (sole) PAN sharpened MS product are critically reviewed. Their conceptual and implementation drawbacks are highlighted to be overcome for quality improvement. Second, a novel (to the best of these authors' knowledge, the first) protocol for Q2A of MS image PAN sharpening product and process is designed, implemented and validated by independent means. Third, within this protocol, an innovative categorization of spectral and spatial image quality indicators and metrics is presented. Fourth, according to this new taxonomy, an original third order isotropic multi scale gray level co occurrence matrix (TIMS GLCM) calculator and a TIMS GLCM texture feature extractor are proposed to replace popular second order GLCMs.



### A Framework for Wasserstein-1-Type Metrics
- **Arxiv ID**: http://arxiv.org/abs/1701.01945v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1701.01945v2)
- **Published**: 2017-01-08 11:31:30+00:00
- **Updated**: 2018-03-12 09:24:56+00:00
- **Authors**: Bernhard Schmitzer, Benedikt Wirth
- **Comment**: to appear in Journal of Convex Analysis
- **Journal**: None
- **Summary**: We propose a unifying framework for generalising the Wasserstein-1 metric to a discrepancy measure between nonnegative measures of different mass. This generalization inherits the convexity and computational efficiency from the Wasserstein-1 metric, and it includes several previous approaches from the literature as special cases. For various specific instances of the generalized Wasserstein-1 metric we furthermore demonstrate their usefulness in applications by numerical experiments.



### MS and PAN image fusion by combining Brovey and wavelet methods
- **Arxiv ID**: http://arxiv.org/abs/1701.01996v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1701.01996v1)
- **Published**: 2017-01-08 18:39:04+00:00
- **Updated**: 2017-01-08 18:39:04+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 9 pages; 2 figures, conference
- **Journal**: None
- **Summary**: Among the existing fusion algorithms, the wavelet fusion method is the most frequently discussed one in recent publications because the wavelet approach preserves the spectral characteristics of the multispectral image better than other methods. The Brovey is also a popular fusion method used for its ability in preserving the spatial information of the PAN image. This study presents a new fusion approach that integrates the advantages of both the Brovey (which preserves a high degree of spatial information) and the wavelet (which preserves a high degree of spectral information) techniques to reduce the colour distortion of fusion results. Visual and statistical analyzes show that the proposed algorithm clearly improves the merging quality in terms of: correlation coefficient and UIQI; compared to fusion methods including, IHS, Brovey, PCA , HPF, discrete wavelet transform (DWT), and a-trous wavelet.




# Arxiv Papers in cs.CV on 2017-02-25
### Adaptive Neural Networks for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/1702.07811v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.07811v2)
- **Published**: 2017-02-25 00:22:51+00:00
- **Updated**: 2017-09-18 18:14:49+00:00
- **Authors**: Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama
- **Comment**: None
- **Journal**: Proceedings of the 34th International Conference on Machine
  Learning, PMLR 70:527-536, 2017
- **Summary**: We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1%) loss of top5 accuracy.



### Synthesizing Training Data for Object Detection in Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1702.07836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1702.07836v2)
- **Published**: 2017-02-25 06:04:42+00:00
- **Updated**: 2017-09-08 00:29:55+00:00
- **Authors**: Georgios Georgakis, Arsalan Mousavian, Alexander C. Berg, Jana Kosecka
- **Comment**: Added more experiments and link to project webpage
- **Journal**: None
- **Summary**: Detection of objects in cluttered indoor environments is one of the key enabling functionalities for service robots. The best performing object detection approaches in computer vision exploit deep Convolutional Neural Networks (CNN) to simultaneously detect and categorize the objects of interest in cluttered scenes. Training of such models typically requires large amounts of annotated training data which is time consuming and costly to obtain. In this work we explore the ability of using synthetically generated composite images for training state-of-the-art object detectors, especially for object instance detection. We superimpose 2D images of textured object models into images of real environments at variety of locations and scales. Our experiments evaluate different superimposition strategies ranging from purely image-based blending all the way to depth and semantics informed positioning of the object models into real scenes. We demonstrate the effectiveness of these object detector training strategies on two publicly available datasets, the GMU-Kitchens and the Washington RGB-D Scenes v2. As one observation, augmenting some hand-labeled training data with synthetic examples carefully composed onto scenes yields object detectors with comparable performance to using much more hand-labeled data. Broadly, this work charts new opportunities for training detectors for new objects by exploiting existing object model repositories in either a purely automatic fashion or with only a very small number of human-annotated examples.



### Transfer Learning for Domain Adaptation in MRI: Application in Brain Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1702.07841v1
- **DOI**: 10.1007/978-3-319-66179-7_59
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07841v1)
- **Published**: 2017-02-25 07:04:25+00:00
- **Updated**: 2017-02-25 07:04:25+00:00
- **Authors**: Mohsen Ghafoorian, Alireza Mehrtash, Tina Kapur, Nico Karssemeijer, Elena Marchiori, Mehran Pesteie, Charles R. G. Guttmann, Frank-Erik de Leeuw, Clare M. Tempany, Bram van Ginneken, Andriy Fedorov, Purang Abolmaesumi, Bram Platel, William M. Wells III
- **Comment**: 8 pages, 3 figures
- **Journal**: Medical Image Computing and Computer-Assisted Intervention 2017,
  Vol 10435, 516-524
- **Summary**: Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis and treatment. However, variations in MRI acquisition protocols result in different appearances of normal and diseased tissue in the images. Convolutional neural networks (CNNs), which have shown to be successful in many medical image analysis tasks, are typically sensitive to the variations in imaging protocols. Therefore, in many cases, networks trained on data acquired with one MRI protocol, do not perform satisfactorily on data acquired with different protocols. This limits the use of models trained with large annotated legacy datasets on a new dataset with a different domain which is often a recurring situation in clinical settings. In this study, we aim to answer the following central questions regarding domain adaptation in medical image analysis: Given a fitted legacy model, 1) How much data from the new domain is required for a decent adaptation of the original network?; and, 2) What portion of the pre-trained model parameters should be retrained given a certain number of the new domain training samples? To address these questions, we conducted extensive experiments in white matter hyperintensity segmentation task. We trained a CNN on legacy MR images of brain and evaluated the performance of the domain-adapted network on the same task with images from a different domain. We then compared the performance of the model to the surrogate scenarios where either the same trained network is used or a new network is trained from scratch on the new dataset.The domain-adapted network tuned only by two training examples achieved a Dice score of 0.63 substantially outperforming a similar network trained on the same set of examples from scratch.



### An EM Based Probabilistic Two-Dimensional CCA with Application to Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1702.07884v1
- **DOI**: 10.1007/s10489-017-1012-2
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1702.07884v1)
- **Published**: 2017-02-25 12:50:35+00:00
- **Updated**: 2017-02-25 12:50:35+00:00
- **Authors**: Mehran Safayani, Seyed Hashem Ahmadi, Homayun Afrabandpey, Abdolreza Mirzaei
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, two-dimensional canonical correlation analysis (2DCCA) has been successfully applied for image feature extraction. The method instead of concatenating the columns of the images to the one-dimensional vectors, directly works with two-dimensional image matrices. Although 2DCCA works well in different recognition tasks, it lacks a probabilistic interpretation. In this paper, we present a probabilistic framework for 2DCCA called probabilistic 2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the parameters. Experimental results on synthetic and real data demonstrate superior performance in loading factor estimation for P2DCCA compared to 2DCCA. For real data, three subsets of AR face database and also the UMIST face database confirm the robustness of the proposed algorithm in face recognition tasks with different illumination conditions, facial expressions, poses and occlusions.



### Learning Deep NBNN Representations for Robust Place Categorization
- **Arxiv ID**: http://arxiv.org/abs/1702.07898v2
- **DOI**: 10.1109/LRA.2017.2705282
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1702.07898v2)
- **Published**: 2017-02-25 14:50:43+00:00
- **Updated**: 2017-05-04 16:33:32+00:00
- **Authors**: Massimiliano Mancini, Samuel Rota Bulò, Elisa Ricci, Barbara Caputo
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters, Vol. 2, n. 3, July 2017
- **Summary**: This paper presents an approach for semantic place categorization using data obtained from RGB cameras. Previous studies on visual place recognition and classification have shown that, by considering features derived from pre-trained Convolutional Neural Networks (CNNs) in combination with part-based classification models, high recognition accuracy can be achieved, even in presence of occlusions and severe viewpoint changes. Inspired by these works, we propose to exploit local deep representations, representing images as set of regions applying a Na\"{i}ve Bayes Nearest Neighbor (NBNN) model for image classification. As opposed to previous methods where CNNs are merely used as feature extractors, our approach seamlessly integrates the NBNN model into a fully-convolutional neural network. Experimental results show that the proposed algorithm outperforms previous methods based on pre-trained CNN models and that, when employed in challenging robot place recognition tasks, it is robust to occlusions, environmental and sensor changes.



### CHAOS: A Parallelization Scheme for Training Convolutional Neural Networks on Intel Xeon Phi
- **Arxiv ID**: http://arxiv.org/abs/1702.07908v1
- **DOI**: 10.1007/s11227-017-1994-x
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1702.07908v1)
- **Published**: 2017-02-25 15:48:44+00:00
- **Updated**: 2017-02-25 15:48:44+00:00
- **Authors**: Andre Viebke, Suejb Memeti, Sabri Pllana, Ajith Abraham
- **Comment**: The Journal of Supercomputing, 2017
- **Journal**: None
- **Summary**: Deep learning is an important component of big-data analytic tools and intelligent applications, such as, self-driving cars, computer vision, speech recognition, or precision medicine. However, the training process is computationally intensive, and often requires a large amount of time if performed sequentially. Modern parallel computing systems provide the capability to reduce the required training time of deep neural networks. In this paper, we present our parallelization scheme for training convolutional neural networks (CNN) named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS). Major features of CHAOS include the support for thread and vector parallelism, non-instant updates of weight parameters during back-propagation without a significant delay, and implicit synchronization in arbitrary order. CHAOS is tailored for parallel computing systems that are accelerated with the Intel Xeon Phi. We evaluate our parallelization approach empirically using measurement techniques and performance modeling for various numbers of threads and CNN architectures. Experimental results for the MNIST dataset of handwritten digits using the total number of threads on the Xeon Phi show speedups of up to 103x compared to the execution on one thread of the Xeon Phi, 14x compared to the sequential execution on Intel Xeon E5, and 58x compared to the sequential execution on Intel Core i5.



### Image Stitching by Line-guided Local Warping with Global Similarity Constraint
- **Arxiv ID**: http://arxiv.org/abs/1702.07935v2
- **DOI**: 10.1016/j.patcog.2018.06.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1702.07935v2)
- **Published**: 2017-02-25 18:15:51+00:00
- **Updated**: 2017-10-30 09:36:09+00:00
- **Authors**: Tian-Zhu Xiang, Gui-Song Xia, Xiang Bai, Liangpei Zhang
- **Comment**: None
- **Journal**: Pattern Recognition, Vol. 83, 2018, PP. 481-497
- **Summary**: Low-textured image stitching remains a challenging problem. It is difficult to achieve good alignment and it is easy to break image structures due to insufficient and unreliable point correspondences. Moreover, because of the viewpoint variations between multiple images, the stitched images suffer from projective distortions. To solve these problems, this paper presents a line-guided local warping method with a global similarity constraint for image stitching. Line features which serve well for geometric descriptions and scene constraints, are employed to guide image stitching accurately. On one hand, the line features are integrated into a local warping model through a designed weight function. On the other hand, line features are adopted to impose strong geometric constraints, including line correspondence and line colinearity, to improve the stitching performance through mesh optimization. To mitigate projective distortions, we adopt a global similarity constraint, which is integrated with the projective warps via a designed weight strategy. This constraint causes the final warp to slowly change from a projective to a similarity transformation across the image. Finally, the images undergo a two-stage alignment scheme that provides accurate alignment and reduces projective distortion. We evaluate our method on a series of images and compare it with several other methods. The experimental results demonstrate that the proposed method provides a convincing stitching performance and that it outperforms other state-of-the-art methods.



### BARCHAN: Blob Alignment for Robust CHromatographic ANalysis
- **Arxiv ID**: http://arxiv.org/abs/1702.07942v1
- **DOI**: 10.1016/j.chroma.2017.01.003
- **Categories**: **cs.CV**, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1702.07942v1)
- **Published**: 2017-02-25 19:59:39+00:00
- **Updated**: 2017-02-25 19:59:39+00:00
- **Authors**: Camille Couprie, Laurent Duval, Maxime Moreaud, Sophie Hénon, Mélinda Tebib, Vincent Souchon
- **Comment**: 15 pages, published in the Special issue for RIVA 2016, 40th
  International Symposium on Capillary Chromatography and 13th GCxGC Symposium
- **Journal**: Journal of Chromatography A, Volume 1484, February 2017, Pages
  65-72
- **Summary**: Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role into the elucidation of complex samples. The automation of the identification of peak areas is of prime interest to obtain a fast and repeatable analysis of chromatograms. To determine the concentration of compounds or pseudo-compounds, templates of blobs are defined and superimposed on a reference chromatogram. The templates then need to be modified when different chromatograms are recorded. In this study, we present a chromatogram and template alignment method based on peak registration called BARCHAN. Peaks are identified using a robust mathematical morphology tool. The alignment is performed by a probabilistic estimation of a rigid transformation along the first dimension, and a non-rigid transformation in the second dimension, taking into account noise, outliers and missing peaks in a fully automated way. Resulting aligned chromatograms and masks are presented on two datasets. The proposed algorithm proves to be fast and reliable. It significantly reduces the time to results for GCxGC analysis.




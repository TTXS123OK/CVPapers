# Arxiv Papers in cs.CV on 2017-04-25
### A Labeling-Free Approach to Supervising Deep Neural Networks for Retinal Blood Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.07502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07502v2)
- **Published**: 2017-04-25 01:04:21+00:00
- **Updated**: 2017-05-01 12:13:47+00:00
- **Authors**: Yongliang Chen
- **Comment**: 10 pages, 8 figures, 3 tables, forbidden work, correct the citation
  typo of [29]
- **Journal**: None
- **Summary**: Segmenting blood vessels in fundus imaging plays an important role in medical diagnosis. Many algorithms have been proposed. While deep Neural Networks have been attracting enormous attention from computer vision community recent years and several novel works have been done in terms of its application in retinal blood vessel segmentation, most of them are based on supervised learning which requires amount of labeled data, which is both scarce and expensive to obtain. We leverage the power of Deep Convolutional Neural Networks (DCNN) in feature learning, in this work, to achieve this ultimate goal. The highly efficient feature learning of DCNN inspires our novel approach that trains the networks with automatically-generated samples to achieve desirable performance on real-world fundus images. For this, we design a set of rules abstracted from the domain-specific prior knowledge to generate these samples. We argue that, with the high efficiency of DCNN in feature learning, one can achieve this goal by constructing the training dataset with prior knowledge, no manual labeling is needed. This approach allows us to take advantages of supervised learning without labeling. We also build a naive DCNN model to test it. The results on standard benchmarks of fundus imaging show it is competitive to the state-of-the-art methods which implies a potential way to leverage the power of DCNN in feature learning.



### Sharing deep generative representation for perceived image reconstruction from human brain activity
- **Arxiv ID**: http://arxiv.org/abs/1704.07575v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1704.07575v3)
- **Published**: 2017-04-25 08:20:42+00:00
- **Updated**: 2017-07-11 01:50:34+00:00
- **Authors**: Changde Du, Changying Du, Huiguang He
- **Comment**: None
- **Journal**: None
- **Summary**: Decoding human brain activities via functional magnetic resonance imaging (fMRI) has gained increasing attention in recent years. While encouraging results have been reported in brain states classification tasks, reconstructing the details of human visual experience still remains difficult. Two main challenges that hinder the development of effective models are the perplexing fMRI measurement noise and the high dimensionality of limited data instances. Existing methods generally suffer from one or both of these issues and yield dissatisfactory results. In this paper, we tackle this problem by casting the reconstruction of visual stimulus as the Bayesian inference of missing view in a multiview latent variable model. Sharing a common latent representation, our joint generative model of external stimulus and brain response is not only "deep" in extracting nonlinear features from visual images, but also powerful in capturing correlations among voxel activities of fMRI recordings. The nonlinearity and deep structure endow our model with strong representation ability, while the correlations of voxel activities are critical for suppressing noise and improving prediction. We devise an efficient variational Bayesian method to infer the latent variables and the model parameters. To further improve the reconstruction accuracy, the latent representations of testing instances are enforced to be close to that of their neighbours from the training set via posterior regularization. Experiments on three fMRI recording datasets demonstrate that our approach can more accurately reconstruct visual stimuli.



### Towards a quality metric for dense light fields
- **Arxiv ID**: http://arxiv.org/abs/1704.07576v1
- **DOI**: 10.1109/CVPR.2017.396
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07576v1)
- **Published**: 2017-04-25 08:21:47+00:00
- **Updated**: 2017-04-25 08:21:47+00:00
- **Authors**: Vamsi Kiran Adhikarla, Marek Vinkler, Denis Sumin, Rafał K. Mantiuk, Karol Myszkowski, Hans-Peter Seidel, Piotr Didyk
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2017
- **Summary**: Light fields become a popular representation of three dimensional scenes, and there is interest in their processing, resampling, and compression. As those operations often result in loss of quality, there is a need to quantify it. In this work, we collect a new dataset of dense reference and distorted light fields as well as the corresponding quality scores which are scaled in perceptual units. The scores were acquired in a subjective experiment using an interactive light-field viewing setup. The dataset contains typical artifacts that occur in light-field processing chain due to light-field reconstruction, multi-view compression, and limitations of automultiscopic displays. We test a number of existing objective quality metrics to determine how well they can predict the quality of light fields. We find that the existing image quality metrics provide good measures of light-field quality, but require dense reference light- fields for optimal performance. For more complex tasks of comparing two distorted light fields, their performance drops significantly, which reveals the need for new, light-field-specific metrics.



### Skeleton-based Action Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.07595v1
- **DOI**: 10.1109/LSP.2017.2678539
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07595v1)
- **Published**: 2017-04-25 09:09:00+00:00
- **Updated**: 2017-04-25 09:09:00+00:00
- **Authors**: Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu
- **Comment**: ICMEW 2017
- **Journal**: None
- **Summary**: Current state-of-the-art approaches to skeleton-based action recognition are mostly based on recurrent neural networks (RNN). In this paper, we propose a novel convolutional neural networks (CNN) based framework for both action classification and detection. Raw skeleton coordinates as well as skeleton motion are fed directly into CNN for label prediction. A novel skeleton transformer module is designed to rearrange and select important skeleton joints automatically. With a simple 7-layer network, we obtain 89.3% accuracy on validation set of the NTU RGB+D dataset. For action detection in untrimmed videos, we develop a window proposal network to extract temporal segment proposals, which are further classified within the same network. On the recent PKU-MMD dataset, we achieve 93.7% mAP, surpassing the baseline by a large margin.



### Joint Layout Estimation and Global Multi-View Registration for Indoor Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1704.07632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07632v2)
- **Published**: 2017-04-25 11:11:20+00:00
- **Updated**: 2017-09-06 11:48:16+00:00
- **Authors**: Jeong-Kyun Lee, Jae-Won Yea, Min-Gyu Park, Kuk-Jin Yoon
- **Comment**: Accepted to 2017 IEEE International Conference on Computer Vision
  (ICCV)
- **Journal**: None
- **Summary**: In this paper, we propose a novel method to jointly solve scene layout estimation and global registration problems for accurate indoor 3D reconstruction. Given a sequence of range data, we first build a set of scene fragments using KinectFusion and register them through pose graph optimization. Afterwards, we alternate between layout estimation and layout-based global registration processes in iterative fashion to complement each other. We extract the scene layout through hierarchical agglomerative clustering and energy-based multi-model fitting in consideration of noisy measurements. Having the estimated scene layout in one hand, we register all the range data through the global iterative closest point algorithm where the positions of 3D points that belong to the layout such as walls and a ceiling are constrained to be close to the layout. We experimentally verify the proposed method with the publicly available synthetic and real-world datasets in both quantitative and qualitative ways.



### Perivascular Spaces Segmentation in Brain MRI Using Optimal 3D Filtering
- **Arxiv ID**: http://arxiv.org/abs/1704.07699v1
- **DOI**: 10.1038/s41598-018-19781-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07699v1)
- **Published**: 2017-04-25 14:02:06+00:00
- **Updated**: 2017-04-25 14:02:06+00:00
- **Authors**: Lucia Ballerini, Ruggiero Lovreglio, Maria del C. Valdes-Hernandez, Joel Ramirez, Bradley J. MacIntosh, Sandra E. Black, Joanna M. Wardlaw
- **Comment**: None
- **Journal**: None
- **Summary**: Perivascular Spaces (PVS) are a recently recognised feature of Small Vessel Disease (SVD), also indicating neuroinflammation, and are an important part of the brain's circulation and glymphatic drainage system. Quantitative analysis of PVS on Magnetic Resonance Images (MRI) is important for understanding their relationship with neurological diseases. In this work, we propose a segmentation technique based on the 3D Frangi filtering for extraction of PVS from MRI. Based on prior knowledge from neuroradiological ratings of PVS, we used ordered logit models to optimise Frangi filter parameters in response to the variability in the scanner's parameters and study protocols. We optimized and validated our proposed models on two independent cohorts, a dementia sample (N=20) and patients who previously had mild to moderate stroke (N=48). Results demonstrate the robustness and generalisability of our segmentation method. Segmentation-based PVS burden estimates correlated with neuroradiological assessments (Spearman's $\rho$ = 0.74, p $<$ 0.001), suggesting the great potential of our proposed method



### Inception Recurrent Convolutional Neural Network for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.07709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07709v1)
- **Published**: 2017-04-25 14:19:26+00:00
- **Updated**: 2017-04-25 14:19:26+00:00
- **Authors**: Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha
- **Comment**: 11 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) are an influential tool for solving various problems in the machine learning and computer vision fields. In this paper, we introduce a new deep learning model called an Inception- Recurrent Convolutional Neural Network (IRCNN), which utilizes the power of an inception network combined with recurrent layers in DCNN architecture. We have empirically evaluated the recognition performance of the proposed IRCNN model using different benchmark datasets such as MNIST, CIFAR-10, CIFAR- 100, and SVHN. Experimental results show similar or higher recognition accuracy when compared to most of the popular DCNNs including the RCNN. Furthermore, we have investigated IRCNN performance against equivalent Inception Networks and Inception-Residual Networks using the CIFAR-100 dataset. We report about 3.5%, 3.47% and 2.54% improvement in classification accuracy when compared to the RCNN, equivalent Inception Networks, and Inception- Residual Networks on the augmented CIFAR- 100 dataset respectively.



### An ADMM Approach to Masked Signal Decomposition Using Subspace Representation
- **Arxiv ID**: http://arxiv.org/abs/1704.07711v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07711v2)
- **Published**: 2017-04-25 14:22:44+00:00
- **Updated**: 2018-12-25 21:11:43+00:00
- **Authors**: Shervin Minaee, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Signal decomposition is a classical problem in signal processing, which aims to separate an observed signal into two or more components each with its own property. Usually each component is described by its own subspace or dictionary. Extensive research has been done for the case where the components are additive, but in real world applications, the components are often non-additive. For example, an image may consist of a foreground object overlaid on a background, where each pixel either belongs to the foreground or the background. In such a situation, to separate signal components, we need to find a binary mask which shows the location of each component. Therefore it requires to solve a binary optimization problem. Since most of the binary optimization problems are intractable, we relax this problem to the approximated continuous problem, and solve it by alternating optimization technique. We show the application of the proposed algorithm for three applications: separation of text from background in images, separation of moving objects from a background undergoing global camera motion in videos, separation of sinusoidal and spike components in one dimensional signals. We demonstrate in each case that considering the non-additive nature of the problem can lead to significant improvement.



### Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units
- **Arxiv ID**: http://arxiv.org/abs/1704.07724v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07724v2)
- **Published**: 2017-04-25 14:56:19+00:00
- **Updated**: 2017-05-15 05:03:28+00:00
- **Authors**: Shaohuai Shi, Xiaowen Chu
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Rectifier neuron units (ReLUs) have been widely used in deep convolutional networks. An ReLU converts negative values to zeros, and does not change positive values, which leads to a high sparsity of neurons. In this work, we first examine the sparsity of the outputs of ReLUs in some popular deep convolutional architectures. And then we use the sparsity property of ReLUs to accelerate the calculation of convolution by skipping calculations of zero-valued neurons. The proposed sparse convolution algorithm achieves some speedup improvements on CPUs compared to the traditional matrix-matrix multiplication algorithm for convolution when the sparsity is not less than 0.9.



### Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.07754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07754v1)
- **Published**: 2017-04-25 15:54:56+00:00
- **Updated**: 2017-04-25 15:54:56+00:00
- **Authors**: Kuan-Lun Tseng, Yen-Liang Lin, Winston Hsu, Chung-Yang Huang
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Deep learning models such as convolutional neural net- work have been widely used in 3D biomedical segmentation and achieve state-of-the-art performance. However, most of them often adapt a single modality or stack multiple modalities as different input channels. To better leverage the multi- modalities, we propose a deep encoder-decoder structure with cross-modality convolution layers to incorporate different modalities of MRI data. In addition, we exploit convolutional LSTM to model a sequence of 2D slices, and jointly learn the multi-modalities and convolutional LSTM in an end-to-end manner. To avoid converging to the certain labels, we adopt a re-weighting scheme and two-phase training to handle the label imbalance. Experimental results on BRATS-2015 show that our method outperforms state-of-the-art biomedical segmentation approaches.



### Arabidopsis roots segmentation based on morphological operations and CRFs
- **Arxiv ID**: http://arxiv.org/abs/1704.07793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07793v1)
- **Published**: 2017-04-25 17:19:19+00:00
- **Updated**: 2017-04-25 17:19:19+00:00
- **Authors**: José Ignacio Orlando, Hugo Luis Manterola, Enzo Ferrante, Federico Ariel
- **Comment**: None
- **Journal**: None
- **Summary**: Arabidopsis thaliana is a plant species widely utilized by scientists to estimate the impact of genetic differences in root morphological features. For this purpose, images of this plant after genetic modifications are taken to study differences in the root architecture. This task requires manual segmentations of radicular structures, although this is a particularly tedious and time-consuming labor. In this work, we present an unsupervised method for Arabidopsis thaliana root segmentation based on morphological operations and fully-connected Conditional Random Fields. Although other approaches have been proposed to this purpose, all of them are based on more complex and expensive imaging modalities. Our results prove that our method can be easily applied over images taken using conventional scanners, with a minor user intervention. A first data set, our results and a fully open source implementation are available online.



### SfM-Net: Learning of Structure and Motion from Video
- **Arxiv ID**: http://arxiv.org/abs/1704.07804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07804v1)
- **Published**: 2017-04-25 17:30:05+00:00
- **Updated**: 2017-04-25 17:30:05+00:00
- **Authors**: Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose SfM-Net, a geometry-aware neural network for motion estimation in videos that decomposes frame-to-frame pixel motion in terms of scene and object depth, camera motion and 3D object rotations and translations. Given a sequence of frames, SfM-Net predicts depth, segmentation, camera and rigid object motions, converts those into a dense frame-to-frame motion field (optical flow), differentiably warps frames in time to match pixels and back-propagates. The model can be trained with various degrees of supervision: 1) self-supervised by the re-projection photometric error (completely unsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by depth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth estimates and successfully estimates frame-to-frame camera rotations and translations. It often successfully segments the moving objects in the scene, even though such supervision is never provided.



### Hand Keypoint Detection in Single Images using Multiview Bootstrapping
- **Arxiv ID**: http://arxiv.org/abs/1704.07809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07809v1)
- **Published**: 2017-04-25 17:37:48+00:00
- **Updated**: 2017-04-25 17:37:48+00:00
- **Authors**: Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.



### Unsupervised Learning of Depth and Ego-Motion from Video
- **Arxiv ID**: http://arxiv.org/abs/1704.07813v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07813v2)
- **Published**: 2017-04-25 17:44:33+00:00
- **Updated**: 2017-08-01 02:23:45+00:00
- **Authors**: Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe
- **Comment**: Accepted to CVPR 2017. Project webpage:
  https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/
- **Journal**: None
- **Summary**: We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings.



### Introspective Classification with Convolutional Nets
- **Arxiv ID**: http://arxiv.org/abs/1704.07816v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.07816v2)
- **Published**: 2017-04-25 17:49:03+00:00
- **Updated**: 2018-01-05 05:09:48+00:00
- **Authors**: Long Jin, Justin Lazarow, Zhuowen Tu
- **Comment**: 12 pages, 3 figure
- **Journal**: None
- **Summary**: We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.



### Introspective Generative Modeling: Decide Discriminatively
- **Arxiv ID**: http://arxiv.org/abs/1704.07820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.07820v1)
- **Published**: 2017-04-25 17:57:33+00:00
- **Updated**: 2017-04-25 17:57:33+00:00
- **Authors**: Justin Lazarow, Long Jin, Zhuowen Tu
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semi-supervised learning.



### Multi-View Dynamic Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.07863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.07863v2)
- **Published**: 2017-04-25 18:59:33+00:00
- **Updated**: 2018-08-20 13:05:29+00:00
- **Authors**: Andres Romero, Juan Leon, Pablo Arbelaez
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel convolutional neural network approach to address the fine-grained recognition problem of multi-view dynamic facial action unit detection. We leverage recent gains in large-scale object recognition by formulating the task of predicting the presence or absence of a specific action unit in a still image of a human face as holistic classification. We then explore the design space of our approach by considering both shared and independent representations for separate action units, and also different CNN architectures for combining color and motion information. We then move to the novel setup of the FERA 2017 Challenge, in which we propose a multi-view extension of our approach that operates by first predicting the viewpoint from which the video was taken, and then evaluating an ensemble of action unit detectors that were trained for that specific viewpoint. Our approach is holistic, efficient, and modular, since new action units can be easily included in the overall system. Our approach significantly outperforms the baseline of the FERA 2017 Challenge, with an absolute improvement of 14% on the F1-metric. Additionally, it compares favorably against the winner of the FERA 2017 challenge. Code source is available at https://github.com/BCV-Uniandes/AUNets.



### Adaptive Cost Function for Pointcloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1704.07910v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.07910v1)
- **Published**: 2017-04-25 21:25:13+00:00
- **Updated**: 2017-04-25 21:25:13+00:00
- **Authors**: Johan Ekekrantz, John Folkesson, Patric Jensfelt
- **Comment**: 10 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: In this paper we introduce an adaptive cost function for pointcloud registration. The algorithm automatically estimates the sensor noise, which is important for generalization across different sensors and environments. Through experiments on real and synthetic data, we show significant improvements in accuracy and robustness over state-of-the-art solutions.



### Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car
- **Arxiv ID**: http://arxiv.org/abs/1704.07911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1704.07911v1)
- **Published**: 2017-04-25 21:25:41+00:00
- **Updated**: 2017-04-25 21:25:41+00:00
- **Authors**: Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Lawrence Jackel, Urs Muller
- **Comment**: None
- **Journal**: None
- **Summary**: As part of a complete software stack for autonomous driving, NVIDIA has created a neural-network-based system, known as PilotNet, which outputs steering angles given images of the road ahead. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the necessary domain knowledge by observing human drivers. This eliminates the need for human engineers to anticipate what is important in an image and foresee all the necessary rules for safe driving. Road tests demonstrated that PilotNet can successfully perform lane keeping in a wide variety of driving conditions, regardless of whether lane markings are present or not.   The goal of the work described here is to explain what PilotNet learns and how it makes its decisions. To this end we developed a method for determining which elements in the road image most influence PilotNet's steering decision. Results show that PilotNet indeed learns to recognize relevant objects on the road.   In addition to learning the obvious features such as lane markings, edges of roads, and other cars, PilotNet learns more subtle features that would be hard to anticipate and program by engineers, for example, bushes lining the edge of the road and atypical vehicle classes.




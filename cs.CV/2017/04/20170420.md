# Arxiv Papers in cs.CV on 2017-04-20
### Fast Generation for Convolutional Autoregressive Models
- **Arxiv ID**: http://arxiv.org/abs/1704.06001v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.06001v1)
- **Published**: 2017-04-20 04:13:21+00:00
- **Updated**: 2017-04-20 04:13:21+00:00
- **Authors**: Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A. Hasegawa-Johnson, Roy H. Campbell, Thomas S. Huang
- **Comment**: Accepted at ICLR 2017 Workshop
- **Journal**: None
- **Summary**: Convolutional autoregressive models have recently demonstrated state-of-the-art performance on a number of generation tasks. While fast, parallel training methods have been crucial for their success, generation is typically implemented in a na\"{i}ve fashion where redundant computations are unnecessarily repeated. This results in slow generation, making such models infeasible for production environments. In this work, we describe a method to speed up generation in convolutional autoregressive models. The key idea is to cache hidden states to avoid redundant computation. We apply our fast generation method to the Wavenet and PixelCNN++ models and achieve up to $21\times$ and $183\times$ speedups respectively.



### BranchConnect: Large-Scale Visual Recognition with Learned Branch Connections
- **Arxiv ID**: http://arxiv.org/abs/1704.06010v3
- **DOI**: 10.1109/WACV.2018.00141
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06010v3)
- **Published**: 2017-04-20 04:48:58+00:00
- **Updated**: 2018-07-29 18:56:25+00:00
- **Authors**: Karim Ahmed, Lorenzo Torresani
- **Comment**: WACV 2018
- **Journal**: IEEE Winter Conference on Applications of Computer Vision (WACV)
  2018, pp. 1244-1253
- **Summary**: We introduce an architecture for large-scale image categorization that enables the end-to-end learning of separate visual features for the different classes to distinguish. The proposed model consists of a deep CNN shaped like a tree. The stem of the tree includes a sequence of convolutional layers common to all classes. The stem then splits into multiple branches implementing parallel feature extractors, which are ultimately connected to the final classification layer via learned gated connections. These learned gates determine for each individual class the subset of features to use. Such a scheme naturally encourages the learning of a heterogeneous set of specialized features through the separate branches and it allows each class to use the subset of features that are optimal for its recognition. We show the generality of our proposed method by reshaping several popular CNNs from the literature into our proposed architecture. Our experiments on the CIFAR100, CIFAR10, and Synth datasets show that in each case our resulting model yields a substantial improvement in accuracy over the original CNN. Our empirical analysis also suggests that our scheme acts as a form of beneficial regularization improving generalization performance.



### A Fuzzy Brute Force Matching Method for Binary Image Features
- **Arxiv ID**: http://arxiv.org/abs/1704.06018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06018v1)
- **Published**: 2017-04-20 05:29:06+00:00
- **Updated**: 2017-04-20 05:29:06+00:00
- **Authors**: Erkan Bostanci, Nadia Kanwal, Betul Bostanci, Mehmet Serdar Guzel
- **Comment**: None
- **Journal**: None
- **Summary**: Matching of binary image features is an important step in many different computer vision applications. Conventionally, an arbitrary threshold is used to identify a correct match from incorrect matches using Hamming distance which may improve or degrade the matching results for different input images. This is mainly due to the image content which is affected by the scene, lighting and imaging conditions. This paper presents a fuzzy logic based approach for brute force matching of image features to overcome this situation. The method was tested using a well-known image database with known ground truth. The approach is shown to produce a higher number of correct matches when compared against constant distance thresholds. The nature of fuzzy logic which allows the vagueness of information and tolerance to errors has been successfully exploited in an image processing context. The uncertainty arising from the imaging conditions has been overcome with the use of compact fuzzy matching membership functions.



### Enhancing Person Re-identification in a Self-trained Subspace
- **Arxiv ID**: http://arxiv.org/abs/1704.06020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06020v2)
- **Published**: 2017-04-20 05:43:05+00:00
- **Updated**: 2017-04-30 00:28:52+00:00
- **Authors**: Xun Yang, Meng Wang, Richang Hong, Qi Tian, Yong Rui
- **Comment**: Accepted by ACM Transactions on Multimedia Computing, Communications,
  and Applications (TOMM)
- **Journal**: None
- **Summary**: Despite the promising progress made in recent years, person re-identification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. For this challenging problem, a large variety of algorithms have been developed in the fully-supervised setting, requiring access to a large amount of labeled training data. However, the main bottleneck for fully-supervised re-ID is the limited availability of labeled training samples. To address this problem, in this paper, we propose a self-trained subspace learning paradigm for person re-ID which effectively utilizes both labeled and unlabeled data to learn a discriminative subspace where person images across disjoint camera views can be easily matched. The proposed approach first constructs pseudo pairwise relationships among unlabeled persons using the k-nearest neighbors algorithm. Then, with the pseudo pairwise relationships, the unlabeled samples can be easily combined with the labeled samples to learn a discriminative projection by solving an eigenvalue problem. In addition, we refine the pseudo pairwise relationships iteratively, which further improves the learning performance. A multi-kernel embedding strategy is also incorporated into the proposed approach to cope with the non-linearity in person's appearance and explore the complementation of multiple kernels. In this way, the performance of person re-ID can be greatly enhanced when training data are insufficient. Experimental results on six widely-used datasets demonstrate the effectiveness of our approach and its performance can be comparable to the reported results of most state-of-the-art fully-supervised methods while using much fewer labeled data.



### Predicting Cognitive Decline with Deep Learning of Brain Metabolism and Amyloid Imaging
- **Arxiv ID**: http://arxiv.org/abs/1704.06033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.06033v1)
- **Published**: 2017-04-20 07:33:18+00:00
- **Updated**: 2017-04-20 07:33:18+00:00
- **Authors**: Hongyoon Choi, Kyong Hwan Jin
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: For effective treatment of Alzheimer disease (AD), it is important to identify subjects who are most likely to exhibit rapid cognitive decline. Herein, we developed a novel framework based on a deep convolutional neural network which can predict future cognitive decline in mild cognitive impairment (MCI) patients using flurodeoxyglucose and florbetapir positron emission tomography (PET). The architecture of the network only relies on baseline PET studies of AD and normal subjects as the training dataset. Feature extraction and complicated image preprocessing including nonlinear warping are unnecessary for our approach. Accuracy of prediction (84.2%) for conversion to AD in MCI patients outperformed conventional feature-based quantification approaches. ROC analyses revealed that performance of CNN-based approach was significantly higher than that of the conventional quantification methods (p < 0.05). Output scores of the network were strongly correlated with the longitudinal change in cognitive measurements. These results show the feasibility of deep learning as a tool for predicting disease outcome using brain images.



### End-to-end representation learning for Correlation Filter based tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.06036v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.06036v1)
- **Published**: 2017-04-20 07:51:27+00:00
- **Updated**: 2017-04-20 07:51:27+00:00
- **Authors**: Jack Valmadre, Luca Bertinetto, João F. Henriques, Andrea Vedaldi, Philip H. S. Torr
- **Comment**: To appear at CVPR 2017
- **Journal**: None
- **Summary**: The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.



### Understanding the Mechanisms of Deep Transfer Learning for Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1704.06040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06040v1)
- **Published**: 2017-04-20 08:04:52+00:00
- **Updated**: 2017-04-20 08:04:52+00:00
- **Authors**: Hariharan Ravishankar, Prasad Sudhakar, Rahul Venkataramani, Sheshadri Thiruvenkadam, Pavan Annangi, Narayanan Babu, Vivek Vaidya
- **Comment**: Published in MICCAI Workshop on Deep Learning in Medical Image
  Analysis, 2016
- **Journal**: None
- **Summary**: The ability to automatically learn task specific feature representations has led to a huge success of deep learning methods. When large training data is scarce, such as in medical imaging problems, transfer learning has been very effective. In this paper, we systematically investigate the process of transferring a Convolutional Neural Network, trained on ImageNet images to perform image classification, to kidney detection problem in ultrasound images. We study how the detection performance depends on the extent of transfer. We show that a transferred and tuned CNN can outperform a state-of-the-art feature engineered pipeline and a hybridization of these two techniques achieves 20\% higher performance. We also investigate how the evolution of intermediate response images from our network. Finally, we compare these responses to state-of-the-art image processing filters in order to gain greater insight into how transfer learning is able to effectively manage widely varying imaging regimes.



### End-to-End Unsupervised Deformable Image Registration with a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1704.06065v1
- **DOI**: 10.1007/978-3-319-67558-9_24
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06065v1)
- **Published**: 2017-04-20 09:40:50+00:00
- **Updated**: 2017-04-20 09:40:50+00:00
- **Authors**: Bob D. de Vos, Floris F. Berendsen, Max A. Viergever, Marius Staring, Ivana Išgum
- **Comment**: None
- **Journal**: DLMIA/ML-CDS@MICCAI 2017
- **Summary**: In this work we propose a deep learning network for deformable image registration (DIRNet). The DIRNet consists of a convolutional neural network (ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet analyzes a pair of fixed and moving images and outputs parameters for the spatial transformer, which generates the displacement vector field that enables the resampler to warp the moving image to the fixed image. The DIRNet is trained end-to-end by unsupervised optimization of a similarity metric between input image pairs. A trained DIRNet can be applied to perform registration on unseen image pairs in one pass, thus non-iteratively. Evaluation was performed with registration of images of handwritten digits (MNIST) and cardiac cine MR scans (Sunnybrook Cardiac Data). The results demonstrate that registration with DIRNet is as accurate as a conventional deformable image registration method with substantially shorter execution times.



### Using Mise-En-Scène Visual Features based on MPEG-7 and Deep Learning for Movie Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1704.06109v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1704.06109v1)
- **Published**: 2017-04-20 12:33:48+00:00
- **Updated**: 2017-04-20 12:33:48+00:00
- **Authors**: Yashar Deldjoo, Massimo Quadrana, Mehdi Elahi, Paolo Cremonesi
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Item features play an important role in movie recommender systems, where recommendations can be generated by using explicit or implicit preferences of users on traditional features (attributes) such as tag, genre, and cast. Typically, movie features are human-generated, either editorially (e.g., genre and cast) or by leveraging the wisdom of the crowd (e.g., tag), and as such, they are prone to noise and are expensive to collect. Moreover, these features are often rare or absent for new items, making it difficult or even impossible to provide good quality recommendations.   In this paper, we show that user's preferences on movies can be better described in terms of the mise-en-sc\`ene features, i.e., the visual aspects of a movie that characterize design, aesthetics and style (e.g., colors, textures). We use both MPEG-7 visual descriptors and Deep Learning hidden layers as example of mise-en-sc\`ene features that can visually describe movies. Interestingly, mise-en-sc\`ene features can be computed automatically from video files or even from trailers, offering more flexibility in handling new items, avoiding the need for costly and error-prone human-based tagging, and providing good scalability.   We have conducted a set of experiments on a large catalogue of 4K movies. Results show that recommendations based on mise-en-sc\`ene features consistently provide the best performance with respect to richer sets of more traditional features, such as genre and tag.



### Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.06176v5
- **DOI**: 10.1038/s41598-018-34817-6
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.06176v5)
- **Published**: 2017-04-20 14:54:29+00:00
- **Updated**: 2019-02-05 14:46:00+00:00
- **Authors**: Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, James S. Babb, Stephen Honig, Kyunghyun Cho, Gregory Chang
- **Comment**: This is a pre-print of an article published in Scientific Reports.
  The final authenticated version is available online at:
  https://doi.org/10.1038/s41598-018-34817-6
- **Journal**: Scientific Reports, volume 8, Article number: 16485 (2018)
- **Summary**: Magnetic resonance imaging (MRI) has been proposed as a complimentary method to measure bone quality and assess fracture risk. However, manual segmentation of MR images of bone is time-consuming, limiting the use of MRI measurements in the clinical practice. The purpose of this paper is to present an automatic proximal femur segmentation method that is based on deep convolutional neural networks (CNNs). This study had institutional review board approval and written informed consent was obtained from all subjects. A dataset of volumetric structural MR images of the proximal femur from 86 subject were manually-segmented by an expert. We performed experiments by training two different CNN architectures with multiple number of initial feature maps and layers, and tested their segmentation performance against the gold standard of manual segmentations using four-fold cross-validation. Automatic segmentation of the proximal femur achieved a high dice similarity score of 0.94$\pm$0.05 with precision = 0.95$\pm$0.02, and recall = 0.94$\pm$0.08 using a CNN architecture based on 3D convolution exceeding the performance of 2D CNNs. The high segmentation accuracy provided by CNNs has the potential to help bring the use of structural MRI measurements of bone quality into clinical practice for management of osteoporosis.



### Exploring epoch-dependent stochastic residual networks
- **Arxiv ID**: http://arxiv.org/abs/1704.06178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06178v1)
- **Published**: 2017-04-20 15:08:28+00:00
- **Updated**: 2017-04-20 15:08:28+00:00
- **Authors**: Fabio Carrara, Andrea Esuli, Fabrizio Falchi, Alejandro Moreo Fernández
- **Comment**: Preliminary report
- **Journal**: None
- **Summary**: The recently proposed stochastic residual networks selectively activate or bypass the layers during training, based on independent stochastic choices, each of which following a probability distribution that is fixed in advance. In this paper we present a first exploration on the use of an epoch-dependent distribution, starting with a higher probability of bypassing deeper layers and then activating them more frequently as training progresses. Preliminary results are mixed, yet they show some potential of adding an epoch-dependent management of distributions, worth of further investigation.



### Training object class detectors with click supervision
- **Arxiv ID**: http://arxiv.org/abs/1704.06189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06189v2)
- **Published**: 2017-04-20 15:31:48+00:00
- **Updated**: 2017-05-19 17:19:38+00:00
- **Authors**: Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Training object class detectors typically requires a large set of images with objects annotated by bounding boxes. However, manually drawing bounding boxes is very time consuming. In this paper we greatly reduce annotation time by proposing center-click annotations: we ask annotators to click on the center of an imaginary bounding box which tightly encloses the object instance. We then incorporate these clicks into existing Multiple Instance Learning techniques for weakly supervised object localization, to jointly localize object bounding boxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS COCO show that: (1) our scheme delivers high-quality detectors, performing substantially better than those produced by weakly supervised techniques, with a modest extra annotation effort; (2) these detectors in fact perform in a range close to those trained from manually drawn bounding boxes; (3) as the center-click task is very fast, our scheme reduces total annotation time by 9x to 18x.



### Temporal Action Detection with Structured Segment Networks
- **Arxiv ID**: http://arxiv.org/abs/1704.06228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06228v2)
- **Published**: 2017-04-20 16:51:45+00:00
- **Updated**: 2017-09-18 08:43:11+00:00
- **Authors**: Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, Dahua Lin
- **Comment**: To appear in ICCV2017. Code & models available at
  http://yjxiong.me/others/ssn
- **Journal**: None
- **Summary**: Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.



### Towards Large-Pose Face Frontalization in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1704.06244v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06244v3)
- **Published**: 2017-04-20 17:36:41+00:00
- **Updated**: 2017-08-17 18:12:07+00:00
- **Authors**: Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, Manmohan Chandraker
- **Comment**: To appear at ICCV2017. Details refer to
  http://cvlab.cse.msu.edu/project-face-frontalization.html
- **Journal**: None
- **Summary**: Despite recent advances in face recognition using deep learning, severe accuracy drops are observed for large pose variations in unconstrained environments. Learning pose-invariant features is one solution, but needs expensively labeled large-scale data and carefully designed feature learning algorithms. In this work, we focus on frontalizing faces in the wild under various head poses, including extreme profile views. We propose a novel deep 3D Morphable Model (3DMM) conditioned Face Frontalization Generative Adversarial Network (GAN), termed as FF-GAN, to generate neutral head pose face images. Our framework differs from both traditional GANs and 3DMM based modeling. Incorporating 3DMM into the GAN structure provides shape and appearance priors for fast convergence with less training data, while also supporting end-to-end training. The 3DMM-conditioned GAN employs not only the discriminator and generator loss but also a new masked symmetry loss to retain visual quality under occlusions, besides an identity loss to recover high frequency information. Experiments on face recognition, landmark localization and 3D reconstruction consistently show the advantage of our frontalization method on faces in the wild datasets.



### Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency
- **Arxiv ID**: http://arxiv.org/abs/1704.06254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06254v1)
- **Published**: 2017-04-20 17:56:53+00:00
- **Updated**: 2017-04-20 17:56:53+00:00
- **Authors**: Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, Jitendra Malik
- **Comment**: To appear at CVPR 2017. Project webpage :
  https://shubhtuls.github.io/drc/
- **Journal**: None
- **Summary**: We study the notion of consistency between a 3D shape and a 2D observation and propose a differentiable formulation which allows computing gradients of the 3D shape given an observation from an arbitrary view. We do so by reformulating view consistency using a differentiable ray consistency (DRC) term. We show that this formulation can be incorporated in a learning framework to leverage different types of multi-view observations e.g. foreground masks, depth, color images, semantics etc. as supervision for learning single-view 3D prediction. We present empirical analysis of our technique in a controlled setting. We also show that this approach allows us to improve over existing techniques for single-view reconstruction of objects from the PASCAL VOC dataset.



### Efficient Gender Classification Using a Deep LDA-Pruned Net
- **Arxiv ID**: http://arxiv.org/abs/1704.06305v3
- **DOI**: 10.1109/CVPRW.2017.78
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06305v3)
- **Published**: 2017-04-20 19:06:55+00:00
- **Updated**: 2018-10-23 12:40:11+00:00
- **Authors**: Qing Tian, Tal Arbel, James J. Clark
- **Comment**: The only difference with the previous version v2 is the title on the
  arxiv page. I am changing it back to the original title in v1 because
  otherwise google scholar cannot track the citations to this arxiv paper
  correctly. You could cite either the conference version or this arxiv
  version. They are equivalent
- **Journal**: 2017 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Summary**: Many real-time tasks, such as human-computer interaction, require fast and efficient facial gender classification. Although deep CNN nets have been very effective for a multitude of classification tasks, their high space and time demands make them impractical for personal computers and mobile devices without a powerful GPU. In this paper, we develop a 16-layer, yet lightweight, neural network which boosts efficiency while maintaining high accuracy. Our net is pruned from the VGG-16 model starting from the last convolutional (conv) layer where we find neuron activations are highly uncorrelated given the gender. Through Fisher's Linear Discriminant Analysis (LDA), we show that this high decorrelation makes it safe to discard directly last conv layer neurons with high within-class variance and low between-class variance. Combined with either Support Vector Machines (SVM) or Bayesian classification, the reduced CNNs are capable of achieving comparable (or even higher) accuracies on the LFW and CelebA datasets than the original net with fully connected layers. On LFW, only four Conv5_3 neurons are able to maintain a comparably high recognition accuracy, which results in a reduction of total network size by a factor of 70X with a 11 fold speedup. Comparisons with a state-of-the-art pruning method as well as two smaller nets in terms of accuracy loss and convolutional layers pruning rate are also provided.



### Good Features to Correlate for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1704.06326v2
- **DOI**: 10.1109/TIP.2018.2806280
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06326v2)
- **Published**: 2017-04-20 20:24:50+00:00
- **Updated**: 2018-03-10 12:43:55+00:00
- **Authors**: Erhan Gundogdu, A. Aydin Alatan
- **Comment**: Accepted version of IEEE Transactions on Image Processing
- **Journal**: IEEE Transactions on Image Processing, vol. 27, no. 5, pp.
  2526-2540, May 2018
- **Summary**: During the recent years, correlation filters have shown dominant and spectacular results for visual object tracking. The types of the features that are employed in these family of trackers significantly affect the performance of visual tracking. The ultimate goal is to utilize robust features invariant to any kind of appearance change of the object, while predicting the object location as properly as in the case of no appearance change. As the deep learning based methods have emerged, the study of learning features for specific tasks has accelerated. For instance, discriminative visual tracking methods based on deep architectures have been studied with promising performance. Nevertheless, correlation filter based (CFB) trackers confine themselves to use the pre-trained networks which are trained for object classification problem. To this end, in this manuscript the problem of learning deep fully convolutional features for the CFB visual tracking is formulated. In order to learn the proposed model, a novel and efficient backpropagation algorithm is presented based on the loss function of the network. The proposed learning framework enables the network model to be flexible for a custom design. Moreover, it alleviates the dependency on the network trained for classification. Extensive performance analysis shows the efficacy of the proposed custom design in the CFB tracking framework. By fine-tuning the convolutional parts of a state-of-the-art network and integrating this model to a CFB tracker, which is the top performing one of VOT2016, 18% increase is achieved in terms of expected average overlap, and tracking failures are decreased by 25%, while maintaining the superiority over the state-of-the-art methods in OTB-2013 and OTB-2015 tracking datasets.



### Identifying First-person Camera Wearers in Third-person Videos
- **Arxiv ID**: http://arxiv.org/abs/1704.06340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.06340v1)
- **Published**: 2017-04-20 21:16:26+00:00
- **Updated**: 2017-04-20 21:16:26+00:00
- **Authors**: Chenyou Fan, Jangwon Lee, Mingze Xu, Krishna Kumar Singh, Yong Jae Lee, David J. Crandall, Michael S. Ryoo
- **Comment**: None
- **Journal**: None
- **Summary**: We consider scenarios in which we wish to perform joint scene understanding, object tracking, activity recognition, and other tasks in environments in which multiple people are wearing body-worn cameras while a third-person static camera also captures the scene. To do this, we need to establish person-level correspondences across first- and third-person videos, which is challenging because the camera wearer is not visible from his/her own egocentric video, preventing the use of direct feature matching. In this paper, we propose a new semi-Siamese Convolutional Neural Network architecture to address this novel challenge. We formulate the problem as learning a joint embedding space for first- and third-person videos that considers both spatial- and motion-domain cues. A new triplet loss function is designed to minimize the distance between correct first- and third-person matches while maximizing the distance between incorrect ones. This end-to-end approach performs significantly better than several baselines, in part by learning the first- and third-person features optimized for matching jointly with the distance measure itself.



### An Optimal Dimensionality Multi-shell Sampling Scheme with Accurate and Efficient Transforms for Diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1705.04336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1705.04336v1)
- **Published**: 2017-04-20 22:08:45+00:00
- **Updated**: 2017-04-20 22:08:45+00:00
- **Authors**: Alice P. Bates, Zubair Khalid, Jason D. McEwen, Rodney A. Kennedy
- **Comment**: 4 pages, 4 figures presented at ISBI 2017
- **Journal**: None
- **Summary**: This paper proposes a multi-shell sampling scheme and corresponding transforms for the accurate reconstruction of the diffusion signal in diffusion MRI by expansion in the spherical polar Fourier (SPF) basis. The sampling scheme uses an optimal number of samples, equal to the degrees of freedom of the band-limited diffusion signal in the SPF domain, and allows for computationally efficient reconstruction. We use synthetic data sets to demonstrate that the proposed scheme allows for greater reconstruction accuracy of the diffusion signal than the multi-shell sampling schemes obtained using the generalised electrostatic energy minimisation (gEEM) method used in the Human Connectome Project. We also demonstrate that the proposed sampling scheme allows for increased angular discrimination and improved rotational invariance of reconstruction accuracy than the gEEM schemes.



### Hard Mixtures of Experts for Large Scale Weakly Supervised Vision
- **Arxiv ID**: http://arxiv.org/abs/1704.06363v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1704.06363v1)
- **Published**: 2017-04-20 23:45:27+00:00
- **Updated**: 2017-04-20 23:45:27+00:00
- **Authors**: Sam Gross, Marc'Aurelio Ranzato, Arthur Szlam
- **Comment**: Appearing in CVPR 2017
- **Journal**: None
- **Summary**: Training convolutional networks (CNN's) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice. However, there is still no effective method for training large CNN's that do not fit in the memory of a few GPU cards, or for parallelizing CNN training. In this work we show that a simple hard mixture of experts model can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks. Mixture of experts models are not new (Jacobs et. al. 1991, Collobert et. al. 2003), but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation. We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert. Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model. Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space. We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.




# Arxiv Papers in cs.CV on 2017-04-19
### Illuminant Spectra-based Source Separation Using Flash Photography
- **Arxiv ID**: http://arxiv.org/abs/1704.05564v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05564v2)
- **Published**: 2017-04-19 00:09:12+00:00
- **Updated**: 2017-11-27 00:37:26+00:00
- **Authors**: Zhuo Hui, Kalyan Sunkavalli, Sunil Hadap, Aswin C. Sankaranarayanan
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world lighting often consists of multiple illuminants with different spectra. Separating and manipulating these illuminants in post-process is a challenging problem that requires either significant manual input or calibrated scene geometry and lighting. In this work, we leverage a flash/no-flash image pair to analyze and edit scene illuminants based on their spectral differences. We derive a novel physics-based relationship between color variations in the observed flash/no-flash intensities and the spectra and surface shading corresponding to individual scene illuminants. Our technique uses this constraint to automatically separate an image into constituent images lit by each illuminant. This separation can be used to support applications like white balancing, lighting editing, and RGB photometric stereo, where we demonstrate results that outperform state-of-the-art techniques on a wide range of images.



### Learning to Fly by Crashing
- **Arxiv ID**: http://arxiv.org/abs/1704.05588v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.05588v2)
- **Published**: 2017-04-19 02:20:20+00:00
- **Updated**: 2017-04-27 00:13:19+00:00
- **Authors**: Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: https://youtu.be/u151hJaGKUo



### OCRAPOSE II: An OCR-based indoor positioning system using mobile phone images
- **Arxiv ID**: http://arxiv.org/abs/1704.05591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1704.05591v1)
- **Published**: 2017-04-19 02:43:23+00:00
- **Updated**: 2017-04-19 02:43:23+00:00
- **Authors**: Hamed Sadeghi, Shahrokh Valaee, Shahram Shirani
- **Comment**: 14 pages, 22 Figures
- **Journal**: None
- **Summary**: In this paper, we propose an OCR (optical character recognition)-based localization system called OCRAPOSE II, which is applicable in a number of indoor scenarios including office buildings, parkings, airports, grocery stores, etc. In these scenarios, characters (i.e. texts or numbers) can be used as suitable distinctive landmarks for localization. The proposed system takes advantage of OCR to read these characters in the query still images and provides a rough location estimate using a floor plan. Then, it finds depth and angle-of-view of the query using the information provided by the OCR engine in order to refine the location estimate. We derive novel formulas for the query angle-of-view and depth estimation using image line segments and the OCR box information. We demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios. It is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition.



### Insensitive Stochastic Gradient Twin Support Vector Machine for Large Scale Problems
- **Arxiv ID**: http://arxiv.org/abs/1704.05596v2
- **DOI**: 10.1016/j.ins.2018.06.007
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.05596v2)
- **Published**: 2017-04-19 03:08:38+00:00
- **Updated**: 2017-11-23 06:38:48+00:00
- **Authors**: Zhen Wang, Yuan-Hai Shao, Lan Bai, Li-Ming Liu, Nai-Yang Deng
- **Comment**: 31 pages, 31 figures
- **Journal**: Information Sciences, Volume 462, September 2018, Pages 114-131
- **Summary**: Stochastic gradient descent algorithm has been successfully applied on support vector machines (called PEGASOS) for many classification problems. In this paper, stochastic gradient descent algorithm is investigated to twin support vector machines for classification. Compared with PEGASOS, the proposed stochastic gradient twin support vector machines (SGTSVM) is insensitive on stochastic sampling for stochastic gradient descent algorithm. In theory, we prove the convergence of SGTSVM instead of almost sure convergence of PEGASOS. For uniformly sampling, the approximation between SGTSVM and twin support vector machines is also given, while PEGASOS only has an opportunity to obtain an approximation of support vector machines. In addition, the nonlinear SGTSVM is derived directly from its linear case. Experimental results on both artificial datasets and large scale problems show the stable performance of SGTSVM with a fast learning speed.



### FSITM: A Feature Similarity Index For Tone-Mapped Images
- **Arxiv ID**: http://arxiv.org/abs/1704.05624v1
- **DOI**: 10.1109/LSP.2014.2381458
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05624v1)
- **Published**: 2017-04-19 06:23:21+00:00
- **Updated**: 2017-04-19 06:23:21+00:00
- **Authors**: Hossein Ziaei Nafchi, Atena Shahkolaei, Reza Farrahi Moghaddam, Mohamed Cheriet
- **Comment**: 4 Pages, 1 Figure, 1 Table
- **Journal**: IEEE Signal Processing Letters, vol. 22, no. 8, Aug 2015
- **Summary**: In this work, based on the local phase information of images, an objective index, called the feature similarity index for tone-mapped images (FSITM), is proposed. To evaluate a tone mapping operator (TMO), the proposed index compares the locally weighted mean phase angle map of an original high dynamic range (HDR) to that of its associated tone-mapped image calculated using the output of the TMO method. In experiments on two standard databases, it is shown that the proposed FSITM method outperforms the state-of-the-art index, the tone mapped quality index (TMQI). In addition, a higher performance is obtained by combining the FSITM and TMQI indices. The MATLAB source code of the proposed metric(s) is available at https://www.mathworks.com/matlabcentral/fileexchange/59814.



### ConvNet-Based Localization of Anatomical Structures in 3D Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1704.05629v1
- **DOI**: 10.1109/TMI.2017.2673121
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05629v1)
- **Published**: 2017-04-19 06:54:34+00:00
- **Updated**: 2017-04-19 06:54:34+00:00
- **Authors**: Bob D. de Vos, Jelmer M. Wolterink, Pim A. de Jong, Tim Leiner, Max A. Viergever, Ivana Išgum
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging , vol.PP, no.99, pp.1-1
  (2017)
- **Summary**: Localization of anatomical structures is a prerequisite for many tasks in medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3D medical images through detection of their presence in 2D image slices using a convolutional neural network (ConvNet).   A single ConvNet is trained to detect presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3D bounding boxes are created by combining the output of the ConvNet in all slices.   In the experiments 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls.   The best results were achieved in localization of structures with clearly defined boundaries (e.g. aortic arch) and the worst when the structure boundary was not clearly visible (e.g. liver). The method was more robust and accurate in localization multiple structures.



### Skeleton Boxes: Solving skeleton based action detection with a single deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1704.05643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05643v1)
- **Published**: 2017-04-19 08:16:13+00:00
- **Updated**: 2017-04-19 08:16:13+00:00
- **Authors**: Bo Li, Huahui Chen, Yucheng Chen, Yuchao Dai, Mingyi He
- **Comment**: 4 pages,3 figures, icmew 2017
- **Journal**: icmew 2017
- **Summary**: Action recognition from well-segmented 3D skeleton video has been intensively studied. However, due to the difficulty in representing the 3D skeleton video and the lack of training data, action detection from streaming 3D skeleton video still lags far behind its recognition counterpart and image based object detection. In this paper, we propose a novel approach for this problem, which leverages both effective skeleton video encoding and deep regression based object detection from images. Our framework consists of two parts: skeleton-based video image mapping, which encodes a skeleton video to a color image in a temporal preserving way, and an end-to-end trainable fast skeleton action detector (Skeleton Boxes) based on image detection. Experimental results on the latest and largest PKU-MMD benchmark dataset demonstrate that our method outperforms the state-of-the-art methods with a large margin. We believe our idea would inspire and benefit future research in this important area.



### Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn
- **Arxiv ID**: http://arxiv.org/abs/1704.05645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05645v2)
- **Published**: 2017-04-19 08:30:19+00:00
- **Updated**: 2017-06-13 01:59:13+00:00
- **Authors**: Bo Li, Mingyi He, Xuelian Cheng, Yucheng Chen, Yuchao Dai
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an image classification based approach for skeleton-based video action recognition problem. Firstly, A dataset independent translation-scale invariant image mapping method is proposed, which transformes the skeleton videos to colour images, named skeleton-images. Secondly, A multi-scale deep convolutional neural network (CNN) architecture is proposed which could be built and fine-tuned on the powerful pre-trained CNNs, e.g., AlexNet, VGGNet, ResNet etal.. Even though the skeleton-images are very different from natural images, the fine-tune strategy still works well. At last, we prove that our method could also work well on 2D skeleton video data. We achieve the state-of-the-art results on the popular benchmard datasets e.g. NTU RGB+D, UTD-MHAD, MSRC-12, and G3D. Especially on the largest and challenge NTU RGB+D, UTD-MHAD, and MSRC-12 dataset, our method outperforms other methods by a large margion, which proves the efficacy of the proposed method.



### Unsupervised object segmentation in video by efficient selection of highly probable positive features
- **Arxiv ID**: http://arxiv.org/abs/1704.05674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05674v1)
- **Published**: 2017-04-19 10:00:46+00:00
- **Updated**: 2017-04-19 10:00:46+00:00
- **Authors**: Emanuela Haller, Marius Leordeanu
- **Comment**: None
- **Journal**: None
- **Summary**: We address an essential problem in computer vision, that of unsupervised object segmentation in video, where a main object of interest in a video sequence should be automatically separated from its background. An efficient solution to this task would enable large-scale video interpretation at a high semantic level in the absence of the costly manually labeled ground truth. We propose an efficient unsupervised method for generating foreground object soft-segmentation masks based on automatic selection and learning from highly probable positive features. We show that such features can be selected efficiently by taking into consideration the spatio-temporal, appearance and motion consistency of the object during the whole observed sequence. We also emphasize the role of the contrasting properties between the foreground object and its background. Our model is created in two stages: we start from pixel level analysis, on top of which we add a regression model trained on a descriptor that considers information over groups of pixels and is both discriminative and invariant to many changes that the object undergoes throughout the video. We also present theoretical properties of our unsupervised learning method, that under some mild constraints is guaranteed to learn a correct discriminative classifier even in the unsupervised case. Our method achieves competitive and even state of the art results on the challenging Youtube-Objects and SegTrack datasets, while being at least one order of magnitude faster than the competition. We believe that the competitive performance of our method in practice, along with its theoretical properties, constitute an important step towards solving unsupervised discovery in video.



### Design of low-cost, compact and weather-proof whole sky imagers for high-dynamic-range captures
- **Arxiv ID**: http://arxiv.org/abs/1704.05678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05678v1)
- **Published**: 2017-04-19 10:27:30+00:00
- **Updated**: 2017-04-19 10:27:30+00:00
- **Authors**: Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in Proc. IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS), July 2015
- **Journal**: None
- **Summary**: Ground-based whole sky imagers are popular for monitoring cloud formations, which is necessary for various applications. We present two new Wide Angle High-Resolution Sky Imaging System (WAHRSIS) models, which were designed especially to withstand the hot and humid climate of Singapore. The first uses a fully sealed casing, whose interior temperature is regulated using a Peltier cooler. The second features a double roof design with ventilation grids on the sides, allowing the outside air to flow through the device. Measurements of temperature inside these two devices show their ability to operate in Singapore weather conditions. Unlike our original WAHRSIS model, neither uses a mechanical sun blocker to prevent the direct sunlight from reaching the camera; instead they rely on high-dynamic-range imaging (HDRI) techniques to reduce the glare from the sun.



### Unsupervised Creation of Parameterized Avatars
- **Arxiv ID**: http://arxiv.org/abs/1704.05693v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.05693v2)
- **Published**: 2017-04-19 11:19:45+00:00
- **Updated**: 2017-07-09 16:10:53+00:00
- **Authors**: Lior Wolf, Yaniv Taigman, Adam Polyak
- **Comment**: v2 -- a change in the references due to a request from authors
- **Journal**: None
- **Summary**: We study the problem of mapping an input image to a tied pair consisting of a vector of parameters and an image that is created using a graphical engine from the vector of parameters. The mapping's objective is to have the output image as similar as possible to the input image. During training, no supervision is given in the form of matching inputs and outputs.   This learning problem extends two literature problems: unsupervised domain adaptation and cross domain transfer. We define a generalization bound that is based on discrepancy, and employ a GAN to implement a network solution that corresponds to this bound. Experimentally, our method is shown to solve the problem of automatically creating avatars.



### Automatic Segmentation of the Left Ventricle in Cardiac CT Angiography Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1704.05698v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1704.05698v1)
- **Published**: 2017-04-19 11:29:59+00:00
- **Updated**: 2017-04-19 11:29:59+00:00
- **Authors**: Majd Zreik, Tim Leiner, Bob D. de Vos, Robbert W. van Hamersvelt, Max A. Viergever, Ivana Isgum
- **Comment**: This work has been published as: Zreik, M., Leiner, T., de Vos, B.
  D., van Hamersvelt, R. W., Viergever, M. A., I\v{s}gum, I. (2016, April).
  Automatic segmentation of the left ventricle in cardiac CT angiography using
  convolutional neural networks. In Biomedical Imaging (ISBI), 2016 IEEE 13th
  International Symposium on (pp. 40-43). IEEE
- **Journal**: None
- **Summary**: Accurate delineation of the left ventricle (LV) is an important step in evaluation of cardiac function. In this paper, we present an automatic method for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation is performed in two stages. First, a bounding box around the LV is detected using a combination of three convolutional neural networks (CNNs). Subsequently, to obtain the segmentation of the LV, voxel classification is performed within the defined bounding box using a CNN. The study included CCTA scans of sixty patients, fifty scans were used to train the CNNs for the LV localization, five scans were used to train LV segmentation and the remaining five scans were used for testing the method. Automatic segmentation resulted in the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1 mm. The results demonstrate that automatic segmentation of the LV in CCTA scans using voxel classification with convolutional neural networks is feasible.



### A Deep Learning Framework using Passive WiFi Sensing for Respiration Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1704.05708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1704.05708v1)
- **Published**: 2017-04-19 12:35:17+00:00
- **Updated**: 2017-04-19 12:35:17+00:00
- **Authors**: U. M. Khan, Z. Kabir, S. A. Hassan, S. H. Ahmed
- **Comment**: 7 pages, 11 figures
- **Journal**: None
- **Summary**: This paper presents an end-to-end deep learning framework using passive WiFi sensing to classify and estimate human respiration activity. A passive radar test-bed is used with two channels where the first channel provides the reference WiFi signal, whereas the other channel provides a surveillance signal that contains reflections from the human target. Adaptive filtering is performed to make the surveillance signal source-data invariant by eliminating the echoes of the direct transmitted signal. We propose a novel convolutional neural network to classify the complex time series data and determine if it corresponds to a breathing activity, followed by a random forest estimator to determine breathing rate. We collect an extensive dataset to train the learning models and develop reference benchmarks for the future studies in the field. Based on the results, we conclude that deep learning techniques coupled with passive radars offer great potential for end-to-end human activity recognition.



### Universal Adversarial Perturbations Against Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.05712v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1704.05712v3)
- **Published**: 2017-04-19 12:48:52+00:00
- **Updated**: 2017-07-31 18:55:54+00:00
- **Authors**: Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker Fischer
- **Comment**: Final version for ICCV including supplementary material
- **Journal**: None
- **Summary**: While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.



### Learning Video Object Segmentation with Visual Memory
- **Arxiv ID**: http://arxiv.org/abs/1704.05737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05737v2)
- **Published**: 2017-04-19 14:09:49+00:00
- **Updated**: 2017-07-12 13:26:13+00:00
- **Authors**: Pavel Tokmakov, Karteek Alahari, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time. The module to build a "visual memory" in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given a video frame as input, our approach assigns each pixel an object or background label based on the learned spatio-temporal features as well as the "visual memory" specific to the video, acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results. For example, our approach outperforms the top method on the DAVIS dataset by nearly 6%. We also provide an extensive ablative analysis to investigate the influence of each component in the proposed framework.



### A location-aware embedding technique for accurate landmark recognition
- **Arxiv ID**: http://arxiv.org/abs/1704.05754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05754v1)
- **Published**: 2017-04-19 14:45:23+00:00
- **Updated**: 2017-04-19 14:45:23+00:00
- **Authors**: Federico Magliani, Navid Mahmoudian Bidgoli, Andrea Prati
- **Comment**: 6 pages, 5 figures, ICDSC 2017
- **Journal**: None
- **Summary**: The current state of the research in landmark recognition highlights the good accuracy which can be achieved by embedding techniques, such as Fisher vector and VLAD. All these techniques do not exploit spatial information, i.e. consider all the features and the corresponding descriptors without embedding their location in the image. This paper presents a new variant of the well-known VLAD (Vector of Locally Aggregated Descriptors) embedding technique which accounts, at a certain degree, for the location of features. The driving motivation comes from the observation that, usually, the most interesting part of an image (e.g., the landmark to be recognized) is almost at the center of the image, while the features at the borders are irrelevant features which do no depend on the landmark. The proposed variant, called locVLAD (location-aware VLAD), computes the mean of the two global descriptors: the VLAD executed on the entire original image, and the one computed on a cropped image which removes a certain percentage of the image borders. This simple variant shows an accuracy greater than the existing state-of-the-art approach. Experiments are conducted on two public datasets (ZuBuD and Holidays) which are used both for training and testing. Morever a more balanced version of ZuBuD is proposed.



### Derivation of the Asymptotic Eigenvalue Distribution for Causal 2D-AR Models under Upscaling
- **Arxiv ID**: http://arxiv.org/abs/1704.05773v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1704.05773v1)
- **Published**: 2017-04-19 15:25:45+00:00
- **Updated**: 2017-04-19 15:25:45+00:00
- **Authors**: David Vázquez-Padín, Fernando Pérez-González, Pedro Comesaña-Alfaro
- **Comment**: This technical report complements the work by David
  V\'azquez-Pad\'in, Fernando P\'erez-Gonz\'alez, and Pedro Comesa\~na-Alfaro,
  "A random matrix approach to the forensic analysis of upscaled images,"
  submitted to IEEE Transactions on Information Forensics and Security
- **Journal**: None
- **Summary**: This technical report describes the derivation of the asymptotic eigenvalue distribution for causal 2D-AR models under an upscaling scenario. Specifically, it tackles the analytical derivation of the asymptotic eigenvalue distribution of the sample autocorrelation matrix corresponding to genuine and upscaled images. It also includes the pseudocode of the derived approaches for resampling detection and resampling factor estimation that are based on this analysis.



### Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection
- **Arxiv ID**: http://arxiv.org/abs/1704.05775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05775v2)
- **Published**: 2017-04-19 15:30:20+00:00
- **Updated**: 2017-04-20 05:39:50+00:00
- **Authors**: Pierre Baqué, François Fleuret, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: People detection in single 2D images has improved greatly in recent years. However, comparatively little of this progress has percolated into multi-camera multi-people tracking algorithms, whose performance still degrades severely when scenes become very crowded. In this work, we introduce a new architecture that combines Convolutional Neural Nets and Conditional Random Fields to explicitly model those ambiguities. One of its key ingredients are high-order CRF terms that model potential occlusions and give our approach its robustness even when many people are present. Our model is trained end-to-end and we show that it outperforms several state-of-art algorithms on challenging scenes.



### Accurate Single Stage Detector Using Recurrent Rolling Convolution
- **Arxiv ID**: http://arxiv.org/abs/1704.05776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05776v1)
- **Published**: 2017-04-19 15:31:01+00:00
- **Updated**: 2017-04-19 15:31:01+00:00
- **Authors**: Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan, Yu-Wing Tai, Li Xu
- **Comment**: CVPR 2017
- **Journal**: None
- **Summary**: Most of the recent successful methods in accurate object detection and localization used some variants of R-CNN style two stage Convolutional Neural Networks (CNN) where plausible regions were proposed in the first stage then followed by a second stage for decision refinement. Despite the simplicity of training and the efficiency in deployment, the single stage detection methods have not been as competitive when evaluated in benchmarks consider mAP for high IoU thresholds. In this paper, we proposed a novel single stage end-to-end trainable object detection network to overcome this limitation. We achieved this by introducing Recurrent Rolling Convolution (RRC) architecture over multi-scale feature maps to construct object classifiers and bounding box regressors which are "deep in context". We evaluated our method in the challenging KITTI dataset which measures methods under IoU threshold of 0.7. We showed that with RRC, a single reduced VGG-16 based model already significantly outperformed all the previously published results. At the time this paper was written our models ranked the first in KITTI car detection (the hard level), the first in cyclist detection and the second in pedestrian detection. These results were not reached by the previous single stage methods. The code is publicly available.



### Network Dissection: Quantifying Interpretability of Deep Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/1704.05796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1704.05796v1)
- **Published**: 2017-04-19 16:10:38+00:00
- **Updated**: 2017-04-19 16:10:38+00:00
- **Authors**: David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba
- **Comment**: First two authors contributed equally. Oral presentation at CVPR 2017
- **Journal**: None
- **Summary**: We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.



### Learn to Model Motion from Blurry Footages
- **Arxiv ID**: http://arxiv.org/abs/1704.05817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05817v1)
- **Published**: 2017-04-19 16:54:54+00:00
- **Updated**: 2017-04-19 16:54:54+00:00
- **Authors**: Wenbin Li, Da Chen, Zhihan Lv, Yan Yan, Darren Cosker
- **Comment**: Preprint of our paper accepted by Pattern Recognition
- **Journal**: None
- **Summary**: It is difficult to recover the motion field from a real-world footage given a mixture of camera shake and other photometric effects. In this paper we propose a hybrid framework by interleaving a Convolutional Neural Network (CNN) and a traditional optical flow energy. We first conduct a CNN architecture using a novel learnable directional filtering layer. Such layer encodes the angle and distance similarity matrix between blur and camera motion, which is able to enhance the blur features of the camera-shake footages. The proposed CNNs are then integrated into an iterative optical flow framework, which enable the capability of modelling and solving both the blind deconvolution and the optical flow estimation problems simultaneously. Our framework is trained end-to-end on a synthetic dataset and yields competitive precision and performance against the state-of-the-art approaches.



### Learning to Generate Long-term Future via Hierarchical Prediction
- **Arxiv ID**: http://arxiv.org/abs/1704.05831v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05831v5)
- **Published**: 2017-04-19 17:25:56+00:00
- **Updated**: 2018-01-08 01:24:36+00:00
- **Authors**: Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, Honglak Lee
- **Comment**: International Conference on Machine Learning (ICML) 2017
- **Journal**: None
- **Summary**: We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.



### SkiMap: An Efficient Mapping Framework for Robot Navigation
- **Arxiv ID**: http://arxiv.org/abs/1704.05832v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1704.05832v1)
- **Published**: 2017-04-19 17:29:04+00:00
- **Updated**: 2017-04-19 17:29:04+00:00
- **Authors**: Daniele De Gregorio, Luigi Di Stefano
- **Comment**: Accepted by International Conference on Robotics and Automation
  (ICRA) 2017. This is the submitted version. The final published version may
  be slightly different
- **Journal**: None
- **Summary**: We present a novel mapping framework for robot navigation which features a multi-level querying system capable to obtain rapidly representations as diverse as a 3D voxel grid, a 2.5D height map and a 2D occupancy grid. These are inherently embedded into a memory and time efficient core data structure organized as a Tree of SkipLists. Compared to the well-known Octree representation, our approach exhibits a better time efficiency, thanks to its simple and highly parallelizable computational structure, and a similar memory footprint when mapping large workspaces. Peculiarly within the realm of mapping for robot navigation, our framework supports realtime erosion and re-integration of measurements upon reception of optimized poses from the sensor tracker, so as to improve continuously the accuracy of the map.



### Generative Face Completion
- **Arxiv ID**: http://arxiv.org/abs/1704.05838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05838v1)
- **Published**: 2017-04-19 17:53:29+00:00
- **Updated**: 2017-04-19 17:53:29+00:00
- **Authors**: Yijun Li, Sifei Liu, Jimei Yang, Ming-Hsuan Yang
- **Comment**: Accepted by CVPR 2017
- **Journal**: None
- **Summary**: In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results.



### HPatches: A benchmark and evaluation of handcrafted and learned local descriptors
- **Arxiv ID**: http://arxiv.org/abs/1704.05939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05939v1)
- **Published**: 2017-04-19 21:37:03+00:00
- **Updated**: 2017-04-19 21:37:03+00:00
- **Authors**: Vassileios Balntas, Karel Lenc, Andrea Vedaldi, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel benchmark for evaluating local image descriptors. We demonstrate that the existing datasets and evaluation protocols do not specify unambiguously all aspects of evaluation, leading to ambiguities and inconsistencies in results reported in the literature. Furthermore, these datasets are nearly saturated due to the recent improvements in local descriptors obtained by learning them from large annotated datasets. Therefore, we introduce a new large dataset suitable for training and testing modern descriptors, together with strictly defined evaluation protocols in several tasks such as matching, retrieval and classification. This allows for more realistic, and thus more reliable comparisons in different application scenarios. We evaluate the performance of several state-of-the-art descriptors and analyse their properties. We show that a simple normalisation of traditional hand-crafted descriptors can boost their performance to the level of deep learning based descriptors within a realistic benchmarks evaluation.



### Unassisted Quantitative Evaluation Of Despeckling Filters
- **Arxiv ID**: http://arxiv.org/abs/1704.05952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.05952v1)
- **Published**: 2017-04-19 23:01:30+00:00
- **Updated**: 2017-04-19 23:01:30+00:00
- **Authors**: Luis Gomez, Raydonal Ospina, Alejandro C. Frery
- **Comment**: Accepted for publication in Remote Sensing - Open Access Journal
- **Journal**: None
- **Summary**: SAR (Synthetic Aperture Radar) imaging plays a central role in Remote Sensing due to, among other important features, its ability to provide high-resolution, day-and-night and almost weather-independent images. SAR images are affected from a granular contamination, speckle, that can be described by a multiplicative model. Many despeckling techniques have been proposed in the literature, as well as measures of the quality of the results they provide. Assuming the multiplicative model, the observed image $Z$ is the product of two independent fields: the backscatter $X$ and the speckle $Y$. The result of any speckle filter is $\widehat X$, an estimator of the backscatter $X$, based solely on the observed data $Z$. An ideal estimator would be the one for which the ratio of the observed image to the filtered one $I=Z/\widehat X$ is only speckle: a collection of independent identically distributed samples from Gamma variates. We, then, assess the quality of a filter by the closeness of $I$ to the hypothesis that it is adherent to the statistical properties of pure speckle. We analyze filters through the ratio image they produce with regards to first- and second-order statistics: the former check marginal properties, while the latter verifies lack of structure. A new quantitative image-quality index is then defined, and applied to state-of-the-art despeckling filters. This new measure provides consistent results with commonly used quality measures (equivalent number of looks, PSNR, MSSIM, $\beta$ edge correlation, and preservation of the mean), and ranks the filters results also in agreement with their visual analysis. We conclude our study showing that the proposed measure can be successfully used to optimize the (often many) parameters that define a speckle filter.



### SLAM with Objects using a Nonparametric Pose Graph
- **Arxiv ID**: http://arxiv.org/abs/1704.05959v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1704.05959v1)
- **Published**: 2017-04-19 23:54:57+00:00
- **Updated**: 2017-04-19 23:54:57+00:00
- **Authors**: Beipeng Mu, Shih-Yuan Liu, Liam Paull, John Leonard, Jonathan How
- **Comment**: published at IROS 2016
- **Journal**: None
- **Summary**: Mapping and self-localization in unknown environments are fundamental capabilities in many robotic applications. These tasks typically involve the identification of objects as unique features or landmarks, which requires the objects both to be detected and then assigned a unique identifier that can be maintained when viewed from different perspectives and in different images. The \textit{data association} and \textit{simultaneous localization and mapping} (SLAM) problems are, individually, well-studied in the literature. But these two problems are inherently tightly coupled, and that has not been well-addressed. Without accurate SLAM, possible data associations are combinatorial and become intractable easily. Without accurate data association, the error of SLAM algorithms diverge easily. This paper proposes a novel nonparametric pose graph that models data association and SLAM in a single framework. An algorithm is further introduced to alternate between inferring data association and performing SLAM. Experimental results show that our approach has the new capability of associating object detections and localizing objects at the same time, leading to significantly better performance on both the data association and SLAM problems than achieved by considering only one and ignoring imperfections in the other.




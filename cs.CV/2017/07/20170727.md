# Arxiv Papers in cs.CV on 2017-07-27
### Context-Aware Single-Shot Detector
- **Arxiv ID**: http://arxiv.org/abs/1707.08682v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08682v2)
- **Published**: 2017-07-27 01:50:17+00:00
- **Updated**: 2018-03-24 19:22:12+00:00
- **Authors**: Wei Xiang, Dong-Qing Zhang, Heather Yu, Vassilis Athitsos
- **Comment**: None
- **Journal**: None
- **Summary**: SSD is one of the state-of-the-art object detection algorithms, and it combines high detection accuracy with real-time speed. However, it is widely recognized that SSD is less accurate in detecting small objects compared to large objects, because it ignores the context from outside the proposal boxes. In this paper, we present CSSD--a shorthand for context-aware single-shot multibox object detector. CSSD is built on top of SSD, with additional layers modeling multi-scale contexts. We describe two variants of CSSD, which differ in their context layers, using dilated convolution layers (DiCSSD) and deconvolution layers (DeCSSD) respectively. The experimental results show that the multi-scale context modeling significantly improves the detection accuracy. In addition, we study the relationship between effective receptive fields (ERFs) and the theoretical receptive fields (TRFs), particularly on a VGGNet. The empirical results further strengthen our conclusion that SSD coupled with context layers achieves better detection results especially for small objects ($+3.2\% {\rm AP}_{@0.5}$ on MS-COCO compared to the newest SSD), while maintaining comparable runtime performance.



### A Jointly Learned Deep Architecture for Facial Attribute Analysis and Face Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1707.08705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08705v1)
- **Published**: 2017-07-27 04:45:42+00:00
- **Updated**: 2017-07-27 04:45:42+00:00
- **Authors**: Keke He, Yanwei Fu, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Facial attribute analysis in the real world scenario is very challenging mainly because of complex face variations. Existing works of analyzing face attributes are mostly based on the cropped and aligned face images. However, this result in the capability of attribute prediction heavily relies on the preprocessing of face detector. To address this problem, we present a novel jointly learned deep architecture for both facial attribute analysis and face detection. Our framework can process the natural images in the wild and our experiments on CelebA and LFWA datasets clearly show that the state-of-the-art performance is obtained.



### Ultra-low-power Wireless Streaming Cameras
- **Arxiv ID**: http://arxiv.org/abs/1707.08718v1
- **DOI**: None
- **Categories**: **cs.ET**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.08718v1)
- **Published**: 2017-07-27 06:43:18+00:00
- **Updated**: 2017-07-27 06:43:18+00:00
- **Authors**: Saman Naderiparizi, Mehrdad Hessar, Vamsi Talla, Shyamnath Gollakota, Joshua R. Smith
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: Wireless video streaming has traditionally been considered an extremely power-hungry operation. Existing approaches optimize the camera and communication modules individually to minimize their power consumption. However, the joint redesign and optimization of wireless communication as well as the camera is what that provides more power saving. We present an ultra-low-power wireless video streaming camera. To achieve this, we present a novel "analog" video backscatter technique that feeds analog pixels from the photo-diodes directly to the backscatter hardware, thereby eliminating power consuming hardware components such as ADCs and amplifiers. We prototype our wireless camera using off-the-shelf hardware and show that our design can stream video at up to 13 FPS and can operate up to a distance of 150 feet from the access point. Our COTS prototype consumes 2.36mW. Finally, to demonstrate the potential of our design, we built two proof-of-concept applications: video streaming for micro-robots and security cameras for face detection.



### Exploiting Web Images for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.08721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08721v2)
- **Published**: 2017-07-27 06:58:39+00:00
- **Updated**: 2017-07-28 04:37:16+00:00
- **Authors**: Qingyi Tao, Hao Yang, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the performance of object detection has advanced significantly with the evolving deep convolutional neural networks. However, the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labelling. Object detection without bounding box annotations, i.e, weakly supervised detection methods, are still lagging far behind. As weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image, it is generally very difficult to distill knowledge of the actual appearances of objects. Inspired by curriculum learning, this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point. While exploiting large-scale free web imagery, we introduce a sophisticated labour free method to construct a web dataset with good diversity in object appearance. After that, semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme. Our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes especially for the classes that are often considered hard in other works.



### Algebraic Relations and Triangulation of Unlabeled Image Points
- **Arxiv ID**: http://arxiv.org/abs/1707.08722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08722v1)
- **Published**: 2017-07-27 07:00:02+00:00
- **Updated**: 2017-07-27 07:00:02+00:00
- **Authors**: André Wagner
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: In multiview geometry when correspondences among multiple views are unknown the image points can be understood as being unlabeled. This is a common problem in computer vision. We give a novel approach to handle such a situation by regarding unlabeled point configurations as points on the Chow variety $\text{Sym}_m(\mathbb{P}^2)$. For two unlabeled points we design an algorithm that solves the triangulation problem with unknown correspondences. Further the unlabeled multiview variety $\text{Sym}_m(V_A)$ is studied.



### A Comparative Study of the Clinical use of Motion Analysis from Kinect Skeleton Data
- **Arxiv ID**: http://arxiv.org/abs/1707.08813v2
- **DOI**: 10.1109/SMC.2017.8123052
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08813v2)
- **Published**: 2017-07-27 10:55:43+00:00
- **Updated**: 2017-07-31 08:42:04+00:00
- **Authors**: Sean Maudsley-Barton, Jamie McPheey, Anthony Bukowski, Daniel Leightley, Moi Hoon Yap
- **Comment**: None
- **Journal**: None
- **Summary**: The analysis of human motion as a clinical tool can bring many benefits such as the early detection of disease and the monitoring of recovery, so in turn helping people to lead independent lives. However, it is currently under used. Developments in depth cameras, such as Kinect, have opened up the use of motion analysis in settings such as GP surgeries, care homes and private homes. To provide an insight into the use of Kinect in the healthcare domain, we present a review of the current state of the art. We then propose a method that can represent human motions from time-series data of arbitrary length, as a single vector. Finally, we demonstrate the utility of this method by extracting a set of clinically significant features and using them to detect the age related changes in the motions of a set of 54 individuals, with a high degree of certainty (F1- score between 0.9 - 1.0). Indicating its potential application in the detection of a range of age-related motion impairments.



### Representation-Aggregation Networks for Segmentation of Multi-Gigapixel Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1707.08814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08814v1)
- **Published**: 2017-07-27 10:56:58+00:00
- **Updated**: 2017-07-27 10:56:58+00:00
- **Authors**: Abhinav Agarwalla, Muhammad Shaban, Nasir M. Rajpoot
- **Comment**: Published in Workshop on Deep Learning in Irregular Domains (DLID) in
  BMVC2017
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) models have become the state-of-the-art for most computer vision tasks with natural images. However, these are not best suited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology slides due to large size of these images. Current approaches construct smaller patches from WSIs which results in the loss of contextual information. We propose to capture the spatial context using novel Representation-Aggregation Network (RAN) for segmentation purposes, wherein the first network learns patch-level representation and the second network aggregates context from a grid of neighbouring patches. We can use any CNN for representation learning, and can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for context-aggregation. Our method significantly outperformed conventional patch-based CNN approaches on segmentation of tumour in WSIs of breast cancer tissue sections.



### Food Ingredients Recognition through Multi-label Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.08816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08816v1)
- **Published**: 2017-07-27 11:16:42+00:00
- **Updated**: 2017-07-27 11:16:42+00:00
- **Authors**: Marc Bolaños, Aina Ferrà, Petia Radeva
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Automatically constructing a food diary that tracks the ingredients consumed can help people follow a healthy diet. We tackle the problem of food ingredients recognition as a multi-label learning problem. We propose a method for adapting a highly performing state of the art CNN in order to act as a multi-label predictor for learning recipes in terms of their list of ingredients. We prove that our model is able to, given a picture, predict its list of ingredients, even if the recipe corresponding to the picture has never been seen by the model. We make public two new datasets suitable for this purpose. Furthermore, we prove that a model trained with a high variability of recipes and ingredients is able to generalize better on new data, and visualize how it specializes each of its neurons to different ingredients.



### A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets
- **Arxiv ID**: http://arxiv.org/abs/1707.08819v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.08819v3)
- **Published**: 2017-07-27 11:22:22+00:00
- **Updated**: 2017-08-23 16:06:20+00:00
- **Authors**: Patryk Chrabaszcz, Ilya Loshchilov, Frank Hutter
- **Comment**: None
- **Journal**: None
- **Summary**: The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and ImageNet16$\times$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32_Scripts



### Serious Games Application for Memory Training Using Egocentric Images
- **Arxiv ID**: http://arxiv.org/abs/1707.08821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08821v1)
- **Published**: 2017-07-27 11:36:26+00:00
- **Updated**: 2017-07-27 11:36:26+00:00
- **Authors**: Gabriel Oliveira-Barra, Marc Bolaños, Estefania Talavera, Adrián Dueñas, Olga Gelonch, Maite Garolera
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Mild cognitive impairment is the early stage of several neurodegenerative diseases, such as Alzheimer's. In this work, we address the use of lifelogging as a tool to obtain pictures from a patient's daily life from an egocentric point of view. We propose to use them in combination with serious games as a way to provide a non-pharmacological treatment to improve their quality of life. To do so, we introduce a novel computer vision technique that classifies rich and non rich egocentric images and uses them in serious games. We present results over a dataset composed by 10,997 images, recorded by 7 different users, achieving 79% of F1-score. Our model presents the first method used for automatic egocentric images selection applicable to serious games.



### STN-OCR: A single Neural Network for Text Detection and Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.08831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08831v1)
- **Published**: 2017-07-27 12:22:34+00:00
- **Updated**: 2017-07-27 12:22:34+00:00
- **Authors**: Christian Bartz, Haojin Yang, Christoph Meinel
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Detecting and recognizing text in natural scene images is a challenging, yet not completely solved task. In re- cent years several new systems that try to solve at least one of the two sub-tasks (text detection and text recognition) have been proposed. In this paper we present STN-OCR, a step towards semi-supervised neural networks for scene text recognition, that can be optimized end-to-end. In contrast to most existing works that consist of multiple deep neural networks and several pre-processing steps we propose to use a single deep neural network that learns to detect and recognize text from natural images in a semi-supervised way. STN-OCR is a network that integrates and jointly learns a spatial transformer network, that can learn to detect text regions in an image, and a text recognition network that takes the identified text regions and recognizes their textual content. We investigate how our model behaves on a range of different tasks (detection and recognition of characters, and lines of text). Experimental results on public benchmark datasets show the ability of our model to handle a variety of different tasks, without substantial changes in its overall network structure.



### Anisotropic EM Segmentation by 3D Affinity Learning and Agglomeration
- **Arxiv ID**: http://arxiv.org/abs/1707.08935v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08935v2)
- **Published**: 2017-07-27 17:08:28+00:00
- **Updated**: 2018-08-03 15:22:41+00:00
- **Authors**: Toufiq Parag, Fabian Tschopp, William Grisaitis, Srinivas C Turaga, Xuewen Zhang, Brian Matejek, Lee Kamentsky, Jeff W. Lichtman, Hanspeter Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: The field of connectomics has recently produced neuron wiring diagrams from relatively large brain regions from multiple animals. Most of these neural reconstructions were computed from isotropic (e.g., FIBSEM) or near isotropic (e.g., SBEM) data. In spite of the remarkable progress on algorithms in recent years, automatic dense reconstruction from anisotropic data remains a challenge for the connectomics community. One significant hurdle in the segmentation of anisotropic data is the difficulty in generating a suitable initial over-segmentation. In this study, we present a segmentation method for anisotropic EM data that agglomerates a 3D over-segmentation computed from the 3D affinity prediction. A 3D U-net is trained to predict 3D affinities by the MALIS approach. Experiments on multiple datasets demonstrates the strength and robustness of the proposed method for anisotropic EM segmentation.



### Concise Radiometric Calibration Using The Power of Ranking
- **Arxiv ID**: http://arxiv.org/abs/1707.08943v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08943v3)
- **Published**: 2017-07-27 17:31:25+00:00
- **Updated**: 2018-03-15 11:10:59+00:00
- **Authors**: Han Gong, Graham D. Finlayson, Maryam M. Darrodi
- **Comment**: accepted by BMVC 2017. Correction: Note that the reported model
  parameter number in the original BMVC paper was wrong. It should be 408
  rather than 158
- **Journal**: None
- **Summary**: Compared with raw images, the more common JPEG images are less useful for machine vision algorithms and professional photographers because JPEG-sRGB does not preserve a linear relation between pixel values and the light measured from the scene. A camera is said to be radiometrically calibrated if there is a computational model which can predict how the raw linear sensor image is mapped to the corresponding rendered image (e.g. JPEGs) and vice versa. This paper begins with the observation that the rank order of pixel values are mostly preserved post colour correction. We show that this observation is the key to solving for the whole camera pipeline (colour correction, tone and gamut mapping). Our rank-based calibration method is simpler than the prior art and so is parametrised by fewer variables which, concomitantly, can be solved for using less calibration data. Another advantage is that we can derive the camera pipeline from a single pair of raw-JPEG images. Experiments demonstrate that our method delivers state-of-the-art results (especially for the most interesting case of JPEG to raw).



### Handwritten character recognition using some (anti)-diagonal structural features
- **Arxiv ID**: http://arxiv.org/abs/1707.08951v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08951v3)
- **Published**: 2017-07-27 17:50:13+00:00
- **Updated**: 2018-02-14 16:51:15+00:00
- **Authors**: José Manuel Casas, Nick Inassaridze, Manuel Ladra, Susana Ladra
- **Comment**: Revised version with a number of improvements and update references,
  9 pages
- **Journal**: None
- **Summary**: In this paper, we present a methodology for off-line handwritten character recognition. The proposed methodology relies on a new feature extraction technique based on structural characteristics, histograms and profiles. As novelty, we propose the extraction of new eight histograms and four profiles from the $32\times 32$ matrices that represent the characters, creating 256-dimension feature vectors. These feature vectors are then employed in a classification step that uses a $k$-means algorithm. We performed experiments using the NIST database to evaluate our proposal. Namely, the recognition system was trained using 1000 samples and 64 classes for each symbol and was tested on 500 samples for each symbol. We obtain promising accuracy results that vary from 81.74\% to 93.75\%, depending on the difficulty of the character category, showing better accuracy results than other methods from the state of the art also based on structural characteristics.



### Building Detection from Satellite Images on a Global Scale
- **Arxiv ID**: http://arxiv.org/abs/1707.08952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08952v1)
- **Published**: 2017-07-27 17:56:30+00:00
- **Updated**: 2017-07-27 17:56:30+00:00
- **Authors**: Amy Zhang, Xianming Liu, Andreas Gros, Tobias Tiecke
- **Comment**: None
- **Journal**: None
- **Summary**: In the last several years, remote sensing technology has opened up the possibility of performing large scale building detection from satellite imagery. Our work is some of the first to create population density maps from building detection on a large scale. The scale of our work on population density estimation via high resolution satellite images raises many issues, that we will address in this paper. The first was data acquisition. Labeling buildings from satellite images is a hard problem, one where we found our labelers to only be about 85% accurate at. There is a tradeoff of quantity vs. quality of labels, so we designed two separate policies for labels meant for training sets and those meant for test sets, since our requirements of the two set types are quite different. We also trained weakly supervised footprint detection models with the classification labels, and semi-supervised approaches with a small number of pixel-level labels, which are very expensive to procure.



### Understanding Aesthetics in Photography using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.08985v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.08985v2)
- **Published**: 2017-07-27 18:15:10+00:00
- **Updated**: 2017-08-08 20:39:29+00:00
- **Authors**: Maciej Suchecki, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluating aesthetic value of digital photographs is a challenging task, mainly due to numerous factors that need to be taken into account and subjective manner of this process. In this paper, we propose to approach this problem using deep convolutional neural networks. Using a dataset of over 1.7 million photos collected from Flickr, we train and evaluate a deep learning model whose goal is to classify input images by analysing their aesthetic value. The result of this work is a publicly available Web-based application that can be used in several real-life applications, e.g. to improve the workflow of professional photographers by pre-selecting the best photos.



### A Locally Adapting Technique for Boundary Detection using Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.09030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09030v1)
- **Published**: 2017-07-27 20:06:52+00:00
- **Updated**: 2017-07-27 20:06:52+00:00
- **Authors**: Marylesa Howard, Margaret C. Hock, B. T. Meehan, Leora Dresselhaus-Cooper
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid growth in the field of quantitative digital image analysis is paving the way for researchers to make precise measurements about objects in an image. To compute quantities from the image such as the density of compressed materials or the velocity of a shockwave, we must determine object boundaries. Images containing regions that each have a spatial trend in intensity are of particular interest. We present a supervised image segmentation method that incorporates spatial information to locate boundaries between regions with overlapping intensity histograms. The segmentation of a pixel is determined by comparing its intensity to distributions from local, nearby pixel intensities. Because of the statistical nature of the algorithm, we use maximum likelihood estimation theory to quantify uncertainty about each boundary. We demonstrate the success of this algorithm on a radiograph of a multicomponent cylinder and on an optical image of a laser-induced shockwave, and we provide final boundary locations with associated bands of uncertainty.



### Learning from Video and Text via Large-Scale Discriminative Clustering
- **Arxiv ID**: http://arxiv.org/abs/1707.09074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09074v1)
- **Published**: 2017-07-27 23:30:53+00:00
- **Updated**: 2017-07-27 23:30:53+00:00
- **Authors**: Antoine Miech, Jean-Baptiste Alayrac, Piotr Bojanowski, Ivan Laptev, Josef Sivic
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: Discriminative clustering has been successfully applied to a number of weakly-supervised learning tasks. Such applications include person and action recognition, text-to-video alignment, object co-segmentation and colocalization in videos and images. One drawback of discriminative clustering, however, is its limited scalability. We address this issue and propose an online optimization algorithm based on the Block-Coordinate Frank-Wolfe algorithm. We apply the proposed method to the problem of weakly supervised learning of actions and actors from movies together with corresponding movie scripts. The scaling up of the learning problem to 66 feature length movies enables us to significantly improve weakly supervised action recognition.




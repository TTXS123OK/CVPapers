# Arxiv Papers in cs.CV on 2017-07-21
### Temporal Convolution Based Action Proposal: Submission to ActivityNet 2017
- **Arxiv ID**: http://arxiv.org/abs/1707.06750v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06750v3)
- **Published**: 2017-07-21 03:30:00+00:00
- **Updated**: 2018-09-26 10:54:24+00:00
- **Authors**: Tianwei Lin, Xu Zhao, Zheng Shou
- **Comment**: 4 pages, Presented at ActivityNet Large Scale Activity Recognition
  Challenge workshop at CVPR 2017
- **Journal**: None
- **Summary**: In this notebook paper, we describe our approach in the submission to the temporal action proposal (task 3) and temporal action localization (task 4) of ActivityNet Challenge hosted at CVPR 2017. Since the accuracy in action classification task is already very high (nearly 90% in ActivityNet dataset), we believe that the main bottleneck for temporal action localization is the quality of action proposals. Therefore, we mainly focus on the temporal action proposal task and propose a new proposal model based on temporal convolutional network. Our approach achieves the state-of-the-art performances on both temporal action proposal task and temporal action localization task.



### (k,q)-Compressed Sensing for dMRI with Joint Spatial-Angular Sparsity Prior
- **Arxiv ID**: http://arxiv.org/abs/1707.09958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.09958v2)
- **Published**: 2017-07-21 03:50:44+00:00
- **Updated**: 2017-08-02 18:36:57+00:00
- **Authors**: Evan Schwab, René Vidal, Nicolas Charon
- **Comment**: To be published in the 2017 Computational Diffusion MRI Workshop of
  MICCAI
- **Journal**: None
- **Summary**: Advanced diffusion magnetic resonance imaging (dMRI) techniques, like diffusion spectrum imaging (DSI) and high angular resolution diffusion imaging (HARDI), remain underutilized compared to diffusion tensor imaging because the scan times needed to produce accurate estimations of fiber orientation are significantly longer. To accelerate DSI and HARDI, recent methods from compressed sensing (CS) exploit a sparse underlying representation of the data in the spatial and angular domains to undersample in the respective k- and q-spaces. State-of-the-art frameworks, however, impose sparsity in the spatial and angular domains separately and involve the sum of the corresponding sparse regularizers. In contrast, we propose a unified (k,q)-CS formulation which imposes sparsity jointly in the spatial-angular domain to further increase sparsity of dMRI signals and reduce the required subsampling rate. To efficiently solve this large-scale global reconstruction problem, we introduce a novel adaptation of the FISTA algorithm that exploits dictionary separability. We show on phantom and real HARDI data that our approach achieves significantly more accurate signal reconstructions than the state of the art while sampling only 2-4% of the (k,q)-space, allowing for the potential of new levels of dMRI acceleration.



### A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics
- **Arxiv ID**: http://arxiv.org/abs/1707.06757v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.DS, 68T05, H.2.8; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1707.06757v2)
- **Published**: 2017-07-21 05:04:07+00:00
- **Updated**: 2018-07-13 17:38:33+00:00
- **Authors**: Kelum Gajamannage, Randy Paffenroth, Erik M. Bollt
- **Comment**: 13 pages, 7 figures, submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Existing dimensionality reduction methods are adept at revealing hidden underlying manifolds arising from high-dimensional data and thereby producing a low-dimensional representation. However, the smoothness of the manifolds produced by classic techniques over sparse and noisy data is not guaranteed. In fact, the embedding generated using such data may distort the geometry of the manifold and thereby produce an unfaithful embedding. Herein, we propose a framework for nonlinear dimensionality reduction that generates a manifold in terms of smooth geodesics that is designed to treat problems in which manifold measurements are either sparse or corrupted by noise. Our method generates a network structure for given high-dimensional data using a nearest neighbors search and then produces piecewise linear shortest paths that are defined as geodesics. Then, we fit points in each geodesic by a smoothing spline to emphasize the smoothness. The robustness of this approach for sparse and noisy datasets is demonstrated by the implementation of the method on synthetic and real-world datasets.



### Improved Bilinear Pooling with CNNs
- **Arxiv ID**: http://arxiv.org/abs/1707.06772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06772v1)
- **Published**: 2017-07-21 06:49:04+00:00
- **Updated**: 2017-07-21 06:49:04+00:00
- **Authors**: Tsung-Yu Lin, Subhransu Maji
- **Comment**: None
- **Journal**: None
- **Summary**: Bilinear pooling of Convolutional Neural Network (CNN) features [22, 23], and their compact variants [10], have been shown to be effective at fine-grained recognition, scene categorization, texture recognition, and visual question-answering tasks among others. The resulting representation captures second-order statistics of convolutional features in a translationally invariant manner. In this paper we investigate various ways of normalizing these statistics to improve their representation power. In particular we find that the matrix square-root normalization offers significant improvements and outperforms alternative schemes such as the matrix logarithm normalization when combined with elementwise square-root and l2 normalization. This improves the accuracy by 2-3% on a range of fine-grained recognition datasets leading to a new state of the art. We also investigate how the accuracy of matrix function computations effect network training and evaluation. In particular we compare against a technique for estimating matrix square-root gradients via solving a Lyapunov equation that is more numerically accurate than computing gradients via a Singular Value Decomposition (SVD). We find that while SVD gradients are numerically inaccurate the overall effect on the final accuracy is negligible once boundary cases are handled carefully. We present an alternative scheme for computing gradients that is faster and yet it offers improvements over the baseline model. Finally we show that the matrix square-root computed approximately using a few Newton iterations is just as accurate for the classification task but allows an order-of-magnitude faster GPU implementation compared to SVD decomposition.



### Neural Person Search Machines
- **Arxiv ID**: http://arxiv.org/abs/1707.06777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06777v1)
- **Published**: 2017-07-21 07:11:51+00:00
- **Updated**: 2017-07-21 07:11:51+00:00
- **Authors**: Hao Liu, Jiashi Feng, Zequn Jie, Karlekar Jayashree, Bo Zhao, Meibin Qi, Jianguo Jiang, Shuicheng Yan
- **Comment**: ICCV2017 camera ready
- **Journal**: None
- **Summary**: We investigate the problem of person search in the wild in this work. Instead of comparing the query against all candidate regions generated in a query-blind manner, we propose to recursively shrink the search area from the whole image till achieving precise localization of the target person, by fully exploiting information from the query and contextual cues in every recursive search step. We develop the Neural Person Search Machines (NPSM) to implement such recursive localization for person search. Benefiting from its neural search mechanism, NPSM is able to selectively shrink its focus from a loose region to a tighter one containing the target automatically. In this process, NPSM employs an internal primitive memory component to memorize the query representation which modulates the attention and augments its robustness to other distracting regions. Evaluations on two benchmark datasets, CUHK-SYSU Person Search dataset and PRW dataset, have demonstrated that our method can outperform current state-of-the-arts in both mAP and top-1 evaluation protocols.



### 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1707.06783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06783v1)
- **Published**: 2017-07-21 07:28:14+00:00
- **Updated**: 2017-07-21 07:28:14+00:00
- **Authors**: Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye, Yuebin Wang, Jiwen Lu
- **Comment**: IEEE International Conference on Computer Vision (ICCV) 2017
- **Journal**: None
- **Summary**: Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.



### Head Detection with Depth Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1707.06786v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06786v2)
- **Published**: 2017-07-21 07:35:21+00:00
- **Updated**: 2017-11-08 18:10:39+00:00
- **Authors**: Diego Ballotta, Guido Borghi, Roberto Vezzani, Rita Cucchiara
- **Comment**: Accepted as full paper (oral) at VISAPP 2018
- **Journal**: None
- **Summary**: Head detection and localization is a demanding task and a key element for many computer vision applications, like video surveillance, Human Computer Interaction and face analysis. The stunning amount of work done for detecting faces on RGB images, together with the availability of huge face datasets, allowed to setup very effective systems on that domain. However, due to illumination issues, infrared or depth cameras may be required in real applications. In this paper, we introduce a novel method for head detection on depth images that exploits the classification ability of deep learning approaches. In addition to reduce the dependency on the external illumination, depth images implicitly embed useful information to deal with the scale of the target objects. Two public datasets have been exploited: the first one, called Pandora, is used to train a deep binary classifier with face and non-face images. The second one, collected by Cornell University, is used to perform a cross-dataset test during daily activities in unconstrained environments. Experimental results show that the proposed method overcomes the performance of state-of-art methods working on depth images.



### Recurrent Neural Networks for Online Video Popularity Prediction
- **Arxiv ID**: http://arxiv.org/abs/1707.06807v1
- **DOI**: 10.1007/978-3-319-60438-1_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06807v1)
- **Published**: 2017-07-21 09:03:34+00:00
- **Updated**: 2017-07-21 09:03:34+00:00
- **Authors**: Tomasz Trzcinski, Pawel Andruszkiewicz, Tomasz Bochenski, Przemyslaw Rokita
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of popularity prediction of online videos shared in social media. We prove that this challenging task can be approached using recently proposed deep neural network architectures. We cast the popularity prediction problem as a classification task and we aim to solve it using only visual cues extracted from videos. To that end, we propose a new method based on a Long-term Recurrent Convolutional Network (LRCN) that incorporates the sequentiality of the information in the model. Results obtained on a dataset of over 37'000 videos published on Facebook show that using our method leads to over 30% improvement in prediction performance over the traditional shallow approaches and can provide valuable insights for content creators.



### Text Recognition in Scene Image and Video Frame using Color Channel Selection
- **Arxiv ID**: http://arxiv.org/abs/1707.06810v2
- **DOI**: 10.1007/s11042-017-4750-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06810v2)
- **Published**: 2017-07-21 09:25:55+00:00
- **Updated**: 2017-07-27 23:12:13+00:00
- **Authors**: Ayan Kumar Bhunia, Gautam Kumar, Partha Pratim Roy, R. Balasubramanian, Umapada Pal
- **Comment**: Multimedia Tools and Applications, Springer
- **Journal**: None
- **Summary**: In recent years, recognition of text from natural scene image and video frame has got increased attention among the researchers due to its various complexities and challenges. Because of low resolution, blurring effect, complex background, different fonts, color and variant alignment of text within images and video frames, etc., text recognition in such scenario is difficult. Most of the current approaches usually apply a binarization algorithm to convert them into binary images and next OCR is applied to get the recognition result. In this paper, we present a novel approach based on color channel selection for text recognition from scene images and video frames. In the approach, at first, a color channel is automatically selected and then selected color channel is considered for text recognition. Our text recognition framework is based on Hidden Markov Model (HMM) which uses Pyramidal Histogram of Oriented Gradient features extracted from selected color channel. From each sliding window of a color channel our color-channel selection approach analyzes the image properties from the sliding window and then a multi-label Support Vector Machine (SVM) classifier is applied to select the color channel that will provide the best recognition results in the sliding window. This color channel selection for each sliding window has been found to be more fruitful than considering a single color channel for the whole word image. Five different features have been analyzed for multi-label SVM based color channel selection where wavelet transform based feature outperforms others. Our framework has been tested on different publicly available scene/video text image datasets. For Devanagari script, we collected our own data dataset. The performances obtained from experimental results are encouraging and show the advantage of the proposed method.



### Evaluation of Hashing Methods Performance on Binary Feature Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1707.06825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06825v1)
- **Published**: 2017-07-21 10:17:33+00:00
- **Updated**: 2017-07-21 10:17:33+00:00
- **Authors**: Jacek Komorowski, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we evaluate performance of data-dependent hashing methods on binary data. The goal is to find a hashing method that can effectively produce lower dimensional binary representation of 512-bit FREAK descriptors. A representative sample of recent unsupervised, semi-supervised and supervised hashing methods was experimentally evaluated on large datasets of labelled binary FREAK feature descriptors.



### HMM-based Writer Identification in Music Score Documents without Staff-Line Removal
- **Arxiv ID**: http://arxiv.org/abs/1707.06828v2
- **DOI**: 10.1016/j.eswa.2017.07.031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06828v2)
- **Published**: 2017-07-21 10:34:05+00:00
- **Updated**: 2017-07-27 23:11:51+00:00
- **Authors**: Partha Pratim Roy, Ayan Kumar Bhunia, Umapada Pal
- **Comment**: Expert Systems with Applications, Elsevier(2017)
- **Journal**: None
- **Summary**: Writer identification from musical score documents is a challenging task due to its inherent problem of overlapping of musical symbols with staff lines. Most of the existing works in the literature of writer identification in musical score documents were performed after a preprocessing stage of staff lines removal. In this paper we propose a novel writer identification framework in musical documents without removing staff lines from documents. In our approach, Hidden Markov Model has been used to model the writing style of the writers without removing staff lines. The sliding window features are extracted from musical score lines and they are used to build writer specific HMM models. Given a query musical sheet, writer specific confidence for each musical line is returned by each writer specific model using a loglikelihood score. Next, a loglikelihood score in page level is computed by weighted combination of these scores from the corresponding line images of the page. A novel Factor Analysis based feature selection technique is applied in sliding window features to reduce the noise appearing from staff lines which proves efficiency in writer identification performance.In our framework we have also proposed a novel score line detection approach in musical sheet using HMM. The experiment has been performed in CVC-MUSCIMA dataset and the results obtained that the proposed approach is efficient for score line detection and writer identification without removing staff lines. To get the idea of computation time of our method, detail analysis of execution time is also provided.



### Date-Field Retrieval in Scene Image and Video Frames using Text Enhancement and Shape Coding
- **Arxiv ID**: http://arxiv.org/abs/1707.06833v1
- **DOI**: 10.1016/j.neucom.2016.08.141
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06833v1)
- **Published**: 2017-07-21 10:45:00+00:00
- **Updated**: 2017-07-21 10:45:00+00:00
- **Authors**: Partha Pratim Roy, Ayan Kumar Bhunia, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Text recognition in scene image and video frames is difficult because of low resolution, blur, background noise, etc. Since traditional OCRs do not perform well in such images, information retrieval using keywords could be an alternative way to index/retrieve such text information. Date is a useful piece of information which has various applications including date-wise videos/scene searching, indexing or retrieval. This paper presents a date spotting based information retrieval system for natural scene image and video frames where text appears with complex backgrounds. We propose a line based date spotting approach using Hidden Markov Model (HMM) which is used to detect the date information in a given text. Different date models are searched from a line without segmenting characters or words. Given a text line image in RGB, we apply an efficient gray image conversion to enhance the text information. Wavelet decomposition and gradient sub-bands are used to enhance text information in gray scale. Next, Pyramid Histogram of Oriented Gradient (PHOG) feature has been extracted from gray image and binary images for date-spotting framework. Binary and gray image features are combined by MLP based Tandem approach. Finally, to boost the performance further, a shape coding based scheme is used to combine the similar shape characters in same class during word spotting. For our experiment, three different date models have been constructed to search similar date information having numeric dates that contains numeral values and punctuations and semi-numeric that contains dates with numerals along with months in scene/video text. We have tested our system on 1648 text lines and the results show the effectiveness of our proposed date spotting approach.



### Neuron Pruning for Compressing Deep Networks using Maxout Architectures
- **Arxiv ID**: http://arxiv.org/abs/1707.06838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06838v1)
- **Published**: 2017-07-21 10:53:37+00:00
- **Updated**: 2017-07-21 10:53:37+00:00
- **Authors**: Fernando Moya Rueda, Rene Grzeszick, Gernot A. Fink
- **Comment**: 10 pages, to be published in GCPR2017
- **Journal**: None
- **Summary**: This paper presents an efficient and robust approach for reducing the size of deep neural networks by pruning entire neurons. It exploits maxout units for combining neurons into more complex convex functions and it makes use of a local relevance measurement that ranks neurons according to their activation on the training set for pruning them. Additionally, a parameter reduction comparison between neuron and weight pruning is shown. It will be empirically shown that the proposed neuron pruning reduces the number of parameters dramatically. The evaluation is performed on two tasks, the MNIST handwritten digit recognition and the LFW face verification, using a LeNet-5 and a VGG16 network architecture. The network size is reduced by up to $74\%$ and $61\%$, respectively, without affecting the network's performance. The main advantage of neuron pruning is its direct influence on the size of the network architecture. Furthermore, it will be shown that neuron pruning can be combined with subsequent weight pruning, reducing the size of the LeNet-5 and VGG16 up to $92\%$ and $80\%$ respectively.



### Retinal Microaneurysms Detection using Local Convergence Index Features
- **Arxiv ID**: http://arxiv.org/abs/1707.06865v1
- **DOI**: 10.1109/TIP.2018.2815345
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06865v1)
- **Published**: 2017-07-21 12:30:12+00:00
- **Updated**: 2017-07-21 12:30:12+00:00
- **Authors**: Behdad Dashtbozorg, Jiong Zhang, Bart M. ter Haar Romeny
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal microaneurysms are the earliest clinical sign of diabetic retinopathy disease. Detection of microaneurysms is crucial for the early diagnosis of diabetic retinopathy and prevention of blindness. In this paper, a novel and reliable method for automatic detection of microaneurysms in retinal images is proposed. In the first stage of the proposed method, several preliminary microaneurysm candidates are extracted using a gradient weighting technique and an iterative thresholding approach. In the next stage, in addition to intensity and shape descriptors, a new set of features based on local convergence index filters is extracted for each candidate. Finally, the collective set of features is fed to a hybrid sampling/boosting classifier to discriminate the MAs from non-MAs candidates. The method is evaluated on images with different resolutions and modalities (RGB and SLO) using five publicly available datasets including the Retinopathy Online Challenge's dataset. The proposed method achieves an average sensitivity score of 0.471 on the ROC dataset outperforming state-of-the-art approaches in an extensive comparison. The experimental results on the other four datasets demonstrate the effectiveness and robustness of the proposed microaneurysms detection method regardless of different image resolutions and modalities.



### Semantic Image Synthesis via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.06873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06873v1)
- **Published**: 2017-07-21 12:45:46+00:00
- **Updated**: 2017-07-21 12:45:46+00:00
- **Authors**: Hao Dong, Simiao Yu, Chao Wu, Yike Guo
- **Comment**: Accepted to ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we propose a way of synthesizing realistic images directly with natural language description, which has many useful applications, e.g. intelligent image manipulation. We attempt to accomplish such synthesis: given a source image and a target text description, our model synthesizes images to meet two requirements: 1) being realistic while matching the target text description; 2) maintaining other image features that are irrelevant to the text description. The model should be able to disentangle the semantic information from the two modalities (image and text), and generate new images from the combined semantics. To achieve this, we proposed an end-to-end neural architecture that leverages adversarial learning to automatically learn implicit loss functions, which are optimized to fulfill the aforementioned two requirements. We have evaluated our model by conducting experiments on Caltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated that our model is capable of synthesizing realistic images that match the given descriptions, while still maintain other features of original images.



### Learning Aerial Image Segmentation from Online Maps
- **Arxiv ID**: http://arxiv.org/abs/1707.06879v1
- **DOI**: 10.1109/TGRS.2017.2719738
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06879v1)
- **Published**: 2017-07-21 12:58:18+00:00
- **Updated**: 2017-07-21 12:58:18+00:00
- **Authors**: Pascal Kaiser, Jan Dirk Wegner, Aurelien Lucchi, Martin Jaggi, Thomas Hofmann, Konrad Schindler
- **Comment**: Published in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
- **Journal**: None
- **Summary**: This study deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data-hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps which can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale, publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled, pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.



### What Looks Good with my Sofa: Multimodal Search Engine for Interior Design
- **Arxiv ID**: http://arxiv.org/abs/1707.06907v2
- **DOI**: 10.15439/2017F56
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06907v2)
- **Published**: 2017-07-21 14:08:42+00:00
- **Updated**: 2018-01-08 14:14:59+00:00
- **Authors**: Ivona Tautkute, Aleksandra Możejko, Wojciech Stokowiec, Tomasz Trzciński, Łukasz Brocki, Krzysztof Marasek
- **Comment**: FEDCSIS 5th Conference on Multimedia, Interaction, Design and
  Innovation (MIDI), 2017
- **Journal**: Proceedings of the 2017 Federated Conference on Computer Science
  and Information Systems
- **Summary**: In this paper, we propose a multi-modal search engine for interior design that combines visual and textual queries. The goal of our engine is to retrieve interior objects, e.g. furniture or wall clocks, that share visual and aesthetic similarities with the query. Our search engine allows the user to take a photo of a room and retrieve with a high recall a list of items identical or visually similar to those present in the photo. Additionally, it allows to return other items that aesthetically and stylistically fit well together. To achieve this goal, our system blends the results obtained using textual and visual modalities. Thanks to this blending strategy, we increase the average style similarity score of the retrieved items by 11%. Our work is implemented as a Web-based application and it is planned to be opened to the public.



### Multi-kernel learning of deep convolutional features for action recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.06923v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.06923v2)
- **Published**: 2017-07-21 14:45:48+00:00
- **Updated**: 2017-11-12 09:56:45+00:00
- **Authors**: Biswa Sengupta, Yu Qian
- **Comment**: ICCV 2017 Workshop on Video and Language Understanding: MovieQA and
  the Large Scale Movie Description Challenge
- **Journal**: None
- **Summary**: Image understanding using deep convolutional network has reached human-level performance, yet a closely related problem of video understanding especially, action recognition has not reached the requisite level of maturity. We combine multi-kernels based support-vector-machines (SVM) with a multi-stream deep convolutional neural network to achieve close to state-of-the-art performance on a 51-class activity recognition problem (HMDB-51 dataset); this specific dataset has proved to be particularly challenging for deep neural networks due to the heterogeneity in camera viewpoints, video quality, etc. The resulting architecture is named pillar networks as each (very) deep neural network acts as a pillar for the hierarchical classifiers. In addition, we illustrate that hand-crafted features such as improved dense trajectories (iDT) and Multi-skip Feature Stacking (MIFS), as additional pillars, can further supplement the performance.



### A Multi-Scale CNN and Curriculum Learning Strategy for Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/1707.06978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06978v1)
- **Published**: 2017-07-21 17:16:12+00:00
- **Updated**: 2017-07-21 17:16:12+00:00
- **Authors**: William Lotter, Greg Sorensen, David Cox
- **Comment**: Accepted to MICCAI 2017 Workshop on Deep Learning in Medical Image
  Analysis
- **Journal**: None
- **Summary**: Screening mammography is an important front-line tool for the early detection of breast cancer, and some 39 million exams are conducted each year in the United States alone. Here, we describe a multi-scale convolutional neural network (CNN) trained with a curriculum learning strategy that achieves high levels of accuracy in classifying mammograms. Specifically, we first train CNN-based patch classifiers on segmentation masks of lesions in mammograms, and then use the learned features to initialize a scanning-based model that renders a decision on the whole image, trained end-to-end on outcome data. We demonstrate that our approach effectively handles the "needle in a haystack" nature of full-image mammogram classification, achieving 0.92 AUROC on the DDSM dataset.



### Persistent-homology-based gait recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.06982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06982v1)
- **Published**: 2017-07-21 17:24:54+00:00
- **Updated**: 2017-07-21 17:24:54+00:00
- **Authors**: J. Lamar-Leon, Raul Alonso-Baryolo, Edel Garcia-Reyes, R. Gonzalez-Diaz
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition is an important biometric technique for video surveillance tasks, due to the advantage of using it at distance. In this paper, we present a persistent homology-based method to extract topological features (the so-called {\it topological gait signature}) from the the body silhouettes of a gait sequence. % It has been used before in several conference papers of the same authors for human identification, gender classification, carried object detection and monitoring human activities at distance. % The novelty of this paper is the study of the stability of the topological gait signature under small perturbations and the number of gait cycles contained in a gait sequence. In other words, we show that the topological gait signature is robust to the presence of noise in the body silhouettes and to the number of gait cycles contained in a given gait sequence. % We also show that computing our topological gait signature of only the lowest fourth part of the body silhouette, we avoid the upper body movements that are unrelated to the natural dynamic of the gait, caused for example by carrying a bag or wearing a coat.



### Memory-Efficient Implementation of DenseNets
- **Arxiv ID**: http://arxiv.org/abs/1707.06990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.06990v1)
- **Published**: 2017-07-21 17:51:36+00:00
- **Updated**: 2017-07-21 17:51:36+00:00
- **Authors**: Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der Maaten, Kilian Q. Weinberger
- **Comment**: Technical report
- **Journal**: None
- **Summary**: The DenseNet architecture is highly computationally efficient as a result of feature reuse. However, a naive DenseNet implementation can require a significant amount of GPU memory: If not properly managed, pre-activation batch normalization and contiguous convolution operations can produce feature maps that grow quadratically with network depth. In this technical report, we introduce strategies to reduce the memory consumption of DenseNets during training. By strategically using shared memory allocations, we reduce the memory cost for storing feature maps from quadratic to linear. Without the GPU memory bottleneck, it is now possible to train extremely deep DenseNets. Networks with 14M parameters can be trained on a single GPU, up from 4M. A 264-layer DenseNet (73M parameters), which previously would have been infeasible to train, can now be trained on a single workstation with 8 NVIDIA Tesla M40 GPUs. On the ImageNet ILSVRC classification dataset, this large DenseNet obtains a state-of-the-art single-crop top-1 error of 20.26%.



### Learning Transferable Architectures for Scalable Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.07012v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.07012v4)
- **Published**: 2017-07-21 18:10:26+00:00
- **Updated**: 2018-04-11 05:12:21+00:00
- **Authors**: Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le
- **Comment**: None
- **Journal**: None
- **Summary**: Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.



### Confidence estimation in Deep Neural networks via density modelling
- **Arxiv ID**: http://arxiv.org/abs/1707.07013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07013v1)
- **Published**: 2017-07-21 18:12:15+00:00
- **Updated**: 2017-07-21 18:12:15+00:00
- **Authors**: Akshayvarun Subramanya, Suraj Srinivas, R. Venkatesh Babu
- **Comment**: ICME 2017
- **Journal**: None
- **Summary**: State-of-the-art Deep Neural Networks can be easily fooled into providing incorrect high-confidence predictions for images with small amounts of adversarial noise. Does this expose a flaw with deep neural networks, or do we simply need a better way to estimate confidence? In this paper we consider the problem of accurately estimating predictive confidence. We formulate this problem as that of density modelling, and show how traditional methods such as softmax produce poor estimates. To address this issue, we propose a novel confidence measure based on density modelling approaches. We test these measures on images distorted by blur, JPEG compression, random noise and adversarial noise. Experiments show that our confidence measure consistently shows reduced confidence scores in the presence of such distortions - a property which softmax often lacks.



### What-and-Where to Match: Deep Spatially Multiplicative Integration Networks for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1707.07074v4
- **DOI**: 10.1016/j.patcog.2017.10.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.07074v4)
- **Published**: 2017-07-21 23:50:58+00:00
- **Updated**: 2017-10-14 00:24:20+00:00
- **Authors**: Lin Wu, Yang Wang, Xue Li, Junbin Gao
- **Comment**: Published at Pattern Recognition, Elsevier
- **Journal**: None
- **Summary**: Matching pedestrians across disjoint camera views, known as person re-identification (re-id), is a challenging problem that is of importance to visual recognition and surveillance. Most existing methods exploit local regions within spatial manipulation to perform matching in local correspondence. However, they essentially extract \emph{fixed} representations from pre-divided regions for each image and perform matching based on the extracted representation subsequently. For models in this pipeline, local finer patterns that are crucial to distinguish positive pairs from negative ones cannot be captured, and thus making them underperformed. In this paper, we propose a novel deep multiplicative integration gating function, which answers the question of \emph{what-and-where to match} for effective person re-id. To address \emph{what} to match, our deep network emphasizes common local patterns by learning joint representations in a multiplicative way. The network comprises two Convolutional Neural Networks (CNNs) to extract convolutional activations, and generates relevant descriptors for pedestrian matching. This thus, leads to flexible representations for pair-wise images. To address \emph{where} to match, we combat the spatial misalignment by performing spatially recurrent pooling via a four-directional recurrent neural network to impose spatial dependency over all positions with respect to the entire image. The proposed network is designed to be end-to-end trainable to characterize local pairwise feature interactions in a spatially aligned manner. To demonstrate the superiority of our method, extensive experiments are conducted over three benchmark data sets: VIPeR, CUHK03 and Market-1501.




# Arxiv Papers in cs.CV on 2017-07-10
### Learning in High-Dimensional Multimedia Data: The State of the Art
- **Arxiv ID**: http://arxiv.org/abs/1707.02683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02683v1)
- **Published**: 2017-07-10 03:12:41+00:00
- **Updated**: 2017-07-10 03:12:41+00:00
- **Authors**: Lianli Gao, Jingkuan Song, Xingyi Liu, Junming Shao, Jiajun Liu, Jie Shao
- **Comment**: None
- **Journal**: None
- **Summary**: During the last decade, the deluge of multimedia data has impacted a wide range of research areas, including multimedia retrieval, 3D tracking, database management, data mining, machine learning, social media analysis, medical imaging, and so on. Machine learning is largely involved in multimedia applications of building models for classification and regression tasks etc., and the learning principle consists in designing the models based on the information contained in the multimedia dataset. While many paradigms exist and are widely used in the context of machine learning, most of them suffer from the `curse of dimensionality', which means that some strange phenomena appears when data are represented in a high-dimensional space. Given the high dimensionality and the high complexity of multimedia data, it is important to investigate new machine learning algorithms to facilitate multimedia data analysis. To deal with the impact of high dimensionality, an intuitive way is to reduce the dimensionality. On the other hand, some researchers devoted themselves to designing some effective learning schemes for high-dimensional data. In this survey, we cover feature transformation, feature selection and feature encoding, three approaches fighting the consequences of the curse of dimensionality. Next, we briefly introduce some recent progress of effective learning algorithms. Finally, promising future trends on multimedia learning are envisaged.



### Anisotropic Diffusion-based Kernel Matrix Model for Face Liveness Detection
- **Arxiv ID**: http://arxiv.org/abs/1707.02692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02692v1)
- **Published**: 2017-07-10 04:26:07+00:00
- **Updated**: 2017-07-10 04:26:07+00:00
- **Authors**: Changyong Yu, Yunde Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Facial recognition and verification is a widely used biometric technology in security system. Unfortunately, face biometrics is vulnerable to spoofing attacks using photographs or videos. In this paper, we present an anisotropic diffusion-based kernel matrix model (ADKMM) for face liveness detection to prevent face spoofing attacks. We use the anisotropic diffusion to enhance the edges and boundary locations of a face image, and the kernel matrix model to extract face image features which we call the diffusion-kernel (D-K) features. The D-K features reflect the inner correlation of the face image sequence. We introduce convolution neural networks to extract the deep features, and then, employ a generalized multiple kernel learning method to fuse the D-K features and the deep features to achieve better performance. Our experimental evaluation on the two publicly available datasets shows that the proposed method outperforms the state-of-art face liveness detection methods.



### Topology Reduction in Deep Convolutional Feature Extraction Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.02711v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.FA, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1707.02711v2)
- **Published**: 2017-07-10 06:35:48+00:00
- **Updated**: 2018-03-14 08:59:37+00:00
- **Authors**: Thomas Wiatowski, Philipp Grohs, Helmut Bölcskei
- **Comment**: Corrected errors in arguments on spectral decay of Sobolev functions.
  Replaced part of the decay results (Sections 5-7) by corresponding statements
  for effectively band-limited functions
- **Journal**: Proc. of SPIE (Wavelets and Sparsity XVII), San Diego, USA, Vol.
  10394, pp. 1039418:1-1039418:12, Aug. 2017, (invited paper)
- **Summary**: Deep convolutional neural networks (CNNs) used in practice employ potentially hundreds of layers and $10$,$000$s of nodes. Such network sizes entail significant computational complexity due to the large number of convolutions that need to be carried out; in addition, a large number of parameters needs to be learned and stored. Very deep and wide CNNs may therefore not be well suited to applications operating under severe resource constraints as is the case, e.g., in low-power embedded and mobile platforms. This paper aims at understanding the impact of CNN topology, specifically depth and width, on the network's feature extraction capabilities. We address this question for the class of scattering networks that employ either Weyl-Heisenberg filters or wavelets, the modulus non-linearity, and no pooling. The exponential feature map energy decay results in Wiatowski et al., 2017, are generalized to $\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a>1$ can be realized through suitable choice of the Weyl-Heisenberg prototype function or the mother wavelet. We then show how networks of fixed (possibly small) depth $N$ can be designed to guarantee that $((1-\varepsilon)\cdot 100)\%$ of the input signal's energy are contained in the feature vector. Based on the notion of operationally significant nodes, we characterize, partly rigorously and partly heuristically, the topology-reducing effects of (effectively) band-limited input signals, band-limited filters, and feature map symmetries. Finally, for networks based on Weyl-Heisenberg filters, we determine the prototype function bandwidth that minimizes---for fixed network depth $N$---the average number of operationally significant nodes per layer.



### Interleaved Group Convolutions for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1707.02725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02725v2)
- **Published**: 2017-07-10 07:28:57+00:00
- **Updated**: 2017-07-18 07:46:45+00:00
- **Authors**: Ting Zhang, Guo-Jun Qi, Bin Xiao, Jingdong Wang
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we present a simple and modularized neural network architecture, named interleaved group convolutional neural networks (IGCNets). The main point lies in a novel building block, a pair of two successive interleaved group convolutions: primary group convolution and secondary group convolution. The two group convolutions are complementary: (i) the convolution on each partition in primary group convolution is a spatial convolution, while on each partition in secondary group convolution, the convolution is a point-wise convolution; (ii) the channels in the same secondary partition come from different primary partitions. We discuss one representative advantage: Wider than a regular convolution with the number of parameters and the computation complexity preserved. We also show that regular convolutions, group convolution with summation fusion, and the Xception block are special cases of interleaved group convolutions. Empirical results over standard benchmarks, CIFAR-$10$, CIFAR-$100$, SVHN and ImageNet demonstrate that our networks are more efficient in using parameters and computation complexity with similar or higher accuracy.



### On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.02975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1707.02975v1)
- **Published**: 2017-07-10 07:34:29+00:00
- **Updated**: 2017-07-10 07:34:29+00:00
- **Authors**: Song Wang, Jun Sun, Satoshi Naoi
- **Comment**: Rejected by ICDAR 2017
- **Journal**: None
- **Summary**: The handwritten string recognition is still a challengeable task, though the powerful deep learning tools were introduced. In this paper, based on TAO-FCN, we proposed an end-to-end system for handwritten string recognition. Compared with the conventional methods, there is no preprocess nor manually designed rules employed. With enough labelled data, it is easy to apply the proposed method to different applications. Although the performance of the proposed method may not be comparable with the state-of-the-art approaches, it's usability and robustness are more meaningful for practical applications.



### Synthesis-based Robust Low Resolution Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1707.02733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02733v1)
- **Published**: 2017-07-10 08:12:07+00:00
- **Updated**: 2017-07-10 08:12:07+00:00
- **Authors**: Sumit Shekhar, Vishal M. Patel, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of low resolution face images is a challenging problem in many practical face recognition systems. Methods have been proposed in the face recognition literature for the problem which assume that the probe is low resolution, but a high resolution gallery is available for recognition. These attempts have been aimed at modifying the probe image such that the resultant image provides better discrimination. We formulate the problem differently by leveraging the information available in the high resolution gallery image and propose a dictionary learning approach for classifying the low-resolution probe image. An important feature of our algorithm is that it can handle resolution change along with illumination variations. Furthermore, we also kernelize the algorithm to handle non-linearity in data and present a joint dictionary learning technique for robust recognition at low resolutions. The effectiveness of the proposed method is demonstrated using standard datasets and a challenging outdoor face dataset. It is shown that our method is efficient and can perform significantly better than many competitive low resolution face recognition algorithms.



### Improving speaker turn embedding by crossmodal transfer learning from face embedding
- **Arxiv ID**: http://arxiv.org/abs/1707.02749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02749v1)
- **Published**: 2017-07-10 08:51:53+00:00
- **Updated**: 2017-07-10 08:51:53+00:00
- **Authors**: Nam Le, Jean-Marc Odobez
- **Comment**: None
- **Journal**: None
- **Summary**: Learning speaker turn embeddings has shown considerable improvement in situations where conventional speaker modeling approaches fail. However, this improvement is relatively limited when compared to the gain observed in face embedding learning, which has been proven very successful for face verification and clustering tasks. Assuming that face and voices from the same identities share some latent properties (like age, gender, ethnicity), we propose three transfer learning approaches to leverage the knowledge from the face domain (learned from thousands of images and identities) for tasks in the speaker domain. These approaches, namely target embedding transfer, relative distance transfer, and clustering structure transfer, utilize the structure of the source face embedding space at different granularities to regularize the target speaker turn embedding space as optimizing terms. Our methods are evaluated on two public broadcast corpora and yield promising advances over competitive baselines in verification and audio clustering tasks, especially when dealing with short speaker utterances. The analysis of the results also gives insight into characteristics of the embedding spaces and shows their potential applications.



### Automatic Construction of Real-World Datasets for 3D Object Localization using Two Cameras
- **Arxiv ID**: http://arxiv.org/abs/1707.02978v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1707.02978v3)
- **Published**: 2017-07-10 09:50:48+00:00
- **Updated**: 2018-09-11 08:49:07+00:00
- **Authors**: Joris Guérin, Olivier Gibaru, Eric Nyiri, Stéphane Thiery
- **Comment**: 5 pages, 3 figures, to appear in the proceedings of IECON 2018
  (Special session on Collaborative Robots in Smart Manufacturing), Washington
  D.C., USA
- **Journal**: None
- **Summary**: Unlike classification, position labels cannot be assigned manually by humans. For this reason, generating supervision for precise object localization is a hard task. This paper details a method to create large datasets for 3D object localization, with real world images, using an industrial robot to generate position labels. By knowledge of the geometry of the robot, we are able to automatically synchronize the images of the two cameras and the object 3D position. We applied it to generate a screw-driver localization dataset with stereo images, using a KUKA LBR iiwa robot. This dataset could then be used to train a CNN regressor to learn end-to-end stereo object localization from a set of two standard uncalibrated cameras.



### Deep Reinforcement Learning Attention Selection for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1707.02785v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02785v4)
- **Published**: 2017-07-10 10:17:55+00:00
- **Updated**: 2018-07-07 13:24:42+00:00
- **Authors**: Xu Lan, Hanxiao Wang, Shaogang Gong, Xiatian Zhu
- **Comment**: Additional revision is needed
- **Journal**: None
- **Summary**: Existing person re-identification (re-id) methods assume the provision of accurately cropped person bounding boxes with minimum background noise, mostly by manually cropping. This is significantly breached in practice when person bounding boxes must be detected automatically given a very large number of images and/or videos processed. Compared to carefully cropped manually, auto-detected bounding boxes are far less accurate with random amount of background clutter which can degrade notably person re-id matching accuracy. In this work, we develop a joint learning deep model that optimises person re-id attention selection within any auto-detected person bounding boxes by reinforcement learning of background clutter minimisation subject to re-id label pairwise constraints. Specifically, we formulate a novel unified re-id architecture called Identity DiscriminativE Attention reinforcement Learning (IDEAL) to accurately select re-id attention in auto-detected bounding boxes for optimising re-id performance. Our model can improve re-id accuracy comparable to that from exhaustive human manual cropping of bounding boxes with additional advantages from identity discriminative attention selection that specially benefits re-id tasks beyond human knowledge. Extensive comparative evaluations demonstrate the re-id advantages of the proposed IDEAL model over a wide range of state-of-the-art re-id methods on two auto-detected re-id benchmarks CUHK03 and Market-1501.



### Towards Crafting Text Adversarial Samples
- **Arxiv ID**: http://arxiv.org/abs/1707.02812v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.02812v1)
- **Published**: 2017-07-10 11:58:08+00:00
- **Updated**: 2017-07-10 11:58:08+00:00
- **Authors**: Suranjana Samanta, Sameep Mehta
- **Comment**: 11 pages, 5 figues
- **Journal**: None
- **Summary**: Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. An attacker introduces specially crafted adversarial samples to a deployed classifier, which are being mis-classified by the classifier. However, the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. Most of the prior works have been focused on synthesizing adversarial samples in the image domain. In this paper, we propose a new method of crafting adversarial text samples by modification of the original samples. Modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. Our algorithm works best for the datasets which have sub-categories within each of the classes of examples. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficiency of our proposed method.



### Scale-Regularized Filter Learning
- **Arxiv ID**: http://arxiv.org/abs/1707.02813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02813v1)
- **Published**: 2017-07-10 12:01:30+00:00
- **Updated**: 2017-07-10 12:01:30+00:00
- **Authors**: Marco Loog, François Lauze
- **Comment**: Original submission to SSVM 2015 with a few minor corrections.
  Version with minor changes to appear in Proceedings of the BMVC 2017 as
  \emph{Supervised Scale-Regularized Linear Convolutionary Filters}
- **Journal**: None
- **Summary**: We start out by demonstrating that an elementary learning task, corresponding to the training of a single linear neuron in a convolutional neural network, can be solved for feature spaces of very high dimensionality. In a second step, acknowledging that such high-dimensional learning tasks typically benefit from some form of regularization and arguing that the problem of scale has not been taken care of in a very satisfactory manner, we come to a combined resolution of both of these shortcomings by proposing a form of scale regularization. Moreover, using variational method, this regularization problem can also be solved rather efficiently and we demonstrate, on an artificial filter learning problem, the capabilities of our basic linear neuron. From a more general standpoint, we see this work as prime example of how learning and variational methods could, or even should work to their mutual benefit.



### Adaptive Binarization for Weakly Supervised Affordance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1707.02850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02850v1)
- **Published**: 2017-07-10 13:36:25+00:00
- **Updated**: 2017-07-10 13:36:25+00:00
- **Authors**: Johann Sawatzky, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: The concept of affordance is important to understand the relevance of object parts for a certain functional interaction. Affordance types generalize across object categories and are not mutually exclusive. This makes the segmentation of affordance regions of objects in images a difficult task. In this work, we build on an iterative approach that learns a convolutional neural network for affordance segmentation from sparse keypoints. During this process, the predictions of the network need to be binarized. In this work, we propose an adaptive approach for binarization and estimate the parameters for initialization by approximated cross validation. We evaluate our approach on two affordance datasets where our approach outperforms the state-of-the-art for weakly supervised affordance segmentation.



### Deep Bilateral Learning for Real-Time Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1707.02880v2
- **DOI**: 10.1145/3072959.3073592
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1707.02880v2)
- **Published**: 2017-07-10 14:34:06+00:00
- **Updated**: 2017-08-22 19:26:08+00:00
- **Authors**: Michaël Gharbi, Jiawen Chen, Jonathan T. Barron, Samuel W. Hasinoff, Frédo Durand
- **Comment**: 12 pages, 14 figures, Siggraph 2017
- **Journal**: ACM Trans. Graph. 36, 4, Article 118 (2017)
- **Summary**: Performance is a critical challenge in mobile image processing. Given a reference imaging pipeline, or even human-adjusted pairs of images, we seek to reproduce the enhancements and enable real-time evaluation. For this, we introduce a new neural network architecture inspired by bilateral grid processing and local affine color transforms. Using pairs of input/output images, we train a convolutional neural network to predict the coefficients of a locally-affine model in bilateral space. Our architecture learns to make local, global, and content-dependent decisions to approximate the desired image transformation. At runtime, the neural network consumes a low-resolution version of the input image, produces a set of affine transformations in bilateral space, upsamples those transformations in an edge-preserving fashion using a new slicing node, and then applies those upsampled transformations to the full-resolution image. Our algorithm processes high-resolution images on a smartphone in milliseconds, provides a real-time viewfinder at 1080p resolution, and matches the quality of state-of-the-art approximation techniques on a large class of image operators. Unlike previous work, our model is trained off-line from data and therefore does not require access to the original operator at runtime. This allows our model to learn complex, scene-dependent transformations for which no reference implementation is available, such as the photographic edits of a human retoucher.



### An Analysis of Human-centered Geolocation
- **Arxiv ID**: http://arxiv.org/abs/1707.02905v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02905v3)
- **Published**: 2017-07-10 15:25:02+00:00
- **Updated**: 2018-01-31 16:17:48+00:00
- **Authors**: Kaili Wang, Yu-Hui Huang, Jose Oramas, Luc Van Gool, Tinne Tuytelaars
- **Comment**: WACV'18
- **Journal**: None
- **Summary**: Online social networks contain a constantly increasing amount of images - most of them focusing on people. Due to cultural and climate factors, fashion trends and physical appearance of individuals differ from city to city. In this paper we investigate to what extent such cues can be exploited in order to infer the geographic location, i.e. the city, where a picture was taken. We conduct a user study, as well as an evaluation of automatic methods based on convolutional neural networks. Experiments on the Fashion 144k and a Pinterest-based dataset show that the automatic methods succeed at this task to a reasonable extent. As a matter of fact, our empirical results suggest that automatic methods can surpass human performance by a large margin. Further inspection of the trained models shows that human-centered characteristics, like clothing style, physical features, and accessories, are informative for the task at hand. Moreover, it reveals that also contextual features, e.g. wall type, natural environment, etc., are taken into account by the automatic methods.



### Enhanced Deep Residual Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1707.02921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02921v1)
- **Published**: 2017-07-10 16:07:30+00:00
- **Updated**: 2017-07-10 16:07:30+00:00
- **Authors**: Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee
- **Comment**: To appear in CVPR 2017 workshop. Best paper award of the NTIRE2017
  workshop, and the winners of the NTIRE2017 Challenge on Single Image
  Super-Resolution
- **Journal**: None
- **Summary**: Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.



### Wavelet-based Reflection Symmetry Detection via Textural and Color Histograms
- **Arxiv ID**: http://arxiv.org/abs/1707.02931v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02931v4)
- **Published**: 2017-07-10 16:32:43+00:00
- **Updated**: 2017-07-22 02:05:16+00:00
- **Authors**: Mohamed Elawady, Christophe Ducottet, Olivier Alata, Cecile Barat, Philippe Colantoni
- **Comment**: Draft submission for ICCV 2017 Workshop (Detecting Symmetry in the
  Wild) [Paper track]
- **Journal**: None
- **Summary**: Symmetry is one of the significant visual properties inside an image plane, to identify the geometrically balanced structures through real-world objects. Existing symmetry detection methods rely on descriptors of the local image features and their neighborhood behavior, resulting incomplete symmetrical axis candidates to discover the mirror similarities on a global scale. In this paper, we propose a new reflection symmetry detection scheme, based on a reliable edge-based feature extraction using Log-Gabor filters, plus an efficient voting scheme parameterized by their corresponding textural and color neighborhood information. Experimental evaluation on four single-case and three multiple-case symmetry detection datasets validates the superior achievement of the proposed work to find global symmetries inside an image.



### Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize
- **Arxiv ID**: http://arxiv.org/abs/1707.02937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.02937v1)
- **Published**: 2017-07-10 16:41:22+00:00
- **Updated**: 2017-07-10 16:41:22+00:00
- **Authors**: Andrew Aitken, Christian Ledig, Lucas Theis, Jose Caballero, Zehan Wang, Wenzhe Shi
- **Comment**: None
- **Journal**: None
- **Summary**: The most prominent problem associated with the deconvolution layer is the presence of checkerboard artifacts in output images and dense labels. To combat this problem, smoothness constraints, post processing and different architecture designs have been proposed. Odena et al. highlight three sources of checkerboard artifacts: deconvolution overlap, random initialization and loss functions. In this note, we proposed an initialization method for sub-pixel convolution known as convolution NN resize. Compared to sub-pixel convolution initialized with schemes designed for standard convolution kernels, it is free from checkerboard artifacts immediately after initialization. Compared to resize convolution, at the same computational complexity, it has more modelling power and converges to solutions with smaller test errors.



### Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
- **Arxiv ID**: http://arxiv.org/abs/1707.02968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1707.02968v2)
- **Published**: 2017-07-10 17:54:31+00:00
- **Updated**: 2017-08-04 01:33:22+00:00
- **Authors**: Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta
- **Comment**: ICCV 2017 camera ready
- **Journal**: None
- **Summary**: The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.



### Foot anthropometry device and single object image thresholding
- **Arxiv ID**: http://arxiv.org/abs/1707.03004v1
- **DOI**: 10.5121/sipij.2017.8301
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03004v1)
- **Published**: 2017-07-10 18:21:03+00:00
- **Updated**: 2017-07-10 18:21:03+00:00
- **Authors**: Amir Mohammad Esmaieeli Sikaroudi, Sasan Ghaffari, Ali Yousefi, Hassan Sadeghi Naeini
- **Comment**: None
- **Journal**: Signal & Image Processing : An International Journal (SIPIJ)
  Vol.8, No.3, June 2017
- **Summary**: This paper introduces a device, algorithm and graphical user interface to obtain anthropometric measurements of foot. Presented device facilitates obtaining scale of image and image processing by taking one image from side foot and underfoot simultaneously. Introduced image processing algorithm minimizes a noise criterion, which is suitable for object detection in single object images and outperforms famous image thresholding methods when lighting condition is poor. Performance of image-based method is compared to manual method. Image-based measurements of underfoot in average was 4mm less than actual measures. Mean absolute error of underfoot length was 1.6mm, however length obtained from side foot had 4.4mm mean absolute error. Furthermore, based on t-test and f-test results, no significant difference between manual and image-based anthropometry observed. In order to maintain anthropometry process performance in different situations user interface designed for handling changes in light conditions and altering speed of the algorithm.



### Learning Visual Reasoning Without Strong Priors
- **Arxiv ID**: http://arxiv.org/abs/1707.03017v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1707.03017v5)
- **Published**: 2017-07-10 18:49:28+00:00
- **Updated**: 2017-12-18 21:37:16+00:00
- **Authors**: Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron Courville
- **Comment**: Full AAAI 2018 paper is at arXiv:1709.07871. Presented at ICML 2017's
  Machine Learning in Speech and Language Processing Workshop. Code is at
  http://github.com/ethanjperez/film
- **Journal**: None
- **Summary**: Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.



### Automatic Understanding of Image and Video Advertisements
- **Arxiv ID**: http://arxiv.org/abs/1707.03067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1707.03067v1)
- **Published**: 2017-07-10 21:25:44+00:00
- **Updated**: 2017-07-10 21:25:44+00:00
- **Authors**: Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, Adriana Kovashka
- **Comment**: To appear in CVPR 2017; data available on
  http://cs.pitt.edu/~kovashka/ads
- **Journal**: None
- **Summary**: There is more to images than their objective physical content: for example, advertisements are created to persuade a viewer to take a certain action. We propose the novel problem of automatic advertisement understanding. To enable research on this problem, we create two datasets: an image dataset of 64,832 image ads, and a video dataset of 3,477 ads. Our data contains rich annotations encompassing the topic and sentiment of the ads, questions and answers describing what actions the viewer is prompted to take and the reasoning that the ad presents to persuade the viewer ("What should I do according to this ad, and why should I do it?"), and symbolic references ads make (e.g. a dove symbolizes peace). We also analyze the most common persuasive strategies ads use, and the capabilities that computer vision systems should have to understand these strategies. We present baseline classification results for several prediction tasks, including automatically answering questions about the messages of the ads.




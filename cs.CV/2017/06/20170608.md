# Arxiv Papers in cs.CV on 2017-06-08
### Evaluating (and improving) the correspondence between deep neural networks and human representations
- **Arxiv ID**: http://arxiv.org/abs/1706.02417v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02417v3)
- **Published**: 2017-06-08 00:05:13+00:00
- **Updated**: 2018-07-24 00:57:30+00:00
- **Authors**: Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths
- **Comment**: 35 pages, 8 figures, accepted for publication in Cognitive Science
- **Journal**: None
- **Summary**: Decades of psychological research have been aimed at modeling how people learn features and categories. The empirical validation of these theories is often based on artificial stimuli with simple representations. Recently, deep neural networks have reached or surpassed human accuracy on tasks such as identifying objects in natural images. These networks learn representations of real-world stimuli that can potentially be leveraged to capture psychological representations. We find that state-of-the-art object classification networks provide surprisingly accurate predictions of human similarity judgments for natural images, but fail to capture some of the structure represented by people. We show that a simple transformation that corrects these discrepancies can be obtained through convex optimization. We use the resulting representations to predict the difficulty of learning novel categories of natural images. Our results extend the scope of psychological experiments and computational modeling by enabling tractable use of large natural stimulus sets.



### C-arm Tomographic Imaging Technique for Nephrolithiasis and Detection of Kidney Stones
- **Arxiv ID**: http://arxiv.org/abs/1706.02425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02425v1)
- **Published**: 2017-06-08 01:43:22+00:00
- **Updated**: 2017-06-08 01:43:22+00:00
- **Authors**: Nuhad A. Malalla, Ying Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigated a C-arm tomographic technique as a new three dimensional (3D) kidney imaging method for nephrolithiasis and kidney stone detection over view angle less than 180o. Our C-arm tomographic technique provides a series of two dimensional (2D) images with a single scan over 40o view angle. Experimental studies were performed with a kidney phantom that was formed from a pig kidney with two embedded kidney stones. Different reconstruction methods were developed for C-arm tomographic technique to generate 3D kidney information including: point by point back projection (BP), filtered back projection (FBP), simultaneous algebraic reconstruction technique (SART) and maximum likelihood expectation maximization (MLEM). Computer simulation study was also done with simulated 3D spherical object to evaluate the reconstruction results. Preliminary results demonstrated the capability of our C-arm tomographic technique to generate 3D kidney information for kidney stone detection with low exposure of radiation. The kidney stones are visible on reconstructed planes with identifiable shapes and sizes.



### Image Captioning with Object Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/1706.02430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02430v1)
- **Published**: 2017-06-08 02:23:33+00:00
- **Updated**: 2017-06-08 02:23:33+00:00
- **Authors**: Zhongliang Yang, Yu-Jin Zhang, Sadaqat ur Rehman, Yongfeng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically generating a natural language description of an image is a task close to the heart of image understanding. In this paper, we present a multi-model neural network method closely related to the human visual system that automatically learns to describe the content of images. Our model consists of two sub-models: an object detection and localization model, which extract the information of objects and their spatial relationship in images respectively; Besides, a deep recurrent neural network (RNN) based on long short-term memory (LSTM) units with attention mechanism for sentences generation. Each word of the description will be automatically aligned to different objects of the input image when it is generated. This is similar to the attention mechanism of the human visual system. Experimental results on the COCO dataset showcase the merit of the proposed method, which outperforms previous benchmark models.



### Automatic tracking of vessel-like structures from a single starting point
- **Arxiv ID**: http://arxiv.org/abs/1706.02434v1
- **DOI**: 10.1016/j.compmedimag.2015.11.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02434v1)
- **Published**: 2017-06-08 02:45:27+00:00
- **Updated**: 2017-06-08 02:45:27+00:00
- **Authors**: Dario Augusto Borges Oliveira, Laura Leal-Taixe, Raul Queiroz Feitosa, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: The identification of vascular networks is an important topic in the medical image analysis community. While most methods focus on single vessel tracking, the few solutions that exist for tracking complete vascular networks are usually computationally intensive and require a lot of user interaction. In this paper we present a method to track full vascular networks iteratively using a single starting point. Our approach is based on a cloud of sampling points distributed over concentric spherical layers. We also proposed a vessel model and a metric of how well a sample point fits this model. Then, we implement the network tracking as a min-cost flow problem, and propose a novel optimization scheme to iteratively track the vessel structure by inherently handling bifurcations and paths. The method was tested using both synthetic and real images. On the 9 different data-sets of synthetic blood vessels, we achieved maximum accuracies of more than 98\%. We further use the synthetic data-set to analyse the sensibility of our method to parameter setting, showing the robustness of the proposed algorithm. For real images, we used coronary, carotid and pulmonary data to segment vascular structures and present the visual results. Still for real images, we present numerical and visual results for networks of nerve fibers in the olfactory system. Further visual results also show the potential of our approach for identifying vascular networks topologies. The presented method delivers good results for the several different datasets tested and have potential for segmenting vessel-like structures. Also, the topology information, inherently extracted, can be used for further analysis to computed aided diagnosis and surgical planning. Finally, the method's modular aspect holds potential for problem-oriented adjustments and improvements.



### Learning Deep Representations for Scene Labeling with Semantic Context Guided Supervision
- **Arxiv ID**: http://arxiv.org/abs/1706.02493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02493v2)
- **Published**: 2017-06-08 09:44:00+00:00
- **Updated**: 2017-06-09 04:15:55+00:00
- **Authors**: Zhe Wang, Hongsheng Li, Wanli Ouyang, Xiaogang Wang
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Scene labeling is a challenging classification problem where each input image requires a pixel-level prediction map. Recently, deep-learning-based methods have shown their effectiveness on solving this problem. However, we argue that the large intra-class variation provides ambiguous training information and hinders the deep models' ability to learn more discriminative deep feature representations. Unlike existing methods that mainly utilize semantic context for regularizing or smoothing the prediction map, we design novel supervisions from semantic context for learning better deep feature representations. Two types of semantic context, scene names of images and label map statistics of image patches, are exploited to create label hierarchies between the original classes and newly created subclasses as the learning supervisions. Such subclasses show lower intra-class variation, and help CNN detect more meaningful visual patterns and learn more effective deep features. Novel training strategies and network structure that take advantages of such label hierarchies are introduced. Our proposed method is evaluated extensively on four popular datasets, Stanford Background (8 classes), SIFTFlow (33 classes), Barcelona (170 classes) and LM+Sun datasets (232 classes) with 3 different networks structures, and show state-of-the-art performance. The experiments show that our proposed method makes deep models learn more discriminative feature representations without increasing model size or complexity.



### ToxTrac: a fast and robust software for tracking organisms
- **Arxiv ID**: http://arxiv.org/abs/1706.02577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02577v1)
- **Published**: 2017-06-08 13:37:38+00:00
- **Updated**: 2017-06-08 13:37:38+00:00
- **Authors**: Alvaro Rodriquez, Hanqing Zhang, Jonatan Klaminder, Tomas Brodin, Patrik L. Andersson, Magnus Andersson
- **Comment**: File contains supplementary materials (user guide)
- **Journal**: None
- **Summary**: 1. Behavioral analysis based on video recording is becoming increasingly popular within research fields such as; ecology, medicine, ecotoxicology, and toxicology. However, the programs available to analyze the data, which are; free of cost, user-friendly, versatile, robust, fast and provide reliable statistics for different organisms (invertebrates, vertebrates and mammals) are significantly limited.   2. We present an automated open-source executable software (ToxTrac) for image-based tracking that can simultaneously handle several organisms monitored in a laboratory environment. We compare the performance of ToxTrac with current accessible programs on the web.   3. The main advantages of ToxTrac are: i) no specific knowledge of the geometry of the tracked bodies is needed; ii) processing speed, ToxTrac can operate at a rate >25 frames per second in HD videos using modern desktop computers; iii) simultaneous tracking of multiple organisms in multiple arenas; iv) integrated distortion correction and camera calibration; v) robust against false positives; vi) preservation of individual identification if crossing occurs; vii) useful statistics and heat maps in real scale are exported in: image, text and excel formats.   4. ToxTrac can be used for high speed tracking of insects, fish, rodents or other species, and provides useful locomotor information. We suggest using ToxTrac for future studies of animal behavior independent of research area. Download ToxTrac here: https://toxtrac.sourceforge.io



### Sliced Wasserstein Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1706.02631v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.02631v4)
- **Published**: 2017-06-08 15:16:36+00:00
- **Updated**: 2019-04-15 20:56:19+00:00
- **Authors**: Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool
- **Comment**: This paper is accepted by CVPR 2019, accidentally uploaded as a new
  submission (arXiv:1904.05408, which has been withdrawn). The code is
  available at this https URL https:// github.com/musikisomorphie/swd.git
- **Journal**: None
- **Summary**: In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.



### Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour
- **Arxiv ID**: http://arxiv.org/abs/1706.02677v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.02677v2)
- **Published**: 2017-06-08 16:51:53+00:00
- **Updated**: 2018-04-30 21:53:41+00:00
- **Authors**: Priya Goyal, Piotr Doll√°r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He
- **Comment**: Tech report (v2: correct typos)
- **Journal**: None
- **Summary**: Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.



### Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1706.02684v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.02684v3)
- **Published**: 2017-06-08 17:03:34+00:00
- **Updated**: 2017-10-05 16:32:20+00:00
- **Authors**: Jean-Charles Vialatte, Vincent Gripon, Gilles Coppin
- **Comment**: To appear in 2017, 5th IEEE Global Conference on Signal and
  Information Processing, 5 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: We propose a simple and generic layer formulation that extends the properties of convolutional layers to any domain that can be described by a graph. Namely, we use the support of its adjacency matrix to design learnable weight sharing filters able to exploit the underlying structure of signals in the same fashion as for images. The proposed formulation makes it possible to learn the weights of the filter as well as a scheme that controls how they are shared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.



### Structured Light Phase Measuring Profilometry Pattern Design for Binary Spatial Light Modulators
- **Arxiv ID**: http://arxiv.org/abs/1706.02698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02698v1)
- **Published**: 2017-06-08 17:58:10+00:00
- **Updated**: 2017-06-08 17:58:10+00:00
- **Authors**: Daniel L. Lau, Yu Zhang, Kai Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Structured light illumination is an active 3-D scanning technique based on projecting/capturing a set of striped patterns and measuring the warping of the patterns as they reflect off a target object's surface. In the case of phase measuring profilometry (PMP), the projected patterns are composed of a rolling sinusoidal wave, but as a set of time-multiplexed patterns, PMP requires the target surface to remain motionless or for scanning to be performed at such high rates that any movement is small. But high speed scanning places a significant burden on the projector electronics to produce contone patterns inside of short exposure intervals. Binary patterns are, therefore, of great value, but converting contone patterns into binary comes with significant risk. As such, this paper introduces a contone-to-binary conversion algorithm for deriving binary patterns that best mimic their contone counterparts. Experimental results will show a greater than 3 times reduction in pattern noise over traditional halftoning procedures.



### Causes and Corrections for Bimodal Multipath Scanning with Structured Light
- **Arxiv ID**: http://arxiv.org/abs/1706.02715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02715v1)
- **Published**: 2017-06-08 18:01:04+00:00
- **Updated**: 2017-06-08 18:01:04+00:00
- **Authors**: Yu Zhang, Daniel L. Lau, Ying Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Structured light illumination is an active 3-D scanning technique based on projecting/capturing a set of striped patterns and measuring the warping of the patterns as they reflect off a target object's surface. As designed, each pixel in the camera sees exactly one pixel from the projector; however, there are exceptions to this when the scanned surface has a complicated geometry with step edges and other discontinuities in depth or where the target surface has specularities that reflect light away from the camera. These situations are generally referred to multipath where a given camera pixel receives light from multiple positions from the projector. In the case of bimodal multipath, the camera pixel receives light from exactly two positions from the projector which occurs when light bounce back from a reflective surface or along a step edge where the edge slices through a pixel so that the pixel sees both a foreground and background surface. In this paper, we present a general mathematical model and address the bimodal multipath issue in a phase measuring profilometry scanner to measure the constructive and destructive interference between the two light paths, and by taking advantage of this interesting cue, separate the paths and make two separated depth measurements. We also validate our algorithm with both simulation and a number of challenging real cases.



### CortexNet: a Generic Network Family for Robust Visual Temporal Representations
- **Arxiv ID**: http://arxiv.org/abs/1706.02735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.02735v2)
- **Published**: 2017-06-08 19:17:52+00:00
- **Updated**: 2017-06-14 17:53:32+00:00
- **Authors**: Alfredo Canziani, Eugenio Culurciello
- **Comment**: 8 pages, 4 figures. Edit: 4.2 - define n = t - 1; fix grammar/meaning
  in last sentence. 5.2 - add Open Images data set ref
- **Journal**: None
- **Summary**: In the past five years we have observed the rise of incredibly well performing feed-forward neural networks trained supervisedly for vision related tasks. These models have achieved super-human performance on object recognition, localisation, and detection in still images. However, there is a need to identify the best strategy to employ these networks with temporal visual inputs and obtain a robust and stable representation of video data. Inspired by the human visual system, we propose a deep neural network family, CortexNet, which features not only bottom-up feed-forward connections, but also it models the abundant top-down feedback and lateral connections, which are present in our visual cortex. We introduce two training schemes - the unsupervised MatchNet and weakly supervised TempoNet modes - where a network learns how to correctly anticipate a subsequent frame in a video clip or the identity of its predominant subject, by learning egomotion clues and how to automatically track several objects in the current scene. Find the project website at https://engineering.purdue.edu/elab/CortexNet/.




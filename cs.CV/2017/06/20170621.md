# Arxiv Papers in cs.CV on 2017-06-21
### Multi-Modal Trip Hazard Affordance Detection On Construction Sites
- **Arxiv ID**: http://arxiv.org/abs/1706.06718v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06718v1)
- **Published**: 2017-06-21 01:58:18+00:00
- **Updated**: 2017-06-21 01:58:18+00:00
- **Authors**: Sean McMahon, Niko Sünderhauf, Ben Upcroft, Michael Milford
- **Comment**: 9 Pages, 12 Figures, 2 Tables, Accepted to Robotics and Automation
  Letters (RA-L)
- **Journal**: None
- **Summary**: Trip hazards are a significant contributor to accidents on construction and manufacturing sites, where over a third of Australian workplace injuries occur [1]. Current safety inspections are labour intensive and limited by human fallibility,making automation of trip hazard detection appealing from both a safety and economic perspective. Trip hazards present an interesting challenge to modern learning techniques because they are defined as much by affordance as by object type; for example wires on a table are not a trip hazard, but can be if lying on the ground. To address these challenges, we conduct a comprehensive investigation into the performance characteristics of 11 different colour and depth fusion approaches, including 4 fusion and one non fusion approach; using colour and two types of depth images. Trained and tested on over 600 labelled trip hazards over 4 floors and 2000m$\mathrm{^{2}}$ in an active construction site,this approach was able to differentiate between identical objects in different physical configurations (see Figure 1). Outperforming a colour-only detector, our multi-modal trip detector fuses colour and depth information to achieve a 4% absolute improvement in F1-score. These investigative results and the extensive publicly available dataset moves us one step closer to assistive or fully automated safety inspection systems on construction sites.



### Deep Learning Autoencoder Approach for Handwritten Arabic Digits Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.06720v1
- **DOI**: 10.1007/978-3-319-48308-5_54
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1706.06720v1)
- **Published**: 2017-06-21 02:02:31+00:00
- **Updated**: 2017-06-21 02:02:31+00:00
- **Authors**: Mohamed Loey, Ahmed El-Sawy, Hazem EL-Bakry
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: This paper presents a new unsupervised learning approach with stacked autoencoder (SAE) for Arabic handwritten digits categorization. Recently, Arabic handwritten digits recognition has been an important area due to its applications in several fields. This work is focusing on the recognition part of handwritten Arabic digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases. Arabic digits contains ten numbers that were descended from the Indian digits system. Stacked autoencoder (SAE) tested and trained the MADBase database (Arabic handwritten digits images) that contain 10000 testing images and 60000 training images. We show that the use of SAE leads to significant improvements across different machine-learning classification algorithms. SAE is giving an average accuracy of 98.5%.



### GPGPU Acceleration of the KAZE Image Feature Extraction Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1706.06750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06750v1)
- **Published**: 2017-06-21 06:14:42+00:00
- **Updated**: 2017-06-21 06:14:42+00:00
- **Authors**: Ramkumar B, R. S. Hegde, Rob Laber, Hristo Bojinov
- **Comment**: 10 pages,7 figures
- **Journal**: None
- **Summary**: The recently proposed open-source KAZE image feature detection and description algorithm offers unprecedented performance in comparison to conventional ones like SIFT and SURF as it relies on nonlinear scale spaces instead of Gaussian linear scale spaces. The improved performance, however, comes with a significant computational cost limiting its use for many applications. We report a GPGPU implementation of the KAZE algorithm without resorting to binary descriptors for gaining speedup. For a 1920 by 1200 sized image our Compute Unified Device Architecture (CUDA) C based GPU version took around 300 milliseconds on a NVIDIA GeForce GTX Titan X (Maxwell Architecture-GM200) card in comparison to nearly 2400 milliseconds for a multithreaded CPU version (16 threaded Intel(R) Xeon(R) CPU E5-2650 processsor). The CUDA based parallel implementation is described in detail with fine-grained comparison between the GPU and CPU implementations. By achieving nearly 8 fold speedup without performance degradation our work expands the applicability of the KAZE algorithm. Additionally, the strategies described here can prove useful for the GPU implementation of other nonlinear scale space based methods.



### Comicolorization: Semi-Automatic Manga Colorization
- **Arxiv ID**: http://arxiv.org/abs/1706.06759v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1706.06759v4)
- **Published**: 2017-06-21 06:52:09+00:00
- **Updated**: 2017-09-28 12:58:52+00:00
- **Authors**: Chie Furusawa, Kazuyuki Hiroshiba, Keisuke Ogaki, Yuri Odagiri
- **Comment**: to appear in SIGGRAPH Asia 2017 Technical Brief. Project page:
  https://nico-opendata.jp/en/casestudy/comicolorization/index.html
- **Journal**: None
- **Summary**: We developed "Comicolorization", a semi-automatic colorization system for manga images. Given a monochrome manga and reference images as inputs, our system generates a plausible color version of the manga. This is the first work to address the colorization of an entire manga title (a set of manga pages). Our method colorizes a whole page (not a single panel) semi-automatically, with the same color for the same character across multiple panels. To colorize the target character by the color from the reference image, we extract a color feature from the reference and feed it to the colorization network to help the colorization. Our approach employs adversarial loss to encourage the effect of the color features. Optionally, our tool allows users to revise the colorization result interactively. By feeding the color features to our deep colorization network, we accomplish colorization of the entire manga using the desired colors for each panel.



### Saliency Guided End-to-End Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1706.06768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06768v1)
- **Published**: 2017-06-21 07:29:21+00:00
- **Updated**: 2017-06-21 07:29:21+00:00
- **Authors**: Baisheng Lai, Xiaojin Gong
- **Comment**: Accepted to appear in IJCAI 2017
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD), which is the problem of learning detectors using only image-level labels, has been attracting more and more interest. However, this problem is quite challenging due to the lack of location supervision. To address this issue, this paper integrates saliency into a deep architecture, in which the location in- formation is explored both explicitly and implicitly. Specifically, we select highly confident object pro- posals under the guidance of class-specific saliency maps. The location information, together with semantic and saliency information, of the selected proposals are then used to explicitly supervise the network by imposing two additional losses. Meanwhile, a saliency prediction sub-network is built in the architecture. The prediction results are used to implicitly guide the localization procedure. The entire network is trained end-to-end. Experiments on PASCAL VOC demonstrate that our approach outperforms all state-of-the-arts.



### Object Detection Using Deep CNNs Trained on Synthetic Images
- **Arxiv ID**: http://arxiv.org/abs/1706.06782v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06782v2)
- **Published**: 2017-06-21 08:16:29+00:00
- **Updated**: 2017-09-18 11:42:06+00:00
- **Authors**: Param S. Rajpura, Hristo Bojinov, Ravi S. Hegde
- **Comment**: None
- **Journal**: None
- **Summary**: The need for large annotated image datasets for training Convolutional Neural Networks (CNNs) has been a significant impediment for their adoption in computer vision applications. We show that with transfer learning an effective object detector can be trained almost entirely on synthetically rendered datasets. We apply this strategy for detecting pack- aged food products clustered in refrigerator scenes. Our CNN trained only with 4000 synthetic images achieves mean average precision (mAP) of 24 on a test set with 55 distinct products as objects of interest and 17 distractor objects. A further increase of 12% in the mAP is obtained by adding only 400 real images to these 4000 synthetic images in the training set. A high degree of photorealism in the synthetic images was not essential in achieving this performance. We analyze factors like training data set size and 3D model dictionary size for their influence on detection performance. Additionally, training strategies like fine-tuning with selected layers and early stopping which affect transfer learning from synthetic scenes to real scenes are explored. Training CNNs with synthetic datasets is a novel application of high-performance computing and a promising approach for object detection applications in domains where there is a dearth of large annotated image data.



### GM-Net: Learning Features with More Efficiency
- **Arxiv ID**: http://arxiv.org/abs/1706.06792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06792v1)
- **Published**: 2017-06-21 08:45:15+00:00
- **Updated**: 2017-06-21 08:45:15+00:00
- **Authors**: Yujia Chen, Ce Li
- **Comment**: 6 Pages, 5 figures
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) are capable of learning unprecedentedly effective features from images. Some researchers have struggled to enhance the parameters' efficiency using grouped convolution. However, the relation between the optimal number of convolutional groups and the recognition performance remains an open problem. In this paper, we propose a series of Basic Units (BUs) and a two-level merging strategy to construct deep CNNs, referred to as a joint Grouped Merging Net (GM-Net), which can produce joint grouped and reused deep features while maintaining the feature discriminability for classification tasks. Our GM-Net architectures with the proposed BU_A (dense connection) and BU_B (straight mapping) lead to significant reduction in the number of network parameters and obtain performance improvement in image classification tasks. Extensive experiments are conducted to validate the superior performance of the GM-Net than the state-of-the-arts on the benchmark datasets, e.g., MNIST, CIFAR-10, CIFAR-100 and SVHN.



### MEC: Memory-efficient Convolution for Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1706.06873v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1706.06873v1)
- **Published**: 2017-06-21 13:00:39+00:00
- **Updated**: 2017-06-21 13:00:39+00:00
- **Authors**: Minsik Cho, Daniel Brand
- **Comment**: ICML2017
- **Journal**: None
- **Summary**: Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2col-based convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memory-overhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.



### Learnable pooling with Context Gating for video classification
- **Arxiv ID**: http://arxiv.org/abs/1706.06905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06905v2)
- **Published**: 2017-06-21 13:49:14+00:00
- **Updated**: 2018-03-05 12:30:37+00:00
- **Authors**: Antoine Miech, Ivan Laptev, Josef Sivic
- **Comment**: Presented at Youtube 8M CVPR17 Workshop. Kaggle Winning model. Under
  review for TPAMI
- **Journal**: None
- **Summary**: Current methods for video analysis often extract frame-level features using pre-trained convolutional neural networks (CNNs). Such features are then aggregated over time e.g., by simple temporal averaging or more sophisticated recurrent neural networks such as long short-term memory (LSTM) or gated recurrent units (GRU). In this work we revise existing video representations and study alternative methods for temporal aggregation. We first explore clustering-based aggregation layers and propose a two-stream architecture aggregating audio and visual features. We then introduce a learnable non-linear unit, named Context Gating, aiming to model interdependencies among network activations. Our experimental results show the advantage of both improvements for the task of video classification. In particular, we evaluate our method on the large-scale multi-modal Youtube-8M v2 dataset and outperform all other methods in the Youtube 8M Large-Scale Video Understanding challenge.



### Class-specific image denoising using importance sampling
- **Arxiv ID**: http://arxiv.org/abs/1706.06917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06917v1)
- **Published**: 2017-06-21 14:11:29+00:00
- **Updated**: 2017-06-21 14:11:29+00:00
- **Authors**: Milad Niknejad, Jose M. Bioucas-Dias, Mario A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new image denoising method, tailored to specific classes of images, assuming that a dataset of clean images of the same class is available. Similarly to the non-local means (NLM) algorithm, the proposed method computes a weighted average of non-local patches, which we interpret under the importance sampling framework. This viewpoint introduces flexibility regarding the adopted priors, the noise statistics, and the computation of Bayesian estimates. The importance sampling viewpoint is exploited to approximate the minimum mean squared error (MMSE) patch estimates, using the true underlying prior on image patches. The estimates thus obtained converge to the true MMSE estimates, as the number of samples approaches infinity. Experimental results provide evidence that the proposed denoiser outperforms the state-of-the-art in the specific classes of face and text images.



### cGAN-based Manga Colorization Using a Single Training Image
- **Arxiv ID**: http://arxiv.org/abs/1706.06918v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, 68U10, 68U05
- **Links**: [PDF](http://arxiv.org/pdf/1706.06918v1)
- **Published**: 2017-06-21 14:11:32+00:00
- **Updated**: 2017-06-21 14:11:32+00:00
- **Authors**: Paulina Hensman, Kiyoharu Aizawa
- **Comment**: 8 pages, 13 figures
- **Journal**: None
- **Summary**: The Japanese comic format known as Manga is popular all over the world. It is traditionally produced in black and white, and colorization is time consuming and costly. Automatic colorization methods generally rely on greyscale values, which are not present in manga. Furthermore, due to copyright protection, colorized manga available for training is scarce. We propose a manga colorization method based on conditional Generative Adversarial Networks (cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of training images, our method requires only a single colorized reference image for training, avoiding the need of a large dataset. Colorizing manga using cGANs can produce blurry results with artifacts, and the resolution is limited. We therefore also propose a method of segmentation and color-correction to mitigate these issues. The final results are sharp, clear, and in high resolution, and stay true to the character's original color scheme.



### Graphcut Texture Synthesis for Single-Image Superresolution
- **Arxiv ID**: http://arxiv.org/abs/1706.06942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06942v1)
- **Published**: 2017-06-21 14:54:15+00:00
- **Updated**: 2017-06-21 14:54:15+00:00
- **Authors**: Douglas Summers-Stay
- **Comment**: NYU Master's Thesis from 2006
- **Journal**: None
- **Summary**: Texture synthesis has proven successful at imitating a wide variety of textures. Adding additional constraints (in the form of a low-resolution version of the texture to be synthesized) makes it possible to use texture synthesis methods for texture superresolution.



### Comparing deep neural networks against humans: object recognition when the signal gets weaker
- **Arxiv ID**: http://arxiv.org/abs/1706.06969v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1706.06969v2)
- **Published**: 2017-06-21 15:46:52+00:00
- **Updated**: 2018-12-11 17:06:24+00:00
- **Authors**: Robert Geirhos, David H. J. Janssen, Heiko H. Schütt, Jonas Rauber, Matthias Bethge, Felix A. Wichmann
- **Comment**: updated article with reference to resulting publication (Geirhos et
  al, NeurIPS 2018)
- **Journal**: None
- **Summary**: Human visual object recognition is typically rapid and seemingly effortless, as well as largely independent of viewpoint and object orientation. Until very recently, animate visual systems were the only ones capable of this remarkable computational feat. This has changed with the rise of a class of computer vision algorithms called deep neural networks (DNNs) that achieve human-level classification performance on object recognition tasks. Furthermore, a growing number of studies report similarities in the way DNNs and the human visual system process objects, suggesting that current DNNs may be good models of human visual object recognition. Yet there clearly exist important architectural and processing differences between state-of-the-art DNNs and the primate visual system. The potential behavioural consequences of these differences are not well understood. We aim to address this issue by comparing human and DNN generalisation abilities towards image degradations. We find the human visual system to be more robust to image manipulations like contrast reduction, additive noise or novel eidolon-distortions. In addition, we find progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker, indicating that there may still be marked differences in the way humans and current DNNs perform visual object recognition. We envision that our findings as well as our carefully measured and freely available behavioural datasets provide a new useful benchmark for the computer vision community to improve the robustness of DNNs and a motivation for neuroscientists to search for mechanisms in the brain that could facilitate this robustness.



### Scalable Online Convolutional Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1706.06972v3
- **DOI**: 10.1109/TIP.2018.2842152
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06972v3)
- **Published**: 2017-06-21 15:50:12+00:00
- **Updated**: 2017-11-02 15:55:05+00:00
- **Authors**: Yaqing Wang, Quanming Yao, James T. Kwok, Lionel M. Ni
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional sparse coding (CSC) improves sparse coding by learning a shift-invariant dictionary from the data. However, existing CSC algorithms operate in the batch mode and are expensive, in terms of both space and time, on large datasets. In this paper, we alleviate these problems by using online learning. The key is a reformulation of the CSC objective so that convolution can be handled easily in the frequency domain and much smaller history matrices are needed. We use the alternating direction method of multipliers (ADMM) to solve the resulting optimization problem and the ADMM subproblems have efficient closed-form solutions. Theoretical analysis shows that the learned dictionary converges to a stationary point of the optimization problem. Extensive experiments show that convergence of the proposed method is much faster and its reconstruction performance is also better. Moreover, while existing CSC algorithms can only run on a small number of images, the proposed method can handle at least ten times more images.



### Two-Stream Convolutional Networks for Dynamic Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1706.06982v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.06982v4)
- **Published**: 2017-06-21 16:09:28+00:00
- **Updated**: 2018-04-12 21:39:51+00:00
- **Authors**: Matthew Tesfaldet, Marcus A. Brubaker, Konstantinos G. Derpanis
- **Comment**: In proc. CVPR 2018. Full results available at
  https://ryersonvisionlab.github.io/two-stream-projpage/
- **Journal**: None
- **Summary**: We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.



### Uncertainty-Aware Organ Classification for Surgical Data Science Applications in Laparoscopy
- **Arxiv ID**: http://arxiv.org/abs/1706.07002v2
- **DOI**: 10.1109/TBME.2018.2813015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.07002v2)
- **Published**: 2017-06-21 16:49:39+00:00
- **Updated**: 2018-10-19 12:38:24+00:00
- **Authors**: S. Moccia, S. J. Wirkert, H. Kenngott, A. S. Vemuri, M. Apitz, B. Mayer, E. De Momi, L. S. Mattos, L. Maier-Hein
- **Comment**: 7 pages, 6 images, 2 tables
- **Journal**: None
- **Summary**: Objective: Surgical data science is evolving into a research field that aims to observe everything occurring within and around the treatment process to provide situation-aware data-driven assistance. In the context of endoscopic video analysis, the accurate classification of organs in the field of view of the camera proffers a technical challenge. Herein, we propose a new approach to anatomical structure classification and image tagging that features an intrinsic measure of confidence to estimate its own performance with high reliability and which can be applied to both RGB and multispectral imaging (MI) data. Methods: Organ recognition is performed using a superpixel classification strategy based on textural and reflectance information. Classification confidence is estimated by analyzing the dispersion of class probabilities. Assessment of the proposed technology is performed through a comprehensive in vivo study with seven pigs. Results: When applied to image tagging, mean accuracy in our experiments increased from 65% (RGB) and 80% (MI) to 90% (RGB) and 96% (MI) with the confidence measure. Conclusion: Results showed that the confidence measure had a significant influence on the classification accuracy, and MI data are better suited for anatomical structure labeling than RGB data. Significance: This work significantly enhances the state of art in automatic labeling of endoscopic videos by introducing the use of the confidence metric, and by being the first study to use MI data for in vivo laparoscopic tissue classification. The data of our experiments will be released as the first in vivo MI dataset upon publication of this paper.



### Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1706.07036v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.07036v1)
- **Published**: 2017-06-21 17:56:59+00:00
- **Updated**: 2017-06-21 17:56:59+00:00
- **Authors**: Chen-Hsuan Lin, Chen Kong, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional methods of 3D object generative modeling learn volumetric predictions using deep networks with 3D convolutional operations, which are direct analogies to classical 2D ones. However, these methods are computationally wasteful in attempt to predict 3D shapes, where information is rich only on the surfaces. In this paper, we propose a novel 3D generative modeling framework to efficiently generate object shapes in the form of dense point clouds. We use 2D convolutional operations to predict the 3D structure from multiple viewpoints and jointly apply geometric reasoning with 2D projection optimization. We introduce the pseudo-renderer, a differentiable module to approximate the true rendering operation, to synthesize novel depth maps for optimization. Experimental results for single-image 3D object reconstruction tasks show that we outperforms state-of-the-art methods in terms of shape similarity and prediction density.




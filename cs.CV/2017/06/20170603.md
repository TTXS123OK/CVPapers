# Arxiv Papers in cs.CV on 2017-06-03
### IDK Cascades: Fast Deep Learning by Learning not to Overthink
- **Arxiv ID**: http://arxiv.org/abs/1706.00885v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.00885v4)
- **Published**: 2017-06-03 02:29:12+00:00
- **Updated**: 2018-06-27 07:11:26+00:00
- **Authors**: Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov, Fisher Yu, Joseph E. Gonzalez
- **Comment**: UAI 2018 camera ready
- **Journal**: None
- **Summary**: Advances in deep learning have led to substantial increases in prediction accuracy but have been accompanied by increases in the cost of rendering predictions. We conjecture that fora majority of real-world inputs, the recent advances in deep learning have created models that effectively "overthink" on simple inputs. In this paper, we revisit the classic question of building model cascades that primarily leverage class asymmetry to reduce cost. We introduce the "I Don't Know"(IDK) prediction cascades framework, a general framework to systematically compose a set of pre-trained models to accelerate inference without a loss in prediction accuracy. We propose two search based methods for constructing cascades as well as a new cost-aware objective within this framework. The proposed IDK cascade framework can be easily adopted in the existing model serving systems without additional model re-training. We evaluate the proposed techniques on a range of benchmarks to demonstrate the effectiveness of the proposed framework.



### Learning Person Trajectory Representations for Team Activity Analysis
- **Arxiv ID**: http://arxiv.org/abs/1706.00893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00893v1)
- **Published**: 2017-06-03 03:44:42+00:00
- **Updated**: 2017-06-03 03:44:42+00:00
- **Authors**: Nazanin Mehrasa, Yatao Zhong, Frederick Tung, Luke Bornn, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: Activity analysis in which multiple people interact across a large space is challenging due to the interplay of individual actions and collective group dynamics. We propose an end-to-end approach for learning person trajectory representations for group activity analysis. The learned representations encode rich spatio-temporal dependencies and capture useful motion patterns for recognizing individual events, as well as characteristic group dynamics that can be used to identify groups from their trajectories alone. We develop our deep learning approach in the context of team sports, which provide well-defined sets of events (e.g. pass, shot) and groups of people (teams). Analysis of events and team formations using NHL hockey and NBA basketball datasets demonstrate the generality of our approach.



### Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1706.00906v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00906v3)
- **Published**: 2017-06-03 07:37:59+00:00
- **Updated**: 2017-09-28 11:11:57+00:00
- **Authors**: Hu Han, Anil K. Jain, Fang Wang, Shiguang Shan, Xilin Chen
- **Comment**: To appear in the IEEE Trans. Pattern Analysis and Machine
  Intelligence (final)
- **Journal**: None
- **Summary**: Face attribute estimation has many potential applications in video surveillance, face retrieval, and social media. While a number of methods have been proposed for face attribute estimation, most of them did not explicitly consider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal and holistic vs. local) during feature representation learning. In this paper, we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate multiple heterogeneous attributes from a single face image. In DMTL, we tackle attribute correlation and heterogeneity with convolutional neural networks (CNNs) consisting of shared feature learning for all the attributes, and category-specific feature learning for heterogeneous attributes. We also introduce an unconstrained face database (LFW+), an extension of public-domain LFW, with heterogeneous demographic attributes (age, gender, and race) obtained via crowdsourcing. Experimental results on benchmarks with multiple face attributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed approach has superior performance compared to state of the art. Finally, evaluations on a public-domain face database (LAP) with a single attribute show that the proposed approach has excellent generalization ability.



### Learning by Association - A versatile semi-supervised training method for neural networks
- **Arxiv ID**: http://arxiv.org/abs/1706.00909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1706.00909v1)
- **Published**: 2017-06-03 08:08:56+00:00
- **Updated**: 2017-06-03 08:08:56+00:00
- **Authors**: Philip Häusser, Alexander Mordvintsev, Daniel Cremers
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2017
- **Journal**: None
- **Summary**: In many real-world scenarios, labeled data for a specific machine learning task is costly to obtain. Semi-supervised training methods make use of abundantly available unlabeled data and a smaller number of labeled examples. We propose a new framework for semi-supervised training of deep neural networks inspired by learning in humans. "Associations" are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data. In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN.



### Discrete Gyrator Transforms: Computational Algorithms and Applications
- **Arxiv ID**: http://arxiv.org/abs/1707.03689v1
- **DOI**: 10.1109/TSP.2015.2437845
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1707.03689v1)
- **Published**: 2017-06-03 08:53:16+00:00
- **Updated**: 2017-06-03 08:53:16+00:00
- **Authors**: Soo-Chang Pei, Shih-Gu Huang, Jian-Jiun Ding
- **Comment**: Accepted by IEEE Transactions on Signal Processing
- **Journal**: IEEE Transactions on Signal Processing, Volume 63, Issue 16,
  Aug.15, 2015
- **Summary**: As an extension of the 2D fractional Fourier transform (FRFT) and a special case of the 2D linear canonical transform (LCT), the gyrator transform was introduced to produce rotations in twisted space/spatial-frequency planes. It is a useful tool in optics, signal processing and image processing. In this paper, we develop discrete gyrator transforms (DGTs) based on the 2D LCT. Taking the advantage of the additivity property of the 2D LCT, we propose three kinds of DGTs, each of which is a cascade of low-complexity operators. These DGTs have different constraints, characteristics, and properties, and are realized by different computational algorithms. Besides, we propose a kind of DGT based on the eigenfunctions of the gyrator transform. This DGT is an orthonormal transform, and thus its comprehensive properties, especially the additivity property, make it more useful in many applications. We also develop an efficient computational algorithm to significantly reduce the complexity of this DGT. At the end, a brief review of some important applications of the DGTs is presented, including mode conversion, sampling and reconstruction, watermarking, and image encryption.



### Deep-Learning Convolutional Neural Networks for scattered shrub detection with Google Earth Imagery
- **Arxiv ID**: http://arxiv.org/abs/1706.00917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00917v1)
- **Published**: 2017-06-03 09:13:22+00:00
- **Updated**: 2017-06-03 09:13:22+00:00
- **Authors**: Emilio Guirado, Siham Tabik, Domingo Alcaraz-Segura, Javier Cabello, Francisco Herrera
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image can hardly be extrapolated to a different image. Recently, the deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in the field of computer vision. However, they have not been fully explored yet in land cover mapping for detecting species of high biodiversity conservation interest. This paper analyzes the potential of CNNs-based methods for plant species detection using free high-resolution Google Earth T M images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. According to our results, compared to OBIA-based methods, the proposed CNN-based detection model, in combination with data-augmentation, transfer learning and pre-processing, achieves higher performance with less human intervention and the knowledge it acquires in the first image can be transferred to other images, which makes the detection process very fast. The provided methodology can be systematically reproduced for other species detection.



### Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1706.00931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00931v2)
- **Published**: 2017-06-03 11:07:23+00:00
- **Updated**: 2022-04-24 05:04:29+00:00
- **Authors**: Xiangbo Shu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamics for single-person action recognition due to its ability of modeling the temporal information in various ranges of dynamic contexts. However, existing RNN models only focus on capturing the temporal dynamics of the person-person interactions by naively combining the activity dynamics of individuals or modeling them as a whole. This neglects the inter-related dynamics of how person-person interactions change over time. To this end, we propose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to model the long-term inter-related dynamics between two interacting people on the bounding boxes covering people. Specifically, for each frame, two sub-memory units store individual motion information, while a concurrent LSTM unit selectively integrates and stores inter-related motion information between interacting people from these two sub-memory units via a new co-memory cell. Experimental results on the BIT and UT datasets show the superiority of Co-LSTSM compared with the state-of-the-art methods.



### See, Hear, and Read: Deep Aligned Representations
- **Arxiv ID**: http://arxiv.org/abs/1706.00932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00932v1)
- **Published**: 2017-06-03 11:11:13+00:00
- **Updated**: 2017-06-03 11:11:13+00:00
- **Authors**: Yusuf Aytar, Carl Vondrick, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.



### Graph-Cut RANSAC
- **Arxiv ID**: http://arxiv.org/abs/1706.00984v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00984v2)
- **Published**: 2017-06-03 17:52:53+00:00
- **Updated**: 2017-11-16 08:29:03+00:00
- **Authors**: Daniel Barath, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: A novel method for robust estimation, called Graph-Cut RANSAC, GC-RANSAC in short, is introduced. To separate inliers and outliers, it runs the graph-cut algorithm in the local optimization (LO) step which is applied when a so-far-the-best model is found. The proposed LO step is conceptually simple, easy to implement, globally optimal and efficient. GC-RANSAC is shown experimentally, both on synthesized tests and real image pairs, to be more geometrically accurate than state-of-the-art methods on a range of problems, e.g. line fitting, homography, affine transformation, fundamental and essential matrix estimation. It runs in real-time for many problems at a speed approximately equal to that of the less accurate alternatives (in milliseconds on standard CPU).



### Order embeddings and character-level convolutions for multimodal alignment
- **Arxiv ID**: http://arxiv.org/abs/1706.00999v1
- **DOI**: 10.1016/j.patrec.2017.11.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.00999v1)
- **Published**: 2017-06-03 20:24:32+00:00
- **Updated**: 2017-06-03 20:24:32+00:00
- **Authors**: Jônatas Wehrmann, Anderson Mattjie, Rodrigo C. Barros
- **Comment**: 7 pages, 5 figures, submitted to Pattern Recognition Letters
- **Journal**: Pattern Recognition Letters, vol. 102, 15, January 2018
- **Summary**: With the novel and fast advances in the area of deep neural networks, several challenging image-based tasks have been recently approached by researchers in pattern recognition and computer vision. In this paper, we address one of these tasks, which is to match image content with natural language descriptions, sometimes referred as multimodal content retrieval. Such a task is particularly challenging considering that we must find a semantic correspondence between captions and the respective image, a challenge for both computer vision and natural language processing areas. For such, we propose a novel multimodal approach based solely on convolutional neural networks for aligning images with their captions by directly convolving raw characters. Our proposed character-based textual embeddings allow the replacement of both word-embeddings and recurrent neural networks for text understanding, saving processing time and requiring fewer learnable parameters. Our method is based on the idea of projecting both visual and textual information into a common embedding space. For training such embeddings we optimize a contrastive loss function that is computed to minimize order-violations between images and their respective descriptions. We achieve state-of-the-art performance in the largest and most well-known image-text alignment dataset, namely Microsoft COCO, with a method that is conceptually much simpler and that possesses considerably fewer parameters than current approaches.



### Image Compression Based on Compressive Sensing: End-to-End Comparison with JPEG
- **Arxiv ID**: http://arxiv.org/abs/1706.01000v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1706.01000v3)
- **Published**: 2017-06-03 20:35:30+00:00
- **Updated**: 2020-01-19 03:26:54+00:00
- **Authors**: Xin Yuan, Raziel Haimi-Cohen
- **Comment**: 17 pages, 13 figures
- **Journal**: IEEE Transactions on Multimedia 2020
- **Summary**: We present an end-to-end image compression system based on compressive sensing. The presented system integrates the conventional scheme of compressive sampling and reconstruction with quantization and entropy coding. The compression performance, in terms of decoded image quality versus data rate, is shown to be comparable with JPEG and significantly better at the low rate range. We study the parameters that influence the system performance, including (i) the choice of sensing matrix, (ii) the trade-off between quantization and compression ratio, and (iii) the reconstruction algorithms. We propose an effective method to jointly control the quantization step and compression ratio in order to achieve near optimal quality at any given bit rate. Furthermore, our proposed image compression system can be directly used in the compressive sensing camera, e.g. the single pixel camera, to construct a hardware compressive sampling system.




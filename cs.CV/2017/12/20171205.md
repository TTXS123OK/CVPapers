# Arxiv Papers in cs.CV on 2017-12-05
### Human activity recognition from mobile inertial sensors using recurrence plots
- **Arxiv ID**: http://arxiv.org/abs/1712.01429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01429v1)
- **Published**: 2017-12-05 00:49:07+00:00
- **Updated**: 2017-12-05 00:49:07+00:00
- **Authors**: Otávio A. B. Penatti, Milton F. S. Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Inertial sensors are present in most mobile devices nowadays and such devices are used by people during most of their daily activities. In this paper, we present an approach for human activity recognition based on inertial sensors by employing recurrence plots (RP) and visual descriptors. The pipeline of the proposed approach is the following: compute RPs from sensor data, compute visual features from RPs and use them in a machine learning protocol. As RPs generate texture visual patterns, we transform the problem of sensor data classification to a problem of texture classification. Experiments for classifying human activities based on accelerometer data showed that the proposed approach obtains the highest accuracies, outperforming time- and frequency-domain features directly extracted from sensor data. The best results are obtained when using RGB RPs, in which each RGB channel corresponds to the RP of an independent accelerometer axis.



### AI Oriented Large-Scale Video Management for Smart City: Technologies, Standards and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1712.01432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01432v1)
- **Published**: 2017-12-05 01:02:33+00:00
- **Updated**: 2017-12-05 01:02:33+00:00
- **Authors**: Lingyu Duan, Yihang Lou, Shiqi Wang, Wen Gao, Yong Rui
- **Comment**: 8 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Deep learning has achieved substantial success in a series of tasks in computer vision. Intelligent video analysis, which can be broadly applied to video surveillance in various smart city applications, can also be driven by such powerful deep learning engines. To practically facilitate deep neural network models in the large-scale video analysis, there are still unprecedented challenges for the large-scale video data management. Deep feature coding, instead of video coding, provides a practical solution for handling the large-scale video surveillance data. To enable interoperability in the context of deep feature coding, standardization is urgent and important. However, due to the explosion of deep learning algorithms and the particularity of feature coding, there are numerous remaining problems in the standardization process. This paper envisions the future deep feature coding standard for the AI oriented large-scale video management, and discusses existing techniques, standards and possible solutions for these open problems.



### Zone-based Keyword Spotting in Bangla and Devanagari Documents
- **Arxiv ID**: http://arxiv.org/abs/1712.01434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01434v1)
- **Published**: 2017-12-05 01:12:25+00:00
- **Updated**: 2017-12-05 01:12:25+00:00
- **Authors**: Ayan Kumar Bhunia, Partha Pratim Roy, Umapada Pal
- **Comment**: Preprint Submitted
- **Journal**: None
- **Summary**: In this paper we present a word spotting system in text lines for offline Indic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown that zone-wise recognition method improves the word recognition performance than conventional full word recognition system in Indic scripts. Inspired with this idea we consider the zone segmentation approach and use middle zone information to improve the traditional word spotting performance. To avoid the problem of zone segmentation using heuristic approach, we propose here an HMM based approach to segment the upper and lower zone components from the text line images. The candidate keywords are searched from a line without segmenting characters or words. Also, we propose a novel feature combining foreground and background information of text line images for keyword-spotting by character filler models. A significant improvement in performance is noted by using both foreground and background information than their individual one. Pyramid Histogram of Oriented Gradient (PHOG) feature has been used in our word spotting framework. From the experiment, it has been noted that the proposed zone-segmentation based system outperforms traditional approaches of word spotting.



### 4DFAB: A Large Scale 4D Facial Expression Database for Biometric Applications
- **Arxiv ID**: http://arxiv.org/abs/1712.01443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01443v2)
- **Published**: 2017-12-05 02:13:39+00:00
- **Updated**: 2018-06-14 10:32:17+00:00
- **Authors**: Shiyang Cheng, Irene Kotsia, Maja Pantic, Stefanos Zafeiriou
- **Comment**: The original paper is published in CVPR 2018
- **Journal**: None
- **Summary**: The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.



### Multimodal Storytelling via Generative Adversarial Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.01455v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.01455v1)
- **Published**: 2017-12-05 02:51:35+00:00
- **Updated**: 2017-12-05 02:51:35+00:00
- **Authors**: Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo, Jing Dai, Chang-Tien Lu
- **Comment**: IJCAI 2017
- **Journal**: None
- **Summary**: Deriving event storylines is an effective summarization method to succinctly organize extensive information, which can significantly alleviate the pain of information overload. The critical challenge is the lack of widely recognized definition of storyline metric. Prior studies have developed various approaches based on different assumptions about users' interests. These works can extract interesting patterns, but their assumptions do not guarantee that the derived patterns will match users' preference. On the other hand, their exclusiveness of single modality source misses cross-modality information. This paper proposes a method, multimodal imitation learning via generative adversarial networks(MIL-GAN), to directly model users' interests as reflected by various data. In particular, the proposed model addresses the critical challenge by imitating users' demonstrated storylines. Our proposed model is designed to learn the reward patterns given user-provided storylines and then applies the learned policy to unseen data. The proposed approach is demonstrated to be capable of acquiring the user's implicit intent and outperforming competing methods by a substantial margin with a user study.



### Adversarial Attribute-Image Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1712.01493v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01493v3)
- **Published**: 2017-12-05 06:06:32+00:00
- **Updated**: 2018-07-04 16:49:39+00:00
- **Authors**: Zhou Yin, Wei-Shi Zheng, Ancong Wu, Hong-Xing Yu, Hai Wan, Xiaowei Guo, Feiyue Huang, Jianhuang Lai
- **Comment**: None
- **Journal**: None
- **Summary**: While attributes have been widely used for person re-identification (Re-ID) which aims at matching the same person images across disjoint camera views, they are used either as extra features or for performing multi-task learning to assist the image-image matching task. However, how to find a set of person images according to a given attribute description, which is very practical in many surveillance applications, remains a rarely investigated cross-modality matching problem in person Re-ID. In this work, we present this challenge and formulate this task as a joint space learning problem. By imposing an attribute-guided attention mechanism for images and a semantic consistent adversary strategy for attributes, each modality, i.e., images and attributes, successfully learns semantically correlated concepts under the guidance of the other. We conducted extensive experiments on three attribute datasets and demonstrated that the proposed joint space learning method is so far the most effective method for the attribute-image cross-modality person Re-ID problem.



### Fully Automatic Segmentation of Lumbar Vertebrae from CT Images using Cascaded 3D Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.01509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01509v1)
- **Published**: 2017-12-05 07:24:57+00:00
- **Updated**: 2017-12-05 07:24:57+00:00
- **Authors**: Rens Janssens, Guodong Zeng, Guoyan Zheng
- **Comment**: 5 pages and 5 figures
- **Journal**: None
- **Summary**: We present a method to address the challenging problem of segmentation of lumbar vertebrae from CT images acquired with varying fields of view. Our method is based on cascaded 3D Fully Convolutional Networks (FCNs) consisting of a localization FCN and a segmentation FCN. More specifically, in the first step we train a regression 3D FCN (we call it "LocalizationNet") to find the bounding box of the lumbar region. After that, a 3D U-net like FCN (we call it "SegmentationNet") is then developed, which after training, can perform a pixel-wise multi-class segmentation to map a cropped lumber region volumetric data to its volume-wise labels. Evaluated on publicly available datasets, our method achieved an average Dice coefficient of 95.77 $\pm$ 0.81% and an average symmetric surface distance of 0.37 $\pm$ 0.06 mm.



### Cooperative Training of Deep Aggregation Networks for RGB-D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.01080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01080v1)
- **Published**: 2017-12-05 07:33:20+00:00
- **Updated**: 2017-12-05 07:33:20+00:00
- **Authors**: Pichao Wang, Wanqing Li, Jun Wan, Philip Ogunbona, Xinwang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: A novel deep neural network training paradigm that exploits the conjoint information in multiple heterogeneous sources is proposed. Specifically, in a RGB-D based action recognition task, it cooperatively trains a single convolutional neural network (named c-ConvNet) on both RGB visual features and depth features, and deeply aggregates the two kinds of features for action recognition. Differently from the conventional ConvNet that learns the deep separable features for homogeneous modality-based classification with only one softmax loss function, the c-ConvNet enhances the discriminative power of the deeply learned features and weakens the undesired modality discrepancy by jointly optimizing a ranking loss and a softmax loss for both homogeneous and heterogeneous modalities. The ranking loss consists of intra-modality and cross-modality triplet losses, and it reduces both the intra-modality and cross-modality feature variations. Furthermore, the correlations between RGB and depth data are embedded in the c-ConvNet, and can be retrieved by either of the modalities and contribute to the recognition in the case even only one of the modalities is available. The proposed method was extensively evaluated on two large RGB-D action recognition datasets, ChaLearn LAP IsoGD and NTU RGB+D datasets, and one small dataset, SYSU 3D HOI, and achieved state-of-the-art results.



### Successive Embedding and Classification Loss for Aerial Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1712.01511v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01511v3)
- **Published**: 2017-12-05 07:45:18+00:00
- **Updated**: 2019-09-24 05:57:53+00:00
- **Authors**: Jiayun Wang, Patrick Virtue, Stella X. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks can be effective means to automatically classify aerial images but is easy to overfit to the training data. It is critical for trained neural networks to be robust to variations that exist between training and test environments. To address the overfitting problem in aerial image classification, we consider the neural network as successive transformations of an input image into embedded feature representations and ultimately into a semantic class label, and train neural networks to optimize image representations in the embedded space in addition to optimizing the final classification score. We demonstrate that networks trained with this dual embedding and classification loss outperform networks with classification loss only. %We also study placing the embedding loss on different network layers. We also find that moving the embedding loss from commonly-used feature space to the classifier space, which is the space just before softmax nonlinearization, leads to the best classification performance for aerial images. Visualizations of the network's embedded representations reveal that the embedding loss encourages greater separation between target class clusters for both training and testing partitions of two aerial image classification benchmark datasets, MSTAR and AID. Our code is publicly available on GitHub.



### O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1712.01537v1
- **DOI**: 10.1145/3072959.3073608
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01537v1)
- **Published**: 2017-12-05 09:25:19+00:00
- **Updated**: 2017-12-05 09:25:19+00:00
- **Authors**: Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, Xin Tong
- **Comment**: None
- **Journal**: None
- **Summary**: We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.



### Manifold-valued Image Generation with Wasserstein Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1712.01551v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.01551v2)
- **Published**: 2017-12-05 10:02:05+00:00
- **Updated**: 2019-01-03 16:09:47+00:00
- **Authors**: Zhiwu Huang, Jiqing Wu, Luc Van Gool
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Generative modeling over natural images is one of the most fundamental machine learning problems. However, few modern generative models, including Wasserstein Generative Adversarial Nets (WGANs), are studied on manifold-valued images that are frequently encountered in real-world applications. To fill the gap, this paper first formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image generation, chromaticity-brightness (CB) color image generation, and diffusion-tensor (DT) image generation. For the proposed generative modeling problem, we then introduce a theorem of optimal transport to derive a new Wasserstein distance of data distributions on complete manifolds, enabling us to achieve a tractable objective under the WGAN framework. In addition, we recommend three benchmark datasets that are CIFAR-10 HSV/CB color images, ImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we experimentally demonstrate the proposed manifold-aware WGAN model can generate more plausible manifold-valued images than its competitors.



### Deep learning for semantic segmentation of remote sensing images with rich spectral content
- **Arxiv ID**: http://arxiv.org/abs/1712.01600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.01600v1)
- **Published**: 2017-12-05 12:25:43+00:00
- **Updated**: 2017-12-05 12:25:43+00:00
- **Authors**: A Hamida, A. Benoît, P. Lambert, L Klein, C Amar, N. Audebert, S. Lefèvre
- **Comment**: IEEE International Geoscience and Remote Sensing Symposium, Jul 2017,
  Fort Worth, United States. 2017
- **Journal**: None
- **Summary**: With the rapid development of Remote Sensing acquisition techniques, there is a need to scale and improve processing tools to cope with the observed increase of both data volume and richness. Among popular techniques in remote sensing, Deep Learning gains increasing interest but depends on the quality of the training data. Therefore, this paper presents recent Deep Learning approaches for fine or coarse land cover semantic segmentation estimation. Various 2D architectures are tested and a new 3D model is introduced in order to jointly process the spatial and spectral dimensions of the data. Such a set of networks enables the comparison of the different spectral fusion schemes. Besides, we also assess the use of a " noisy ground truth " (i.e. outdated and low spatial resolution labels) for training and testing the networks.



### Deep Learning for automatic sale receipt understanding
- **Arxiv ID**: http://arxiv.org/abs/1712.01606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01606v1)
- **Published**: 2017-12-05 12:40:20+00:00
- **Updated**: 2017-12-05 12:40:20+00:00
- **Authors**: Rizlène Raoui-Outach, Cécile Million-Rousseau, Alexandre Benoit, Patrick Lambert
- **Comment**: International Conference on Image Processing Theory, Tools and
  Applications, Nov 2017, Montreal, Canada. 2017,
  http://www.ipta-conference.com/ipta17
- **Journal**: None
- **Summary**: As a general rule, data analytics are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality.In an industrial context, this work focuses on the automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) on one hand and more classical image and text processings on another hand.The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis.



### Empirically Analyzing the Effect of Dataset Biases on Deep Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/1712.01619v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01619v4)
- **Published**: 2017-12-05 13:52:42+00:00
- **Updated**: 2018-04-19 15:05:39+00:00
- **Authors**: Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, Thomas Vetter
- **Comment**: Accepted to CVPR 2018 Workshop on Analysis and Modeling of Faces and
  Gestures (AMFG)
- **Journal**: None
- **Summary**: It is unknown what kind of biases modern in the wild face datasets have because of their lack of annotation. A direct consequence of this is that total recognition rates alone only provide limited insight about the generalization ability of a Deep Convolutional Neural Networks (DCNNs). We propose to empirically study the effect of different types of dataset biases on the generalization ability of DCNNs. Using synthetically generated face images, we study the face recognition rate as a function of interpretable parameters such as face pose and light. The proposed method allows valuable details about the generalization performance of different DCNN architectures to be observed and compared. In our experiments, we find that: 1) Indeed, dataset bias has a significant influence on the generalization performance of DCNNs. 2) DCNNs can generalize surprisingly well to unseen illumination conditions and large sampling gaps in the pose variation. 3) Using the presented methodology we reveal that the VGG-16 architecture outperforms the AlexNet architecture at face recognition tasks because it can much better generalize to unseen face poses, although it has significantly more parameters. 4) We uncover a main limitation of current DCNN architectures, which is the difficulty to generalize when different identities to not share the same pose variation. 5) We demonstrate that our findings on synthetic data also apply when learning from real-world data. Our face image generator is publicly available to enable the community to benchmark other DCNN architectures.



### On Deterministic Sampling Patterns for Robust Low-Rank Matrix Completion
- **Arxiv ID**: http://arxiv.org/abs/1712.01628v1
- **DOI**: 10.1109/LSP.2017.2780983
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1712.01628v1)
- **Published**: 2017-12-05 14:06:44+00:00
- **Updated**: 2017-12-05 14:06:44+00:00
- **Authors**: Morteza Ashraphijuo, Vaneet Aggarwal, Xiaodong Wang
- **Comment**: Accepted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: In this letter, we study the deterministic sampling patterns for the completion of low rank matrix, when corrupted with a sparse noise, also known as robust matrix completion. We extend the recent results on the deterministic sampling patterns in the absence of noise based on the geometric analysis on the Grassmannian manifold. A special case where each column has a certain number of noisy entries is considered, where our probabilistic analysis performs very efficiently. Furthermore, assuming that the rank of the original matrix is not given, we provide an analysis to determine if the rank of a valid completion is indeed the actual rank of the data corrupted with sparse noise by verifying some conditions.



### Recognizing Gender from Human Facial Regions using Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1712.01661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01661v1)
- **Published**: 2017-12-05 14:33:57+00:00
- **Updated**: 2017-12-05 14:33:57+00:00
- **Authors**: Avirup Bhattacharyya, Rajkumar Saini, Partha Pratim Roy, Debi Prosad Dogra, Samarjit Kar
- **Comment**: Preprint Submitted
- **Journal**: None
- **Summary**: Recently, recognition of gender from facial images has gained a lot of importance. There exist a handful of research work that focus on feature extraction to obtain gender specific information from facial images. However, analyzing different facial regions and their fusion help in deciding the gender of a person from facial images. In this paper, we propose a new approach to identify gender from frontal facial images that is robust to background, illumination, intensity, and facial expression. In our framework, first the frontal face image is divided into a number of distinct regions based on facial landmark points that are obtained by the Chehra model proposed by Asthana et al. The model provides 49 facial landmark points covering different regions of the face, e.g. forehead, left eye, right eye, lips. Next, a face image is segmented into facial regions using landmark points and features are extracted from each region. The Compass LBP feature, a variant of LBP feature, has been used in our framework to obtain discriminative gender-specific information. Following this, a Support Vector Machine based classifier has been used to compute the probability scores from each facial region. Finally, the classification scores obtained from individual regions are combined with a genetic algorithm based learning to improve the overall classification accuracy. The experiments have been performed on popular face image datasets such as Adience, cFERET (color FERET), LFW and two sketch datasets, namely CUFS and CUFSF. Through experiments, we have observed that, the proposed method outperforms existing approaches.



### IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions
- **Arxiv ID**: http://arxiv.org/abs/1712.01707v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01707v4)
- **Published**: 2017-12-05 15:19:54+00:00
- **Updated**: 2018-03-08 03:18:13+00:00
- **Authors**: Chaolu Feng
- **Comment**: 27 pages, 14 figures
- **Journal**: None
- **Summary**: Image segmentation is still an open problem especially when intensities of the interested objects are overlapped due to the presence of intensity inhomogeneity (also known as bias field). To segment images with intensity inhomogeneities, a bias correction embedded level set model is proposed where Inhomogeneities are Estimated by Orthogonal Primary Functions (IEOPF). In the proposed model, the smoothly varying bias is estimated by a linear combination of a given set of orthogonal primary functions. An inhomogeneous intensity clustering energy is then defined and membership functions of the clusters described by the level set function are introduced to rewrite the energy as a data term of the proposed model. Similar to popular level set methods, a regularization term and an arc length term are also included to regularize and smooth the level set function, respectively. The proposed model is then extended to multichannel and multiphase patterns to segment colourful images and images with multiple objects, respectively. It has been extensively tested on both synthetic and real images that are widely used in the literature and public BrainWeb and IBSR datasets. Experimental results and comparison with state-of-the-art methods demonstrate that advantages of the proposed model in terms of bias correction and segmentation accuracy.



### Automated Pruning for Deep Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1712.01721v2
- **DOI**: 10.1109/ICPR.2018.8546129
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01721v2)
- **Published**: 2017-12-05 15:58:44+00:00
- **Updated**: 2019-01-06 14:19:12+00:00
- **Authors**: Franco Manessi, Alessandro Rozza, Simone Bianco, Paolo Napoletano, Raimondo Schettini
- **Comment**: 8 pages, 5 figures. Published as a conference paper at ICPR 2018
- **Journal**: 2018 24th International Conference on Pattern Recognition (ICPR),
  Beijing, 2018, pp. 657-664
- **Summary**: In this work we present a method to improve the pruning step of the current state-of-the-art methodology to compress neural networks. The novelty of the proposed pruning technique is in its differentiability, which allows pruning to be performed during the backpropagation phase of the network training. This enables an end-to-end learning and strongly reduces the training time. The technique is based on a family of differentiable pruning functions and a new regularizer specifically designed to enforce pruning. The experimental results show that the joint optimization of both the thresholds and the network weights permits to reach a higher compression rate, reducing the number of weights of the pruned network by a further 14% to 33% compared to the current state-of-the-art. Furthermore, we believe that this is the first study where the generalization capabilities in transfer learning tasks of the features extracted by a pruned network are analyzed. To achieve this goal, we show that the representations learned using the proposed pruning methodology maintain the same effectiveness and generality of those learned by the corresponding non-compressed network on a set of different recognition tasks.



### OLÉ: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.01727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.01727v1)
- **Published**: 2017-12-05 16:03:56+00:00
- **Updated**: 2017-12-05 16:03:56+00:00
- **Authors**: José Lezama, Qiang Qiu, Pablo Musé, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are ubiquitous tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, such solutions carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this paper, we propose a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple and elegant geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\'E) does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss, being the first reported deep metric learning framework of its kind. Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative, and more robust. We demonstrate improved classification performance in general object recognition, plugging the proposed loss term into existing off-the-shelf architectures. In particular, we show the advantage of the proposed loss in the small data/model scenario, and we significantly advance the state-of-the-art on the Stanford STL-10 benchmark.



### Convolutional Recurrent Neural Networks for Dynamic MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1712.01751v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01751v3)
- **Published**: 2017-12-05 16:49:07+00:00
- **Updated**: 2018-10-14 21:14:32+00:00
- **Authors**: Chen Qin, Jo Schlemper, Jose Caballero, Anthony Price, Joseph V. Hajnal, Daniel Rueckert
- **Comment**: Published in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Accelerating the data acquisition of dynamic magnetic resonance imaging (MRI) leads to a challenging ill-posed inverse problem, which has received great interest from both the signal processing and machine learning community over the last decades. The key ingredient to the problem is how to exploit the temporal correlation of the MR sequence to resolve the aliasing artefact. Traditionally, such observation led to a formulation of a non-convex optimisation problem, which were solved using iterative algorithms. Recently, however, deep learning based-approaches have gained significant popularity due to its ability to solve general inversion problems. In this work, we propose a unique, novel convolutional recurrent neural network (CRNN) architecture which reconstructs high quality cardiac MR images from highly undersampled k-space data by jointly exploiting the dependencies of the temporal sequences as well as the iterative nature of the traditional optimisation algorithms. In particular, the proposed architecture embeds the structure of the traditional iterative algorithms, efficiently modelling the recurrence of the iterative reconstruction stages by using recurrent hidden connections over such iterations. In addition, spatiotemporal dependencies are simultaneously learnt by exploiting bidirectional recurrent hidden connections across time sequences. The proposed algorithm is able to learn both the temporal dependency and the iterative reconstruction process effectively with only a very small number of parameters, while outperforming current MR reconstruction methods in terms of computational complexity, reconstruction accuracy and speed.



### Tech Report: A Fast Multiscale Spatial Regularization for Sparse Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1712.01770v3
- **DOI**: 10.1109/LGRS.2018.2878394
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01770v3)
- **Published**: 2017-12-05 17:24:54+00:00
- **Updated**: 2018-10-25 22:21:08+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse hyperspectral unmixing from large spectral libraries has been considered to circumvent limitations of endmember extraction algorithms in many applications. This strategy often leads to ill-posed inverse problems, which can benefit from spatial regularization strategies. While existing spatial regularization methods improve the problem conditioning and promote piecewise smooth solutions, they lead to large nonsmooth optimization problems. Thus, efficiently introducing spatial context in the unmixing problem remains a challenge, and a necessity for many real world applications. In this paper, a novel multiscale spatial regularization approach for sparse unmixing is proposed. The method uses a signal-adaptive spatial multiscale decomposition based on superpixels to decompose the unmixing problem into two simpler problems, one in the approximation domain and another in the original domain. Simulation results using both synthetic and real data indicate that the proposed method can outperform state-of-the-art Total Variation-based algorithms with a computation time comparable to that of their unregularized counterparts.



### Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/1712.01785v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.01785v4)
- **Published**: 2017-12-05 17:49:18+00:00
- **Updated**: 2022-12-20 07:14:37+00:00
- **Authors**: Kexin Pei, Linjie Zhu, Yinzhi Cao, Junfeng Yang, Carl Vondrick, Suman Jana
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the increasing usage of machine learning (ML) techniques in security- and safety-critical domains, such as autonomous systems and medical diagnosis, ensuring correct behavior of ML systems, especially for different corner cases, is of growing importance. In this paper, we propose a generic framework for evaluating security and robustness of ML systems using different real-world safety properties. We further design, implement and evaluate VeriVis, a scalable methodology that can verify a diverse set of safety properties for state-of-the-art computer vision systems with only blackbox access. VeriVis leverage different input space reduction techniques for efficient verification of different safety properties. VeriVis is able to find thousands of safety violations in fifteen state-of-the-art computer vision systems including ten Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving system with thousands of neurons as well as five commercial third-party vision APIs including Google vision and Clarifai for twelve different safety properties. Furthermore, VeriVis can successfully verify local safety properties, on average, for around 31.7% of the test images. VeriVis finds up to 64.8x more violations than existing gradient-based methods that, unlike VeriVis, cannot ensure non-existence of any violations. Finally, we show that retraining using the safety violations detected by VeriVis can reduce the average number of violations up to 60.2%.



### R-FCN-3000 at 30fps: Decoupling Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1712.01802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01802v1)
- **Published**: 2017-12-05 18:30:19+00:00
- **Updated**: 2017-12-05 18:30:19+00:00
- **Authors**: Bharat Singh, Hengduo Li, Abhishek Sharma, Larry S. Davis
- **Comment**: CVPR 2018 submission
- **Journal**: None
- **Summary**: We present R-FCN-3000, a large-scale real-time object detector in which objectness detection and classification are decoupled. To obtain the detection score for an RoI, we multiply the objectness score with the fine-grained classification score. Our approach is a modification of the R-FCN architecture in which position-sensitive filters are shared across different object classes for performing localization. For fine-grained classification, these position-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9% on the ImageNet detection dataset and outperforms YOLO-9000 by 18% while processing 30 images per second. We also show that the objectness learned by R-FCN-3000 generalizes to novel classes and the performance increases with the number of training object classes - supporting the hypothesis that it is possible to learn a universal objectness detector. Code will be made available.



### Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene
- **Arxiv ID**: http://arxiv.org/abs/1712.01812v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01812v2)
- **Published**: 2017-12-05 18:42:13+00:00
- **Updated**: 2018-04-24 17:34:15+00:00
- **Authors**: Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A. Efros, Jitendra Malik
- **Comment**: Project url with code: https://shubhtuls.github.io/factored3d
- **Journal**: None
- **Summary**: The goal of this paper is to take a single 2D image of a scene and recover the 3D structure in terms of a small set of factors: a layout representing the enclosing surfaces as well as a set of objects represented in terms of shape and pose. We propose a convolutional neural network-based approach to predict this representation and benchmark it on a large dataset of indoor scenes. Our experiments evaluate a number of practical design questions, demonstrate that we can infer this representation, and quantitatively and qualitatively demonstrate its merits compared to alternate representations.



### Structured Set Matching Networks for One-Shot Part Labeling
- **Arxiv ID**: http://arxiv.org/abs/1712.01867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01867v2)
- **Published**: 2017-12-05 19:03:08+00:00
- **Updated**: 2018-04-03 20:49:05+00:00
- **Authors**: Jonghyun Choi, Jayant Krishnamurthy, Aniruddha Kembhavi, Ali Farhadi
- **Comment**: one shot part labeling. CVPR 2018 accepted as spotlight presentation
- **Journal**: None
- **Summary**: Diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning. However, understanding diagrams using natural image understanding approaches requires large training datasets of diagrams, which are very hard to obtain. Instead, this can be addressed as a matching problem either between labeled diagrams, images or both. This problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning. We consider the problem of one-shot part labeling: labeling multiple parts of an object in a target image given only a single source image of that category. For this set-to-set matching problem, we introduce the Structured Set Matching Network (SSMN), a structured prediction model that incorporates convolutional neural networks. The SSMN is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements, while also enforcing a matching constraint between the two sets. The SSMN significantly outperforms several strong baselines on three label transfer scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200 categories; image-to-image, evaluated on a dataset built on top of the Pascal Part Dataset; and image-to-diagram, evaluated on transferring labels across these datasets.



### Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training
- **Arxiv ID**: http://arxiv.org/abs/1712.01887v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.01887v3)
- **Published**: 2017-12-05 19:48:11+00:00
- **Updated**: 2020-06-23 03:28:30+00:00
- **Authors**: Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally
- **Comment**: we find 99.9% of the gradient exchange in distributed SGD is
  redundant; we reduce the communication bandwidth by two orders of magnitude
  without losing accuracy. Code is available at:
  https://github.com/synxlin/deep-gradient-compression
- **Journal**: ICLR 2018
- **Summary**: Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: https://github.com/synxlin/deep-gradient-compression.



### Grounding Referring Expressions in Images by Variational Context
- **Arxiv ID**: http://arxiv.org/abs/1712.01892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01892v2)
- **Published**: 2017-12-05 19:57:52+00:00
- **Updated**: 2018-03-31 04:54:08+00:00
- **Authors**: Hanwang Zhang, Yulei Niu, Shih-Fu Chang
- **Comment**: in 2018 Conference on Computer Vision and Pattern Recognition
  (CVPR'18)
- **Journal**: None
- **Summary**: We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., "largest elephant standing behind baby elephant". This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context --- visual attributes (e.g., "largest", "baby") and relationships (e.g., "behind") that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences the estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced, resulting in better localization of referent. We develop a novel cue-specific language-vision embedding network that learns this reciprocity model end-to-end. We also extend the model to the unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings.



### Population-based Respiratory 4D Motion Atlas Construction and its Application for VR Simulations of Liver Punctures
- **Arxiv ID**: http://arxiv.org/abs/1712.01893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01893v2)
- **Published**: 2017-12-05 19:59:20+00:00
- **Updated**: 2017-12-22 13:54:10+00:00
- **Authors**: Andre Mastmeyer, Matthias Wilms, Heinz Handels
- **Comment**: 7 pages, 4 figures, 1 movie, Proc. SPIE Medical Imaging: Image
  Processing 2018
- **Journal**: None
- **Summary**: Virtual reality (VR) training simulators of liver needle insertion in the hepatic area of breathing virtual patients currently need 4D data acquisitions as a prerequisite. Here, first a population-based breathing virtual patient 4D atlas can be built and second the requirement of a dose-relevant or expensive acquisition of a 4D data set for a new static 3D patient can be mitigated by warping the mean atlas motion. The breakthrough contribution of this work is the construction and reuse of population-based learned 4D motion models.



### Co-domain Embedding using Deep Quadruplet Networks for Unseen Traffic Sign Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.01907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01907v1)
- **Published**: 2017-12-05 20:24:18+00:00
- **Updated**: 2017-12-05 20:24:18+00:00
- **Authors**: Junsik Kim, Seokju Lee, Tae-Hyun Oh, In So Kweon
- **Comment**: To appear in AAAI 2018
- **Journal**: None
- **Summary**: Recent advances in visual recognition show overarching success by virtue of large amounts of supervised data. However,the acquisition of a large supervised dataset is often challenging. This is also true for intelligent transportation applications, i.e., traffic sign recognition. For example, a model trained with data of one country may not be easily generalized to another country without much data. We propose a novel feature embedding scheme for unseen class classification when the representative class template is given. Traffic signs, unlike other objects, have official images. We perform co-domain embedding using a quadruple relationship from real and synthetic domains. Our quadruplet network fully utilizes the explicit pairwise similarity relationships among samples from different domains. We validate our method on three datasets with two experiments involving one-shot classification and feature generalization. The results show that the proposed method outperforms competing approaches on both seen and unseen classes.



### iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects
- **Arxiv ID**: http://arxiv.org/abs/1712.01924v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01924v3)
- **Published**: 2017-12-05 21:01:24+00:00
- **Updated**: 2018-06-18 11:08:04+00:00
- **Authors**: Omid Hosseini Jafari, Siva Karthik Mustikovela, Karl Pertsch, Eric Brachmann, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: We address the task of 6D pose estimation of known rigid objects from single input images in scenarios where the objects are partly occluded. Recent RGB-D-based methods are robust to moderate degrees of occlusion. For RGB inputs, no previous method works well for partly occluded objects. Our main contribution is to present the first deep learning-based system that estimates accurate poses for partly occluded objects from RGB-D and RGB input. We achieve this with a new instance-aware pipeline that decomposes 6D object pose estimation into a sequence of simpler steps, where each step removes specific aspects of the problem. The first step localizes all known objects in the image using an instance segmentation network, and hence eliminates surrounding clutter and occluders. The second step densely maps pixels to 3D object surface positions, so called object coordinates, using an encoder-decoder network, and hence eliminates object appearance. The third, and final, step predicts the 6D pose using geometric optimization. We demonstrate that we significantly outperform the state-of-the-art for pose estimation of partly occluded objects for both RGB and RGB-D input.



### Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.01928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01928v2)
- **Published**: 2017-12-05 21:16:52+00:00
- **Updated**: 2018-03-31 06:43:52+00:00
- **Authors**: Long Chen, Hanwang Zhang, Jun Xiao, Wei Liu, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem --- semantic loss --- in the prevailing family of embedding-based ZSL, where some semantics would be discarded during training if they are non-discriminative for training classes, but could become critical for recognizing test classes. Specifically, SP-AEN prevents the semantic loss by introducing an independent visual-to-semantic space embedder which disentangles the semantic space into two subspaces for the two arguably conflicting objectives: classification and reconstruction. Through adversarial learning of the two subspaces, SP-AEN can transfer the semantics from the reconstructive subspace to the discriminative one, accomplishing the improved zero-shot recognition of unseen classes. Comparing with prior works, SP-AEN can not only improve classification but also generate photo-realistic images, demonstrating the effectiveness of semantic preservation. On four popular benchmarks: CUB, AWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art methods by an absolute performance difference of 12.2\%, 9.3\%, 4.0\%, and 3.6\% in terms of harmonic mean values



### Blind Image Deblurring Using Row-Column Sparse Representations
- **Arxiv ID**: http://arxiv.org/abs/1712.01937v1
- **DOI**: 10.1109/LSP.2017.2782570
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01937v1)
- **Published**: 2017-12-05 21:39:29+00:00
- **Updated**: 2017-12-05 21:39:29+00:00
- **Authors**: Mohammad Tofighi, Yuelong Li, Vishal Monga
- **Comment**: Accepted to IEEE Signal Processing Letters, December 2017
- **Journal**: None
- **Summary**: Blind image deblurring is a particularly challenging inverse problem where the blur kernel is unknown and must be estimated en route to recover the deblurred image. The problem is of strong practical relevance since many imaging devices such as cellphone cameras, must rely on deblurring algorithms to yield satisfactory image quality. Despite significant research effort, handling large motions remains an open problem. In this paper, we develop a new method called Blind Image Deblurring using Row-Column Sparsity (BD-RCS) to address this issue. Specifically, we model the outer product of kernel and image coefficients in certain transformation domains as a rank-one matrix, and recover it by solving a rank minimization problem. Our central contribution then includes solving {\em two new} optimization problems involving row and column sparsity to automatically determine blur kernel and image support sequentially. The kernel and image can then be recovered through a singular value decomposition (SVD). Experimental results on linear motion deblurring demonstrate that BD-RCS can yield better results than state of the art, particularly when the blur is caused by large motion. This is confirmed both visually and through quantitative measures.



### Learning Latent Super-Events to Detect Multiple Activities in Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.01938v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01938v2)
- **Published**: 2017-12-05 21:40:09+00:00
- **Updated**: 2018-03-29 05:12:22+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: CVPR 2018
- **Journal**: CVPR 2018
- **Summary**: In this paper, we introduce the concept of learning latent super-events from activity videos, and present how it benefits activity detection in continuous videos. We define a super-event as a set of multiple events occurring together in videos with a particular temporal organization; it is the opposite concept of sub-events. Real-world videos contain multiple activities and are rarely segmented (e.g., surveillance videos), and learning latent super-events allows the model to capture how the events are temporally related in videos. We design temporal structure filters that enable the model to focus on particular sub-intervals of the videos, and use them together with a soft attention mechanism to learn representations of latent super-events. Super-event representations are combined with per-frame or per-segment CNNs to provide frame-level annotations. Our approach is designed to be fully differentiable, enabling end-to-end learning of latent super-event representations jointly with the activity detector using them. Our experiments with multiple public video datasets confirm that the proposed concept of latent super-event learning significantly benefits activity detection, advancing the state-of-the-arts.



### Learning to Forecast Videos of Human Activity with Multi-granularity Models and Adaptive Rendering
- **Arxiv ID**: http://arxiv.org/abs/1712.01955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01955v1)
- **Published**: 2017-12-05 22:26:43+00:00
- **Updated**: 2017-12-05 22:26:43+00:00
- **Authors**: Mengyao Zhai, Jiacheng Chen, Ruizhi Deng, Lei Chen, Ligeng Zhu, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for forecasting video of complex human activity involving multiple people. Direct pixel-level prediction is too simple to handle the appearance variability in complex activities. Hence, we develop novel intermediate representations. An architecture combining a hierarchical temporal model for predicting human poses and encoder-decoder convolutional neural networks for rendering target appearances is proposed. Our hierarchical model captures interactions among people by adopting a dynamic group-based interaction mechanism. Next, our appearance rendering network encodes the targets' appearances by learning adaptive appearance filters using a fully convolutional network. Finally, these filters are placed in encoder-decoder neural networks to complete the rendering. We demonstrate that our model can generate videos that are superior to state-of-the-art methods, and can handle complex human activity scenarios in video forecasting.



### What's in my closet?: Image classification using fuzzy logic
- **Arxiv ID**: http://arxiv.org/abs/1712.01970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.01970v1)
- **Published**: 2017-12-05 23:41:47+00:00
- **Updated**: 2017-12-05 23:41:47+00:00
- **Authors**: Amina E. Hussein
- **Comment**: 12 pages, 8 Figures
- **Journal**: None
- **Summary**: A fuzzy system was created in MATLAB to identify an item of clothing as a dress, shirt, or pair of pants from a series of input images. The system was initialized using a high-contrast vector-image of each item of clothing as the state closest to a direct solution. Nine other user-input images (three of each item) were also used to determine the characteristic function of each item and recognize each pattern. Mamdani inference systems were used for edge location and identification of characteristic regions of interest for each item of clothing. Based on these non-dimensional trends, a second Mamdani fuzzy inference system was used to characterize each image as containing a shirt, a dress, or a pair of pants. An outline of the fuzzy inference system and image processing techniques used for creating an image pattern recognition system are discussed.




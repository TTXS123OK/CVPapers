# Arxiv Papers in cs.CV on 2017-12-12
### Learning Compressible 360° Video Isomers
- **Arxiv ID**: http://arxiv.org/abs/1712.04083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04083v1)
- **Published**: 2017-12-12 00:35:23+00:00
- **Updated**: 2017-12-12 00:35:23+00:00
- **Authors**: Yu-Chuan Su, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Standard video encoders developed for conventional narrow field-of-view video are widely applied to 360{\deg} video as well, with reasonable results. However, while this approach commits arbitrarily to a projection of the spherical frames, we observe that some orientations of a 360{\deg} video, once projected, are more compressible than others. We introduce an approach to predict the sphere rotation that will yield the maximal compression rate. Given video clips in their original encoding, a convolutional neural network learns the association between a clip's visual content and its compressibility at different rotations of a cubemap projection. Given a novel video, our learning-based approach efficiently infers the most compressible direction in one shot, without repeated rendering and compression of the source video. We validate our idea on thousands of video clips and multiple popular video codecs. The results show that this untapped dimension of 360{\deg} compression has substantial potential--"good" rotations are typically 8-10% more compressible than bad ones, and our learning approach can predict them reliably 82% of the time.



### Im2Flow: Motion Hallucination from Static Images for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1712.04109v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04109v3)
- **Published**: 2017-12-12 03:11:34+00:00
- **Updated**: 2018-05-30 16:18:10+00:00
- **Authors**: Ruohan Gao, Bo Xiong, Kristen Grauman
- **Comment**: Published in CVPR 2018, project page:
  http://vision.cs.utexas.edu/projects/im2flow/
- **Journal**: None
- **Summary**: Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.



### 200x Low-dose PET Reconstruction using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.04119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04119v1)
- **Published**: 2017-12-12 04:13:27+00:00
- **Updated**: 2017-12-12 04:13:27+00:00
- **Authors**: Junshen Xu, Enhao Gong, John Pauly, Greg Zaharchuk
- **Comment**: None
- **Journal**: None
- **Summary**: Positron emission tomography (PET) is widely used in various clinical applications, including cancer diagnosis, heart disease and neuro disorders. The use of radioactive tracer in PET imaging raises concerns due to the risk of radiation exposure. To minimize this potential risk in PET imaging, efforts have been made to reduce the amount of radio-tracer usage. However, lowing dose results in low Signal-to-Noise-Ratio (SNR) and loss of information, both of which will heavily affect clinical diagnosis. Besides, the ill-conditioning of low-dose PET image reconstruction makes it a difficult problem for iterative reconstruction algorithms. Previous methods proposed are typically complicated and slow, yet still cannot yield satisfactory results at significantly low dose. Here, we propose a deep learning method to resolve this issue with an encoder-decoder residual deep network with concatenate skip connections. Experiments shows the proposed method can reconstruct low-dose PET image to a standard-dose quality with only two-hundredth dose. Different cost functions for training model are explored. Multi-slice input strategy is introduced to provide the network with more structural information and make it more robust to noise. Evaluation on ultra-low-dose clinical data shows that the proposed method can achieve better result than the state-of-the-art methods and reconstruct images with comparable quality using only 0.5% of the original regular dose.



### A vision based system for underwater docking
- **Arxiv ID**: http://arxiv.org/abs/1712.04138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04138v1)
- **Published**: 2017-12-12 05:59:49+00:00
- **Updated**: 2017-12-12 05:59:49+00:00
- **Authors**: Shuang Liu, Mete Ozay, Takayuki Okatani, Hongli Xu, Kai Sun, Yang Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous underwater vehicles (AUVs) have been deployed for underwater exploration. However, its potential is confined by its limited on-board battery energy and data storage capacity. This problem has been addressed using docking systems by underwater recharging and data transfer for AUVs. In this work, we propose a vision based framework for underwater docking following these systems. The proposed framework comprises two modules; (i) a detection module which provides location information on underwater docking stations in 2D images captured by an on-board camera, and (ii) a pose estimation module which recovers the relative 3D position and orientation between docking stations and AUVs from the 2D images. For robust and credible detection of docking stations, we propose a convolutional neural network called Docking Neural Network (DoNN). For accurate pose estimation, a perspective-n-point algorithm is integrated into our framework. In order to examine our framework in underwater docking tasks, we collected a dataset of 2D images, named Underwater Docking Images Dataset (UDID), in an experimental water pool. To the best of our knowledge, UDID is the first publicly available underwater docking dataset. In the experiments, we first evaluate performance of the proposed detection module on UDID and its deformed variations. Next, we assess the accuracy of the pose estimation module by ground experiments, since it is not feasible to obtain true relative position and orientation between docking stations and AUVs under water. Then, we examine the pose estimation module by underwater experiments in our experimental water pool. Experimental results show that the proposed framework can be used to detect docking stations and estimate their relative pose efficiently and successfully, compared to the state-of-the-art baseline systems.



### Deep learning enhanced mobile-phone microscopy
- **Arxiv ID**: http://arxiv.org/abs/1712.04139v1
- **DOI**: 10.1021/acsphotonics.8b00146
- **Categories**: **cs.LG**, cs.CV, physics.med-ph, 68T01, 68T05, 68U10, 62M45, 78M32, 92C50, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.3; I.3.3; I.4.3; I.4.4; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1712.04139v1)
- **Published**: 2017-12-12 06:03:27+00:00
- **Updated**: 2017-12-12 06:03:27+00:00
- **Authors**: Yair Rivenson, Hatice Ceylan Koydemir, Hongda Wang, Zhensong Wei, Zhengshuang Ren, Harun Gunaydin, Yibo Zhang, Zoltan Gorocs, Kyle Liang, Derek Tseng, Aydogan Ozcan
- **Comment**: None
- **Journal**: ACS Photonics (2018)
- **Summary**: Mobile-phones have facilitated the creation of field-portable, cost-effective imaging and sensing technologies that approach laboratory-grade instrument performance. However, the optical imaging interfaces of mobile-phones are not designed for microscopy and produce spatial and spectral distortions in imaging microscopic specimens. Here, we report on the use of deep learning to correct such distortions introduced by mobile-phone-based microscopes, facilitating the production of high-resolution, denoised and colour-corrected images, matching the performance of benchtop microscopes with high-end objective lenses, also extending their limited depth-of-field. After training a convolutional neural network, we successfully imaged various samples, including blood smears, histopathology tissue sections, and parasites, where the recorded images were highly compressed to ease storage and transmission for telemedicine applications. This method is applicable to other low-cost, aberrated imaging systems, and could offer alternatives for costly and bulky microscopes, while also providing a framework for standardization of optical images for clinical and biomedical applications.



### Direction-aware Spatial Context Features for Shadow Detection
- **Arxiv ID**: http://arxiv.org/abs/1712.04142v2
- **DOI**: 10.1109/CVPR.2018.00778
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04142v2)
- **Published**: 2017-12-12 06:29:51+00:00
- **Updated**: 2018-05-16 11:28:57+00:00
- **Authors**: Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, Pheng-Ann Heng
- **Comment**: Accepted for oral presentation in CVPR 2018. The journal version of
  this paper is arXiv:1805.04635
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  pp. 7454-7462, 2018
- **Summary**: Shadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows. This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows. This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels. Moreover, a weighted cross entropy loss is designed to make the training more effective. We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network. Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate.



### Benchmarking Single Image Dehazing and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1712.04143v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1712.04143v4)
- **Published**: 2017-12-12 06:33:20+00:00
- **Updated**: 2019-04-22 00:13:07+00:00
- **Authors**: Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, Zhangyang Wang
- **Comment**: IEEE Transactions on Image Processing(TIP 2019)
- **Journal**: None
- **Summary**: We present a comprehensive study and evaluation of existing single image dehazing algorithms, using a new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. We further provide a rich variety of criteria for dehazing algorithm evaluation, ranging from full-reference metrics, to no-reference metrics, to subjective evaluation and the novel task-driven evaluation. Experiments on RESIDE shed light on the comparisons and limitations of state-of-the-art dehazing algorithms, and suggest promising future directions.



### 3D Face Reconstruction with Region Based Best Fit Blending Using Mobile Phone for Virtual Reality Based Social Media
- **Arxiv ID**: http://arxiv.org/abs/1801.01089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01089v1)
- **Published**: 2017-12-12 07:46:17+00:00
- **Updated**: 2017-12-12 07:46:17+00:00
- **Authors**: Gholamreza Anbarjafari, Rain Eric Haamer, Iiris Lusi, Toomas Tikk, Lembit Valgma
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: The use of virtual reality (VR) is exponentially increasing and due to that many researchers has started to work on developing new VR based social media. For this purpose it is important to have an avatar of the users which look like them to be easily generated by the devices which are accessible, such as mobile phone. In this paper, we propose a novel method of recreating a 3D human face model captured with a phone camera image or video data. The method focuses more on model shape than texture in order to make the face recognizable. We detect 68 facial feature points and use them to separate a face into four regions. For each area the best fitting models are found and are further morphed combined to find the best fitting models for each area. These are then combined and further morphed in order to restore the original facial proportions. We also present a method of texturing the resulting model, where the aforementioned feature points are used to generate a texture for the resulting model



### Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1712.04248v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1712.04248v2)
- **Published**: 2017-12-12 11:36:26+00:00
- **Updated**: 2018-02-16 14:40:42+00:00
- **Authors**: Wieland Brendel, Jonas Rauber, Matthias Bethge
- **Comment**: Published as a conference paper at the Sixth International Conference
  on Learning Representations (ICLR 2018)
  https://openreview.net/forum?id=SyZI0GWCZ
- **Journal**: None
- **Summary**: Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .



### Review. Machine learning techniques for traffic sign detection
- **Arxiv ID**: http://arxiv.org/abs/1712.04391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04391v2)
- **Published**: 2017-12-12 17:00:29+00:00
- **Updated**: 2017-12-13 04:49:56+00:00
- **Authors**: Rinat Mukhometzianov, Ying Wang
- **Comment**: None
- **Journal**: None
- **Summary**: An automatic road sign detection system localizes road signs from within images captured by an on-board camera of a vehicle, and support the driver to properly ride the vehicle. Most existing algorithms include a preprocessing step, feature extraction and detection step. This paper arranges the methods applied to road sign detection into two groups: general machine learning, neural networks. In this review, the issues related to automatic road sign detection are addressed, the popular existing methods developed to tackle the road sign detection problem are reviewed, and a comparison of the features of these methods is composed.



### Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1712.04407v1
- **DOI**: 10.1109/CVPR.2018.00616
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1712.04407v1)
- **Published**: 2017-12-12 17:51:23+00:00
- **Updated**: 2017-12-12 17:51:23+00:00
- **Authors**: Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training. We are able to generate a high diversity of plausible logos and we demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. Moreover, we validate the proposed clustered GAN training on CIFAR 10, achieving state-of-the-art Inception scores when using synthetic labels obtained via clustering the features of an ImageNet classifier. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models will be made publicly available at https://data.vision.ee.ethz.ch/cvl/lld/.



### Deception Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1712.04415v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1712.04415v1)
- **Published**: 2017-12-12 18:16:43+00:00
- **Updated**: 2017-12-12 18:16:43+00:00
- **Authors**: Zhe Wu, Bharat Singh, Larry S. Davis, V. S. Subrahmanian
- **Comment**: AAAI 2018, project page: https://doubaibai.github.io/DARE/
- **Journal**: None
- **Summary**: We present a system for covert automated deception detection in real-life courtroom trial videos. We study the importance of different modalities like vision, audio and text for this task. On the vision side, our system uses classifiers trained on low level video features which predict human micro-expressions. We show that predictions of high-level micro-expressions can be used as features for deception prediction. Surprisingly, IDT (Improved Dense Trajectory) features which have been widely used for action recognition, are also very good at predicting deception in videos. We fuse the score of classifiers trained on IDT features and high-level micro-expressions to improve performance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio domain also provide a significant boost in performance, while information from transcripts is not very beneficial for our system. Using various classifiers, our automated system obtains an AUC of 0.877 (10-fold cross-validation) when evaluated on subjects which were not part of the training set. Even though state-of-the-art methods use human annotations of micro-expressions for deception detection, our fully automated approach outperforms them by 5%. When combined with human annotations of micro-expressions, our AUC improves to 0.922. We also present results of a user-study to analyze how well do average humans perform on this task, what modalities they use for deception detection and how they perform if only one modality is accessible. Our project page can be found at \url{https://doubaibai.github.io/DARE/}.



### Conditional Generative Adversarial Networks for Emoji Synthesis with Word Embedding Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1712.04421v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04421v3)
- **Published**: 2017-12-12 18:22:33+00:00
- **Updated**: 2017-12-14 19:00:16+00:00
- **Authors**: Dianna Radpour, Vivek Bheda
- **Comment**: 5 pages, 3 figures, 2 graphs
- **Journal**: None
- **Summary**: Emojis have become a very popular part of daily digital communication. Their appeal comes largely in part due to their ability to capture and elicit emotions in a more subtle and nuanced way than just plain text is able to. In line with recent advances in the field of deep learning, there are far reaching implications and applications that generative adversarial networks (GANs) can have for image generation. In this paper, we present a novel application of deep convolutional GANs (DC-GANs) with an optimized training procedure. We show that via incorporation of word embeddings conditioned on Google's word2vec model into the network, the generator is able to synthesize highly realistic emojis that are virtually identical to the real ones.



### 3D Object Classification via Spherical Projections
- **Arxiv ID**: http://arxiv.org/abs/1712.04426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04426v1)
- **Published**: 2017-12-12 18:37:34+00:00
- **Updated**: 2017-12-12 18:37:34+00:00
- **Authors**: Zhangjie Cao, Qixing Huang, Karthik Ramani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new method for classifying 3D objects. Our main idea is to project a 3D object onto a spherical domain centered around its barycenter and develop neural network to classify the spherical projection. We introduce two complementary projections. The first captures depth variations of a 3D object, and the second captures contour-information viewed from different angles. Spherical projections combine key advantages of two main-stream 3D classification methods: image-based and 3D-based. Specifically, spherical projections are locally planar, allowing us to use massive image datasets (e.g, ImageNet) for pre-training. Also spherical projections are similar to voxel-based methods, as they encode complete information of a 3D object in a single neural network capturing dependencies across different views. Our novel network design can fully utilize these advantages. Experimental results on ModelNet40 and ShapeNetCore show that our method is superior to prior methods.



### Data Distillation: Towards Omni-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1712.04440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04440v1)
- **Published**: 2017-12-12 18:55:57+00:00
- **Updated**: 2017-12-12 18:55:57+00:00
- **Authors**: Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He
- **Comment**: tech report
- **Journal**: None
- **Summary**: We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.



### Learning a Complete Image Indexing Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1712.04480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04480v1)
- **Published**: 2017-12-12 19:37:43+00:00
- **Updated**: 2017-12-12 19:37:43+00:00
- **Authors**: Himalaya Jain, Joaquin Zepeda, Patrick Pérez, Rémi Gribonval
- **Comment**: None
- **Journal**: None
- **Summary**: To work at scale, a complete image indexing system comprises two components: An inverted file index to restrict the actual search to only a subset that should contain most of the items relevant to the query; An approximate distance computation mechanism to rapidly scan these lists. While supervised deep learning has recently enabled improvements to the latter, the former continues to be based on unsupervised clustering in the literature. In this work, we propose a first system that learns both components within a unifying neural framework of structured binary encoding.



### Image Registration for the Alignment of Digitized Historical Documents
- **Arxiv ID**: http://arxiv.org/abs/1712.04482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1712.04482v1)
- **Published**: 2017-12-12 19:41:42+00:00
- **Updated**: 2017-12-12 19:41:42+00:00
- **Authors**: AmirAbbas Davari, Tobias Lindenberger, Armin Häberle, Vincent Christlein, Andreas Maier, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we conducted a survey on different registration algorithms and investigated their suitability for hyperspectral historical image registration applications. After the evaluation of different algorithms, we choose an intensity based registration algorithm with a curved transformation model. For the transformation model, we select cubic B-splines since they should be capable to cope with all non-rigid deformations in our hyperspectral images. From a number of similarity measures, we found that residual complexity and localized mutual information are well suited for the task at hand. In our evaluation, both measures show an acceptable performance in handling all difficulties, e.g., capture range, non-stationary and spatially varying intensity distortions or multi-modality that occur in our application.



### Fingerprint Spoof Buster
- **Arxiv ID**: http://arxiv.org/abs/1712.04489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04489v1)
- **Published**: 2017-12-12 20:05:08+00:00
- **Updated**: 2017-12-12 20:05:08+00:00
- **Authors**: Tarang Chugh, Kai Cao, Anil K. Jain
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: The primary purpose of a fingerprint recognition system is to ensure a reliable and accurate user authentication, but the security of the recognition system itself can be jeopardized by spoof attacks. This study addresses the problem of developing accurate, generalizable, and efficient algorithms for detecting fingerprint spoof attacks. Specifically, we propose a deep convolutional neural network based approach utilizing local patches centered and aligned using fingerprint minutiae. Experimental results on three public-domain LivDet datasets (2011, 2013, and 2015) show that the proposed approach provides state-of-the-art accuracies in fingerprint spoof detection for intra-sensor, cross-material, cross-sensor, as well as cross-dataset testing scenarios. For example, in LivDet 2015, the proposed approach achieves 99.03% average accuracy over all sensors compared to 95.51% achieved by the LivDet 2015 competition winners. Additionally, two new fingerprint presentation attack datasets containing more than 20,000 images, using two different fingerprint readers, and over 12 different spoof fabrication materials are collected. We also present a graphical user interface, called Fingerprint Spoof Buster, that allows the operator to visually examine the local regions of the fingerprint highlighted as live or spoof, instead of relying on only a single score as output by the traditional approaches.



### Camera Calibration for Daylight Specular-Point Locus
- **Arxiv ID**: http://arxiv.org/abs/1712.04509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04509v1)
- **Published**: 2017-12-12 20:51:48+00:00
- **Updated**: 2017-12-12 20:51:48+00:00
- **Authors**: Mark S. Drew, Hamid Reza Vaezi Joze, Graham D. Finlayson
- **Comment**: April 2012
- **Journal**: None
- **Summary**: In this paper we present a new camera calibration method aimed at finding a straight-line locus, in a special colour feature space, that is traversed by daylights and as well also approximately followed by specular points. The aim of the calibration is to enable recovering the colour of the illuminant in a scene, using the calibrated camera. First we prove theoretically that any candidate specular points, for an image that is generated by a specific camera and taken under a daylight, must lie on a straight line in log-chromaticity space, for a chromaticity that is generated using a geometric-mean denominator. Use is made of the assumptions that daylight illuminants can be approximated using Planckians and that camera sensors are narrowband or can be made so by spectral sharpening. Then we show how a particular camera can be calibrated so as to discover this locus. As applications we use this curve for illuminant detection, and also for re-lighting of images to show they would appear under lighting having a different colour temperature.



### Face-from-Depth for Head Pose Estimation on Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1712.05277v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.05277v2)
- **Published**: 2017-12-12 20:57:15+00:00
- **Updated**: 2018-08-30 13:51:43+00:00
- **Authors**: Guido Borghi, Matteo Fabbri, Roberto Vezzani, Simone Calderara, Rita Cucchiara
- **Comment**: Submitted to IEEE Transactions on PAMI, updated version (second
  round). arXiv admin note: substantial text overlap with arXiv:1611.10195
- **Journal**: None
- **Summary**: Depth cameras allow to set up reliable solutions for people monitoring and behavior understanding, especially when unstable or poor illumination conditions make unusable common RGB sensors. Therefore, we propose a complete framework for the estimation of the head and shoulder pose based on depth images only. A head detection and localization module is also included, in order to develop a complete end-to-end system. The core element of the framework is a Convolutional Neural Network, called POSEidon+, that receives as input three types of images and provides the 3D angles of the pose as output. Moreover, a Face-from-Depth component based on a Deterministic Conditional GAN model is able to hallucinate a face from the corresponding depth image. We empirically demonstrate that this positively impacts the system performances. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Experimental results show that our method overcomes several recent state-of-art works based on both intensity and depth input data, running in real-time at more than 30 frames per second.



### Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View
- **Arxiv ID**: http://arxiv.org/abs/1712.04569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1712.04569v1)
- **Published**: 2017-12-12 23:47:53+00:00
- **Updated**: 2017-12-12 23:47:53+00:00
- **Authors**: Shuran Song, Andy Zeng, Angel X. Chang, Manolis Savva, Silvio Savarese, Thomas Funkhouser
- **Comment**: Video summary: https://youtu.be/Au3GmktK-So
- **Journal**: None
- **Summary**: We present Im2Pano3D, a convolutional neural network that generates a dense prediction of 3D structure and a probability distribution of semantic labels for a full 360 panoramic view of an indoor scene when given only a partial observation (<= 50%) in the form of an RGB-D image. To make this possible, Im2Pano3D leverages strong contextual priors learned from large-scale synthetic and real-world indoor scenes. To ease the prediction of 3D structure, we propose to parameterize 3D surfaces with their plane equations and train the model to predict these parameters directly. To provide meaningful training supervision, we use multiple loss functions that consider both pixel level accuracy and global context consistency. Experiments demon- strate that Im2Pano3D is able to predict the semantics and 3D structure of the unobserved scene with more than 56% pixel accuracy and less than 0.52m average distance error, which is significantly better than alternative approaches.




# Arxiv Papers in cs.CV on 2017-03-08
### Tree-Structured Reinforcement Learning for Sequential Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1703.02710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02710v1)
- **Published**: 2017-03-08 05:24:52+00:00
- **Updated**: 2017-03-08 05:24:52+00:00
- **Authors**: Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Feng Lu, Shuicheng Yan
- **Comment**: Advances in Neural Information Processing Systems 2016
- **Journal**: In Advances in Neural Information Processing Systems (pp. 127-135)
  (2016)
- **Summary**: Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to find multiple objects with a single feed-forward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.



### A Pursuit of Temporal Accuracy in General Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.02716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02716v1)
- **Published**: 2017-03-08 05:52:52+00:00
- **Updated**: 2017-03-08 05:52:52+00:00
- **Authors**: Yuanjun Xiong, Yue Zhao, Limin Wang, Dahua Lin, Xiaoou Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting activities in untrimmed videos is an important but challenging task. The performance of existing methods remains unsatisfactory, e.g., they often meet difficulties in locating the beginning and end of a long complex action. In this paper, we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos. Our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries. The other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance. On two challenging temporal activity detection datasets, THUMOS14 and ActivityNet, the proposed framework significantly outperforms the existing state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures.



### Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1703.02719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02719v1)
- **Published**: 2017-03-08 06:14:55+00:00
- **Updated**: 2017-03-08 06:14:55+00:00
- **Authors**: Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: One of recent trends [30, 31, 14] in network architec- ture design is stacking small filters (e.g., 1x1 or 3x3) in the entire network because the stacked small filters is more ef- ficient than a large kernel, given the same computational complexity. However, in the field of semantic segmenta- tion, where we need to perform dense per-pixel prediction, we find that the large kernel (and effective receptive field) plays an important role when we have to perform the clas- sification and localization tasks simultaneously. Following our design principle, we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. We also suggest a residual-based boundary refinement to further refine the ob- ject boundaries. Our approach achieves state-of-art perfor- mance on two public benchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.



### A Linear Extrinsic Calibration of Kaleidoscopic Imaging System from Single 3D Point
- **Arxiv ID**: http://arxiv.org/abs/1703.02826v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02826v3)
- **Published**: 2017-03-08 13:17:13+00:00
- **Updated**: 2017-05-27 13:45:09+00:00
- **Authors**: Kosuke Takahashi, Akihiro Miyata, Shohei Nobuhara, Takashi Matsuyama
- **Comment**: to appear in CVPR 2017
- **Journal**: None
- **Summary**: This paper proposes a new extrinsic calibration of kaleidoscopic imaging system by estimating normals and distances of the mirrors. The problem to be solved in this paper is a simultaneous estimation of all mirror parameters consistent throughout multiple reflections. Unlike conventional methods utilizing a pair of direct and mirrored images of a reference 3D object to estimate the parameters on a per-mirror basis, our method renders the simultaneous estimation problem into solving a linear set of equations. The key contribution of this paper is to introduce a linear estimation of multiple mirror parameters from kaleidoscopic 2D projections of a single 3D point of unknown geometry. Evaluations with synthesized and real images demonstrate the performance of the proposed algorithm in comparison with conventional methods.



### Deep Bayesian Active Learning with Image Data
- **Arxiv ID**: http://arxiv.org/abs/1703.02910v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.02910v1)
- **Published**: 2017-03-08 16:53:57+00:00
- **Updated**: 2017-03-08 16:53:57+00:00
- **Authors**: Yarin Gal, Riashat Islam, Zoubin Ghahramani
- **Comment**: None
- **Journal**: None
- **Summary**: Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).



### Transformation-Grounded Image Generation Network for Novel 3D View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1703.02921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02921v1)
- **Published**: 2017-03-08 17:16:15+00:00
- **Updated**: 2017-03-08 17:16:15+00:00
- **Authors**: Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, Alexander C. Berg
- **Comment**: To appear in CVPR 2017
- **Journal**: None
- **Summary**: We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Instead of taking a 'blank slate' approach, we first explicitly infer the parts of the geometry visible both in the input and novel views and then re-cast the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.



### Fast Gesture Recognition with Multiple Stream Discrete HMMs on 3D Skeletons
- **Arxiv ID**: http://arxiv.org/abs/1703.02931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02931v1)
- **Published**: 2017-03-08 17:37:13+00:00
- **Updated**: 2017-03-08 17:37:13+00:00
- **Authors**: Guido Borghi, Roberto Vezzani, Rita Cucchiara
- **Comment**: Accepted in ICPR 2016
- **Journal**: None
- **Summary**: HMMs are widely used in action and gesture recognition due to their implementation simplicity, low computational requirement, scalability and high parallelism. They have worth performance even with a limited training set. All these characteristics are hard to find together in other even more accurate methods. In this paper, we propose a novel double-stage classification approach, based on Multiple Stream Discrete Hidden Markov Models (MSD-HMM) and 3D skeleton joint data, able to reach high performances maintaining all advantages listed above. The approach allows both to quickly classify pre-segmented gestures (offline classification), and to perform temporal segmentation on streams of gestures (online classification) faster than real time. We test our system on three public datasets, MSRAction3D, UTKinect-Action and MSRDailyAction, and on a new dataset, Kinteract Dataset, explicitly created for Human Computer Interaction (HCI). We obtain state of the art performances on all of them.



### QuaSI: Quantile Sparse Image Prior for Spatio-Temporal Denoising of Retinal OCT Data
- **Arxiv ID**: http://arxiv.org/abs/1703.02942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.02942v1)
- **Published**: 2017-03-08 17:59:51+00:00
- **Updated**: 2017-03-08 17:59:51+00:00
- **Authors**: Franziska Schirrmacher, Thomas KÃ¶hler, Lennart Husvogt, James G. Fujimoto, Joachim Hornegger, Andreas K. Maier
- **Comment**: submitted to MICCAI'17
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) enables high-resolution and non-invasive 3D imaging of the human retina but is inherently impaired by speckle noise. This paper introduces a spatio-temporal denoising algorithm for OCT data on a B-scan level using a novel quantile sparse image (QuaSI) prior. To remove speckle noise while preserving image structures of diagnostic relevance, we implement our QuaSI prior via median filter regularization coupled with a Huber data fidelity model in a variational approach. For efficient energy minimization, we develop an alternating direction method of multipliers (ADMM) scheme using a linearization of median filtering. Our spatio-temporal method can handle both, denoising of single B-scans and temporally consecutive B-scans, to gain volumetric OCT data with enhanced signal-to-noise ratio. Our algorithm based on 4 B-scans only achieved comparable performance to averaging 13 B-scans and outperformed other current denoising methods.



### A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile Analytics
- **Arxiv ID**: http://arxiv.org/abs/1703.02952v7
- **DOI**: 10.1109/JIOT.2020.2967734
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.02952v7)
- **Published**: 2017-03-08 18:21:03+00:00
- **Updated**: 2019-12-27 00:15:48+00:00
- **Authors**: Seyed Ali Osia, Ali Shahin Shamsabadi, Sina Sajadmanesh, Ali Taheri, Kleomenis Katevas, Hamid R. Rabiee, Nicholas D. Lane, Hamed Haddadi
- **Comment**: To appear in IEEE Internet of Things Journal
- **Journal**: IEEE Internet of Things Journal, May 2020
- **Summary**: Internet of Things (IoT) devices and applications are being deployed in our homes and workplaces. These devices often rely on continuous data collection to feed machine learning models. However, this approach introduces several privacy and efficiency challenges, as the service operator can perform unwanted inferences on the available data. Recently, advances in edge processing have paved the way for more efficient, and private, data processing at the source for simple tasks and lighter models, though they remain a challenge for larger, and more complicated models. In this paper, we present a hybrid approach for breaking down large, complex deep neural networks for cooperative, privacy-preserving analytics. To this end, instead of performing the whole operation on the cloud, we let an IoT device to run the initial layers of the neural network, and then send the output to the cloud to feed the remaining layers and produce the final result. In order to ensure that the user's device contains no extra information except what is necessary for the main task and preventing any secondary inference on the data, we introduce Siamese fine-tuning. We evaluate the privacy benefits of this approach based on the information exposed to the cloud service. We also assess the local inference cost of different layers on a modern handset. Our evaluations show that by using Siamese fine-tuning and at a small processing cost, we can greatly reduce the level of unnecessary, potentially sensitive information in the personal data, and thus achieving the desired trade-off between utility, privacy, and performance.



### Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.03054v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.03054v1)
- **Published**: 2017-03-08 22:09:10+00:00
- **Updated**: 2017-03-08 22:09:10+00:00
- **Authors**: Xiaodan Liang, Lisa Lee, Eric P. Xing
- **Comment**: This manuscript is accepted by CVPR 2017 as a spotlight paper
- **Journal**: None
- **Summary**: Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.



### Interpretable Structure-Evolving LSTM
- **Arxiv ID**: http://arxiv.org/abs/1703.03055v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.03055v1)
- **Published**: 2017-03-08 22:09:38+00:00
- **Updated**: 2017-03-08 22:09:38+00:00
- **Authors**: Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing
- **Comment**: To appear in CVPR 2017 as a spotlight paper
- **Journal**: None
- **Summary**: This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.



### Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations
- **Arxiv ID**: http://arxiv.org/abs/1703.03073v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.03073v1)
- **Published**: 2017-03-08 23:49:20+00:00
- **Updated**: 2017-03-08 23:49:20+00:00
- **Authors**: Liangzhen Lai, Naveen Suda, Vikas Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices. To alleviate these problems to some extent, prior research utilize low precision fixed-point numbers to represent the CNN weights and activations. However, the minimum required data precision of fixed-point weights varies across different networks and also across different layers of the same network. In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We show that using floating-point representation for weights is more efficient than fixed-point representation for the same bit-width and demonstrate it on popular large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such a representation scheme enables compact hardware multiply-and-accumulate (MAC) unit design. Experimental results show that the proposed scheme reduces the weight storage by up to 36% and power consumption of the hardware multiplier by up to 50%.




# Arxiv Papers in cs.CV on 2017-03-20
### Object category understanding via eye fixations on freehand sketches
- **Arxiv ID**: http://arxiv.org/abs/1703.06554v1
- **DOI**: 10.1109/TIP.2017.2675539
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1703.06554v1)
- **Published**: 2017-03-20 01:13:33+00:00
- **Updated**: 2017-03-20 01:13:33+00:00
- **Authors**: Ravi Kiran Sarvadevabhatla, Sudharshan Suresh, R. Venkatesh Babu
- **Comment**: Accepted for publication in Transactions on Image Processing
  (http://ieeexplore.ieee.org/document/7866001/)
- **Journal**: None
- **Summary**: The study of eye gaze fixations on photographic images is an active research area. In contrast, the image subcategory of freehand sketches has not received as much attention for such studies. In this paper, we analyze the results of a free-viewing gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories. Our analysis shows that fixation sequences exhibit marked consistency within a sketch, across sketches of a category and even across suitably grouped sets of categories. This multi-level consistency is remarkable given the variability in depiction and extreme image content sparsity that characterizes hand-drawn object sketches. In our paper, we show that the multi-level consistency in the fixation data can be exploited to (a) predict a test sketch's category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects. We hope that our findings motivate the community to deem sketch-like representations worthy of gaze-based studies vis-a-vis photographic images.



### Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.06585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.06585v2)
- **Published**: 2017-03-20 03:50:57+00:00
- **Updated**: 2017-03-21 17:41:23+00:00
- **Authors**: Abhishek Das, Satwik Kottur, José M. F. Moura, Stefan Lee, Dhruv Batra
- **Comment**: 11 pages, 4 figures, 2 tables, webpage: http://visualdialog.org/
- **Journal**: None
- **Summary**: We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.



### Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1703.06618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06618v1)
- **Published**: 2017-03-20 06:56:33+00:00
- **Updated**: 2017-03-20 06:56:33+00:00
- **Authors**: Yuting Hu, Liang Zheng, Yi Yang, Yongfeng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper contributes a new large-scale dataset for weakly supervised cross-media retrieval, named Twitter100k. Current datasets, such as Wikipedia, NUS Wide and Flickr30k, have two major limitations. First, these datasets are lacking in content diversity, i.e., only some pre-defined classes are covered. Second, texts in these datasets are written in well-organized language, leading to inconsistency with realistic applications. To overcome these drawbacks, the proposed Twitter100k dataset is characterized by two aspects: 1) it has 100,000 image-text pairs randomly crawled from Twitter and thus has no constraint in the image categories; 2) text in Twitter100k is written in informal language by the users.   Since strongly supervised methods leverage the class labels that may be missing in practice, this paper focuses on weakly supervised learning for cross-media retrieval, in which only text-image pairs are exploited during training. We extensively benchmark the performance of four subspace learning methods and three variants of the Correspondence AutoEncoder, along with various text features on Wikipedia, Flickr30k and Twitter100k. Novel insights are provided. As a minor contribution, inspired by the characteristic of Twitter100k, we propose an OCR-based cross-media retrieval method. In experiment, we show that the proposed OCR-based method improves the baseline performance.



### I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.06676v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1703.06676v3)
- **Published**: 2017-03-20 11:11:38+00:00
- **Updated**: 2017-06-03 22:46:46+00:00
- **Authors**: Hao Dong, Jingqing Zhang, Douglas McIlwraith, Yike Guo
- **Comment**: International Conference on Image Processing (ICIP) 2017
- **Journal**: None
- **Summary**: Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that %the capability of our method to understand the sentence descriptions, so as to I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose



### Second-order Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.06817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06817v1)
- **Published**: 2017-03-20 16:05:21+00:00
- **Updated**: 2017-03-20 16:05:21+00:00
- **Authors**: Kaicheng Yu, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been successfully applied to many computer vision tasks, such as image classification. By performing linear combinations and element-wise nonlinear operations, these networks can be thought of as extracting solely first-order information from an input image. In the past, however, second-order statistics computed from handcrafted features, e.g., covariances, have proven highly effective in diverse recognition tasks. In this paper, we introduce a novel class of CNNs that exploit second-order statistics. To this end, we design a series of new layers that (i) extract a covariance matrix from convolutional activations, (ii) compute a parametric, second-order transformation of a matrix, and (iii) perform a parametric vectorization of a matrix. These operations can be assembled to form a Covariance Descriptor Unit (CDU), which replaces the fully-connected layers of standard CNNs. Our experiments demonstrate the benefits of our new architecture, which outperform the first-order CNNs, while relying on up to 90% fewer parameters.



### On the Limitation of Convolutional Neural Networks in Recognizing Negative Images
- **Arxiv ID**: http://arxiv.org/abs/1703.06857v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1703.06857v2)
- **Published**: 2017-03-20 17:21:19+00:00
- **Updated**: 2017-08-07 20:53:28+00:00
- **Authors**: Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal, Radha Poovendran
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance on a variety of computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. In this paper, we examine whether CNNs are capable of learning the semantics of training data. To this end, we evaluate CNNs on negative images, since they share the same structure and semantics as regular images and humans can classify them correctly. Our experimental results indicate that when training on regular images and testing on negative images, the model accuracy is significantly lower than when it is tested on regular images. This leads us to the conjecture that current training methods do not effectively train models to generalize the concepts. We then introduce the notion of semantic adversarial examples - transformed inputs that semantically represent the same objects, but the model does not classify them correctly - and present negative images as one class of such inputs.



### Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/1703.06868v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06868v2)
- **Published**: 2017-03-20 17:51:31+00:00
- **Updated**: 2017-07-30 09:32:17+00:00
- **Authors**: Xun Huang, Serge Belongie
- **Comment**: ICCV 2017. Code is available:
  https://github.com/xunhuang1995/AdaIN-style
- **Journal**: None
- **Summary**: Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.



### Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1703.06870v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06870v3)
- **Published**: 2017-03-20 17:53:38+00:00
- **Updated**: 2018-01-24 07:54:08+00:00
- **Authors**: Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick
- **Comment**: open source; appendix on more results
- **Journal**: None
- **Summary**: We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron



### Learning Correspondence Structures for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1703.06931v3
- **DOI**: 10.1109/TIP.2017.2683063
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1703.06931v3)
- **Published**: 2017-03-20 19:17:14+00:00
- **Updated**: 2017-04-27 16:15:30+00:00
- **Authors**: Weiyao Lin, Yang Shen, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang, Ke Lu
- **Comment**: IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017.
  The project page for this paper is available at
  http://min.sjtu.edu.cn/lwydemo/personReID.htm arXiv admin note: text overlap
  with arXiv:1504.06243
- **Journal**: None
- **Summary**: This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.



### Fast Spectral Ranking for Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1703.06935v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06935v3)
- **Published**: 2017-03-20 19:27:20+00:00
- **Updated**: 2018-03-29 14:00:49+00:00
- **Authors**: Ahmet Iscen, Yannis Avrithis, Giorgos Tolias, Teddy Furon, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline. This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search, we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search.



### Multi-style Generative Network for Real-time Transfer
- **Arxiv ID**: http://arxiv.org/abs/1703.06953v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06953v2)
- **Published**: 2017-03-20 20:24:11+00:00
- **Updated**: 2017-11-16 19:38:35+00:00
- **Authors**: Hang Zhang, Kristin Dana
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the rapid progress in style transfer, existing approaches using feed-forward generative network for multi-style or arbitrary-style transfer are usually compromised of image quality and model flexibility. We find it is fundamentally difficult to achieve comprehensive style modeling using 1-dimensional style embedding. Motivated by this, we introduce CoMatch Layer that learns to match the second order feature statistics with the target styles. With the CoMatch Layer, we build a Multi-style Generative Network (MSG-Net), which achieves real-time performance. We also employ an specific strategy of upsampled convolution which avoids checkerboard artifacts caused by fractionally-strided convolution. Our method has achieved superior image quality comparing to state-of-the-art approaches. The proposed MSG-Net as a general approach for real-time style transfer is compatible with most existing techniques including content-style interpolation, color-preserving, spatial control and brush stroke size control. MSG-Net is the first to achieve real-time brush-size control in a purely feed-forward manner for style transfer. Our implementations and pre-trained models for Torch, PyTorch and MXNet frameworks will be publicly available.



### Active Decision Boundary Annotation with Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1703.06971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.06971v2)
- **Published**: 2017-03-20 21:20:21+00:00
- **Updated**: 2017-08-02 09:36:55+00:00
- **Authors**: Miriam W. Huijser, Jan C. van Gemert
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: This paper is on active learning where the goal is to reduce the data annotation burden by interacting with a (human) oracle during training. Standard active learning methods ask the oracle to annotate data samples. Instead, we take a profoundly different approach: we ask for annotations of the decision boundary. We achieve this using a deep generative model to create novel instances along a 1d line. A point on the decision boundary is revealed where the instances change class. Experimentally we show on three data sets that our method can be plugged-in to other active learning schemes, that human oracles can effectively annotate points on the decision boundary, that our method is robust to annotation noise, and that decision boundary annotations improve over annotating data samples.



### SORT: Second-Order Response Transform for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.06993v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06993v3)
- **Published**: 2017-03-20 22:51:56+00:00
- **Updated**: 2017-09-14 13:25:00+00:00
- **Authors**: Yan Wang, Lingxi Xie, Chenxi Liu, Ya Zhang, Wenjun Zhang, Alan Yuille
- **Comment**: To appear in ICCV 2017 (10 pages, 4 figures)
- **Journal**: None
- **Summary**: In this paper, we reveal the importance and benefits of introducing second-order operations into deep neural networks. We propose a novel approach named Second-Order Response Transform (SORT), which appends element-wise product transform to the linear sum of a two-branch network module. A direct advantage of SORT is to facilitate cross-branch response propagation, so that each branch can update its weights based on the current status of the other branch. Moreover, SORT augments the family of transform operations and increases the nonlinearity of the network, making it possible to learn flexible functions to fit the complicated distribution of feature space. SORT can be applied to a wide range of network architectures, including a branched variant of a chain-styled network and a residual network, with very light-weighted modifications. We observe consistent accuracy gain on both small (CIFAR10, CIFAR100 and SVHN) and big (ILSVRC2012) datasets. In addition, SORT is very efficient, as the extra computation overhead is less than 5%.



### Spatio-Temporal Facial Expression Recognition Using Convolutional Neural Networks and Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1703.06995v2
- **DOI**: 10.1109/FG.2017.99
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.06995v2)
- **Published**: 2017-03-20 23:08:21+00:00
- **Updated**: 2017-04-24 23:08:17+00:00
- **Authors**: Behzad Hasani, Mohammad H. Mahoor
- **Comment**: To appear in 12th IEEE Conference on Automatic Face and Gesture
  Recognition Workshop
- **Journal**: 2017 12th IEEE International Conference on Automatic Face &
  Gesture Recognition (FG 2017)
- **Summary**: Automated Facial Expression Recognition (FER) has been a challenging task for decades. Many of the existing works use hand-crafted features such as LBP, HOG, LPQ, and Histogram of Optical Flow (HOF) combined with classifiers such as Support Vector Machines for expression recognition. These methods often require rigorous hyperparameter tuning to achieve good results. Recently Deep Neural Networks (DNN) have shown to outperform traditional methods in visual object recognition. In this paper, we propose a two-part network consisting of a DNN-based architecture followed by a Conditional Random Field (CRF) module for facial expression recognition in videos. The first part captures the spatial relation within facial images using convolutional layers followed by three Inception-ResNet modules and two fully-connected layers. To capture the temporal relation between the image frames, we use linear chain CRF in the second part of our network. We evaluate our proposed network on three publicly available databases, viz. CK+, MMI, and FERA. Experiments are performed in subject-independent and cross-database manners. Our experimental results show that cascading the deep network architecture with the CRF module considerably increases the recognition of facial expressions in videos and in particular it outperforms the state-of-the-art methods in the cross-database experiments and yields comparable results in the subject-independent experiments.




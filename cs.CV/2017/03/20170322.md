# Arxiv Papers in cs.CV on 2017-03-22
### PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding
- **Arxiv ID**: http://arxiv.org/abs/1703.07475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07475v2)
- **Published**: 2017-03-22 00:22:49+00:00
- **Updated**: 2017-03-28 01:01:29+00:00
- **Authors**: Chunhui Liu, Yueyu Hu, Yanghao Li, Sijie Song, Jiaying Liu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Despite the fact that many 3D human activity benchmarks being proposed, most existing action datasets focus on the action recognition tasks for the segmented videos. There is a lack of standard large-scale benchmarks, especially for current popular data-hungry deep learning based methods. In this paper, we introduce a new large scale benchmark (PKU-MMD) for continuous multi-modality 3D human action understanding and cover a wide range of complex human activities with well annotated information. PKU-MMD contains 1076 long video sequences in 51 action categories, performed by 66 subjects in three camera views. It contains almost 20,000 action instances and 5.4 million frames in total. Our dataset also provides multi-modality data sources, including RGB, depth, Infrared Radiation and Skeleton. With different modalities, we conduct extensive experiments on our dataset in terms of two scenarios and evaluate different methods by various metrics, including a new proposed evaluation protocol 2D-AP. We believe this large-scale dataset will benefit future researches on action detection for the community.



### Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes
- **Arxiv ID**: http://arxiv.org/abs/1703.07478v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07478v3)
- **Published**: 2017-03-22 00:44:26+00:00
- **Updated**: 2017-04-11 18:36:34+00:00
- **Authors**: S. Alireza Golestaneh, Lina J. Karam
- **Comment**: Accepted to CVPR 2017
- **Journal**: None
- **Summary**: The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address the blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.



### Knowledge Transfer for Melanoma Screening with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.07479v1
- **DOI**: 10.1109/ISBI.2017.7950523
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07479v1)
- **Published**: 2017-03-22 00:51:14+00:00
- **Updated**: 2017-03-22 00:51:14+00:00
- **Authors**: Afonso Menegola, Michel Fornaciali, Ramon Pires, Flávia Vasques Bittencourt, Sandra Avila, Eduardo Valle
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Knowledge transfer impacts the performance of deep learning -- the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.



### Deep Photo Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1703.07511v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07511v3)
- **Published**: 2017-03-22 04:21:41+00:00
- **Updated**: 2017-04-11 03:53:28+00:00
- **Authors**: Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.



### Video Frame Interpolation via Adaptive Convolution
- **Arxiv ID**: http://arxiv.org/abs/1703.07514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07514v1)
- **Published**: 2017-03-22 04:31:38+00:00
- **Updated**: 2017-03-22 04:31:38+00:00
- **Authors**: Simon Niklaus, Long Mai, Feng Liu
- **Comment**: CVPR 2017, http://graphics.cs.pdx.edu/project/adaconv
- **Journal**: None
- **Summary**: Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.



### Joint Intermodal and Intramodal Label Transfers for Extremely Rare or Unseen Classes
- **Arxiv ID**: http://arxiv.org/abs/1703.07519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07519v1)
- **Published**: 2017-03-22 04:40:51+00:00
- **Updated**: 2017-03-22 04:40:51+00:00
- **Authors**: Guo-Jun Qi, Wei Liu, Charu Aggarwal, Thomas Huang
- **Comment**: The paper has been accepted by IEEE Transactions on Pattern Analysis
  and Machine Intelligence. It will apear in a future issue
- **Journal**: None
- **Summary**: In this paper, we present a label transfer model from texts to images for image classification tasks. The problem of image classification is often much more challenging than text classification. On one hand, labeled text data is more widely available than the labeled images for classification tasks. On the other hand, text data tends to have natural semantic interpretability, and they are often more directly related to class labels. On the contrary, the image features are not directly related to concepts inherent in class labels. One of our goals in this paper is to develop a model for revealing the functional relationships between text and image features as to directly transfer intermodal and intramodal labels to annotate the images. This is implemented by learning a transfer function as a bridge to propagate the labels between two multimodal spaces. However, the intermodal label transfers could be undermined by blindly transferring the labels of noisy texts to annotate images. To mitigate this problem, we present an intramodal label transfer process, which complements the intermodal label transfer by transferring the image labels instead when relevant text is absent from the source corpus. In addition, we generalize the inter-modal label transfer to zero-shot learning scenario where there are only text examples available to label unseen classes of images without any positive image examples. We evaluate our algorithm on an image classification task and show the effectiveness with respect to the other compared algorithms.



### Deeply-Supervised CNN for Prostate Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.07523v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07523v3)
- **Published**: 2017-03-22 04:48:36+00:00
- **Updated**: 2017-03-28 13:12:39+00:00
- **Authors**: Qikui Zhu, Bo Du, Baris Turkbey, Peter L . Choyke, Pingkun Yan
- **Comment**: Due to a crucial sign error in equation 1
- **Journal**: None
- **Summary**: Prostate segmentation from Magnetic Resonance (MR) images plays an important role in image guided interven- tion. However, the lack of clear boundary specifically at the apex and base, and huge variation of shape and texture between the images from different patients make the task very challenging. To overcome these problems, in this paper, we propose a deeply supervised convolutional neural network (CNN) utilizing the convolutional information to accurately segment the prostate from MR images. The proposed model can effectively detect the prostate region with additional deeply supervised layers compared with other approaches. Since some information will be abandoned after convolution, it is necessary to pass the features extracted from early stages to later stages. The experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.



### Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D vehicle analysis from monocular image
- **Arxiv ID**: http://arxiv.org/abs/1703.07570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07570v1)
- **Published**: 2017-03-22 09:03:25+00:00
- **Updated**: 2017-03-22 09:03:25+00:00
- **Authors**: Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline Teulière, Thierry Chateau
- **Comment**: CVPR 2017 (to appear)
- **Journal**: None
- **Summary**: In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.



### An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1703.07579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07579v1)
- **Published**: 2017-03-22 09:25:49+00:00
- **Updated**: 2017-03-22 09:25:49+00:00
- **Authors**: Fan Wu, Zhongwen Xu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.



### Can you tell where in India I am from? Comparing humans and computers on fine-grained race face classification
- **Arxiv ID**: http://arxiv.org/abs/1703.07595v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07595v2)
- **Published**: 2017-03-22 10:35:58+00:00
- **Updated**: 2018-02-19 06:39:41+00:00
- **Authors**: Harish Katti, S. P. Arun
- **Comment**: 9 pages, 5 figure, 2 tables
- **Journal**: None
- **Summary**: Faces form the basis for a rich variety of judgments in humans, yet the underlying features remain poorly understood. Although fine-grained distinctions within a race might more strongly constrain possible facial features used by humans than in case of coarse categories such as race or gender, such fine grained distinctions are relatively less studied. Fine-grained race classification is also interesting because even humans may not be perfectly accurate on these tasks. This allows us to compare errors made by humans and machines, in contrast to standard object detection tasks where human performance is nearly perfect. We have developed a novel face database of close to 1650 diverse Indian faces labeled for fine-grained race (South vs North India) as well as for age, weight, height and gender. We then asked close to 130 human subjects who were instructed to categorize each face as belonging toa Northern or Southern state in India. We then compared human performance on this task with that of computational models trained on the ground-truth labels. Our main results are as follows: (1) Humans are highly consistent (average accuracy: 63.6%), with some faces being consistently classified with > 90% accuracy and others consistently misclassified with < 30% accuracy; (2) Models trained on ground-truth labels showed slightly worse performance (average accuracy: 62%) but showed higher accuracy (72.2%) on faces classified with > 80% accuracy by humans. This was true for models trained on simple spatial and intensity measurements extracted from faces as well as deep neural networks trained on race or gender classification; (3) Using overcomplete banks of features derived from each face part, we found that mouth shape was the single largest contributor towards fine-grained race classification, whereas distances between face parts was the strongest predictor of gender.



### Neural Ctrl-F: Segmentation-free Query-by-String Word Spotting in Handwritten Manuscript Collections
- **Arxiv ID**: http://arxiv.org/abs/1703.07645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07645v2)
- **Published**: 2017-03-22 13:35:49+00:00
- **Updated**: 2017-08-17 13:19:02+00:00
- **Authors**: Tomas Wilkinson, Jonas Lindström, Anders Brun
- **Comment**: To appear in ICCV 2017
- **Journal**: None
- **Summary**: In this paper, we approach the problem of segmentation-free query-by-string word spotting for handwritten documents. In other words, we use methods inspired from computer vision and machine learning to search for words in large collections of digitized manuscripts. In particular, we are interested in historical handwritten texts, which are often far more challenging than modern printed documents. This task is important, as it provides people with a way to quickly find what they are looking for in large collections that are tedious and difficult to read manually. To this end, we introduce an end-to-end trainable model based on deep neural networks that we call Ctrl-F-Net. Given a full manuscript page, the model simultaneously generates region proposals, and embeds these into a distributed word embedding space, where searches are performed. We evaluate the model on common benchmarks for handwritten word spotting, outperforming the previous state-of-the-art segmentation-free approaches by a large margin, and in some cases even segmentation-based approaches. One interesting real-life application of our approach is to help historians to find and count specific words in court records that are related to women's sustenance activities and division of labor. We provide promising preliminary experiments that validate our method on this task.



### ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.07655v2
- **DOI**: 10.1109/JETCAS.2017.2769684
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.07655v2)
- **Published**: 2017-03-22 13:48:47+00:00
- **Updated**: 2018-06-08 20:17:33+00:00
- **Authors**: Priyadarshini Panda, Jason M. Allred, Shriram Ramanathan, Kaushik Roy
- **Comment**: 14 pages, 14 figures
- **Journal**: IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems (Volume: 8, Issue: 1, March 2018)
- **Summary**: A fundamental feature of learning in animals is the "ability to forget" that allows an organism to perceive, model and make decisions from disparate streams of information and adapt to changing environments. Against this backdrop, we present a novel unsupervised learning mechanism ASP (Adaptive Synaptic Plasticity) for improved recognition with Spiking Neural Networks (SNNs) for real time on-line learning in a dynamic environment. We incorporate an adaptive weight decay mechanism with the traditional Spike Timing Dependent Plasticity (STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic weights is modulated based on the temporal correlation between the spiking patterns of the pre- and post-synaptic neurons. This mechanism helps in gradual forgetting of insignificant data while retaining significant, yet old, information. ASP, thus, maintains a balance between forgetting and immediate learning to construct a stable-plastic self-adaptive SNN for continuously changing inputs. We demonstrate that the proposed learning methodology addresses catastrophic forgetting while yielding significantly improved accuracy over the conventional STDP learning method for digit recognition applications. Additionally, we observe that the proposed learning model automatically encodes selective attention towards relevant features in the input data while eliminating the influence of background noise (or denoising) further improving the robustness of the ASP learning.



### Predicting Deeper into the Future of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1703.07684v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.07684v3)
- **Published**: 2017-03-22 14:45:15+00:00
- **Updated**: 2017-08-08 10:02:36+00:00
- **Authors**: Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Verbeek, Yann LeCun
- **Comment**: Accepted to ICCV 2017. Supplementary material available on the
  authors' webpages
- **Journal**: None
- **Summary**: The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.



### Classifying Symmetrical Differences and Temporal Change in Mammography Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1703.07715v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07715v2)
- **Published**: 2017-03-22 15:46:49+00:00
- **Updated**: 2017-08-01 16:11:36+00:00
- **Authors**: Thijs Kooi, Nico Karssemeijer
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the addition of symmetry and temporal context information to a deep Convolutional Neural Network (CNN) with the purpose of detecting malignant soft tissue lesions in mammography. We employ a simple linear mapping that takes the location of a mass candidate and maps it to either the contra-lateral or prior mammogram and Regions Of Interest (ROI) are extracted around each location. We subsequently explore two different architectures (1) a fusion model employing two datastreams were both ROIs are fed to the network during training and testing and (2) a stage-wise approach where a single ROI CNN is trained on the primary image and subsequently used as feature extractor for both primary and symmetrical or prior ROIs. A 'shallow' Gradient Boosted Tree (GBT) classifier is then trained on the concatenation of these features and used to classify the joint representation. Results shown a significant increase in performance using the first architecture and symmetry information, but only marginal gains in performance using temporal data and the other setting. We feel results are promising and can greatly be improved when more temporal data becomes available.



### In Defense of the Triplet Loss for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1703.07737v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.07737v4)
- **Published**: 2017-03-22 16:34:29+00:00
- **Updated**: 2017-11-21 15:35:07+00:00
- **Authors**: Alexander Hermans, Lucas Beyer, Bastian Leibe
- **Comment**: Lucas Beyer and Alexander Hermans contributed equally. Updates: Minor
  fixes, new SOTA comparisons, add CUHK03 results
- **Journal**: None
- **Summary**: In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.



### R-C3D: Region Convolutional 3D Network for Temporal Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1703.07814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07814v2)
- **Published**: 2017-03-22 18:49:05+00:00
- **Updated**: 2017-08-04 22:37:54+00:00
- **Authors**: Huijuan Xu, Abir Das, Kate Saenko
- **Comment**: ICCV 2017 Camera Ready Version
- **Journal**: Proceedings of the International Conference on Computer Vision
  (ICCV), 2017
- **Summary**: We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at http://ai.bu.edu/r-c3d/.



### Cross-View Image Matching for Geo-localization in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/1703.07815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07815v1)
- **Published**: 2017-03-22 18:51:51+00:00
- **Updated**: 2017-03-22 18:51:51+00:00
- **Authors**: Yicong Tian, Chen Chen, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the $k$ nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.



### Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression
- **Arxiv ID**: http://arxiv.org/abs/1703.07834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.07834v2)
- **Published**: 2017-03-22 20:00:15+00:00
- **Updated**: 2017-09-08 09:10:08+00:00
- **Authors**: Aaron S. Jackson, Adrian Bulat, Vasileios Argyriou, Georgios Tzimiropoulos
- **Comment**: 10 pages, ICCV 2017
- **Journal**: None
- **Summary**: 3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon



### Two-Stream RNN/CNN for Action Recognition in 3D Videos
- **Arxiv ID**: http://arxiv.org/abs/1703.09783v2
- **DOI**: 10.1109/IROS.2017.8206288
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1703.09783v2)
- **Published**: 2017-03-22 22:29:56+00:00
- **Updated**: 2018-10-02 16:16:31+00:00
- **Authors**: Rui Zhao, Haider Ali, Patrick van der Smagt
- **Comment**: Published in 2017 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
- **Journal**: None
- **Summary**: The recognition of actions from video sequences has many applications in health monitoring, assisted living, surveillance, and smart homes. Despite advances in sensing, in particular related to 3D video, the methodologies to process the data are still subject to research. We demonstrate superior results by a system which combines recurrent neural networks with convolutional neural networks in a voting approach. The gated-recurrent-unit-based neural networks are particularly well-suited to distinguish actions based on long-term information from optical tracking data; the 3D-CNNs focus more on detailed, recent information from video data. The resulting features are merged in an SVM which then classifies the movement. In this architecture, our method improves recognition rates of state-of-the-art methods by 14% on standard data sets.



### Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling
- **Arxiv ID**: http://arxiv.org/abs/1703.07886v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1703.07886v2)
- **Published**: 2017-03-22 23:35:51+00:00
- **Updated**: 2017-07-26 14:50:13+00:00
- **Authors**: Mehdi Bahri, Yannis Panagakis, Stefanos Zafeiriou
- **Comment**: Accepted for publication at ICCV 2017
- **Journal**: None
- **Summary**: Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. However, K-SVD is sensitive to the presence of noise and outliers in the training set. Additionally, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.




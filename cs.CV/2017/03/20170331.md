# Arxiv Papers in cs.CV on 2017-03-31
### Deep Domain Adaptation Based Video Smoke Detection using Synthetic Smoke Images
- **Arxiv ID**: http://arxiv.org/abs/1703.10729v1
- **DOI**: 10.1016/j.firesaf.2017.08.004
- **Categories**: **cs.CV**, 68T45, I.4.9; I.5.4; I.6.8; J.2
- **Links**: [PDF](http://arxiv.org/pdf/1703.10729v1)
- **Published**: 2017-03-31 01:42:46+00:00
- **Updated**: 2017-03-31 01:42:46+00:00
- **Authors**: Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Jinjun Wang
- **Comment**: The manuscript approved by all authors is our original work, and has
  submitted to Fire Safety Journal for peer review previously. There are 4516
  words, 8 figures and 2 tables in this manuscript
- **Journal**: Fire Safety Journal 93C (2017) pp. 53-59
- **Summary**: In this paper, a deep domain adaptation based method for video smoke detection is proposed to extract a powerful feature representation of smoke. Due to the smoke image samples limited in scale and diversity for deep CNN training, we systematically produced adequate synthetic smoke images with a wide variation in the smoke shape, background and lighting conditions. Considering that the appearance gap (dataset bias) between synthetic and real smoke images degrades significantly the performance of the trained model on the test set composed fully of real images, we build deep architectures based on domain adaptation to confuse the distributions of features extracted from synthetic and real smoke images. This approach expands the domain-invariant feature space for smoke image samples. With their approximate feature distribution off non-smoke images, the recognition rate of the trained model is improved significantly compared to the model trained directly on mixed dataset of synthetic and real images. Experimentally, several deep architectures with different design choices are applied to the smoke detector. The ultimate framework can get a satisfactory result on the test set. We believe that our approach is a start in the direction of utilizing deep neural networks enhanced with synthetic smoke images for video smoke detection.



### Unsupervised Holistic Image Generation from Key Local Patches
- **Arxiv ID**: http://arxiv.org/abs/1703.10730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10730v2)
- **Published**: 2017-03-31 01:43:06+00:00
- **Updated**: 2017-04-03 00:38:12+00:00
- **Authors**: Donghoon Lee, Sangdoo Yun, Sungjoon Choi, Hwiyeon Yoo, Ming-Hsuan Yang, Songhwai Oh
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: We introduce a new problem of generating an image based on a small number of key local patches without any geometric prior. In this work, key local patches are defined as informative regions of the target object or scene. This is a challenging problem since it requires generating realistic images and predicting locations of parts at the same time. We construct adversarial networks to tackle this problem. A generator network generates a fake image as well as a mask based on the encoder-decoder framework. On the other hand, a discriminator network aims to detect fake images. The network is trained with three losses to consider spatial, appearance, and adversarial information. The spatial loss determines whether the locations of predicted parts are correct. Input patches are restored in the output image without much modification due to the appearance loss. The adversarial loss ensures output images are realistic. The proposed network is trained without supervisory signals since no labels of key parts are required. Experimental results on six datasets demonstrate that the proposed algorithm performs favorably on challenging objects and scenes.



### Novel Framework for Spectral Clustering using Topological Node Features(TNF)
- **Arxiv ID**: http://arxiv.org/abs/1703.10756v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10756v2)
- **Published**: 2017-03-31 04:56:46+00:00
- **Updated**: 2017-04-08 17:00:22+00:00
- **Authors**: Lalith Srikanth Chintalapati, Raghunatha Sarma Rachakonda
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Spectral clustering has gained importance in recent years due to its ability to cluster complex data as it requires only pairwise similarity among data points with its ease of implementation. The central point in spectral clustering is the process of capturing pair-wise similarity. In the literature, many research techniques have been proposed for effective construction of affinity matrix with suitable pair- wise similarity. In this paper a general framework for capturing pairwise affinity using local features such as density, proximity and structural similarity is been proposed. Topological Node Features are exploited to define the notion of density and local structure. These local features are incorporated into the construction of the affinity matrix. Experimental results, on widely used datasets such as synthetic shape datasets, UCI real datasets and MNIST handwritten datasets show that the proposed framework outperforms standard spectral clustering methods.



### Diabetic Retinopathy Detection via Deep Convolutional Networks for Discriminative Localization and Visual Explanation
- **Arxiv ID**: http://arxiv.org/abs/1703.10757v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1703.10757v3)
- **Published**: 2017-03-31 05:10:56+00:00
- **Updated**: 2019-12-02 22:05:07+00:00
- **Authors**: Zhiguang Wang, Jianbo Yang
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: We proposed a deep learning method for interpretable diabetic retinopathy (DR) detection. The visual-interpretable feature of the proposed method is achieved by adding the regression activation map (RAM) after the global averaging pooling layer of the convolutional networks (CNN). With RAM, the proposed model can localize the discriminative regions of an retina image to show the specific region of interest in terms of its severity level. We believe this advantage of the proposed deep learning model is highly desired for DR detection because in practice, users are not only interested with high prediction performance, but also keen to understand the insights of DR detection and why the adopted learning model works. In the experiments conducted on a large scale of retina image dataset, we show that the proposed CNN model can achieve high performance on DR detection compared with the state-of-the-art while achieving the merits of providing the RAM to highlight the salient regions of the input image.



### A Hybrid Data Association Framework for Robust Online Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1703.10764v1
- **DOI**: 10.1109/TIP.2017.2745103
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10764v1)
- **Published**: 2017-03-31 05:49:33+00:00
- **Updated**: 2017-03-31 05:49:33+00:00
- **Authors**: Min Yang, Yuwei Wu, Yunde Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Global optimization algorithms have shown impressive performance in data-association based multi-object tracking, but handling online data remains a difficult hurdle to overcome. In this paper, we present a hybrid data association framework with a min-cost multi-commodity network flow for robust online multi-object tracking. We build local target-specific models interleaved with global optimization of the optimal data association over multiple video frames. More specifically, in the min-cost multi-commodity network flow, the target-specific similarities are online learned to enforce the local consistency for reducing the complexity of the global data association. Meanwhile, the global data association taking multiple video frames into account alleviates irrecoverable errors caused by the local data association between adjacent frames. To ensure the efficiency of online tracking, we give an efficient near-optimal solution to the proposed min-cost multi-commodity flow problem, and provide the empirical proof of its sub-optimality. The comprehensive experiments on real data demonstrate the superior tracking performance of our approach in various challenging situations.



### Semantic-driven Generation of Hyperlapse from $360^\circ$ Video
- **Arxiv ID**: http://arxiv.org/abs/1703.10798v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10798v4)
- **Published**: 2017-03-31 08:42:00+00:00
- **Updated**: 2017-10-10 00:05:17+00:00
- **Authors**: Wei-Sheng Lai, Yujia Huang, Neel Joshi, Chris Buehler, Ming-Hsuan Yang, Sing Bing Kang
- **Comment**: This work is accepted in Transactions on Visualization and Computer
  Graphics (TVCG)
- **Journal**: None
- **Summary**: We present a system for converting a fully panoramic ($360^\circ$) video into a normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our system exploits visual saliency and semantics to non-uniformly sample in space and time for generating hyperlapses. In addition, users can optionally choose objects of interest for customizing the hyperlapses. We first stabilize an input $360^\circ$ video by smoothing the rotation between adjacent frames and then compute regions of interest and saliency scores. An initial hyperlapse is generated by optimizing the saliency and motion smoothness followed by the saliency-aware frame selection. We further smooth the result using an efficient 2D video stabilization approach that adaptively selects the motion model to generate the final hyperlapse. We validate the design of our system by showing results for a variety of scenes and comparing against the state-of-the-art method through a user study.



### End-To-End Face Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1703.10818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10818v1)
- **Published**: 2017-03-31 09:48:32+00:00
- **Updated**: 2017-03-31 09:48:32+00:00
- **Authors**: Liying Chi, Hongxin Zhang, Mingxiu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Plenty of face detection and recognition methods have been proposed and got delightful results in decades. Common face recognition pipeline consists of: 1) face detection, 2) face alignment, 3) feature extraction, 4) similarity calculation, which are separated and independent from each other. The separated face analyzing stages lead the model redundant calculation and are hard for end-to-end training. In this paper, we proposed a novel end-to-end trainable convolutional network framework for face detection and recognition, in which a geometric transformation matrix was directly learned to align the faces, instead of predicting the facial landmarks. In training stage, our single CNN model is supervised only by face bounding boxes and personal identities, which are publicly available from WIDER FACE \cite{Yang2016} dataset and CASIA-WebFace \cite{Yi2014} dataset. Tested on Face Detection Dataset and Benchmark (FDDB) \cite{Jain2010} dataset and Labeled Face in the Wild (LFW) \cite{Huang2007} dataset, we have achieved 89.24\% recall for face detection task and 98.63\% verification accuracy for face recognition task simultaneously, which are comparable to state-of-the-art results.



### (DE)^2 CO: Deep Depth Colorization
- **Arxiv ID**: http://arxiv.org/abs/1703.10881v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10881v3)
- **Published**: 2017-03-31 12:30:30+00:00
- **Updated**: 2018-02-21 14:06:03+00:00
- **Authors**: F. M. Carlucci, P. Russo, B. Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to classify objects is fundamental for robots. Besides knowledge about their visual appearance, captured by the RGB channel, robots heavily need also depth information to make sense of the world. While the use of deep networks on RGB robot images has benefited from the plethora of results obtained on databases like ImageNet, using convnets on depth images requires mapping them into three dimensional channels. This transfer learning procedure makes them processable by pre-trained deep architectures. Current mappings are based on heuristic assumptions over preprocessing steps and on what depth properties should be most preserved, resulting often in cumbersome data visualizations, and in sub-optimal performance in terms of generality and recognition results. Here we take an alternative route and we attempt instead to learn an optimal colorization mapping for any given pre-trained architecture, using as training data a reference RGB-D database. We propose a deep network architecture, exploiting the residual paradigm, that learns how to map depth data to three channel images. A qualitative analysis of the images obtained with this approach clearly indicates that learning the optimal mapping preserves the richness of depth information better than current hand-crafted approaches. Experiments on the Washington, JHUIT-50 and BigBIRD public benchmark databases, using CaffeNet, VGG16, GoogleNet, and ResNet50 clearly showcase the power of our approach, with gains in performance of up to 16% compared to state of the art competitors on the depth channel only, leading to top performances when dealing with RGB-D data



### Single Image Super Resolution - When Model Adaptation Matters
- **Arxiv ID**: http://arxiv.org/abs/1703.10889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10889v1)
- **Published**: 2017-03-31 13:20:19+00:00
- **Updated**: 2017-03-31 13:20:19+00:00
- **Authors**: Yudong Liang, Radu Timofte, Jinjun Wang, Yihong Gong, Nanning Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent years impressive advances were made for single image super-resolution. Deep learning is behind a big part of this success. Deep(er) architecture design and external priors modeling are the key ingredients. The internal contents of the low resolution input image is neglected with deep modeling despite the earlier works showing the power of using such internal priors. In this paper we propose a novel deep convolutional neural network carefully designed for robustness and efficiency at both learning and testing. Moreover, we propose a couple of model adaptation strategies to the internal contents of the low resolution input image and analyze their strong points and weaknesses. By trading runtime and using internal priors we achieve 0.1 up to 0.3dB PSNR improvements over best reported results on standard datasets. Our adaptation especially favors images with repetitive structures or under large resolutions. Moreover, it can be combined with other simple techniques, such as back-projection or enhanced prediction, for further improvements.



### BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth
- **Arxiv ID**: http://arxiv.org/abs/1703.10896v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10896v2)
- **Published**: 2017-03-31 13:56:35+00:00
- **Updated**: 2018-03-26 16:08:36+00:00
- **Authors**: Mahdi Rad, Vincent Lepetit
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a "holistic" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.



### Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/1703.10898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10898v1)
- **Published**: 2017-03-31 13:59:31+00:00
- **Updated**: 2017-03-31 13:59:31+00:00
- **Authors**: Jie Song, Limin Wang, Luc Van Gool, Otmar Hilliges
- **Comment**: Preliminary version to appear in CVPR2017
- **Journal**: None
- **Summary**: Deep ConvNets have been shown to be effective for the task of human pose estimation from single images. However, several challenging issues arise in the video-based case such as self-occlusion, motion blur, and uncommon poses with few or no examples in training data sets. Temporal information can provide additional cues about the location of body joints and help to alleviate these issues. In this paper, we propose a deep structured model to estimate a sequence of human poses in unconstrained videos. This model can be efficiently trained in an end-to-end manner and is capable of representing appearance of body joints and their spatio-temporal relationships simultaneously. Domain knowledge about the human body is explicitly incorporated into the network providing effective priors to regularize the skeletal structure and to enforce temporal consistency. The proposed end-to-end architecture is evaluated on two widely used benchmarks (Penn Action dataset and JHMDB dataset) for video-based pose estimation. Our approach significantly outperforms the existing state-of-the-art methods.



### Unsupervised learning from video to detect foreground objects in single images
- **Arxiv ID**: http://arxiv.org/abs/1703.10901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10901v1)
- **Published**: 2017-03-31 14:05:13+00:00
- **Updated**: 2017-03-31 14:05:13+00:00
- **Authors**: Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning from visual data is one of the most difficult challenges in computer vision, being a fundamental task for understanding how visual recognition works. From a practical point of view, learning from unsupervised visual input has an immense practical value, as very large quantities of unlabeled videos can be collected at low cost. In this paper, we address the task of unsupervised learning to detect and segment foreground objects in single images. We achieve our goal by training a student pathway, consisting of a deep neural network. It learns to predict from a single input image (a video frame) the output for that particular frame, of a teacher pathway that performs unsupervised object discovery in video. Our approach is different from the published literature that performs unsupervised discovery in videos or in collections of images at test time. We move the unsupervised discovery phase during the training stage, while at test time we apply the standard feed-forward processing along the student pathway. This has a dual benefit: firstly, it allows in principle unlimited possibilities of learning and generalization during training, while remaining very fast at testing. Secondly, the student not only becomes able to detect in single images significantly better than its unsupervised video discovery teacher, but it also achieves state of the art results on two important current benchmarks, YouTube Objects and Object Discovery datasets. Moreover, at test time, our system is at least two orders of magnitude faster than other previous methods.



### Fast Predictive Multimodal Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1703.10902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10902v1)
- **Published**: 2017-03-31 14:05:57+00:00
- **Updated**: 2017-03-31 14:05:57+00:00
- **Authors**: Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer
- **Comment**: Accepted as a conference paper for ISBI 2017
- **Journal**: None
- **Summary**: We introduce a deep encoder-decoder architecture for image deformation prediction from multimodal images. Specifically, we design an image-patch-based deep network that jointly (i) learns an image similarity measure and (ii) the relationship between image patches and deformation parameters. While our method can be applied to general image registration formulations, we focus on the Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By predicting the initial momentum of the shooting formulation of LDDMM, we preserve its mathematical properties and drastically reduce the computation time, compared to optimization-based approaches. Furthermore, we create a Bayesian probabilistic version of the network that allows evaluation of registration uncertainty via sampling of the network at test time. We evaluate our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our experiments show that our method generates accurate predictions and that learning the similarity measure leads to more consistent registrations than relying on generic multimodal image similarity measures, such as mutual information. Our approach is an order of magnitude faster than optimization-based LDDMM.



### Quicksilver: Fast Predictive Image Registration - a Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1703.10908v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10908v4)
- **Published**: 2017-03-31 14:13:55+00:00
- **Updated**: 2017-07-19 18:40:48+00:00
- **Authors**: Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer
- **Comment**: Add new discussions
- **Journal**: None
- **Summary**: This paper introduces Quicksilver, a fast deformable image registration method. Quicksilver registration for image-pairs works by patch-wise prediction of a deformation model based directly on image appearance. A deep encoder-decoder network is used as the prediction model. While the prediction strategy is general, we focus on predictions for the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model. Specifically, we predict the momentum-parameterization of LDDMM, which facilitates a patch-wise prediction strategy while maintaining the theoretical properties of LDDMM, such as guaranteed diffeomorphic mappings for sufficiently strong regularization. We also provide a probabilistic version of our prediction network which can be sampled during the testing time to calculate uncertainties in the predicted deformations. Finally, we introduce a new correction network which greatly increases the prediction accuracy of an already existing prediction network. We show experimental results for uni-modal atlas-to-image as well as uni- / multi- modal image-to-image registrations. These experiments demonstrate that our method accurately predicts registrations obtained by numerical optimization, is very fast, achieves state-of-the-art registration results on four standard validation datasets, and can jointly learn an image similarity measure. Quicksilver is freely available as an open-source software.



### InverseFaceNet: Deep Monocular Inverse Face Rendering
- **Arxiv ID**: http://arxiv.org/abs/1703.10956v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1703.10956v2)
- **Published**: 2017-03-31 15:47:27+00:00
- **Updated**: 2018-05-16 04:31:57+00:00
- **Authors**: Hyeongwoo Kim, Michael Zollh√∂fer, Ayush Tewari, Justus Thies, Christian Richardt, Christian Theobalt
- **Comment**: CVPR 2018 (poster) 10 pages (+5 pages)
- **Journal**: None
- **Summary**: We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. By estimating all parameters from just a single image, advanced editing possibilities on a single face image, such as appearance editing and relighting, become feasible in real time. Most previous learning-based face reconstruction approaches do not jointly recover all dimensions, or are severely limited in terms of visual quality. In contrast, we propose to recover high-quality facial pose, shape, expression, reflectance and illumination using a deep neural network that is trained using a large, synthetically created training corpus. Our approach builds on a novel loss function that measures model-space similarity directly in parameter space and significantly improves reconstruction accuracy. We further propose a self-supervised bootstrapping process in the network training loop, which iteratively updates the synthetic training corpus to better reflect the distribution of real-world imagery. We demonstrate that this strategy outperforms completely synthetically trained networks. Finally, we show high-quality reconstructions and compare our approach to several state-of-the-art approaches.



### Transfer of View-manifold Learning to Similarity Perception of Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/1704.00033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00033v1)
- **Published**: 2017-03-31 19:50:06+00:00
- **Updated**: 2017-03-31 19:50:06+00:00
- **Authors**: Xingyu Lin, Hao Wang, Zhihao Li, Yimeng Zhang, Alan Yuille, Tai Sing Lee
- **Comment**: Accepted to ICLR2017
- **Journal**: None
- **Summary**: We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.



### Efficient Registration of Pathological Images: A Joint PCA/Image-Reconstruction Approach
- **Arxiv ID**: http://arxiv.org/abs/1704.00036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00036v1)
- **Published**: 2017-03-31 19:57:12+00:00
- **Updated**: 2017-03-31 19:57:12+00:00
- **Authors**: Xu Han, Xiao Yang, Stephen Aylward, Roland Kwitt, Marc Niethammer
- **Comment**: Accepted as a conference paper for ISBI 2017
- **Journal**: None
- **Summary**: Registration involving one or more images containing pathologies is challenging, as standard image similarity measures and spatial transforms cannot account for common changes due to pathologies. Low-rank/Sparse (LRS) decomposition removes pathologies prior to registration; however, LRS is memory-demanding and slow, which limits its use on larger data sets. Additionally, LRS blurs normal tissue regions, which may degrade registration performance. This paper proposes an efficient alternative to LRS: (1) normal tissue appearance is captured by principal component analysis (PCA) and (2) blurring is avoided by an integrated model for pathology removal and image reconstruction. Results on synthetic and BRATS 2015 data demonstrate its utility.



### Geodesic Distance Histogram Feature for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1704.00077v1
- **DOI**: 10.1007/978-3-319-54181-5_18
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00077v1)
- **Published**: 2017-03-31 22:39:32+00:00
- **Updated**: 2017-03-31 22:39:32+00:00
- **Authors**: Hieu Le, Vu Nguyen, Chen-Ping Yu, Dimitris Samaras
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a geodesic-distance-based feature that encodes global information for improved video segmentation algorithms. The feature is a joint histogram of intensity and geodesic distances, where the geodesic distances are computed as the shortest paths between superpixels via their boundaries. We also incorporate adaptive voting weights and spatial pyramid configurations to include spatial information into the geodesic histogram feature and show that this further improves results. The feature is generic and can be used as part of various algorithms. In experiments, we test the geodesic histogram feature by incorporating it into two existing video segmentation frameworks. This leads to significantly better performance in 3D video segmentation benchmarks on two datasets.



### Efficient Asymmetric Co-Tracking using Uncertainty Sampling
- **Arxiv ID**: http://arxiv.org/abs/1704.00083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00083v1)
- **Published**: 2017-03-31 23:28:20+00:00
- **Updated**: 2017-03-31 23:28:20+00:00
- **Authors**: Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba, Shin Ishii
- **Comment**: Submitted to IEEE ICSIPA'2017
- **Journal**: None
- **Summary**: Adaptive tracking-by-detection approaches are popular for tracking arbitrary objects. They treat the tracking problem as a classification task and use online learning techniques to update the object model. However, these approaches are heavily invested in the efficiency and effectiveness of their detectors. Evaluating a massive number of samples for each frame (e.g., obtained by a sliding window) forces the detector to trade the accuracy in favor of speed. Furthermore, misclassification of borderline samples in the detector introduce accumulating errors in tracking. In this study, we propose a co-tracking based on the efficient cooperation of two detectors: a rapid adaptive exemplar-based detector and another more sophisticated but slower detector with a long-term memory. The sampling labeling and co-learning of the detectors are conducted by an uncertainty sampling unit, which improves the speed and accuracy of the system. We also introduce a budgeting mechanism which prevents the unbounded growth in the number of examples in the first detector to maintain its rapid response. Experiments demonstrate the efficiency and effectiveness of the proposed tracker against its baselines and its superior performance against state-of-the-art trackers on various benchmark videos.



### View Selection with Geometric Uncertainty Modeling
- **Arxiv ID**: http://arxiv.org/abs/1704.00085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1704.00085v2)
- **Published**: 2017-03-31 23:33:06+00:00
- **Updated**: 2018-02-25 23:46:34+00:00
- **Authors**: Cheng Peng, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating positions of world points from features observed in images is a key problem in 3D reconstruction, image mosaicking,simultaneous localization and mapping and structure from motion. We consider a special instance in which there is a dominant ground plane $\mathcal{G}$ viewed from a parallel viewing plane $\mathcal{S}$ above it. Such instances commonly arise, for example, in aerial photography. Consider a world point $g \in \mathcal{G}$ and its worst case reconstruction uncertainty $\varepsilon(g,\mathcal{S})$ obtained by merging \emph{all} possible views of $g$ chosen from $\mathcal{S}$. We first show that one can pick two views $s_p$ and $s_q$ such that the uncertainty $\varepsilon(g,\{s_p,s_q\})$ obtained using only these two views is almost as good as (i.e. within a small constant factor of) $\varepsilon(g,\mathcal{S})$. Next, we extend the result to the entire ground plane $\mathcal{G}$ and show that one can pick a small subset of $\mathcal{S'} \subseteq \mathcal{S}$ (which grows only linearly with the area of $\mathcal{G}$) and still obtain a constant factor approximation, for every point $g \in \mathcal{G}$, to the minimum worst case estimate obtained by merging all views in $\mathcal{S}$. Finally, we present a multi-resolution view selection method which extends our techniques to non-planar scenes. We show that the method can produce rich and accurate dense reconstructions with a small number of views. Our results provide a view selection mechanism with provable performance guarantees which can drastically increase the speed of scene reconstruction algorithms. In addition to theoretical results, we demonstrate their effectiveness in an application where aerial imagery is used for monitoring farms and orchards.




# Arxiv Papers in cs.CV on 2017-09-13
### Joint Learning of Set Cardinality and State Distribution
- **Arxiv ID**: http://arxiv.org/abs/1709.04093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04093v2)
- **Published**: 2017-09-13 00:33:50+00:00
- **Updated**: 2017-11-21 02:29:33+00:00
- **Authors**: S. Hamid Rezatofighi, Anton Milan, Qinfeng Shi, Anthony Dick, Ian Reid
- **Comment**: Accepted in AAAI 2018. arXiv admin note: text overlap with
  arXiv:1611.08998
- **Journal**: None
- **Summary**: We present a novel approach for learning to predict sets using deep learning. In recent years, deep neural networks have shown remarkable results in computer vision, natural language processing and other related problems. Despite their success, traditional architectures suffer from a serious limitation in that they are built to deal with structured input and output data, i.e. vectors or matrices. Many real-world problems, however, are naturally described as sets, rather than vectors. Existing techniques that allow for sequential data, such as recurrent neural networks, typically heavily depend on the input and output order and do not guarantee a valid solution. Here, we derive in a principled way, a mathematical formulation for set prediction where the output is permutation invariant. In particular, our approach jointly learns both the cardinality and the state distribution of the target set. We demonstrate the validity of our method on the task of multi-label image classification and achieve a new state of the art on the PASCAL VOC and MS COCO datasets.



### AJILE Movement Prediction: Multimodal Deep Learning for Natural Human Neural Recordings and Video
- **Arxiv ID**: http://arxiv.org/abs/1709.05939v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1709.05939v2)
- **Published**: 2017-09-13 01:28:44+00:00
- **Updated**: 2018-03-01 22:40:58+00:00
- **Authors**: Nancy Xin Ru Wang, Ali Farhadi, Rajesh Rao, Bingni Brunton
- **Comment**: None
- **Journal**: Thirty-Second AAAI Conference On Artificial Intelligence (2018)
- **Summary**: Developing useful interfaces between brains and machines is a grand challenge of neuroengineering. An effective interface has the capacity to not only interpret neural signals, but predict the intentions of the human to perform an action in the near future; prediction is made even more challenging outside well-controlled laboratory experiments. This paper describes our approach to detect and to predict natural human arm movements in the future, a key challenge in brain computer interfacing that has never before been attempted. We introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset; AJILE includes automatically annotated poses of 7 upper body joints for four human subjects over 670 total hours (more than 72 million frames), along with the corresponding simultaneously acquired intracranial neural recordings. The size and scope of AJILE greatly exceeds all previous datasets with movements and electrocorticography (ECoG), making it possible to take a deep learning approach to movement prediction. We propose a multimodal model that combines deep convolutional neural networks (CNN) with long short-term memory (LSTM) blocks, leveraging both ECoG and video modalities. We demonstrate that our models are able to detect movements and predict future movements up to 800 msec before movement initiation. Further, our multimodal movement prediction models exhibit resilience to simulated ablation of input neural signals. We believe a multimodal approach to natural neural decoding that takes context into account is critical in advancing bioelectronic technologies and human neuroscience.



### Co-training for Demographic Classification Using Deep Learning from Label Proportions
- **Arxiv ID**: http://arxiv.org/abs/1709.04108v1
- **DOI**: 10.1109/ICDMW.2017.144
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.04108v1)
- **Published**: 2017-09-13 02:06:19+00:00
- **Updated**: 2017-09-13 02:06:19+00:00
- **Authors**: Ehsan Mohammady Ardehaly, Aron Culotta
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms have recently produced state-of-the-art accuracy in many classification tasks, but this success is typically dependent on access to many annotated training examples. For domains without such data, an attractive alternative is to train models with light, or distant supervision. In this paper, we introduce a deep neural network for the Learning from Label Proportion (LLP) setting, in which the training data consist of bags of unlabeled instances with associated label distributions for each bag. We introduce a new regularization layer, Batch Averager, that can be appended to the last layer of any deep neural network to convert it from supervised learning to LLP. This layer can be implemented readily with existing deep learning packages. To further support domains in which the data consist of two conditionally independent feature views (e.g. image and text), we propose a co-training algorithm that iteratively generates pseudo bags and refits the deep LLP model to improve classification accuracy. We demonstrate our models on demographic attribute classification (gender and race/ethnicity), which has many applications in social media analysis, public health, and marketing. We conduct experiments to predict demographics of Twitter users based on their tweets and profile image, without requiring any user-level annotations for training. We find that the deep LLP approach outperforms baselines for both text and image features separately. Additionally, we find that co-training algorithm improves image and text classification by 4% and 8% absolute F1, respectively. Finally, an ensemble of text and image classifiers further improves the absolute F1 measure by 4% on average.



### Meta Networks for Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1709.04111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04111v1)
- **Published**: 2017-09-13 02:18:39+00:00
- **Updated**: 2017-09-13 02:18:39+00:00
- **Authors**: Falong Shen, Shuicheng Yan, Gang Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a new method to get the specified network parameters through one time feed-forward propagation of the meta networks and explore the application to neural style transfer. Recent works on style transfer typically need to train image transformation networks for every new style, and the style is encoded in the network parameters by enormous iterations of stochastic gradient descent. To tackle these issues, we build a meta network which takes in the style image and produces a corresponding image transformations network directly. Compared with optimization-based methods for every style, our meta networks can handle an arbitrary new style within $19ms$ seconds on one modern GPU card. The fast image transformation network generated by our meta network is only 449KB, which is capable of real-time executing on a mobile device. We also investigate the manifold of the style transfer networks by operating the hidden features from meta networks. Experiments have well validated the effectiveness of our method. Code and trained models has been released https://github.com/FalongShen/styletransfer.



### Sketch-pix2seq: a Model to Generate Sketches of Multiple Categories
- **Arxiv ID**: http://arxiv.org/abs/1709.04121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04121v1)
- **Published**: 2017-09-13 03:22:27+00:00
- **Updated**: 2017-09-13 03:22:27+00:00
- **Authors**: Yajing Chen, Shikui Tu, Yuqi Yi, Lei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Sketch is an important media for human to communicate ideas, which reflects the superiority of human intelligence. Studies on sketch can be roughly summarized into recognition and generation. Existing models on image recognition failed to obtain satisfying performance on sketch classification. But for sketch generation, a recent study proposed a sequence-to-sequence variational-auto-encoder (VAE) model called sketch-rnn which was able to generate sketches based on human inputs. The model achieved amazing results when asked to learn one category of object, such as an animal or a vehicle. However, the performance dropped when multiple categories were fed into the model. Here, we proposed a model called sketch-pix2seq which could learn and draw multiple categories of sketches. Two modifications were made to improve the sketch-rnn model: one is to replace the bidirectional recurrent neural network (BRNN) encoder with a convolutional neural network(CNN); the other is to remove the Kullback-Leibler divergence from the objective function of VAE. Experimental results showed that models with CNN encoders outperformed those with RNN encoders in generating human-style sketches. Visualization of the latent space illustrated that the removal of KL-divergence made the encoder learn a posterior of latent space that reflected the features of different categories. Moreover, the combination of CNN encoder and removal of KL-divergence, i.e., the sketch-pix2seq model, had better performance in learning and generating sketches of multiple categories and showed promising results in creativity tasks.



### A New Multifocus Image Fusion Method Using Contourlet Transform
- **Arxiv ID**: http://arxiv.org/abs/1709.09528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.09528v1)
- **Published**: 2017-09-13 11:51:41+00:00
- **Updated**: 2017-09-13 11:51:41+00:00
- **Authors**: Fatemeh Vakili Moghadam, Hamid Reza Shahdoosti
- **Comment**: 8 pages, 7 figures, conference. in Persian
- **Journal**: None
- **Summary**: A new multifocus image fusion approach is presented in this paper. First the contourlet transform is used to decompose the source images into different components. Then, some salient features are extracted from components. In order to extract salient features, spatial frequency is used. Subsequently, the best coefficients from the components are selected by the maximum selection rule. Finally, the inverse contourlet transform is applied to the selected coefficients. Experiments show the superiority of the proposed method.



### A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1709.05945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.05945v1)
- **Published**: 2017-09-13 12:36:33+00:00
- **Updated**: 2017-09-13 12:36:33+00:00
- **Authors**: Bartolomeo Della Corte, Igor Bogoslavskyi, Cyrill Stachniss, Giorgio Grisetti
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The ability to build maps is a key functionality for the majority of mobile robots. A central ingredient to most mapping systems is the registration or alignment of the recorded sensor data. In this paper, we present a general methodology for photometric registration that can deal with multiple different cues. We provide examples for registering RGBD as well as 3D LIDAR data. In contrast to popular point cloud registration approaches such as ICP our method does not rely on explicit data association and exploits multiple modalities such as raw range and image data streams. Color, depth, and normal information are handled in an uniform manner and the registration is obtained by minimizing the pixel-wise difference between two multi-channel images. We developed a flexible and general framework and implemented our approach inside that framework. We also released our implementation as open source C++ code. The experiments show that our approach allows for an accurate registration of the sensor data without requiring an explicit data association or model-specific adaptations to datasets or sensors. Our approach exploits the different cues in a natural and consistent way and the registration can be done at framerate for a typical range or imaging sensor.



### Densely tracking sequences of 3D face scans
- **Arxiv ID**: http://arxiv.org/abs/1709.04295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04295v1)
- **Published**: 2017-09-13 12:49:30+00:00
- **Updated**: 2017-09-13 12:49:30+00:00
- **Authors**: Huaxiong Ding, Liming Chen
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: 3D face dense tracking aims to find dense inter-frame correspondences in a sequence of 3D face scans and constitutes a powerful tool for many face analysis tasks, e.g., 3D dynamic facial expression analysis. The majority of the existing methods just fit a 3D face surface or model to a 3D target surface without considering temporal information between frames. In this paper, we propose a novel method for densely tracking sequences of 3D face scans, which ex- tends the non-rigid ICP algorithm by adding a novel specific criterion for temporal information. A novel fitting framework is presented for automatically tracking a full sequence of 3D face scans. The results of experiments carried out on the BU4D-FE database are promising, showing that the proposed algorithm outperforms state-of-the-art algorithms for 3D face dense tracking.



### A Survey of Calibration Methods for Optical See-Through Head-Mounted Displays
- **Arxiv ID**: http://arxiv.org/abs/1709.04299v1
- **DOI**: 10.1109/TVCG.2017.2754257
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.04299v1)
- **Published**: 2017-09-13 12:55:45+00:00
- **Updated**: 2017-09-13 12:55:45+00:00
- **Authors**: Jens Grubert, Yuta Itoh, Kenneth Moser, J. Edward Swan II
- **Comment**: None
- **Journal**: None
- **Summary**: Optical see-through head-mounted displays (OST HMDs) are a major output medium for Augmented Reality, which have seen significant growth in popularity and usage among the general public due to the growing release of consumer-oriented models, such as the Microsoft Hololens. Unlike Virtual Reality headsets, OST HMDs inherently support the addition of computer-generated graphics directly into the light path between a user's eyes and their view of the physical world. As with most Augmented and Virtual Reality systems, the physical position of an OST HMD is typically determined by an external or embedded 6-Degree-of-Freedom tracking system. However, in order to properly render virtual objects, which are perceived as spatially aligned with the physical environment, it is also necessary to accurately measure the position of the user's eyes within the tracking system's coordinate frame. For over 20 years, researchers have proposed various calibration methods to determine this needed eye position. However, to date, there has not been a comprehensive overview of these procedures and their requirements. Hence, this paper surveys the field of calibration methods for OST HMDs. Specifically, it provides insights into the fundamentals of calibration techniques, and presents an overview of both manual and automatic approaches, as well as evaluation methods and metrics. Finally, it also identifies opportunities for future research. % relative to the tracking coordinate system, and, hence, its position in 3D space.



### Reading Scene Text with Attention Convolutional Sequence Modeling
- **Arxiv ID**: http://arxiv.org/abs/1709.04303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04303v1)
- **Published**: 2017-09-13 12:57:47+00:00
- **Updated**: 2017-09-13 12:57:47+00:00
- **Authors**: Yunze Gao, Yingying Chen, Jinqiao Wang, Hanqing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Reading text in the wild is a challenging task in the field of computer vision. Existing approaches mainly adopted Connectionist Temporal Classification (CTC) or Attention models based on Recurrent Neural Network (RNN), which is computationally expensive and hard to train. In this paper, we present an end-to-end Attention Convolutional Network for scene text recognition. Firstly, instead of RNN, we adopt the stacked convolutional layers to effectively capture the contextual dependencies of the input sequence, which is characterized by lower computational complexity and easier parallel computation. Compared to the chain structure of recurrent networks, the Convolutional Neural Network (CNN) provides a natural way to capture long-term dependencies between elements, which is 9 times faster than Bidirectional Long Short-Term Memory (BLSTM). Furthermore, in order to enhance the representation of foreground text and suppress the background noise, we incorporate the residual attention modules into a small densely connected network to improve the discriminability of CNN features. We validate the performance of our approach on the standard benchmarks, including the Street View Text, IIIT5K and ICDAR datasets. As a result, state-of-the-art or highly-competitive performance and efficiency show the superiority of the proposed approach.



### GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1709.04329v1
- **DOI**: 10.1145/3123266.3123279
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04329v1)
- **Published**: 2017-09-13 13:44:46+00:00
- **Updated**: 2017-09-13 13:44:46+00:00
- **Authors**: Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, Qi Tian
- **Comment**: Accepted by ACM MM2017, 9 pages, 5 figures
- **Journal**: None
- **Summary**: The huge variance of human pose and the misalignment of detected human images significantly increase the difficulty of person Re-Identification (Re-ID). Moreover, efficient Re-ID systems are required to cope with the massive visual data being produced by video surveillance systems. Targeting to solve these problems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an efficient indexing and retrieval framework, respectively. GLAD explicitly leverages the local and global cues in human body to generate a discriminative and robust representation. It consists of part extraction and descriptor learning modules, where several part regions are first detected and then deep neural networks are designed for representation learning on both the local and global regions. A hierarchical indexing and retrieval framework is designed to eliminate the huge redundancy in the gallery set, and accelerate the online Re-ID procedure. Extensive experimental results show GLAD achieves competitive accuracy compared to the state-of-the-art methods. Our retrieval framework significantly accelerates the online Re-ID procedure without loss of accuracy. Therefore, this work has potential to work better on person Re-ID tasks in real scenarios.



### Flexible Network Binarization with Layer-wise Priority
- **Arxiv ID**: http://arxiv.org/abs/1709.04344v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04344v3)
- **Published**: 2017-09-13 14:14:15+00:00
- **Updated**: 2018-02-16 06:14:44+00:00
- **Authors**: Lixue Zhuang, Yi Xu, Bingbing Ni, Hongteng Xu
- **Comment**: More experiments on image classification are planned
- **Journal**: None
- **Summary**: How to effectively approximate real-valued parameters with binary codes plays a central role in neural network binarization. In this work, we reveal an important fact that binarizing different layers has a widely-varied effect on the compression ratio of network and the loss of performance. Based on this fact, we propose a novel and flexible neural network binarization method by introducing the concept of layer-wise priority which binarizes parameters in inverse order of their layer depth. In each training step, our method selects a specific network layer, minimizes the discrepancy between the original real-valued weights and its binary approximations, and fine-tunes the whole network accordingly. During the iteration of the above process, it is significant that we can flexibly decide whether to binarize the remaining floating layers or not and explore a trade-off between the loss of performance and the compression ratio of model. The resulting binary network is applied for efficient pedestrian detection. Extensive experimental results on several benchmarks show that under the same compression ratio, our method achieves much lower miss rate and faster detection speed than the state-of-the-art neural network binarization method.



### Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.04347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04347v2)
- **Published**: 2017-09-13 14:18:45+00:00
- **Updated**: 2018-06-08 09:59:28+00:00
- **Authors**: Hongyang Li, Yu Liu, Wanli Ouyang, Xiaogang Wang
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: In this paper, we propose a zoom-out-and-in network for generating object proposals. A key observation is that it is difficult to classify anchors of different sizes with the same set of features. Anchors of different sizes should be placed accordingly based on different depth within a network: smaller boxes on high-resolution layers with a smaller stride while larger boxes on low-resolution counterparts with a larger stride. Inspired by the conv/deconv structure, we fully leverage the low-level local details and high-level regional semantics from two feature map streams, which are complimentary to each other, to identify the objectness in an image. A map attention decision (MAD) unit is further proposed to aggressively search for neuron activations among two streams and attend the most contributive ones on the feature learning of the final loss. The unit serves as a decisionmaker to adaptively activate maps along certain channels with the solely purpose of optimizing the overall training loss. One advantage of MAD is that the learned weights enforced on each feature channel is predicted on-the-fly based on the input context, which is more suitable than the fixed enforcement of a convolutional kernel. Experimental results on three datasets, including PASCAL VOC 2007, ImageNet DET, MS COCO, demonstrate the effectiveness of our proposed algorithm over other state-of-the-arts, in terms of average recall (AR) for region proposal and average precision (AP) for object detection.



### An Efficient Evolutionary Based Method For Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.04393v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04393v2)
- **Published**: 2017-09-13 16:00:01+00:00
- **Updated**: 2017-12-06 18:06:26+00:00
- **Authors**: Roohollah Aslanzadeh, Kazem Qazanfari, Mohammad Rahmati
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: The goal of this paper is to present a new efficient image segmentation method based on evolutionary computation which is a model inspired from human behavior. Based on this model, a four layer process for image segmentation is proposed using the split/merge approach. In the first layer, an image is split into numerous regions using the watershed algorithm. In the second layer, a co-evolutionary process is applied to form centers of finals segments by merging similar primary regions. In the third layer, a meta-heuristic process uses two operators to connect the residual regions to their corresponding determined centers. In the final layer, an evolutionary algorithm is used to combine the resulted similar and neighbor regions. Different layers of the algorithm are totally independent, therefore for certain applications a specific layer can be changed without constraint of changing other layers. Some properties of this algorithm like the flexibility of its method, the ability to use different feature vectors for segmentation (grayscale, color, texture, etc), the ability to control uniformity and the number of final segments using free parameters and also maintaining small regions, makes it possible to apply the algorithm to different applications. Moreover, the independence of each region from other regions in the second layer, and the independence of centers in the third layer, makes parallel implementation possible. As a result the algorithm speed will increase. The presented algorithm was tested on a standard dataset (BSDS 300) of images, and the region boundaries were compared with different people segmentation contours. Results show the efficiency of the algorithm and its improvement to similar methods. As an instance, in 70% of tested images, results are better than ACT algorithm, besides in 100% of tested images, we had better results in comparison with VSP algorithm.



### A Tutorial on Deep Learning for Music Information Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1709.04396v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1709.04396v2)
- **Published**: 2017-09-13 16:05:51+00:00
- **Updated**: 2018-05-03 06:29:55+00:00
- **Authors**: Keunwoo Choi, Gy√∂rgy Fazekas, Kyunghyun Cho, Mark Sandler
- **Comment**: None
- **Journal**: None
- **Summary**: Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight. The goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for MIR. We lay out the basic principles and review prominent works in this hard to navigate the field. We then outline the network structures that have been successful in MIR problems and facilitate the selection of building blocks for the problems at hand. Finally, guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field.



### Exploiting skeletal structure in computer vision annotation with Benders decomposition
- **Arxiv ID**: http://arxiv.org/abs/1709.04411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04411v1)
- **Published**: 2017-09-13 16:43:07+00:00
- **Updated**: 2017-09-13 16:43:07+00:00
- **Authors**: Shaofei Wang, Konrad Kording, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: Many annotation problems in computer vision can be phrased as integer linear programs (ILPs). The use of standard industrial solvers does not to exploit the underlying structure of such problems eg, the skeleton in pose estimation. The leveraging of the underlying structure in conjunction with industrial solvers promises increases in both speed and accuracy. Such structure can be exploited using Bender's decomposition, a technique from operations research, that solves complex ILPs or mixed integer linear programs by decomposing them into sub-problems that communicate via a master problem. The intuition is that conditioned on a small subset of the variables the solution to the remaining variables can be computed easily by taking advantage of properties of the ILP constraint matrix such as block structure. In this paper we apply Benders decomposition to a typical problem in computer vision where we have many sub-ILPs (eg, partitioning of detections, body-parts) coupled to a master ILP (eg, constructing skeletons). Dividing inference problems into a master problem and sub-problems motivates the development of a plethora of novel models, and inference approaches for the field of computer vision.



### Contrast Enhancement of Brightness-Distorted Images by Improved Adaptive Gamma Correction
- **Arxiv ID**: http://arxiv.org/abs/1709.04427v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.04427v2)
- **Published**: 2017-09-13 17:17:01+00:00
- **Updated**: 2022-08-30 03:13:50+00:00
- **Authors**: Gang Cao, Lihui Huang, Huawei Tian, Xianglin Huang, Yongbin Wang, Ruicong Zhi
- **Comment**: None
- **Journal**: None
- **Summary**: As an efficient image contrast enhancement (CE) tool, adaptive gamma correction (AGC) was previously proposed by relating gamma parameter with cumulative distribution function (CDF) of the pixel gray levels within an image. ACG deals well with most dimmed images, but fails for globally bright images and the dimmed images with local bright regions. Such two categories of brightness-distorted images are universal in real scenarios, such as improper exposure and white object regions. In order to attenuate such deficiencies, here we propose an improved AGC algorithm. The novel strategy of negative images is used to realize CE of the bright images, and the gamma correction modulated by truncated CDF is employed to enhance the dimmed ones. As such, local over-enhancement and structure distortion can be alleviated. Both qualitative and quantitative experimental results show that our proposed method yields consistently good CE results.



### A Learning and Masking Approach to Secure Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.04447v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.04447v4)
- **Published**: 2017-09-13 17:44:35+00:00
- **Updated**: 2018-01-27 04:08:56+00:00
- **Authors**: Linh Nguyen, Sky Wang, Arunesh Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been shown to be vulnerable against adversarial examples, which are data points cleverly constructed to fool the classifier. Such attacks can be devastating in practice, especially as DNNs are being applied to ever increasing critical tasks like image recognition in autonomous driving. In this paper, we introduce a new perspective on the problem. We do so by first defining robustness of a classifier to adversarial exploitation. Next, we show that the problem of adversarial example generation can be posed as learning problem. We also categorize attacks in literature into high and low perturbation attacks; well-known attacks like fast-gradient sign method (FGSM) and our attack produce higher perturbation adversarial examples while the more potent but computationally inefficient Carlini-Wagner (CW) attack is low perturbation. Next, we show that the dual approach of the attack learning problem can be used as a defensive technique that is effective against high perturbation attacks. Finally, we show that a classifier masking method achieved by adding noise to the a neural network's logit output protects against low distortion attacks such as the CW attack. We also show that both our learning and masking defense can work simultaneously to protect against multiple attacks. We demonstrate the efficacy of our techniques by experimenting with the MNIST and CIFAR-10 datasets.



### An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.04496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04496v2)
- **Published**: 2017-09-13 18:36:48+00:00
- **Updated**: 2017-10-10 08:09:31+00:00
- **Authors**: Christian F. Baumgartner, Lisa M. Koch, Marc Pollefeys, Ender Konukoglu
- **Comment**: to appear in STACOM 2017 proceedings
- **Journal**: None
- **Summary**: Accurate segmentation of the heart is an important step towards evaluating cardiac function. In this paper, we present a fully automated framework for segmentation of the left (LV) and right (RV) ventricular cavities and the myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and 3D convolutional neural network architectures for this task. We investigate the suitability of various state-of-the art 2D and 3D convolutional neural network architectures, as well as slight modifications thereof, for this task. Experiments were performed on the ACDC 2017 challenge training dataset comprising cardiac MR images of 100 patients, where manual reference segmentations were made available for end-diastolic (ED) and end-systolic (ES) frames. We find that processing the images in a slice-by-slice fashion using 2D networks is beneficial due to a relatively large slice thickness. However, the exact network architecture only plays a minor role. We report mean Dice coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively with an average evaluation time of 1.1 seconds per volume on a modern GPU.



### Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.04518v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.04518v4)
- **Published**: 2017-09-13 19:54:56+00:00
- **Updated**: 2018-04-08 01:14:34+00:00
- **Authors**: Qihang Yu, Lingxi Xie, Yan Wang, Yuyin Zhou, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Accepted to CVPR 2018 (10 pages, 6 figures)
- **Journal**: None
- **Summary**: We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage.   This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice.



### A Computational Model of Afterimages based on Simultaneous and Successive Contrasts
- **Arxiv ID**: http://arxiv.org/abs/1709.04550v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1709.04550v1)
- **Published**: 2017-09-13 21:54:58+00:00
- **Updated**: 2017-09-13 21:54:58+00:00
- **Authors**: Jinhui Yu, Kailin Wu, Kang Zhang, Xianjun Sam Zheng
- **Comment**: 10 pages, 6 figues
- **Journal**: None
- **Summary**: Negative afterimage appears in our vision when we shift our gaze from an over stimulated original image to a new area with a uniform color. The colors of negative afterimages differ from the old stimulating colors in the original image when the color in the new area is either neutral or chromatic. The interaction between stimulating colors in the test and inducing field in the original image changes our color perception due to simultaneous contrast, and the interaction between changed colors perceived in the previously-viewed field and the color in the currently-viewed field also affects our perception of colors in negative afterimages due to successive contrast. Based on these observations we propose a computational model to estimate colors of negative afterimages in more general cases where the original stimulating color in the test field is chromatic, and the original stimulating color in the inducing field and the new stimulating color can be either neutral or chromatic. We validate our model with human experiments.




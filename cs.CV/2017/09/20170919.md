# Arxiv Papers in cs.CV on 2017-09-19
### Reducing Complexity of HEVC: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1710.01218v3
- **DOI**: 10.1109/TIP.2018.2847035
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01218v3)
- **Published**: 2017-09-19 02:02:00+00:00
- **Updated**: 2018-03-22 11:13:05+00:00
- **Authors**: Mai Xu, Tianyi Li, Zulin Wang, Xin Deng, Ren Yang, Zhenyu Guan
- **Comment**: 17 pages, with 12 figures and 7 tables
- **Journal**: Published in IEEE Transactions on Image Processing, Oct. 2018
- **Summary**: High Efficiency Video Coding (HEVC) significantly reduces bit-rates over the proceeding H.264 standard but at the expense of extremely high encoding complexity. In HEVC, the quad-tree partition of coding unit (CU) consumes a large proportion of the HEVC encoding complexity, due to the bruteforce search for rate-distortion optimization (RDO). Therefore, this paper proposes a deep learning approach to predict the CU partition for reducing the HEVC complexity at both intra- and inter-modes, which is based on convolutional neural network (CNN) and long- and short-term memory (LSTM) network. First, we establish a large-scale database including substantial CU partition data for HEVC intra- and inter-modes. This enables deep learning on the CU partition. Second, we represent the CU partition of an entire coding tree unit (CTU) in the form of a hierarchical CU partition map (HCPM). Then, we propose an early-terminated hierarchical CNN (ETH-CNN) for learning to predict the HCPM. Consequently, the encoding complexity of intra-mode HEVC can be drastically reduced by replacing the brute-force search with ETH-CNN to decide the CU partition. Third, an early-terminated hierarchical LSTM (ETH-LSTM) is proposed to learn the temporal correlation of the CU partition. Then, we combine ETH-LSTM and ETH-CNN to predict the CU partition for reducing the HEVC complexity for inter-mode. Finally, experimental results show that our approach outperforms other state-of-the-art approaches in reducing the HEVC complexity at both intra- and inter-modes.



### CISRDCNN: Super-resolution of compressed images using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1709.06229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06229v1)
- **Published**: 2017-09-19 02:45:12+00:00
- **Updated**: 2017-09-19 02:45:12+00:00
- **Authors**: Honggang Chen, Xiaohai He, Chao Ren, Linbo Qing, Qizhi Teng
- **Comment**: 32 pages, 17 figures, 5 tables, preprint submitted to Neurocomputing
- **Journal**: None
- **Summary**: In recent years, much research has been conducted on image super-resolution (SR). To the best of our knowledge, however, few SR methods were concerned with compressed images. The SR of compressed images is a challenging task due to the complicated compression artifacts, while many images suffer from them in practice. The intuitive solution for this difficult task is to decouple it into two sequential but independent subproblems, i.e., compression artifacts reduction (CAR) and SR. Nevertheless, some useful details may be removed in CAR stage, which is contrary to the goal of SR and makes the SR stage more challenging. In this paper, an end-to-end trainable deep convolutional neural network is designed to perform SR on compressed images (CISRDCNN), which reduces compression artifacts and improves image resolution jointly. Experiments on compressed images produced by JPEG (we take the JPEG as an example in this paper) demonstrate that the proposed CISRDCNN yields state-of-the-art SR performance on commonly used test images and imagesets. The results of CISRDCNN on real low quality web images are also very impressive, with obvious quality enhancement. Further, we explore the application of the proposed SR method in low bit-rate image coding, leading to better rate-distortion performance than JPEG.



### Rethink ReLU to Training Better CNNs
- **Arxiv ID**: http://arxiv.org/abs/1709.06247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06247v2)
- **Published**: 2017-09-19 04:27:56+00:00
- **Updated**: 2018-08-31 08:25:33+00:00
- **Authors**: Gangming Zhao, Zhaoxiang Zhang, He Guan, Peng Tang, Jingdong Wang
- **Comment**: 8 pages,10 figures, conference
- **Journal**: None
- **Summary**: Most of convolutional neural networks share the same characteristic: each convolutional layer is followed by a nonlinear activation layer where Rectified Linear Unit (ReLU) is the most widely used. In this paper, we argue that the designed structure with the equal ratio between these two layers may not be the best choice since it could result in the poor generalization ability. Thus, we try to investigate a more suitable method on using ReLU to explore the better network architectures. Specifically, we propose a proportional module to keep the ratio between convolution and ReLU amount to be N:M (N>M). The proportional module can be applied in almost all networks with no extra computational cost to improve the performance. Comprehensive experimental results indicate that the proposed method achieves better performance on different benchmarks with different network architectures, thus verify the superiority of our work.



### Look Wider to Match Image Patches with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.06248v1
- **DOI**: 10.1109/LSP.2016.2637355
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06248v1)
- **Published**: 2017-09-19 04:31:43+00:00
- **Updated**: 2017-09-19 04:31:43+00:00
- **Authors**: Haesol Park, Kyoung Mu Lee
- **Comment**: published in SPL
- **Journal**: H. Park and K. M. Lee, "Look Wider to Match Image Patches with
  Convolutional Neural Networks," in IEEE Signal Processing Letters, vol. PP,
  no. 99, pp. 1-1, 2016
- **Summary**: When a human matches two images, the viewer has a natural tendency to view the wide area around the target pixel to obtain clues of right correspondence. However, designing a matching cost function that works on a large window in the same way is difficult. The cost function is typically not intelligent enough to discard the information irrelevant to the target pixel, resulting in undesirable artifacts. In this paper, we propose a novel learn a stereo matching cost with a large-sized window. Unlike conventional pooling layers with strides, the proposed per-pixel pyramid-pooling layer can cover a large area without a loss of resolution and detail. Therefore, the learned matching cost function can successfully utilize the information from a large area without introducing the fattening effect. The proposed method is robust despite the presence of weak textures, depth discontinuity, illumination, and exposure difference. The proposed method achieves near-peak performance on the Middlebury benchmark.



### Deep-Learnt Classification of Light Curves
- **Arxiv ID**: http://arxiv.org/abs/1709.06257v1
- **DOI**: 10.1109/SSCI.2017.8280984
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.06257v1)
- **Published**: 2017-09-19 05:00:58+00:00
- **Updated**: 2017-09-19 05:00:58+00:00
- **Authors**: Ashish Mahabal, Kshiteej Sheth, Fabian Gieseke, Akshay Pai, S. George Djorgovski, Andrew Drake, Matthew Graham, the CSS/CRTS/PTF Collaboration
- **Comment**: 8 pages, 9 figures, 6 tables, 2 listings. Accepted to 2017 IEEE
  Symposium Series on Computational Intelligence (SSCI)
- **Journal**: 2017 IEEE Symposium Series on Computational Intelligence (SSCI),
  Honolulu, HI, USA, 2017, p2757
- **Summary**: Astronomy light curves are sparse, gappy, and heteroscedastic. As a result standard time series methods regularly used for financial and similar datasets are of little help and astronomers are usually left to their own instruments and techniques to classify light curves. A common approach is to derive statistical features from the time series and to use machine learning methods, generally supervised, to separate objects into a few of the standard classes. In this work, we transform the time series to two-dimensional light curve representations in order to classify them using modern deep learning techniques. In particular, we show that convolutional neural networks based classifiers work well for broad characterization and classification. We use labeled datasets of periodic variables from CRTS survey and show how this opens doors for a quick classification of diverse classes with several possible exciting extensions.



### Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.06262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06262v2)
- **Published**: 2017-09-19 05:50:19+00:00
- **Updated**: 2017-10-10 03:53:33+00:00
- **Authors**: Julian Faraone, Nicholas Fraser, Giulio Gambardella, Michaela Blott, Philip H. W. Leong
- **Comment**: To appear as a conference paper at the 24th International Conference
  On Neural Information Processing (ICONIP 2017)
- **Journal**: None
- **Summary**: A low precision deep neural network training technique for producing sparse, ternary neural networks is presented. The technique incorporates hard- ware implementation costs during training to achieve significant model compression for inference. Training involves three stages: network training using L2 regularization and a quantization threshold regularizer, quantization pruning, and finally retraining. Resulting networks achieve improved accuracy, reduced memory footprint and reduced computational complexity compared with conventional methods, on MNIST and CIFAR10 datasets. Our networks are up to 98% sparse and 5 & 11 times smaller than equivalent binary and ternary models, translating to significant resource and speed benefits for hardware implementations.



### Colour Terms: a Categorisation Model Inspired by Visual Cortex Neurons
- **Arxiv ID**: http://arxiv.org/abs/1709.06300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06300v1)
- **Published**: 2017-09-19 08:53:28+00:00
- **Updated**: 2017-09-19 08:53:28+00:00
- **Authors**: Arash Akbarinia, C. Alejandro Parraga
- **Comment**: None
- **Journal**: None
- **Summary**: Although it seems counter-intuitive, categorical colours do not exist as external physical entities but are very much the product of our brains. Our cortical machinery segments the world and associate objects to specific colour terms, which is not only convenient for communication but also increases the efficiency of visual processing by reducing the dimensionality of input scenes. Although the neural substrate for this phenomenon is unknown, a recent study of cortical colour processing has discovered a set of neurons that are isoresponsive to stimuli in the shape of 3D-ellipsoidal surfaces in colour-opponent space. We hypothesise that these neurons might help explain the underlying mechanisms of colour naming in the visual cortex.   Following this, we propose a biologically-inspired colour naming model where each colour term - e.g. red, green, blue, yellow, etc. - is represented through an ellipsoid in 3D colour-opponent space. This paradigm is also supported by previous psychophysical colour categorisation experiments whose results resemble such shapes. "Belongingness" of each pixel to different colour categories is computed by a non-linear sigmoidal logistic function. The final colour term for a given pixel is calculated by a maximum pooling mechanism. The simplicity of our model allows its parameters to be learnt from a handful of segmented images. It also offers a straightforward extension to include further colour terms. Additionally, ellipsoids of proposed model can adapt to image contents offering a dynamical solution in order to address phenomenon of colour constancy. Our results on the Munsell chart and two datasets of real-world images show an overall improvement comparing to state-of-the-art algorithms.



### Exploring Human-like Attention Supervision in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1709.06308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06308v1)
- **Published**: 2017-09-19 09:19:08+00:00
- **Updated**: 2017-09-19 09:19:08+00:00
- **Authors**: Tingting Qiao, Jianfeng Dong, Duanqing Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanisms have been widely applied in the Visual Question Answering (VQA) task, as they help to focus on the area-of-interest of both visual and textual information. To answer the questions correctly, the model needs to selectively target different areas of an image, which suggests that an attention-based model may benefit from an explicit attention supervision. In this work, we aim to address the problem of adding attention supervision to VQA models. Since there is a lack of human attention data, we first propose a Human Attention Network (HAN) to generate human-like attention maps, training on a recently released dataset called Human ATtention Dataset (VQA-HAT). Then, we apply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the human-like attention maps for all image-question pairs. The generated human-like attention map dataset for the VQA v2.0 dataset is named as Human-Like ATtention (HLAT) dataset. Finally, we apply human-like attention supervision to an attention-based VQA model. The experiments show that adding human-like supervision yields a more accurate attention together with a better performance, showing a promising future for human-like attention supervision in VQA.



### Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1709.06310v4
- **DOI**: 10.1109/LRA.2018.2793357
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.06310v4)
- **Published**: 2017-09-19 09:26:05+00:00
- **Updated**: 2018-01-22 15:31:17+00:00
- **Authors**: Antoni Rosinol Vidal, Henri Rebecq, Timo Horstschaefer, Davide Scaramuzza
- **Comment**: 8 pages, 9 figures, 2 tables
- **Journal**: Robot.Autom.Lett. 3 (2018) 994-1001
- **Summary**: Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate - to the best of our knowledge - the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.



### Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM
- **Arxiv ID**: http://arxiv.org/abs/1709.06316v3
- **DOI**: 10.1007/978-3-030-01264-9_37
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06316v3)
- **Published**: 2017-09-19 09:45:03+00:00
- **Updated**: 2019-01-14 18:11:58+00:00
- **Authors**: Lai Jiang, Mai Xu, Zulin Wang
- **Comment**: Jiang, Lai and Xu, Mai and Liu, Tie and Qiao, Minglang and Wang,
  Zulin; DeepVS: A Deep Learning Based Video Saliency Prediction Approach;The
  European Conference on Computer Vision (ECCV); September 2018
- **Journal**: None
- **Summary**: Over the past few years, deep neural networks (DNNs) have exhibited great success in predicting the saliency of images. However, there are few works that apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of our LEDOV database, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. Therefore, we develop a two-layer convolutional long short-term memory (2C-LSTM) network in our DNN-based method, using the extracted features of OM-CNN as the input. Consequently, the inter-frame saliency maps of videos can be generated, which consider the transition of attention across video frames. Finally, the experimental results show that our method advances the state-of-the-art in video saliency prediction.



### On the Generalized Essential Matrix Correction: An efficient solution to the problem and its applications
- **Arxiv ID**: http://arxiv.org/abs/1709.06328v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06328v3)
- **Published**: 2017-09-19 10:10:09+00:00
- **Updated**: 2020-03-16 11:39:40+00:00
- **Authors**: Pedro Miraldo, Joao R. Cardoso
- **Comment**: 14 pages, 7 figures, journal
- **Journal**: Journal of Mathematical Imaging and Vision, 2020
- **Summary**: This paper addresses the problem of finding the closest generalized essential matrix from a given $6\times 6$ matrix, with respect to the Frobenius norm. To the best of our knowledge, this nonlinear constrained optimization problem has not been addressed in the literature yet. Although it can be solved directly, it involves a large number of constraints, and any optimization method to solve it would require much computational effort. We start by deriving a couple of unconstrained formulations of the problem. After that, we convert the original problem into a new one, involving only orthogonal constraints, and propose an efficient algorithm of steepest descent-type to find its solution. To test the algorithms, we evaluate the methods with synthetic data and conclude that the proposed steepest descent-type approach is much faster than the direct application of general optimization techniques to the original formulation with 33 constraints and to the unconstrained ones. To further motivate the relevance of our method, we apply it in two pose problems (relative and absolute) using synthetic and real data.



### 3D Reconstruction in Canonical Co-ordinate Space from Arbitrarily Oriented 2D Images
- **Arxiv ID**: http://arxiv.org/abs/1709.06341v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06341v4)
- **Published**: 2017-09-19 10:50:20+00:00
- **Updated**: 2018-01-23 18:21:29+00:00
- **Authors**: Benjamin Hou, Bishesh Khanal, Amir Alansary, Steven McDonagh, Alice Davidson, Mary Rutherford, Jo V. Hajnal, Daniel Rueckert, Ben Glocker, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Limited capture range, and the requirement to provide high quality initialization for optimization-based 2D/3D image registration methods, can significantly degrade the performance of 3D image reconstruction and motion compensation pipelines. Challenging clinical imaging scenarios, which contain significant subject motion such as fetal in-utero imaging, complicate the 3D image and volume reconstruction process. In this paper we present a learning based image registration method capable of predicting 3D rigid transformations of arbitrarily oriented 2D image slices, with respect to a learned canonical atlas co-ordinate system. Only image slice intensity information is used to perform registration and canonical alignment, no spatial transform initialization is required. To find image transformations we utilize a Convolutional Neural Network (CNN) architecture to learn the regression function capable of mapping 2D image slices to a 3D canonical atlas space. We extensively evaluate the effectiveness of our approach quantitatively on simulated Magnetic Resonance Imaging (MRI), fetal brain imagery with synthetic motion and further demonstrate qualitative results on real fetal MRI data where our method is integrated into a full reconstruction and motion compensation pipeline. Our learning based registration achieves an average spatial prediction error of 7 mm on simulated data and produces qualitatively improved reconstructions for heavily moving fetuses with gestational ages of approximately 20 weeks. Our model provides a general and computationally efficient solution to the 2D/3D registration initialization problem and is suitable for real-time scenarios.



### APPD: Adaptive and Precise Pupil Boundary Detection using Entropy of Contour Gradients
- **Arxiv ID**: http://arxiv.org/abs/1709.06366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06366v2)
- **Published**: 2017-09-19 12:09:34+00:00
- **Updated**: 2018-08-09 15:26:35+00:00
- **Authors**: Cihan Topal, Halil Ibrahim Cakir, Cuneyt Akinlar
- **Comment**: Demo video: https://www.youtube.com/watch?v=cPLmmVLGrdQ&t=5s
- **Journal**: None
- **Summary**: Eye tracking spreads through a vast area of applications from ophthalmology, assistive technologies to gaming and virtual reality. Precisely detecting the pupil's contour and center is the very first step in many of these tasks, hence needs to be performed accurately. Although detection of pupil is a simple problem when it is entirely visible; occlusions and oblique view angles complicate the solution. In this study, we propose APPD, an adaptive and precise pupil boundary detection method that is able to infer whether entire pupil is in clearly visible by a heuristic that estimates the shape of a contour in a computationally efficient way. Thus, a faster detection is performed with the assumption of no occlusions. If the heuristic fails, a more comprehensive search among extracted image features is executed to maintain accuracy. Furthermore, the algorithm can find out if there is no pupil as an helpful information for many applications. We provide a dataset containing 3904 high resolution eye images collected from 12 subjects and perform an extensive set of experiments to obtain quantitative results in terms of accuracy, localization and timing. The proposed method outperforms three other state of the art algorithms and has an average execution time $\sim$5 ms in single-thread on a standard laptop computer for 720p images.



### A General Framework for the Recognition of Online Handwritten Graphics
- **Arxiv ID**: http://arxiv.org/abs/1709.06389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06389v1)
- **Published**: 2017-09-19 13:06:32+00:00
- **Updated**: 2017-09-19 13:06:32+00:00
- **Authors**: Frank Julca-Aguilar, Harold Mouch√®re, Christian Viard-Gaudin, Nina S. T. Hirata
- **Comment**: Submitted to TPAMI
- **Journal**: None
- **Summary**: We propose a new framework for the recognition of online handwritten graphics. Three main features of the framework are its ability to treat symbol and structural level information in an integrated way, its flexibility with respect to different families of graphics, and means to control the tradeoff between recognition effectiveness and computational cost. We model a graphic as a labeled graph generated from a graph grammar. Non-terminal vertices represent subcomponents, terminal vertices represent symbols, and edges represent relations between subcomponents or symbols. We then model the recognition problem as a graph parsing problem: given an input stroke set, we search for a parse tree that represents the best interpretation of the input. Our graph parsing algorithm generates multiple interpretations (consistent with the grammar) and then we extract an optimal interpretation according to a cost function that takes into consideration the likelihood scores of symbols and structures. The parsing algorithm consists in recursively partitioning the stroke set according to structures defined in the grammar and it does not impose constraints present in some previous works (e.g. stroke ordering). By avoiding such constraints and thanks to the powerful representativeness of graphs, our approach can be adapted to the recognition of different graphic notations. We show applications to the recognition of mathematical expressions and flowcharts. Experimentation shows that our method obtains state-of-the-art accuracy in both applications.



### Human Action Forecasting by Learning Task Grammars
- **Arxiv ID**: http://arxiv.org/abs/1709.06391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06391v1)
- **Published**: 2017-09-19 13:12:36+00:00
- **Updated**: 2017-09-19 13:12:36+00:00
- **Authors**: Tengda Han, Jue Wang, Anoop Cherian, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: For effective human-robot interaction, it is important that a robotic assistant can forecast the next action a human will consider in a given task. Unfortunately, real-world tasks are often very long, complex, and repetitive; as a result forecasting is not trivial. In this paper, we propose a novel deep recurrent architecture that takes as input features from a two-stream Residual action recognition framework, and learns to estimate the progress of human activities from video sequences -- this surrogate progress estimation task implicitly learns a temporal task grammar with respect to which activities can be localized and forecasted. To learn the task grammar, we propose a stacked LSTM based multi-granularity progress estimation framework that uses a novel cumulative Euclidean loss as objective. To demonstrate the effectiveness of our proposed architecture, we showcase experiments on two challenging robotic assistive tasks, namely (i) assembling an Ikea table from its constituents, and (ii) changing the tires of a car. Our results demonstrate that learning task grammars offers highly discriminative cues improving the forecasting accuracy by more than 9% over the baseline two-stream forecasting model, while also outperforming other competitive schemes.



### Automatic Leaf Extraction from Outdoor Images
- **Arxiv ID**: http://arxiv.org/abs/1709.06437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06437v1)
- **Published**: 2017-09-19 14:08:56+00:00
- **Updated**: 2017-09-19 14:08:56+00:00
- **Authors**: N. Anantrasirichai, Sion Hannuna, Nishan Canagarajah
- **Comment**: 13 pages, India-UK Advanced Technology Centre of Excellence in Next
  Generation Networks, Systems and Services (IU-ATC), 2010
- **Journal**: None
- **Summary**: Automatic plant recognition and disease analysis may be streamlined by an image of a complete, isolated leaf as an initial input. Segmenting leaves from natural images is a hard problem. Cluttered and complex backgrounds: often composed of other leaves are commonplace. Furthermore, their appearance is highly dependent upon illumination and viewing perspective. In order to address these issues we propose a methodology which exploits the leaves venous systems in tandem with other low level features. Background and leaf markers are created using colour, intensity and texture. Two approaches are investigated: watershed and graph-cut and results compared. Primary-secondary vein detection and a protrusion-notch removal are applied to refine the extracted leaf. The efficacy of our approach is demonstrated against existing work.



### Human Activity Recognition Using Robust Adaptive Privileged Probabilistic Learning
- **Arxiv ID**: http://arxiv.org/abs/1709.06447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06447v1)
- **Published**: 2017-09-19 14:21:28+00:00
- **Updated**: 2017-09-19 14:21:28+00:00
- **Authors**: Michalis Vrigkas, Evangelos Kazakos, Christophoros Nikou, Ioannis A. Kakadiaris
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a novel method based on the learning using privileged information (LUPI) paradigm for recognizing complex human activities is proposed that handles missing information during testing. We present a supervised probabilistic approach that integrates LUPI into a hidden conditional random field (HCRF) model. The proposed model is called HCRF+ and may be trained using both maximum likelihood and maximum margin approaches. It employs a self-training technique for automatic estimation of the regularization parameters of the objective functions. Moreover, the method provides robustness to outliers (such as noise or missing data) by modeling the conditional distribution of the privileged information by a Student's \textit{t}-density function, which is naturally integrated into the HCRF+ framework. Different forms of privileged information were investigated. The proposed method was evaluated using four challenging publicly available datasets and the experimental results demonstrate its effectiveness with respect to the-state-of-the-art in the LUPI framework using both hand-crafted features and features extracted from a convolutional neural network.



### 3D Reconstruction with Low Resolution, Small Baseline and High Radial Distortion Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/1709.06451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06451v1)
- **Published**: 2017-09-19 14:22:22+00:00
- **Updated**: 2017-09-19 14:22:22+00:00
- **Authors**: Tiago Dias, Helder Araujo, Pedro Miraldo
- **Comment**: None
- **Journal**: ACM Int'l Conf. Distributed Smart Cameras (ICDSC), 2016
- **Summary**: In this paper we analyze and compare approaches for 3D reconstruction from low-resolution (250x250), high radial distortion stereo images, which are acquired with small baseline (approximately 1mm). These images are acquired with the system NanEye Stereo manufactured by CMOSIS/AWAIBA. These stereo cameras have also small apertures, which means that high levels of illumination are required. The goal was to develop an approach yielding accurate reconstructions, with a low computational cost, i.e., avoiding non-linear numerical optimization algorithms. In particular we focused on the analysis and comparison of radial distortion models. To perform the analysis and comparison, we defined a baseline method based on available software and methods, such as the Bouguet toolbox [2] or the Computer Vision Toolbox from Matlab. The approaches tested were based on the use of the polynomial model of radial distortion, and on the application of the division model. The issue of the center of distortion was also addressed within the framework of the application of the division model. We concluded that the division model with a single radial distortion parameter has limitations.



### Image operator learning coupled with CNN classification and its application to staff line removal
- **Arxiv ID**: http://arxiv.org/abs/1709.06476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06476v1)
- **Published**: 2017-09-19 15:00:09+00:00
- **Updated**: 2017-09-19 15:00:09+00:00
- **Authors**: Frank D. Julca-Aguilar, Nina S. T. Hirata
- **Comment**: To appear in ICDAR 2017
- **Journal**: None
- **Summary**: Many image transformations can be modeled by image operators that are characterized by pixel-wise local functions defined on a finite support window. In image operator learning, these functions are estimated from training data using machine learning techniques. Input size is usually a critical issue when using learning algorithms, and it limits the size of practicable windows. We propose the use of convolutional neural networks (CNNs) to overcome this limitation. The problem of removing staff-lines in music score images is chosen to evaluate the effects of window and convolutional mask sizes on the learned image operator performance. Results show that the CNN based solution outperforms previous ones obtained using conventional learning algorithms or heuristic algorithms, indicating the potential of CNNs as base classifiers in image operator learning. The implementations will be made available on the TRIOSlib project site.



### Convolutional Long Short-Term Memory Networks for Recognizing First Person Interactions
- **Arxiv ID**: http://arxiv.org/abs/1709.06495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06495v1)
- **Published**: 2017-09-19 15:58:28+00:00
- **Updated**: 2017-09-19 15:58:28+00:00
- **Authors**: Swathikiran Sudhakaran, Oswald Lanz
- **Comment**: Accepted on the second International Workshop on Egocentric
  Perception, Interaction and Computing(EPIC) at International Conference on
  Computer Vision(ICCV-17)
- **Journal**: None
- **Summary**: In this paper, we present a novel deep learning based approach for addressing the problem of interaction recognition from a first person perspective. The proposed approach uses a pair of convolutional neural networks, whose parameters are shared, for extracting frame level features from successive frames of the video. The frame level features are then aggregated using a convolutional long short-term memory. The hidden state of the convolutional long short-term memory, after all the input video frames are processed, is used for classification in to the respective categories. The two branches of the convolutional neural network perform feature encoding on a short time interval whereas the convolutional long short term memory encodes the changes on a longer temporal duration. In our network the spatio-temporal structure of the input is preserved till the very final processing stage. Experimental results show that our method outperforms the state of the art on most recent first person interactions datasets that involve complex ego-motion. In particular, on UTKinect-FirstPerson it competes with methods that use depth image and skeletal joints information along with RGB images, while it surpasses all previous methods that use only RGB images by more than 20% in recognition accuracy.



### SalNet360: Saliency Maps for omni-directional images with CNN
- **Arxiv ID**: http://arxiv.org/abs/1709.06505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06505v2)
- **Published**: 2017-09-19 16:21:09+00:00
- **Updated**: 2018-05-10 09:47:27+00:00
- **Authors**: Rafael Monroy, Sebastian Lutz, Tejo Chalasani, Aljosa Smolic
- **Comment**: None
- **Journal**: None
- **Summary**: The prediction of Visual Attention data from any kind of media is of valuable use to content creators and used to efficiently drive encoding algorithms. With the current trend in the Virtual Reality (VR) field, adapting known techniques to this new kind of media is starting to gain momentum. In this paper, we present an architectural extension to any Convolutional Neural Network (CNN) to fine-tune traditional 2D saliency prediction to Omnidirectional Images (ODIs) in an end-to-end manner. We show that each step in the proposed pipeline works towards making the generated saliency map more accurate with respect to ground truth data.



### Learning to Detect Violent Videos using Convolutional Long Short-Term Memory
- **Arxiv ID**: http://arxiv.org/abs/1709.06531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06531v1)
- **Published**: 2017-09-19 16:58:32+00:00
- **Updated**: 2017-09-19 16:58:32+00:00
- **Authors**: Swathikiran Sudhakaran, Oswald Lanz
- **Comment**: Accepted in International Conference on Advanced Video and Signal
  based Surveillance(AVSS 2017)
- **Journal**: None
- **Summary**: Developing a technique for the automatic analysis of surveillance videos in order to identify the presence of violence is of broad interest. In this work, we propose a deep neural network for the purpose of recognizing violent videos. A convolutional neural network is used to extract frame level features from a video. The frame level features are then aggregated using a variant of the long short term memory that uses convolutional gates. The convolutional neural network along with the convolutional long short term memory is capable of capturing localized spatio-temporal features which enables the analysis of local motion taking place in the video. We also propose to use adjacent frame differences as the input to the model thereby forcing it to encode the changes occurring in the video. The performance of the proposed feature extraction pipeline is evaluated on three standard benchmark datasets in terms of recognition accuracy. Comparison of the results obtained with the state of the art techniques revealed the promising capability of the proposed method in recognizing violent videos.



### When 3D-Aided 2D Face Recognition Meets Deep Learning: An extended UR2D for Pose-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.06532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06532v1)
- **Published**: 2017-09-19 17:02:15+00:00
- **Updated**: 2017-09-19 17:02:15+00:00
- **Authors**: Xiang Xu, Pengfei Dou, Ha A. Le, Ioannis A. Kakadiaris
- **Comment**: Submitted to Special Issue on Biometrics in the Wild, Image and
  Vision Computing
- **Journal**: None
- **Summary**: Most of the face recognition works focus on specific modules or demonstrate a research idea. This paper presents a pose-invariant 3D-aided 2D face recognition system (UR2D) that is robust to pose variations as large as 90? by leveraging deep learning technology. The architecture and the interface of UR2D are described, and each module is introduced in detail. Extensive experiments are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset and 3% on the IJB-A dataset on average in face identification tasks. UR2D also achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing the Rank-1 accuracy score from template matching. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques.



### Curriculum Learning of Visual Attribute Clusters for Multi-Task Classification
- **Arxiv ID**: http://arxiv.org/abs/1709.06664v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06664v3)
- **Published**: 2017-09-19 22:37:42+00:00
- **Updated**: 2018-07-10 01:17:35+00:00
- **Authors**: Nikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou, Ioannis A. Kakadiaris
- **Comment**: Published in Pattern Recognition
- **Journal**: None
- **Summary**: Visual attributes, from simple objects (e.g., backpacks, hats) to soft-biometrics (e.g., gender, height, clothing) have proven to be a powerful representational approach for many applications such as image description and human identification. In this paper, we introduce a novel method to combine the advantages of both multi-task and curriculum learning in a visual attribute classification framework. Individual tasks are grouped after performing hierarchical clustering based on their correlation. The clusters of tasks are learned in a curriculum learning setup by transferring knowledge between clusters. The learning process within each cluster is performed in a multi-task classification setup. By leveraging the acquired knowledge, we speed-up the process and improve performance. We demonstrate the effectiveness of our method via ablation studies and a detailed analysis of the covariates, on a variety of publicly available datasets of humans standing with their full-body visible. Extensive experimentation has proven that the proposed approach boosts the performance by 4% to 10%.




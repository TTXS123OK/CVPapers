# Arxiv Papers in cs.CV on 2017-09-18
### Facial Feature Tracking under Varying Facial Expressions and Face Poses based on Restricted Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1709.05731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05731v1)
- **Published**: 2017-09-18 00:11:25+00:00
- **Updated**: 2017-09-18 00:11:25+00:00
- **Authors**: Yue Wu, Zuoguan Wang, Qiang Ji
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition, 2013
- **Journal**: None
- **Summary**: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, since faces may have varying facial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.



### A Hierarchical Probabilistic Model for Facial Feature Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.05732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05732v1)
- **Published**: 2017-09-18 00:20:20+00:00
- **Updated**: 2017-09-18 00:20:20+00:00
- **Authors**: Yue Wu, Ziheng Wang, Qiang Ji
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition, 2014
- **Journal**: None
- **Summary**: Facial feature detection from facial images has attracted great attention in the field of computer vision. It is a nontrivial task since the appearance and shape of the face tend to change under different conditions. In this paper, we propose a hierarchical probabilistic model that could infer the true locations of facial features given the image measurements even if the face is with significant facial expression and pose. The hierarchical model implicitly captures the lower level shape variations of facial components using the mixture model. Furthermore, in the higher level, it also learns the joint relationship among facial components, the facial expression, and the pose information through automatic structure learning and parameter estimation of the probabilistic model. Experimental results on benchmark databases demonstrate the effectiveness of the proposed hierarchical probabilistic model.



### Joint Estimation of Camera Pose, Depth, Deblurring, and Super-Resolution from a Blurred Image Sequence
- **Arxiv ID**: http://arxiv.org/abs/1709.05745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05745v1)
- **Published**: 2017-09-18 02:24:31+00:00
- **Updated**: 2017-09-18 02:24:31+00:00
- **Authors**: Haesol Park, Kyoung Mu Lee
- **Comment**: accepted to ICCV 2017
- **Journal**: None
- **Summary**: The conventional methods for estimating camera poses and scene structures from severely blurry or low resolution images often result in failure. The off-the-shelf deblurring or super-resolution methods may show visually pleasing results. However, applying each technique independently before matching is generally unprofitable because this naive series of procedures ignores the consistency between images. In this paper, we propose a pioneering unified framework that solves four problems simultaneously, namely, dense depth reconstruction, camera pose estimation, super-resolution, and deblurring. By reflecting a physical imaging process, we formulate a cost minimization problem and solve it using an alternating optimization technique. The experimental results on both synthetic and real videos show high-quality depth maps derived from severely degraded images that contrast the failures of naive multi-view stereo methods. Our proposed method also produces outstanding deblurred and super-resolved images unlike the independent application or combination of conventional video deblurring, super-resolution methods.



### Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies
- **Arxiv ID**: http://arxiv.org/abs/1709.05746v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1709.05746v2)
- **Published**: 2017-09-18 02:27:02+00:00
- **Updated**: 2018-05-31 09:38:25+00:00
- **Authors**: Fangyi Zhang, Jürgen Leitner, Zongyuan Ge, Michael Milford, Peter Corke
- **Comment**: Under review for the International Journal of Robotics Research
- **Journal**: None
- **Summary**: Various approaches have been proposed to learn visuo-motor policies for real-world robotic applications. One solution is first learning in simulation then transferring to the real world. In the transfer, most existing approaches need real-world images with labels. However, the labelling process is often expensive or even impractical in many robotic applications. In this paper, we propose an adversarial discriminative sim-to-real transfer approach to reduce the cost of labelling real data. The effectiveness of the approach is demonstrated with modular networks in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The adversarial transfer approach reduced the labelled real data requirement by 50%. Policies can be transferred to real environments with only 93 labelled and 186 unlabelled real images. The transferred visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 97.8% success rate and 1.8 cm control accuracy.



### Where to Focus: Deep Attention-based Spatially Recurrent Bilinear Networks for Fine-Grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.05769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05769v1)
- **Published**: 2017-09-18 03:56:08+00:00
- **Updated**: 2017-09-18 03:56:08+00:00
- **Authors**: Lin Wu, Yang Wang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Fine-grained visual recognition typically depends on modeling subtle difference from object parts. However, these parts often exhibit dramatic visual variations such as occlusions, viewpoints, and spatial transformations, making it hard to detect. In this paper, we present a novel attention-based model to automatically, selectively and accurately focus on critical object regions with higher importance against appearance variations. Given an image, two different Convolutional Neural Networks (CNNs) are constructed, where the outputs of two CNNs are correlated through bilinear pooling to simultaneously focus on discriminative regions and extract relevant features. To capture spatial distributions among the local regions with visual attention, soft attention based spatial Long-Short Term Memory units (LSTMs) are incorporated to realize spatially recurrent yet visually selective over local input patterns. All the above intuitions equip our network with the following novel model: two-stream CNN layers, bilinear pooling layer, spatial recurrent layer with location attention are jointly trained via an end-to-end fashion to serve as the part detector and feature extractor, whereby relevant features are localized and extracted attentively. We show the significance of our network against two well-known visual recognition tasks: fine-grained image classification and person re-identification.



### Wide and deep volumetric residual networks for volumetric image classification
- **Arxiv ID**: http://arxiv.org/abs/1710.01217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1710.01217v1)
- **Published**: 2017-09-18 04:30:13+00:00
- **Updated**: 2017-09-18 04:30:13+00:00
- **Authors**: Varun Arvind, Anthony Costa, Marcus Badgeley, Samuel Cho, Eric Oermann
- **Comment**: None
- **Journal**: None
- **Summary**: 3D shape models that directly classify objects from 3D information have become more widely implementable. Current state of the art models rely on deep convolutional and inception models that are resource intensive. Residual neural networks have been demonstrated to be easier to optimize and do not suffer from vanishing/exploding gradients observed in deep networks. Here we implement a residual neural network for 3D object classification of the 3D Princeton ModelNet dataset. Further, we show that widening network layers dramatically improves accuracy in shallow residual nets, and residual neural networks perform comparable to state-of-the-art 3D shape net models, and we show that widening network layers improves classification accuracy. We provide extensive training and architecture parameters providing a better understanding of available network architectures for use in 3D object classification.



### Direction-Aware Semi-Dense SLAM
- **Arxiv ID**: http://arxiv.org/abs/1709.05774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1709.05774v1)
- **Published**: 2017-09-18 04:49:40+00:00
- **Updated**: 2017-09-18 04:49:40+00:00
- **Authors**: Julian Straub, Randi Cabezas, John Leonard, John W. Fisher III
- **Comment**: None
- **Journal**: None
- **Summary**: To aide simultaneous localization and mapping (SLAM), future perception systems will incorporate forms of scene understanding. In a step towards fully integrated probabilistic geometric scene understanding, localization and mapping we propose the first direction-aware semi-dense SLAM system. It jointly infers the directional Stata Center World (SCW) segmentation and a surfel-based semi-dense map while performing real-time camera tracking. The joint SCW map model connects a scene-wide Bayesian nonparametric Dirichlet Process von-Mises-Fisher mixture model (DP-vMF) prior on surfel orientations with the local surfel locations via a conditional random field (CRF). Camera tracking leverages the SCW segmentation to improve efficiency via guided observation selection. Results demonstrate improved SLAM accuracy and tracking efficiency at state of the art performance.



### Social Style Characterization from Egocentric Photo-streams
- **Arxiv ID**: http://arxiv.org/abs/1709.05775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05775v1)
- **Published**: 2017-09-18 04:50:30+00:00
- **Updated**: 2017-09-18 04:50:30+00:00
- **Authors**: Maedeh Aghaei, Mariella Dimiccoli, Cristian Canton Ferrer, Petia Radeva
- **Comment**: International Conference on Computer Vision (ICCV). Workshop on
  Egocentric Percetion, Interaction and Computing
- **Journal**: None
- **Summary**: This paper proposes a system for automatic social pattern characterization using a wearable photo-camera. The proposed pipeline consists of three major steps. First, detection of people with whom the camera wearer interacts and, second, categorization of the detected social interactions into formal and informal. These two steps act at event-level where each potential social event is modeled as a multi-dimensional time-series, whose dimensions correspond to a set of relevant features for each task, and a LSTM network is employed for time-series classification. In the last step, recurrences of the same person across the whole set of social interactions are clustered to achieve a comprehensive understanding of the diversity and frequency of the social relations of the user. Experiments over a dataset acquired by a user wearing a photo-camera during a month show promising results on the task of social pattern characterization from egocentric photo-streams.



### StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.05788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05788v1)
- **Published**: 2017-09-18 06:38:10+00:00
- **Updated**: 2017-09-18 06:38:10+00:00
- **Authors**: Sanghyun Woo, Soonmin Hwang, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: One-stage object detectors such as SSD or YOLO already have shown promising accuracy with small memory footprint and fast speed. However, it is widely recognized that one-stage detectors have difficulty in detecting small objects while they are competitive with two-stage methods on large objects. In this paper, we investigate how to alleviate this problem starting from the SSD framework. Due to their pyramidal design, the lower layer that is responsible for small objects lacks strong semantics(e.g contextual information). We address this problem by introducing a feature combining module that spreads out the strong semantics in a top-down manner. Our final model StairNet detector unifies the multi-scale representations and semantic distribution effectively. Experiments on PASCAL VOC 2007 and PASCAL VOC 2012 datasets demonstrate that StairNet significantly improves the weakness of SSD and outperforms the other state-of-the-art one-stage detectors.



### Direct Pose Estimation with a Monocular Camera
- **Arxiv ID**: http://arxiv.org/abs/1709.05815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05815v1)
- **Published**: 2017-09-18 08:37:05+00:00
- **Updated**: 2017-09-18 08:37:05+00:00
- **Authors**: Darius Burschka, Elmar Mair
- **Comment**: Robot Vision 2008, Auckland, New Zealand
- **Journal**: None
- **Summary**: We present a direct method to calculate a 6DoF pose change of a monocular camera for mobile navigation. The calculated pose is estimated up to a constant unknown scale parameter that is kept constant over the entire reconstruction process. This method allows a direct cal- culation of the metric position and rotation without any necessity to fuse the information in a probabilistic approach over longer frame sequence as it is the case in most currently used VSLAM approaches. The algorithm provides two novel aspects to the field of monocular navigation. It allows a direct pose estimation without any a-priori knowledge about the world directly from any two images and it provides a quality measure for the estimated motion parameters that allows to fuse the resulting information in Kalman Filters. We present the mathematical formulation of the approach together with experimental validation on real scene images.



### Beyond SIFT using Binary features for Loop Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/1709.05833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05833v1)
- **Published**: 2017-09-18 09:36:13+00:00
- **Updated**: 2017-09-18 09:36:13+00:00
- **Authors**: Lei Han, Guyue Zhou, Lan Xu, Lu Fang
- **Comment**: IROS 2017 paper for loop closure detection
- **Journal**: None
- **Summary**: In this paper a binary feature based Loop Closure Detection (LCD) method is proposed, which for the first time achieves higher precision-recall (PR) performance compared with state-of-the-art SIFT feature based approaches. The proposed system originates from our previous work Multi-Index hashing for Loop closure Detection (MILD), which employs Multi-Index Hashing (MIH)~\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of binary features. As the accuracy of MILD is limited by repeating textures and inaccurate image similarity measurement, burstiness handling is introduced to solve this problem and achieves considerable accuracy improvement. Additionally, a comprehensive theoretical analysis on MIH used in MILD is conducted to further explore the potentials of hashing methods for ANN search of binary features from probabilistic perspective. This analysis provides more freedom on best parameter choosing in MIH for different application scenarios. Experiments on popular public datasets show that the proposed approach achieved the highest accuracy compared with state-of-the-art while running at 30Hz for databases containing thousands of images.



### Microscopy Cell Segmentation via Adversarial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.05860v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05860v4)
- **Published**: 2017-09-18 10:55:31+00:00
- **Updated**: 2018-09-13 05:59:52+00:00
- **Authors**: Assaf Arbelle, Tammy Riklin Raviv
- **Comment**: Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)
  2018
- **Journal**: None
- **Summary**: We present a novel method for cell segmentation in microscopy images which is inspired by the Generative Adversarial Neural Network (GAN) approach. Our framework is built on a pair of two competitive artificial neural networks, with a unique architecture, termed Rib Cage, which are trained simultaneously and together define a min-max game resulting in an accurate segmentation of a given image. Our approach has two main strengths, similar to the GAN, the method does not require a formulation of a loss function for the optimization process. This allows training on a limited amount of annotated data in a weakly supervised manner. Promising segmentation results on real fluorescent microscopy data are presented. The code is freely available at: https://github.com/arbellea/DeepCellSeg.git



### Continuous Multimodal Emotion Recognition Approach for AVEC 2017
- **Arxiv ID**: http://arxiv.org/abs/1709.05861v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1709.05861v2)
- **Published**: 2017-09-18 11:01:43+00:00
- **Updated**: 2017-10-24 12:08:09+00:00
- **Authors**: Narotam Singh, Nittin Singh, Abhinav Dhall
- **Comment**: 4 pages, 3 figures, arXiv:1605.06778, arXiv:1512.03385
- **Journal**: None
- **Summary**: This paper reports the analysis of audio and visual features in predicting the continuous emotion dimensions under the seventh Audio/Visual Emotion Challenge (AVEC 2017), which was done as part of a B.Tech. 2nd year internship project. For visual features we used the HOG (Histogram of Gradients) features, Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network layers as features; all these extracted for each video clip. For audio features we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level descriptors) generated by openXBOW provided by the organisers of the event. Then we trained fully connected neural network regression model on the dataset for all these different modalities. We applied multimodal fusion on the output models to get the Concordance correlation coefficient on Development set as well as Test set.



### Recognizing Objects In-the-wild: Where Do We Stand?
- **Arxiv ID**: http://arxiv.org/abs/1709.05862v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.05862v2)
- **Published**: 2017-09-18 11:11:31+00:00
- **Updated**: 2018-05-22 11:55:27+00:00
- **Authors**: Mohammad Reza Loghmani, Barbara Caputo, Markus Vincze
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to recognize objects is an essential skill for a robotic system acting in human-populated environments. Despite decades of effort from the robotic and vision research communities, robots are still missing good visual perceptual systems, preventing the use of autonomous agents for real-world applications. The progress is slowed down by the lack of a testbed able to accurately represent the world perceived by the robot in-the-wild. In order to fill this gap, we introduce a large-scale, multi-view object dataset collected with an RGB-D camera mounted on a mobile robot. The dataset embeds the challenges faced by a robot in a real-life application and provides a useful tool for validating object recognition algorithms. Besides describing the characteristics of the dataset, the paper evaluates the performance of a collection of well-established deep convolutional networks on the new dataset and analyzes the transferability of deep representations from Web images to robotic data. Despite the promising results obtained with such representations, the experiments demonstrate that object classification with real-life robotic data is far from being solved. Finally, we provide a comparative study to analyze and highlight the open challenges in robot vision, explaining the discrepancies in the performance.



### Depression Scale Recognition from Audio, Visual and Text Analysis
- **Arxiv ID**: http://arxiv.org/abs/1709.05865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1709.05865v1)
- **Published**: 2017-09-18 11:26:01+00:00
- **Updated**: 2017-09-18 11:26:01+00:00
- **Authors**: Shubham Dham, Anirudh Sharma, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: Depression is a major mental health disorder that is rapidly affecting lives worldwide. Depression not only impacts emotional but also physical and psychological state of the person. Its symptoms include lack of interest in daily activities, feeling low, anxiety, frustration, loss of weight and even feeling of self-hatred. This report describes work done by us for Audio Visual Emotion Challenge (AVEC) 2017 during our second year BTech summer internship. With the increase in demand to detect depression automatically with the help of machine learning algorithms, we present our multimodal feature extraction and decision level fusion approach for the same. Features are extracted by processing on the provided Distress Analysis Interview Corpus-Wizard of Oz (DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector approach were applied on the visual data; statistical descriptors on gaze, pose; low level audio features and head pose and text features were also extracted. Classification is done on fused as well as independent features using Support Vector Machine (SVM) and neural networks. The results obtained were able to cross the provided baseline on validation data set by 17% on audio features and 24.5% on video features.



### Combinational neural network using Gabor filters for the classification of handwritten digits
- **Arxiv ID**: http://arxiv.org/abs/1709.05867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05867v1)
- **Published**: 2017-09-18 11:28:15+00:00
- **Updated**: 2017-09-18 11:28:15+00:00
- **Authors**: N. Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: A classification algorithm that combines the components of k-nearest neighbours and multilayer neural networks has been designed and tested. With this method the computational time required for training the dataset has been reduced substancially. Gabor filters were used for the feature extraction to ensure a better performance. This algorithm is tested with MNIST dataset and it will be integrated as a module in the object recognition software which is currently under development.



### E$^2$BoWs: An End-to-End Bag-of-Words Model via Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1709.05903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05903v2)
- **Published**: 2017-09-18 13:00:30+00:00
- **Updated**: 2017-09-20 02:57:01+00:00
- **Authors**: Xiaobin Liu, Shiliang Zhang, Tiejun Huang, Qi Tian
- **Comment**: 8 pages, ChinaMM 2017, image retrieval
- **Journal**: None
- **Summary**: Traditional Bag-of-visual Words (BoWs) model is commonly generated with many steps including local feature extraction, codebook generation, and feature quantization, etc. Those steps are relatively independent with each other and are hard to be jointly optimized. Moreover, the dependency on hand-crafted local feature makes BoWs model not effective in conveying high-level semantics. These issues largely hinder the performance of BoWs model in large-scale image applications. To conquer these issues, we propose an End-to-End BoWs (E$^2$BoWs) model based on Deep Convolutional Neural Network (DCNN). Our model takes an image as input, then identifies and separates the semantic objects in it, and finally outputs the visual words with high semantic discriminative power. Specifically, our model firstly generates Semantic Feature Maps (SFMs) corresponding to different object categories through convolutional layers, then introduces Bag-of-Words Layers (BoWL) to generate visual words for each individual feature map. We also introduce a novel learning algorithm to reinforce the sparsity of the generated E$^2$BoWs model, which further ensures the time and memory efficiency. We evaluate the proposed E$^2$BoWs model on several image search datasets including CIFAR-10, CIFAR-100, MIRFLICKR-25K and NUS-WIDE. Experimental results show that our method achieves promising accuracy and efficiency compared with recent deep learning based retrieval works.



### Object Recognition from very few Training Examples for Enhancing Bicycle Maps
- **Arxiv ID**: http://arxiv.org/abs/1709.05910v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.05910v4)
- **Published**: 2017-09-18 13:21:46+00:00
- **Updated**: 2018-05-28 09:41:06+00:00
- **Authors**: Christoph Reinders, Hanno Ackermann, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: Submitted to IV 2018. This research was supported by German Research
  Foundation DFG within Priority Research Programme 1894 "Volunteered
  Geographic Information: Interpretation, Visualization and Social Computing"
- **Journal**: None
- **Summary**: In recent years, data-driven methods have shown great success for extracting information about the infrastructure in urban areas. These algorithms are usually trained on large datasets consisting of thousands or millions of labeled training examples. While large datasets have been published regarding cars, for cyclists very few labeled data is available although appearance, point of view, and positioning of even relevant objects differ. Unfortunately, labeling data is costly and requires a huge amount of work. In this paper, we thus address the problem of learning with very few labels. The aim is to recognize particular traffic signs in crowdsourced data to collect information which is of interest to cyclists. We propose a system for object recognition that is trained with only 15 examples per class on average. To achieve this, we combine the advantages of convolutional neural networks and random forests to learn a patch-wise classifier. In the next step, we map the random forest to a neural network and transform the classifier to a fully convolutional network. Thereby, the processing of full images is significantly accelerated and bounding boxes can be predicted. Finally, we integrate data of the Global Positioning System (GPS) to localize the predictions on the map. In comparison to Faster R-CNN and other networks for object recognition or algorithms for transfer learning, we considerably reduce the required amount of labeled data. We demonstrate good performance on the recognition of traffic signs for cyclists as well as their localization in maps.



### Multi-Task Learning for Segmentation of Building Footprints with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.05932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05932v1)
- **Published**: 2017-09-18 13:47:45+00:00
- **Updated**: 2017-09-18 13:47:45+00:00
- **Authors**: Benjamin Bischke, Patrick Helber, Joachim Folz, Damian Borth, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: The increased availability of high resolution satellite imagery allows to sense very detailed structures on the surface of our planet. Access to such information opens up new directions in the analysis of remote sensing imagery. However, at the same time this raises a set of new challenges for existing pixel-based prediction methods, such as semantic segmentation approaches. While deep neural networks have achieved significant advances in the semantic segmentation of high resolution images in the past, most of the existing approaches tend to produce predictions with poor boundaries. In this paper, we address the problem of preserving semantic segmentation boundaries in high resolution satellite imagery by introducing a new cascaded multi-task loss. We evaluate our approach on Inria Aerial Image Labeling Dataset which contains large-scale and high resolution images. Our results show that we are able to outperform state-of-the-art methods by 8.3\% without any additional post-processing step.



### Normal Integration: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1709.05940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05940v1)
- **Published**: 2017-09-18 13:56:11+00:00
- **Updated**: 2017-09-18 13:56:11+00:00
- **Authors**: Yvain Quéau, Jean-Denis Durou, Jean-François Aujol
- **Comment**: None
- **Journal**: None
- **Summary**: The need for efficient normal integration methods is driven by several computer vision tasks such as shape-from-shading, photometric stereo, deflectometry, etc. In the first part of this survey, we select the most important properties that one may expect from a normal integration method, based on a thorough study of two pioneering works by Horn and Brooks [28] and by Frankot and Chellappa [19]. Apart from accuracy, an integration method should at least be fast and robust to a noisy normal field. In addition, it should be able to handle several types of boundary condition, including the case of a free boundary, and a reconstruction domain of any shape i.e., which is not necessarily rectangular. It is also much appreciated that a minimum number of parameters have to be tuned, or even no parameter at all. Finally, it should preserve the depth discontinuities. In the second part of this survey, we review most of the existing methods in view of this analysis, and conclude that none of them satisfies all of the required properties. This work is complemented by a companion paper entitled Variational Methods for Normal Integration, in which we focus on the problem of normal integration in the presence of depth discontinuities, a problem which occurs as soon as there are occlusions.



### Fast YOLO: A Fast You Only Look Once System for Real-time Embedded Object Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1709.05943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1709.05943v1)
- **Published**: 2017-09-18 13:57:16+00:00
- **Updated**: 2017-09-18 13:57:16+00:00
- **Authors**: Mohammad Javad Shafiee, Brendan Chywl, Francis Li, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is considered one of the most challenging problems in this field of computer vision, as it involves the combination of object classification and object localization within a scene. Recently, deep neural networks (DNNs) have been demonstrated to achieve superior object detection performance compared to other approaches, with YOLOv2 (an improved You Only Look Once model) being one of the state-of-the-art in DNN-based object detection methods in terms of both speed and accuracy. Although YOLOv2 can achieve real-time performance on a powerful GPU, it still remains very challenging for leveraging this approach for real-time object detection in video on embedded computing devices with limited computational power and limited memory. In this paper, we propose a new framework called Fast YOLO, a fast You Only Look Once framework which accelerates YOLOv2 to be able to perform object detection in video on embedded devices in a real-time manner. First, we leverage the evolutionary deep intelligence framework to evolve the YOLOv2 network architecture and produce an optimized architecture (referred to as O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To further reduce power consumption on embedded devices while maintaining performance, a motion-adaptive inference method is introduced into the proposed Fast YOLO framework to reduce the frequency of deep inference with O-YOLOv2 based on temporal motion characteristics. Experimental results show that the proposed Fast YOLO framework can reduce the number of deep inferences by an average of 38.13%, and an average speedup of ~3.3X for objection detection in video compared to the original YOLOv2, leading Fast YOLO to run an average of ~18FPS on a Nvidia Jetson TX1 embedded system.



### Variational Methods for Normal Integration
- **Arxiv ID**: http://arxiv.org/abs/1709.05965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05965v1)
- **Published**: 2017-09-18 14:17:48+00:00
- **Updated**: 2017-09-18 14:17:48+00:00
- **Authors**: Yvain Quéau, Jean-Denis Durou, Jean-François Aujol
- **Comment**: None
- **Journal**: None
- **Summary**: The need for an efficient method of integration of a dense normal field is inspired by several computer vision tasks, such as shape-from-shading, photometric stereo, deflectometry, etc. Inspired by edge-preserving methods from image processing, we study in this paper several variational approaches for normal integration, with a focus on non-rectangular domains, free boundary and depth discontinuities. We first introduce a new discretization for quadratic integration, which is designed to ensure both fast recovery and the ability to handle non-rectangular domains with a free boundary. Yet, with this solver, discontinuous surfaces can be handled only if the scene is first segmented into pieces without discontinuity. Hence, we then discuss several discontinuity-preserving strategies. Those inspired, respectively, by the Mumford-Shah segmentation method and by anisotropic diffusion, are shown to be the most effective for recovering discontinuities.



### Multi-Person Pose Estimation via Column Generation
- **Arxiv ID**: http://arxiv.org/abs/1709.05982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.05982v1)
- **Published**: 2017-09-18 14:40:57+00:00
- **Updated**: 2017-09-18 14:40:57+00:00
- **Authors**: Shaofei Wang, Chong Zhang, Miguel A. Gonzalez-Ballester, Alexander Ihler, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of multi-person pose estimation in natural images. A pose estimate describes the spatial position and identity (head, foot, knee, etc.) of every non-occluded body part of a person. Pose estimation is difficult due to issues such as deformation and variation in body configurations and occlusion of parts, while multi-person settings add complications such as an unknown number of people, with unknown appearance and possible interactions in their poses and part locations. We give a novel integer program formulation of the multi-person pose estimation problem, in which variables correspond to assignments of parts in the image to poses in a two-tier, hierarchical way. This enables us to develop an efficient custom optimization procedure based on column generation, where columns are produced by exact optimization of very small scale integer programs. We demonstrate improved accuracy and speed for our method on the MPII multi-person pose estimation benchmark.



### LS-VO: Learning Dense Optical Subspace for Robust Visual Odometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/1709.06019v2
- **DOI**: 10.1109/LRA.2018.2803211
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1709.06019v2)
- **Published**: 2017-09-18 16:11:34+00:00
- **Updated**: 2017-12-12 22:25:23+00:00
- **Authors**: Gabriele Costante, Thomas A. Ciarfuglia
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a novel deep network architecture to solve the camera Ego-Motion estimation problem. A motion estimation network generally learns features similar to Optical Flow (OF) fields starting from sequences of images. This OF can be described by a lower dimensional latent space. Previous research has shown how to find linear approximations of this space. We propose to use an Auto-Encoder network to find a non-linear representation of the OF manifold. In addition, we propose to learn the latent space jointly with the estimation task, so that the learned OF features become a more robust description of the OF input. We call this novel architecture LS-VO.   The experiments show that LS-VO achieves a considerable increase in performances in respect to baselines, while the number of parameters of the estimation network only slightly increases.



### Video Object Segmentation Without Temporal Information
- **Arxiv ID**: http://arxiv.org/abs/1709.06031v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06031v2)
- **Published**: 2017-09-18 16:28:02+00:00
- **Updated**: 2018-05-16 12:16:48+00:00
- **Authors**: Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, Luc Van Gool
- **Comment**: Accepted to T-PAMI. Extended version of "One-Shot Video Object
  Segmentation", CVPR 2017 (arXiv:1611.05198). Project page:
  http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/
- **Journal**: None
- **Summary**: Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames. When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly or they may not even produce any result at all. This paper explores the orthogonal approach of processing each frame independently, i.e disregarding the temporal information. In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame. We present Semantic One-Shot Video Object Segmentation (OSVOS-S), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot). We show that instance level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS. We perform experiments on two recent video segmentation databases, which show that OSVOS-S is both the fastest and most accurate method in the state of the art.



### Vehicle Tracking in Wide Area Motion Imagery via Stochastic Progressive Association Across Multiple Frames (SPAAM)
- **Arxiv ID**: http://arxiv.org/abs/1709.06035v1
- **DOI**: 10.1109/TIP.2018.2818443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06035v1)
- **Published**: 2017-09-18 16:40:28+00:00
- **Updated**: 2017-09-18 16:40:28+00:00
- **Authors**: Ahmed Elliethy, Gaurav Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle tracking in Wide Area Motion Imagery (WAMI) relies on associating vehicle detections across multiple WAMI frames to form tracks corresponding to individual vehicles. The temporal window length, i.e., the number $M$ of sequential frames, over which associations are collectively estimated poses a trade-off between accuracy and computational complexity. A larger $M$ improves performance because the increased temporal context enables the use of motion models and allows occlusions and spurious detections to be handled better. The number of total hypotheses tracks, on the other hand, grows exponentially with increasing $M$, making larger values of $M$ computationally challenging to tackle. In this paper, we introduce SPAAM an iterative approach that progressively grows $M$ with each iteration to improve estimated tracks by exploiting the enlarged temporal context while keeping computation manageable through two novel approaches for pruning association hypotheses. First, guided by a road network, accurately co-registered to the WAMI frames, we disregard unlikely associations that do not agree with the road network. Second, as $M$ is progressively enlarged at each iteration, the related increase in association hypotheses is limited by revisiting only the subset of association possibilities rendered open by stochastically determined dis-associations for the previous iteration. The stochastic dis-association at each iteration maintains each estimated association according to an estimated probability for confidence, obtained via a probabilistic model. Associations at each iteration are then estimated globally over the $M$ frames by (approximately) solving a binary integer programming problem for selecting a set of compatible tracks. Vehicle tracking results obtained over test WAMI datasets indicate that our proposed approach provides significant performance improvements over 3 alternatives.



### Coupled Ensembles of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.06053v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.06053v1)
- **Published**: 2017-09-18 17:16:26+00:00
- **Updated**: 2017-09-18 17:16:26+00:00
- **Authors**: Anuvabh Dutt, Denis Pellerin, Georges Quénot
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate in this paper the architecture of deep convolutional networks. Building on existing state of the art models, we propose a reconfiguration of the model parameters into several parallel branches at the global network level, with each branch being a standalone CNN. We show that this arrangement is an efficient way to significantly reduce the number of parameters without losing performance or to significantly improve the performance with the same level of performance. The use of branches brings an additional form of regularization. In addition to the split into parallel branches, we propose a tighter coupling of these branches by placing the "fuse (averaging) layer" before the Log-Likelihood and SoftMax layers during training. This gives another significant performance improvement, the tighter coupling favouring the learning of better representations, even at the level of the individual branches. We refer to this branched architecture as "coupled ensembles". The approach is very generic and can be applied with almost any DCNN architecture. With coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and SVHN tasks. For the same budget, DenseNet-BC has error rate of 3.46%, 17.18%, and 1.8% respectively. With ensembles of coupled ensembles, of DenseNet-BC networks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks.



### Target-adaptive CNN-based pansharpening
- **Arxiv ID**: http://arxiv.org/abs/1709.06054v3
- **DOI**: 10.1109/TGRS.2018.2817393
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06054v3)
- **Published**: 2017-09-18 17:16:55+00:00
- **Updated**: 2018-02-23 21:44:16+00:00
- **Authors**: Giuseppe Scarpa, Sergio Vitale, Davide Cozzolino
- **Comment**: None
- **Journal**: None
- **Summary**: We recently proposed a convolutional neural network (CNN) for remote sensing image pansharpening obtaining a significant performance gain over the state of the art. In this paper, we explore a number of architectural and training variations to this baseline, achieving further performance gains with a lightweight network which trains very fast. Leveraging on this latter property, we propose a target-adaptive usage modality which ensures a very good performance also in the presence of a mismatch w.r.t. the training set, and even across different sensors. The proposed method, published online as an off-the-shelf software tool, allows users to perform fast and high-quality CNN-based pansharpening of their own target images on general-purpose hardware.



### Rotation Adaptive Visual Object Tracking with Motion Consistency
- **Arxiv ID**: http://arxiv.org/abs/1709.06057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06057v2)
- **Published**: 2017-09-18 17:24:18+00:00
- **Updated**: 2017-11-22 17:18:10+00:00
- **Authors**: Litu Rout, Sidhartha, Gorthi R. K. S. S. Manyam, Deepak Mishra
- **Comment**: Accepted conference paper WACV 2018
- **Journal**: None
- **Summary**: Visual Object tracking research has undergone significant improvement in the past few years. The emergence of tracking by detection approach in tracking paradigm has been quite successful in many ways. Recently, deep convolutional neural networks have been extensively used in most successful trackers. Yet, the standard approach has been based on correlation or feature selection with minimal consideration given to motion consistency. Thus, there is still a need to capture various physical constraints through motion consistency which will improve accuracy, robustness and more importantly rotation adaptiveness. Therefore, one of the major aspects of this paper is to investigate the outcome of rotation adaptiveness in visual object tracking. Among other key contributions, the paper also includes various consistencies that turn out to be extremely effective in numerous challenging sequences than the current state-of-the-art.



### Fiber-Flux Diffusion Density for White Matter Tracts Analysis: Application to Mild Anomalies Localization in Contact Sports Players
- **Arxiv ID**: http://arxiv.org/abs/1709.06122v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1709.06122v1)
- **Published**: 2017-09-18 18:54:18+00:00
- **Updated**: 2017-09-18 18:54:18+00:00
- **Authors**: Itay Benou, Ronel Veksler, Alon Friedman, Tammy Riklin Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We present the concept of fiber-flux density for locally quantifying white matter (WM) fiber bundles. By combining scalar diffusivity measures (e.g., fractional anisotropy) with fiber-flux measurements, we define new local descriptors called Fiber-Flux Diffusion Density (FFDD) vectors. Applying each descriptor throughout fiber bundles allows along-tract coupling of a specific diffusion measure with geometrical properties, such as fiber orientation and coherence. A key step in the proposed framework is the construction of an FFDD dissimilarity measure for sub-voxel alignment of fiber bundles, based on the fast marching method (FMM). The obtained aligned WM tract-profiles enable meaningful inter-subject comparisons and group-wise statistical analysis. We demonstrate our method using two different datasets of contact sports players. Along-tract pairwise comparison as well as group-wise analysis, with respect to non-player healthy controls, reveal significant and spatially-consistent FFDD anomalies. Comparing our method with along-tract FA analysis shows improved sensitivity to subtle structural anomalies in football players over standard FA measurements.



### How intelligent are convolutional neural networks?
- **Arxiv ID**: http://arxiv.org/abs/1709.06126v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06126v2)
- **Published**: 2017-09-18 19:04:36+00:00
- **Updated**: 2017-10-31 20:29:12+00:00
- **Authors**: Zhennan Yan, Xiang Sean Zhou
- **Comment**: add one more experiment: common fate task; add link to github
- **Journal**: None
- **Summary**: Motivated by the Gestalt pattern theory, and the Winograd Challenge for language understanding, we design synthetic experiments to investigate a deep learning algorithm's ability to infer simple (at least for human) visual concepts, such as symmetry, from examples. A visual concept is represented by randomly generated, positive as well as negative, example images. We then test the ability and speed of algorithms (and humans) to learn the concept from these images. The training and testing are performed progressively in multiple rounds, with each subsequent round deliberately designed to be more complex and confusing than the previous round(s), especially if the concept was not grasped by the learner. However, if the concept was understood, all the deliberate tests would become trivially easy. Our experiments show that humans can often infer a semantic concept quickly after looking at only a very small number of examples (this is often referred to as an "aha moment": a moment of sudden realization), and performs perfectly during all testing rounds (except for careless mistakes). On the contrary, deep convolutional neural networks (DCNN) could approximate some concepts statistically, but only after seeing many (x10^4) more examples. And it will still make obvious mistakes, especially during deliberate testing rounds or on samples outside the training distributions. This signals a lack of true "understanding", or a failure to reach the right "formula" for the semantics. We did find that some concepts are easier for DCNN than others. For example, simple "counting" is more learnable than "symmetry", while "uniformity" or "conformance" are much more difficult for DCNN to learn. To conclude, we propose an "Aha Challenge" for visual perception, calling for focused and quantitative research on Gestalt-style machine intelligence using limited training examples.



### When is a Convolutional Filter Easy To Learn?
- **Arxiv ID**: http://arxiv.org/abs/1709.06129v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.06129v2)
- **Published**: 2017-09-18 19:09:24+00:00
- **Updated**: 2018-02-28 17:08:26+00:00
- **Authors**: Simon S. Du, Jason D. Lee, Yuandong Tian
- **Comment**: Published as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.



### White Matter Fiber Segmentation Using Functional Varifolds
- **Arxiv ID**: http://arxiv.org/abs/1709.06144v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1709.06144v1)
- **Published**: 2017-09-18 20:05:19+00:00
- **Updated**: 2017-09-18 20:05:19+00:00
- **Authors**: Kuldeep Kumar, Pietro Gori, Benjamin Charlier, Stanley Durrleman, Olivier Colliot, Christian Desrosiers
- **Comment**: None
- **Journal**: Graphs in Biomedical Image Analysis, Computational Anatomy and
  Imaging Genetics, pp 92-100, Lecture Notes in Computer Science, volume 10551,
  Springer, 2017
- **Summary**: The extraction of fibers from dMRI data typically produces a large number of fibers, it is common to group fibers into bundles. To this end, many specialized distance measures, such as MCP, have been used for fiber similarity. However, these distance based approaches require point-wise correspondence and focus only on the geometry of the fibers. Recent publications have highlighted that using microstructure measures along fibers improves tractography analysis. Also, many neurodegenerative diseases impacting white matter require the study of microstructure measures as well as the white matter geometry. Motivated by these, we propose to use a novel computational model for fibers, called functional varifolds, characterized by a metric that considers both the geometry and microstructure measure (e.g. GFA) along the fiber pathway. We use it to cluster fibers with a dictionary learning and sparse coding-based framework, and present a preliminary analysis using HCP data.



### Multi-modal analysis of genetically-related subjects using SIFT descriptors in brain MRI
- **Arxiv ID**: http://arxiv.org/abs/1709.06151v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1709.06151v1)
- **Published**: 2017-09-18 20:12:32+00:00
- **Updated**: 2017-09-18 20:12:32+00:00
- **Authors**: Kuldeep Kumar, Laurent Chauvin, Mathew Toews, Olivier Colliot, Christian Desrosiers
- **Comment**: None
- **Journal**: Proc. Computational Diffusion MRI, MICCAI Workshop, Qu\'ebec City,
  Canada, September 2017
- **Summary**: So far, fingerprinting studies have focused on identifying features from single-modality MRI data, which capture individual characteristics in terms of brain structure, function, or white matter microstructure. However, due to the lack of a framework for comparing across multiple modalities, studies based on multi-modal data remain elusive. This paper presents a multi-modal analysis of genetically-related subjects to compare and contrast the information provided by various MRI modalities. The proposed framework represents MRI scans as bags of SIFT features, and uses these features in a nearest-neighbor graph to measure subject similarity. Experiments using the T1/T2-weighted MRI and diffusion MRI data of 861 Human Connectome Project subjects demonstrate strong links between the proposed similarity measure and genetic proximity.



### Matterport3D: Learning from RGB-D Data in Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1709.06158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06158v1)
- **Published**: 2017-09-18 20:34:48+00:00
- **Updated**: 2017-09-18 20:34:48+00:00
- **Authors**: Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.



### A Fast Algorithm Based on a Sylvester-like Equation for LS Regression with GMRF Prior
- **Arxiv ID**: http://arxiv.org/abs/1709.06178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.06178v2)
- **Published**: 2017-09-18 21:57:47+00:00
- **Updated**: 2017-10-09 16:37:14+00:00
- **Authors**: Qi Wei, Emilie Chouzenoux, Jean-Yves Tourneret, Jean-Christophe Pesquet
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a fast approach for penalized least squares (LS) regression problems using a 2D Gaussian Markov random field (GMRF) prior. More precisely, the computation of the proximity operator of the LS criterion regularized by different GMRF potentials is formulated as solving a Sylvester-like matrix equation. By exploiting the structural properties of GMRFs, this matrix equation is solved columnwise in an analytical way. The proposed algorithm can be embedded into a wide range of proximal algorithms to solve LS regression problems including a convex penalty. Experiments carried out in the case of a constrained LS regression problem arising in a multichannel image processing application, provide evidence that an alternating direction method of multipliers performs quite efficiently in this context.



### Measurement of amplitude of the moiré patterns in digital autostereoscopic 3D display
- **Arxiv ID**: http://arxiv.org/abs/1709.07745v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.07745v1)
- **Published**: 2017-09-18 22:55:09+00:00
- **Updated**: 2017-09-18 22:55:09+00:00
- **Authors**: Vladimir Saveljev, Sung-Kyu Kim
- **Comment**: 13 pages, 14 figures, 12 equations
- **Journal**: None
- **Summary**: The article presents the experimental measurements of the amplitude of the moir\'e patterns in a digital autostereoscopic barrier-type 3D display across a wide angular range with a small increment. The period and orientation of the moir\'e patterns were also measured as functions of the angle. Simultaneous branches are observed and analyzed. The theoretical interpretation is also given. The results can help preventing or minimizing the moir\'e effect in displays.



### Protest Activity Detection and Perceived Violence Estimation from Social Media Images
- **Arxiv ID**: http://arxiv.org/abs/1709.06204v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1709.06204v1)
- **Published**: 2017-09-18 23:57:42+00:00
- **Updated**: 2017-09-18 23:57:42+00:00
- **Authors**: Donghyeon Won, Zachary C. Steinert-Threlkeld, Jungseock Joo
- **Comment**: To appear in Proceedings of the 25th ACM International Conference on
  Multimedia 2017 (full research paper)
- **Journal**: None
- **Summary**: We develop a novel visual model which can recognize protesters, describe their activities by visual attributes and estimate the level of perceived violence in an image. Studies of social media and protests use natural language processing to track how individuals use hashtags and links, often with a focus on those items' diffusion. These approaches, however, may not be effective in fully characterizing actual real-world protests (e.g., violent or peaceful) or estimating the demographics of participants (e.g., age, gender, and race) and their emotions. Our system characterizes protests along these dimensions. We have collected geotagged tweets and their images from 2013-2017 and analyzed multiple major protest events in that period. A multi-task convolutional neural network is employed in order to automatically classify the presence of protesters in an image and predict its visual attributes, perceived violence and exhibited emotions. We also release the UCLA Protest Image Dataset, our novel dataset of 40,764 images (11,659 protest images and hard negatives) with various annotations of visual attributes and sentiments. Using this dataset, we train our model and demonstrate its effectiveness. We also present experimental results from various analysis on geotagged image data in several prevalent protest events. Our dataset will be made accessible at https://www.sscnet.ucla.edu/comm/jjoo/mm-protest/.




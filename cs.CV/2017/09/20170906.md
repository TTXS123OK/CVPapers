# Arxiv Papers in cs.CV on 2017-09-06
### Learning to Compose Domain-Specific Transformations for Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1709.01643v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.01643v3)
- **Published**: 2017-09-06 01:17:31+00:00
- **Updated**: 2017-09-30 04:27:53+00:00
- **Authors**: Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher Ré
- **Comment**: To appear at Neural Information Processing Systems (NIPS) 2017
- **Journal**: Advances in Neural Information Processing Systems 30, 2017,
  3236--3246
- **Summary**: Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.



### Deep Convolutional Neural Network for Age Estimation based on VGG-Face Model
- **Arxiv ID**: http://arxiv.org/abs/1709.01664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01664v1)
- **Published**: 2017-09-06 03:37:12+00:00
- **Updated**: 2017-09-06 03:37:12+00:00
- **Authors**: Zakariya Qawaqneh, Arafat Abu Mallouh, Buket D. Barkana
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Automatic age estimation from real-world and unconstrained face images is rapidly gaining importance. In our proposed work, a deep CNN model that was trained on a database for face recognition task is used to estimate the age information on the Adience database. This paper has three significant contributions in this field. (1) This work proves that a CNN model, which was trained for face recognition task, can be utilized for age estimation to improve performance; (2) Over fitting problem can be overcome by employing a pretrained CNN on a large database for face recognition task; (3) Not only the number of training images and the number subjects in a training database effect the performance of the age estimation model, but also the pre-training task of the employed CNN determines the performance of the model.



### Distributed Deep Neural Networks over the Cloud, the Edge and End Devices
- **Arxiv ID**: http://arxiv.org/abs/1709.01921v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1709.01921v1)
- **Published**: 2017-09-06 04:00:18+00:00
- **Updated**: 2017-09-06 04:00:18+00:00
- **Authors**: Surat Teerapittayanon, Bradley McDanel, H. T. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: We propose distributed deep neural networks (DDNNs) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network (DNN) in the cloud, a DDNN also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a DDNN can scale up in neural network size and scale out in geographical span. Due to its distributed nature, DDNNs enhance sensor fusion, system fault tolerance and data privacy for DNN applications. In implementing a DDNN, we map sections of a DNN onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a DDNN can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of offloading raw sensor data to be processed in the cloud, DDNN locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.



### BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01686v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.01686v1)
- **Published**: 2017-09-06 06:30:51+00:00
- **Updated**: 2017-09-06 06:30:51+00:00
- **Authors**: Surat Teerapittayanon, Bradley McDanel, H. T. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present BranchyNet, a novel deep network architecture that is augmented with additional side branch classifiers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high confidence. BranchyNet exploits the observation that features learned at an early layer of a network may often be sufficient for the classification of many data points. For more difficult samples, which are expected less frequently, BranchyNet will use further or all network layers to provide the best likelihood of correct prediction. We study the BranchyNet architecture using several well-known networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that it can both improve accuracy and significantly reduce the inference time of the network.



### Embedded Binarized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.02260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.02260v1)
- **Published**: 2017-09-06 06:45:33+00:00
- **Updated**: 2017-09-06 06:45:33+00:00
- **Authors**: Bradley McDanel, Surat Teerapittayanon, H. T. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing current binarized neural networks (BNNs) in the literature to perform feedforward inference efficiently on small embedded devices. We focus on minimizing the required memory footprint, given that these devices often have memory as small as tens of kilobytes (KB). Beyond minimizing the memory required to store weights, as in a BNN, we show that it is essential to minimize the memory used for temporaries which hold intermediate results between layers in feedforward inference. To accomplish this, eBNN reorders the computation of inference while preserving the original BNN structure, and uses just a single floating-point temporary for the entire neural network. All intermediate results from a layer are stored as binary values, as opposed to floating-points used in current BNN implementations, leading to a 32x reduction in required temporary space. We provide empirical evidence that our proposed eBNN approach allows efficient inference (10s of ms) on devices with severely limited memory (10s of KB). For example, eBNN achieves 95\% accuracy on the MNIST dataset running on an Intel Curie with only 15 KB of usable memory with an inference runtime of under 50 ms per sample. To ease the development of applications in embedded contexts, we make our source code available that allows users to train and discover eBNN models for a learning task at hand, which fit within the memory constraint of the target device.



### Group-level Emotion Recognition using Transfer Learning from Face Identification
- **Arxiv ID**: http://arxiv.org/abs/1709.01688v3
- **DOI**: 10.1145/3136755.3143007
- **Categories**: **cs.CV**, 68T10, 68T45, I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1709.01688v3)
- **Published**: 2017-09-06 06:47:23+00:00
- **Updated**: 2017-10-30 19:33:12+00:00
- **Authors**: Alexandr G. Rassadin, Alexey S. Gruzdev, Andrey V. Savchenko
- **Comment**: 5 pages, 3 figures, accepted for publication at ICMI17 (EmotiW Grand
  Challenge)
- **Journal**: Proceedings of the 19th ACM International Conference on Multimodal
  Interaction (ICMI), 2017, pp. 544-548
- **Summary**: In this paper, we describe our algorithmic approach, which was used for submissions in the fifth Emotion Recognition in the Wild (EmotiW 2017) group-level emotion recognition sub-challenge. We extracted feature vectors of detected faces using the Convolutional Neural Network trained for face identification task, rather than traditional pre-training on emotion recognition problems. In the final pipeline an ensemble of Random Forest classifiers was learned to predict emotion score using available training set. In case when the faces have not been detected, one member of our ensemble extracts features from the whole image. During our experimental study, the proposed approach showed the lowest error rate when compared to other explored techniques. In particular, we achieved 75.4% accuracy on the validation data, which is 20% higher than the handcrafted feature-based baseline. The source code using Keras framework is publicly available.



### A Compact Kernel Approximation for 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1709.01695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01695v2)
- **Published**: 2017-09-06 07:02:58+00:00
- **Updated**: 2017-10-04 11:05:35+00:00
- **Authors**: Jacopo Cavazza, Pietro Morerio, Vittorio Murino
- **Comment**: Best paper award special mention at the 19th edition of the GIRPR
  International Conference on Image Analysis and Processing (ICIAP) 2017
- **Journal**: None
- **Summary**: 3D action recognition was shown to benefit from a covariance representation of the input data (joint 3D positions). A kernel machine feed with such feature is an effective paradigm for 3D action recognition, yielding state-of-the-art results. Yet, the whole framework is affected by the well-known scalability issue. In fact, in general, the kernel function has to be evaluated for all pairs of instances inducing a Gram matrix whose complexity is quadratic in the number of samples. In this work we reduce such complexity to be linear by proposing a novel and explicit feature map to approximate the kernel function. This allows to train a linear classifier with an explicit feature encoding, which implicitly implements a Log-Euclidean machine in a scalable fashion. Not only we prove that the proposed approximation is unbiased, but also we work out an explicit strong bound for its variance, attesting a theoretical superiority of our approach with respect to existing ones. Experimentally, we verify that our representation provides a compact encoding and outperforms other approximation schemes on a number of publicly available benchmark datasets for 3D action recognition.



### Blind image deblurring using class-adapted image priors
- **Arxiv ID**: http://arxiv.org/abs/1709.01710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01710v1)
- **Published**: 2017-09-06 08:20:10+00:00
- **Updated**: 2017-09-06 08:20:10+00:00
- **Authors**: Marina Ljubenović, Mário A. T. Figueiredo
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Blind image deblurring (BID) is an ill-posed inverse problem, usually addressed by imposing prior knowledge on the (unknown) image and on the blurring filter. Most of the work on BID has focused on natural images, using image priors based on statistical properties of generic natural images. However, in many applications, it is known that the image being recovered belongs to some specific class (e.g., text, face, fingerprints), and exploiting this knowledge allows obtaining more accurate priors. In this work, we propose a method where a Gaussian mixture model (GMM) is used to learn a class-adapted prior, by training on a dataset of clean images of that class. Experiments show the competitiveness of the proposed method in terms of restoration quality when dealing with images containing text, faces, or fingerprints. Additionally, experiments show that the proposed method is able to handle text images at high noise levels, outperforming state-of-the-art methods specifically designed for BID of text images.



### Detecting animals in African Savanna with UAVs and the crowds
- **Arxiv ID**: http://arxiv.org/abs/1709.01722v1
- **DOI**: 10.1016/j.rse.2017.08.026
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01722v1)
- **Published**: 2017-09-06 08:45:09+00:00
- **Updated**: 2017-09-06 08:45:09+00:00
- **Authors**: Nicolas Rey, Michele Volpi, Stéphane Joost, Devis Tuia
- **Comment**: None
- **Journal**: Remote Sensing of Environment, 200, pp. 341-351, 2017
- **Summary**: Unmanned aerial vehicles (UAVs) offer new opportunities for wildlife monitoring, with several advantages over traditional field-based methods. They have readily been used to count birds, marine mammals and large herbivores in different environments, tasks which are routinely performed through manual counting in large collections of images. In this paper, we propose a semi-automatic system able to detect large mammals in semi-arid Savanna. It relies on an animal-detection system based on machine learning, trained with crowd-sourced annotations provided by volunteers who manually interpreted sub-decimeter resolution color images. The system achieves a high recall rate and a human operator can then eliminate false detections with limited effort. Our system provides good perspectives for the development of data-driven management practices in wildlife conservation. It shows that the detection of large mammals in semi-arid Savanna can be approached by processing data provided by standard RGB cameras mounted on affordable fixed wings UAVs.



### Scene Text Recognition with Sliding Convolutional Character Models
- **Arxiv ID**: http://arxiv.org/abs/1709.01727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01727v1)
- **Published**: 2017-09-06 09:01:53+00:00
- **Updated**: 2017-09-06 09:01:53+00:00
- **Authors**: Fei Yin, Yi-Chao Wu, Xu-Yao Zhang, Cheng-Lin Liu
- **Comment**: 10 pages,4 figures
- **Journal**: None
- **Summary**: Scene text recognition has attracted great interests from the computer vision and pattern recognition community in recent years. State-of-the-art methods use concolutional neural networks (CNNs), recurrent neural networks with long short-term memory (RNN-LSTM) or the combination of them. In this paper, we investigate the intrinsic characteristics of text recognition, and inspired by human cognition mechanisms in reading texts, we propose a scene text recognition method with character models on convolutional feature map. The method simultaneously detects and recognizes characters by sliding the text line image with character models, which are learned end-to-end on text line images labeled with text transcripts. The character classifier outputs on the sliding windows are normalized and decoded with Connectionist Temporal Classification (CTC) based algorithm. Compared to previous methods, our method has a number of appealing properties: (1) It avoids the difficulty of character segmentation which hinders the performance of segmentation-based recognition methods; (2) The model can be trained simply and efficiently because it avoids gradient vanishing/exploding in training RNN-LSTM based models; (3) It bases on character models trained free of lexicon, and can recognize unknown words. (4) The recognition process is highly parallel and enables fast recognition. Our experiments on several challenging English and Chinese benchmarks, including the IIIT-5K, SVT, ICDAR03/13 and TRW15 datasets, demonstrate that the proposed method yields superior or comparable performance to state-of-the-art methods while the model size is relatively small.



### Deep learning from crowds
- **Arxiv ID**: http://arxiv.org/abs/1709.01779v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.01779v2)
- **Published**: 2017-09-06 11:41:19+00:00
- **Updated**: 2017-12-25 12:30:12+00:00
- **Authors**: Filipe Rodrigues, Francisco Pereira
- **Comment**: 10 pages, The Thirty-Second AAAI Conference on Artificial
  Intelligence (AAAI), 2018
- **Journal**: None
- **Summary**: Over the last few years, deep learning has revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains. However, as the size of supervised artificial neural networks grows, typically so does the need for larger labeled datasets. Recently, crowdsourcing has established itself as an efficient and cost-effective solution for labeling large sets of data in a scalable manner, but it often requires aggregating labels from multiple noisy contributors with different levels of expertise. In this paper, we address the problem of learning deep neural networks from crowds. We begin by describing an EM algorithm for jointly learning the parameters of the network and the reliabilities of the annotators. Then, a novel general-purpose crowd layer is proposed, which allows us to train deep neural networks end-to-end, directly from the noisy labels of multiple annotators, using only backpropagation. We empirically show that the proposed approach is able to internally capture the reliability and biases of different annotators and achieve new state-of-the-art results for various crowdsourced datasets across different settings, namely classification, regression and sequence labeling.



### Automatic Document Image Binarization using Bayesian Optimization
- **Arxiv ID**: http://arxiv.org/abs/1709.01782v3
- **DOI**: 10.1145/3151509.3151520
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1709.01782v3)
- **Published**: 2017-09-06 11:47:31+00:00
- **Updated**: 2017-10-21 11:20:24+00:00
- **Authors**: Ekta Vats, Anders Hast, Prashant Singh
- **Comment**: None
- **Journal**: 4th International Workshop on Historical Document Imaging and
  Processing (HIP2017). ACM, New York, NY, USA, 89-94
- **Summary**: Document image binarization is often a challenging task due to various forms of degradation. Although there exist several binarization techniques in literature, the binarized image is typically sensitive to control parameter settings of the employed technique. This paper presents an automatic document image binarization algorithm to segment the text from heavily degraded document images. The proposed technique uses a two band-pass filtering approach for background noise removal, and Bayesian optimization for automatic hyperparameter selection for optimal results. The effectiveness of the proposed binarization technique is empirically demonstrated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets.



### Cross-Domain Image Retrieval with Attention Modeling
- **Arxiv ID**: http://arxiv.org/abs/1709.01784v1
- **DOI**: 10.1145/3123266.3123429
- **Categories**: **cs.MM**, cs.CV, cs.IR, 68, I.4.7; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1709.01784v1)
- **Published**: 2017-09-06 11:49:46+00:00
- **Updated**: 2017-09-06 11:49:46+00:00
- **Authors**: Xin Ji, Wei Wang, Meihui Zhang, Yang Yang
- **Comment**: 8 pages with an extra reference page
- **Journal**: 2017 ACM Multimedia Conference
- **Summary**: With the proliferation of e-commerce websites and the ubiquitousness of smart phones, cross-domain image retrieval using images taken by smart phones as queries to search products on e-commerce websites is emerging as a popular application. One challenge of this task is to locate the attention of both the query and database images. In particular, database images, e.g. of fashion products, on e-commerce websites are typically displayed with other accessories, and the images taken by users contain noisy background and large variations in orientation and lighting. Consequently, their attention is difficult to locate. In this paper, we exploit the rich tag information available on the e-commerce websites to locate the attention of database images. For query images, we use each candidate image in the database as the context to locate the query attention. Novel deep convolutional neural network architectures, namely TagYNet and CtxYNet, are proposed to learn the attention weights and then extract effective representations of the images. Experimental results on public datasets confirm that our approaches have significant improvement over the existing methods in terms of the retrieval accuracy and efficiency.



### Radial Line Fourier Descriptor for Historical Handwritten Text Representation
- **Arxiv ID**: http://arxiv.org/abs/1709.01788v4
- **DOI**: 10.24132/JWSCG.2018.26.1.4
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1709.01788v4)
- **Published**: 2017-09-06 11:56:34+00:00
- **Updated**: 2018-03-20 15:23:08+00:00
- **Authors**: Anders Hast, Ekta Vats
- **Comment**: under review
- **Journal**: None
- **Summary**: Automatic recognition of historical handwritten manuscripts is a daunting task due to paper degradation over time. Recognition-free retrieval or word spotting is popularly used for information retrieval and digitization of the historical handwritten documents. However, the performance of word spotting algorithms depends heavily on feature detection and representation methods. Although there exist popular feature descriptors such as Scale Invariant Feature Transform (SIFT) and Speeded Up Robust Features (SURF), the invariant properties of these descriptors amplify the noise in the degraded document images, rendering them more sensitive to noise and complex characteristics of historical manuscripts. Therefore, an efficient and relaxed feature descriptor is required as handwritten words across different documents are indeed similar, but not identical. This paper introduces a Radial Line Fourier (RLF) descriptor for handwritten word representation, with a short feature vector of 32 dimensions. A segmentation-free and training-free handwritten word spotting method is studied herein that relies on the proposed RLF descriptor, takes into account different keypoint representations and uses a simple preconditioner-based feature matching algorithm. The effectiveness of the RLF descriptor for segmentation-free handwritten word spotting is empirically evaluated on well-known historical handwritten datasets using standard evaluation measures.



### A Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging
- **Arxiv ID**: http://arxiv.org/abs/1709.01922v3
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1709.01922v3)
- **Published**: 2017-09-06 12:44:01+00:00
- **Updated**: 2021-02-22 13:21:38+00:00
- **Authors**: Keunwoo Choi, György Fazekas, Kyunghyun Cho, Mark Sandler
- **Comment**: 5 pages. EUSIPCO 2018 camera-ready. arXiv:1706.02361 does not have
  the overlapped part with this submission anymore
- **Journal**: None
- **Summary**: In this paper, we empirically investigate the effect of audio preprocessing on music tagging with deep neural networks. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting, and scaling. We show that many commonly used input preprocessing techniques are redundant except magnitude compression.



### CNN-Based Projected Gradient Descent for Consistent Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1709.01809v1
- **DOI**: 10.1109/TMI.2018.2832656
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01809v1)
- **Published**: 2017-09-06 12:55:56+00:00
- **Updated**: 2017-09-06 12:55:56+00:00
- **Authors**: Harshit Gupta, Kyong Hwan Jin, Ha Q. Nguyen, Michael T. McCann, Michael Unser
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp.
  1440-1453, June 2018
- **Summary**: We present a new method for image reconstruction which replaces the projector in a projected gradient descent (PGD) with a convolutional neural network (CNN). CNNs trained as high-dimensional (image-to-image) regressors have recently been used to efficiently solve inverse problems in imaging. However, these approaches lack a feedback mechanism to enforce that the reconstructed image is consistent with the measurements. This is crucial for inverse problems, and more so in biomedical imaging, where the reconstructions are used for diagnosis. In our scheme, the gradient descent enforces measurement consistency, while the CNN recursively projects the solution closer to the space of desired reconstruction images. We provide a formal framework to ensure that the classical PGD converges to a local minimizer of a non-convex constrained least-squares problem. When the projector is replaced with a CNN, we propose a relaxed PGD, which always converges. Finally, we propose a simple scheme to train a CNN to act like a projector. Our experiments on sparse view Computed Tomography (CT) reconstruction for both noiseless and noisy measurements show an improvement over the total-variation (TV) method and a recent CNN-based technique.



### Towards Automated Cadastral Boundary Delineation from UAV Data
- **Arxiv ID**: http://arxiv.org/abs/1709.01813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01813v1)
- **Published**: 2017-09-06 13:24:41+00:00
- **Updated**: 2017-09-06 13:24:41+00:00
- **Authors**: Sophie Crommelinck, Michael Ying Yang, Mila Koeva, Markus Gerke, Rohan Bennett, George Vosselman
- **Comment**: Report on current state (August 2017) of PhD work of first author.
  Further info: https://its4land.com/automate-it-wp5/
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAV) are evolving as an alternative tool to acquire land tenure data. UAVs can capture geospatial data at high quality and resolution in a cost-effective, transparent and flexible manner, from which visible land parcel boundaries, i.e., cadastral boundaries are delineable. This delineation is to no extent automated, even though physical objects automatically retrievable through image analysis methods mark a large portion of cadastral boundaries. This study proposes (i) a workflow that automatically extracts candidate cadastral boundaries from UAV orthoimages and (ii) a tool for their semi-automatic processing to delineate final cadastral boundaries. The workflow consists of two state-of-the-art computer vision methods, namely gPb contour detection and SLIC superpixels that are transferred to remote sensing in this study. The tool combines the two methods, allows a semi-automatic final delineation and is implemented as a publicly available QGIS plugin. The approach does not yet aim to provide a comparable alternative to manual cadastral mapping procedures. However, the methodological development of the tool towards this goal is developed in this paper. A study with 13 volunteers investigates the design and implementation of the approach and gathers initial qualitative as well as quantitate results. The study revealed points for improvement, which are prioritized based on the study results and which will be addressed in future work.



### Soft Proposal Networks for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1709.01829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01829v1)
- **Published**: 2017-09-06 14:11:59+00:00
- **Updated**: 2017-09-06 14:11:59+00:00
- **Authors**: Yi Zhu, Yanzhao Zhou, Qixiang Ye, Qiang Qiu, Jianbin Jiao
- **Comment**: ICCV 2017
- **Journal**: None
- **Summary**: Weakly supervised object localization remains challenging, where only image labels instead of bounding boxes are available during training. Object proposal is an effective component in localization, but often computationally expensive and incapable of joint optimization with some of the remaining modules. In this paper, to the best of our knowledge, we for the first time integrate weakly supervised object proposal into convolutional neural networks (CNNs) in an end-to-end learning manner. We design a network component, Soft Proposal (SP), to be plugged into any standard convolutional architecture to introduce the nearly cost-free object proposal, orders of magnitude faster than state-of-the-art methods. In the SP-augmented CNNs, referred to as Soft Proposal Networks (SPNs), iteratively evolved object proposals are generated based on the deep feature maps then projected back, and further jointly optimized with network parameters, with image-level supervision only. Through the unified learning process, SPNs learn better object-centric filters, discover more discriminative visual evidence, and suppress background interference, significantly boosting both weakly supervised object localization and classification performance. We report the best results on popular benchmarks, including PASCAL VOC, MS COCO, and ImageNet.



### Adaptive Real-Time Removal of Impulse Noise in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1709.02270v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02270v2)
- **Published**: 2017-09-06 14:37:20+00:00
- **Updated**: 2017-10-12 09:06:38+00:00
- **Authors**: Zohreh HosseinKhani, Mohsen Hajabdollahi, Nader Karimi, Reza Soroushmehr, Shahram Shirani, Kayvan Najarian, Shadrokh Samavi
- **Comment**: 9 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: Noise is an important factor that degrades the quality of medical images. Impulse noise is a common noise, which is caused by malfunctioning of sensor elements or errors in the transmission of images. In medical images due to presence of white foreground and black background, many pixels have intensities similar to impulse noise and distinction between noisy and regular pixels is difficult. In software techniques, the accuracy of the noise removal is more important than the algorithm's complexity. But for hardware implementation having a low complexity algorithm with an acceptable accuracy is essential. In this paper a low complexity de-noising method is proposed that removes the noise by local analysis of the image blocks. The proposed method distinguishes non-noisy pixels that have noise-like intensities. All steps are designed to have low hardware complexity. Simulation results show that for different magnetic resonance images, the proposed method removes impulse noise with an acceptable accuracy.



### An inner-loop free solution to inverse problems using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01841v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01841v3)
- **Published**: 2017-09-06 14:41:33+00:00
- **Updated**: 2017-11-14 23:00:28+00:00
- **Authors**: Qi Wei, Kai Fan, Lawrence Carin, Katherine A. Heller
- **Comment**: None
- **Journal**: 2017 Conference on Neural Information Processing Systems (NIPS)
- **Summary**: We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.



### Clustering of Data with Missing Entries using Non-convex Fusion Penalties
- **Arxiv ID**: http://arxiv.org/abs/1709.01870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1709.01870v1)
- **Published**: 2017-09-06 15:59:57+00:00
- **Updated**: 2017-09-06 15:59:57+00:00
- **Authors**: Sunrita Poddar, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: The presence of missing entries in data often creates challenges for pattern recognition algorithms. Traditional algorithms for clustering data assume that all the feature values are known for every data point. We propose a method to cluster data in the presence of missing information. Unlike conventional clustering techniques where every feature is known for each point, our algorithm can handle cases where a few feature values are unknown for every point. For this more challenging problem, we provide theoretical guarantees for clustering using a $\ell_0$ fusion penalty based optimization problem. Furthermore, we propose an algorithm to solve a relaxation of this problem using saturating non-convex fusion penalties. It is observed that this algorithm produces solutions that degrade gradually with an increase in the fraction of missing feature values. We demonstrate the utility of the proposed method using a simulated dataset, the Wine dataset and also an under-sampled cardiac MRI dataset. It is shown that the proposed method is a promising clustering technique for datasets with large fractions of missing entries.



### Synthetic Medical Images from Dual Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01872v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01872v3)
- **Published**: 2017-09-06 16:07:30+00:00
- **Updated**: 2018-01-08 20:32:51+00:00
- **Authors**: John T. Guibas, Tejpal S. Virdi, Peter S. Li
- **Comment**: First two authors contributed equally. Accepted to NIPS 2017
  Workshops on Medical Imaging and Machine Learning for Health
- **Journal**: None
- **Summary**: Currently there is strong interest in data-driven approaches to medical image classification. However, medical imaging data is scarce, expensive, and fraught with legal concerns regarding patient privacy. Typical consent forms only allow for patient data to be used in medical journals or education, meaning the majority of medical data is inaccessible for general public research. We propose a novel, two-stage pipeline for generating synthetic medical images from a pair of generative adversarial networks, tested in practice on retinal fundi images. We develop a hierarchical generation process to divide the complex image generation task into two parts: geometry and photorealism. We hope researchers will use our pipeline to bring private medical data into the public domain, sparking growth in imaging tasks that have previously relied on the hand-tuning of models. We have begun this initiative through the development of SynthMed, an online repository for synthetic medical images.



### Polar Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1709.01889v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01889v3)
- **Published**: 2017-09-06 16:43:59+00:00
- **Updated**: 2018-02-01 06:19:51+00:00
- **Authors**: Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, Kostas Daniilidis
- **Comment**: Accepted as a conference paper at ICLR 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.



### Learning Dilation Factors for Semantic Segmentation of Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/1709.01956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01956v1)
- **Published**: 2017-09-06 18:19:10+00:00
- **Updated**: 2017-09-06 18:19:10+00:00
- **Authors**: Yang He, Margret Keuper, Bernt Schiele, Mario Fritz
- **Comment**: GCPR2017
- **Journal**: None
- **Summary**: Contextual information is crucial for semantic segmentation. However, finding the optimal trade-off between keeping desired fine details and at the same time providing sufficiently large receptive fields is non trivial. This is even more so, when objects or classes present in an image significantly vary in size. Dilated convolutions have proven valuable for semantic segmentation, because they allow to increase the size of the receptive field without sacrificing image resolution. However, in current state-of-the-art methods, dilation parameters are hand-tuned and fixed. In this paper, we present an approach for learning dilation parameters adaptively per channel, consistently improving semantic segmentation results on street-scene datasets like Cityscapes and Camvid.



### Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images
- **Arxiv ID**: http://arxiv.org/abs/1709.01993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.01993v1)
- **Published**: 2017-09-06 20:46:17+00:00
- **Updated**: 2017-09-06 20:46:17+00:00
- **Authors**: Hao Zhou, Jin Sun, Yaser Yacoob, David W. Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: Lighting estimation from face images is an important task and has applications in many areas such as image editing, intrinsic image decomposition, and image forgery detection. We propose to train a deep Convolutional Neural Network (CNN) to regress lighting parameters from a single face image. Lacking massive ground truth lighting labels for face images in the wild, we use an existing method to estimate lighting parameters, which are treated as ground truth with unknown noises. To alleviate the effect of such noises, we utilize the idea of Generative Adversarial Networks (GAN) and propose a Label Denoising Adversarial Network (LDAN) to make use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images. Experiments show that our network outperforms existing methods in producing consistent lighting parameters of different faces under similar lighting conditions. Moreover, our method is 100,000 times faster in execution time than prior optimization-based lighting estimation approaches.



### Image Splicing Localization Using A Multi-Task Fully Convolutional Network (MFCN)
- **Arxiv ID**: http://arxiv.org/abs/1709.02016v1
- **DOI**: 10.1016/j.jvcir.2018.01.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1709.02016v1)
- **Published**: 2017-09-06 22:23:05+00:00
- **Updated**: 2017-09-06 22:23:05+00:00
- **Authors**: Ronald Salloum, Yuzhuo Ren, C. -C. Jay Kuo
- **Comment**: This manuscript was submitted for publication
- **Journal**: Journal of Visual Communication and Image Representation, Volume
  51, February 2018, Pages 201-209
- **Summary**: In this work, we propose a technique that utilizes a fully convolutional network (FCN) to localize image splicing attacks. We first evaluated a single-task FCN (SFCN) trained only on the surface label. Although the SFCN is shown to provide superior performance over existing methods, it still provides a coarse localization output in certain cases. Therefore, we propose the use of a multi-task FCN (MFCN) that utilizes two output branches for multi-task learning. One branch is used to learn the surface label, while the other branch is used to learn the edge or boundary of the spliced region. We trained the networks using the CASIA v2.0 dataset, and tested the trained models on the CASIA v1.0, Columbia Uncompressed, Carvalho, and the DARPA/NIST Nimble Challenge 2016 SCI datasets. Experiments show that the SFCN and MFCN outperform existing splicing localization algorithms, and that the MFCN can achieve finer localization than the SFCN.




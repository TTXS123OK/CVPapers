# Arxiv Papers in cs.CV on 2022-06-04
### Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2206.01856v2
- **DOI**: 10.1007/978-3-031-16452-1_53
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01856v2)
- **Published**: 2022-06-04 00:08:58+00:00
- **Updated**: 2022-06-27 05:05:05+00:00
- **Authors**: Calvin-Khang Ta, Abhishek Aich, Akash Gupta, Amit K. Roy-Chowdhury
- **Comment**: Accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Image enhancement approaches often assume that the noise is signal independent, and approximate the degradation model as zero-mean additive Gaussian. However, this assumption does not hold for biomedical imaging systems where sensor-based sources of noise are proportional to signal strengths, and the noise is better represented as a Poisson process. In this work, we explore a sparsity and dictionary learning-based approach and present a novel self-supervised learning method for single-image denoising where the noise is approximated as a Poisson process, requiring no clean ground-truth data. Specifically, we approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network that enforces sparsity with respect to the weights of the network. Since the sparse representations are based on the underlying image, it is able to suppress the spurious components (noise) in the image patches, thereby introducing implicit regularization for denoising tasks through the network structure. Experiments on two bio-imaging datasets demonstrate that our method outperforms the state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that, in addition to higher performance on standard quantitative metrics, we are able to recover much more subtle details than other compared approaches. Our code is made publicly available at https://github.com/tacalvin/Poisson2Sparse



### Image Data collection and implementation of deep learning-based model in detecting Monkeypox disease using modified VGG16
- **Arxiv ID**: http://arxiv.org/abs/2206.01862v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01862v1)
- **Published**: 2022-06-04 00:34:15+00:00
- **Updated**: 2022-06-04 00:34:15+00:00
- **Authors**: Md Manjurul Ahsan, Muhammad Ramiz Uddin, Mithila Farjana, Ahmed Nazmus Sakib, Khondhaker Al Momin, Shahana Akter Luna
- **Comment**: None
- **Journal**: None
- **Summary**: While the world is still attempting to recover from the damage caused by the broad spread of COVID-19, the Monkeypox virus poses a new threat of becoming a global pandemic. Although the Monkeypox virus itself is not deadly and contagious as COVID-19, still every day, new patients case has been reported from many nations. Therefore, it will be no surprise if the world ever faces another global pandemic due to the lack of proper precautious steps. Recently, Machine learning (ML) has demonstrated huge potential in image-based diagnoses such as cancer detection, tumor cell identification, and COVID-19 patient detection. Therefore, a similar application can be adopted to diagnose the Monkeypox-related disease as it infected the human skin, which image can be acquired and further used in diagnosing the disease. Considering this opportunity, in this work, we introduce a newly developed "Monkeypox2022" dataset that is publicly available to use and can be obtained from our shared GitHub repository. The dataset is created by collecting images from multiple open-source and online portals that do not impose any restrictions on use, even for commercial purposes, hence giving a safer path to use and disseminate such data when constructing and deploying any type of ML model. Further, we propose and evaluate a modified VGG16 model, which includes two distinct studies: Study One and Two. Our exploratory computational results indicate that our suggested model can identify Monkeypox patients with an accuracy of $97\pm1.8\%$ (AUC=97.2) and $88\pm0.8\%$ (AUC=0.867) for Study One and Two, respectively. Additionally, we explain our model's prediction and feature extraction utilizing Local Interpretable Model-Agnostic Explanations (LIME) help to a deeper insight into specific features that characterize the onset of the Monkeypox virus.



### Recursive Deformable Image Registration Network with Mutual Attention
- **Arxiv ID**: http://arxiv.org/abs/2206.01863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01863v2)
- **Published**: 2022-06-04 00:35:14+00:00
- **Updated**: 2022-06-30 11:56:27+00:00
- **Authors**: Jian-Qing Zheng, Ziyang Wang, Baoru Huang, Ngee Han Lim, Tonia Vincent, Bartlomiej W. Papiez
- **Comment**: arXiv admin note: text overlap with arXiv:2203.04290
- **Journal**: None
- **Summary**: Deformable image registration, estimating the spatial transformation between different images, is an important task in medical imaging. Many previous studies have used learning-based methods for multi-stage registration to perform 3D image registration to improve performance. The performance of the multi-stage approach, however, is limited by the size of the receptive field where complex motion does not occur at a single spatial scale. We propose a new registration network combining recursive network architecture and mutual attention mechanism to overcome these limitations. Compared with the state-of-the-art deep learning methods, our network based on the recursive structure achieves the highest accuracy in lung Computed Tomography (CT) data set (Dice score of 92\% and average surface distance of 3.8mm for lungs) and one of the most accurate results in abdominal CT data set with 9 organs of various sizes (Dice score of 55\% and average surface distance of 7.8mm). We also showed that adding 3 recursive networks is sufficient to achieve the state-of-the-art results without a significant increase in the inference time.



### SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space
- **Arxiv ID**: http://arxiv.org/abs/2206.01867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01867v1)
- **Published**: 2022-06-04 00:51:00+00:00
- **Updated**: 2022-06-04 00:51:00+00:00
- **Authors**: Zihan Wang, Ruimin Chen, Mengxuan Liu, Guanfang Dong, Anup Basu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method SPGNet for 3D human pose estimation that mixes multi-dimensional re-projection into supervised learning. In this method, the 2D-to-3D-lifting network predicts the global position and coordinates of the 3D human pose. Then, we re-project the estimated 3D pose back to the 2D key points along with spatial adjustments. The loss functions compare the estimated 3D pose with the 3D pose ground truth, and re-projected 2D pose with the input 2D pose. In addition, we propose a kinematic constraint to restrict the predicted target with constant human bone length. Based on the estimation results for the dataset Human3.6M, our approach outperforms many state-of-the-art methods both qualitatively and quantitatively.



### Face Recognition Accuracy Across Demographics: Shining a Light Into the Problem
- **Arxiv ID**: http://arxiv.org/abs/2206.01881v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01881v2)
- **Published**: 2022-06-04 02:36:35+00:00
- **Updated**: 2023-04-16 14:43:00+00:00
- **Authors**: Haiyu Wu, Vítor Albiero, K. S. Krishnapriya, Michael C. King, Kevin W. Bowyer
- **Comment**: None
- **Journal**: None
- **Summary**: We explore varying face recognition accuracy across demographic groups as a phenomenon partly caused by differences in face illumination. We observe that for a common operational scenario with controlled image acquisition, there is a large difference in face region brightness between African-American and Caucasian, and also a smaller difference between male and female. We show that impostor image pairs with both faces under-exposed, or both overexposed, have an increased false match rate (FMR). Conversely, image pairs with strongly different face brightness have a decreased similarity measure. We propose a brightness information metric to measure variation in brightness in the face and show that face brightness that is too low or too high has reduced information in the face region, providing a cause for the lower accuracy. Based on this, for operational scenarios with controlled image acquisition, illumination should be adjusted for each individual to obtain appropriate face image brightness. This is the first work that we are aware of to explore how the level of brightness of the skin region in a pair of face images (rather than a single image) impacts face recognition accuracy, and to evaluate this as a systematic factor causing unequal accuracy across demographics. The code is at https://github.com/HaiyuWu/FaceBrightness.



### A Superimposed Divide-and-Conquer Image Recognition Method for SEM Images of Nanoparticles on The Surface of Monocrystalline silicon with High Aggregation Degree
- **Arxiv ID**: http://arxiv.org/abs/2206.01884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.01884v1)
- **Published**: 2022-06-04 02:42:57+00:00
- **Updated**: 2022-06-04 02:42:57+00:00
- **Authors**: Ruiling Xiao, Jiayang Niu
- **Comment**: None
- **Journal**: None
- **Summary**: The nanoparticle size and distribution information in the SEM images of silicon crystals are generally counted by manual methods. The realization of automatic machine recognition is significant in materials science. This paper proposed a superposition partitioning image recognition method to realize automatic recognition and information statistics of silicon crystal nanoparticle SEM images. Especially for the complex and highly aggregated characteristics of silicon crystal particle size, an accurate recognition step and contour statistics method based on morphological processing are given. This method has technical reference value for the recognition of Monocrystalline silicon surface nanoparticle images under different SEM shooting conditions. Besides, it outperforms other methods in terms of recognition accuracy and algorithm efficiency.



### Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients
- **Arxiv ID**: http://arxiv.org/abs/2206.01897v1
- **DOI**: 10.1109/ISBI48211.2021.9434053
- **Categories**: **eess.IV**, cs.AI, cs.CV, q-bio.GN, q-bio.QM, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2206.01897v1)
- **Published**: 2022-06-04 03:52:12+00:00
- **Updated**: 2022-06-04 03:52:12+00:00
- **Authors**: Ahmad Chaddad, Mingli Zhang, Lama Hassan, Tamim Niazi
- **Comment**: None
- **Journal**: None
- **Summary**: Radiomics has shown a capability for different types of cancers such as glioma to predict the clinical outcome. It can have a non-invasive means of evaluating the immunotherapy response prior to treatment. However, the use of deep convolutional neural networks (CNNs)-based radiomics requires large training image sets. To avoid this problem, we investigate a new imaging features that model distribution with a Gaussian mixture model (GMM) of learned 3D CNN features. Using these deep radiomic features (DRFs), we aim to predict the immune marker status (low versus high) and overall survival for glioma patients. We extract the DRFs by aggregating the activation maps of a pre-trained 3D-CNN within labeled tumor regions of MRI scans that corresponded immune markers of 151 patients. Our experiments are performed to assess the relationship between the proposed DRFs, three immune cell markers (Macrophage M1, Neutrophils and T Cells Follicular Helper), and measure their association with overall survival. Using the random forest (RF) model, DRFs was able to predict the immune marker status with area under the ROC curve (AUC) of 78.67, 83.93 and 75.67\% for Macrophage M1, Neutrophils and T Cells Follicular Helper, respectively. Combined the immune markers with DRFs and clinical variables, Kaplan-Meier estimator and Log-rank test achieved the most significant difference between predicted groups of patients (short-term versus long-term survival) with p\,=\,4.31$\times$10$^{-7}$ compared to p\,=\,0.03 for Immune cell markers, p\,=\,0.07 for clinical variables , and p\,=\,1.45$\times$10$^{-5}$ for DRFs. Our findings indicate that the proposed features (DRFs) used in RF models may significantly consider prognosticating patients with brain tumour prior to surgery through regularly acquired imaging data.



### Saliency Attack: Towards Imperceptible Black-box Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2206.01898v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2206.01898v1)
- **Published**: 2022-06-04 03:56:07+00:00
- **Updated**: 2022-06-04 03:56:07+00:00
- **Authors**: Zeyu Dai, Shengcai Liu, Ke Tang, Qing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial examples, even in the black-box setting where the attacker is only accessible to the model output. Recent studies have devised effective black-box attacks with high query efficiency. However, such performance is often accompanied by compromises in attack imperceptibility, hindering the practical use of these approaches. In this paper, we propose to restrict the perturbations to a small salient region to generate adversarial examples that can hardly be perceived. This approach is readily compatible with many existing black-box attacks and can significantly improve their imperceptibility with little degradation in attack success rate. Further, we propose the Saliency Attack, a new black-box attack aiming to refine the perturbations in the salient region to achieve even better imperceptibility. Extensive experiments show that compared to the state-of-the-art black-box attacks, our approach achieves much better imperceptibility scores, including most apparent distortion (MAD), $L_0$ and $L_2$ distances, and also obtains significantly higher success rates judged by a human-like threshold on MAD. Importantly, the perturbations generated by our approach are interpretable to some extent. Finally, it is also demonstrated to be robust to different detection-based defenses.



### Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2206.01903v1
- **DOI**: 10.1109/TNNLS.2021.3119071
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2206.01903v1)
- **Published**: 2022-06-04 04:10:21+00:00
- **Updated**: 2022-06-04 04:10:21+00:00
- **Authors**: Ahmad Chaddad, Lama Hassan, Christian Desrosiers
- **Comment**: None
- **Journal**: IEEE Trans Neural Netw Learn Syst. 2022 Jan;33(1):3-11
- **Summary**: This paper proposes to encode the distribution of features learned from a convolutional neural network using a Gaussian Mixture Model. These parametric features, called GMM-CNN, are derived from chest computed tomography and X-ray scans of patients with Coronavirus Disease 2019. We use the proposed GMM-CNN features as input to a robust classifier based on random forests to differentiate between COVID-19 and other pneumonia cases. Our experiments assess the advantage of GMM-CNN features compared to standard CNN classification on test images. Using a random forest classifier (80\% samples for training; 20\% samples for testing), GMM-CNN features encoded with two mixture components provided a significantly better performance than standard CNN classification (p\,$<$\,0.05). Specifically, our method achieved an accuracy in the range of 96.00\,--\,96.70\% and an area under the ROC curve in the range of 99.29\,--\,99.45\%, with the best performance obtained by combining GMM-CNN features from both computed tomography and X-ray images. Our results suggest that the proposed GMM-CNN features could improve the prediction of COVID-19 in chest computed tomography and X-ray scans.



### Video-based Human-Object Interaction Detection from Tubelet Tokens
- **Arxiv ID**: http://arxiv.org/abs/2206.01908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01908v1)
- **Published**: 2022-06-04 04:27:59+00:00
- **Updated**: 2022-06-04 04:27:59+00:00
- **Authors**: Danyang Tu, Wei Sun, Xiongkuo Min, Guangtao Zhai, Wei Shen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel vision Transformer, named TUTOR, which is able to learn tubelet tokens, served as highly-abstracted spatiotemporal representations, for video-based human-object interaction (V-HOI) detection. The tubelet tokens structurize videos by agglomerating and linking semantically-related patch tokens along spatial and temporal domains, which enjoy two benefits: 1) Compactness: each tubelet token is learned by a selective attention mechanism to reduce redundant spatial dependencies from others; 2) Expressiveness: each tubelet token is enabled to align with a semantic instance, i.e., an object or a human, across frames, thanks to agglomeration and linking. The effectiveness and efficiency of TUTOR are verified by extensive experiments. Results shows our method outperforms existing works by large margins, with a relative mAP gain of $16.14\%$ on VidHOI and a 2 points gain on CAD-120 as well as a $4 \times$ speedup.



### The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2206.01910v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.01910v2)
- **Published**: 2022-06-04 04:37:56+00:00
- **Updated**: 2022-06-07 05:33:19+00:00
- **Authors**: Zihao Zhao, Yanhong Wang, Qiaosha Zou, Tie Xu, Fangbo Tao, Jiansong Zhang, Xiaoan Wang, C. -J. Richard Shi, Junwen Luo, Yuan Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition is an exciting research avenue for artificial intelligence since it may be a game changer in the emerging industrial fields such as robotic visions and automobiles. However, current deep learning faces major challenges for such applications because of the huge computational cost and the inefficient learning. Hence, we develop a novel brain-inspired Spiking Neural Network (SNN) based system titled Spiking Gating Flow (SGF) for online action learning. The developed system consists of multiple SGF units which assembled in a hierarchical manner. A single SGF unit involves three layers: a feature extraction layer, an event-driven layer and a histogram-based training layer. To demonstrate the developed system capabilities, we employ a standard Dynamic Vision Sensor (DVS) gesture classification as a benchmark. The results indicate that we can achieve 87.5% accuracy which is comparable with Deep Learning (DL), but at smaller training/inference data number ratio 1.5:1. And only a single training epoch is required during the learning process. Meanwhile, to the best of our knowledge, this is the highest accuracy among the non-backpropagation algorithm based SNNs. At last, we conclude the few-shot learning paradigm of the developed network: 1) a hierarchical structure-based network design involves human prior knowledge; 2) SNNs for content based global dynamic feature detection.



### NeMF: Neural Motion Fields for Kinematic Animation
- **Arxiv ID**: http://arxiv.org/abs/2206.03287v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.03287v3)
- **Published**: 2022-06-04 05:53:27+00:00
- **Updated**: 2022-10-09 08:20:44+00:00
- **Authors**: Chengan He, Jun Saito, James Zachary, Holly Rushmeier, Yi Zhou
- **Comment**: Accepted to NeurIPS 2022. Project page:
  https://cs.yale.edu/homes/che/projects/nemf/
- **Journal**: None
- **Summary**: We present an implicit neural representation to learn the spatio-temporal space of kinematic motions. Unlike previous work that represents motion as discrete sequential samples, we propose to express the vast motion space as a continuous function over time, hence the name Neural Motion Fields (NeMF). Specifically, we use a neural network to learn this function for miscellaneous sets of motions, which is designed to be a generative model conditioned on a temporal coordinate $t$ and a random vector $z$ for controlling the style. The model is then trained as a Variational Autoencoder (VAE) with motion encoders to sample the latent space. We train our model with a diverse human motion dataset and quadruped dataset to prove its versatility, and finally deploy it as a generic motion prior to solve task-agnostic problems and show its superiority in different motion generation and editing applications, such as motion interpolation, in-betweening, and re-navigating. More details can be found on our project page: https://cs.yale.edu/homes/che/projects/nemf/.



### Nerfels: Renderable Neural Codes for Improved Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2206.01916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01916v1)
- **Published**: 2022-06-04 06:29:46+00:00
- **Updated**: 2022-06-04 06:29:46+00:00
- **Authors**: Gil Avraham, Julian Straub, Tianwei Shen, Tsun-Yi Yang, Hugo Germain, Chris Sweeney, Vasileios Balntas, David Novotny, Daniel DeTone, Richard Newcombe
- **Comment**: Published at CVPRW with supplementary material
- **Journal**: None
- **Summary**: This paper presents a framework that combines traditional keypoint-based camera pose optimization with an invertible neural rendering mechanism. Our proposed 3D scene representation, Nerfels, is locally dense yet globally sparse. As opposed to existing invertible neural rendering systems which overfit a model to the entire scene, we adopt a feature-driven approach for representing scene-agnostic, local 3D patches with renderable codes. By modelling a scene only where local features are detected, our framework effectively generalizes to unseen local regions in the scene via an optimizable code conditioning mechanism in the neural renderer, all while maintaining the low memory footprint of a sparse 3D map representation. Our model can be incorporated to existing state-of-the-art hand-crafted and learned local feature pose estimators, yielding improved performance when evaluating on ScanNet for wide camera baseline scenarios.



### From Pixels to Objects: Cubic Visual Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2206.01923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01923v1)
- **Published**: 2022-06-04 07:03:18+00:00
- **Updated**: 2022-06-04 07:03:18+00:00
- **Authors**: Jingkuan Song, Pengpeng Zeng, Lianli Gao, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, attention-based Visual Question Answering (VQA) has achieved great success by utilizing question to selectively target different visual areas that are related to the answer. Existing visual attention models are generally planar, i.e., different channels of the last conv-layer feature map of an image share the same weight. This conflicts with the attention mechanism because CNN features are naturally spatial and channel-wise. Also, visual attention models are usually conducted on pixel-level, which may cause region discontinuous problems. In this paper, we propose a Cubic Visual Attention (CVA) model by successfully applying a novel channel and spatial attention on object regions to improve VQA task. Specifically, instead of attending to pixels, we first take advantage of the object proposal networks to generate a set of object candidates and extract their associated conv features. Then, we utilize the question to guide channel attention and spatial attention calculation based on the con-layer feature map. Finally, the attended visual features and the question are combined to infer the answer. We assess the performance of our proposed CVA on three public image QA datasets, including COCO-QA, VQA and Visual7W. Experimental results show that our proposed method significantly outperforms the state-of-the-arts.



### Occlusion-Resistant Instance Segmentation of Piglets in Farrowing Pens Using Center Clustering Network
- **Arxiv ID**: http://arxiv.org/abs/2206.01942v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01942v3)
- **Published**: 2022-06-04 08:43:30+00:00
- **Updated**: 2023-02-17 08:19:41+00:00
- **Authors**: Endai Huang, Axiu Mao, Junhui Hou, Yongjian Wu, Weitao Xu, Maria Camila Ceballos, Thomas D. Parsons, Kai Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision enables the development of new approaches to monitor the behavior, health, and welfare of animals. Instance segmentation is a high-precision method in computer vision for detecting individual animals of interest. This method can be used for in-depth analysis of animals, such as examining their subtle interactive behaviors, from videos and images. However, existing deep-learning-based instance segmentation methods have been mostly developed based on public datasets, which largely omit heavy occlusion problems; therefore, these methods have limitations in real-world applications involving object occlusions, such as farrowing pen systems used on pig farms in which the farrowing crates often impede the sow and piglets. In this paper, we adapt a Center Clustering Network originally designed for counting to achieve instance segmentation, dubbed as CClusnet-Inseg. Specifically, CClusnet-Inseg uses each pixel to predict object centers and trace these centers to form masks based on clustering results, which consists of a network for segmentation and center offset vector map, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, Centers-to-Mask (C2M), and Remain-Centers-to-Mask (RC2M) algorithms. In all, 4,600 images were extracted from six videos collected from three closed and three half-open farrowing crates to train and validate our method. CClusnet-Inseg achieves a mean average precision (mAP) of 84.1 and outperforms all other methods compared in this study. We conduct comprehensive ablation studies to demonstrate the advantages and effectiveness of core modules of our method. In addition, we apply CClusnet-Inseg to multi-object tracking for animal monitoring, and the predicted object center that is a conjunct output could serve as an occlusion-resistant representation of the location of an object.



### C$^3$Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/2206.01961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01961v1)
- **Published**: 2022-06-04 10:38:19+00:00
- **Updated**: 2022-06-04 10:38:19+00:00
- **Authors**: Erez Posner, Adi Zholkover, Netanel Frank, Moshe Bouhnik
- **Comment**: None
- **Journal**: None
- **Summary**: 3D colon reconstruction from Optical Colonoscopy (OC) to detect non-examined surfaces remains an unsolved problem. The challenges arise from the nature of optical colonoscopy data, characterized by highly reflective low-texture surfaces, drastic illumination changes and frequent tracking loss. Recent methods demonstrate compelling results, but suffer from: (1) frangible frame-to-frame (or frame-to-model) pose estimation resulting in many tracking failures; or (2) rely on point-based representations at the cost of scan quality. In this paper, we propose a novel reconstruction framework that addresses these issues end to end, which result in both quantitatively and qualitatively accurate and robust 3D colon reconstruction. Our SLAM approach, which employs correspondences based on contrastive deep features, and deep consistent depth maps, estimates globally optimized poses, is able to recover from frequent tracking failures, and estimates a global consistent 3D model; all within a single framework. We perform an extensive experimental evaluation on multiple synthetic and real colonoscopy videos, showing high-quality results and comparisons against relevant baselines.



### Delving into the Openness of CLIP
- **Arxiv ID**: http://arxiv.org/abs/2206.01986v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01986v3)
- **Published**: 2022-06-04 13:07:30+00:00
- **Updated**: 2023-05-07 15:04:28+00:00
- **Authors**: Shuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, Xu Sun
- **Comment**: Accepted by Findings of ACL 2023 (Long Paper). Code is available at
  https://github.com/lancopku/clip-openness
- **Journal**: None
- **Summary**: Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can recognize images from an open class set (also known as an open vocabulary) in a zero-shot manner. However, evaluating the openness of CLIP-like models is challenging, as the models are open to arbitrary vocabulary in theory, but their accuracy varies in practice. To address this, we resort to an incremental perspective to assess the openness through vocabulary expansions, and define extensibility to measure a model's ability to handle novel classes. Our evaluation shows that CLIP-like models are not truly open, and their performance deteriorates as the vocabulary expands. We further dissect the feature space of CLIP from the perspectives of representation alignment and uniformity. Our investigation reveals that the overestimation of openness is due to confusion among competing text features, rather than a failure to capture the similarity between image features and text features of novel classes. We hope that our investigation and analysis will facilitate future research on the CLIP openness issue.



### Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.01988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2206.01988v1)
- **Published**: 2022-06-04 13:16:30+00:00
- **Updated**: 2022-06-04 13:16:30+00:00
- **Authors**: Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan Liang, Xiaojun Chang
- **Comment**: CVPR 2022 (Poster)
- **Journal**: None
- **Summary**: Automatic generation of ophthalmic reports using data-driven neural networks has great potential in clinical practice. When writing a report, ophthalmologists make inferences with prior clinical knowledge. This knowledge has been neglected in prior medical report generation methods. To endow models with the capability of incorporating expert knowledge, we propose a Cross-modal clinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in which clinical relation triples are injected into the visual features as prior knowledge to drive the decoding procedure. However, two major common Knowledge Noise (KN) issues may affect models' effectiveness. 1) Existing general biomedical knowledge bases such as the UMLS may not align meaningfully to the specific context and language of the report, limiting their utility for knowledge injection. 2) Incorporating too much knowledge may divert the visual features from their correct meaning. To overcome these limitations, we design an automatic information extraction scheme based on natural language processing to obtain clinical entities and relations directly from in-domain training reports. Given a set of ophthalmic images, our CGT first restores a sub-graph from the clinical graph and injects the restored triples into visual features. Then visible matrix is employed during the encoding procedure to limit the impact of knowledge. Finally, reports are predicted by the encoded cross-modal features via a Transformer decoder. Extensive experiments on the large-scale FFA-IR benchmark demonstrate that the proposed CGT is able to outperform previous benchmark methods and achieve state-of-the-art performances.



### CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks
- **Arxiv ID**: http://arxiv.org/abs/2206.01992v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.01992v7)
- **Published**: 2022-06-04 13:45:08+00:00
- **Updated**: 2022-12-15 10:15:34+00:00
- **Authors**: Ruiqing Yan, Fan Zhang, Mengyuan Huang, Wu Liu, Dongyu Hu, Jinfeng Li, Qiang Liu, Jinrong Jiang, Qianjin Guo, Linghan Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of object anomalies is crucial in industrial processes, but unsupervised anomaly detection and localization is particularly important due to the difficulty of obtaining a large number of defective samples and the unpredictable types of anomalies in real life. Among the existing unsupervised anomaly detection and localization methods, the NF-based scheme has achieved better results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and $t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze the input visual features from 2D flattening to 1D, destroying the spatial location relationship in the feature map and losing the spatial structure information. In order to retain and effectively extract spatial structure information, we design in this study a complex function model with alternating CBAM embedded in a stacked $3\times3$ full convolution, which is able to retain and effectively extract spatial structure information in the normalized flow model. Extensive experimental results on the MVTec AD dataset show that CAINNFlow achieves advanced levels of accuracy and inference efficiency based on CNN and Transformer backbone networks as feature extractors, and CAINNFlow achieves a pixel-level AUC of $98.64\%$ for anomaly detection in MVTec AD.



### MSR: Making Self-supervised learning Robust to Aggressive Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2206.01999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.01999v1)
- **Published**: 2022-06-04 14:27:29+00:00
- **Updated**: 2022-06-04 14:27:29+00:00
- **Authors**: Yingbin Bai, Erkun Yang, Zhaoqing Wang, Yuxuan Du, Bo Han, Cheng Deng, Dadong Wang, Tongliang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent self-supervised learning methods learn visual representation by contrasting different augmented views of images. Compared with supervised learning, more aggressive augmentations have been introduced to further improve the diversity of training pairs. However, aggressive augmentations may distort images' structures leading to a severe semantic shift problem that augmented views of the same image may not share the same semantics, thus degrading the transfer performance. To address this problem, we propose a new SSL paradigm, which counteracts the impact of semantic shift by balancing the role of weak and aggressively augmented pairs. Specifically, semantically inconsistent pairs are of minority and we treat them as noisy pairs. Note that deep neural networks (DNNs) have a crucial memorization effect that DNNs tend to first memorize clean (majority) examples before overfitting to noisy (minority) examples. Therefore, we set a relatively large weight for aggressively augmented data pairs at the early learning stage. With the training going on, the model begins to overfit noisy pairs. Accordingly, we gradually reduce the weights of aggressively augmented pairs. In doing so, our method can better embrace the aggressive augmentations and neutralize the semantic shift problem. Experiments show that our model achieves 73.1% top-1 accuracy on ImageNet-1K with ResNet-50 for 200 epochs, which is a 2.5% improvement over BYOL. Moreover, experiments also demonstrate that the learned representations can transfer well for various downstream tasks.



### CVNets: High Performance Library for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2206.02002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02002v1)
- **Published**: 2022-06-04 14:55:24+00:00
- **Updated**: 2022-06-04 14:55:24+00:00
- **Authors**: Sachin Mehta, Farzad Abdolhosseini, Mohammad Rastegari
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We introduce CVNets, a high-performance open-source library for training deep neural networks for visual recognition tasks, including classification, detection, and segmentation. CVNets supports image and video understanding tools, including data loading, data transformations, novel data sampling methods, and implementations of several standard networks with similar or better performance than previous studies.   Our source code is available at: \url{https://github.com/apple/ml-cvnets}.



### APES: Articulated Part Extraction from Sprite Sheets
- **Arxiv ID**: http://arxiv.org/abs/2206.02015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2206.02015v1)
- **Published**: 2022-06-04 15:44:04+00:00
- **Updated**: 2022-06-04 15:44:04+00:00
- **Authors**: Zhan Xu, Matthew Fisher, Yang Zhou, Deepali Aneja, Rushikesh Dudhat, Li Yi, Evangelos Kalogerakis
- **Comment**: None
- **Journal**: None
- **Summary**: Rigged puppets are one of the most prevalent representations to create 2D character animations. Creating these puppets requires partitioning characters into independently moving parts. In this work, we present a method to automatically identify such articulated parts from a small set of character poses shown in a sprite sheet, which is an illustration of the character that artists often draw before puppet creation. Our method is trained to infer articulated parts, e.g. head, torso and limbs, that can be re-assembled to best reconstruct the given poses. Our results demonstrate significantly better performance than alternatives qualitatively and quantitatively.Our project page https://zhan-xu.github.io/parts/ includes our code and data.



### Study on Image Filtering -- Techniques, Algorithm and Applications
- **Arxiv ID**: http://arxiv.org/abs/2207.06481v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2207.06481v1)
- **Published**: 2022-06-04 15:54:21+00:00
- **Updated**: 2022-06-04 15:54:21+00:00
- **Authors**: Bhishman Desai, Manish Paliwal, Kapil Kumar Nagwanshi
- **Comment**: None
- **Journal**: None
- **Summary**: Image processing is one of the most immerging and widely growing techniques making it a lively research field. Image processing is converting an image to a digital format and then doing different operations on it, such as improving the image or extracting various valuable data. Image filtering is one of the fascinating applications of image processing. Image filtering is a technique for altering the size, shape, color, depth, smoothness, and other image properties. It alters the pixels of the image to transform it into the desired form using different types of graphical editing methods through graphic design and editing software. This paper introduces various image filtering techniques and their wide applications.



### Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering
- **Arxiv ID**: http://arxiv.org/abs/2206.02027v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2206.02027v2)
- **Published**: 2022-06-04 17:16:09+00:00
- **Updated**: 2022-12-05 00:58:40+00:00
- **Authors**: Tin Vlašić, Hieu Nguyen, AmirEhsan Khorashadizadeh, Ivan Dokmanić
- **Comment**: 6 pages, 8 figures, to be published in 2022 Asilomar Conference on
  Signals, Systems, and Computers
- **Journal**: 2022 Asilomar Conference on Signals, Systems, and Computers
- **Summary**: Implicit representation of shapes as level sets of multilayer perceptrons has recently flourished in different shape analysis, compression, and reconstruction tasks. In this paper, we introduce an implicit neural representation-based framework for solving the inverse obstacle scattering problem in a mesh-free fashion. We express the obstacle shape as the zero-level set of a signed distance function which is implicitly determined by network parameters. To solve the direct scattering problem, we implement the implicit boundary integral method. It uses projections of the grid points in the tubular neighborhood onto the boundary to compute the PDE solution directly in the level-set framework. The proposed implicit representation conveniently handles the shape perturbation in the optimization process. To update the shape, we use PyTorch's automatic differentiation to backpropagate the loss function w.r.t. the network parameters, allowing us to avoid complex and error-prone manual derivation of the shape derivative. Additionally, we propose a deep generative model of implicit neural shape representations that can fit into the framework. The deep generative model effectively regularizes the inverse obstacle scattering problem, making it more tractable and robust, while yielding high-quality reconstruction results even in noise-corrupted setups.



### Guided Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2206.02029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.02029v1)
- **Published**: 2022-06-04 17:34:11+00:00
- **Updated**: 2022-06-04 17:34:11+00:00
- **Authors**: Jorge Gonzalez-Zapata, Ivan Reyes-Amezcua, Daniel Flores-Araiza, Mauricio Mendez-Ruiz, Gilberto Ochoa-Ruiz, Andres Mendez-Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) methods have been proven relevant for visual similarity learning. However, they sometimes lack generalization properties because they are trained often using an inappropriate sample selection strategy or due to the difficulty of the dataset caused by a distributional shift in the data. These represent a significant drawback when attempting to learn the underlying data manifold. Therefore, there is a pressing need to develop better ways of obtaining generalization and representation of the underlying manifold. In this paper, we propose a novel approach to DML that we call Guided Deep Metric Learning, a novel architecture oriented to learning more compact clusters, improving generalization under distributional shifts in DML. This novel architecture consists of two independent models: A multi-branch master model, inspired from a Few-Shot Learning (FSL) perspective, generates a reduced hypothesis space based on prior knowledge from labeled data, which guides or regularizes the decision boundary of a student model during training under an offline knowledge distillation scheme. Experiments have shown that the proposed method is capable of a better manifold generalization and representation to up to 40% improvement (Recall@1, CIFAR10), using guidelines suggested by Musgrave et al. to perform a more fair and realistic comparison, which is currently absent in the literature



### Learning Speaker-specific Lip-to-Speech Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.02050v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2206.02050v2)
- **Published**: 2022-06-04 19:40:02+00:00
- **Updated**: 2022-08-20 18:50:17+00:00
- **Authors**: Munender Varshney, Ravindra Yadav, Vinay P. Namboodiri, Rajesh M Hegde
- **Comment**: Accepted at ICPR 2022
- **Journal**: None
- **Summary**: Understanding the lip movement and inferring the speech from it is notoriously difficult for the common person. The task of accurate lip-reading gets help from various cues of the speaker and its contextual or environmental setting. Every speaker has a different accent and speaking style, which can be inferred from their visual and speech features. This work aims to understand the correlation/mapping between speech and the sequence of lip movement of individual speakers in an unconstrained and large vocabulary. We model the frame sequence as a prior to the transformer in an auto-encoder setting and learned a joint embedding that exploits temporal properties of both audio and video. We learn temporal synchronization using deep metric learning, which guides the decoder to generate speech in sync with input lip movements. The predictive posterior thus gives us the generated speech in speaker speaking style. We have trained our model on the Grid and Lip2Wav Chemistry lecture dataset to evaluate single speaker natural speech generation tasks from lip movement in an unconstrained natural setting. Extensive evaluation using various qualitative and quantitative metrics with human evaluation also shows that our method outperforms the Lip2Wav Chemistry dataset(large vocabulary in an unconstrained setting) by a good margin across almost all evaluation metrics and marginally outperforms the state-of-the-art on GRID dataset.



### Low Power Neuromorphic EMG Gesture Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.02061v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AR, cs.CV, cs.HC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2206.02061v1)
- **Published**: 2022-06-04 22:09:34+00:00
- **Updated**: 2022-06-04 22:09:34+00:00
- **Authors**: Sai Sukruth Bezugam, Ahmed Shaban, Manan Suri
- **Comment**: 3 Pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: EMG (Electromyograph) signal based gesture recognition can prove vital for applications such as smart wearables and bio-medical neuro-prosthetic control. Spiking Neural Networks (SNNs) are promising for low-power, real-time EMG gesture recognition, owing to their inherent spike/event driven spatio-temporal dynamics. In literature, there are limited demonstrations of neuromorphic hardware implementation (at full chip/board/system scale) for EMG gesture classification. Moreover, most literature attempts exploit primitive SNNs based on LIF (Leaky Integrate and Fire) neurons. In this work, we address the aforementioned gaps with following key contributions: (1) Low-power, high accuracy demonstration of EMG-signal based gesture recognition using neuromorphic Recurrent Spiking Neural Networks (RSNN). In particular, we propose a multi-time scale recurrent neuromorphic system based on special double-exponential adaptive threshold (DEXAT) neurons. Our network achieves state-of-the-art classification accuracy (90%) while using ~53% lesser neurons than best reported prior art on Roshambo EMG dataset. (2) A new multi-channel spike encoder scheme for efficient processing of real-valued EMG data on neuromorphic systems. (3) Unique multi-compartment methodology to implement complex adaptive neurons on Intel's dedicated neuromorphic Loihi chip is shown. (4) RSNN implementation on Loihi (Nahuku 32) achieves significant energy/latency benefits of ~983X/19X compared to GPU for batch size as 50.



### PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers
- **Arxiv ID**: http://arxiv.org/abs/2206.02066v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2206.02066v3)
- **Published**: 2022-06-04 23:16:52+00:00
- **Updated**: 2023-04-07 01:10:17+00:00
- **Authors**: Jiacong Xu, Zixiang Xiong, Shankar P. Bhattacharyya
- **Comment**: 11 pages, 9 figures; This paper will be published by CVPR2023 soon,
  please refer to the official version then
- **Journal**: None
- **Summary**: Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-Integral-Derivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel three-branch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best trade-off between inference speed and accuracy and their accuracy surpasses all the existing models with similar inference speed on the Cityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6% mIOU with inference speed of 93.2 FPS on Cityscapes and 80.1% mIOU with speed of 153.7 FPS on CamVid.



### Priors in Deep Image Restoration and Enhancement: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2206.02070v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2206.02070v2)
- **Published**: 2022-06-04 23:33:34+00:00
- **Updated**: 2023-07-26 03:53:19+00:00
- **Authors**: Yunfan Lu, Yiqi Lin, Hao Wu, Yunhao Luo, Xu Zheng, Hui Xiong, Lin Wang
- **Comment**: Preprint. Under review
- **Journal**: None
- **Summary**: Image restoration and enhancement is a process of improving the image quality by removing degradations, such as noise, blur, and resolution degradation. Deep learning (DL) has recently been applied to image restoration and enhancement. Due to its ill-posed property, plenty of works have been explored priors to facilitate training deep neural networks (DNNs). However, the importance of priors has not been systematically studied and analyzed by far in the research community. Therefore, this paper serves as the first study that provides a comprehensive overview of recent advancements in priors for deep image restoration and enhancement. Our work covers five primary contents: (1) A theoretical analysis of priors for deep image restoration and enhancement; (2) A hierarchical and structural taxonomy of priors commonly used in the DL-based methods; (3) An insightful discussion on each prior regarding its principle, potential, and applications; (4) A summary of crucial problems by highlighting the potential future directions, especially adopting the large-scale foundation models as prior, to spark more research in the community; (5) An open-source repository that provides a taxonomy of all mentioned works and code links.




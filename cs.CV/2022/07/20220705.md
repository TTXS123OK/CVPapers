# Arxiv Papers in cs.CV on 2022-07-05
### Attention Guided Network for Salient Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2207.01755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01755v1)
- **Published**: 2022-07-05 01:01:03+00:00
- **Updated**: 2022-07-05 01:01:03+00:00
- **Authors**: Yuhan Lin, Han Sun, Ningzhong Liu, Yetong Bian, Jun Cen, Huiyu Zhou
- **Comment**: accepted by ICANN2022, The code is available at
  https://github.com/NuaaYH/AGNet
- **Journal**: None
- **Summary**: Due to the extreme complexity of scale and shape as well as the uncertainty of the predicted location, salient object detection in optical remote sensing images (RSI-SOD) is a very difficult task. The existing SOD methods can satisfy the detection performance for natural scene images, but they are not well adapted to RSI-SOD due to the above-mentioned image characteristics in remote sensing images. In this paper, we propose a novel Attention Guided Network (AGNet) for SOD in optical RSIs, including position enhancement stage and detail refinement stage. Specifically, the position enhancement stage consists of a semantic attention module and a contextual attention module to accurately describe the approximate location of salient objects. The detail refinement stage uses the proposed self-refinement module to progressively refine the predicted results under the guidance of attention and reverse attention. In addition, the hybrid loss is applied to supervise the training of the network, which can improve the performance of the model from three perspectives of pixel, region and statistics. Extensive experiments on two popular benchmarks demonstrate that AGNet achieves competitive performance compared to other state-of-the-art methods. The code will be available at https://github.com/NuaaYH/AGNet.



### Universal Domain Adaptive Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2207.01756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01756v1)
- **Published**: 2022-07-05 01:01:19+00:00
- **Updated**: 2022-07-05 01:01:19+00:00
- **Authors**: Wenxu Shi, Lei Zhang, Weijie Chen, Shiliang Pu
- **Comment**: Accepted to ACM MM2022
- **Journal**: None
- **Summary**: Universal domain adaptive object detection (UniDAOD)is more challenging than domain adaptive object detection (DAOD) since the label space of the source domain may not be the same as that of the target and the scale of objects in the universal scenarios can vary dramatically (i.e, category shift and scale shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer effect during training while maximizing transferability as well as discriminability in both domains under a variety of scales. Specifically, our method is implemented by two modules: 1) We facilitate the feature alignment of common classes and suppress the interference of private classes by designing a Filter Mechanism module to overcome the negative transfer caused by category shift. 2) We fill the blank of scale-aware adaptation in object detection by introducing a new Multi-Label Scale-Aware Adapter to perform individual alignment between the corresponding scale for two domains. Experiments show that US-DAF achieves state-of-the-art results on three scenarios (i.e, Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative improvement on benchmark datasets Clipart1k and Watercolor in particular.



### FDVTS's Solution for 2nd COV19D Competition on COVID-19 Detection and Severity Analysis
- **Arxiv ID**: http://arxiv.org/abs/2207.01758v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01758v1)
- **Published**: 2022-07-05 01:35:29+00:00
- **Updated**: 2022-07-05 01:35:29+00:00
- **Authors**: Junlin Hou, Jilan Xu, Rui Feng, Yuejie Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents our solution for the 2nd COVID-19 Competition, occurring in the framework of the AIMIA Workshop in the European Conference on Computer Vision (ECCV 2022). In our approach, we employ an effective 3D Contrastive Mixup Classification network for COVID-19 diagnosis on chest CT images, which is composed of contrastive representation learning and mixup classification. For the COVID-19 detection challenge, our approach reaches 0.9245 macro F1 score on 484 validation CT scans, which significantly outperforms the baseline method by 16.5%. In the COVID-19 severity detection challenge, our approach achieves 0.7186 macro F1 score on 61 validation samples, which also surpasses the baseline by 8.86%.



### GP22: A Car Styling Dataset for Automotive Designers
- **Arxiv ID**: http://arxiv.org/abs/2207.01760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.01760v1)
- **Published**: 2022-07-05 01:39:34+00:00
- **Updated**: 2022-07-05 01:39:34+00:00
- **Authors**: Gyunpyo Lee, Taesu Kim, Hyeon-Jeong Suk
- **Comment**: 5th CVFAD workshop, CVPR2022
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2022, pp. 2268-2272
- **Summary**: An automated design data archiving could reduce the time wasted by designers from working creatively and effectively. Though many datasets on classifying, detecting, and instance segmenting on car exterior exist, these large datasets are not relevant for design practices as the primary purpose lies in autonomous driving or vehicle verification. Therefore, we release GP22, composed of car styling features defined by automotive designers. The dataset contains 1480 car side profile images from 37 brands and ten car segments. It also contains annotations of design features that follow the taxonomy of the car exterior design features defined in the eye of the automotive designer. We trained the baseline model using YOLO v5 as the design feature detection model with the dataset. The presented model resulted in an mAP score of 0.995 and a recall of 0.984. Furthermore, exploration of the model performance on sketches and rendering images of the car side profile implies the scalability of the dataset for design purposes.



### Rank-Based Filter Pruning for Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2207.01768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01768v1)
- **Published**: 2022-07-05 02:13:53+00:00
- **Updated**: 2022-07-05 02:13:53+00:00
- **Authors**: Xucheng Wang, Dan Zeng, Qijun Zhao, Shuiwang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Unmanned aerial vehicle (UAV) tracking has wide potential applications in such as agriculture, navigation, and public security. However, the limitations of computing resources, battery capacity, and maximum load of UAV hinder the deployment of deep learning-based tracking algorithms on UAV. Consequently, discriminative correlation filters (DCF) trackers stand out in the UAV tracking community because of their high efficiency. However, their precision is usually much lower than trackers based on deep learning. Model compression is a promising way to narrow the gap (i.e., effciency, precision) between DCF- and deep learning- based trackers, which has not caught much attention in UAV tracking. In this paper, we propose the P-SiamFC++ tracker, which is the first to use rank-based filter pruning to compress the SiamFC++ model, achieving a remarkable balance between efficiency and precision. Our method is general and may encourage further studies on UAV tracking with model compression. Extensive experiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and Vistrone2018, show that P-SiamFC++ tracker significantly outperforms state-of-the-art UAV tracking methods.



### SESS: Saliency Enhancing with Scaling and Sliding
- **Arxiv ID**: http://arxiv.org/abs/2207.01769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01769v1)
- **Published**: 2022-07-05 02:16:23+00:00
- **Updated**: 2022-07-05 02:16:23+00:00
- **Authors**: Osman Tursun, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: This paper will be presented at ECCV2022
- **Journal**: None
- **Summary**: High-quality saliency maps are essential in several machine learning application areas including explainable AI and weakly supervised object detection and segmentation. Many techniques have been developed to generate better saliency using neural networks. However, they are often limited to specific saliency visualisation methods or saliency issues. We propose a novel saliency enhancing approach called SESS (Saliency Enhancing with Scaling and Sliding). It is a method and model agnostic extension to existing saliency map generation methods. With SESS, existing saliency approaches become robust to scale variance, multiple occurrences of target objects, presence of distractors and generate less noisy and more discriminative saliency maps. SESS improves saliency by fusing saliency maps extracted from multiple patches at different scales from different areas, and combines these individual maps using a novel fusion scheme that incorporates channel-wise weights and spatial weighted average. To improve efficiency, we introduce a pre-filtering step that can exclude uninformative saliency maps to improve efficiency while still enhancing overall results. We evaluate SESS on object recognition and detection benchmarks where it achieves significant improvement. The code is released publicly to enable researchers to verify performance and further development. Code is available at: https://github.com/neouyghur/SESS



### Object-Level Targeted Selection via Deep Template Matching
- **Arxiv ID**: http://arxiv.org/abs/2207.01778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01778v1)
- **Published**: 2022-07-05 02:32:34+00:00
- **Updated**: 2022-07-05 02:32:34+00:00
- **Authors**: Suraj Kothawade, Donna Roy, Michele Fenzi, Elmar Haussmann, Jose M. Alvarez, Christoph Angerer
- **Comment**: In Proceedings of the Intelligent Vehicles Symposium, IV 2022
- **Journal**: None
- **Summary**: Retrieving images with objects that are semantically similar to objects of interest (OOI) in a query image has many practical use cases. A few examples include fixing failures like false negatives/positives of a learned model or mitigating class imbalance in a dataset. The targeted selection task requires finding the relevant data from a large-scale pool of unlabeled data. Manual mining at this scale is infeasible. Further, the OOI are often small and occupy less than 1% of image area, are occluded, and co-exist with many semantically different objects in cluttered scenes. Existing semantic image retrieval methods often focus on mining for larger sized geographical landmarks, and/or require extra labeled data, such as images/image-pairs with similar objects, for mining images with generic objects. We propose a fast and robust template matching algorithm in the DNN feature space, that retrieves semantically similar images at the object-level from a large unlabeled pool of data. We project the region(s) around the OOI in the query image to the DNN feature space for use as the template. This enables our method to focus on the semantics of the OOI without requiring extra labeled data. In the context of autonomous driving, we evaluate our system for targeted selection by using failure cases of object detectors as OOI. We demonstrate its efficacy on a large unlabeled dataset with 2.2M images and show high recall in mining for images with small-sized OOI. We compare our method against a well-known semantic image retrieval method, which also does not require extra labeled data. Lastly, we show that our method is flexible and retrieves images with one or more semantically different co-occurring OOI seamlessly.



### 3D Part Assembly Generation with Instance Encoded Transformer
- **Arxiv ID**: http://arxiv.org/abs/2207.01779v1
- **DOI**: 10.1109/LRA.2022.3188098
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01779v1)
- **Published**: 2022-07-05 02:40:57+00:00
- **Updated**: 2022-07-05 02:40:57+00:00
- **Authors**: Rufeng Zhang, Tao Kong, Weihao Wang, Xuan Han, Mingyu You
- **Comment**: 8 pages, 7 figures
- **Journal**: IROS 2022 and IEEE Robotics and Automation Letters (RA-L), 2022
- **Summary**: It is desirable to enable robots capable of automatic assembly. Structural understanding of object parts plays a crucial role in this task yet remains relatively unexplored. In this paper, we focus on the setting of furniture assembly from a complete set of part geometries, which is essentially a 6-DoF part pose estimation problem. We propose a multi-layer transformer-based framework that involves geometric and relational reasoning between parts to update the part poses iteratively. We carefully design a unique instance encoding to solve the ambiguity between geometrically-similar parts so that all parts can be distinguished. In addition to assembling from scratch, we extend our framework to a new task called in-process part assembly. Analogous to furniture maintenance, it requires robots to continue with unfinished products and assemble the remaining parts into appropriate positions. Our method achieves far more than 10% improvements over the current state-of-the-art in multiple metrics on the public PartNet dataset. Extensive experiments and quantitative comparisons demonstrate the effectiveness of the proposed framework.



### Softmax-free Linear Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.03341v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03341v2)
- **Published**: 2022-07-05 03:08:27+00:00
- **Updated**: 2023-07-05 00:28:06+00:00
- **Authors**: Li Zhang, Jiachen Lu, Junge Zhang, Xiatian Zhu, Jianfeng Feng, Tao Xiang
- **Comment**: Extended journal version of NeurIPS conference submission
  arXiv:2110.11945
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under low-rank matrix decomposition. For computational robustness, we estimate the Moore-Penrose inverse using an iterative Newton-Raphson method in the forward process only, while calculating its theoretical gradients only once in the backward process. To further expand applicability (e.g., dense prediction tasks), an efficient symmetric normalization technique is introduced. Extensive experiments on ImageNet, COCO, and ADE20K show that our SOFT significantly improves the computational efficiency of existing ViT variants. With linear complexity, much longer token sequences are permitted by SOFT, resulting in superior trade-off between accuracy and complexity. Code and models are available at https://github.com/fudan-zvg/SOFT.



### A deep cascade of ensemble of dual domain networks with gradient-based T1 assistance and perceptual refinement for fast MRI reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2207.01791v1
- **DOI**: 10.1016/j.compmedimag.2021.101942
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01791v1)
- **Published**: 2022-07-05 03:29:40+00:00
- **Updated**: 2022-07-05 03:29:40+00:00
- **Authors**: Balamurali Murugesan, Sriprabha Ramanarayanan, Sricharan Vijayarangan, Keerthi Ram, Naranamangalam R Jagannathan, Mohanasankar Sivaprakasam
- **Comment**: Accepted in CMIG 2021
- **Journal**: None
- **Summary**: Deep learning networks have shown promising results in fast magnetic resonance imaging (MRI) reconstruction. In our work, we develop deep networks to further improve the quantitative and the perceptual quality of reconstruction. To begin with, we propose reconsynergynet (RSN), a network that combines the complementary benefits of independently operating on both the image and the Fourier domain. For a single-coil acquisition, we introduce deep cascade RSN (DC-RSN), a cascade of RSN blocks interleaved with data fidelity (DF) units. Secondly, we improve the structure recovery of DC-RSN for T2 weighted Imaging (T2WI) through assistance of T1 weighted imaging (T1WI), a sequence with short acquisition time. T1 assistance is provided to DC-RSN through a gradient of log feature (GOLF) fusion. Furthermore, we propose perceptual refinement network (PRN) to refine the reconstructions for better visual information fidelity (VIF), a metric highly correlated to radiologists opinion on the image quality. Lastly, for multi-coil acquisition, we propose variable splitting RSN (VS-RSN), a deep cascade of blocks, each block containing RSN, multi-coil DF unit, and a weighted average module. We extensively validate our models DC-RSN and VS-RSN for single-coil and multi-coil acquisitions and report the state-of-the-art performance. We obtain a SSIM of 0.768, 0.923, 0.878 for knee single-coil-4x, multi-coil-4x, and multi-coil-8x in fastMRI. We also conduct experiments to demonstrate the efficacy of GOLF based T1 assistance and PRN.



### PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch
- **Arxiv ID**: http://arxiv.org/abs/2207.01795v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.01795v3)
- **Published**: 2022-07-05 03:49:08+00:00
- **Updated**: 2022-09-05 18:40:14+00:00
- **Authors**: Ke Xu, Yao Xiao, Zhaoheng Zheng, Kaijie Cai, Ram Nevatia
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Adversarial patch attacks mislead neural networks by injecting adversarial pixels within a local region. Patch attacks can be highly effective in a variety of tasks and physically realizable via attachment (e.g. a sticker) to the real-world objects. Despite the diversity in attack patterns, adversarial patches tend to be highly textured and different in appearance from natural images. We exploit this property and present PatchZero, a general defense pipeline against white-box adversarial patches without retraining the downstream classifier or detector. Specifically, our defense detects adversaries at the pixel-level and "zeros out" the patch region by repainting with mean pixel values. We further design a two-stage adversarial training scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA defense performance on the image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and video classification (UCF101) tasks with little degradation in benign performance. In addition, PatchZero transfers to different patch shapes and attack types.



### Deep Parametric 3D Filters for Joint Video Denoising and Illumination Enhancement in Video Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2207.01797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01797v2)
- **Published**: 2022-07-05 03:57:25+00:00
- **Updated**: 2022-12-01 07:54:04+00:00
- **Authors**: Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, Jiaya Jia
- **Comment**: AAAI2023
- **Journal**: None
- **Summary**: Despite the quality improvement brought by the recent methods, video super-resolution (SR) is still very challenging, especially for videos that are low-light and noisy. The current best solution is to subsequently employ best models of video SR, denoising, and illumination enhancement, but doing so often lowers the image quality, due to the inconsistency between the models. This paper presents a new parametric representation called the Deep Parametric 3D Filters (DP3DF), which incorporates local spatiotemporal information to enable simultaneous denoising, illumination enhancement, and SR efficiently in a single encoder-and-decoder network. Also, a dynamic residual frame is jointly learned with the DP3DF via a shared backbone to further boost the SR quality. We performed extensive experiments, including a large-scale user study, to show our method's effectiveness. Our method consistently surpasses the best state-of-the-art methods on all the challenging real datasets with top PSNR and user ratings, yet having a very fast run time.



### GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.01798v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.01798v2)
- **Published**: 2022-07-05 04:04:37+00:00
- **Updated**: 2022-07-08 09:11:55+00:00
- **Authors**: Zhi Chen, Yadan Luo, Sen Wang, Jingjing Li, Zi Huang
- **Comment**: IEEE Transactions on Multimedia 2022. Journal Extension from
  "Mitigating Generation Shifts for Generalized Zero-Shot Learning", ACM MM
  2021. arXiv admin note: substantial text overlap with arXiv:2107.03163
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the seen and unseen classes by transferring semantic knowledge from seen to unseen classes. It is a promising solution to take the advantage of generative models to hallucinate realistic unseen samples based on the knowledge learned from the seen classes. However, due to the generation shifts, the synthesized samples by most existing methods may drift from the real distribution of the unseen data. To address this issue, we propose a novel flow-based generative framework that consists of multiple conditional affine coupling layers for learning unseen data generation. Specifically, we discover and address three potential problems that trigger the generation shifts, i.e., semantic inconsistency, variance collapse, and structure disorder. First, to enhance the reflection of the semantic information in the generated samples, we explicitly embed the semantic information into the transformation in each conditional affine coupling layer. Second, to recover the intrinsic variance of the real unseen features, we introduce a boundary sample mining strategy with entropy maximization to discover more difficult visual variants of semantic prototypes and hereby adjust the decision boundary of the classifiers. Third, a relative positioning strategy is proposed to revise the attribute embeddings, guiding them to fully preserve the inter-class geometric structure and further avoid structure disorder in the semantic space. Extensive experimental results on four GZSL benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art performance on GZSL.



### ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2207.01805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01805v1)
- **Published**: 2022-07-05 04:21:35+00:00
- **Updated**: 2022-07-05 04:21:35+00:00
- **Authors**: Jiawei Yang, Hanbo Chen, Yu Zhao, Fan Yang, Yao Zhang, Lei He, Jianhua Yao
- **Comment**: Published in MICCAI 2022. Code: https://github.com/Jiawei-Yang/ReMix
  or https://github.com/TencentAILabHealthcare/ReMix
- **Journal**: None
- **Summary**: Whole slide image (WSI) classification often relies on deep weakly supervised multiple instance learning (MIL) methods to handle gigapixel resolution images and slide-level labels. Yet the decent performance of deep learning comes from harnessing massive datasets and diverse samples, urging the need for efficient training pipelines for scaling to large datasets and data augmentation techniques for diversifying samples. However, current MIL-based WSI classification pipelines are memory-expensive and computation-inefficient since they usually assemble tens of thousands of patches as bags for computation. On the other hand, despite their popularity in other tasks, data augmentations are unexplored for WSI MIL frameworks. To address them, we propose ReMix, a general and efficient framework for MIL based WSI classification. It comprises two steps: reduce and mix. First, it reduces the number of instances in WSI bags by substituting instances with instance prototypes, i.e., patch cluster centroids. Then, we propose a ``Mix-the-bag'' augmentation that contains four online, stochastic and flexible latent space augmentations. It brings diverse and reliable class-identity-preserving semantic changes in the latent space while enforcing semantic-perturbation invariance. We evaluate ReMix on two public datasets with two state-of-the-art MIL methods. In our experiments, consistent improvements in precision, accuracy, and recall have been achieved but with orders of magnitude reduced training time and memory consumption, demonstrating ReMix's effectiveness and efficiency. Code is available.



### Aesthetic Attribute Assessment of Images Numerically on Mixed Multi-attribute Datasets
- **Arxiv ID**: http://arxiv.org/abs/2207.01806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01806v1)
- **Published**: 2022-07-05 04:42:10+00:00
- **Updated**: 2022-07-05 04:42:10+00:00
- **Authors**: Xin Jin, Xinning Li, Hao Lou, Chenyu Fan, Qiang Deng, Chaoen Xiao, Shuai Cui, Amit Kumar Singh
- **Comment**: 7 pages, 9figures, to appear: ACM Transactions on Multimedia
  Computing Communications and Applications (TOMM)
- **Journal**: None
- **Summary**: With the continuous development of social software and multimedia technology, images have become a kind of important carrier for spreading information and socializing. How to evaluate an image comprehensively has become the focus of recent researches. The traditional image aesthetic assessment methods often adopt single numerical overall assessment scores, which has certain subjectivity and can no longer meet the higher aesthetic requirements. In this paper, we construct an new image attribute dataset called aesthetic mixed dataset with attributes(AMD-A) and design external attribute features for fusion. Besides, we propose a efficient method for image aesthetic attribute assessment on mixed multi-attribute dataset and construct a multitasking network architecture by using the EfficientNet-B0 as the backbone network. Our model can achieve aesthetic classification, overall scoring and attribute scoring. In each sub-network, we improve the feature extraction through ECA channel attention module. As for the final overall scoring, we adopt the idea of the teacher-student network and use the classification sub-network to guide the aesthetic overall fine-grain regression. Experimental results, using the MindSpore, show that our proposed method can effectively improve the performance of the aesthetic overall and attribute assessment.



### Deriving Surface Resistivity from Polarimetric SAR Data Using Dual-Input UNet
- **Arxiv ID**: http://arxiv.org/abs/2207.01811v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2207.01811v1)
- **Published**: 2022-07-05 05:10:59+00:00
- **Updated**: 2022-07-05 05:10:59+00:00
- **Authors**: Bibin Wilson, Rajiv Kumar, Narayanarao Bhogapurapu, Anand Singh, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional survey methods for finding surface resistivity are time-consuming and labor intensive. Very few studies have focused on finding the resistivity/conductivity using remote sensing data and deep learning techniques. In this line of work, we assessed the correlation between surface resistivity and Synthetic Aperture Radar (SAR) by applying various deep learning methods and tested our hypothesis in the Coso Geothermal Area, USA. For detecting the resistivity, L-band full polarimetric SAR data acquired by UAVSAR were used, and MT (Magnetotellurics) inverted resistivity data of the area were used as the ground truth. We conducted experiments to compare various deep learning architectures and suggest the use of Dual Input UNet (DI-UNet) architecture. DI-UNet uses a deep learning architecture to predict the resistivity using full polarimetric SAR data by promising a quick survey addition to the traditional method. Our proposed approach accomplished improved outcomes for the mapping of MT resistivity from SAR data.



### Cov3d: Detection of the presence and severity of COVID-19 from CT scans using 3D ResNets
- **Arxiv ID**: http://arxiv.org/abs/2207.12218v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2207.12218v1)
- **Published**: 2022-07-05 05:22:38+00:00
- **Updated**: 2022-07-05 05:22:38+00:00
- **Authors**: Robert Turnbull
- **Comment**: 10 pages, 2 figures, conference
- **Journal**: None
- **Summary**: Deep learning has been used to assist in the analysis of medical imaging. One such use is the classification of Computed Tomography (CT) scans when detecting for COVID-19 in subjects. This paper presents Cov3d, a three dimensional convolutional neural network for detecting the presence and severity of COVID19 from chest CT scans. Trained on the COV19-CT-DB dataset with human expert annotations, it achieves a macro f1 score of 0.9476 on the validation set for the task of detecting the presence of COVID19. For the task of classifying the severity of COVID19, it achieves a macro f1 score of 0.7552. Both results improve on the baseline results of the `AI-enabled Medical Image Analysis Workshop and Covid-19 Diagnosis Competition' (MIA-COV19D) in 2022.



### Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases
- **Arxiv ID**: http://arxiv.org/abs/2207.01821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01821v2)
- **Published**: 2022-07-05 05:50:12+00:00
- **Updated**: 2023-05-27 10:03:34+00:00
- **Authors**: Zhihao Yuan, Xu Yan, Zhuo Li, Xuhao Li, Yao Guo, Shuguang Cui, Zhen Li
- **Comment**: New dataset for 3D visual grounding is available at
  https://yanx27.github.io/phraserefer/
- **Journal**: None
- **Summary**: Recent progress in 3D scene understanding has explored visual grounding (3DVG) to localize a target object through a language description. However, existing methods only consider the dependency between the entire sentence and the target object, ignoring fine-grained relationships between contexts and non-target ones. In this paper, we extend 3DVG to a more fine-grained and interpretable task, called 3D Phrase Aware Grounding (3DPAG). The 3DPAG task aims to localize the target objects in a 3D scene by explicitly identifying all phrase-related objects and then conducting the reasoning according to contextual phrases. To tackle this problem, we manually labeled about 227K phrase-level annotations using a self-developed platform, from 88K sentences of widely used 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer. By tapping on our datasets, we can extend previous 3DVG methods to the fine-grained phrase-aware scenario. It is achieved through the proposed novel phrase-object alignment optimization and phrase-specific pre-training, boosting conventional 3DVG performance as well. Extensive results confirm significant improvements, i.e., previous state-of-the-art method achieves 3.9%, 3.5% and 4.6% overall accuracy gains on Nr3D, Sr3D and ScanRefer respectively.



### Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.01823v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01823v1)
- **Published**: 2022-07-05 05:54:20+00:00
- **Updated**: 2022-07-05 05:54:20+00:00
- **Authors**: Bin Li, Yixuan Weng, Ziyu Ma, Bin Sun, Shutao Li
- **Comment**: Accepted in NLPCC 2022
- **Journal**: None
- **Summary**: This paper introduces the schemes of Team LingJing's experiments in NLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation (MDUG). The MDUG task can be divided into two phases: multi-modal context understanding and response generation. To fully leverage the visual information for both scene understanding and dialogue generation, we propose the scene-aware prompt for the MDUG task. Specifically, we utilize the multi-tasking strategy for jointly modelling the scene- and session- multi-modal understanding. The visual captions are adopted to aware the scene information, while the fixed-type templated prompt based on the scene- and session-aware labels are used to further improve the dialogue generation performance. Extensive experimental results show that the proposed method has achieved state-of-the-art (SOTA) performance compared with other competitive methods, where we rank the 1-st in all three subtasks in this MDUG competition.



### Learning Local Implicit Fourier Representation for Image Warping
- **Arxiv ID**: http://arxiv.org/abs/2207.01831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01831v1)
- **Published**: 2022-07-05 06:30:17+00:00
- **Updated**: 2022-07-05 06:30:17+00:00
- **Authors**: Jaewon Lee, Kwang Pyo Choi, Kyong Hwan Jin
- **Comment**: ECCV 2022 camera-ready version (https://ipl.dgist.ac.kr/LTEW.pdf)
- **Journal**: None
- **Summary**: Image warping aims to reshape images defined on rectangular grids into arbitrary shapes. Recently, implicit neural functions have shown remarkable performances in representing images in a continuous manner. However, a standalone multi-layer perceptron suffers from learning high-frequency Fourier coefficients. In this paper, we propose a local texture estimator for image warping (LTEW) followed by an implicit neural representation to deform images into continuous shapes. Local textures estimated from a deep super-resolution (SR) backbone are multiplied by locally-varying Jacobian matrices of a coordinate transformation to predict Fourier responses of a warped image. Our LTEW-based neural function outperforms existing warping methods for asymmetric-scale SR and homography transform. Furthermore, our algorithm well generalizes arbitrary coordinate transformations, such as homography transform with a large magnification factor and equirectangular projection (ERP) perspective transform, which are not provided in training.



### ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2207.01842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01842v1)
- **Published**: 2022-07-05 07:06:57+00:00
- **Updated**: 2022-07-05 07:06:57+00:00
- **Authors**: Zhizhong Chai, Huangjing Lin, Luyang Luo, Pheng-Ann Heng, Hao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing object detection works are based on the bounding box annotation: each object has a precise annotated box. However, for rib fractures, the bounding box annotation is very labor-intensive and time-consuming because radiologists need to investigate and annotate the rib fractures on a slice-by-slice basis. Although a few studies have proposed weakly-supervised methods or semi-supervised methods, they could not handle different forms of supervision simultaneously. In this paper, we proposed a novel omni-supervised object detection network, which can exploit multiple different forms of annotated data to further improve the detection performance. Specifically, the proposed network contains an omni-supervised detection head, in which each form of annotation data corresponds to a unique classification branch. Furthermore, we proposed a dynamic label assignment strategy for different annotated forms of data to facilitate better learning for each branch. Moreover, we also design a confidence-aware classification loss to emphasize the samples with high confidence and further improve the model's performance. Extensive experiments conducted on the testing dataset show our proposed method outperforms other state-of-the-art approaches consistently, demonstrating the efficacy of deep omni-supervised learning on improving rib fracture detection performance.



### Efficient Representation Learning via Adaptive Context Pooling
- **Arxiv ID**: http://arxiv.org/abs/2207.01844v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01844v1)
- **Published**: 2022-07-05 07:10:31+00:00
- **Updated**: 2022-07-05 07:10:31+00:00
- **Authors**: Chen Huang, Walter Talbott, Navdeep Jaitly, Josh Susskind
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Self-attention mechanisms model long-range context by using pairwise attention between all input tokens. In doing so, they assume a fixed attention granularity defined by the individual tokens (e.g., text characters or image pixels), which may not be optimal for modeling complex dependencies at higher levels. In this paper, we propose ContextPool to address this problem by adapting the attention granularity for each token. Inspired by the success of ConvNets that are combined with pooling to capture long-range dependencies, we learn to pool neighboring features for each token before computing attention in a given attention layer. The pooling weights and support size are adaptively determined, allowing the pooled features to encode meaningful context with varying scale. We show that ContextPool makes attention models more expressive, achieving strong performance often with fewer layers and thus significantly reduced cost. Experiments validate that our ContextPool module, when plugged into transformer models, matches or surpasses state-of-the-art performance using less compute on several language and image benchmarks, outperforms recent works with learned context sizes or sparse attention patterns, and is also applicable to ConvNets for efficient feature learning.



### Hierarchical Average Precision Training for Pertinent Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.04873v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04873v2)
- **Published**: 2022-07-05 07:55:18+00:00
- **Updated**: 2022-07-22 13:24:04+00:00
- **Authors**: Elias Ramzi, Nicolas Audebert, Nicolas Thome, Cl√©ment Rambour, Xavier Bitot
- **Comment**: None
- **Journal**: ECCV 2022, Oct 2022, Tel-Aviv, Israel
- **Summary**: Image Retrieval is commonly evaluated with Average Precision (AP) or Recall@k. Yet, those metrics, are limited to binary labels and do not take into account errors' severity. This paper introduces a new hierarchical AP training method for pertinent image retrieval (HAP-PIER). HAPPIER is based on a new H-AP metric, which leverages a concept hierarchy to refine AP by integrating errors' importance and better evaluate rankings. To train deep models with H-AP, we carefully study the problem's structure and design a smooth lower bound surrogate combined with a clustering loss that ensures consistent ordering. Extensive experiments on 6 datasets show that HAPPIER significantly outperforms state-of-the-art methods for hierarchical retrieval, while being on par with the latest approaches when evaluating fine-grained ranking performances. Finally, we show that HAPPIER leads to better organization of the embedding space, and prevents most severe failure cases of non-hierarchical methods. Our code is publicly available at: https://github.com/elias-ramzi/HAPPIER.



### Bayesian approaches for Quantifying Clinicians' Variability in Medical Image Quantification
- **Arxiv ID**: http://arxiv.org/abs/2207.01868v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.01868v2)
- **Published**: 2022-07-05 08:04:02+00:00
- **Updated**: 2022-07-06 06:14:40+00:00
- **Authors**: Jaeik Jeon, Yeonggul Jang, Youngtaek Hong, Hackjoon Shim, Sekeun Kim
- **Comment**: Interpretable Machine Learning in Healthcare
- **Journal**: None
- **Summary**: Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in clinical decisions. Accurate segmentation is essential to measure the structure of interest from the image. However, manual segmentation is highly operator-dependent, which leads to high inter and intra-variability of quantitative measurements. In this paper, we explore the feasibility that Bayesian predictive distribution parameterized by deep neural networks can capture the clinicians' inter-intra variability. By exploring and analyzing recently emerged approximate inference schemes, we evaluate whether approximate Bayesian deep learning with the posterior over segmentations can learn inter-intra rater variability both in segmentation and clinical measurements. The experiments are performed with two different imaging modalities: MRI and ultrasound. We empirically demonstrated that Bayesian predictive distribution parameterized by deep neural networks could approximate the clinicians' inter-intra variability. We show a new perspective in analyzing medical images quantitatively by providing clinical measurement uncertainty.



### Distance Matters in Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.01869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.01869v1)
- **Published**: 2022-07-05 08:06:05+00:00
- **Updated**: 2022-07-05 08:06:05+00:00
- **Authors**: Guangzhi Wang, Yangyang Guo, Yongkang Wong, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection has received considerable attention in the context of scene understanding. Despite the growing progress on benchmarks, we realize that existing methods often perform unsatisfactorily on distant interactions, where the leading causes are two-fold: 1) Distant interactions are by nature more difficult to recognize than close ones. A natural scene often involves multiple humans and objects with intricate spatial relations, making the interaction recognition for distant human-object largely affected by complex visual context. 2) Insufficient number of distant interactions in benchmark datasets results in under-fitting on these instances. To address these problems, in this paper, we propose a novel two-stage method for better handling distant interactions in HOI detection. One essential component in our method is a novel Far Near Distance Attention module. It enables information propagation between humans and objects, whereby the spatial distance is skillfully taken into consideration. Besides, we devise a novel Distance-Aware loss function which leads the model to focus more on distant yet rare interactions. We conduct extensive experiments on two challenging datasets - HICO-DET and V-COCO. The results demonstrate that the proposed method can surpass existing approaches by a large margin, resulting in new state-of-the-art performance.



### Latents2Segments: Disentangling the Latent Space of Generative Models for Semantic Segmentation of Face Images
- **Arxiv ID**: http://arxiv.org/abs/2207.01871v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2207.01871v2)
- **Published**: 2022-07-05 08:09:15+00:00
- **Updated**: 2022-07-06 06:54:09+00:00
- **Authors**: Snehal Singh Tomar, A. N. Rajagopalan
- **Comment**: 5 pages, 4 figures, 2 tables. The paper has already been accepted to
  and presented at CVPR Workshop on Computer Vision for Augmented and Virtual
  Reality, New Orleans, LA, 2022
- **Journal**: None
- **Summary**: With the advent of an increasing number of Augmented and Virtual Reality applications that aim to perform meaningful and controlled style edits on images of human faces, the impetus for the task of parsing face images to produce accurate and fine-grained semantic segmentation maps is more than ever before. Few State of the Art (SOTA) methods which solve this problem, do so by incorporating priors with respect to facial structure or other face attributes such as expression and pose in their deep classifier architecture. Our endeavour in this work is to do away with the priors and complex pre-processing operations required by SOTA multi-class face segmentation models by reframing this operation as a downstream task post infusion of disentanglement with respect to facial semantic regions of interest (ROIs) in the latent space of a Generative Autoencoder model. We present results for our model's performance on the CelebAMask-HQ and HELEN datasets. The encoded latent space of our model achieves significantly higher disentanglement with respect to semantic ROIs than that of other SOTA works. Moreover, it achieves a 13% faster inference rate and comparable accuracy with respect to the publicly available SOTA for the downstream task of semantic segmentation of face images.



### Vision-based Uneven BEV Representation Learning with Polar Rasterization and Surface Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.01878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01878v1)
- **Published**: 2022-07-05 08:20:36+00:00
- **Updated**: 2022-07-05 08:20:36+00:00
- **Authors**: Zhi Liu, Shaoyu Chen, Xiaojie Guo, Xinggang Wang, Tianheng Cheng, Hongmei Zhu, Qian Zhang, Wenyu Liu, Yi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose PolarBEV for vision-based uneven BEV representation learning. To adapt to the foreshortening effect of camera imaging, we rasterize the BEV space both angularly and radially, and introduce polar embedding decomposition to model the associations among polar grids. Polar grids are rearranged to an array-like regular representation for efficient processing. Besides, to determine the 2D-to-3D correspondence, we iteratively update the BEV surface based on a hypothetical plane, and adopt height-based feature transformation. PolarBEV keeps real-time inference speed on a single 2080Ti GPU, and outperforms other methods for both BEV semantic segmentation and BEV instance segmentation. Thorough ablations are presented to validate the design. The code will be released at \url{https://github.com/SuperZ-Liu/PolarBEV}.



### MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.01883v1
- **DOI**: 10.1109/ICIP46576.2022.9897591
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01883v1)
- **Published**: 2022-07-05 08:24:46+00:00
- **Updated**: 2022-07-05 08:24:46+00:00
- **Authors**: Ziyuan Zhao, Jinxuan Hu, Zeng Zeng, Xulei Yang, Peisheng Qian, Bharadwaj Veeravalli, Cuntai Guan
- **Comment**: Accepted by IEEE International Conference on Image Processing (ICIP
  2022)
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP)
- **Summary**: With large-scale well-labeled datasets, deep learning has shown significant success in medical image segmentation. However, it is challenging to acquire abundant annotations in clinical practice due to extensive expertise requirements and costly labeling efforts. Recently, contrastive learning has shown a strong capacity for visual representation learning on unlabeled data, achieving impressive performance rivaling supervised learning in many domains. In this work, we propose a novel multi-scale multi-view global-local contrastive learning (MMGL) framework to thoroughly explore global and local features from different scales and views for robust contrastive learning performance, thereby improving segmentation performance with limited annotations. Extensive experiments on the MM-WHS dataset demonstrate the effectiveness of MMGL framework on semi-supervised cardiac image segmentation, outperforming the state-of-the-art contrastive learning methods by a large margin.



### Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2207.01887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01887v2)
- **Published**: 2022-07-05 08:32:18+00:00
- **Updated**: 2023-02-01 10:59:03+00:00
- **Authors**: Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Bo Ren, Shu-Tao Xia
- **Comment**: AAAI 2023 (Oral presentation paper). Updated version
- **Journal**: None
- **Summary**: Real-world recognition system often encounters the challenge of unseen labels. To identify such unseen labels, multi-label zero-shot learning (ML-ZSL) focuses on transferring knowledge by a pre-trained textual label embedding (e.g., GloVe). However, such methods only exploit single-modal knowledge from a language model, while ignoring the rich semantic information inherent in image-text pairs. Instead, recently developed open-vocabulary (OV) based methods succeed in exploiting such information of image-text pairs in object detection, and achieve impressive performance. Inspired by the success of OV-based methods, we propose a novel open-vocabulary framework, named multi-modal knowledge transfer (MKT), for multi-label classification. Specifically, our method exploits multi-modal knowledge of image-text pairs based on a vision and language pre-training (VLP) model. To facilitate transferring the image-text matching ability of VLP model, knowledge distillation is employed to guarantee the consistency of image and label embeddings, along with prompt tuning to further update the label embeddings. To further enable the recognition of multiple objects, a simple but effective two-stream module is developed to capture both local and global features. Extensive experimental results show that our method significantly outperforms state-of-the-art methods on public benchmark datasets. The source code is available at https://github.com/sunanhe/MKT.



### ACT-Net: Asymmetric Co-Teacher Network for Semi-supervised Memory-efficient Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.01900v1
- **DOI**: 10.1109/ICIP46576.2022.9897494
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01900v1)
- **Published**: 2022-07-05 08:58:15+00:00
- **Updated**: 2022-07-05 08:58:15+00:00
- **Authors**: Ziyuan Zhao, Andong Zhu, Zeng Zeng, Bharadwaj Veeravalli, Cuntai Guan
- **Comment**: None
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP)
- **Summary**: While deep models have shown promising performance in medical image segmentation, they heavily rely on a large amount of well-annotated data, which is difficult to access, especially in clinical practice. On the other hand, high-accuracy deep models usually come in large model sizes, limiting their employment in real scenarios. In this work, we propose a novel asymmetric co-teacher framework, ACT-Net, to alleviate the burden on both expensive annotations and computational costs for semi-supervised knowledge distillation. We advance teacher-student learning with a co-teacher network to facilitate asymmetric knowledge distillation from large models to small ones by alternating student and teacher roles, obtaining tiny but accurate models for clinical employment. To verify the effectiveness of our ACT-Net, we employ the ACDC dataset for cardiac substructure segmentation in our experiments. Extensive experimental results demonstrate that ACT-Net outperforms other knowledge distillation methods and achieves lossless segmentation performance with 250x fewer parameters.



### Spatial-Temporal Frequency Forgery Clue for Video Forgery Detection in VIS and NIR Scenario
- **Arxiv ID**: http://arxiv.org/abs/2207.01906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01906v1)
- **Published**: 2022-07-05 09:27:53+00:00
- **Updated**: 2022-07-05 09:27:53+00:00
- **Authors**: Yukai Wang, Chunlei Peng, Decheng Liu, Nannan Wang, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, with the rapid development of face editing and generation, more and more fake videos are circulating on social media, which has caused extreme public concerns. Existing face forgery detection methods based on frequency domain find that the GAN forged images have obvious grid-like visual artifacts in the frequency spectrum compared to the real images. But for synthesized videos, these methods only confine to single frame and pay little attention to the most discriminative part and temporal frequency clue among different frames. To take full advantage of the rich information in video sequences, this paper performs video forgery detection on both spatial and temporal frequency domains and proposes a Discrete Cosine Transform-based Forgery Clue Augmentation Network (FCAN-DCT) to achieve a more comprehensive spatial-temporal feature representation. FCAN-DCT consists of a backbone network and two branches: Compact Feature Extraction (CFE) module and Frequency Temporal Attention (FTA) module. We conduct thorough experimental assessments on two visible light (VIS) based datasets WildDeepfake and Celeb-DF (v2), and our self-built video forgery dataset DeepfakeNIR, which is the first video forgery dataset on near-infrared modality. The experimental results demonstrate the effectiveness of our method on detecting forgery videos in both VIS and NIR scenarios.



### StyleFlow For Content-Fixed Image to Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2207.01909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01909v1)
- **Published**: 2022-07-05 09:40:03+00:00
- **Updated**: 2022-07-05 09:40:03+00:00
- **Authors**: Weichen Fan, Jinghuan Chen, Jiabin Ma, Jun Hou, Shuai Yi
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image (I2I) translation is a challenging topic in computer vision. We divide this problem into three tasks: strongly constrained translation, normally constrained translation, and weakly constrained translation. The constraint here indicates the extent to which the content or semantic information in the original image is preserved. Although previous approaches have achieved good performance in weakly constrained tasks, they failed to fully preserve the content in both strongly and normally constrained tasks, including photo-realism synthesis, style transfer, and colorization, etc. To achieve content-preserving transfer in strongly constrained and normally constrained tasks, we propose StyleFlow, a new I2I translation model that consists of normalizing flows and a novel Style-Aware Normalization (SAN) module. With the invertible network structure, StyleFlow first projects input images into deep feature space in the forward pass, while the backward pass utilizes the SAN module to perform content-fixed feature transformation and then projects back to image space. Our model supports both image-guided translation and multi-modal synthesis. We evaluate our model in several I2I translation benchmarks, and the results show that the proposed model has advantages over previous methods in both strongly constrained and normally constrained tasks.



### Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models
- **Arxiv ID**: http://arxiv.org/abs/2207.01916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.01916v1)
- **Published**: 2022-07-05 09:51:39+00:00
- **Updated**: 2022-07-05 09:51:39+00:00
- **Authors**: Ainkaran Santhirasekaram, Avinash Kori, Andrea Rockall, Mathias Winkler, Francesca Toni, Ben Glocker
- **Comment**: None
- **Journal**: None
- **Summary**: Explanations for \emph{black-box} models help us understand model decisions as well as provide information on model biases and inconsistencies. Most of the current explainability techniques provide a single level of explanation, often in terms of feature importance scores or feature attention maps in input space. Our focus is on explaining deep discriminative models at \emph{multiple levels of abstraction}, from fine-grained to fully abstract explanations. We achieve this by using the natural properties of \emph{hyperbolic geometry} to more efficiently model a hierarchy of symbolic features and generate \emph{hierarchical symbolic rules} as part of our explanations. Specifically, for any given deep discriminative model, we distill the underpinning knowledge by discretisation of the continuous latent space using vector quantisation to form symbols, followed by a \emph{hyperbolic reasoning block} to induce an \emph{abstraction tree}. We traverse the tree to extract explanations in terms of symbolic rules and its corresponding visual semantics. We demonstrate the effectiveness of our method on the MNIST and AFHQ high-resolution animal faces dataset. Our framework is available at \url{https://github.com/koriavinash1/SymbolicInterpretability}.



### GLANCE: Global to Local Architecture-Neutral Concept-based Explanations
- **Arxiv ID**: http://arxiv.org/abs/2207.01917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.01917v1)
- **Published**: 2022-07-05 09:52:09+00:00
- **Updated**: 2022-07-05 09:52:09+00:00
- **Authors**: Avinash Kori, Ben Glocker, Francesca Toni
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the current explainability techniques focus on capturing the importance of features in input space. However, given the complexity of models and data-generating processes, the resulting explanations are far from being `complete', in that they lack an indication of feature interactions and visualization of their `effect'. In this work, we propose a novel twin-surrogate explainability framework to explain the decisions made by any CNN-based image classifier (irrespective of the architecture). For this, we first disentangle latent features from the classifier, followed by aligning these features to observed/human-defined `context' features. These aligned features form semantically meaningful concepts that are used for extracting a causal graph depicting the `perceived' data-generating process, describing the inter- and intra-feature interactions between unobserved latent features and observed `context' features. This causal graph serves as a global model from which local explanations of different forms can be extracted. Specifically, we provide a generator to visualize the `effect' of interactions among features in latent space and draw feature importance therefrom as local explanations. Our framework utilizes adversarial knowledge distillation to faithfully learn a representation from the classifiers' latent space and use it for extracting visual explanations. We use the styleGAN-v2 architecture with an additional regularization term to enforce disentanglement and alignment. We demonstrate and evaluate explanations obtained with our framework on Morpho-MNIST and on the FFHQ human faces dataset. Our framework is available at \url{https://github.com/koriavinash1/GLANCE-Explanations}.



### Vector Quantisation for Robust Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.01919v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.01919v1)
- **Published**: 2022-07-05 09:52:53+00:00
- **Updated**: 2022-07-05 09:52:53+00:00
- **Authors**: Ainkaran Santhirasekaram, Avinash Kori, Mathias Winkler, Andrea Rockall, Ben Glocker
- **Comment**: None
- **Journal**: None
- **Summary**: The reliability of segmentation models in the medical domain depends on the model's robustness to perturbations in the input space. Robustness is a particular challenge in medical imaging exhibiting various sources of image noise, corruptions, and domain shifts. Obtaining robustness is often attempted via simulating heterogeneous environments, either heuristically in the form of data augmentation or by learning to generate specific perturbations in an adversarial manner. We propose and justify that learning a discrete representation in a low dimensional embedding space improves robustness of a segmentation model. This is achieved with a dictionary learning method called vector quantisation. We use a set of experiments designed to analyse robustness in both the latent and output space under domain shift and noise perturbations in the input space. We adapt the popular UNet architecture, inserting a quantisation block in the bottleneck. We demonstrate improved segmentation accuracy and better robustness on three segmentation tasks. Code is available at \url{https://github.com/AinkaranSanthi/Vector-Quantisation-for-Robust-Segmentation}



### FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration
- **Arxiv ID**: http://arxiv.org/abs/2207.01925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01925v2)
- **Published**: 2022-07-05 09:59:32+00:00
- **Updated**: 2022-08-01 12:19:57+00:00
- **Authors**: Shangrong Yang, Chunyu Lin, Kang Liao, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous significant progress on fisheye image rectification has been achieved through CNN. Nevertheless, constrained by a fixed receptive field, the global distribution and the local symmetry of the distortion have not been fully exploited. To leverage these two characteristics, we introduced Fishformer that processes the fisheye image as a sequence to enhance global and local perception. We tuned the Transformer according to the structural properties of fisheye images. First, the uneven distortion distribution in patches generated by the existing square slicing method confuses the network, resulting in difficult training. Therefore, we propose an annulus slicing method to maintain the consistency of the distortion in each patch, thus perceiving the distortion distribution well. Second, we analyze that different distortion parameters have their own efficacy domains. Hence, the perception of the local area is as important as the global, but Transformer has a weakness for local texture perception. Therefore, we propose a novel layer attention mechanism to enhance the local perception and texture transfer. Our network simultaneously implements global perception and focused local perception decided by the different parameters. Extensive experiments demonstrate that our method provides superior performance compared with state-of-the-art methods.



### Drone Detection and Tracking in Real-Time by Fusion of Different Sensing Modalities
- **Arxiv ID**: http://arxiv.org/abs/2207.01927v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01927v2)
- **Published**: 2022-07-05 10:00:58+00:00
- **Updated**: 2022-10-14 14:24:14+00:00
- **Authors**: Fredrik Svanstr√∂m, Fernando Alonso-Fernandez, Cristofer Englund
- **Comment**: Under consideration at Drones
- **Journal**: None
- **Summary**: Automatic detection of flying drones is a key issue where its presence, especially if unauthorized, can create risky situations or compromise security. Here, we design and evaluate a multi-sensor drone detection system. In conjunction with standard video cameras and microphone sensors, we explore the use of thermal infrared cameras, pointed out as a feasible and promising solution that is scarcely addressed in the related literature. Our solution integrates a fish-eye camera as well to monitor a wider part of the sky and steer the other cameras towards objects of interest. The sensing solutions are complemented with an ADS-B receiver, a GPS receiver, and a radar module. However, our final deployment has not included the latter due to its limited detection range. The thermal camera is shown to be a feasible solution as good as the video camera, even if the camera employed here has a lower resolution. Two other novelties of our work are the creation of a new public dataset of multi-sensor annotated data that expands the number of classes compared to existing ones, as well as the study of the detector performance as a function of the sensor-to-target distance. Sensor fusion is also explored, showing that the system can be made more robust in this way, mitigating false detections of the individual sensors.



### Image Coding for Machines with Omnipotent Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.01932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01932v2)
- **Published**: 2022-07-05 10:13:11+00:00
- **Updated**: 2022-07-07 05:26:07+00:00
- **Authors**: Ruoyu Feng, Xin Jin, Zongyu Guo, Runsen Feng, Yixin Gao, Tianyu He, Zhizheng Zhang, Simeng Sun, Zhibo Chen
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Image Coding for Machines (ICM) aims to compress images for AI tasks analysis rather than meeting human perception. Learning a kind of feature that is both general (for AI tasks) and compact (for compression) is pivotal for its success. In this paper, we attempt to develop an ICM framework by learning universal features while also considering compression. We name such features as omnipotent features and the corresponding framework as Omni-ICM. Considering self-supervised learning (SSL) improves feature generalization, we integrate it with the compression task into the Omni-ICM framework to learn omnipotent features. However, it is non-trivial to coordinate semantics modeling in SSL and redundancy removing in compression, so we design a novel information filtering (IF) module between them by co-optimization of instance distinguishment and entropy minimization to adaptively drop information that is weakly related to AI tasks (e.g., some texture redundancy). Different from previous task-specific solutions, Omni-ICM could directly support AI tasks analysis based on the learned omnipotent features without joint training or extra transformation. Albeit simple and intuitive, Omni-ICM significantly outperforms existing traditional and learning-based codecs on multiple fundamental vision tasks.



### A Safe Semi-supervised Graph Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/2207.01960v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.01960v1)
- **Published**: 2022-07-05 11:03:12+00:00
- **Updated**: 2022-07-05 11:03:12+00:00
- **Authors**: Zhi Yang, Yadong Yan, Haitao Gan, Jing Zhao, Zhiwei Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In the semi-supervised learning field, Graph Convolution Network (GCN), as a variant model of GNN, has achieved promising results for non-Euclidean data by introducing convolution into GNN. However, GCN and its variant models fail to safely use the information of risk unlabeled data, which will degrade the performance of semi-supervised learning. Therefore, we propose a Safe GCN framework (Safe-GCN) to improve the learning performance. In the Safe-GCN, we design an iterative process to label the unlabeled data. In each iteration, a GCN and its supervised version(S-GCN) are learned to find the unlabeled data with high confidence. The high-confidence unlabeled data and their pseudo labels are then added to the label set. Finally, both added unlabeled data and labeled ones are used to train a S-GCN which can achieve the safe exploration of the risk unlabeled data and enable safe use of large numbers of unlabeled data. The performance of Safe-GCN is evaluated on three well-known citation network datasets and the obtained results demonstrate the effectiveness of the proposed framework over several graph-based semi-supervised learning methods.



### DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2207.01971v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.01971v6)
- **Published**: 2022-07-05 11:30:37+00:00
- **Updated**: 2023-03-27 04:10:14+00:00
- **Authors**: Yan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, Hao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: It is essential yet challenging for future home-assistant robots to understand and manipulate diverse 3D objects in daily human environments. Towards building scalable systems that can perform diverse manipulation tasks over various 3D shapes, recent works have advocated and demonstrated promising results learning visual actionable affordance, which labels every point over the input 3D geometry with an action likelihood of accomplishing the downstream task (e.g., pushing or picking-up). However, these works only studied single-gripper manipulation tasks, yet many real-world tasks require two hands to achieve collaboratively. In this work, we propose a novel learning framework, DualAfford, to learn collaborative affordance for dual-gripper manipulation tasks. The core design of the approach is to reduce the quadratic problem for two grippers into two disentangled yet interconnected subtasks for efficient learning. Using the large-scale PartNet-Mobility and ShapeNet datasets, we set up four benchmark tasks for dual-gripper manipulation. Experiments prove the effectiveness and superiority of our method over three baselines.



### Understanding and Improving Group Normalization
- **Arxiv ID**: http://arxiv.org/abs/2207.01972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01972v1)
- **Published**: 2022-07-05 11:31:39+00:00
- **Updated**: 2022-07-05 11:31:39+00:00
- **Authors**: Agus Gunawan, Xu Yin, Kang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Various normalization layers have been proposed to help the training of neural networks. Group Normalization (GN) is one of the effective and attractive studies that achieved significant performances in the visual recognition task. Despite the great success achieved, GN still has several issues that may negatively impact neural network training. In this paper, we introduce an analysis framework and discuss the working principles of GN in affecting the training process of the neural network. From experimental results, we conclude the real cause of GN's inferior performance against Batch normalization (BN): 1) \textbf{unstable training performance}, 2) \textbf{more sensitive} to distortion, whether it comes from external noise or perturbations introduced by the regularization. In addition, we found that GN can only help the neural network training in some specific period, unlike BN, which helps the network throughout the training. To solve these issues, we propose a new normalization layer built on top of GN, by incorporating the advantages of BN. Experimental results on the image classification task demonstrated that the proposed normalization layer outperforms the official GN to improve recognition accuracy regardless of the batch sizes and stabilize the network training.



### Federated Self-supervised Learning for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2207.01975v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01975v2)
- **Published**: 2022-07-05 11:39:35+00:00
- **Updated**: 2022-07-19 18:11:36+00:00
- **Authors**: Yasar Abbas Ur Rehman, Yan Gao, Jiajun Shen, Pedro Porto Buarque de Gusmao, Nicholas Lane
- **Comment**: None
- **Journal**: None
- **Summary**: The ubiquity of camera-enabled mobile devices has lead to large amounts of unlabelled video data being produced at the edge. Although various self-supervised learning (SSL) methods have been proposed to harvest their latent spatio-temporal representations for task-specific training, practical challenges including privacy concerns and communication costs prevent SSL from being deployed at large scales. To mitigate these issues, we propose the use of Federated Learning (FL) to the task of video SSL. In this work, we evaluate the performance of current state-of-the-art (SOTA) video-SSL techniques and identify their shortcomings when integrated into the large-scale FL setting simulated with kinetics-400 dataset. We follow by proposing a novel federated SSL framework for video, dubbed FedVSSL, that integrates different aggregation strategies and partial weight updating. Extensive experiments demonstrate the effectiveness and significance of FedVSSL as it outperforms the centralized SOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on HMDB-51.



### Query-Efficient Adversarial Attack Based on Latin Hypercube Sampling
- **Arxiv ID**: http://arxiv.org/abs/2207.02391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02391v1)
- **Published**: 2022-07-05 12:04:44+00:00
- **Updated**: 2022-07-05 12:04:44+00:00
- **Authors**: Dan Wang, Jiayu Lin, Yuan-Gen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In order to be applicable in real-world scenario, Boundary Attacks (BAs) were proposed and ensured one hundred percent attack success rate with only decision information. However, existing BA methods craft adversarial examples by leveraging a simple random sampling (SRS) to estimate the gradient, consuming a large number of model queries. To overcome the drawback of SRS, this paper proposes a Latin Hypercube Sampling based Boundary Attack (LHS-BA) to save query budget. Compared with SRS, LHS has better uniformity under the same limited number of random samples. Therefore, the average on these random samples is closer to the true gradient than that estimated by SRS. Various experiments are conducted on benchmark datasets including MNIST, CIFAR, and ImageNet-1K. Experimental results demonstrate the superiority of the proposed LHS-BA over the state-of-the-art BA methods in terms of query efficiency. The source codes are publicly available at https://github.com/GZHU-DVL/LHS-BA.



### Open-Vocabulary 3D Detection via Image-level Class and Debiased Cross-modal Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.01987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.01987v1)
- **Published**: 2022-07-05 12:13:52+00:00
- **Updated**: 2022-07-05 12:13:52+00:00
- **Authors**: Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer, Shanghang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Current point-cloud detection methods have difficulty detecting the open-vocabulary objects in the real world, due to their limited generalization capability. Moreover, it is extremely laborious and expensive to collect and fully annotate a point-cloud detection dataset with numerous classes of objects, leading to the limited classes of existing point-cloud datasets and hindering the model to learn general representations to achieve open-vocabulary point-cloud detection. As far as we know, we are the first to study the problem of open-vocabulary 3D point-cloud detection. Instead of seeking a point-cloud dataset with full labels, we resort to ImageNet1K to broaden the vocabulary of the point-cloud detector. We propose OV-3DETIC, an Open-Vocabulary 3D DETector using Image-level Class supervision. Specifically, we take advantage of two modalities, the image modality for recognition and the point-cloud modality for localization, to generate pseudo labels for unseen classes. Then we propose a novel debiased cross-modal contrastive learning method to transfer the knowledge from image modality to point-cloud modality during training. Without hurting the latency during inference, OV-3DETIC makes the point-cloud detector capable of achieving open-vocabulary detection. Extensive experiments demonstrate that the proposed OV-3DETIC achieves at least 10.77 % mAP improvement (absolute value) and 9.56 % mAP improvement (absolute value) by a wide range of baselines on the SUN-RGBD dataset and ScanNet dataset, respectively. Besides, we conduct sufficient experiments to shed light on why the proposed OV-3DETIC works.



### Multiview Detection with Cardboard Human Modeling
- **Arxiv ID**: http://arxiv.org/abs/2207.02013v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02013v5)
- **Published**: 2022-07-05 12:47:26+00:00
- **Updated**: 2023-01-06 00:55:01+00:00
- **Authors**: Jiahao Ma, Zicheng Duan, Liang Zheng, Chuong Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Multiview detection uses multiple calibrated cameras with overlapping fields of views to locate occluded pedestrians. In this field, existing methods typically adopt a ``human modeling - aggregation'' strategy. To find robust pedestrian representations, some intuitively incorporate 2D perception results from each frame, while others use entire frame features projected to the ground plane. However, the former does not consider the human appearance and leads to many ambiguities, and the latter suffers from projection errors due to the lack of accurate height of the human torso and head. In this paper, we propose a new pedestrian representation scheme based on human point clouds modeling. Specifically, using ray tracing for holistic human depth estimation, we model pedestrians as upright, thin cardboard point clouds on the ground. Then, we aggregate the point clouds of the pedestrian cardboard across multiple views for a final decision. Compared with existing representations, the proposed method explicitly leverages human appearance and reduces projection errors significantly by relatively accurate height estimation. On four standard evaluation benchmarks, the proposed method achieves very competitive results. Our code and data will be released at https://github.com/ZichengDuan/MvCHM.



### DeepPS2: Revisiting Photometric Stereo Using Two Differently Illuminated Images
- **Arxiv ID**: http://arxiv.org/abs/2207.02025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02025v2)
- **Published**: 2022-07-05 13:14:10+00:00
- **Updated**: 2022-08-30 08:56:21+00:00
- **Authors**: Ashish Tiwari, Shanmuganathan Raman
- **Comment**: Accepted in ECCV 2022 Project Page:
  https://sites.google.com/iitgn.ac.in/deepps2/home
- **Journal**: None
- **Summary**: Photometric stereo, a problem of recovering 3D surface normals using images of an object captured under different lightings, has been of great interest and importance in computer vision research. Despite the success of existing traditional and deep learning-based methods, it is still challenging due to: (i) the requirement of three or more differently illuminated images, (ii) the inability to model unknown general reflectance, and (iii) the requirement of accurate 3D ground truth surface normals and known lighting information for training. In this work, we attempt to address an under-explored problem of photometric stereo using just two differently illuminated images, referred to as the PS2 problem. It is an intermediate case between a single image-based reconstruction method like Shape from Shading (SfS) and the traditional Photometric Stereo (PS), which requires three or more images. We propose an inverse rendering-based deep learning framework, called DeepPS2, that jointly performs surface normal, albedo, lighting estimation, and image relighting in a completely self-supervised manner with no requirement of ground truth data. We demonstrate how image relighting in conjunction with image reconstruction enhances the lighting estimation in a self-supervised setting.



### CNN-based Local Vision Transformer for COVID-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2207.02027v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02027v1)
- **Published**: 2022-07-05 13:16:57+00:00
- **Updated**: 2022-07-05 13:16:57+00:00
- **Authors**: Hongyan Xu, Xiu Su, Dadong Wang
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning technology can be used as an assistive technology to help doctors quickly and accurately identify COVID-19 infections. Recently, Vision Transformer (ViT) has shown great potential towards image classification due to its global receptive field. However, due to the lack of inductive biases inherent to CNNs, the ViT-based structure leads to limited feature richness and difficulty in model training. In this paper, we propose a new structure called Transformer for COVID-19 (COVT) to improve the performance of ViT-based architectures on small COVID-19 datasets. It uses CNN as a feature extractor to effectively extract local structural information, and introduces average pooling to ViT's Multilayer Perception(MLP) module for global information. Experiments show the effectiveness of our method on the two COVID-19 datasets and the ImageNet dataset.



### AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture
- **Arxiv ID**: http://arxiv.org/abs/2207.02031v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02031v2)
- **Published**: 2022-07-05 13:21:01+00:00
- **Updated**: 2022-07-12 07:53:48+00:00
- **Authors**: Zhe Li, Zerong Zheng, Hongwen Zhang, Chaonan Ji, Yebin Liu
- **Comment**: Accepted by ECCV 2022, project page:
  http://www.liuyebin.com/avatarcap/avatarcap.html, code:
  https://github.com/lizhe00/AvatarCap
- **Journal**: None
- **Summary**: To address the ill-posed problem caused by partial observations in monocular human volumetric capture, we present AvatarCap, a novel framework that introduces animatable avatars into the capture pipeline for high-fidelity reconstruction in both visible and invisible regions. Our method firstly creates an animatable avatar for the subject from a small number (~20) of 3D scans as a prior. Then given a monocular RGB video of this subject, our method integrates information from both the image observation and the avatar prior, and accordingly recon-structs high-fidelity 3D textured models with dynamic details regardless of the visibility. To learn an effective avatar for volumetric capture from only few samples, we propose GeoTexAvatar, which leverages both geometry and texture supervisions to constrain the pose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned volumetric capture method that involves a canonical normal fusion and a reconstruction network is further proposed to integrate both image observations and avatar dynamics for high-fidelity reconstruction in both observed and invisible regions. Overall, our method enables monocular human volumetric capture with detailed and pose-dependent dynamics, and the experiments show that our method outperforms state of the art. Code is available at https://github.com/lizhe00/AvatarCap.



### PRoA: A Probabilistic Robustness Assessment against Functional Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2207.02036v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, 68T07, I.2; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2207.02036v1)
- **Published**: 2022-07-05 13:27:38+00:00
- **Updated**: 2022-07-05 13:27:38+00:00
- **Authors**: Tianle Zhang, Wenjie Ruan, Jonathan E. Fieldsend
- **Comment**: The short version of this work will appear in the Proceedings of the
  2022 European Conference on Machine Learning and Data Mining (ECML-PKDD 2022)
- **Journal**: None
- **Summary**: In safety-critical deep learning applications robustness measurement is a vital pre-deployment phase. However, existing robustness verification methods are not sufficiently practical for deploying machine learning systems in the real world. On the one hand, these methods attempt to claim that no perturbations can ``fool'' deep neural networks (DNNs), which may be too stringent in practice. On the other hand, existing works rigorously consider $L_p$ bounded additive perturbations on the pixel space, although perturbations, such as colour shifting and geometric transformations, are more practically and frequently occurring in the real world. Thus, from the practical standpoint, we present a novel and general {\it probabilistic robustness assessment method} (PRoA) based on the adaptive concentration, and it can measure the robustness of deep learning models against functional perturbations. PRoA can provide statistical guarantees on the probabilistic robustness of a model, \textit{i.e.}, the probability of failure encountered by the trained model after deployment. Our experiments demonstrate the effectiveness and flexibility of PRoA in terms of evaluating the probabilistic robustness against a broad range of functional perturbations, and PRoA can scale well to various large-scale deep neural networks compared to existing state-of-the-art baselines. For the purpose of reproducibility, we release our tool on GitHub: \url{ https://github.com/TrustAI/PRoA}.



### PKD: General Distillation Framework for Object Detectors via Pearson Correlation Coefficient
- **Arxiv ID**: http://arxiv.org/abs/2207.02039v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.02039v2)
- **Published**: 2022-07-05 13:37:34+00:00
- **Updated**: 2022-11-30 15:01:22+00:00
- **Authors**: Weihan Cao, Yifan Zhang, Jianfei Gao, Anda Cheng, Ke Cheng, Jian Cheng
- **Comment**: Accepted in NeurIPS 2022
- **Journal**: None
- **Summary**: Knowledge distillation(KD) is a widely-used technique to train compact models in object detection. However, there is still a lack of study on how to distill between heterogeneous detectors. In this paper, we empirically find that better FPN features from a heterogeneous teacher detector can help the student although their detection heads and label assignments are different. However, directly aligning the feature maps to distill detectors suffers from two problems. First, the difference in feature magnitude between the teacher and the student could enforce overly strict constraints on the student. Second, the FPN stages and channels with large feature magnitude from the teacher model could dominate the gradient of distillation loss, which will overwhelm the effects of other features in KD and introduce much noise. To address the above issues, we propose to imitate features with Pearson Correlation Coefficient to focus on the relational information from the teacher and relax constraints on the magnitude of the features. Our method consistently outperforms the existing detection KD methods and works for both homogeneous and heterogeneous student-teacher pairs. Furthermore, it converges faster. With a powerful MaskRCNN-Swin detector as the teacher, ResNet-50 based RetinaNet and FCOS achieve 41.5% and 43.9% mAP on COCO2017, which are 4.1\% and 4.8\% higher than the baseline, respectively.



### MVP: Robust Multi-View Practice for Driving Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2207.02042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02042v1)
- **Published**: 2022-07-05 13:38:10+00:00
- **Updated**: 2022-07-05 13:38:10+00:00
- **Authors**: Jingjie Shang, Kunchang Li, Kaibin Tian, Haisheng Su, Yangguang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Distracted driving causes thousands of deaths per year, and how to apply deep-learning methods to prevent these tragedies has become a crucial problem. In Track3 of the 6th AI City Challenge, researchers provide a high-quality video dataset with densely action annotations. Due to the small data scale and unclear action boundary, the dataset presents a unique challenge to precisely localize all the different actions and classify their categories. In this paper, we make good use of the multi-view synchronization among videos, and conduct robust Multi-View Practice (MVP) for driving action localization. To avoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the feature extractor. Then the features of different views are passed to ActionFormer to generate candidate action proposals. For precisely localizing all the actions, we design elaborate post-processing, including model voting, threshold filtering and duplication removal. The results show that our MVP is robust for driving action localization, which achieves 28.49% F1-score in the Track3 test set.



### Transformer based Models for Unsupervised Anomaly Segmentation in Brain MR Images
- **Arxiv ID**: http://arxiv.org/abs/2207.02059v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2207.02059v2)
- **Published**: 2022-07-05 14:07:53+00:00
- **Updated**: 2022-08-29 11:37:39+00:00
- **Authors**: Ahmed Ghorbel, Ahmed Aldahdooh, Shadi Albarqouni, Wassim Hamidouche
- **Comment**: None
- **Journal**: None
- **Summary**: The quality of patient care associated with diagnostic radiology is proportionate to a physician workload. Segmentation is a fundamental limiting precursor to both diagnostic and therapeutic procedures. Advances in machine learning (ML) aim to increase diagnostic efficiency by replacing a single application with generalized algorithms. The goal of unsupervised anomaly detection (UAD) is to identify potential anomalous regions unseen during training, where convolutional neural network (CNN) based autoencoders (AEs) and variational autoencoders (VAEs) are considered a de facto approach for reconstruction based-anomaly segmentation. The restricted receptive field in CNNs limits the CNN to model the global context. Hence, if the anomalous regions cover large parts of the image, the CNN-based AEs are not capable of bringing a semantic understanding of the image. Meanwhile, vision transformers (ViTs) have emerged as a competitive alternative to CNNs. It relies on the self-attention mechanism that can relate image patches to each other. We investigate in this paper Transformer capabilities in building AEs for the reconstruction-based UAD task to reconstruct a coherent and more realistic image. We focus on anomaly segmentation for brain magnetic resonance imaging (MRI) and present five Transformer-based models while enabling segmentation performance comparable to or superior to state-of-the-art (SOTA) models. The source code is made publicly available on GitHub: https://github.com/ahmedgh970/Transformers_Unsupervised_Anomaly_Segmentation.git.



### Image Amodal Completion: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2207.02062v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02062v2)
- **Published**: 2022-07-05 14:13:22+00:00
- **Updated**: 2022-12-16 03:51:02+00:00
- **Authors**: Jiayang Ao, Qiuhong Ke, Krista A. Ehinger
- **Comment**: The manuscript is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: Existing computer vision systems can compete with humans in understanding the visible parts of objects, but still fall far short of humans when it comes to depicting the invisible parts of partially occluded objects. Image amodal completion aims to equip computers with human-like amodal completion functions to understand an intact object despite it being partially occluded. The main purpose of this survey is to provide an intuitive understanding of the research hotspots, key technologies and future trends in the field of image amodal completion. Firstly, we present a comprehensive review of the latest literature in this emerging field, exploring three key tasks in image amodal completion, including amodal shape completion, amodal appearance completion, and order perception. Then we examine popular datasets related to image amodal completion along with their common data collection methods and evaluation metrics. Finally, we discuss real-world applications and future research directions for image amodal completion, facilitating the reader's understanding of the challenges of existing technologies and upcoming research trends.



### RepMix: Representation Mixing for Robust Attribution of Synthesized Images
- **Arxiv ID**: http://arxiv.org/abs/2207.02063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02063v2)
- **Published**: 2022-07-05 14:14:06+00:00
- **Updated**: 2022-07-12 22:47:35+00:00
- **Authors**: Tu Bui, Ning Yu, John Collomosse
- **Comment**: Accepted at ECCV 2022; fix typo, add supmat
- **Journal**: None
- **Summary**: Rapid advances in Generative Adversarial Networks (GANs) raise new challenges for image attribution; detecting whether an image is synthetic and, if so, determining which GAN architecture created it. Uniquely, we present a solution to this task capable of 1) matching images invariant to their semantic content; 2) robust to benign transformations (changes in quality, resolution, shape, etc.) commonly encountered as images are re-shared online. In order to formalize our research, a challenging benchmark, Attribution88, is collected for robust and practical image attribution. We then propose RepMix, our GAN fingerprinting technique based on representation mixing and a novel loss. We validate its capability of tracing the provenance of GAN-generated images invariant to the semantic content of the image and also robust to perturbations. We show our approach improves significantly from existing GAN fingerprinting works on both semantic generalization and robustness. Data and code are available at https://github.com/TuBui/image_attribution.



### Test-time Adaptation for Real Image Denoising via Meta-transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.02066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02066v1)
- **Published**: 2022-07-05 14:17:35+00:00
- **Updated**: 2022-07-05 14:17:35+00:00
- **Authors**: Agus Gunawan, Muhammad Adi Nugroho, Se Jin Park
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, a ton of research has been conducted on real image denoising tasks. However, the efforts are more focused on improving real image denoising through creating a better network architecture. We explore a different direction where we propose to improve real image denoising performance through a better learning strategy that can enable test-time adaptation on the multi-task network. The learning strategy is two stages where the first stage pre-train the network using meta-auxiliary learning to get better meta-initialization. Meanwhile, we use meta-learning for fine-tuning (meta-transfer learning) the network as the second stage of our training to enable test-time adaptation on real noisy images. To exploit a better learning strategy, we also propose a network architecture with self-supervised masked reconstruction loss. Experiments on a real noisy dataset show the contribution of the proposed method and show that the proposed method can outperform other SOTA methods.



### A Densely Interconnected Network for Deep Learning Accelerated MRI
- **Arxiv ID**: http://arxiv.org/abs/2207.02073v2
- **DOI**: 10.1007/s10334-022-01041-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02073v2)
- **Published**: 2022-07-05 14:24:05+00:00
- **Updated**: 2023-05-22 11:54:09+00:00
- **Authors**: Jon Andre Ottesen, Matthan W. A. Caan, Inge Rasmus Groote, Atle Bj√∏rnerud
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: To improve accelerated MRI reconstruction through a densely connected cascading deep learning reconstruction framework.   Materials and Methods: A cascading deep learning reconstruction framework (baseline model) was modified by applying three architectural modifications: Input-level dense connections between cascade inputs and outputs, an improved deep learning sub-network, and long-range skip-connections between subsequent deep learning networks. An ablation study was performed, where five model configurations were trained on the NYU fastMRI neuro dataset with an end-to-end scheme conjunct on four- and eight-fold acceleration. The trained models were evaluated by comparing their respective structural similarity index measure (SSIM), normalized mean square error (NMSE) and peak signal to noise ratio (PSNR).   Results: The proposed densely interconnected residual cascading network (DIRCN), utilizing all three suggested modifications, achieved a SSIM improvement of 8% and 11% for four- and eight-fold acceleration, respectively. For eight-fold acceleration, the model achieved a 23% decrease in the NMSE when compared to the baseline model. In an ablation study, the individual architectural modifications all contributed to this improvement, by reducing the SSIM and NMSE with approximately 3% and 5% for four-fold acceleration, respectively.   Conclusion: The proposed architectural modifications allow for simple adjustments on an already existing cascading framework to further improve the resulting reconstructions.



### SiamMask: A Framework for Fast Online Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.02088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02088v1)
- **Published**: 2022-07-05 14:47:17+00:00
- **Updated**: 2022-07-05 14:47:17+00:00
- **Authors**: Weiming Hu, Qiang Wang, Li Zhang, Luca Bertinetto, Philip H. S. Torr
- **Comment**: 17 pages, Accepted by TPAMI 2022. arXiv admin note: substantial text
  overlap with arXiv:1812.05050
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI) 2022
- **Summary**: In this paper we introduce SiamMask, a framework to perform both visual object tracking and video object segmentation, in real-time, with the same simple method. We improve the offline training procedure of popular fully-convolutional Siamese approaches by augmenting their losses with a binary segmentation task. Once the offline training is completed, SiamMask only requires a single bounding box for initialization and can simultaneously carry out visual object tracking and segmentation at high frame-rates. Moreover, we show that it is possible to extend the framework to handle multiple object tracking and segmentation by simply re-using the multi-task model in a cascaded fashion. Experimental results show that our approach has high processing efficiency, at around 55 frames per second. It yields real-time state-of-the-art results on visual-object tracking benchmarks, while at the same time demonstrating competitive performance at a high speed for video object segmentation benchmarks.



### CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis
- **Arxiv ID**: http://arxiv.org/abs/2207.02091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02091v1)
- **Published**: 2022-07-05 14:50:21+00:00
- **Updated**: 2022-07-05 14:50:21+00:00
- **Authors**: Ignacio Sarasua, Sebastian P√∂lsterl, Christian Wachinger
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling temporal changes in subcortical structures is crucial for a better understanding of the progression of Alzheimer's disease (AD). Given their flexibility to adapt to heterogeneous sequence lengths, mesh-based transformer architectures have been proposed in the past for predicting hippocampus deformations across time. However, one of the main limitations of transformers is the large amount of trainable parameters, which makes the application on small datasets very challenging. In addition, current methods do not include relevant non-image information that can help to identify AD-related patterns in the progression. To this end, we introduce CASHformer, a transformer-based framework to model longitudinal shape trajectories in AD. CASHformer incorporates the idea of pre-trained transformers as universal compute engines that generalize across a wide range of tasks by freezing most layers during fine-tuning. This reduces the number of parameters by over 90% with respect to the original model and therefore enables the application of large models on small datasets without overfitting. In addition, CASHformer models cognitive decline to reveal AD atrophy patterns in the temporal sequence. Our results show that CASHformer reduces the reconstruction error by 73% compared to previously proposed methods. Moreover, the accuracy of detecting patients progressing to AD increases by 3% with imputing missing longitudinal shape data.



### Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/2207.02094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02094v1)
- **Published**: 2022-07-05 14:55:56+00:00
- **Updated**: 2022-07-05 14:55:56+00:00
- **Authors**: Marla Narazani, Ignacio Sarasua, Sebastian P√∂lsterl, Aldana Lizarraga, Igor Yakushev, Christian Wachinger
- **Comment**: None
- **Journal**: None
- **Summary**: Alzheimer's Disease (AD) is the most common form of dementia and often difficult to diagnose due to the multifactorial etiology of dementia. Recent works on neuroimaging-based computer-aided diagnosis with deep neural networks (DNNs) showed that fusing structural magnetic resonance images (sMRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) leads to improved accuracy in a study population of healthy controls and subjects with AD. However, this result conflicts with the established clinical knowledge that FDG-PET better captures AD-specific pathologies than sMRI. Therefore, we propose a framework for the systematic evaluation of multi-modal DNNs and critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI for binary healthy vs. AD, and three-way healthy/mild cognitive impairment/AD classification. Our experiments demonstrate that a single-modality network using FDG-PET performs better than MRI (accuracy 0.91 vs 0.87) and does not show improvement when combined. This conforms with the established clinical knowledge on AD biomarkers, but raises questions about the true benefit of multi-modal DNNs. We argue that future work on multi-modal fusion should systematically assess the contribution of individual modalities following our proposed evaluation framework. Finally, we encourage the community to go beyond healthy vs. AD classification and focus on differential diagnosis of dementia, where fusing multi-modal image information conforms with a clinical need.



### Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality
- **Arxiv ID**: http://arxiv.org/abs/2207.02119v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2207.02119v1)
- **Published**: 2022-07-05 15:39:29+00:00
- **Updated**: 2022-07-05 15:39:29+00:00
- **Authors**: Yue Song, Nicu Sebe, Wei Wang
- **Comment**: Accepted by ECCV22
- **Journal**: None
- **Summary**: Inserting an SVD meta-layer into neural networks is prone to make the covariance ill-conditioned, which could harm the model in the training stability and generalization abilities. In this paper, we systematically study how to improve the covariance conditioning by enforcing orthogonality to the Pre-SVD layer. Existing orthogonal treatments on the weights are first investigated. However, these techniques can improve the conditioning but would hurt the performance. To avoid such a side effect, we propose the Nearest Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of our methods is validated in two applications: decorrelated Batch Normalization (BN) and Global Covariance Pooling (GCP). Extensive experiments on visual recognition demonstrate that our methods can simultaneously improve the covariance conditioning and generalization. Moreover, the combinations with orthogonal weight can further boost the performances.



### Improving Semantic Segmentation in Transformers using Hierarchical Inter-Level Attention
- **Arxiv ID**: http://arxiv.org/abs/2207.02126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02126v1)
- **Published**: 2022-07-05 15:47:31+00:00
- **Updated**: 2022-07-05 15:47:31+00:00
- **Authors**: Gary Leung, Jun Gao, Xiaohui Zeng, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: Existing transformer-based image backbones typically propagate feature information in one direction from lower to higher-levels. This may not be ideal since the localization ability to delineate accurate object boundaries, is most prominent in the lower, high-resolution feature maps, while the semantics that can disambiguate image signals belonging to one object vs. another, typically emerges in a higher level of processing. We present Hierarchical Inter-Level Attention (HILA), an attention-based method that captures Bottom-Up and Top-Down Updates between features of different levels. HILA extends hierarchical vision transformer architectures by adding local connections between features of higher and lower levels to the backbone encoder. In each iteration, we construct a hierarchy by having higher-level features compete for assignments to update lower-level features belonging to them, iteratively resolving object-part relationships. These improved lower-level features are then used to re-update the higher-level features. HILA can be integrated into the majority of hierarchical architectures without requiring any changes to the base model. We add HILA into SegFormer and the Swin Transformer and show notable improvements in accuracy in semantic segmentation with fewer parameters and FLOPS. Project website and code: https://www.cs.toronto.edu/~garyleung/hila/



### Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI
- **Arxiv ID**: http://arxiv.org/abs/2207.02390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02390v1)
- **Published**: 2022-07-05 15:56:46+00:00
- **Updated**: 2022-07-05 15:56:46+00:00
- **Authors**: Jiahao Huang, Xiaodan Xing, Zhifan Gao, Guang Yang
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Fast MRI aims to reconstruct a high fidelity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.



### Deep Learning for Finger Vein Recognition: A Brief Survey of Recent Trend
- **Arxiv ID**: http://arxiv.org/abs/2207.02148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.02148v1)
- **Published**: 2022-07-05 16:14:36+00:00
- **Updated**: 2022-07-05 16:14:36+00:00
- **Authors**: Renye Zhang, Yimin Yin, Wanxia Deng, Chen Li, Jinghua Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Finger vein image recognition technology plays an important role in biometric recognition and has been successfully applied in many fields. Because veins are buried beneath the skin tissue, finger vein image recognition has an unparalleled advantage, which is not easily disturbed by external factors. This review summarizes 46 papers about deep learning for finger vein image recognition from 2017 to 2021. These papers are summarized according to the tasks of deep neural networks. Besides, we present the challenges and potential development directions of finger vein image recognition.



### Class-Specific Semantic Reconstruction for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.02158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02158v1)
- **Published**: 2022-07-05 16:25:34+00:00
- **Updated**: 2022-07-05 16:25:34+00:00
- **Authors**: Hongzhi Huang, Yu Wang, Qinghua Hu, Ming-Ming Cheng
- **Comment**: 14 pages, 10 figures,
- **Journal**: None
- **Summary**: Open set recognition enables deep neural networks (DNNs) to identify samples of unknown classes, while maintaining high classification accuracy on samples of known classes. Existing methods basing on auto-encoder (AE) and prototype learning show great potential in handling this challenging task. In this study, we propose a novel method, called Class-Specific Semantic Reconstruction (CSSR), that integrates the power of AE and prototype learning. Specifically, CSSR replaces prototype points with manifolds represented by class-specific AEs. Unlike conventional prototype-based methods, CSSR models each known class on an individual AE manifold, and measures class belongingness through AE's reconstruction error. Class-specific AEs are plugged into the top of the DNN backbone and reconstruct the semantic representations learned by the DNN instead of the raw image. Through end-to-end learning, the DNN and the AEs boost each other to learn both discriminative and representative information. The results of experiments conducted on multiple datasets show that the proposed method achieves outstanding performance in both close and open set recognition and is sufficiently simple and flexible to incorporate into existing frameworks.



### Robustness Analysis of Video-Language Models Against Visual and Language Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2207.02159v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.02159v4)
- **Published**: 2022-07-05 16:26:05+00:00
- **Updated**: 2023-07-18 17:23:25+00:00
- **Authors**: Madeline C. Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S. Rawat, Vibhav Vineet
- **Comment**: NeurIPS 2022 Datasets and Benchmarks Track. This projects webpage is
  located at https://bit.ly/3CNOly4
- **Journal**: Proceedings of the Neural Information Processing Systems Track on
  Datasets and Benchmarks (2022)
- **Summary**: Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are generally more susceptible when only video is perturbed as opposed to when only text is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.



### Automatic inspection of cultural monuments using deep and tensor-based learning on hyperspectral imagery
- **Arxiv ID**: http://arxiv.org/abs/2207.02163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02163v1)
- **Published**: 2022-07-05 16:38:27+00:00
- **Updated**: 2022-07-05 16:38:27+00:00
- **Authors**: Ioannis N. Tzortzis, Ioannis Rallis, Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis, Athanasios Voulodimos
- **Comment**: Accepted for presentation in IEEE International Conference on Image
  Processing (ICIP 2022)
- **Journal**: None
- **Summary**: In Cultural Heritage, hyperspectral images are commonly used since they provide extended information regarding the optical properties of materials. Thus, the processing of such high-dimensional data becomes challenging from the perspective of machine learning techniques to be applied. In this paper, we propose a Rank-$R$ tensor-based learning model to identify and classify material defects on Cultural Heritage monuments. In contrast to conventional deep learning approaches, the proposed high order tensor-based learning demonstrates greater accuracy and robustness against overfitting. Experimental results on real-world data from UNESCO protected areas indicate the superiority of the proposed scheme compared to conventional deep learning models.



### DBN-Mix: Training Dual Branch Network Using Bilateral Mixup Augmentation for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.02173v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02173v2)
- **Published**: 2022-07-05 17:01:27+00:00
- **Updated**: 2022-08-20 06:32:19+00:00
- **Authors**: Jae Soon Baik, In Young Yoon, Jun Won Choi
- **Comment**: None
- **Journal**: None
- **Summary**: There is growing interest in the challenging visual perception task of learning from long-tailed class distributions. The extreme class imbalance in the training dataset biases the model to prefer recognizing majority class data over minority class data. Furthermore, the lack of diversity in minority class samples makes it difficult to find a good representation. In this paper, we propose an effective data augmentation method, referred to as bilateral mixup augmentation, which can improve the performance of long-tailed visual recognition. The bilateral mixup augmentation combines two samples generated by a uniform sampler and a re-balanced sampler and augments the training dataset to enhance the representation learning for minority classes. We also reduce the classifier bias using class-wise temperature scaling, which scales the logits differently per class in the training phase. We apply both ideas to the dual-branch network (DBN) framework, presenting a new model, named dual-branch network with bilateral mixup (DBN-Mix). Experiments on popular long-tailed visual recognition datasets show that DBN-Mix improves performance significantly over baseline and that the proposed method achieves state-of-the-art performance in some categories of benchmarks.



### Activation Template Matching Loss for Explainable Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.02179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.02179v1)
- **Published**: 2022-07-05 17:16:04+00:00
- **Updated**: 2022-07-05 17:16:04+00:00
- **Authors**: Huawei Lin, Haozhe Liu, Qiufu Li, Linlin Shen
- **Comment**: 13 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Can we construct an explainable face recognition network able to learn a facial part-based feature like eyes, nose, mouth and so forth, without any manual annotation or additionalsion datasets? In this paper, we propose a generic Explainable Channel Loss (ECLoss) to construct an explainable face recognition network. The explainable network trained with ECLoss can easily learn the facial part-based representation on the target convolutional layer, where an individual channel can detect a certain face part. Our experiments on dozens of datasets show that ECLoss achieves superior explainability metrics, and at the same time improves the performance of face verification without face alignment. In addition, our visualization results also illustrate the effectiveness of the proposed ECLoss.



### ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.02182v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02182v2)
- **Published**: 2022-07-05 17:25:59+00:00
- **Updated**: 2022-10-16 14:52:01+00:00
- **Authors**: Jae Soon Baik, In Young Yoon, Jun Won Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning has achieved great success in various fields. However, it requires the labeling of huge amounts of data, which is expensive and labor-intensive. Active learning (AL), which identifies the most informative samples to be labeled, is becoming increasingly important to maximize the efficiency of the training process. The existing AL methods mostly use only a single final fixed model for acquiring the samples to be labeled. This strategy may not be good enough in that the structural uncertainty of a model for given training data is not considered to acquire the samples. In this study, we propose a novel acquisition criterion based on temporal self-ensemble generated by conventional stochastic gradient descent (SGD) optimization. These self-ensemble models are obtained by capturing the intermediate network weights obtained through SGD iterations. Our acquisition function relies on a consistency measure between the student and teacher models. The student models are given a fixed number of temporal self-ensemble models, and the teacher model is constructed by averaging the weights of the student models. Using the proposed acquisition criterion, we present an AL algorithm, namely student-teacher consistency-based AL (ST-CoNAL). Experiments conducted for image classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny ImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly better performance than the existing acquisition methods. Furthermore, extensive experiments show the robustness and effectiveness of our methods.



### CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations
- **Arxiv ID**: http://arxiv.org/abs/2207.02185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02185v1)
- **Published**: 2022-07-05 17:38:59+00:00
- **Updated**: 2022-07-05 17:38:59+00:00
- **Authors**: Jialu Li, Hao Tan, Mohit Bansal
- **Comment**: NAACL 2022 Findings (18 pages)
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) tasks require an agent to navigate through the environment based on language instructions. In this paper, we aim to solve two key challenges in this task: utilizing multilingual instructions for improved instruction-path grounding and navigating through new environments that are unseen during training. To address these challenges, we propose CLEAR: Cross-Lingual and Environment-Agnostic Representations. First, our agent learns a shared and visually-aligned cross-lingual language representation for the three languages (English, Hindi and Telugu) in the Room-Across-Room dataset. Our language representation learning is guided by text pairs that are aligned by visual information. Second, our agent learns an environment-agnostic visual representation by maximizing the similarity between semantically-aligned image pairs (with constraints on object-matching) from different environments. Our environment agnostic visual representation can mitigate the environment bias induced by low-level visual information. Empirically, on the Room-Across-Room dataset, we show that our multilingual agent gets large improvements in all metrics over the strong baseline model when generalizing to unseen environments with the cross-lingual language representation and the environment-agnostic visual representation. Furthermore, we show that our learned language and visual representations can be successfully transferred to the Room-to-Room and Cooperative Vision-and-Dialogue Navigation task, and present detailed qualitative and quantitative generalization and grounding analysis. Our code is available at https://github.com/jialuli-luka/CLEAR



### NeuralPassthrough: Learned Real-Time View Synthesis for VR
- **Arxiv ID**: http://arxiv.org/abs/2207.02186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02186v1)
- **Published**: 2022-07-05 17:39:22+00:00
- **Updated**: 2022-07-05 17:39:22+00:00
- **Authors**: Lei Xiao, Salah Nouri, Joel Hegland, Alberto Garcia Garcia, Douglas Lanman
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: Virtual reality (VR) headsets provide an immersive, stereoscopic visual experience, but at the cost of blocking users from directly observing their physical environment. Passthrough techniques are intended to address this limitation by leveraging outward-facing cameras to reconstruct the images that would otherwise be seen by the user without the headset. This is inherently a real-time view synthesis challenge, since passthrough cameras cannot be physically co-located with the eyes. Existing passthrough techniques suffer from distracting reconstruction artifacts, largely due to the lack of accurate depth information (especially for near-field and disoccluded objects), and also exhibit limited image quality (e.g., being low resolution and monochromatic). In this paper, we propose the first learned passthrough method and assess its performance using a custom VR headset that contains a stereo pair of RGB cameras. Through both simulations and experiments, we demonstrate that our learned passthrough method delivers superior image quality compared to state-of-the-art methods, while meeting strict VR requirements for real-time, perspective-correct stereoscopic view synthesis over a wide field of view for desktop-connected headsets.



### Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling
- **Arxiv ID**: http://arxiv.org/abs/2207.02196v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02196v4)
- **Published**: 2022-07-05 17:55:42+00:00
- **Updated**: 2022-12-05 13:22:07+00:00
- **Authors**: Hengyuan Ma, Li Zhang, Xiatian Zhu, Jianfeng Feng
- **Comment**: ECCV 2022. Code is available at https://github.com/fudan-zvg/PDS
- **Journal**: None
- **Summary**: Score-based generative models (SGMs) have recently emerged as a promising class of generative models. However, a fundamental limitation is that their inference is very slow due to a need for many (e.g., 2000) iterations of sequential computations. An intuitive acceleration method is to reduce the sampling iterations which however causes severe performance degradation. We investigate this problem by viewing the diffusion sampling process as a Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause to be ill-conditioned curvature. Under this insight, we propose a model-agnostic preconditioned diffusion sampling (PDS) method that leverages matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS is proven theoretically to converge to the original target distribution of a SGM, no need for retraining. Extensive experiments on three image datasets with a variety of resolutions and diversity validate that PDS consistently accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In particular, PDS can accelerate by up to 29x on more challenging high resolution (1024x1024) image generation.



### Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.02201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.02201v1)
- **Published**: 2022-07-05 17:59:17+00:00
- **Updated**: 2022-07-05 17:59:17+00:00
- **Authors**: Jiadai Sun, Yuchao Dai, Xianjing Zhang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
- **Comment**: Accepted by IROS2022. Code: https://github.com/haomo-ai/MotionSeg3D
- **Journal**: None
- **Summary**: Accurate moving object segmentation is an essential task for autonomous driving. It can provide effective information for many downstream tasks, such as collision avoidance, path planning, and static map construction. How to effectively exploit the spatial-temporal information is a critical question for 3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a novel deep neural network exploiting both spatial-temporal information and different representation modalities of LiDAR scans to improve LiDAR-MOS performance. Specifically, we first use a range image-based dual-branch structure to separately deal with spatial and temporal information that can be obtained from sequential LiDAR scans, and later combine them using motion-guided attention modules. We also use a point refinement module via 3D sparse convolution to fuse the information from both LiDAR range image and point cloud representations and reduce the artifacts on the borders of the objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods significantly in terms of LiDAR-MOS IoU. Benefiting from the devised coarse-to-fine architecture, our method operates online at sensor frame rate. The implementation of our method is available as open source at: https://github.com/haomo-ai/MotionSeg3D.



### CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.02202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02202v2)
- **Published**: 2022-07-05 17:59:28+00:00
- **Updated**: 2022-09-25 07:19:32+00:00
- **Authors**: Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei Zhou, Jiaqi Ma
- **Comment**: CoRL 2022; code: https://github.com/DerrickXuNu/CoBEVT
- **Journal**: None
- **Summary**: Bird's eye view (BEV) semantic segmentation plays a crucial role in spatial sensing for autonomous driving. Although recent literature has made significant progress on BEV map understanding, they are all based on single-agent camera-based systems. These solutions sometimes have difficulty handling occlusions or detecting distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V) communication technologies have enabled autonomous vehicles to share sensing information, dramatically improving the perception performance and range compared to single-agent systems. In this paper, we propose CoBEVT, the first generic multi-agent multi-camera perception framework that can cooperatively generate BEV map predictions. To efficiently fuse camera features from multi-view and multi-agent data in an underlying Transformer architecture, we design a fused axial attention module (FAX), which captures sparsely local and global spatial interactions across views and agents. The extensive experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT achieves state-of-the-art performance for cooperative BEV semantic segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks, including 1) BEV segmentation with single-agent multi-camera and 2) 3D object detection with multi-agent LiDAR systems, achieving state-of-the-art performance with real-time inference speed. The code is available at https://github.com/DerrickXuNu/CoBEVT.



### Detecting and Recovering Sequential DeepFake Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2207.02204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02204v1)
- **Published**: 2022-07-05 17:59:33+00:00
- **Updated**: 2022-07-05 17:59:33+00:00
- **Authors**: Rui Shao, Tianxing Wu, Ziwei Liu
- **Comment**: ECCV 2022. Project page:
  https://rshaojimmy.github.io/Projects/SeqDeepFake Code:
  https://github.com/rshaojimmy/SeqDeepFake
- **Journal**: None
- **Summary**: Since photorealistic faces can be readily generated by facial manipulation technologies nowadays, potential malicious abuse of these technologies has drawn great concerns. Numerous deepfake detection methods are thus proposed. However, existing methods only focus on detecting one-step facial manipulation. As the emergence of easy-accessible facial editing applications, people can easily manipulate facial components using multi-step operations in a sequential manner. This new threat requires us to detect a sequence of facial manipulations, which is vital for both detecting deepfake media and recovering original faces afterwards. Motivated by this observation, we emphasize the need and propose a novel research problem called Detecting Sequential DeepFake Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only demanding a binary label prediction, detecting Seq-DeepFake manipulation requires correctly predicting a sequential vector of facial manipulation operations. To support a large-scale investigation, we construct the first Seq-DeepFake dataset, where face images are manipulated sequentially with corresponding annotations of sequential facial manipulation vectors. Based on this new dataset, we cast detecting Seq-DeepFake manipulation as a specific image-to-sequence (e.g. image captioning) task and propose a concise yet effective Seq-DeepFake Transformer (SeqFakeFormer). Moreover, we build a comprehensive benchmark and set up rigorous evaluation protocols and metrics for this new research problem. Extensive experiments demonstrate the effectiveness of SeqFakeFormer. Several valuable observations are also revealed to facilitate future research in broader deepfake detection problems.



### Clustered Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.02205v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2207.02205v1)
- **Published**: 2022-07-05 17:59:37+00:00
- **Updated**: 2022-07-05 17:59:37+00:00
- **Authors**: Rezvan Sherkati, James J. Clark
- **Comment**: 21 pages, 4 figures
- **Journal**: None
- **Summary**: We present a new method for image salience prediction, Clustered Saliency Prediction. This method divides individuals into clusters based on their personal features and their known saliency maps, and generates a separate image salience model for each cluster. We test our approach on a public dataset of personalized saliency maps, with varying importance weights for personal feature factors and observe the effects on the clusters. For each cluster, we use an image-to-image translation method, mainly Pix2Pix model, to convert universal saliency maps to saliency maps of that cluster. We try three state-of-the-art universal saliency prediction methods, DeepGaze II, ML-Net and SalGAN, and see their impact on the results. We show that our Clustered Saliency Prediction technique outperforms the state-of-the-art universal saliency prediction models. Also we demonstrate the effectiveness of our clustering method by comparing the results of Clustered Saliency Prediction using clusters obtained by Subject Similarity Clustering algorithm with two baseline methods. We propose an approach to assign new people to the most appropriate cluster, based on their personal features and any known saliency maps. In our experiments we see that this method of assigning new people to a cluster on average chooses the cluster that gives higher saliency scores.



### Segmenting Moving Objects via an Object-Centric Layered Representation
- **Arxiv ID**: http://arxiv.org/abs/2207.02206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02206v2)
- **Published**: 2022-07-05 17:59:43+00:00
- **Updated**: 2022-11-12 20:53:16+00:00
- **Authors**: Junyu Xie, Weidi Xie, Andrew Zisserman
- **Comment**: NeurIPS 2022. Total 29 pages, 13 figures (including main text: 10
  pages, 5 figures)
- **Journal**: None
- **Summary**: The objective of this paper is a model that is able to discover, track and segment multiple moving objects in a video. We make four contributions: First, we introduce an object-centric segmentation model with a depth-ordered layer representation. This is implemented using a variant of the transformer architecture that ingests optical flow, where each query vector specifies an object and its layer for the entire video. The model can effectively discover multiple moving objects and handle mutual occlusions; Second, we introduce a scalable pipeline for generating multi-object synthetic training data via layer compositions, that is used to train the proposed model, significantly reducing the requirements for labour-intensive annotations, and supporting Sim2Real generalisation; Third, we conduct thorough ablation studies, showing that the model is able to learn object permanence and temporal shape consistency, and is able to predict amodal segmentation masks; Fourth, we evaluate our model, trained only on synthetic data, on standard video segmentation benchmarks, DAVIS, MoCA, SegTrack, FBMS-59, and achieve state-of-the-art performance among existing methods that do not rely on any manual annotations. With test-time adaptation, we observe further performance boosts.



### Guiding Machine Perception with Psychophysics
- **Arxiv ID**: http://arxiv.org/abs/2207.02241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2207.02241v1)
- **Published**: 2022-07-05 18:01:38+00:00
- **Updated**: 2022-07-05 18:01:38+00:00
- **Authors**: Justin Dulay, Sonia Poltoratski, Till S. Hartmann, Samuel E. Anthony, Walter J. Scheirer
- **Comment**: 6 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: {G}{ustav} Fechner's 1860 delineation of psychophysics, the measurement of sensation in relation to its stimulus, is widely considered to be the advent of modern psychological science. In psychophysics, a researcher parametrically varies some aspects of a stimulus, and measures the resulting changes in a human subject's experience of that stimulus; doing so gives insight to the determining relationship between a sensation and the physical input that evoked it. This approach is used heavily in perceptual domains, including signal detection, threshold measurement, and ideal observer analysis. Scientific fields like vision science have always leaned heavily on the methods and procedures of psychophysics, but there is now growing appreciation of them by machine learning researchers, sparked by widening overlap between biological and artificial perception \cite{rojas2011automatic, scheirer2014perceptual,escalera2014chalearn,zhang2018agil, grieggs2021measuring}. Machine perception that is guided by behavioral measurements, as opposed to guidance restricted to arbitrarily assigned human labels, has significant potential to fuel further progress in artificial intelligence.



### Video-based Surgical Skills Assessment using Long term Tool Tracking
- **Arxiv ID**: http://arxiv.org/abs/2207.02247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02247v1)
- **Published**: 2022-07-05 18:15:28+00:00
- **Updated**: 2022-07-05 18:15:28+00:00
- **Authors**: Mona Fathollahi, Mohammad Hasan Sarhan, Ramon Pena, Lela DiMonte, Anshu Gupta, Aishani Ataliwala, Jocelyn Barker
- **Comment**: Accepted at MICCAI 2022
- **Journal**: None
- **Summary**: Mastering the technical skills required to perform surgery is an extremely challenging task. Video-based assessment allows surgeons to receive feedback on their technical skills to facilitate learning and development. Currently, this feedback comes primarily from manual video review, which is time-intensive and limits the feasibility of tracking a surgeon's progress over many cases. In this work, we introduce a motion-based approach to automatically assess surgical skills from surgical case video feed. The proposed pipeline first tracks surgical tools reliably to create motion trajectories and then uses those trajectories to predict surgeon technical skill levels. The tracking algorithm employs a simple yet effective re-identification module that improves ID-switch compared to other state-of-the-art methods. This is critical for creating reliable tool trajectories when instruments regularly move on- and off-screen or are periodically obscured. The motion-based classification model employs a state-of-the-art self-attention transformer network to capture short- and long-term motion patterns that are essential for skill evaluation. The proposed method is evaluated on an in-vivo (Cholec80) dataset where an expert-rated GOALS skill assessment of the Calot Triangle Dissection is used as a quantitative skill measure. We compare transformer-based skill assessment with traditional machine learning approaches using the proposed and state-of-the-art tracking. Our result suggests that using motion trajectories from reliable tracking methods is beneficial for assessing surgeon skills based solely on video streams.



### Array Camera Image Fusion using Physics-Aware Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.02250v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02250v1)
- **Published**: 2022-07-05 18:24:20+00:00
- **Updated**: 2022-07-05 18:24:20+00:00
- **Authors**: Qian Huang, Minghao Hu, David Jones Brady
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate a physics-aware transformer for feature-based data fusion from cameras with diverse resolution, color spaces, focal planes, focal lengths, and exposure. We also demonstrate a scalable solution for synthetic training data generation for the transformer using open-source computer graphics software. We demonstrate image synthesis on arrays with diverse spectral responses, instantaneous field of view and frame rate.



### OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.02255v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02255v3)
- **Published**: 2022-07-05 18:32:21+00:00
- **Updated**: 2022-08-02 11:10:12+00:00
- **Authors**: Jialun Pei, Tianyang Cheng, Deng-Ping Fan, He Tang, Chuanbo Chen, Luc Van Gool
- **Comment**: This paper has been accepted by ECCV2022
- **Journal**: None
- **Summary**: We present OSFormer, the first one-stage transformer framework for camouflaged instance segmentation (CIS). OSFormer is based on two key designs. First, we design a location-sensing transformer (LST) to obtain the location label and instance-aware parameters by introducing the location-guided queries and the blend-convolution feedforward network. Second, we develop a coarse-to-fine fusion (CFF) to merge diverse context information from the LST encoder and CNN backbone. Coupling these two components enables OSFormer to efficiently blend local features and long-range context dependencies for predicting camouflaged instances. Compared with two-stage frameworks, our OSFormer reaches 41% AP and achieves good convergence efficiency without requiring enormous training data, i.e., only 3,040 samples under 60 epochs. Code link: https://github.com/PJLallen/OSFormer.



### OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.02261v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02261v2)
- **Published**: 2022-07-05 18:51:05+00:00
- **Updated**: 2022-07-28 04:39:32+00:00
- **Authors**: Mamshad Nayeem Rizve, Navid Kardan, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) is one of the dominant approaches to address the annotation bottleneck of supervised learning. Recent SSL methods can effectively leverage a large repository of unlabeled data to improve performance while relying on a small set of labeled data. One common assumption in most SSL methods is that the labeled and unlabeled data are from the same data distribution. However, this is hardly the case in many real-world scenarios, which limits their applicability. In this work, instead, we attempt to solve the challenging open-world SSL problem that does not make such an assumption. In the open-world SSL problem, the objective is to recognize samples of known classes, and simultaneously detect and cluster samples belonging to novel classes present in unlabeled data. This work introduces OpenLDN that utilizes a pairwise similarity loss to discover novel classes. Using a bi-level optimization rule this pairwise similarity loss exploits the information available in the labeled set to implicitly cluster novel class samples, while simultaneously recognizing samples from known classes. After discovering novel classes, OpenLDN transforms the open-world SSL problem into a standard SSL problem to achieve additional performance gains using existing SSL methods. Our extensive experiments demonstrate that OpenLDN outperforms the current state-of-the-art methods on multiple popular classification benchmarks while providing a better accuracy/training time trade-off.



### Towards Realistic Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.02269v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02269v2)
- **Published**: 2022-07-05 19:04:43+00:00
- **Updated**: 2022-07-28 04:25:40+00:00
- **Authors**: Mamshad Nayeem Rizve, Navid Kardan, Mubarak Shah
- **Comment**: Accepted to ECCV 2022 (Oral)
- **Journal**: None
- **Summary**: Deep learning is pushing the state-of-the-art in many computer vision applications. However, it relies on large annotated data repositories, and capturing the unconstrained nature of the real-world data is yet to be solved. Semi-supervised learning (SSL) complements the annotated training data with a large corpus of unlabeled data to reduce annotation cost. The standard SSL approach assumes unlabeled data are from the same distribution as annotated data. Recently, a more realistic SSL problem, called open-world SSL, is introduced, where the unannotated data might contain samples from unknown classes. In this paper, we propose a novel pseudo-label based approach to tackle SSL in open-world setting. At the core of our method, we utilize sample uncertainty and incorporate prior knowledge about class distribution to generate reliable class-distribution-aware pseudo-labels for unlabeled data belonging to both known and unknown classes. Our extensive experimentation showcases the effectiveness of our approach on several benchmark datasets, where it substantially outperforms the existing state-of-the-art on seven diverse datasets including CIFAR-100 (~17%), ImageNet-100 (~5%), and Tiny ImageNet (~9%). We also highlight the flexibility of our approach in solving novel class discovery task, demonstrate its stability in dealing with imbalanced data, and complement our approach with a technique to estimate the number of novel classes



### Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.02279v1
- **DOI**: 10.1109/SSCI50451.2021.9660004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02279v1)
- **Published**: 2022-07-05 19:44:34+00:00
- **Updated**: 2022-07-05 19:44:34+00:00
- **Authors**: Asiegbu Miracle Kanu-Asiegbu, Ram Vasudevan, Xiaoxiao Du
- **Comment**: Accepted to 2021 IEEE Symposium Series on Computational Intelligence
  (SSCI)
- **Journal**: None
- **Summary**: Video anomaly detection is a core problem in vision. Correctly detecting and identifying anomalous behaviors in pedestrians from video data will enable safety-critical applications such as surveillance, activity monitoring, and human-robot interaction. In this paper, we propose to leverage trajectory localization and prediction for unsupervised pedestrian anomaly event detection. Different than previous reconstruction-based approaches, our proposed framework rely on the prediction errors of normal and abnormal pedestrian trajectories to detect anomalies spatially and temporally. We present experimental results on real-world benchmark datasets on varying timescales and show that our proposed trajectory-predictor-based anomaly detection pipeline is effective and efficient at identifying anomalous activities of pedestrians in videos. Code will be made available at https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.



### BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for Pedestrian Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.02281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02281v1)
- **Published**: 2022-07-05 19:45:49+00:00
- **Updated**: 2022-07-05 19:45:49+00:00
- **Authors**: Asiegbu Miracle Kanu-Asiegbu, Ram Vasudevan, Xiaoxiao Du
- **Comment**: Accepted in Workshop on Safe Learning for Autonomous Driving,
  co-located with the International Conference on Machine Learning (ICML 20222)
- **Journal**: None
- **Summary**: We present BiPOCO, a Bi-directional trajectory predictor with POse COnstraints, for detecting anomalous activities of pedestrians in videos. In contrast to prior work based on feature reconstruction, our work identifies pedestrian anomalous events by forecasting their future trajectories and comparing the predictions with their expectations. We introduce a set of novel compositional pose-based losses with our predictor and leverage prediction errors of each body joint for pedestrian anomaly detection. Experimental results show that our BiPOCO approach can detect pedestrian anomalous activities with a high detection rate (up to 87.0%) and incorporating pose constraints helps distinguish normal and anomalous poses in prediction. This work extends current literature of using prediction-based methods for anomaly detection and can benefit safety-critical applications such as autonomous driving and surveillance. Code is available at https://github.com/akanuasiegbu/BiPOCO.



### Effectivity of super resolution convolutional neural network for the enhancement of land cover classification from medium resolution satellite images
- **Arxiv ID**: http://arxiv.org/abs/2207.02301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02301v1)
- **Published**: 2022-07-05 20:48:03+00:00
- **Updated**: 2022-07-05 20:48:03+00:00
- **Authors**: Pritom Bose, Debolina Halder, Oliur Rahman, Turash Haque Pial
- **Comment**: This manuscript was originally prepared for 38th Asian Conference on
  Remote Sensing, 2017
- **Journal**: None
- **Summary**: In the modern world, satellite images play a key role in forest management and degradation monitoring. For a precise quantification of forest land cover changes, the availability of spatially fine resolution data is a necessity. Since 1972, NASAs LANDSAT Satellites are providing terrestrial images covering every corner of the earth, which have been proved to be a highly useful resource for terrestrial change analysis and have been used in numerous other sectors. However, freely accessible satellite images are, generally, of medium to low resolution which is a major hindrance to the precision of the analysis. Hence, we performed a comprehensive study to prove our point that, enhancement of resolution by Super-Resolution Convolutional Neural Network (SRCNN) will lessen the chance of misclassification of pixels, even under the established recognition methods. We tested the method on original LANDSAT-7 images of different regions of Sundarbans and their upscaled versions which were produced by bilinear interpolation, bicubic interpolation, and SRCNN respectively and it was discovered that SRCNN outperforms the others by a significant amount.



### A Deep Ensemble Learning Approach to Lung CT Segmentation for COVID-19 Severity Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.02322v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.02322v1)
- **Published**: 2022-07-05 21:28:52+00:00
- **Updated**: 2022-07-05 21:28:52+00:00
- **Authors**: Tal Ben-Haim, Ron Moshe Sofer, Gal Ben-Arie, Ilan Shelef, Tammy Riklin-Raviv
- **Comment**: IEEE International Conference on Image Processing (ICIP) 2022
- **Journal**: None
- **Summary**: We present a novel deep learning approach to categorical segmentation of lung CTs of COVID-19 patients. Specifically, we partition the scans into healthy lung tissues, non-lung regions, and two different, yet visually similar, pathological lung tissues, namely, ground-glass opacity and consolidation. This is accomplished via a unique, end-to-end hierarchical network architecture and ensemble learning, which contribute to the segmentation and provide a measure for segmentation uncertainty. The proposed framework achieves competitive results and outstanding generalization capabilities for three COVID-19 datasets. Our method is ranked second in a public Kaggle competition for COVID-19 CT images segmentation. Moreover, segmentation uncertainty regions are shown to correspond to the disagreements between the manual annotations of two different radiologists. Finally, preliminary promising correspondence results are shown for our private dataset when comparing the patients' COVID-19 severity scores (based on clinical measures), and the segmented lung pathologies. Code and data are available at our repository: https://github.com/talbenha/covid-seg



### TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.02327v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02327v3)
- **Published**: 2022-07-05 21:38:26+00:00
- **Updated**: 2022-07-11 02:33:31+00:00
- **Authors**: Fan Zhang, Tengfei Xue, Weidong Cai, Yogesh Rathi, Carl-Fredrik Westin, Lauren J O'Donnell
- **Comment**: 11 pages. 5 figures, MICCAI 2022
- **Journal**: None
- **Summary**: Diffusion MRI tractography is an advanced imaging technique for quantitative mapping of the brain's structural connectivity. Whole brain tractography (WBT) data contains over hundreds of thousands of individual fiber streamlines (estimated brain connections), and this data is usually parcellated to create compact representations for data analysis applications such as disease classification. In this paper, we propose a novel parcellation-free WBT analysis framework, TractoFormer, that leverages tractography information at the level of individual fiber streamlines and provides a natural mechanism for interpretation of results using the attention mechanism of transformers. TractoFormer includes two main contributions. First, we propose a novel and simple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber spatial relationships and any feature of interest that can be computed from individual fibers (such as FA or MD). Second, we design a network based on vision transformers (ViTs) that includes: 1) data augmentation to overcome model overfitting on small datasets, 2) identification of discriminative fibers for interpretation of results, and 3) ensemble learning to leverage fiber information from different brain regions. In a synthetic data experiment, TractoFormer successfully identifies discriminative fibers with simulated group differences. In a disease classification experiment comparing several methods, TractoFormer achieves the highest accuracy in classifying schizophrenia vs control. Discriminative fibers are identified in left hemispheric frontal and parietal superficial white matter regions, which have previously been shown to be affected in schizophrenia patients.



### Weakly Supervised Grounding for VQA in Vision-Language Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.02334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02334v1)
- **Published**: 2022-07-05 22:06:03+00:00
- **Updated**: 2022-07-05 22:06:03+00:00
- **Authors**: Aisha Urooj Khan, Hilde Kuehne, Chuang Gan, Niels Da Vitoria Lobo, Mubarak Shah
- **Comment**: To appear at ECCV 2022
- **Journal**: None
- **Summary**: Transformers for visual-language representation learning have been getting a lot of interest and shown tremendous performance on visual question answering (VQA) and grounding. But most systems that show good performance of those tasks still rely on pre-trained object detectors during training, which limits their applicability to the object classes available for those detectors. To mitigate this limitation, the following paper focuses on the problem of weakly supervised grounding in context of visual question answering in transformers. The approach leverages capsules by grouping each visual token in the visual encoder and uses activations from language self-attention layers as a text-guided selection module to mask those capsules before they are forwarded to the next layer. We evaluate our approach on the challenging GQA as well as VQA-HAT dataset for VQA grounding. Our experiments show that: while removing the information of masked objects from standard transformer architectures leads to a significant drop in performance, the integration of capsules significantly improves the grounding ability of such systems and provides new state-of-the-art results compared to other approaches in the field.



### Multi-Label Retinal Disease Classification using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.02335v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02335v3)
- **Published**: 2022-07-05 22:06:52+00:00
- **Updated**: 2022-07-28 19:35:22+00:00
- **Authors**: M. A. Rodriguez, H. AlMarzouqi, P. Liatsis
- **Comment**: 13 pages, 4 figures, 12 tables. Submitted to IEEE Journal of
  Biomedical and Health Informatics. Dataset:
  https://data.mendeley.com/datasets/pc4mb3h8hz/1
- **Journal**: None
- **Summary**: Early detection of retinal diseases is one of the most important means of preventing partial or permanent blindness in patients. In this research, a novel multi-label classification system is proposed for the detection of multiple retinal diseases, using fundus images collected from a variety of sources. First, a new multi-label retinal disease dataset, the MuReD dataset, is constructed, using a number of publicly available datasets for fundus disease classification. Next, a sequence of post-processing steps is applied to ensure the quality of the image data and the range of diseases, present in the dataset. For the first time in fundus multi-label disease classification, a transformer-based model optimized through extensive experimentation is used for image analysis and decision making. Numerous experiments are performed to optimize the configuration of the proposed system. It is shown that the approach performs better than state-of-the-art works on the same task by 7.9% and 8.1% in terms of AUC score for disease detection and disease classification, respectively. The obtained results further support the potential applications of transformer-based architectures in the medical imaging field.



### Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2207.02337v1
- **DOI**: 10.1007/978-3-031-11748-0_3
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2207.02337v1)
- **Published**: 2022-07-05 22:07:26+00:00
- **Updated**: 2022-07-05 22:07:26+00:00
- **Authors**: Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif
- **Comment**: Accepted for publication in edited book titled "Federated and
  Transfer Learning", Springer, Cham
- **Journal**: Federated and Transfer Learning, Springer International
  Publishing, Cham, pp. 29-55, 2023
- **Summary**: The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use more practical. One of the most significant advancements in this domain is the incorporation of transfer learning into federated learning, which overcomes fundamental constraints of primary federated learning, particularly in terms of security. This chapter performs a comprehensive survey on the intersection of federated and transfer learning from a security point of view. The main goal of this study is to uncover potential vulnerabilities and defense mechanisms that might compromise the privacy and performance of systems that use federated and transfer learning.



### Generalization to translation shifts: a study in architectures and augmentations
- **Arxiv ID**: http://arxiv.org/abs/2207.02349v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.02349v2)
- **Published**: 2022-07-05 22:52:20+00:00
- **Updated**: 2022-11-12 23:00:07+00:00
- **Authors**: Suriya Gunasekar
- **Comment**: None
- **Journal**: None
- **Summary**: We study how effective data augmentation is at capturing the inductive bias of carefully designed network architectures for spatial translation invariance. We evaluate various image classification architectures (antialiased, convolutional, vision transformer, and fully connected MLP networks) and data augmentation techniques towards generalization to large translation shifts. We observe that: (a) without data augmentation, all architectures, including convolutional networks with antialiased modification suffer some degradation in performance when evaluated on translated test distributions. Understandably, both the in-distribution accuracy and degradation to shifts is significantly worse for non-convolutional models. (b) The robustness of performance is improved by even a minimal augmentation of $4$ pixel random crop across all architectures. In some instances, even $1-2$ pixel random crop is sufficient. This suggests that there is a form of meta generalization from augmentation. For non-convolutional architectures, while the absolute accuracy is still low with this basic augmentation, we see substantial improvements in robustness to translation shifts. (c) With a sufficiently advanced augmentation pipeline ($4$ pixel crop+RandAugmentation+Erasing+MixUp), all architectures can be trained to have competitive performance in terms of in-distribution accuracy as well as generalization to large translation shifts.



### SNeRF: Stylized Neural Implicit Representations for 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2207.02363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.02363v1)
- **Published**: 2022-07-05 23:45:02+00:00
- **Updated**: 2022-07-05 23:45:02+00:00
- **Authors**: Thu Nguyen-Phuoc, Feng Liu, Lei Xiao
- **Comment**: SIGGRAPH 2022 (Journal track). Project page:
  https://research.facebook.com/publications/snerf-stylized-neural-implicit-representations-for-3d-scenes/
- **Journal**: None
- **Summary**: This paper presents a stylized novel view synthesis method. Applying state-of-the-art stylization methods to novel views frame by frame often causes jittering artifacts due to the lack of cross-view consistency. Therefore, this paper investigates 3D scene stylization that provides a strong inductive bias for consistent novel view synthesis. Specifically, we adopt the emerging neural radiance fields (NeRF) as our choice of 3D scene representation for their capability to render high-quality novel views for a variety of scenes. However, as rendering a novel view from a NeRF requires a large number of samples, training a stylized NeRF requires a large amount of GPU memory that goes beyond an off-the-shelf GPU capacity. We introduce a new training method to address this problem by alternating the NeRF and stylization optimization steps. Such a method enables us to make full use of our hardware memory capacity to both generate images at higher resolution and adopt more expressive image style transfer methods. Our experiments show that our method produces stylized NeRFs for a wide range of content, including indoor, outdoor and dynamic scenes, and synthesizes high-quality novel views with cross-view consistency.




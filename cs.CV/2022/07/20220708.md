# Arxiv Papers in cs.CV on 2022-07-08
### A Support Vector Model of Pruning Trees Evaluation Based on OTSU Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2207.03638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03638v1)
- **Published**: 2022-07-08 01:24:51+00:00
- **Updated**: 2022-07-08 01:24:51+00:00
- **Authors**: Yuefei Chen, Xinli Zheng, Chunhua Ju, Fuguang Bao
- **Comment**: None
- **Journal**: None
- **Summary**: The tree pruning process is the key to promoting fruits' growth and improving their productions due to effects on the photosynthesis efficiency of fruits and nutrition transportation in branches. Currently, pruning is still highly dependent on human labor. The workers' experience will strongly affect the robustness of the performance of the tree pruning. Thus, it is a challenge for workers and farmers to evaluate the pruning performance. Intended for a better solution to the problem, this paper presents a novel pruning classification strategy model called "OTSU-SVM" to evaluate the pruning performance based on the shadows of branches and leaves. This model considers not only the available illuminated area of the tree but also the uniformity of the illuminated area of the tree. More importantly, our group implements OTSU algorithm into the model, which highly reinforces robustness of the evaluation of this model. In addition, the data from the pear trees in the Yuhang District, Hangzhou is also used in the experiment. In this experiment, we prove that the OTSU-SVM has good accuracy with 80% and high performance in the evaluation of the pruning for the pear trees. It can provide more successful pruning if applied into the orchard. A successful pruning can broaden the illuminated area of individual fruit, and increase nutrition transportation from the target branch, dramatically elevating the weights and production of the fruits.



### Pruning Early Exit Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.03644v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2207.03644v1)
- **Published**: 2022-07-08 01:57:52+00:00
- **Updated**: 2022-07-08 01:57:52+00:00
- **Authors**: Alperen Görmez, Erdem Koyuncu
- **Comment**: 5 pages, 3 figures, Sparsity in Neural Networks Workshop 2022
- **Journal**: None
- **Summary**: Deep learning models that perform well often have high computational costs. In this paper, we combine two approaches that try to reduce the computational cost while keeping the model performance high: pruning and early exit networks. We evaluate two approaches of pruning early exit networks: (1) pruning the entire network at once, (2) pruning the base network and additional linear classifiers in an ordered fashion. Experimental results show that pruning the entire network at once is a better strategy in general. However, at high accuracy rates, the two approaches have a similar performance, which implies that the processes of pruning and early exit can be separated without loss of optimality.



### Abs-CAM: A Gradient Optimization Interpretable Approach for Explanation of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.03648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03648v1)
- **Published**: 2022-07-08 02:06:46+00:00
- **Updated**: 2022-07-08 02:06:46+00:00
- **Authors**: Chunyan Zeng, Kang Yan, Zhifeng Wang, Yan Yu, Shiyan Xia, Nan Zhao
- **Comment**: Abs-CAM for Explanation of Convolutional Neural Networks
- **Journal**: None
- **Summary**: The black-box nature of Deep Neural Networks (DNNs) severely hinders its performance improvement and application in specific scenes. In recent years, class activation mapping-based method has been widely used to interpret the internal decisions of models in computer vision tasks. However, when this method uses backpropagation to obtain gradients, it will cause noise in the saliency map, and even locate features that are irrelevant to decisions. In this paper, we propose an Absolute value Class Activation Mapping-based (Abs-CAM) method, which optimizes the gradients derived from the backpropagation and turns all of them into positive gradients to enhance the visual features of output neurons' activation, and improve the localization ability of the saliency map. The framework of Abs-CAM is divided into two phases: generating initial saliency map and generating final saliency map. The first phase improves the localization ability of the saliency map by optimizing the gradient, and the second phase linearly combines the initial saliency map with the original image to enhance the semantic information of the saliency map. We conduct qualitative and quantitative evaluation of the proposed method, including Deletion, Insertion, and Pointing Game. The experimental results show that the Abs-CAM can obviously eliminate the noise in the saliency map, and can better locate the features related to decisions, and is superior to the previous methods in recognition and localization tasks.



### Video Dialog as Conversation about Objects Living in Space-Time
- **Arxiv ID**: http://arxiv.org/abs/2207.03656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03656v1)
- **Published**: 2022-07-08 02:34:38+00:00
- **Updated**: 2022-07-08 02:34:38+00:00
- **Authors**: Hoang-Anh Pham, Thao Minh Le, Vuong Le, Tu Minh Phuong, Truyen Tran
- **Comment**: Accepted to ECCV 2022, code will be available at
  https://github.com/hoanganhpham1006/COST
- **Journal**: None
- **Summary**: It would be a technological feat to be able to create a system that can hold a meaningful conversation with humans about what they watch. A setup toward that goal is presented as a video dialog task, where the system is asked to generate natural utterances in response to a question in an ongoing dialog. The task poses great visual, linguistic, and reasoning challenges that cannot be easily overcome without an appropriate representation scheme over video and dialog that supports high-level reasoning. To tackle these challenges we present a new object-centric framework for video dialog that supports neural reasoning dubbed COST - which stands for Conversation about Objects in Space-Time. Here dynamic space-time visual content in videos is first parsed into object trajectories. Given this video abstraction, COST maintains and tracks object-associated dialog states, which are updated upon receiving new questions. Object interactions are dynamically and conditionally inferred for each question, and these serve as the basis for relational reasoning among them. COST also maintains a history of previous answers, and this allows retrieval of relevant object-centric information to enrich the answer forming process. Language production then proceeds in a step-wise manner, taking into the context of the current utterance, the existing dialog, the current question. We evaluate COST on the DSTC7 and DSTC8 benchmarks, demonstrating its competitiveness against state-of-the-arts.



### Deepfake Face Traceability with Disentangling Reversing Network
- **Arxiv ID**: http://arxiv.org/abs/2207.03666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03666v1)
- **Published**: 2022-07-08 03:05:28+00:00
- **Updated**: 2022-07-08 03:05:28+00:00
- **Authors**: Jiaxin Ai, Zhongyuan Wang, Baojin Huang, Zhen Han
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Deepfake face not only violates the privacy of personal identity, but also confuses the public and causes huge social harm. The current deepfake detection only stays at the level of distinguishing true and false, and cannot trace the original genuine face corresponding to the fake face, that is, it does not have the ability to trace the source of evidence. The deepfake countermeasure technology for judicial forensics urgently calls for deepfake traceability. This paper pioneers an interesting question about face deepfake, active forensics that "know it and how it happened". Given that deepfake faces do not completely discard the features of original faces, especially facial expressions and poses, we argue that original faces can be approximately speculated from their deepfake counterparts. Correspondingly, we design a disentangling reversing network that decouples latent space features of deepfake faces under the supervision of fake-original face pair samples to infer original faces in reverse.



### Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers
- **Arxiv ID**: http://arxiv.org/abs/2207.03489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2207.03489v1)
- **Published**: 2022-07-08 03:11:49+00:00
- **Updated**: 2022-07-08 03:11:49+00:00
- **Authors**: Hyuntai Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a mode decomposition (MD) method for degenerated modes has been studied. Convolution neural network (CNN) has been applied for image training and predicting the mode coefficients. Four-fold degenerated $LP_{11}$ series has been the target to be decomposed. Multiple images are regarded as an input to decompose the degenerate modes. Total of seven different images, including the full original near-field image, and images after linear polarizers of four directions (0$^\circ$, 45$^\circ$, 90$^\circ$, and 135$^\circ$), and images after two circular polarizers (right-handed and left-handed) has been considered for training, validation, and test. The output label of the model has been chosen as the real and imaginary components of the mode coefficient, and the loss function has been selected to be the root-mean-square (RMS) of the labels. The RMS and mean-absolute-error (MAE) of the label, intensity, phase, and field correlation between the actual and predicted values have been selected to be the metrics to evaluate the CNN model. The CNN model has been trained with 100,000 three-dimensional images with depths of three, four, and seven. The performance of the trained model was evaluated via 10,000 test samples with four sets of images - images after three linear polarizers (0$^\circ$, 45$^\circ$, 90$^\circ$) and image after right-handed circular polarizer - showed 0.0634 of label RMS, 0.0292 of intensity RMS, 0.1867 rad of phase MAE, and 0.9978 of average field correlation. The performance of 4 image sets showed at least 50.68\% of performance enhancement compared to models considering only images after linear polarizers.



### Learning High-quality Proposals for Acne Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.03674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03674v1)
- **Published**: 2022-07-08 03:28:29+00:00
- **Updated**: 2022-07-08 03:28:29+00:00
- **Authors**: Jianwei Zhang, Lei Zhang, Junyou Wang, Xin Wei, Jiaqi Li, Xian Jiang, Dan Du
- **Comment**: None
- **Journal**: None
- **Summary**: Acne detection is crucial for interpretative diagnosis and precise treatment of skin disease. The arbitrary boundary and small size of acne lesions lead to a significant number of poor-quality proposals in two-stage detection. In this paper, we propose a novel head structure for Region Proposal Network to improve the proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH) structure is proposed to disentangle the representation learning for classification and localization from two different spatial perspectives. The proposed SADH ensures a steeper classification confidence gradient and suppresses the proposals having low intersection-over-union(IoU) with the matched ground truth. Then, we propose a Normalized Wasserstein Distance prediction branch to improve the correlation between the proposals' classification scores and IoUs. In addition, to facilitate further research on acne detection, we construct a new dataset named AcneSCU, with high-resolution imageries, precise annotations, and fine-grained lesion categories. Extensive experiments are conducted on both AcneSCU and the public dataset ACNE04, and the results demonstrate the proposed method could improve the proposals' quality, consistently outperforming state-of-the-art approaches. Code and the collected dataset are available in https://github.com/pingguokiller/acnedetection.



### SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2207.03677v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03677v4)
- **Published**: 2022-07-08 03:44:34+00:00
- **Updated**: 2022-12-19 03:06:16+00:00
- **Authors**: Haoran You, Baopu Li, Zhanyi Sun, Xu Ouyang, Yingyan Lin
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has demonstrated amazing success in searching for efficient deep neural networks (DNNs) from a given supernet. In parallel, the lottery ticket hypothesis has shown that DNNs contain small subnetworks that can be trained from scratch to achieve a comparable or higher accuracy than original DNNs. As such, it is currently a common practice to develop efficient DNNs via a pipeline of first search and then prune. Nevertheless, doing so often requires a search-train-prune-retrain process and thus prohibitive computational cost. In this paper, we discover for the first time that both efficient DNNs and their lottery subnetworks (i.e., lottery tickets) can be directly identified from a supernet, which we term as SuperTickets, via a two-in-one training scheme with jointly architecture searching and parameter pruning. Moreover, we develop a progressive and unified SuperTickets identification strategy that allows the connectivity of subnetworks to change during supernet training, achieving better accuracy and efficiency trade-offs than conventional sparse training. Finally, we evaluate whether such identified SuperTickets drawn from one task can transfer well to other tasks, validating their potential of handling multiple tasks simultaneously. Extensive experiments and ablation studies on three tasks and four benchmark datasets validate that our proposed SuperTickets achieve boosted accuracy and efficiency trade-offs than both typical NAS and pruning pipelines, regardless of having retraining or not. Codes and pretrained models are available at https://github.com/RICE-EIC/SuperTickets.



### Music-driven Dance Regeneration with Controllable Key Pose Constraints
- **Arxiv ID**: http://arxiv.org/abs/2207.03682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.03682v1)
- **Published**: 2022-07-08 04:26:45+00:00
- **Updated**: 2022-07-08 04:26:45+00:00
- **Authors**: Junfu Pu, Ying Shan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel framework for music-driven dance motion synthesis with controllable key pose constraint. In contrast to methods that generate dance motion sequences only based on music without any other controllable conditions, this work targets on synthesizing high-quality dance motion driven by music as well as customized poses performed by users. Our model involves two single-modal transformer encoders for music and motion representations and a cross-modal transformer decoder for dance motions generation. The cross-modal transformer decoder achieves the capability of synthesizing smooth dance motion sequences, which keeps a consistency with key poses at corresponding positions, by introducing the local neighbor position embedding. Such mechanism makes the decoder more sensitive to key poses and the corresponding positions. Our dance synthesis model achieves satisfactory performance both on quantitative and qualitative evaluations with extensive experiments, which demonstrates the effectiveness of our proposed method.



### Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization
- **Arxiv ID**: http://arxiv.org/abs/2207.03684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03684v1)
- **Published**: 2022-07-08 04:34:39+00:00
- **Updated**: 2022-07-08 04:34:39+00:00
- **Authors**: Wei Feng, Lin Wang, Lie Ju, Xin Zhao, Xin Wang, Xiaoyu Shi, Zongyuan Ge
- **Comment**: 10 pages, 2 figures, Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Existing unsupervised domain adaptation methods based on adversarial learning have achieved good performance in several medical imaging tasks. However, these methods focus only on global distribution adaptation and ignore distribution constraints at the category level, which would lead to sub-optimal adaptation performance. This paper presents an unsupervised domain adaptation framework based on category-level regularization that regularizes the category distribution from three perspectives. Specifically, for inter-domain category regularization, an adaptive prototype alignment module is proposed to align feature prototypes of the same category in the source and target domains. In addition, for intra-domain category regularization, we tailored a regularization technique for the source and target domains, respectively. In the source domain, a prototype-guided discriminative loss is proposed to learn more discriminative feature representations by enforcing intra-class compactness and inter-class separability, and as a complement to traditional supervised loss. In the target domain, an augmented consistency category regularization loss is proposed to force the model to produce consistent predictions for augmented/unaugmented target images, which encourages semantically similar regions to be given the same label. Extensive experiments on two publicly fundus datasets show that the proposed approach significantly outperforms other state-of-the-art comparison algorithms.



### Test-Time Adaptation via Self-Training with Nearest Neighbor Information
- **Arxiv ID**: http://arxiv.org/abs/2207.10792v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.10792v2)
- **Published**: 2022-07-08 05:02:15+00:00
- **Updated**: 2023-02-28 03:21:35+00:00
- **Authors**: Minguk Jang, Sae-Young Chung, Hye Won Chung
- **Comment**: None
- **Journal**: None
- **Summary**: Test-time adaptation (TTA) aims to adapt a trained classifier using online unlabeled test data only, without any information related to the training procedure. Most existing TTA methods adapt the trained classifier using the classifier's prediction on the test data as pseudo-label. However, under test-time domain shift, accuracy of the pseudo labels cannot be guaranteed, and thus the TTA methods often encounter performance degradation at the adapted classifier. To overcome this limitation, we propose a novel test-time adaptation method, called Test-time Adaptation via Self-Training with nearest neighbor information (TAST), which is composed of the following procedures: (1) adds trainable adaptation modules on top of the trained feature extractor; (2) newly defines a pseudo-label distribution for the test data by using the nearest neighbor information; (3) trains these modules only a few times during test time to match the nearest neighbor-based pseudo label distribution and a prototype-based class distribution for the test data; and (4) predicts the label of test data using the average predicted class distribution from these modules. The pseudo-label generation is based on the basic intuition that a test data and its nearest neighbor in the embedding space are likely to share the same label under the domain shift. By utilizing multiple randomly initialized adaptation modules, TAST extracts useful information for the classification of the test data under the domain shift, using the nearest neighbor information. TAST showed better performance than the state-of-the-art TTA methods on two standard benchmark tasks, domain generalization, namely VLCS, PACS, OfficeHome, and TerraIncognita, and image corruption, particularly CIFAR-10/100C.



### Neural Implicit Dictionary via Mixture-of-Expert Training
- **Arxiv ID**: http://arxiv.org/abs/2207.03691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03691v1)
- **Published**: 2022-07-08 05:07:19+00:00
- **Updated**: 2022-07-08 05:07:19+00:00
- **Authors**: Peihao Wang, Zhiwen Fan, Tianlong Chen, Zhangyang Wang
- **Comment**: International Conference on Machine Learning (ICML), 2022
- **Journal**: None
- **Summary**: Representing visual signals by coordinate-based deep fully-connected networks has been shown advantageous in fitting complex details and solving inverse problems than discrete grid-based representation. However, acquiring such a continuous Implicit Neural Representation (INR) requires tedious per-scene training on tons of signal measurements, which limits its practicality. In this paper, we present a generic INR framework that achieves both data and training efficiency by learning a Neural Implicit Dictionary (NID) from a data collection and representing INR as a functional combination of basis sampled from the dictionary. Our NID assembles a group of coordinate-based subnetworks which are tuned to span the desired function space. After training, one can instantly and robustly acquire an unseen scene representation by solving the coding coefficients. To parallelly optimize a large group of networks, we borrow the idea from Mixture-of-Expert (MoE) to design and train our network with a sparse gating mechanism. Our experiments show that, NID can improve reconstruction of 2D images or 3D scenes by 2 orders of magnitude faster with up to 98% less input data. We further demonstrate various applications of NID in image inpainting and occlusion removal, which are considered to be challenging with vanilla INR. Our codes are available in https://github.com/VITA-Group/Neural-Implicit-Dict.



### Mining Discriminative Food Regions for Accurate Food Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.03692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03692v1)
- **Published**: 2022-07-08 05:09:24+00:00
- **Updated**: 2022-07-08 05:09:24+00:00
- **Authors**: Jianing Qiu, Frank P. -W. Lo, Yingnan Sun, Siyao Wang, Benny Lo
- **Comment**: Accepted in BMVC 2019 as a spotlight paper
- **Journal**: None
- **Summary**: Automatic food recognition is the very first step towards passive dietary monitoring. In this paper, we address the problem of food recognition by mining discriminative food regions. Taking inspiration from Adversarial Erasing, a strategy that progressively discovers discriminative object regions for weakly supervised semantic segmentation, we propose a novel network architecture in which a primary network maintains the base accuracy of classifying an input image, an auxiliary network adversarially mines discriminative food regions, and a region network classifies the resulting mined regions. The global (the original input image) and the local (the mined regions) representations are then integrated for the final prediction. The proposed architecture denoted as PAR-Net is end-to-end trainable, and highlights discriminative regions in an online fashion. In addition, we introduce a new fine-grained food dataset named as Sushi-50, which consists of 50 different sushi categories. Extensive experiments have been conducted to evaluate the proposed approach. On three food datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs consistently and achieves state-of-the-art results (top-1 testing accuracy of $90.4\%$, $90.2\%$, $92.0\%$, respectively) compared with other existing approaches. Dataset and code are available at https://github.com/Jianing-Qiu/PARNet



### SST-Calib: Simultaneous Spatial-Temporal Parameter Calibration between LIDAR and Camera
- **Arxiv ID**: http://arxiv.org/abs/2207.03704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.03704v1)
- **Published**: 2022-07-08 06:21:52+00:00
- **Updated**: 2022-07-08 06:21:52+00:00
- **Authors**: Akio Kodaira, Yiyang Zhou, Pengwei Zang, Wei Zhan, Masayoshi Tomizuka
- **Comment**: 7 pages, 4 figures, 4 tables, accepted by ITSC2022
- **Journal**: None
- **Summary**: With information from multiple input modalities, sensor fusion-based algorithms usually out-perform their single-modality counterparts in robotics. Camera and LIDAR, with complementary semantic and depth information, are the typical choices for detection tasks in complicated driving environments. For most camera-LIDAR fusion algorithms, however, the calibration of the sensor suite will greatly impact the performance. More specifically, the detection algorithm usually requires an accurate geometric relationship among multiple sensors as the input, and it is often assumed that the contents from these sensors are captured at the same time. Preparing such sensor suites involves carefully designed calibration rigs and accurate synchronization mechanisms, and the preparation process is usually done offline. In this work, a segmentation-based framework is proposed to jointly estimate the geometrical and temporal parameters in the calibration of a camera-LIDAR suite. A semantic segmentation mask is first applied to both sensor modalities, and the calibration parameters are optimized through pixel-wise bidirectional loss. We specifically incorporated the velocity information from optical flow for temporal parameters. Since supervision is only performed at the segmentation level, no calibration label is needed within the framework. The proposed algorithm is tested on the KITTI dataset, and the result shows an accurate real-time calibration of both geometric and temporal parameters.



### Video-based Smoky Vehicle Detection with A Coarse-to-Fine Framework
- **Arxiv ID**: http://arxiv.org/abs/2207.03708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03708v1)
- **Published**: 2022-07-08 06:42:45+00:00
- **Updated**: 2022-07-08 06:42:45+00:00
- **Authors**: Xiaojiang Peng, Xiaomao Fan, Qingyang Wu, Jieyan Zhao, Pan Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic smoky vehicle detection in videos is a superior solution to the traditional expensive remote sensing one with ultraviolet-infrared light devices for environmental protection agencies. However, it is challenging to distinguish vehicle smoke from shadow and wet regions coming from rear vehicle or clutter roads, and could be worse due to limited annotated data. In this paper, we first introduce a real-world large-scale smoky vehicle dataset with 75,000 annotated smoky vehicle images, facilitating the effective training of advanced deep learning models. To enable fair algorithm comparison, we also build a smoky vehicle video dataset including 163 long videos with segment-level annotations. Moreover, we present a new Coarse-to-fine Deep Smoky vehicle detection (CoDeS) framework for efficient smoky vehicle detection. The CoDeS first leverages a light-weight YOLO detector for fast smoke detection with high recall rate, and then applies a smoke-vehicle matching strategy to eliminate non-vehicle smoke, and finally uses a elaborately-designed 3D model to further refine the results in spatial temporal space. Extensive experiments in four metrics demonstrate that our framework is significantly superior to those hand-crafted feature based methods and recent advanced methods. The code and dataset will be released at https://github.com/pengxj/smokyvehicle.



### Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.03714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03714v1)
- **Published**: 2022-07-08 07:10:28+00:00
- **Updated**: 2022-07-08 07:10:28+00:00
- **Authors**: Yucheng Suo, Zhedong Zheng, Xiaohan Wang, Bang Zhang, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language is the window for people differently-abled to express their feelings as well as emotions. However, it remains challenging for people to learn sign language in a short time. To address this real-world challenge, in this work, we study the motion transfer system, which can transfer the user photo to the sign language video of specific words. In particular, the appearance content of the output video comes from the provided user image, while the motion of the video is extracted from the specified tutorial video. We observe two primary limitations in adopting the state-of-the-art motion transfer methods to sign language generation:(1) Existing motion transfer works ignore the prior geometrical knowledge of the human body. (2) The previous image animation methods only take image pairs as input in the training stage, which could not fully exploit the temporal information within videos. In an attempt to address the above-mentioned limitations, we propose Structure-aware Temporal Consistency Network (STCNet) to jointly optimize the prior structure of human with the temporal consistency for sign language video generation. There are two main contributions in this paper. (1) We harness a fine-grained skeleton detector to provide prior knowledge of the body keypoints. In this way, we ensure the keypoint movement in a valid range and make the model become more explainable and robust. (2) We introduce two cycle-consistency losses, i.e., short-term cycle loss and long-term cycle loss, which are conducted to assure the continuity of the generated video. We optimize the two losses and keypoint detector network in an end-to-end manner.



### Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom
- **Arxiv ID**: http://arxiv.org/abs/2207.03720v2
- **DOI**: 10.1109/ICIP46576.2022.9897588
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03720v2)
- **Published**: 2022-07-08 07:19:01+00:00
- **Updated**: 2022-11-11 11:10:29+00:00
- **Authors**: Michael G. Adam, Martin Piccolrovazzi, Sebastian Eger, Eckehard Steinbach
- **Comment**: 4 pages+1 Page references, 4 Figures, Best Paper Award First
  Runner-Up @ ICIP2022
- **Journal**: None
- **Summary**: The most popular evaluation metric for object detection in 2D images is Intersection over Union (IoU). Existing implementations of the IoU metric for 3D object detection usually neglect one or more degrees of freedom. In this paper, we first derive the analytic solution for three dimensional bounding boxes. As a second contribution, a closed-form solution of the volume-to-volume distance is derived. Finally, the Bounding Box Disparity is proposed as a combined positive continuous metric. We provide open source implementations of the three metrics as standalone python functions, as well as extensions to the Open3D library and as ROS nodes.



### Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.03723v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03723v1)
- **Published**: 2022-07-08 07:30:51+00:00
- **Updated**: 2022-07-08 07:30:51+00:00
- **Authors**: Liang Liao, Kangmin Xu, Haoning Wu, Chaofeng Chen, Wenxiu Sun, Qiong Yan, Weisi Lin
- **Comment**: Will appear on ACM MM 2022
- **Journal**: 2022 ACM International Conference on Multimedia
- **Summary**: With the rapid growth of in-the-wild videos taken by non-specialists, blind video quality assessment (VQA) has become a challenging and demanding problem. Although lots of efforts have been made to solve this problem, it remains unclear how the human visual system (HVS) relates to the temporal quality of videos. Meanwhile, recent work has found that the frames of natural video transformed into the perceptual domain of the HVS tend to form a straight trajectory of the representations. With the obtained insight that distortion impairs the perceived video quality and results in a curved trajectory of the perceptual representation, we propose a temporal perceptual quality index (TPQI) to measure the temporal distortion by describing the graphic morphology of the representation. Specifically, we first extract the video perceptual representations from the lateral geniculate nucleus (LGN) and primary visual area (V1) of the HVS, and then measure the straightness and compactness of their trajectories to quantify the degradation in naturalness and content continuity of video. Experiments show that the perceptual representation in the HVS is an effective way of predicting subjective temporal quality, and thus TPQI can, for the first time, achieve comparable performance to the spatial quality metric and be even more effective in assessing videos with large temporal variations. We further demonstrate that by combining with NIQE, a spatial quality metric, TPQI can achieve top performance over popular in-the-wild video datasets. More importantly, TPQI does not require any additional information beyond the video being evaluated and thus can be applied to any datasets without parameter tuning. Source code is available at https://github.com/UoLMM/TPQI-VQA.



### TGRMPT: A Head-Shoulder Aided Multi-Person Tracker and a New Large-Scale Dataset for Tour-Guide Robot
- **Arxiv ID**: http://arxiv.org/abs/2207.03726v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.03726v1)
- **Published**: 2022-07-08 07:32:18+00:00
- **Updated**: 2022-07-08 07:32:18+00:00
- **Authors**: Wen Wang, Shunda Hu, Shiqiang Zhu, Wei Song, Zheyuan Lin, Tianlei Jin, Zonghao Mu, Yuanhai Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: A service robot serving safely and politely needs to track the surrounding people robustly, especially for Tour-Guide Robot (TGR). However, existing multi-object tracking (MOT) or multi-person tracking (MPT) methods are not applicable to TGR for the following reasons: 1. lacking relevant large-scale datasets; 2. lacking applicable metrics to evaluate trackers. In this work, we target the visual perceptual tasks for TGR and present the TGRDB dataset, a novel large-scale multi-person tracking dataset containing roughly 5.6 hours of annotated videos and over 450 long-term trajectories. Besides, we propose a more applicable metric to evaluate trackers using our dataset. As part of our work, we present TGRMPT, a novel MPT system that incorporates information from head shoulder and whole body, and achieves state-of-the-art performance. We have released our codes and dataset in https://github.com/wenwenzju/TGRMPT.



### GEMS: Scene Expansion using Generative Models of Graphs
- **Arxiv ID**: http://arxiv.org/abs/2207.03729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03729v1)
- **Published**: 2022-07-08 07:41:28+00:00
- **Updated**: 2022-07-08 07:41:28+00:00
- **Authors**: Rishi Agarwal, Tirupati Saketh Chandra, Vaidehi Patil, Aniruddha Mahapatra, Kuldeep Kulkarni, Vishwa Vinay
- **Comment**: None
- **Journal**: None
- **Summary**: Applications based on image retrieval require editing and associating in intermediate spaces that are representative of the high-level concepts like objects and their relationships rather than dense, pixel-level representations like RGB images or semantic-label maps. We focus on one such representation, scene graphs, and propose a novel scene expansion task where we enrich an input seed graph by adding new nodes (objects) and the corresponding relationships. To this end, we formulate scene graph expansion as a sequential prediction task involving multiple steps of first predicting a new node and then predicting the set of relationships between the newly predicted node and previous nodes in the graph. We propose a sequencing strategy for observed graphs that retains the clustering patterns amongst nodes. In addition, we leverage external knowledge to train our graph generation model, enabling greater generalization of node predictions. Due to the inefficiency of existing maximum mean discrepancy (MMD) based metrics for graph generation problems in evaluating predicted relationships between nodes (objects), we design novel metrics that comprehensively evaluate different aspects of predicted relations. We conduct extensive experiments on Visual Genome and VRD datasets to evaluate the expanded scene graphs using the standard MMD-based metrics and our proposed metrics. We observe that the graphs generated by our method, GEMS, better represent the real distribution of the scene graphs than the baseline methods like GraphRNN.



### Combining Deep Learning with Good Old-Fashioned Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03757v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03757v2)
- **Published**: 2022-07-08 08:58:43+00:00
- **Updated**: 2022-11-11 10:14:02+00:00
- **Authors**: Moshe Sipper
- **Comment**: SN Computer Science, 2022
- **Journal**: None
- **Summary**: We present a comprehensive, stacking-based framework for combining deep learning with good old-fashioned machine learning, called Deep GOld. Our framework involves ensemble selection from 51 retrained pretrained deep networks as first-level models, and 10 machine-learning algorithms as second-level models. Enabled by today's state-of-the-art software tools and hardware platforms, Deep GOld delivers consistent improvement when tested on four image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny ImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original networks' performance.



### Virtual Axle Detector based on Analysis of Bridge Acceleration Measurements by Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2207.03758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03758v1)
- **Published**: 2022-07-08 09:01:04+00:00
- **Updated**: 2022-07-08 09:01:04+00:00
- **Authors**: Steven Robert Lorenzen, Henrik Riedel, Maximilian Michael Rupp, Leon Schmeiser, Hagen Berthold, Andrei Firus, Jens Schneider
- **Comment**: None
- **Journal**: None
- **Summary**: In the practical application of the Bridge Weigh-In-Motion (BWIM) methods, the position of the wheels or axles during the passage of a vehicle is in most cases a prerequisite. To avoid the use of conventional axle detectors and bridge type specific methods, we propose a novel method for axle detection through the placement of accelerometers at any point of a bridge. In order to develop a model that is as simple and comprehensible as possible, the axle detection task is implemented as a binary classification problem instead of a regression problem. The model is implemented as a Fully Convolutional Network to process signals in the form of Continuous Wavelet Transforms. This allows passages of any length to be processed in a single step with maximum efficiency while utilising multiple scales in a single evaluation. This enables our method to use acceleration signals at any location of the bridge structure serving as Virtual Axle Detectors (VADs) without being limited to specific structural types of bridges. To test the proposed method, we analysed 3787 train passages recorded on a steel trough railway bridge of a long-distance traffic line. Our results on the measurement data show that our model detects 95% of the axes, thus, 128,599 of 134,800 previously unseen axles were correctly detected. In total, 90% of the axles can be detected with a maximum spatial error of 20cm, with a maximum velocity of $v_{\mathrm{max}}=56,3~\mathrm{m/s}$. The analysis shows that our developed model can use accelerometers as VADs even under real operating conditions.



### Towards Intrinsic Common Discriminative Features Learning for Face Forgery Detection using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03776v1)
- **Published**: 2022-07-08 09:23:59+00:00
- **Updated**: 2022-07-08 09:23:59+00:00
- **Authors**: Wanyi Zhuang, Qi Chu, Haojie Yuan, Changtao Miao, Bin Liu, Nenghai Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing face forgery detection methods usually treat face forgery detection as a binary classification problem and adopt deep convolution neural networks to learn discriminative features. The ideal discriminative features should be only related to the real/fake labels of facial images. However, we observe that the features learned by vanilla classification networks are correlated to unnecessary properties, such as forgery methods and facial identities. Such phenomenon would limit forgery detection performance especially for the generalization ability. Motivated by this, we propose a novel method which utilizes adversarial learning to eliminate the negative effect of different forgery methods and facial identities, which helps classification network to learn intrinsic common discriminative features for face forgery detection. To leverage data lacking ground truth label of facial identities, we design a special identity discriminator based on similarity information derived from off-the-shelf face recognition model. With the help of adversarial learning, our face forgery detection model learns to extract common discriminative features through eliminating the effect of forgery methods and facial identities. Extensive experiments demonstrate the effectiveness of the proposed method under both intra-dataset and cross-dataset evaluation settings.



### VidConv: A modernized 2D ConvNet for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.03782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03782v1)
- **Published**: 2022-07-08 09:33:46+00:00
- **Updated**: 2022-07-08 09:33:46+00:00
- **Authors**: Chuong H. Nguyen, Su Huynh, Vinh Nguyen, Ngoc Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Since being introduced in 2020, Vision Transformers (ViT) has been steadily breaking the record for many vision tasks and are often described as ``all-you-need" to replace ConvNet. Despite that, ViTs are generally computational, memory-consuming, and unfriendly for embedded devices. In addition, recent research shows that standard ConvNet if redesigned and trained appropriately can compete favorably with ViT in terms of accuracy and scalability. In this paper, we adopt the modernized structure of ConvNet to design a new backbone for action recognition. Particularly, our main target is to serve for industrial product deployment, such as FPGA boards in which only standard operations are supported. Therefore, our network simply consists of 2D convolutions, without using any 3D convolution, long-range attention plugin, or Transformer blocks. While being trained with much fewer epochs (5x-10x), our backbone surpasses the methods using (2+1)D and 3D convolution, and achieve comparable results with ViT on two benchmark datasets.



### Continuous Target-free Extrinsic Calibration of a Multi-Sensor System from a Sequence of Static Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/2207.03785v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03785v1)
- **Published**: 2022-07-08 09:36:17+00:00
- **Updated**: 2022-07-08 09:36:17+00:00
- **Authors**: Philipp Glira, Christoph Weidinger, Johann Weichselbaum
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile robotic applications need precise information about the geometric position of the individual sensors on the platform. This information is given by the extrinsic calibration parameters which define how the sensor is rotated and translated with respect to a fixed reference coordinate system. Erroneous calibration parameters have a negative impact on typical robotic estimation tasks, e.g. SLAM. In this work we propose a new method for a continuous estimation of the calibration parameters during operation of the robot. The parameter estimation is based on the matching of point clouds which are acquired by the sensors from multiple static viewpoints. Consequently, our method does not need any special calibration targets and is applicable to any sensor whose measurements can be converted to point clouds. We demonstrate the suitability of our method by calibrating a multi-sensor system composed by 2 lidar sensors, 3 cameras, and an imaging radar sensor.



### Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.03790v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.03790v2)
- **Published**: 2022-07-08 09:42:40+00:00
- **Updated**: 2022-07-12 12:23:40+00:00
- **Authors**: Vincent Le Guen, Clément Rambour, Nicolas Thome
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art methods for optical flow estimation rely on deep learning, which require complex sequential training schemes to reach optimal performances on real-world data. In this work, we introduce the COMBO deep network that explicitly exploits the brightness constancy (BC) model used in traditional methods. Since BC is an approximate physical model violated in several situations, we propose to train a physically-constrained network complemented with a data-driven network. We introduce a unique and meaningful flow decomposition between the physical prior and the data-driven complement, including an uncertainty quantification of the BC model. We derive a joint training scheme for learning the different components of the decomposition ensuring an optimal cooperation, in a supervised but also in a semi-supervised context. Experiments show that COMBO can improve performances over state-of-the-art supervised networks, e.g. RAFT, reaching state-of-the-art results on several benchmarks. We highlight how COMBO can leverage the BC model and adapt to its limitations. Finally, we show that our semi-supervised method can significantly simplify the training procedure.



### FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.03800v2
- **DOI**: 10.1145/3503161.3548194
- **Categories**: **cs.SD**, cs.CL, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.03800v2)
- **Published**: 2022-07-08 10:10:39+00:00
- **Updated**: 2022-07-13 09:15:36+00:00
- **Authors**: Yongqi Wang, Zhou Zhao
- **Comment**: 10 pages, 5 figures, accepted by ACMMM 2022
- **Journal**: None
- **Summary**: Unconstrained lip-to-speech synthesis aims to generate corresponding speeches from silent videos of talking faces with no restriction on head poses or vocabulary. Current works mainly use sequence-to-sequence models to solve this problem, either in an autoregressive architecture or a flow-based non-autoregressive architecture. However, these models suffer from several drawbacks: 1) Instead of directly generating audios, they use a two-stage pipeline that first generates mel-spectrograms and then reconstructs audios from the spectrograms. This causes cumbersome deployment and degradation of speech quality due to error propagation; 2) The audio reconstruction algorithm used by these models limits the inference speed and audio quality, while neural vocoders are not available for these models since their output spectrograms are not accurate enough; 3) The autoregressive model suffers from high inference latency, while the flow-based model has high memory occupancy: neither of them is efficient enough in both time and memory usage. To tackle these problems, we propose FastLTS, a non-autoregressive end-to-end model which can directly synthesize high-quality speech audios from unconstrained talking videos with low latency, and has a relatively small model size. Besides, different from the widely used 3D-CNN visual frontend for lip movement encoding, we for the first time propose a transformer-based visual frontend for this task. Experiments show that our model achieves $19.76\times$ speedup for audio waveform generation compared with the current autoregressive model on input sequences of 3 seconds, and obtains superior audio quality.



### Beyond Transfer Learning: Co-finetuning for Action Localisation
- **Arxiv ID**: http://arxiv.org/abs/2207.03807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03807v1)
- **Published**: 2022-07-08 10:25:47+00:00
- **Updated**: 2022-07-08 10:25:47+00:00
- **Authors**: Anurag Arnab, Xuehan Xiong, Alexey Gritsenko, Rob Romijnders, Josip Djolonga, Mostafa Dehghani, Chen Sun, Mario Lučić, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning is the predominant paradigm for training deep networks on small target datasets. Models are typically pretrained on large ``upstream'' datasets for classification, as such labels are easy to collect, and then finetuned on ``downstream'' tasks such as action localisation, which are smaller due to their finer-grained annotations. In this paper, we question this approach, and propose co-finetuning -- simultaneously training a single model on multiple ``upstream'' and ``downstream'' tasks. We demonstrate that co-finetuning outperforms traditional transfer learning when using the same total amount of data, and also show how we can easily extend our approach to multiple ``upstream'' datasets to further improve performance. In particular, co-finetuning significantly improves the performance on rare classes in our downstream task, as it has a regularising effect, and enables the network to learn feature representations that transfer between different datasets. Finally, we observe how co-finetuning with public, video classification datasets, we are able to achieve state-of-the-art results for spatio-temporal action localisation on the challenging AVA and AVA-Kinetics datasets, outperforming recent works which develop intricate models.



### Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations
- **Arxiv ID**: http://arxiv.org/abs/2207.03824v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03824v3)
- **Published**: 2022-07-08 11:05:35+00:00
- **Updated**: 2023-07-18 06:57:23+00:00
- **Authors**: Yu Du, Miaojing Shi, Fangyun Wei, Guoqi Li
- **Comment**: Accepted to TNNLS
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize classes that do not have samples in the training set. One representative solution is to directly learn an embedding function associating visual features with corresponding class semantics for recognizing new classes. Many methods extend upon this solution, and recent ones are especially keen on extracting rich features from images, e.g. attribute features. These attribute features are normally extracted within each individual image; however, the common traits for features across images yet belonging to the same attribute are not emphasized. In this paper, we propose a new framework to boost ZSL by explicitly learning attribute prototypes beyond images and contrastively optimizing them with attribute-level features within images. Besides the novel architecture, two elements are highlighted for attribute representations: a new prototype generation module is designed to generate attribute prototypes from attribute semantics; a hard example-based contrastive optimization scheme is introduced to reinforce attribute-level features in the embedding space. We explore two alternative backbones, CNN-based and transformer-based, to build our framework and conduct experiments on three standard benchmarks, CUB, SUN, AwA2. Results on these benchmarks demonstrate that our method improves the state of the art by a considerable margin. Our codes will be available at https://github.com/dyabel/CoAR-ZSL.git



### Continuous Methods : Hamiltonian Domain Translation
- **Arxiv ID**: http://arxiv.org/abs/2207.03843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03843v1)
- **Published**: 2022-07-08 11:54:37+00:00
- **Updated**: 2022-07-08 11:54:37+00:00
- **Authors**: Emmanuel Menier, Michele Alessandro Bucci, Mouadh Yagoubi, Lionel Mathelin, Marc Schoenauer
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel approach to domain translation. Leveraging established parallels between generative models and dynamical systems, we propose a reformulation of the Cycle-GAN architecture. By embedding our model with a Hamiltonian structure, we obtain a continuous, expressive and most importantly invertible generative model for domain translation.



### Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain
- **Arxiv ID**: http://arxiv.org/abs/2207.03860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03860v2)
- **Published**: 2022-07-08 12:32:09+00:00
- **Updated**: 2022-09-14 01:38:19+00:00
- **Authors**: Tong Zhang, Peng Gao, Hao Dong, Yin Zhuang, Guanqun Wang, Wei Zhang, He Chen
- **Comment**: 20 pages,9 figures
- **Journal**: None
- **Summary**: Currently, under supervised learning, a model pretrained by a large-scale nature scene dataset and then fine-tuned on a few specific task labeling data is the paradigm that has dominated the knowledge transfer learning. It has reached the status of consensus solution for task-aware model training in remote sensing domain (RSD). Unfortunately, due to different categories of imaging data and stiff challenges of data annotation, there is not a large enough and uniform remote sensing dataset to support large-scale pretraining in RSD. Moreover, pretraining models on large-scale nature scene datasets by supervised learning and then directly fine-tuning on diverse downstream tasks seems to be a crude method, which is easily affected by inevitable labeling noise, severe domain gaps and task-aware discrepancies. Thus, in this paper, considering the self-supervised pretraining and powerful vision transformer (ViT) architecture, a concise and effective knowledge transfer learning strategy called ConSecutive PreTraining (CSPT) is proposed based on the idea of not stopping pretraining in natural language processing (NLP), which can gradually bridge the domain gap and transfer knowledge from the nature scene domain to the RSD. The proposed CSPT also can release the huge potential of unlabeled data for task-aware model training. Finally, extensive experiments are carried out on twelve datasets in RSD involving three types of downstream tasks (e.g., scene classification, object detection and land cover classification) and two types of imaging data (e.g., optical and SAR). The results show that by utilizing the proposed CSPT for task-aware model training, almost all downstream tasks in RSD can outperform the previous method of supervised pretraining-then-fine-tuning and even surpass the state-of-the-art (SOTA) performance without any expensive labeling consumption and careful model design.



### Pixel-level Correspondence for Self-Supervised Learning from Video
- **Arxiv ID**: http://arxiv.org/abs/2207.03866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03866v1)
- **Published**: 2022-07-08 12:50:13+00:00
- **Updated**: 2022-07-08 12:50:13+00:00
- **Authors**: Yash Sharma, Yi Zhu, Chris Russell, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: While self-supervised learning has enabled effective representation learning in the absence of labels, for vision, video remains a relatively untapped source of supervision. To address this, we propose Pixel-level Correspondence (PiCo), a method for dense contrastive learning from video. By tracking points with optical flow, we obtain a correspondence map which can be used to match local features at different points in time. We validate PiCo on standard benchmarks, outperforming self-supervised baselines on multiple dense prediction tasks, without compromising performance on image classification.



### Learning Sequential Descriptors for Sequence-based Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.03868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03868v1)
- **Published**: 2022-07-08 12:52:04+00:00
- **Updated**: 2022-07-08 12:52:04+00:00
- **Authors**: Riccardo Mereu, Gabriele Trivigno, Gabriele Berton, Carlo Masone, Barbara Caputo
- **Comment**: Accepted at IROS22
- **Journal**: None
- **Summary**: In robotics, Visual Place Recognition is a continuous process that receives as input a video stream to produce a hypothesis of the robot's current position within a map of known places. This task requires robust, scalable, and efficient techniques for real applications. This work proposes a detailed taxonomy of techniques using sequential descriptors, highlighting different mechanism to fuse the information from the individual images. This categorization is supported by a complete benchmark of experimental results that provides evidence on the strengths and weaknesses of these different architectural choices. In comparison to existing sequential descriptors methods, we further investigate the viability of Transformers instead of CNN backbones, and we propose a new ad-hoc sequence-level aggregator called SeqVLAD, which outperforms prior state of the art on different datasets. The code is available at https://github.com/vandal-vpr/vg-transformers.



### BlindSpotNet: Seeing Where We Cannot See
- **Arxiv ID**: http://arxiv.org/abs/2207.03870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03870v1)
- **Published**: 2022-07-08 12:54:18+00:00
- **Updated**: 2022-07-08 12:54:18+00:00
- **Authors**: Taichi Fukuda, Kotaro Hasegawa, Shinya Ishizaki, Shohei Nobuhara, Ko Nishino
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce 2D blind spot estimation as a critical visual task for road scene understanding. By automatically detecting road regions that are occluded from the vehicle's vantage point, we can proactively alert a manual driver or a self-driving system to potential causes of accidents (e.g., draw attention to a road region from which a child may spring out). Detecting blind spots in full 3D would be challenging, as 3D reasoning on the fly even if the car is equipped with LiDAR would be prohibitively expensive and error prone. We instead propose to learn to estimate blind spots in 2D, just from a monocular camera. We achieve this in two steps. We first introduce an automatic method for generating ``ground-truth'' blind spot training data for arbitrary driving videos by leveraging monocular depth estimation, semantic segmentation, and SLAM. The key idea is to reason in 3D but from 2D images by defining blind spots as those road regions that are currently invisible but become visible in the near future. We construct a large-scale dataset with this automatic offline blind spot estimation, which we refer to as Road Blind Spot (RBS) dataset. Next, we introduce BlindSpotNet (BSN), a simple network that fully leverages this dataset for fully automatic estimation of frame-wise blind spot probability maps for arbitrary driving videos. Extensive experimental results demonstrate the validity of our RBS Dataset and the effectiveness of our BSN.



### The Power of Transfer Learning in Agricultural Applications: AgriNet
- **Arxiv ID**: http://arxiv.org/abs/2207.03881v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03881v3)
- **Published**: 2022-07-08 13:15:16+00:00
- **Updated**: 2022-10-06 11:30:21+00:00
- **Authors**: Zahraa Al Sahili, Mariette Awad
- **Comment**: Accepted by Frontiers in Plant Science
- **Journal**: None
- **Summary**: Advances in deep learning and transfer learning have paved the way for various automation classification tasks in agriculture, including plant diseases, pests, weeds, and plant species detection. However, agriculture automation still faces various challenges, such as the limited size of datasets and the absence of plant-domain-specific pretrained models. Domain specific pretrained models have shown state of art performance in various computer vision tasks including face recognition and medical imaging diagnosis. In this paper, we propose AgriNet dataset, a collection of 160k agricultural images from more than 19 geographical locations, several images captioning devices, and more than 423 classes of plant species and diseases. We also introduce AgriNet models, a set of pretrained models on five ImageNet architectures: VGG16, VGG19, Inception-v3, InceptionResNet-v2, and Xception. AgriNet-VGG19 achieved the highest classification accuracy of 94 % and the highest F1-score of 92%. Additionally, all proposed models were found to accurately classify the 423 classes of plant species, diseases, pests, and weeds with a minimum accuracy of 87% for the Inception-v3 model.Finally, experiments to evaluate of superiority of AgriNet models compared to ImageNet models were conducted on two external datasets: pest and plant diseases dataset from Bangladesh and a plant diseases dataset from Kashmir.



### Generative Adversarial Networks and Other Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2207.03887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03887v1)
- **Published**: 2022-07-08 13:23:56+00:00
- **Updated**: 2022-07-08 13:23:56+00:00
- **Authors**: Markus Wenzel
- **Comment**: None
- **Journal**: None
- **Summary**: Generative networks are fundamentally different in their aim and methods compared to CNNs for classification, segmentation, or object detection. They have initially not been meant to be an image analysis tool, but to produce naturally looking images. The adversarial training paradigm has been proposed to stabilize generative methods, and has proven to be highly successful -- though by no means from the first attempt.   This chapter gives a basic introduction into the motivation for Generative Adversarial Networks (GANs) and traces the path of their success by abstracting the basic task and working mechanism, and deriving the difficulty of early practical approaches. Methods for a more stable training will be shown, and also typical signs for poor convergence and their reasons.   Though this chapter focuses on GANs that are meant for image generation and image analysis, the adversarial training paradigm itself is not specific to images, and also generalizes to tasks in image analysis. Examples of architectures for image semantic segmentation and abnormality detection will be acclaimed, before contrasting GANs with further generative modeling approaches lately entering the scene. This will allow a contextualized view on the limits but also benefits of GANs.



### Defense Against Multi-target Trojan Attacks
- **Arxiv ID**: http://arxiv.org/abs/2207.03895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03895v1)
- **Published**: 2022-07-08 13:29:13+00:00
- **Updated**: 2022-07-08 13:29:13+00:00
- **Authors**: Haripriya Harikumar, Santu Rana, Kien Do, Sunil Gupta, Wei Zong, Willy Susilo, Svetha Venkastesh
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks on deep learning-based models pose a significant threat to the current AI infrastructure. Among them, Trojan attacks are the hardest to defend against. In this paper, we first introduce a variation of the Badnet kind of attacks that introduces Trojan backdoors to multiple target classes and allows triggers to be placed anywhere in the image. The former makes it more potent and the latter makes it extremely easy to carry out the attack in the physical space. The state-of-the-art Trojan detection methods fail with this threat model. To defend against this attack, we first introduce a trigger reverse-engineering mechanism that uses multiple images to recover a variety of potential triggers. We then propose a detection mechanism by measuring the transferability of such recovered triggers. A Trojan trigger will have very high transferability i.e. they make other images also go to the same class. We study many practical advantages of our attack method and then demonstrate the detection performance using a variety of image datasets. The experimental results show the superior detection performance of our method over the state-of-the-arts.



### Big Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03899v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03899v4)
- **Published**: 2022-07-08 13:35:21+00:00
- **Updated**: 2023-05-21 04:05:46+00:00
- **Authors**: Yulai Cong, Miaoyun Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in big/foundation models reveal a promising path for deep learning, where the roadmap steadily moves from big data to big models to (the newly-introduced) big learning. Specifically, the big learning exhaustively exploits the information inherent in its large-scale complete/incomplete training data, by simultaneously modeling many/all joint/conditional/marginal data distributions across potentially diverse domains, with one universal foundation model. We reveal that big learning ($i$) underlies most existing foundation models, ($ii$) is equipped with extraordinary flexibilities for complete/incomplete training data and trustworthy data tasks, ($iii$) is capable of delivering all joint/conditional/marginal data capabilities with one universal model, and ($iv$) unifies conventional machine learning paradigms and enables their flexible cooperations, manifested as a universal learning paradigm. Diverse experiments are carried out to validate the effectiveness of the presented big learning.



### Reproducing sensory induced hallucinations via neural fields
- **Arxiv ID**: http://arxiv.org/abs/2207.03901v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, math.AP
- **Links**: [PDF](http://arxiv.org/pdf/2207.03901v1)
- **Published**: 2022-07-08 13:41:02+00:00
- **Updated**: 2022-07-08 13:41:02+00:00
- **Authors**: Cyprien Tamekue, Dario Prandi, Yacine Chitour
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding sensory-induced cortical patterns in the primary visual cortex V1 is an important challenge both for physiological motivations and for improving our understanding of human perception and visual organisation. In this work, we focus on pattern formation in the visual cortex when the cortical activity is driven by a geometric visual hallucination-like stimulus. In particular, we present a theoretical framework for sensory-induced hallucinations which allows one to reproduce novel psychophysical results such as the MacKay effect (Nature, 1957) and the Billock and Tsou experiences (PNAS, 2007).



### A Mask Attention Interaction and Scale Enhancement Network for SAR Ship Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.03912v1
- **DOI**: 10.1109/LGRS.2022.3189961
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03912v1)
- **Published**: 2022-07-08 14:04:04+00:00
- **Updated**: 2022-07-08 14:04:04+00:00
- **Authors**: Tianwen Zhang, Xiaoling Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most of existing synthetic aperture radar (SAR) ship in-stance segmentation models do not achieve mask interac-tion or offer limited interaction performance. Besides, their multi-scale ship instance segmentation performance is moderate especially for small ships. To solve these problems, we propose a mask attention interaction and scale enhancement network (MAI-SE-Net) for SAR ship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP) to gain multi-resolution feature re-sponses, a non-local block (NLB) to model long-range spa-tial dependencies, and a concatenation shuffle attention block (CSAB) to improve interaction benefits. SE uses a content-aware reassembly of features block (CARAFEB) to generate an extra pyramid bottom-level to boost small ship performance, a feature balance operation (FBO) to improve scale feature description, and a global context block (GCB) to refine features. Experimental results on two public SSDD and HRSID datasets reveal that MAI-SE-Net outperforms the other nine competitive models, better than the suboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by 3.0% detection AP and 2.4% segmentation AP on HRSID.



### RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.03917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03917v1)
- **Published**: 2022-07-08 14:12:26+00:00
- **Updated**: 2022-07-08 14:12:26+00:00
- **Authors**: Jinpeng Li, Haibo Jin, Shengcai Liao, Ling Shao, Pheng-Ann Heng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a Refinement Pyramid Transformer (RePFormer) for robust facial landmark detection. Most facial landmark detectors focus on learning representative image features. However, these CNN-based feature representations are not robust enough to handle complex real-world scenarios due to ignoring the internal structure of landmarks, as well as the relations between landmarks and context. In this work, we formulate the facial landmark detection task as refining landmark queries along pyramid memories. Specifically, a pyramid transformer head (PTH) is introduced to build both homologous relations among landmarks and heterologous relations between landmarks and cross-scale contexts. Besides, a dynamic landmark refinement (DLR) module is designed to decompose the landmark regression into an end-to-end refinement procedure, where the dynamically aggregated queries are transformed to residual coordinates predictions. Extensive experimental results on four facial landmark detection benchmarks and their various subsets demonstrate the superior performance and high robustness of our framework.



### Detection of Furigana Text in Images
- **Arxiv ID**: http://arxiv.org/abs/2207.03960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03960v1)
- **Published**: 2022-07-08 15:27:19+00:00
- **Updated**: 2022-07-08 15:27:19+00:00
- **Authors**: Nikolaj Kjøller Bjerregaard, Veronika Cheplygina, Stefan Heinrich
- **Comment**: This project was originally submitted by NKB in fulfillment of the 30
  ECTS MSc thesis at the IT University of Copenhagen
- **Journal**: None
- **Summary**: Furigana are pronunciation notes used in Japanese writing. Being able to detect these can help improve optical character recognition (OCR) performance or make more accurate digital copies of Japanese written media by correctly displaying furigana. This project focuses on detecting furigana in Japanese books and comics. While there has been research into the detection of Japanese text in general, there are currently no proposed methods for detecting furigana.   We construct a new dataset containing Japanese written media and annotations of furigana. We propose an evaluation metric for such data which is similar to the evaluation protocols used in object detection except that it allows groups of objects to be labeled by one annotation. We propose a method for detection of furigana that is based on mathematical morphology and connected component analysis. We evaluate the detections of the dataset and compare different methods for text extraction. We also evaluate different types of images such as books and comics individually and discuss the challenges of each type of image.   The proposed method reaches an F1-score of 76\% on the dataset. The method performs well on regular books, but less so on comics, and books of irregular format. Finally, we show that the proposed method can improve the performance of OCR by 5\% on the manga109 dataset.   Source code is available via \texttt{\url{https://github.com/nikolajkb/FuriganaDetection}}



### CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination
- **Arxiv ID**: http://arxiv.org/abs/2207.03961v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03961v1)
- **Published**: 2022-07-08 15:28:23+00:00
- **Updated**: 2022-07-08 15:28:23+00:00
- **Authors**: Hyounghun Kim, Abhay Zala, Mohit Bansal
- **Comment**: NAACL 2022 (13 pages)
- **Journal**: None
- **Summary**: As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new task/dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. In this task/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5K high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition/removal/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language Transformer (i.e., LXMERT) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging counterfactual, scene imagination task. Our code and dataset are publicly available at: https://github.com/hyounghk/CoSIm



### Event Collapse in Contrast Maximization Frameworks
- **Arxiv ID**: http://arxiv.org/abs/2207.04007v2
- **DOI**: 10.3390/s22145190
- **Categories**: **cs.CV**, cs.RO, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04007v2)
- **Published**: 2022-07-08 16:52:35+00:00
- **Updated**: 2022-07-11 12:56:57+00:00
- **Authors**: Shintaro Shiba, Yoshimitsu Aoki, Guillermo Gallego
- **Comment**: 19 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Contrast maximization (CMax) is a framework that provides state-of-the-art results on several event-based computer vision tasks, such as ego-motion or optical flow estimation. However, it may suffer from a problem called event collapse, which is an undesired solution where events are warped into too few pixels. As prior works have largely ignored the issue or proposed workarounds, it is imperative to analyze this phenomenon in detail. Our work demonstrates event collapse in its simplest form and proposes collapse metrics by using first principles of space-time deformation based on differential geometry and physics. We experimentally show on publicly available datasets that the proposed metrics mitigate event collapse and do not harm well-posed warps. To the best of our knowledge, regularizers based on the proposed metrics are the only effective solution against event collapse in the experimental settings considered, compared with other methods. We hope that this work inspires further research to tackle more complex warp models.



### CoCAtt: A Cognitive-Conditioned Driver Attention Dataset (Supplementary Material)
- **Arxiv ID**: http://arxiv.org/abs/2207.04028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04028v1)
- **Published**: 2022-07-08 17:35:17+00:00
- **Updated**: 2022-07-08 17:35:17+00:00
- **Authors**: Yuan Shen, Niviru Wijayaratne, Pranav Sriram, Aamir Hasan, Peter Du, Katherine Driggs-Campbell
- **Comment**: Supplementary Material for the main paper, "CoCAtt: A
  Cognitive-Conditioned Driver Attention Dataset". Accepted at ITSC2022
- **Journal**: None
- **Summary**: The task of driver attention prediction has drawn considerable interest among researchers in robotics and the autonomous vehicle industry. Driver attention prediction can play an instrumental role in mitigating and preventing high-risk events, like collisions and casualties. However, existing driver attention prediction models neglect the distraction state and intention of the driver, which can significantly influence how they observe their surroundings. To address these issues, we present a new driver attention dataset, CoCAtt (Cognitive-Conditioned Attention). Unlike previous driver attention datasets, CoCAtt includes per-frame annotations that describe the distraction state and intention of the driver. In addition, the attention data in our dataset is captured in both manual and autopilot modes using eye-tracking devices of different resolutions. Our results demonstrate that incorporating the above two driver states into attention modeling can improve the performance of driver attention prediction. To the best of our knowledge, this work is the first to provide autopilot attention data. Furthermore, CoCAtt is currently the largest and the most diverse driver attention dataset in terms of autonomy levels, eye tracker resolutions, and driving scenarios. CoCAtt is available for download at https://cocatt-dataset.github.io.



### kMaX-DeepLab: k-means Mask Transformer
- **Arxiv ID**: http://arxiv.org/abs/2207.04044v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04044v5)
- **Published**: 2022-07-08 17:59:01+00:00
- **Updated**: 2023-07-10 20:59:46+00:00
- **Authors**: Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen
- **Comment**: ECCV 2022. arXiv v2: add results on ADE20K. arXiv v3: fix appendix.
  v4: fix typo. v5: add PyTorch re-implementation. Codes and models are
  available at TensorFlow: https://github.com/google-research/deeplab2 PyTorch:
  https://github.com/bytedance/kmax-deeplab
- **Journal**: None
- **Summary**: The rise of transformers in vision tasks not only advances network backbone designs, but also starts a brand-new page to achieve end-to-end image recognition (e.g., object detection and panoptic segmentation). Originated from Natural Language Processing (NLP), transformer architectures, consisting of self-attention and cross-attention, effectively learn long-range interactions between elements in a sequence. However, we observe that most existing transformer-based vision models simply borrow the idea from NLP, neglecting the crucial difference between languages and images, particularly the extremely large sequence length of spatially flattened pixel features. This subsequently impedes the learning in cross-attention between pixel features and object queries. In this paper, we rethink the relationship between pixels and object queries and propose to reformulate the cross-attention learning as a clustering process. Inspired by the traditional k-means clustering algorithm, we develop a k-means Mask Xformer (kMaX-DeepLab) for segmentation tasks, which not only improves the state-of-the-art, but also enjoys a simple and elegant design. As a result, our kMaX-DeepLab achieves a new state-of-the-art performance on COCO val set with 58.0% PQ, Cityscapes val set with 68.4% PQ, 44.0% AP, and 83.5% mIoU, and ADE20K val set with 50.9% PQ and 55.2% mIoU without test-time augmentation or external dataset. We hope our work can shed some light on designing transformers tailored for vision tasks. TensorFlow code and models are available at https://github.com/google-research/deeplab2 A PyTorch re-implementation is also available at https://github.com/bytedance/kmax-deeplab



### SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance
- **Arxiv ID**: http://arxiv.org/abs/2207.04089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04089v1)
- **Published**: 2022-07-08 18:27:42+00:00
- **Updated**: 2022-07-08 18:27:42+00:00
- **Authors**: Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly
- **Comment**: None
- **Journal**: None
- **Summary**: The leap in performance in state-of-the-art computer vision methods is attributed to the development of deep neural networks. However it often comes at a computational price which may hinder their deployment. To alleviate this limitation, structured pruning is a well known technique which consists in removing channels, neurons or filters, and is commonly applied in order to produce more compact models. In most cases, the computations to remove are selected based on a relative importance criterion. At the same time, the need for explainable predictive models has risen tremendously and motivated the development of robust attribution methods that highlight the relative importance of pixels of an input image or feature map. In this work, we discuss the limitations of existing pruning heuristics, among which magnitude and gradient-based methods. We draw inspiration from attribution methods to design a novel integrated gradient pruning criterion, in which the relevance of each neuron is defined as the integral of the gradient variation on a path towards this neuron removal. Furthermore, we propose an entwined DNN pruning and fine-tuning flowchart to better preserve DNN accuracy while removing parameters. We show through extensive validation on several datasets, architectures as well as pruning scenarios that the proposed method, dubbed SInGE, significantly outperforms existing state-of-the-art DNN pruning methods.



### FAIVConf: Face enhancement for AI-based Video Conference with Low Bit-rate
- **Arxiv ID**: http://arxiv.org/abs/2207.04090v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04090v1)
- **Published**: 2022-07-08 18:29:06+00:00
- **Updated**: 2022-07-08 18:29:06+00:00
- **Authors**: Zhengang Li, Sheng Lin, Shan Liu, Songnan Li, Xue Lin, Wei Wang, Wei Jiang
- **Comment**: ICME 2022
- **Journal**: None
- **Summary**: Recently, high-quality video conferencing with fewer transmission bits has become a very hot and challenging problem. We propose FAIVConf, a specially designed video compression framework for video conferencing, based on the effective neural human face generation techniques. FAIVConf brings together several designs to improve the system robustness in real video conference scenarios: face-swapping to avoid artifacts in background animation; facial blurring to decrease transmission bit-rate and maintain the quality of extracted facial landmarks; and dynamic source update for face view interpolation to accommodate a large range of head poses. Our method achieves a significant bit-rate reduction in the video conference and gives much better visual quality under the same bit-rate compared with H.264 and H.265 coding schemes.



### StatMix: Data augmentation method that relies on image statistics in federated learning
- **Arxiv ID**: http://arxiv.org/abs/2207.04103v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04103v1)
- **Published**: 2022-07-08 19:02:41+00:00
- **Updated**: 2022-07-08 19:02:41+00:00
- **Authors**: Dominik Lewy, Jacek Mańdziuk, Maria Ganzha, Marcin Paprzycki
- **Comment**: None
- **Journal**: None
- **Summary**: Availability of large amount of annotated data is one of the pillars of deep learning success. Although numerous big datasets have been made available for research, this is often not the case in real life applications (e.g. companies are not able to share data due to GDPR or concerns related to intellectual property rights protection). Federated learning (FL) is a potential solution to this problem, as it enables training a global model on data scattered across multiple nodes, without sharing local data itself. However, even FL methods pose a threat to data privacy, if not handled properly. Therefore, we propose StatMix, an augmentation approach that uses image statistics, to improve results of FL scenario(s). StatMix is empirically tested on CIFAR-10 and CIFAR-100, using two neural network architectures. In all FL experiments, application of StatMix improves the average accuracy, compared to the baseline training (with no use of StatMix). Some improvement can also be observed in non-FL setups.



### Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models
- **Arxiv ID**: http://arxiv.org/abs/2207.04104v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04104v3)
- **Published**: 2022-07-08 19:02:50+00:00
- **Updated**: 2023-07-12 01:36:19+00:00
- **Authors**: Gregory Plumb, Nari Johnson, Ángel Alexander Cabrera, Ameet Talwalkar
- **Comment**: reviewed on OpenReview: https://openreview.net/forum?id=MaDvbLaBiF
- **Journal**: TMLR 2023
- **Summary**: A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM design and evaluation. Overall, we hope that the methodology and analyses presented in this work will help facilitate a more rigorous science of blindspot discovery.



### Neuroimaging Feature Extraction using a Neural Network Classifier for Imaging Genetics
- **Arxiv ID**: http://arxiv.org/abs/2207.10794v1
- **DOI**: 10.1186/s12859-023-05394-x
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10794v1)
- **Published**: 2022-07-08 19:03:00+00:00
- **Updated**: 2022-07-08 19:03:00+00:00
- **Authors**: Cédric Beaulac, Sidi Wu, Erin Gibson, Michelle F. Miranda, Jiguo Cao, Leno Rocha, Mirza Faisal Beg, Farouk S. Nathoo
- **Comment**: Under review
- **Journal**: BMC Bioinformatics 24, 271 (2023)
- **Summary**: A major issue in the association of genes to neuroimaging phenotypes is the high dimension of both genetic data and neuroimaging data. In this article, we tackle the latter problem with an eye toward developing solutions that are relevant for disease prediction. Supported by a vast literature on the predictive power of neural networks, our proposed solution uses neural networks to extract from neuroimaging data features that are relevant for predicting Alzheimer's Disease (AD) for subsequent relation to genetics. Our neuroimaging-genetic pipeline is comprised of image processing, neuroimaging feature extraction and genetic association steps. We propose a neural network classifier for extracting neuroimaging features that are related with disease and a multivariate Bayesian group sparse regression model for genetic association. We compare the predictive power of these features to expert selected features and take a closer look at the SNPs identified with the new neuroimaging features.



### Out of Distribution Detection via Neural Network Anchoring
- **Arxiv ID**: http://arxiv.org/abs/2207.04125v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04125v2)
- **Published**: 2022-07-08 21:01:09+00:00
- **Updated**: 2022-12-01 19:20:22+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan
- **Comment**: ACML 2022
- **Journal**: None
- **Summary**: Our goal in this paper is to exploit heteroscedastic temperature scaling as a calibration strategy for out of distribution (OOD) detection. Heteroscedasticity here refers to the fact that the optimal temperature parameter for each sample can be different, as opposed to conventional approaches that use the same value for the entire distribution. To enable this, we propose a new training strategy called anchoring that can estimate appropriate temperature values for each sample, leading to state-of-the-art OOD detection performance across several benchmarks. Using NTK theory, we show that this temperature function estimate is closely linked to the epistemic uncertainty of the classifier, which explains its behavior. In contrast to some of the best-performing OOD detection approaches, our method does not require exposure to additional outlier datasets, custom calibration objectives, or model ensembling. Through empirical studies with different OOD detection settings -- far OOD, near OOD, and semantically coherent OOD - we establish a highly effective OOD detection approach. Code to reproduce our results is available at github.com/LLNL/AMP



### Multi-view Attention for gestational age at birth prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.04130v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04130v1)
- **Published**: 2022-07-08 21:29:35+00:00
- **Updated**: 2022-07-08 21:29:35+00:00
- **Authors**: Mathieu Leclercq, Martin Styner, Juan Carlos Prieto
- **Comment**: 7 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: We present our method for gestational age at birth prediction for the SLCN (surface learning for clinical neuroimaging) challenge. Our method is based on a multi-view shape analysis technique that captures 2D renderings of a 3D object from different viewpoints. We render the brain features on the surface of the sphere and then the 2D images are analyzed via 2D CNNs and an attention layer for the regression task. The regression task achieves a MAE of 1.637 +- 1.3 on the Native space and MAE of 1.38 +- 1.14 on the template space. The source code for this project is available in our github repository https://github.com/MathieuLeclercq/SLCN_challenge_UNC



### Cross-Attention Transformer for Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2207.04132v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04132v2)
- **Published**: 2022-07-08 21:38:54+00:00
- **Updated**: 2022-12-02 02:48:37+00:00
- **Authors**: Hannah Halin Kim, Shuzhi Yu, Shuai Yuan, Carlo Tomasi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose TAIN (Transformers and Attention for video INterpolation), a residual neural network for video interpolation, which aims to interpolate an intermediate frame given two consecutive image frames around it. We first present a novel vision transformer module, named Cross Similarity (CS), to globally aggregate input image features with similar appearance as those of the predicted interpolated frame. These CS features are then used to refine the interpolated prediction. To account for occlusions in the CS features, we propose an Image Attention (IA) module to allow the network to focus on CS features from one frame over those of the other. TAIN outperforms existing methods that do not require flow estimation and performs comparably to flow-based methods while being computationally efficient in terms of inference time on Vimeo90k, UCF101, and SNU-FILM benchmarks.



### L$_0$onie: Compressing COINs with L$_0$-constraints
- **Arxiv ID**: http://arxiv.org/abs/2207.04144v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2207.04144v1)
- **Published**: 2022-07-08 22:24:56+00:00
- **Updated**: 2022-07-08 22:24:56+00:00
- **Authors**: Juan Ramirez, Jose Gallego-Posada
- **Comment**: Presented at the Sparsity in Neural Networks (SNN) Workshop 2022.
  Code available at https://github.com/juan43ramirez/l0onie
- **Journal**: None
- **Summary**: Advances in Implicit Neural Representations (INR) have motivated research on domain-agnostic compression techniques. These methods train a neural network to approximate an object, and then store the weights of the trained model. For example, given an image, a network is trained to learn the mapping from pixel locations to RGB values. In this paper, we propose L$_0$onie, a sparsity-constrained extension of the COIN compression method. Sparsity allows to leverage the faster learning of overparameterized networks, while retaining the desirable compression rate of smaller models. Moreover, our constrained formulation ensures that the final model respects a pre-determined compression rate, dispensing of the need for expensive architecture search.




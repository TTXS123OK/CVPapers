# Arxiv Papers in cs.CV on 2022-09-07
### Interpretations Steered Network Pruning via Amortized Inferred Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/2209.02869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02869v1)
- **Published**: 2022-09-07 01:12:11+00:00
- **Updated**: 2022-09-07 01:12:11+00:00
- **Authors**: Alireza Ganjdanesh, Shangqian Gao, Heng Huang
- **Comment**: Accepted to the European Conference on Computer Vision (ECCV 2022)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) compression is crucial to deploying these models in edge devices with limited resources. Existing channel pruning algorithms for CNNs have achieved plenty of success on complex models. They approach the pruning problem from various perspectives and use different metrics to guide the pruning process. However, these metrics mainly focus on the model's `outputs' or `weights' and neglect its `interpretations' information. To fill in this gap, we propose to address the channel pruning problem from a novel perspective by leveraging the interpretations of a model to steer the pruning process, thereby utilizing information from both inputs and outputs of the model. However, existing interpretation methods cannot get deployed to achieve our goal as either they are inefficient for pruning or may predict non-coherent explanations. We tackle this challenge by introducing a selector model that predicts real-time smooth saliency masks for pruned models. We parameterize the distribution of explanatory masks by Radial Basis Function (RBF)-like functions to incorporate geometric prior of natural images in our selector model's inductive bias. Thus, we can obtain compact representations of explanations to reduce the computational costs of our pruning method. We leverage our selector model to steer the network pruning by maximizing the similarity of explanatory representations for the pruned and original models. Extensive experiments on CIFAR-10 and ImageNet benchmark datasets demonstrate the efficacy of our proposed method. Our implementations are available at \url{https://github.com/Alii-Ganjj/InterpretationsSteeredPruning}



### SUNet: Scale-aware Unified Network for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.02877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02877v1)
- **Published**: 2022-09-07 01:40:41+00:00
- **Updated**: 2022-09-07 01:40:41+00:00
- **Authors**: Weihao Yan, Yeqiang Qian, Chunxiang Wang, Ming Yang
- **Comment**: 10 pages, 7 figures, 8 tables
- **Journal**: None
- **Summary**: Panoptic segmentation combines the advantages of semantic and instance segmentation, which can provide both pixel-level and instance-level environmental perception information for intelligent vehicles. However, it is challenged with segmenting objects of various scales, especially on extremely large and small ones. In this work, we propose two lightweight modules to mitigate this problem. First, Pixel-relation Block is designed to model global context information for large-scale things, which is based on a query-independent formulation and brings small parameter increments. Then, Convectional Network is constructed to collect extra high-resolution information for small-scale stuff, supplying more appropriate semantic features for the downstream segmentation branches. Based on these two modules, we present an end-to-end Scale-aware Unified Network (SUNet), which is more adaptable to multi-scale objects. Extensive experiments on Cityscapes and COCO demonstrate the effectiveness of the proposed methods.



### Multi-Grained Angle Representation for Remote Sensing Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.02884v2
- **DOI**: 10.1109/TGRS.2022.3212592
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02884v2)
- **Published**: 2022-09-07 02:08:50+00:00
- **Updated**: 2022-10-04 15:28:26+00:00
- **Authors**: Hao Wang, Zhanchao Huang, Zhengchao Chen, Ying Song, Wei Li
- **Comment**: 13 pages, 9 figures, 14 tables
- **Journal**: Transactions on Geoscience and Remote Sensing, 2022
- **Summary**: Arbitrary-oriented object detection (AOOD) plays a significant role for image understanding in remote sensing scenarios. The existing AOOD methods face the challenges of ambiguity and high costs in angle representation. To this end, a multi-grained angle representation (MGAR) method, consisting of coarse-grained angle classification (CAC) and fine-grained angle regression (FAR), is proposed. Specifically, the designed CAC avoids the ambiguity of angle prediction by discrete angular encoding (DAE) and reduces complexity by coarsening the granularity of DAE. Based on CAC, FAR is developed to refine the angle prediction with much lower costs than narrowing the granularity of DAE. Furthermore, an Intersection over Union (IoU) aware FAR-Loss (IFL) is designed to improve accuracy of angle prediction using an adaptive re-weighting mechanism guided by IoU. Extensive experiments are performed on several public remote sensing datasets, which demonstrate the effectiveness of the proposed MGAR. Moreover, experiments on embedded devices demonstrate that the proposed MGAR is also friendly for lightweight deployments.



### Data-Driven Target Localization Using Adaptive Radar Processing and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.02890v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2209.02890v4)
- **Published**: 2022-09-07 02:23:40+00:00
- **Updated**: 2023-03-08 23:36:50+00:00
- **Authors**: Shyam Venkatasubramanian, Sandeep Gogineni, Bosung Kang, Ali Pezeshki, Muralidhar Rangaswamy, Vahid Tarokh
- **Comment**: 34 pages, 22 figures. Submitted to IEEE Transactions on Aerospace and
  Electronic Systems
- **Journal**: None
- **Summary**: Facilitated by the recent emergence of radio frequency (RF) modeling and simulation tools purposed for adaptive radar processing applications, data-driven approaches to classical problems in radar have rapidly grown in popularity over the past decade. Despite this surge, limited focus has been directed toward the theoretical foundations of these data-driven approaches. In this regard, using adaptive radar processing techniques, we propose a data-driven approach in this work to address the classical problem of radar target localization post adaptive radar detection. To give context to the performance of this data-driven approach, we first analyze the asymptotic breakdown signal-to-clutter-plus-noise ratio (SCNR) threshold of the normalized adaptive matched filter (NAMF) test statistic within the context of radar target localization, and augment this analysis through our proposed deep learning framework for target location estimation. In this procedure, we generate comprehensive datasets by randomly placing targets of variable strengths in predetermined constrained areas using RFView, a site-specific, digital twin, RF modeling and simulation tool. For each radar return from these predefined constrained areas, we generate heatmap tensors in range, azimuth, and elevation of the NAMF test statistic, and of the output power of a generalized sidelobe canceller (GSC). Using our deep learning framework, we estimate target locations from these heatmap tensors to demonstrate the feasibility of and significant improvements provided by our data-driven approach across matched and mismatched settings.



### Prior Knowledge-Guided Attention in Self-Supervised Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.03745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03745v1)
- **Published**: 2022-09-07 02:30:36+00:00
- **Updated**: 2022-09-07 02:30:36+00:00
- **Authors**: Kevin Miao, Akash Gokul, Raghav Singh, Suzanne Petryk, Joseph Gonzalez, Kurt Keutzer, Trevor Darrell, Colorado Reed
- **Comment**: None
- **Journal**: None
- **Summary**: Recent trends in self-supervised representation learning have focused on removing inductive biases from training pipelines. However, inductive biases can be useful in settings when limited data are available or provide additional insight into the underlying data distribution. We present spatial prior attention (SPAN), a framework that takes advantage of consistent spatial and semantic structure in unlabeled image datasets to guide Vision Transformer attention. SPAN operates by regularizing attention masks from separate transformer heads to follow various priors over semantic regions. These priors can be derived from data statistics or a single labeled sample provided by a domain expert. We study SPAN through several detailed real-world scenarios, including medical image analysis and visual quality assurance. We find that the resulting attention masks are more interpretable than those derived from domain-agnostic pretraining. SPAN produces a 58.7 mAP improvement for lung and heart segmentation. We also find that our method yields a 2.2 mAUC improvement compared to domain-agnostic pretraining when transferring the pretrained model to a downstream chest disease classification task. Lastly, we show that SPAN pretraining leads to higher downstream classification performance in low-data regimes compared to domain-agnostic pretraining.



### Context Recovery and Knowledge Retrieval: A Novel Two-Stream Framework for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.02899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02899v1)
- **Published**: 2022-09-07 03:12:02+00:00
- **Updated**: 2022-09-07 03:12:02+00:00
- **Authors**: Congqi Cao, Yue Lu, Yanning Zhang
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Video anomaly detection aims to find the events in a video that do not conform to the expected behavior. The prevalent methods mainly detect anomalies by snippet reconstruction or future frame prediction error. However, the error is highly dependent on the local context of the current snippet and lacks the understanding of normality. To address this issue, we propose to detect anomalous events not only by the local context, but also according to the consistency between the testing event and the knowledge about normality from the training data. Concretely, we propose a novel two-stream framework based on context recovery and knowledge retrieval, where the two streams can complement each other. For the context recovery stream, we propose a spatiotemporal U-Net which can fully utilize the motion information to predict the future frame. Furthermore, we propose a maximum local error mechanism to alleviate the problem of large recovery errors caused by complex foreground objects. For the knowledge retrieval stream, we propose an improved learnable locality-sensitive hashing, which optimizes hash functions via a Siamese network and a mutual difference loss. The knowledge about normality is encoded and stored in hash tables, and the distance between the testing event and the knowledge representation is used to reveal the probability of anomaly. Finally, we fuse the anomaly scores from the two streams to detect anomalies. Extensive experiments demonstrate the effectiveness and complementarity of the two streams, whereby the proposed two-stream framework achieves state-of-the-art performance on four datasets.



### Magnitude-image based data-consistent deep learning method for MRI super resolution
- **Arxiv ID**: http://arxiv.org/abs/2209.02901v1
- **DOI**: 10.1109/CBMS55023.2022.00060
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02901v1)
- **Published**: 2022-09-07 03:16:35+00:00
- **Updated**: 2022-09-07 03:16:35+00:00
- **Authors**: Ziyan Lin, Zihao Chen
- **Comment**: Accepted by IEEE CBMS 2022
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is important in clinic to produce high resolution images for diagnosis, but its acquisition time is long for high resolution images. Deep learning based MRI super resolution methods can reduce scan time without complicated sequence programming, but may create additional artifacts due to the discrepancy between training data and testing data. Data consistency layer can improve the deep learning results but needs raw k-space data. In this work, we propose a magnitude-image based data consistency deep learning MRI super resolution method to improve super resolution images' quality without raw k-space data. Our experiments show that the proposed method can improve NRMSE and SSIM of super resolution images compared to the same Convolutional Neural Network (CNN) block without data consistency module.



### A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment
- **Arxiv ID**: http://arxiv.org/abs/2209.02905v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02905v2)
- **Published**: 2022-09-07 03:29:26+00:00
- **Updated**: 2023-07-09 15:24:20+00:00
- **Authors**: Hu Ding, Wenjie Liu, Mingquan Ye
- **Comment**: This work has been accepted by ACM Journal of Experimental
  Algorithmics in 2023. arXiv admin note: substantial text overlap with
  arXiv:1811.07455
- **Journal**: None
- **Summary**: Many real-world problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns in the field of computer vision. Recently, the alignment problem in high dimensions finds several novel applications in practice. However, the research is still rather limited in the algorithmic aspect. To the best of our knowledge, most existing approaches are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high computational complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns. Any existing alignment method can be applied to the compressed geometric patterns and the time complexity can be significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. Our framework is a ``data-dependent'' approach that has the complexity depending on the intrinsic dimension of the input data. Our experimental results reveal that running the alignment algorithm on compressed patterns can achieve similar qualities, comparing with the results on the original patterns, but the runtimes (including the times cost for compression) are substantially lower.



### Deep Learning for Medical Imaging From Diagnosis Prediction to its Counterfactual Explanation
- **Arxiv ID**: http://arxiv.org/abs/2209.02929v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02929v1)
- **Published**: 2022-09-07 04:44:56+00:00
- **Updated**: 2022-09-07 04:44:56+00:00
- **Authors**: Sumedha Singla
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: Deep neural networks (DNN) have achieved unprecedented performance in computer-vision tasks almost ubiquitously in business, technology, and science. While substantial efforts are made to engineer highly accurate architectures and provide usable model explanations, most state-of-the-art approaches are first designed for natural vision and then translated to the medical domain. This dissertation seeks to address this gap by proposing novel architectures that integrate the domain-specific constraints of medical imaging into the DNN model and explanation design.



### Facial De-morphing: Extracting Component Faces from a Single Morph
- **Arxiv ID**: http://arxiv.org/abs/2209.02933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02933v1)
- **Published**: 2022-09-07 05:01:02+00:00
- **Updated**: 2022-09-07 05:01:02+00:00
- **Authors**: Sudipta Banerjee, Prateek Jaiswal, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: A face morph is created by strategically combining two or more face images corresponding to multiple identities. The intention is for the morphed image to match with multiple identities. Current morph attack detection strategies can detect morphs but cannot recover the images or identities used in creating them. The task of deducing the individual face images from a morphed face image is known as \textit{de-morphing}. Existing work in de-morphing assume the availability of a reference image pertaining to one identity in order to recover the image of the accomplice - i.e., the other identity. In this work, we propose a novel de-morphing method that can recover images of both identities simultaneously from a single morphed face image without needing a reference image or prior information about the morphing process. We propose a generative adversarial network that achieves single image-based de-morphing with a surprisingly high degree of visual realism and biometric similarity with the original face images. We demonstrate the performance of our method on landmark-based morphs and generative model-based morphs with promising results.



### Boundary Guided Semantic Learning for Real-time COVID-19 Lung Infection Segmentation System
- **Arxiv ID**: http://arxiv.org/abs/2209.02934v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02934v1)
- **Published**: 2022-09-07 05:01:38+00:00
- **Updated**: 2022-09-07 05:01:38+00:00
- **Authors**: Runmin Cong, Yumo Zhang, Ning Yang, Haisheng Li, Xueqi Zhang, Ruochen Li, Zewen Chen, Yao Zhao, Sam Kwong
- **Comment**: Accepted by IEEE Transactions on Consumer Electronics 2022
- **Journal**: None
- **Summary**: The coronavirus disease 2019 (COVID-19) continues to have a negative impact on healthcare systems around the world, though the vaccines have been developed and national vaccination coverage rate is steadily increasing. At the current stage, automatically segmenting the lung infection area from CT images is essential for the diagnosis and treatment of COVID-19. Thanks to the development of deep learning technology, some deep learning solutions for lung infection segmentation have been proposed. However, due to the scattered distribution, complex background interference and blurred boundaries, the accuracy and completeness of the existing models are still unsatisfactory. To this end, we propose a boundary guided semantic learning network (BSNet) in this paper. On the one hand, the dual-branch semantic enhancement module that combines the top-level semantic preservation and progressive semantic integration is designed to model the complementary relationship between different high-level features, thereby promoting the generation of more complete segmentation results. On the other hand, the mirror-symmetric boundary guidance module is proposed to accurately detect the boundaries of the lesion regions in a mirror-symmetric way. Experiments on the publicly available dataset demonstrate that our BSNet outperforms the existing state-of-the-art competitors and achieves a real-time inference speed of 44 FPS.



### Can GAN-induced Attribute Manipulations Impact Face Recognition?
- **Arxiv ID**: http://arxiv.org/abs/2209.02941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02941v1)
- **Published**: 2022-09-07 05:26:25+00:00
- **Updated**: 2022-09-07 05:26:25+00:00
- **Authors**: Sudipta Banerjee, Aditi Aggarwal, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: Impact due to demographic factors such as age, sex, race, etc., has been studied extensively in automated face recognition systems. However, the impact of \textit{digitally modified} demographic and facial attributes on face recognition is relatively under-explored. In this work, we study the effect of attribute manipulations induced via generative adversarial networks (GANs) on face recognition performance. We conduct experiments on the CelebA dataset by intentionally modifying thirteen attributes using AttGAN and STGAN and evaluating their impact on two deep learning-based face verification methods, ArcFace and VGGFace. Our findings indicate that some attribute manipulations involving eyeglasses and digital alteration of sex cues can significantly impair face recognition by up to 73% and need further analysis.



### Privacy-Preserving Deep Learning Model for Covid-19 Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.04445v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04445v2)
- **Published**: 2022-09-07 06:15:02+00:00
- **Updated**: 2022-10-10 03:29:31+00:00
- **Authors**: Vijay Srinivas Tida Sai Venkatesh Chilukoti, Sonya Hsu, Xiali Hei
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies demonstrated that X-ray radiography showed higher accuracy than Polymerase Chain Reaction (PCR) testing for COVID-19 detection. Therefore, applying deep learning models to X-rays and radiography images increases the speed and accuracy of determining COVID-19 cases. However, due to Health Insurance Portability and Accountability (HIPAA) compliance, the hospitals were unwilling to share patient data due to privacy concerns. To maintain privacy, we propose differential private deep learning models to secure the patients' private information. The dataset from the Kaggle website is used to evaluate the designed model for COVID-19 detection. The EfficientNet model version was selected according to its highest test accuracy. The injection of differential privacy constraints into the best-obtained model was made to evaluate performance. The accuracy is noted by varying the trainable layers, privacy loss, and limiting information from each sample. We obtained 84\% accuracy with a privacy loss of 10 during the fine-tuning process.



### Visual Transformer for Soil Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.02950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02950v1)
- **Published**: 2022-09-07 06:20:10+00:00
- **Updated**: 2022-09-07 06:20:10+00:00
- **Authors**: Aaryan Jagetia, Umang Goenka, Priyadarshini Kumari, Mary Samuel
- **Comment**: Presented in 2022 IEEE Students Conference on Engineering and Systems
  (SCES), July 01-03, 2022, Prayagraj, India
- **Journal**: None
- **Summary**: Our food security is built on the foundation of soil. Farmers would be unable to feed us with fiber, food, and fuel if the soils were not healthy. Accurately predicting the type of soil helps in planning the usage of the soil and thus increasing productivity. This research employs state-of-the-art Visual Transformers and also compares performance with different models such as SVM, Alexnet, Resnet, and CNN. Furthermore, this study also focuses on differentiating different Visual Transformers architectures. For the classification of soil type, the dataset consists of 4 different types of soil samples such as alluvial, red, black, and clay. The Visual Transformer model outperforms other models in terms of both test and train accuracies by attaining 98.13% on training and 93.62% while testing. The performance of the Visual Transformer exceeds the performance of other models by at least 2%. Hence, the novel Visual Transformers can be used for Computer Vision tasks including Soil Classification.



### BiFuse++: Self-supervised and Efficient Bi-projection Fusion for 360 Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2209.02952v1
- **DOI**: 10.1109/TPAMI.2022.3203516
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02952v1)
- **Published**: 2022-09-07 06:24:21+00:00
- **Updated**: 2022-09-07 06:24:21+00:00
- **Authors**: Fu-En Wang, Yu-Hsuan Yeh, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun
- **Comment**: Accepted in TPAMI 2022; Code: https://github.com/fuenwang/BiFusev2
- **Journal**: None
- **Summary**: Due to the rise of spherical cameras, monocular 360 depth estimation becomes an important technique for many applications (e.g., autonomous systems). Thus, state-of-the-art frameworks for monocular 360 depth estimation such as bi-projection fusion in BiFuse are proposed. To train such a framework, a large number of panoramas along with the corresponding depth ground truths captured by laser sensors are required, which highly increases the cost of data collection. Moreover, since such a data collection procedure is time-consuming, the scalability of extending these methods to different scenes becomes a challenge. To this end, self-training a network for monocular depth estimation from 360 videos is one way to alleviate this issue. However, there are no existing frameworks that incorporate bi-projection fusion into the self-training scheme, which highly limits the self-supervised performance since bi-projection fusion can leverage information from different projection types. In this paper, we propose BiFuse++ to explore the combination of bi-projection fusion and the self-training scenario. To be specific, we propose a new fusion module and Contrast-Aware Photometric Loss to improve the performance of BiFuse and increase the stability of self-training on real-world videos. We conduct both supervised and self-supervised experiments on benchmark datasets and achieve state-of-the-art performance.



### Semi-supervised Crowd Counting via Density Agency
- **Arxiv ID**: http://arxiv.org/abs/2209.02955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02955v1)
- **Published**: 2022-09-07 06:34:00+00:00
- **Updated**: 2022-09-07 06:34:00+00:00
- **Authors**: Hui Lin, Zhiheng Ma, Xiaopeng Hong, Yaowei Wang, Zhou Su
- **Comment**: This is the accepted version of the Paper & Supp to appear in ACM MM
  2022. Please cite the final published version. Code is available at
  https://github.com/LoraLinH/Semi-supervised-Crowd-Counting-via-Density-Agency
- **Journal**: None
- **Summary**: In this paper, we propose a new agency-guided semi-supervised counting approach. First, we build a learnable auxiliary structure, namely the density agency to bring the recognized foreground regional features close to corresponding density sub-classes (agents) and push away background ones. Second, we propose a density-guided contrastive learning loss to consolidate the backbone feature extractor. Third, we build a regression head by using a transformer structure to refine the foreground features further. Finally, an efficient noise depression loss is provided to minimize the negative influence of annotation noises. Extensive experiments on four challenging crowd counting datasets demonstrate that our method achieves superior performance to the state-of-the-art semi-supervised counting methods by a large margin. Code is available.



### A Weakly Supervised Learning Framework for Salient Object Detection via Hybrid Labels
- **Arxiv ID**: http://arxiv.org/abs/2209.02957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02957v1)
- **Published**: 2022-09-07 06:45:39+00:00
- **Updated**: 2022-09-07 06:45:39+00:00
- **Authors**: Runmin Cong, Qi Qin, Chen Zhang, Qiuping Jiang, Shiqi Wang, Yao Zhao, Sam Kwong
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology 2022
- **Journal**: None
- **Summary**: Fully-supervised salient object detection (SOD) methods have made great progress, but such methods often rely on a large number of pixel-level annotations, which are time-consuming and labour-intensive. In this paper, we focus on a new weakly-supervised SOD task under hybrid labels, where the supervision labels include a large number of coarse labels generated by the traditional unsupervised method and a small number of real labels. To address the issues of label noise and quantity imbalance in this task, we design a new pipeline framework with three sophisticated training strategies. In terms of model framework, we decouple the task into label refinement sub-task and salient object detection sub-task, which cooperate with each other and train alternately. Specifically, the R-Net is designed as a two-stream encoder-decoder model equipped with Blender with Guidance and Aggregation Mechanisms (BGA), aiming to rectify the coarse labels for more reliable pseudo-labels, while the S-Net is a replaceable SOD network supervised by the pseudo labels generated by the current R-Net. Note that, we only need to use the trained S-Net for testing. Moreover, in order to guarantee the effectiveness and efficiency of network training, we design three training strategies, including alternate iteration mechanism, group-wise incremental mechanism, and credibility verification mechanism. Experiments on five SOD benchmarks show that our method achieves competitive performance against weakly-supervised/unsupervised methods both qualitatively and quantitatively.



### Difficulty-Net: Learning to Predict Difficulty for Long-Tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.02960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02960v1)
- **Published**: 2022-09-07 07:04:08+00:00
- **Updated**: 2022-09-07 07:04:08+00:00
- **Authors**: Saptarshi Sinha, Hiroki Ohashi
- **Comment**: Accepted for publication at WACV 2023
- **Journal**: None
- **Summary**: Long-tailed datasets, where head classes comprise much more training samples than tail classes, cause recognition models to get biased towards the head classes. Weighted loss is one of the most popular ways of mitigating this issue, and a recent work has suggested that class-difficulty might be a better clue than conventionally used class-frequency to decide the distribution of weights. A heuristic formulation was used in the previous work for quantifying the difficulty, but we empirically find that the optimal formulation varies depending on the characteristics of datasets. Therefore, we propose Difficulty-Net, which learns to predict the difficulty of classes using the model's performance in a meta-learning framework. To make it learn reasonable difficulty of a class within the context of other classes, we newly introduce two key concepts, namely the relative difficulty and the driver loss. The former helps Difficulty-Net take other classes into account when calculating difficulty of a class, while the latter is indispensable for guiding the learning to a meaningful direction. Extensive experiments on popular long-tailed datasets demonstrated the effectiveness of the proposed method, and it achieved state-of-the-art performance on multiple long-tailed datasets.



### Risk of Bias in Chest X-ray Foundation Models
- **Arxiv ID**: http://arxiv.org/abs/2209.02965v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02965v2)
- **Published**: 2022-09-07 07:16:30+00:00
- **Updated**: 2022-12-19 10:02:16+00:00
- **Authors**: Ben Glocker, Charles Jones, Melanie Bernhardt, Stefan Winzeck
- **Comment**: Code available under https://github.com/biomedia-mira/chexploration
- **Journal**: None
- **Summary**: Foundation models are considered a breakthrough in all applications of AI, promising robust and reusable mechanisms for feature extraction, alleviating the need for large amounts of high quality annotated training data for task-specific prediction models. However, foundation models may potentially encode and even reinforce existing biases present in historic datasets. Given the limited ability to scrutinize foundation models, it remains unclear whether the opportunities outweigh the risks in safety critical applications such as clinical decision making. In our statistical bias analysis of a recently published, and publicly accessible chest X-ray foundation model, we found reasons for concern as the model seems to encode protected characteristics including biological sex and racial identity. When used for the downstream application of disease detection, we observed substantial degradation of performance of the foundation model compared to a standard model with specific disparities in protected subgroups. While research into foundation models for healthcare applications is in an early stage, we hope to raise awareness of the risks by highlighting the importance of conducting thorough bias and subgroup performance analyses.



### YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications
- **Arxiv ID**: http://arxiv.org/abs/2209.02976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02976v1)
- **Published**: 2022-09-07 07:47:58+00:00
- **Updated**: 2022-09-07 07:47:58+00:00
- **Authors**: Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, Xiaolin Wei
- **Comment**: technical report
- **Journal**: None
- **Summary**: For years, the YOLO series has been the de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application.   Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academia. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization, and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9% AP on the COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5% AP at 495 FPS, outperforming other mainstream detectors at the same scale~(YOLOv5-S, YOLOX-S, and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5%/52.3%) than other detectors with a similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at https://github.com/meituan/YOLOv6.



### Shifting Perspective to See Difference: A Novel Multi-View Method for Skeleton based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.02986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02986v1)
- **Published**: 2022-09-07 08:20:37+00:00
- **Updated**: 2022-09-07 08:20:37+00:00
- **Authors**: Ruijie Hou, Yanran Li, Ningyu Zhang, Yulin Zhou, Xiaosong Yang, Zhao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based human action recognition is a longstanding challenge due to its complex dynamics. Some fine-grain details of the dynamics play a vital role in classification. The existing work largely focuses on designing incremental neural networks with more complicated adjacent matrices to capture the details of joints relationships. However, they still have difficulties distinguishing actions that have broadly similar motion patterns but belong to different categories. Interestingly, we found that the subtle differences in motion patterns can be significantly amplified and become easy for audience to distinct through specified view directions, where this property haven't been fully explored before. Drastically different from previous work, we boost the performance by proposing a conceptually simple yet effective Multi-view strategy that recognizes actions from a collection of dynamic view features. Specifically, we design a novel Skeleton-Anchor Proposal (SAP) module which contains a Multi-head structure to learn a set of views. For feature learning of different views, we introduce a novel Angle Representation to transform the actions under different views and feed the transformations into the baseline model. Our module can work seamlessly with the existing action classification model. Incorporated with baseline models, our SAP module exhibits clear performance gains on many challenging benchmarks. Moreover, comprehensive experiments show that our model consistently beats down the state-of-the-art and remains effective and robust especially when dealing with corrupted data. Related code will be available on https://github.com/ideal-idea/SAP .



### Auto-TransRL: Autonomous Composition of Vision Pipelines for Robotic Perception
- **Arxiv ID**: http://arxiv.org/abs/2209.02991v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.02991v1)
- **Published**: 2022-09-07 08:30:23+00:00
- **Updated**: 2022-09-07 08:30:23+00:00
- **Authors**: Aditya Kapoor, Nijil George, Vartika Sengar, Vighnesh Vatsal, Jayavardhana Gubbi
- **Comment**: Presented at the IEEE ICRA 2022 Workshop in Robotic Perception and
  Mapping: Emerging Techniques
- **Journal**: None
- **Summary**: Creating a vision pipeline for different datasets to solve a computer vision task is a complex and time consuming process. Currently, these pipelines are developed with the help of domain experts. Moreover, there is no systematic structure to construct a vision pipeline apart from relying on experience, trial and error or using template-based approaches. As the search space for choosing suitable algorithms for achieving a particular vision task is large, human exploration for finding a good solution requires time and effort. To address the following issues, we propose a dynamic and data-driven way to identify an appropriate set of algorithms that would be fit for building the vision pipeline in order to achieve the goal task. We introduce a Transformer Architecture complemented with Deep Reinforcement Learning to recommend algorithms that can be incorporated at different stages of the vision workflow. This system is both robust and adaptive to dynamic changes in the environment. Experimental results further show that our method also generalizes well to recommend algorithms that have not been used while training and hence alleviates the need of retraining the system on a new set of algorithms introduced during test time.



### On the Transferability of Adversarial Examples between Encrypted Models
- **Arxiv ID**: http://arxiv.org/abs/2209.02997v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02997v1)
- **Published**: 2022-09-07 08:50:26+00:00
- **Updated**: 2022-09-07 08:50:26+00:00
- **Authors**: Miki Tanaka, Isao Echizen, Hitoshi Kiya
- **Comment**: to be appear in ISPACS 2022
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In addition, AEs have adversarial transferability, namely, AEs generated for a source model fool other (target) models. In this paper, we investigate the transferability of models encrypted for adversarially robust defense for the first time. To objectively verify the property of transferability, the robustness of models is evaluated by using a benchmark attack method, called AutoAttack. In an image-classification experiment, the use of encrypted models is confirmed not only to be robust against AEs but to also reduce the influence of AEs in terms of the transferability of models.



### Zoom Text Detector
- **Arxiv ID**: http://arxiv.org/abs/2209.03014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03014v1)
- **Published**: 2022-09-07 09:19:21+00:00
- **Updated**: 2022-09-07 09:19:21+00:00
- **Authors**: Chuang. Yang, Mulin. Chen, Yuan. Yuan, Qi. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: To pursue comprehensive performance, recent text detectors improve detection speed at the expense of accuracy. They adopt shrink-mask based text representation strategies, which leads to a high dependency of detection accuracy on shrink-masks. Unfortunately, three disadvantages cause unreliable shrink-masks. Specifically, these methods try to strengthen the discrimination of shrink-masks from the background by semantic information. However, the feature defocusing phenomenon that coarse layers are optimized by fine-grained objectives limits the extraction of semantic features. Meanwhile, since both shrink-masks and the margins belong to texts, the detail loss phenomenon that the margins are ignored hinders the distinguishment of shrink-masks from the margins, which causes ambiguous shrink-mask edges. Moreover, false-positive samples enjoy similar visual features with shrink-masks. They aggravate the decline of shrink-masks recognition. To avoid the above problems, we propose a Zoom Text Detector (ZTD) inspired by the zoom process of the camera. Specifically, Zoom Out Module (ZOM) is introduced to provide coarse-grained optimization objectives for coarse layers to avoid feature defocusing. Meanwhile, Zoom In Module (ZIM) is presented to enhance the margins recognition to prevent detail loss. Furthermore, Sequential-Visual Discriminator (SVD) is designed to suppress false-positive samples by sequential and visual features. Experiments verify the superior comprehensive performance of ZTD.



### Text Growing on Leaf
- **Arxiv ID**: http://arxiv.org/abs/2209.03016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03016v1)
- **Published**: 2022-09-07 09:22:32+00:00
- **Updated**: 2022-09-07 09:22:32+00:00
- **Authors**: Chuang. Yang, Mulin. Chen, Yuan. Yuan, Qi. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Irregular-shaped texts bring challenges to Scene Text Detection (STD). Although existing contour point sequence-based approaches achieve comparable performances, they fail to cover some highly curved ribbon-like text lines. It leads to limited text fitting ability and STD technique application. Considering the above problem, we combine text geometric characteristics and bionics to design a natural leaf vein-based text representation method (LVT). Concretely, it is found that leaf vein is a generally directed graph, which can easily cover various geometries. Inspired by it, we treat text contour as leaf margin and represent it through main, lateral, and thin veins. We further construct a detection framework based on LVT, namely LeafText. In the text reconstruction stage, LeafText simulates the leaf growth process to rebuild text contour. It grows main vein in Cartesian coordinates to locate text roughly at first. Then, lateral and thin veins are generated along the main vein growth direction in polar coordinates. They are responsible for generating coarse contour and refining it, respectively. Considering the deep dependency of lateral and thin veins on main vein, the Multi-Oriented Smoother (MOS) is proposed to enhance the robustness of main vein to ensure a reliable detection result. Additionally, we propose a global incentive loss to accelerate the predictions of lateral and thin veins. Ablation experiments demonstrate LVT is able to depict arbitrary-shaped texts precisely and verify the effectiveness of MOS and global incentive loss. Comparisons show that LeafText is superior to existing state-of-the-art (SOTA) methods on MSRA-TD500, CTW1500, Total-Text, and ICDAR2015 datasets.



### SIRA: Relightable Avatars from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2209.03027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03027v1)
- **Published**: 2022-09-07 09:47:46+00:00
- **Updated**: 2022-09-07 09:47:46+00:00
- **Authors**: Pol Caselles, Eduard Ramon, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer, Gil Triginer
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering the geometry of a human head from a single image, while factorizing the materials and illumination is a severely ill-posed problem that requires prior information to be solved. Methods based on 3D Morphable Models (3DMM), and their combination with differentiable renderers, have shown promising results. However, the expressiveness of 3DMMs is limited, and they typically yield over-smoothed and identity-agnostic 3D shapes limited to the face region. Highly accurate full head reconstructions have recently been obtained with neural fields that parameterize the geometry using multilayer perceptrons. The versatility of these representations has also proved effective for disentangling geometry, materials and lighting. However, these methods require several tens of input images. In this paper, we introduce SIRA, a method which, from a single image, reconstructs human head avatars with high fidelity geometry and factorized lights and surface materials. Our key ingredients are two data-driven statistical models based on neural fields that resolve the ambiguities of single-view 3D surface reconstruction and appearance factorization. Experiments show that SIRA obtains state of the art results in 3D head reconstruction while at the same time it successfully disentangles the global illumination, and the diffuse and specular albedos. Furthermore, our reconstructions are amenable to physically-based appearance editing and head model relighting.



### Not All Instances Contribute Equally: Instance-adaptive Class Representation Learning for Few-Shot Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.03034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03034v1)
- **Published**: 2022-09-07 10:00:18+00:00
- **Updated**: 2022-09-07 10:00:18+00:00
- **Authors**: Mengya Han, Yibing Zhan, Yong Luo, Bo Du, Han Hu, Yonggang Wen, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot visual recognition refers to recognize novel visual concepts from a few labeled instances. Many few-shot visual recognition methods adopt the metric-based meta-learning paradigm by comparing the query representation with class representations to predict the category of query instance. However, current metric-based methods generally treat all instances equally and consequently often obtain biased class representation, considering not all instances are equally significant when summarizing the instance-level representations for the class-level representation. For example, some instances may contain unrepresentative information, such as too much background and information of unrelated concepts, which skew the results. To address the above issues, we propose a novel metric-based meta-learning framework termed instance-adaptive class representation learning network (ICRL-Net) for few-shot visual recognition. Specifically, we develop an adaptive instance revaluing network with the capability to address the biased representation issue when generating the class representation, by learning and assigning adaptive weights for different instances according to their relative significance in the support set of corresponding class. Additionally, we design an improved bilinear instance representation and incorporate two novel structural losses, i.e., intra-class instance clustering loss and inter-class representation distinguishing loss, to further regulate the instance revaluation process and refine the class representation. We conduct extensive experiments on four commonly adopted few-shot benchmarks: miniImageNet, tieredImageNet, CIFAR-FS, and FC100 datasets. The experimental results compared with the state-of-the-art approaches demonstrate the superiority of our ICRL-Net.



### Multi-Scale Attention-based Multiple Instance Learning for Classification of Multi-Gigapixel Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2209.03041v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.03041v1)
- **Published**: 2022-09-07 10:14:02+00:00
- **Updated**: 2022-09-07 10:14:02+00:00
- **Authors**: Made Satria Wibawa, Kwok-Wai Lo, Lawrence Young, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Histology images with multi-gigapixel of resolution yield rich information for cancer diagnosis and prognosis. Most of the time, only slide-level label is available because pixel-wise annotation is labour intensive task. In this paper, we propose a deep learning pipeline for classification in histology images. Using multiple instance learning, we attempt to predict the latent membrane protein 1 (LMP1) status of nasopharyngeal carcinoma (NPC) based on haematoxylin and eosin-stain (H&E) histology images. We utilised attention mechanism with residual connection for our aggregation layers. In our 3-fold cross-validation experiment, we achieved average accuracy, AUC and F1-score 0.936, 0.995 and 0.862, respectively. This method also allows us to examine the model interpretability by visualising attention scores. To the best of our knowledge, this is the first attempt to predict LMP1 status on NPC using deep learning.



### Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2209.03355v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.03355v2)
- **Published**: 2022-09-07 10:58:41+00:00
- **Updated**: 2023-05-09 15:14:50+00:00
- **Authors**: Simone Angarano, Francesco Salvetti, Mauro Martini, Marcello Chiaberge
- **Comment**: None
- **Journal**: None
- **Summary**: Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real-time Super-Resolution, called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for mobile robotic applications.



### MimCo: Masked Image Modeling Pre-training with Contrastive Teacher
- **Arxiv ID**: http://arxiv.org/abs/2209.03063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03063v2)
- **Published**: 2022-09-07 10:59:05+00:00
- **Updated**: 2023-04-20 07:41:05+00:00
- **Authors**: Qiang Zhou, Chaohui Yu, Hao Luo, Zhibin Wang, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent masked image modeling (MIM) has received much attention in self-supervised learning (SSL), which requires the target model to recover the masked part of the input image. Although MIM-based pre-training methods achieve new state-of-the-art performance when transferred to many downstream tasks, the visualizations show that the learned representations are less separable, especially compared to those based on contrastive learning pre-training. This inspires us to think whether the linear separability of MIM pre-trained representation can be further improved, thereby improving the pre-training performance. Since MIM and contrastive learning tend to utilize different data augmentations and training strategies, combining these two pretext tasks is not trivial. In this work, we propose a novel and flexible pre-training framework, named MimCo, which combines MIM and contrastive learning through two-stage pre-training. Specifically, MimCo takes a pre-trained contrastive learning model as the teacher model and is pre-trained with two types of learning targets: patch-level and image-level reconstruction losses.   Extensive transfer experiments on downstream tasks demonstrate the superior performance of our MimCo pre-training framework. Taking ViT-S as an example, when using the pre-trained MoCov3-ViT-S as the teacher model, MimCo only needs 100 epochs of pre-training to achieve 82.53% top-1 finetuning accuracy on Imagenet-1K, which outperforms the state-of-the-art self-supervised learning counterparts.



### Plant Species Classification Using Transfer Learning by Pretrained Classifier VGG-19
- **Arxiv ID**: http://arxiv.org/abs/2209.03076v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.03076v1)
- **Published**: 2022-09-07 11:28:50+00:00
- **Updated**: 2022-09-07 11:28:50+00:00
- **Authors**: Thiru Siddharth, Bhupendra Singh Kirar, Dheeraj Kumar Agrawal
- **Comment**: Under review process in 'IETE Journal of Research'
- **Journal**: None
- **Summary**: Deep learning is currently the most important branch of machine learning, with applications in speech recognition, computer vision, image classification, and medical imaging analysis. Plant recognition is one of the areas where image classification can be used to identify plant species through their leaves. Botanists devote a significant amount of time to recognizing plant species by personally inspecting. This paper describes a method for dissecting color images of Swedish leaves and identifying plant species. To achieve higher accuracy, the task is completed using transfer learning with the help of pre-trained classifier VGG-19. The four primary processes of classification are image preprocessing, image augmentation, feature extraction, and recognition, which are performed as part of the overall model evaluation. The VGG-19 classifier grasps the characteristics of leaves by employing pre-defined hidden layers such as convolutional layers, max pooling layers, and fully connected layers, and finally uses the soft-max layer to generate a feature representation for all plant classes. The model obtains knowledge connected to aspects of the Swedish leaf dataset, which contains fifteen tree classes, and aids in predicting the proper class of an unknown plant with an accuracy of 99.70% which is higher than previous research works reported.



### MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.03102v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03102v3)
- **Published**: 2022-09-07 12:29:29+00:00
- **Updated**: 2023-03-03 08:16:08+00:00
- **Authors**: Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, Yu-Gang Jiang
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Fusing LiDAR and camera information is essential for achieving accurate and reliable 3D object detection in autonomous driving systems. This is challenging due to the difficulty of combining multi-granularity geometric and semantic features from two drastically different modalities. Recent approaches aim at exploring the semantic densities of camera features through lifting points in 2D camera images (referred to as seeds) into 3D space, and then incorporate 2D semantics via cross-modal interaction or fusion techniques. However, depth information is under-investigated in these approaches when lifting points into 3D space, thus 2D semantics can not be reliably fused with 3D points. Moreover, their multi-modal fusion strategy, which is implemented as concatenation or attention, either can not effectively fuse 2D and 3D information or is unable to perform fine-grained interactions in the voxel space. To this end, we propose a novel framework with better utilization of the depth information and fine-grained cross-modal interaction between LiDAR and camera, which consists of two important components. First, a Multi-Depth Unprojection (MDU) method with depth-aware designs is used to enhance the depth quality of the lifted points at each interaction level. Second, a Gated Modality-Aware Convolution (GMA-Conv) block is applied to modulate voxels involved with the camera modality in a fine-grained manner and then aggregate multi-modal features into a unified space. Together they provide the detection head with more comprehensive features from LiDAR and camera. On the nuScenes test benchmark, our proposed method, abbreviated as MSMDFusion, achieves state-of-the-art 3D object detection results with 71.5% mAP and 74.0% NDS, and strong tracking results with 74.0% AMOTA without using test-time-augmentation and ensemble techniques. The code is available at https://github.com/SxJyJay/MSMDFusion.



### Inference and Learning for Generative Capsule Models
- **Arxiv ID**: http://arxiv.org/abs/2209.03115v2
- **DOI**: 10.1162/neco_a_01564
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03115v2)
- **Published**: 2022-09-07 13:05:47+00:00
- **Updated**: 2022-10-21 14:28:45+00:00
- **Authors**: Alfredo Nazabal, Nikolaos Tsagkas, Christopher K. I. Williams
- **Comment**: 31 pages, 6 figures. This paper extends our previous work
  (arxiv:2103.06676) by covering the learning of the models as well as
  inference. Paper accepted for publication in Neural Computation
- **Journal**: Neural Computation 35(4) (2023) 727-761
- **Summary**: Capsule networks (see e.g. Hinton et al., 2018) aim to encode knowledge of and reason about the relationship between an object and its parts. In this paper we specify a generative model for such data, and derive a variational algorithm for inferring the transformation of each model object in a scene, and the assignments of observed parts to the objects. We derive a learning algorithm for the object models, based on variational expectation maximization (Jordan et al., 1999). We also study an alternative inference algorithm based on the RANSAC method of Fischler and Bolles (1981). We apply these inference methods to (i) data generated from multiple geometric objects like squares and triangles ("constellations"), and (ii) data from a parts-based model of faces. Recent work by Kosiorek et al. (2019) has used amortized inference via stacked capsule autoencoders (SCAEs) to tackle this problem -- our results show that we significantly outperform them where we can make comparisons (on the constellations data).



### A New Method for the High-Precision Assessment of Tumor Changes in Response to Treatment
- **Arxiv ID**: http://arxiv.org/abs/2209.03116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03116v1)
- **Published**: 2022-09-07 13:08:14+00:00
- **Updated**: 2022-09-07 13:08:14+00:00
- **Authors**: P. D. Tar, N. A. Thacker, J. P. B. O'Connor
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging demonstrates that preclinical and human tumors are heterogeneous, i.e. a single tumor can exhibit multiple regions that behave differently during both normal development and also in response to treatment. The large variations observed in control group tumors can obscure detection of significant therapeutic effects due to the ambiguity in attributing causes of change. This can hinder development of effective therapies due to limitations in experimental design, rather than due to therapeutic failure. An improved method to model biological variation and heterogeneity in imaging signals is described. Specifically, Linear Poisson modelling (LPM) evaluates changes in apparent diffusion co-efficient (ADC) before and 72 hours after radiotherapy, in two xenograft models of colorectal cancer. The statistical significance of measured changes are compared to those attainable using a conventional t-test analysis on basic ADC distribution parameters. When LPMs were applied to treated tumors, the LPMs detected highly significant changes. The analyses were significant for all tumors, equating to a gain in power of 4 fold (i.e. equivelent to having a sample size 16 times larger), compared with the conventional approach. In contrast, highly significant changes are only detected at a cohort level using t-tests, restricting their potential use within personalised medicine and increasing the number of animals required during testing. Furthermore, LPM enabled the relative volumes of responding and non-responding tissue to be estimated for each xenograft model. Leave-one-out analysis of the treated xenografts provided quality control and identified potential outliers, raising confidence in LPM data at clinically relevant sample sizes.



### CGAN-ECT: Tomography Image Reconstruction from Electrical Capacitance Measurements Using CGANs
- **Arxiv ID**: http://arxiv.org/abs/2209.03737v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, I.2.6; I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2209.03737v2)
- **Published**: 2022-09-07 13:08:38+00:00
- **Updated**: 2022-09-12 08:51:45+00:00
- **Authors**: Wael Deabes, Alaa E. Abdel-Hakim
- **Comment**: 13 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Due to the rapid growth of Electrical Capacitance Tomography (ECT) applications in several industrial fields, there is a crucial need for developing high quality, yet fast, methodologies of image reconstruction from raw capacitance measurements. Deep learning, as an effective non-linear mapping tool for complicated functions, has been going viral in many fields including electrical tomography. In this paper, we propose a Conditional Generative Adversarial Network (CGAN) model for reconstructing ECT images from capacitance measurements. The initial image of the CGAN model is constructed from the capacitance measurement. To our knowledge, this is the first time to represent the capacitance measurements in an image form. We have created a new massive ECT dataset of 320K synthetic image measurements pairs for training, and testing the proposed model. The feasibility and generalization ability of the proposed CGAN-ECT model are evaluated using testing dataset, contaminated data and flow patterns that are not exposed to the model during the training phase. The evaluation results prove that the proposed CGAN-ECT model can efficiently create more accurate ECT images than traditional and other deep learning-based image reconstruction algorithms. CGAN-ECT achieved an average image correlation coefficient of more than 99.3% and an average relative image error about 0.07.



### DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.03126v2
- **DOI**: 10.1109/ACCESS.2022.3221812
- **Categories**: **cs.MM**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.03126v2)
- **Published**: 2022-09-07 13:25:09+00:00
- **Updated**: 2022-11-22 06:01:10+00:00
- **Authors**: Shunsuke Kitada, Yuki Iwazaki, Riku Togashi, Hitoshi Iyatomi
- **Comment**: 12 pages, 3 figures. Accepted by IEEE Access on Nov. 3, 2022
- **Journal**: in IEEE Access, vol. 10, pp. 120023-120034, 2022
- **Summary**: There is increasing interest in the use of multimodal data in various web applications, such as digital advertising and e-commerce. Typical methods for extracting important information from multimodal data rely on a mid-fusion architecture that combines the feature representations from multiple encoders. However, as the number of modalities increases, several potential problems with the mid-fusion model structure arise, such as an increase in the dimensionality of the concatenated multimodal features and missing modalities. To address these problems, we propose a new concept that considers multimodal inputs as a set of sequences, namely, deep multimodal sequence sets (DM$^2$S$^2$). Our set-aware concept consists of three components that capture the relationships among multiple modalities: (a) a BERT-based encoder to handle the inter- and intra-order of elements in the sequences, (b) intra-modality residual attention (IntraMRA) to capture the importance of the elements in a modality, and (c) inter-modality residual attention (InterMRA) to enhance the importance of elements with modality-level granularity further. Our concept exhibits performance that is comparable to or better than the previous set-aware models. Furthermore, we demonstrate that the visualization of the learned InterMRA and IntraMRA weights can provide an interpretation of the prediction results.



### MSSPN: Automatic First Arrival Picking using Multi-Stage Segmentation Picking Network
- **Arxiv ID**: http://arxiv.org/abs/2209.03132v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03132v1)
- **Published**: 2022-09-07 13:30:51+00:00
- **Updated**: 2022-09-07 13:30:51+00:00
- **Authors**: Hongtao Wang, Jiangshe Zhang, Xiaoli Wei, Chunxia Zhang, Zhenbo Guo, Li Long, Yicheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Picking the first arrival times of prestack gathers is called First Arrival Time (FAT) picking, which is an indispensable step in seismic data processing, and is mainly solved manually in the past. With the current increasing density of seismic data collection, the efficiency of manual picking has been unable to meet the actual needs. Therefore, automatic picking methods have been greatly developed in recent decades, especially those based on deep learning. However, few of the current supervised deep learning-based method can avoid the dependence on labeled samples. Besides, since the gather data is a set of signals which are greatly different from the natural images, it is difficult for the current method to solve the FAT picking problem in case of a low Signal to Noise Ratio (SNR). In this paper, for hard rock seismic gather data, we propose a Multi-Stage Segmentation Pickup Network (MSSPN), which solves the generalization problem across worksites and the picking problem in the case of low SNR. In MSSPN, there are four sub-models to simulate the manually picking processing, which is assumed to four stages from coarse to fine. Experiments on seven field datasets with different qualities show that our MSSPN outperforms benchmarks by a large margin.Particularly, our method can achieve more than 90\% accurate picking across worksites in the case of medium and high SNRs, and even fine-tuned model can achieve 88\% accurate picking of the dataset with low SNR.



### ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections
- **Arxiv ID**: http://arxiv.org/abs/2209.05252v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.05252v1)
- **Published**: 2022-09-07 13:32:45+00:00
- **Updated**: 2022-09-07 13:32:45+00:00
- **Authors**: Manlio Massiris Fernndez, Sanjin Rado, Kreimir Matkovi, M. Eduard Grller, Claudio Delrieux
- **Comment**: Accepted for IEEE VIS 2022 and IEEE TVCG
- **Journal**: None
- **Summary**: Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation, based on expert-assisted observation of the workplaces and manually filling in score tables, is still predominant. Data analysis is usually done with a focus on critical moments, although without the support of contextual information and changes over time. In this paper we introduce ErgoExplorer, a system for the interactive visual analysis of risk assessment data. In contrast to the current practice, we focus on data that span across multiple actions and multiple workers while keeping all contextual information. Data is automatically extracted from video streams. Based on carefully investigated analysis tasks, we introduce new views and their corresponding interactions. These views also incorporate domain-specific score tables to guarantee an easy adoption by domain experts. All views are integrated into ErgoExplorer, which relies on coordinated multiple views to facilitate analysis through interaction. ErgoExplorer makes it possible for the first time to examine complex relationships between risk assessments of individual body parts over long sessions that span multiple operations. The newly introduced approach supports analysis and exploration at several levels of detail, ranging from a general overview, down to inspecting individual frames in the video stream, if necessary. We illustrate the usefulness of the newly proposed approach applying it to several datasets.



### FasterX: Real-Time Object Detection Based on Edge GPUs for UAV Applications
- **Arxiv ID**: http://arxiv.org/abs/2209.03157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2209.03157v1)
- **Published**: 2022-09-07 13:52:25+00:00
- **Updated**: 2022-09-07 13:52:25+00:00
- **Authors**: Wei Zhou, Xuanlin Min, Rui Hu, Yiwen Long, Huan Luo, JunYi
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Real-time object detection on Unmanned Aerial Vehicles (UAVs) is a challenging issue due to the limited computing resources of edge GPU devices as Internet of Things (IoT) nodes. To solve this problem, in this paper, we propose a novel lightweight deep learning architectures named FasterX based on YOLOX model for real-time object detection on edge GPU. First, we design an effective and lightweight PixSF head to replace the original head of YOLOX to better detect small objects, which can be further embedded in the depthwise separable convolution (DS Conv) to achieve a lighter head. Then, a slimmer structure in the Neck layer termed as SlimFPN is developed to reduce parameters of the network, which is a trade-off between accuracy and speed. Furthermore, we embed attention module in the Head layer to improve the feature extraction effect of the prediction head. Meanwhile, we also improve the label assignment strategy and loss function to alleviate category imbalance and box optimization problems of the UAV dataset. Finally, auxiliary heads are presented for online distillation to improve the ability of position embedding and feature extraction in PixSF head. The performance of our lightweight models are validated experimentally on the NVIDIA Jetson NX and Jetson Nano GPU embedded platforms.Extensive experiments show that FasterX models achieve better trade-off between accuracy and latency on VisDrone2021 dataset compared to state-of-the-art models.



### AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.03160v2
- **DOI**: 10.1145/3503161.3547790
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03160v2)
- **Published**: 2022-09-07 13:53:54+00:00
- **Updated**: 2022-09-08 04:24:35+00:00
- **Authors**: Yiyang Ma, Huan Yang, Bei Liu, Jianlong Fu, Jiaying Liu
- **Comment**: None
- **Journal**: None
- **Summary**: AI illustrator aims to automatically design visually appealing images for books to provoke rich thoughts and emotions. To achieve this goal, we propose a framework for translating raw descriptions with complex semantics into semantically corresponding images. The main challenge lies in the complexity of the semantics of raw descriptions, which may be hard to be visualized (e.g., "gloomy" or "Asian"). It usually poses challenges for existing methods to handle such descriptions. To address this issue, we propose a Prompt-based Cross-Modal Generation Framework (PCM-Frame) to leverage two powerful pre-trained models, including CLIP and StyleGAN. Our framework consists of two components: a projection module from Text Embeddings to Image Embeddings based on prompts, and an adapted image generation module built on StyleGAN which takes Image Embeddings as inputs and is trained by combined semantic consistency losses. To bridge the gap between realistic images and illustration designs, we further adopt a stylization model as post-processing in our framework for better visual effects. Benefiting from the pre-trained models, our method can handle complex descriptions and does not require external paired data for training. Furthermore, we have built a benchmark that consists of 200 raw descriptions. We conduct a user study to demonstrate our superiority over the competing methods with complicated texts. We release our code at https://github.com/researchmm/AI_Illustrator.



### Explainable Artificial Intelligence to Detect Image Spam Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2209.03166v1
- **DOI**: 10.1109/ICCR56254.2022.9995839
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2209.03166v1)
- **Published**: 2022-09-07 14:02:16+00:00
- **Updated**: 2022-09-07 14:02:16+00:00
- **Authors**: Zhibo Zhang, Ernesto Damiani, Hussam Al Hamadi, Chan Yeob Yeun, Fatma Taher
- **Comment**: Under review by International Conference on Cyber Resilience (ICCR),
  Dubai 2022
- **Journal**: None
- **Summary**: Image spam threat detection has continually been a popular area of research with the internet's phenomenal expansion. This research presents an explainable framework for detecting spam images using Convolutional Neural Network(CNN) algorithms and Explainable Artificial Intelligence (XAI) algorithms. In this work, we use CNN model to classify image spam respectively whereas the post-hoc XAI methods including Local Interpretable Model Agnostic Explanation (LIME) and Shapley Additive Explanations (SHAP) were deployed to provide explanations for the decisions that the black-box CNN models made about spam image detection. We train and then evaluate the performance of the proposed approach on a 6636 image dataset including spam images and normal images collected from three different publicly available email corpora. The experimental results show that the proposed framework achieved satisfactory detection results in terms of different performance metrics whereas the model-independent XAI algorithms could provide explanations for the decisions of different models which could be utilized for comparison for the future study.



### Morphology-preserving Autoregressive 3D Generative Modelling of the Brain
- **Arxiv ID**: http://arxiv.org/abs/2209.03177v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T99 (Primary) 92C55 (Secondary), I.2.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2209.03177v1)
- **Published**: 2022-09-07 14:17:42+00:00
- **Updated**: 2022-09-07 14:17:42+00:00
- **Authors**: Petru-Daniel Tudosiu, Walter Hugo Lopez Pinaya, Mark S. Graham, Pedro Borges, Virginia Fernandez, Dai Yang, Jeremy Appleyard, Guido Novati, Disha Mehra, Mike Vella, Parashkev Nachev, Sebastien Ourselin, Jorge Cardoso
- **Comment**: 13 pages, 3 figures, 2 tables, accepted at SASHIMI MICCAI 2022
- **Journal**: None
- **Summary**: Human anatomy, morphology, and associated diseases can be studied using medical imaging data. However, access to medical imaging data is restricted by governance and privacy concerns, data ownership, and the cost of acquisition, thus limiting our ability to understand the human body. A possible solution to this issue is the creation of a model able to learn and then generate synthetic images of the human body conditioned on specific characteristics of relevance (e.g., age, sex, and disease status). Deep generative models, in the form of neural networks, have been recently used to create synthetic 2D images of natural scenes. Still, the ability to produce high-resolution 3D volumetric imaging data with correct anatomical morphology has been hampered by data scarcity and algorithmic and computational limitations. This work proposes a generative model that can be scaled to produce anatomically correct, high-resolution, and realistic images of the human brain, with the necessary quality to allow further downstream analyses. The ability to generate a potentially unlimited amount of data not only enables large-scale studies of human anatomy and pathology without jeopardizing patient privacy, but also significantly advances research in the field of anomaly detection, modality synthesis, learning under limited data, and fair and ethical AI. Code and trained models are available at: https://github.com/AmigoLab/SynthAnatomy.



### Hardware faults that matter: Understanding and Estimating the safety impact of hardware faults on object detection DNNs
- **Arxiv ID**: http://arxiv.org/abs/2209.03225v1
- **DOI**: 10.1007/978-3-031-14835-4_20
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03225v1)
- **Published**: 2022-09-07 15:27:09+00:00
- **Updated**: 2022-09-07 15:27:09+00:00
- **Authors**: Syed Qutub, Florian Geissler, Yang Peng, Ralf Grafe, Michael Paulitsch, Gereon Hinz, Alois Knoll
- **Comment**: 15 pages, accepted in safecomp22 conference
- **Journal**: None
- **Summary**: Object detection neural network models need to perform reliably in highly dynamic and safety-critical environments like automated driving or robotics. Therefore, it is paramount to verify the robustness of the detection under unexpected hardware faults like soft errors that can impact a systems perception module. Standard metrics based on average precision produce model vulnerability estimates at the object level rather than at an image level. As we show in this paper, this does not provide an intuitive or representative indicator of the safety-related impact of silent data corruption caused by bit flips in the underlying memory but can lead to an over- or underestimation of typical fault-induced hazards. With an eye towards safety-related real-time applications, we propose a new metric IVMOD (Image-wise Vulnerability Metric for Object Detection) to quantify vulnerability based on an incorrect image-wise object detection due to false positive (FPs) or false negative (FNs) objects, combined with a severity analysis. The evaluation of several representative object detection models shows that even a single bit flip can lead to a severe silent data corruption event with potentially critical safety implications, with e.g., up to (much greater than) 100 FPs generated, or up to approx. 90% of true positives (TPs) are lost in an image. Furthermore, with a single stuck-at-1 fault, an entire sequence of images can be affected, causing temporally persistent ghost detections that can be mistaken for actual objects (covering up to approx. 83% of the image). Furthermore, actual objects in the scene are continuously missed (up to approx. 64% of TPs are lost). Our work establishes a detailed understanding of the safety-related vulnerability of such critical workloads against hardware faults.



### 3D Textured Shape Recovery with Learned Geometric Priors
- **Arxiv ID**: http://arxiv.org/abs/2209.03254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03254v1)
- **Published**: 2022-09-07 16:03:35+00:00
- **Updated**: 2022-09-07 16:03:35+00:00
- **Authors**: Lei Li, Zhizheng Liu, Weining Ren, Liudi Yang, Fangjinhua Wang, Marc Pollefeys, Songyou Peng
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: 3D textured shape recovery from partial scans is crucial for many real-world applications. Existing approaches have demonstrated the efficacy of implicit function representation, but they suffer from partial inputs with severe occlusions and varying object types, which greatly hinders their application value in the real world. This technical report presents our approach to address these limitations by incorporating learned geometric priors. To this end, we generate a SMPL model from learned pose prediction and fuse it into the partial input to add prior knowledge of human bodies. We also propose a novel completeness-aware bounding box adaptation for handling different levels of scales and partialness of partial scans.



### Measuring the Interpretability of Unsupervised Representations via Quantized Reverse Probing
- **Arxiv ID**: http://arxiv.org/abs/2209.03268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03268v1)
- **Published**: 2022-09-07 16:18:50+00:00
- **Updated**: 2022-09-07 16:18:50+00:00
- **Authors**: Iro Laina, Yuki M. Asano, Andrea Vedaldi
- **Comment**: Published at ICLR 2022. Appendix included, 26 pages
- **Journal**: None
- **Summary**: Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., "red apple") instead of just individual attributes ("red" and "apple" independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: {\scriptsize{\url{https://github.com/iro-cp/ssl-qrp}}}.



### Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.05251v2
- **DOI**: 10.1109/TGRS.2023.3293270
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.05251v2)
- **Published**: 2022-09-07 16:40:45+00:00
- **Updated**: 2023-07-06 09:16:57+00:00
- **Authors**: Lorenzo Bonicelli, Angelo Porrello, Stefano Vincenzi, Carla Ippoliti, Federica Iapaolo, Annamaria Conte, Simone Calderara
- **Comment**: 12 pages, 5 figures. Accepted at the IEEE Transactions On Geoscience
  And Remote Sensing
- **Journal**: None
- **Summary**: The occurrence of West Nile Virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth Observation (EO) data, coupled with the continuous advances in the field of Artificial Intelligence, offer valuable opportunities.   In this paper, we seek to predict WNV circulation by feeding Deep Neural Networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon Graph Neural Networks (GNN) to aggregate features from neighbouring places, and further extend these modules to consider multiple relations, such as the difference in temperature and soil moisture between two sites, as well as the geographical distance. Moreover, we inject time-related information directly into the model to take into account the seasonality of virus spread.   We design an experimental setting that combines satellite images - from Landsat and Sentinel missions - with ground truth observations of WNV circulation in Italy. We show that our proposed Multi-Adjacency Graph Attention Network (MAGAT) consistently leads to higher performance when paired with an appropriate pre-training stage. Finally, we assess the importance of each component of MAGAT in our ablation studies.



### Transfer Learning and Vision Transformer based State-of-Health prediction of Lithium-Ion Batteries
- **Arxiv ID**: http://arxiv.org/abs/2209.05253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.05253v1)
- **Published**: 2022-09-07 16:54:15+00:00
- **Updated**: 2022-09-07 16:54:15+00:00
- **Authors**: Pengyu Fu, Liang Chu, Zhuoran Hou, Jincheng Hu, Yanjun Huang, Yuanjian Zhang
- **Comment**: 13 pages,15 figures,13 equations
- **Journal**: None
- **Summary**: In recent years, significant progress has been made in transportation electrification. And lithium-ion batteries (LIB), as the main energy storage devices, have received widespread attention. Accurately predicting the state of health (SOH) can not only ease the anxiety of users about the battery life but also provide important information for the management of the battery. This paper presents a prediction method for SOH based on Vision Transformer (ViT) model. First, discrete charging data of a predefined voltage range is used as an input data matrix. Then, the cycle features of the battery are captured by the ViT which can obtain the global features, and the SOH is obtained by combining the cycle features with the full connection (FC) layer. At the same time, transfer learning (TL) is introduced, and the prediction model based on source task battery training is further fine-tuned according to the early cycle data of the target task battery to provide an accurate prediction. Experiments show that our method can obtain better feature expression compared with existing deep learning methods so that better prediction effect and transfer effect can be achieved.



### Spach Transformer: Spatial and Channel-wise Transformer Based on Local and Global Self-attentions for PET Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2209.03300v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03300v1)
- **Published**: 2022-09-07 16:59:14+00:00
- **Updated**: 2022-09-07 16:59:14+00:00
- **Authors**: Se-In Jang, Tinsu Pan, Ye Li, Pedram Heidari, Junyu Chen, Quanzheng Li, Kuang Gong
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Position emission tomography (PET) is widely used in clinics and research due to its quantitative merits and high sensitivity, but suffers from low signal-to-noise ratio (SNR). Recently convolutional neural networks (CNNs) have been widely used to improve PET image quality. Though successful and efficient in local feature extraction, CNN cannot capture long-range dependencies well due to its limited receptive field. Global multi-head self-attention (MSA) is a popular approach to capture long-range information. However, the calculation of global MSA for 3D images has high computational costs. In this work, we proposed an efficient spatial and channel-wise encoder-decoder transformer, Spach Transformer, that can leverage spatial and channel information based on local and global MSAs. Experiments based on datasets of different PET tracers, i.e., $^{18}$F-FDG, $^{18}$F-ACBC, $^{18}$F-DCFPyL, and $^{68}$Ga-DOTATATE, were conducted to evaluate the proposed framework. Quantitative results show that the proposed Spach Transformer can achieve better performance than other reference methods.



### Securing the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2209.03358v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.03358v2)
- **Published**: 2022-09-07 17:05:48+00:00
- **Updated**: 2022-12-12 19:35:01+00:00
- **Authors**: Nuo Xu, Kaleel Mahmood, Haowen Fang, Ethan Rathbun, Caiwen Ding, Wujie Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remain relatively underdeveloped. In this work we focus on advancing the adversarial attack side of SNNs and make three major contributions. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique. Second, using the best surrogate gradient technique, we analyze the transferability of adversarial attacks on SNNs and other state-of-the-art architectures like Vision Transformers (ViTs) and Big Transfer Convolutional Neural Networks (CNNs). We demonstrate that SNNs are not often deceived by adversarial examples generated by Vision Transformers and certain types of CNNs. Third, due to the lack of an ubiquitous white-box attack that is effective across both the SNN and CNN/ViT domains, we develop a new white-box attack, the Auto Self-Attention Gradient Attack (Auto SAGA). Our novel attack generates adversarial examples capable of fooling both SNN models and non-SNN models simultaneously. Auto SAGA is as much as $87.9\%$ more effective on SNN/ViT model ensembles than conventional white-box attacks like PGD. Our experiments and analyses are broad and rigorous covering three datasets (CIFAR-10, CIFAR-100 and ImageNet), five different white-box attacks and nineteen different classifier models (seven for each CIFAR dataset and five different models for ImageNet).



### What does a platypus look like? Generating customized prompts for zero-shot image classification
- **Arxiv ID**: http://arxiv.org/abs/2209.03320v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.03320v2)
- **Published**: 2022-09-07 17:27:08+00:00
- **Updated**: 2023-07-31 14:39:12+00:00
- **Authors**: Sarah Pratt, Ian Covert, Rosanne Liu, Ali Farhadi
- **Comment**: Accepted at ICCV 2023
- **Journal**: None
- **Summary**: Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a {}") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on these regions in the image when making predictions. We find that this straightforward and general approach improves accuracy on a range of zero-shot image classification benchmarks, including over one percentage point gain on ImageNet. Finally, this simple baseline requires no additional training and remains completely zero-shot. Code available at https://github.com/sarahpratt/CuPL.



### Joint Learning of Deep Texture and High-Frequency Features for Computer-Generated Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.03322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03322v1)
- **Published**: 2022-09-07 17:30:40+00:00
- **Updated**: 2022-09-07 17:30:40+00:00
- **Authors**: Qiang Xu, Shan Jia, Xinghao Jiang, Tanfeng Sun, Zhe Wang, Hong Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Distinguishing between computer-generated (CG) and natural photographic (PG) images is of great importance to verify the authenticity and originality of digital images. However, the recent cutting-edge generation methods enable high qualities of synthesis in CG images, which makes this challenging task even trickier. To address this issue, a joint learning strategy with deep texture and high-frequency features for CG image detection is proposed. We first formulate and deeply analyze the different acquisition processes of CG and PG images. Based on the finding that multiple different modules in image acquisition will lead to different sensitivity inconsistencies to the convolutional neural network (CNN)-based rendering in images, we propose a deep texture rendering module for texture difference enhancement and discriminative texture representation. Specifically, the semantic segmentation map is generated to guide the affine transformation operation, which is used to recover the texture in different regions of the input image. Then, the combination of the original image and the high-frequency components of the original and rendered images are fed into a multi-branch neural network equipped with attention mechanisms, which refines intermediate features and facilitates trace exploration in spatial and channel dimensions respectively. Extensive experiments on two public datasets and a newly constructed dataset with more realistic and diverse images show that the proposed approach outperforms existing methods in the field by a clear margin. Besides, results also demonstrate the detection robustness and generalization ability of the proposed approach to postprocessing operations and generative adversarial network (GAN) generated images.



### Detection and Mapping of Specular Surfaces Using Multibounce Lidar Returns
- **Arxiv ID**: http://arxiv.org/abs/2209.03336v1
- **DOI**: 10.1364/OE.479900
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03336v1)
- **Published**: 2022-09-07 17:49:59+00:00
- **Updated**: 2022-09-07 17:49:59+00:00
- **Authors**: Connor Henley, Siddharth Somasundaram, Joseph Hollmann, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose methods that use specular, multibounce lidar returns to detect and map specular surfaces that might be invisible to conventional lidar systems that rely on direct, single-scatter returns. We derive expressions that relate the time- and angle-of-arrival of these multibounce returns to scattering points on the specular surface, and then use these expressions to formulate techniques for retrieving specular surface geometry when the scene is scanned by a single beam or illuminated with a multi-beam flash. We also consider the special case of transparent specular surfaces, for which surface reflections can be mixed together with light that scatters off of objects lying behind the surface.



### Using Computational Approaches in Visual Identity Design: A Visual Identity for the Design and Multimedia Courses of Faculty of Sciences and Technology of University of Coimbra
- **Arxiv ID**: http://arxiv.org/abs/2209.03420v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.03420v1)
- **Published**: 2022-09-07 18:44:55+00:00
- **Updated**: 2022-09-07 18:44:55+00:00
- **Authors**: Srgio M. Rebelo, Tiago Martins, Artur Rebelo, Joo Bicker, Penousal Machado
- **Comment**: Paper presented in 10th Typography Meeting "Borders", 22--23 Oct.
  2019, Matosinhos, Portugal
- **Journal**: 10th Typography Meeting "Borders", 22--23 Oct. 2019, Matosinhos,
  Portugal
- **Summary**: Computational approaches are beginning to be used to design dynamic visual identities fuelled by data and generative processes. In this work, we explore these computational approaches in order to generate a visual identity that creates bespoke letterings and images. We achieve this developing a generative design system that automatically assembles black and white visual modules. This system generates designs performing two main methods: (i) Assisted generation; and (ii) Automatic generation. Assisted generation method produces outputs wherein the placement of modules is determined by a configuration file previous defined. On the other hand, the Automatic generation method produces outputs wherein the modules are assembled to depict an input image. This system speeds up the process of design and deployment of one visual identity design as well as it generates outputs visual coherent among them. In this paper, we compressively describe this system and its achievements.



### Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions
- **Arxiv ID**: http://arxiv.org/abs/2209.03430v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.03430v2)
- **Published**: 2022-09-07 19:21:19+00:00
- **Updated**: 2023-02-20 09:19:30+00:00
- **Authors**: Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.



### Deep Learning-Based Automatic Diagnosis System for Developmental Dysplasia of the Hip
- **Arxiv ID**: http://arxiv.org/abs/2209.03440v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.03440v1)
- **Published**: 2022-09-07 19:50:30+00:00
- **Updated**: 2022-09-07 19:50:30+00:00
- **Authors**: Yang Li, Leo Yan Li-Han, Hua Tian
- **Comment**: None
- **Journal**: None
- **Summary**: As the first-line diagnostic imaging modality, radiography plays an essential role in the early detection of developmental dysplasia of the hip (DDH). Clinically, the diagnosis of DDH relies on manual measurements and subjective evaluation of different anatomical features from pelvic radiographs. This process is inefficient and error-prone and requires years of clinical experience. In this study, we propose a deep learning-based system that automatically detects 14 keypoints from a radiograph, measures three anatomical angles (center-edge, T\"onnis, and Sharp angles), and classifies DDH hips as grades I-IV based on the Crowe criteria. Moreover, a novel data-driven scoring system is proposed to quantitatively integrate the information from the three angles for DDH diagnosis. The proposed keypoint detection model achieved a mean (95% confidence interval [CI]) average precision of 0.807 (0.804-0.810). The mean (95% CI) intraclass correlation coefficients between the center-edge, Tonnis, and Sharp angles measured by the proposed model and the ground-truth were 0.957 (0.952-0.962), 0.947 (0.941-0.953), and 0.953 (0.947-0.960), respectively, which were significantly higher than those of experienced orthopedic surgeons (p<0.0001). In addition, the mean (95% CI) test diagnostic agreement (Cohen's kappa) obtained using the proposed scoring system was 0.84 (0.83-0.85), which was significantly higher than those obtained from diagnostic criteria for individual angle (0.76 [0.75-0.77]) and orthopedists (0.71 [0.63-0.79]). To the best of our knowledge, this is the first study for objective DDH diagnosis by leveraging deep learning keypoint detection and integrating different anatomical measurements, which can provide reliable and explainable support for clinical decision-making.



### Information Maximization for Extreme Pose Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.03456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.03456v1)
- **Published**: 2022-09-07 20:30:06+00:00
- **Updated**: 2022-09-07 20:30:06+00:00
- **Authors**: Mohammad Saeed Ebrahimi Saadabadi, Sahar Rahimi Malakshan, Sobhan Soleymani, Moktari Mostofa, Nasser M. Nasrabadi
- **Comment**: INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2022)
- **Journal**: None
- **Summary**: In this paper, we seek to draw connections between the frontal and profile face images in an abstract embedding space. We exploit this connection using a coupled-encoder network to project frontal/profile face images into a common latent embedding space. The proposed model forces the similarity of representations in the embedding space by maximizing the mutual information between two views of the face. The proposed coupled-encoder benefits from three contributions for matching faces with extreme pose disparities. First, we leverage our pose-aware contrastive learning to maximize the mutual information between frontal and profile representations of identities. Second, a memory buffer, which consists of latent representations accumulated over past iterations, is integrated into the model so it can refer to relatively much more instances than the mini-batch size. Third, a novel pose-aware adversarial domain adaptation method forces the model to learn an asymmetric mapping from profile to frontal representation. In our framework, the coupled-encoder learns to enlarge the margin between the distribution of genuine and imposter faces, which results in high mutual information between different views of the same identity. The effectiveness of the proposed model is investigated through extensive experiments, evaluations, and ablation studies on four benchmark datasets, and comparison with the compelling state-of-the-art algorithms.



### Supervised GAN Watermarking for Intellectual Property Protection
- **Arxiv ID**: http://arxiv.org/abs/2209.03466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.03466v1)
- **Published**: 2022-09-07 20:52:05+00:00
- **Updated**: 2022-09-07 20:52:05+00:00
- **Authors**: Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a watermarking method for protecting the Intellectual Property (IP) of Generative Adversarial Networks (GANs). The aim is to watermark the GAN model so that any image generated by the GAN contains an invisible watermark (signature), whose presence inside the image can be checked at a later stage for ownership verification. To achieve this goal, a pre-trained CNN watermarking decoding block is inserted at the output of the generator. The generator loss is then modified by including a watermark loss term, to ensure that the prescribed watermark can be extracted from the generated images. The watermark is embedded via fine-tuning, with reduced time complexity. Results show that our method can effectively embed an invisible watermark inside the generated images. Moreover, our method is a general one and can work with different GAN architectures, different tasks, and different resolutions of the output image. We also demonstrate the good robustness performance of the embedded watermark against several post-processing, among them, JPEG compression, noise addition, blurring, and color transformations.



### Multi-NeuS: 3D Head Portraits from Single Image with Neural Implicit Functions
- **Arxiv ID**: http://arxiv.org/abs/2209.04436v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2209.04436v1)
- **Published**: 2022-09-07 21:09:24+00:00
- **Updated**: 2022-09-07 21:09:24+00:00
- **Authors**: Egor Burkov, Ruslan Rakhimov, Aleksandr Safin, Evgeny Burnaev, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for the reconstruction of textured 3D meshes of human heads from one or few views. Since such few-shot reconstruction is underconstrained, it requires prior knowledge which is hard to impose on traditional 3D reconstruction algorithms. In this work, we rely on the recently introduced 3D representation $\unicode{x2013}$ neural implicit functions $\unicode{x2013}$ which, being based on neural networks, allows to naturally learn priors about human heads from data, and is directly convertible to textured mesh. Namely, we extend NeuS, a state-of-the-art neural implicit function formulation, to represent multiple objects of a class (human heads in our case) simultaneously. The underlying neural net architecture is designed to learn the commonalities among these objects and to generalize to unseen ones. Our model is trained on just a hundred smartphone videos and does not require any scanned 3D data. Afterwards, the model can fit novel heads in the few-shot or one-shot modes with good results.



### Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations
- **Arxiv ID**: http://arxiv.org/abs/2209.03494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.03494v1)
- **Published**: 2022-09-07 23:24:09+00:00
- **Updated**: 2022-09-07 23:24:09+00:00
- **Authors**: Vadim Tschernezki, Iro Laina, Diane Larlus, Andrea Vedaldi
- **Comment**: 3DV2022, Oral. Project page: https://www.robots.ox.ac.uk/~vadim/n3f/
- **Journal**: None
- **Summary**: We present Neural Feature Fusion Fields (N3F), a method that improves dense 2D image feature extractors when the latter are applied to the analysis of multiple images reconstructible as a 3D scene. Given an image feature extractor, for example pre-trained using self-supervision, N3F uses it as a teacher to learn a student network defined in 3D space. The 3D student network is similar to a neural radiance field that distills said features and can be trained with the usual differentiable rendering machinery. As a consequence, N3F is readily applicable to most neural rendering formulations, including vanilla NeRF and its extensions to complex dynamic scenes. We show that our method not only enables semantic understanding in the context of scene-specific neural fields without the use of manual labels, but also consistently improves over the self-supervised 2D baselines. This is demonstrated by considering various tasks, such as 2D object retrieval, 3D segmentation, and scene editing, in diverse sequences, including long egocentric videos in the EPIC-KITCHENS benchmark.




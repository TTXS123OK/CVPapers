# Arxiv Papers in cs.CV on 2022-01-15
### Parameter-free Online Test-time Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2201.05718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05718v2)
- **Published**: 2022-01-15 00:29:16+00:00
- **Updated**: 2022-04-04 14:20:22+00:00
- **Authors**: Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, Luca Bertinetto
- **Comment**: CVPR 2022 (oral). Code available at https://github.com/fiveai/LAME
- **Journal**: None
- **Summary**: Training state-of-the-art vision models has become prohibitively expensive for researchers and practitioners. For the sake of accessibility and resource reuse, it is important to focus on adapting these models to a variety of downstream scenarios. An interesting and practical paradigm is online test-time adaptation, according to which training data is inaccessible, no labelled data from the test distribution is available, and adaptation can only happen at test time and on a handful of samples. In this paper, we investigate how test-time adaptation methods fare for a number of pre-trained models on a variety of real-world scenarios, significantly extending the way they have been originally evaluated. We show that they perform well only in narrowly-defined experimental setups and sometimes fail catastrophically when their hyperparameters are not selected for the same scenario in which they are being tested. Motivated by the inherent uncertainty around the conditions that will ultimately be encountered at test time, we propose a particularly "conservative" approach, which addresses the problem with a Laplacian Adjusted Maximum-likelihood Estimation (LAME) objective. By adapting the model's output (not its parameters), and solving our objective with an efficient concave-convex procedure, our approach exhibits a much higher average accuracy across scenarios than existing methods, while being notably faster and have a much lower memory footprint. The code is available at https://github.com/fiveai/LAME.



### Learning Temporally and Semantically Consistent Unpaired Video-to-video Translation Through Pseudo-Supervision From Synthetic Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2201.05723v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05723v3)
- **Published**: 2022-01-15 01:10:34+00:00
- **Updated**: 2022-12-21 04:47:18+00:00
- **Authors**: Kaihong Wang, Kumar Akash, Teruhisa Misu
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired video-to-video translation aims to translate videos between a source and a target domain without the need of paired training data, making it more feasible for real applications. Unfortunately, the translated videos generally suffer from temporal and semantic inconsistency. To address this, many existing works adopt spatiotemporal consistency constraints incorporating temporal information based on motion estimation. However, the inaccuracies in the estimation of motion deteriorate the quality of the guidance towards spatiotemporal consistency, which leads to unstable translation. In this work, we propose a novel paradigm that regularizes the spatiotemporal consistency by synthesizing motions in input videos with the generated optical flow instead of estimating them. Therefore, the synthetic motion can be applied in the regularization paradigm to keep motions consistent across domains without the risk of errors in motion estimation. Thereafter, we utilize our unsupervised recycle and unsupervised spatial loss, guided by the pseudo-supervision provided by the synthetic optical flow, to accurately enforce spatiotemporal consistency in both domains. Experiments show that our method is versatile in various scenarios and achieves state-of-the-art performance in generating temporally and semantically consistent videos. Code is available at: https://github.com/wangkaihong/Unsup_Recycle_GAN/.



### CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2201.05729v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.05729v3)
- **Published**: 2022-01-15 01:54:01+00:00
- **Updated**: 2022-12-28 20:07:58+00:00
- **Authors**: Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Jianwei Yang, Xiyang Dai, Bin Xiao, Haoxuan You, Shih-Fu Chang, Lu Yuan
- **Comment**: This paper is greatly modified and updated to be re-submitted to
  another conference. The new paper is under the name "Multimodal Adaptive
  Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks",
  https://doi.org/10.48550/arXiv.2204.10496
- **Journal**: None
- **Summary**: Contrastive language-image pretraining (CLIP) links vision and language modalities into a unified embedding space, yielding the tremendous potential for vision-language (VL) tasks. While early concurrent works have begun to study this potential on a subset of tasks, important questions remain: 1) What is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches without impacting inference or pretraining complexity? In this work, we seek to answer these questions through two key contributions. First, we introduce an evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of data availability constraints and conditions of domain shift. Second, we propose an approach, named CLIP Targeted Distillation (CLIP-TD), to intelligently distill knowledge from CLIP into existing architectures using a dynamically weighted objective applied to adaptively selected tokens per instance. Experiments demonstrate that our proposed CLIP-TD leads to exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to 71.3%) conditions of VCR, while simultaneously improving performance under standard fully-supervised conditions (up to 2%), achieving state-of-art performance on VCR compared to other single models that are pretrained with image-text data only. On SNLI-VE, CLIP-TD produces significant gains in low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works utilizing CLIP for finetuning, as well as baseline naive distillation approaches. Code will be made available.



### Learning Hierarchical Graph Representation for Image Manipulation Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.05730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05730v1)
- **Published**: 2022-01-15 01:54:25+00:00
- **Updated**: 2022-01-15 01:54:25+00:00
- **Authors**: Wenyan Pan, Zhili Zhou, Miaogen Ling, Xin Geng, Q. M. Jonathan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of image manipulation detection is to identify and locate the manipulated regions in the images. Recent approaches mostly adopt the sophisticated Convolutional Neural Networks (CNNs) to capture the tampering artifacts left in the images to locate the manipulated regions. However, these approaches ignore the feature correlations, i.e., feature inconsistencies, between manipulated regions and non-manipulated regions, leading to inferior detection performance. To address this issue, we propose a hierarchical Graph Convolutional Network (HGCN-Net), which consists of two parallel branches: the backbone network branch and the hierarchical graph representation learning (HGRL) branch for image manipulation detection. Specifically, the feature maps of a given image are extracted by the backbone network branch, and then the feature correlations within the feature maps are modeled as a set of fully-connected graphs for learning the hierarchical graph representation by the HGRL branch. The learned hierarchical graph representation can sufficiently capture the feature correlations across different scales, and thus it provides high discriminability for distinguishing manipulated and non-manipulated regions. Extensive experiments on four public datasets demonstrate that the proposed HGCN-Net not only provides promising detection accuracy, but also achieves strong robustness under a variety of common image attacks in the task of image manipulation detection, compared to the state-of-the-arts.



### Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2201.05739v1
- **DOI**: 10.1145/3453142.3491293
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05739v1)
- **Published**: 2022-01-15 02:29:36+00:00
- **Updated**: 2022-01-15 02:29:36+00:00
- **Authors**: Justin Sanchez, Christopher Neff, Hamed Tabkhi
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition is a key algorithmic part of emerging on-the-edge smart video surveillance and security systems. Skeleton-based action recognition is an attractive approach which, instead of using RGB pixel data, relies on human pose information to classify appropriate actions. However, existing algorithms often assume ideal conditions that are not representative of real-world limitations, such as noisy input, latency requirements, and edge resource constraints.   To address the limitations of existing approaches, this paper presents Real-World Graph Convolution Networks (RW-GCNs), an architecture-level solution for meeting the domain constraints of Real World Skeleton-based Action Recognition. Inspired by the presence of feedback connections in the human visual cortex, RW-GCNs leverage attentive feedback augmentation on existing near state-of-the-art (SotA) Spatial-Temporal Graph Convolution Networks (ST-GCNs). The ST-GCNs' design choices are derived from information theory-centric principles to address both the spatial and temporal noise typically encountered in end-to-end real-time and on-the-edge smart video systems. Our results demonstrate RW-GCNs' ability to serve these applications by achieving a new SotA accuracy on the NTU-RGB-D-120 dataset at 94.1%, and achieving 32X less latency than baseline ST-GCN applications while still achieving 90.4% accuracy on the Northwestern UCLA dataset in the presence of spatial keypoint noise. RW-GCNs further show system scalability by running on the 10X cost effective NVIDIA Jetson Nano (as opposed to NVIDIA Xavier NX), while still maintaining a respectful range of throughput (15.6 to 5.5 Actions per Second) on the resource constrained device. The code is available here: https://github.com/TeCSAR-UNCC/RW-GCN.



### A Survey on RGB-D Datasets
- **Arxiv ID**: http://arxiv.org/abs/2201.05761v2
- **DOI**: 10.1016/j.cviu.2022.103489
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05761v2)
- **Published**: 2022-01-15 05:35:19+00:00
- **Updated**: 2022-08-08 18:57:24+00:00
- **Authors**: Alexandre Lopes, Roberto Souza, Helio Pedrini
- **Comment**: This paper was published at Computer Vision and Image Understanding.
  Access the final paper using the DOI:
  https://doi.org/10.1016/j.cviu.2022.103489
- **Journal**: Computer Vision and Image Understanding 222 (2022) 103489
- **Summary**: RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 203 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field.



### DeepMix: Mobility-aware, Lightweight, and Hybrid 3D Object Detection for Headsets
- **Arxiv ID**: http://arxiv.org/abs/2201.08812v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08812v2)
- **Published**: 2022-01-15 05:50:18+00:00
- **Updated**: 2022-03-16 03:15:09+00:00
- **Authors**: Yongjie Guan, Xueyu Hou, Nan Wu, Bo Han, Tao Han
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile headsets should be capable of understanding 3D physical environments to offer a truly immersive experience for augmented/mixed reality (AR/MR). However, their small form-factor and limited computation resources make it extremely challenging to execute in real-time 3D vision algorithms, which are known to be more compute-intensive than their 2D counterparts. In this paper, we propose DeepMix, a mobility-aware, lightweight, and hybrid 3D object detection framework for improving the user experience of AR/MR on mobile headsets. Motivated by our analysis and evaluation of state-of-the-art 3D object detection models, DeepMix intelligently combines edge-assisted 2D object detection and novel, on-device 3D bounding box estimations that leverage depth data captured by headsets. This leads to low end-to-end latency and significantly boosts detection accuracy in mobile scenarios. A unique feature of DeepMix is that it fully exploits the mobility of headsets to fine-tune detection results and boost detection accuracy. To the best of our knowledge, DeepMix is the first 3D object detection that achieves 30 FPS (an end-to-end latency much lower than the 100 ms stringent requirement of interactive AR/MR). We implement a prototype of DeepMix on Microsoft HoloLens and evaluate its performance via both extensive controlled experiments and a user study with 30+ participants. DeepMix not only improves detection accuracy by 9.1--37.3% but also reduces end-to-end latency by 2.68--9.15x, compared to the baseline that uses existing 3D object detection models.



### Spectral Compressive Imaging Reconstruction Using Convolution and Contextual Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.05768v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.05768v4)
- **Published**: 2022-01-15 06:30:03+00:00
- **Updated**: 2023-07-02 06:44:05+00:00
- **Authors**: Lishun Wang, Zongliang Wu, Yong Zhong, Xin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral compressive imaging (SCI) is able to encode the high-dimensional hyperspectral image to a 2D measurement, and then uses algorithms to reconstruct the spatio-spectral data-cube. At present, the main bottleneck of SCI is the reconstruction algorithm, and the state-of-the-art (SOTA) reconstruction methods generally face the problem of long reconstruction time and/or poor detail recovery. In this paper, we propose a novel hybrid network module, namely CCoT (Convolution and Contextual Transformer) block, which can acquire the inductive bias ability of convolution and the powerful modeling ability of transformer simultaneously,and is conducive to improving the quality of reconstruction to restore fine details. We integrate the proposed CCoT block into deep unfolding framework based on the generalized alternating projection algorithm, and further propose the GAP-CCoT network. Through the experiments of extensive synthetic and real data, our proposed model achieves higher reconstruction quality ($>$2dB in PSNR on simulated benchmark datasets) and shorter running time than existing SOTA algorithms by a large margin. The code and models are publicly available at https://github.com/ucaswangls/GAP-CCoT.



### Asymmetric Hash Code Learning for Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.05772v1
- **DOI**: 10.1109/TGRS.2022.3143571
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2201.05772v1)
- **Published**: 2022-01-15 07:00:38+00:00
- **Updated**: 2022-01-15 07:00:38+00:00
- **Authors**: Weiwei Song, Zhi Gao, Renwei Dian, Pedram Ghamisi, Yongjun Zhang, JÃ³n Atli Benediktsson
- **Comment**: 14 pages, 12 figures, and 2 tables
- **Journal**: None
- **Summary**: Remote sensing image retrieval (RSIR), aiming at searching for a set of similar items to a given query image, is a very important task in remote sensing applications. Deep hashing learning as the current mainstream method has achieved satisfactory retrieval performance. On one hand, various deep neural networks are used to extract semantic features of remote sensing images. On the other hand, the hashing techniques are subsequently adopted to map the high-dimensional deep features to the low-dimensional binary codes. This kind of methods attempts to learn one hash function for both the query and database samples in a symmetric way. However, with the number of database samples increasing, it is typically time-consuming to generate the hash codes of large-scale database images. In this paper, we propose a novel deep hashing method, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL generates the hash codes of query and database images in an asymmetric way. In more detail, the hash codes of query images are obtained by binarizing the output of the network, while the hash codes of database images are directly learned by solving the designed objective function. In addition, we combine the semantic information of each image and the similarity information of pairs of images as supervised information to train a deep hashing network, which improves the representation ability of deep features and hash codes. The experimental results on three public datasets demonstrate that the proposed method outperforms symmetric methods in terms of retrieval accuracy and efficiency. The source code is available at https://github.com/weiweisong415/Demo AHCL for TGRS2022.



### Explainability Tools Enabling Deep Learning in Future In-Situ Real-Time Planetary Explorations
- **Arxiv ID**: http://arxiv.org/abs/2201.05775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.05775v1)
- **Published**: 2022-01-15 07:10:00+00:00
- **Updated**: 2022-01-15 07:10:00+00:00
- **Authors**: Daniel Lundstrom, Alexander Huyen, Arya Mevada, Kyongsik Yun, Thomas Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) has proven to be an effective machine learning and computer vision technique. DL-based image segmentation, object recognition and classification will aid many in-situ Mars rover tasks such as path planning and artifact recognition/extraction. However, most of the Deep Neural Network (DNN) architectures are so complex that they are considered a 'black box'. In this paper, we used integrated gradients to describe the attributions of each neuron to the output classes. It provides a set of explainability tools (ET) that opens the black box of a DNN so that the individual contribution of neurons to category classification can be ranked and visualized. The neurons in each dense layer are mapped and ranked by measuring expected contribution of a neuron to a class vote given a true image label. The importance of neurons is prioritized according to their correct or incorrect contribution to the output classes and suppression or bolstering of incorrect classes, weighted by the size of each class. ET provides an interface to prune the network to enhance high-rank neurons and remove low-performing neurons. ET technology will make DNNs smaller and more efficient for implementation in small embedded systems. It also leads to more explainable and testable DNNs that can make systems easier for Validation \& Verification. The goal of ET technology is to enable the adoption of DL in future in-situ planetary exploration missions.



### Uncertainty-Aware Multi-View Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.05776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05776v1)
- **Published**: 2022-01-15 07:16:20+00:00
- **Updated**: 2022-01-15 07:16:20+00:00
- **Authors**: Yu Geng, Zongbo Han, Changqing Zhang, Qinghua Hu
- **Comment**: AAAI 2021 published paper
- **Journal**: None
- **Summary**: Learning from different data views by exploring the underlying complementary information among them can endow the representation with stronger expressive ability. However, high-dimensional features tend to contain noise, and furthermore, the quality of data usually varies for different samples (even for different views), i.e., one view may be informative for one sample but not the case for another. Therefore, it is quite challenging to integrate multi-view noisy data under unsupervised setting. Traditional multi-view methods either simply treat each view with equal importance or tune the weights of different views to fixed values, which are insufficient to capture the dynamic noise in multi-view data. In this work, we devise a novel unsupervised multi-view learning approach, termed as Dynamic Uncertainty-Aware Networks (DUA-Nets). Guided by the uncertainty of data estimated from the generation perspective, intrinsic information from multiple views is integrated to obtain noise-free representations. Under the help of uncertainty, DUA-Nets weigh each view of individual sample according to data quality so that the high-quality samples (or views) can be fully exploited while the effects from the noisy samples (or views) will be alleviated. Our model achieves superior performance in extensive experiments and shows the robustness to noisy data.



### Semantic decoupled representation learning for remote sensing image change detection
- **Arxiv ID**: http://arxiv.org/abs/2201.05778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05778v1)
- **Published**: 2022-01-15 07:35:26+00:00
- **Updated**: 2022-01-15 07:35:26+00:00
- **Authors**: Hao Chen, Yifan Zao, Liqin Liu, Song Chen, Zhenwei Shi
- **Comment**: Submitted to IEEE for possible publication. 4 pages, 2 figures
- **Journal**: None
- **Summary**: Contemporary transfer learning-based methods to alleviate the data insufficiency in change detection (CD) are mainly based on ImageNet pre-training. Self-supervised learning (SSL) has recently been introduced to remote sensing (RS) for learning in-domain representations. Here, we propose a semantic decoupled representation learning for RS image CD. Typically, the object of interest (e.g., building) is relatively small compared to the vast background. Different from existing methods expressing an image into one representation vector that may be dominated by irrelevant land-covers, we disentangle representations of different semantic regions by leveraging the semantic mask. We additionally force the model to distinguish different semantic representations, which benefits the recognition of objects of interest in the downstream CD task. We construct a dataset of bitemporal images with semantic masks in an effortless manner for pre-training. Experiments on two CD datasets show our model outperforms ImageNet pre-training, in-domain supervised pre-training, and several recent SSL methods.



### OneDConv: Generalized Convolution For Transform-Invariant Representation
- **Arxiv ID**: http://arxiv.org/abs/2201.05781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05781v1)
- **Published**: 2022-01-15 07:44:44+00:00
- **Updated**: 2022-01-15 07:44:44+00:00
- **Authors**: Tong Zhang, Haohan Weng, Ke Yi, C. L. Philip Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have exhibited their great power in a variety of vision tasks. However, the lack of transform-invariant property limits their further applications in complicated real-world scenarios. In this work, we proposed a novel generalized one dimension convolutional operator (OneDConv), which dynamically transforms the convolution kernels based on the input features in a computationally and parametrically efficient manner. The proposed operator can extract the transform-invariant features naturally. It improves the robustness and generalization of convolution without sacrificing the performance on common images. The proposed OneDConv operator can substitute the vanilla convolution, thus it can be incorporated into current popular convolutional architectures and trained end-to-end readily. On several popular benchmarks, OneDConv outperforms the original convolution operation and other proposed models both in canonical and distorted images.



### Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.05809v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2201.05809v2)
- **Published**: 2022-01-15 09:34:50+00:00
- **Updated**: 2022-01-21 17:35:42+00:00
- **Authors**: Qiushi Shi, Ponnuthurai Nagaratnam Suganthan, Rakesh Katuwal
- **Comment**: 8 tables, 8 figures, 31 pages
- **Journal**: None
- **Summary**: In this paper, we first introduce batch normalization to the edRVFL network. This re-normalization method can help the network avoid divergence of the hidden features. Then we propose novel variants of Ensemble Deep Random Vector Functional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to give training samples different weights in different layers according to how the samples were classified confidently in the previous layer thereby increasing the ensemble's diversity and accuracy. Furthermore, a pruning-based edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based on their importance for classification before generating the next hidden layer. Through this method, we ensure that the randomly generated inferior features will not propagate to deeper layers. Subsequently, the combination of weighting and pruning, called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL), is proposed. We compare their performances with other state-of-the-art deep feedforward neural networks (FNNs) on 24 tabular UCI classification datasets. The experimental results illustrate the superior performance of our proposed methods.



### Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2201.05810v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.05810v2)
- **Published**: 2022-01-15 09:40:22+00:00
- **Updated**: 2022-01-21 04:30:37+00:00
- **Authors**: Siming Zheng, Xiaoyu Yang, Xin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the reconstruction problem of video compressive sensing (VCS) under the deep unfolding/rolling structure. Yet, we aim to build a flexible and concise model using minimum stages. Different from existing deep unfolding networks used for inverse problems, where more stages are used for higher performance but without flexibility to different masks and scales, hereby we show that a 2-stage deep unfolding network can lead to the state-of-the-art (SOTA) results (with a 1.7dB gain in PSNR over the single stage model, RevSCI) in VCS. The proposed method possesses the properties of adaptation to new masks and ready to scale to large data without any additional training thanks to the advantages of deep unfolding. Furthermore, we extend the proposed model for color VCS to perform joint reconstruction and demosaicing. Experimental results demonstrate that our 2-stage model has also achieved SOTA on color VCS reconstruction, leading to a >2.3dB gain in PSNR over the previous SOTA algorithm based on plug-and-play framework, meanwhile speeds up the reconstruction by >17 times. In addition, we have found that our network is also flexible to the mask modulation and scale size for color VCS reconstruction so that a single trained network can be applied to different hardware systems. The code and models will be released to the public.



### A Critical Analysis of Image-based Camera Pose Estimation Techniques
- **Arxiv ID**: http://arxiv.org/abs/2201.05816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.05816v1)
- **Published**: 2022-01-15 09:57:45+00:00
- **Updated**: 2022-01-15 09:57:45+00:00
- **Authors**: Meng Xu, Youchen Wang, Bin Xu, Jun Zhang, Jian Ren, Stefan Poslad, Pengfei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Camera, and associated with its objects within the field of view, localization could benefit many computer vision fields, such as autonomous driving, robot navigation, and augmented reality (AR). In this survey, we first introduce specific application areas and the evaluation metrics for camera localization pose according to different sub-tasks (learning-based 2D-2D task, feature-based 2D-3D task, and 3D-3D task). Then, we review common methods for structure-based camera pose estimation approaches, absolute pose regression and relative pose regression approaches by critically modelling the methods to inspire further improvements in their algorithms such as loss functions, neural network structures. Furthermore, we summarise what are the popular datasets used for camera localization and compare the quantitative and qualitative results of these methods with detailed performance metrics. Finally, we discuss future research possibilities and applications.



### Offline-Online Associated Camera-Aware Proxies for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2201.05820v2
- **DOI**: 10.1109/TIP.2022.3213193
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05820v2)
- **Published**: 2022-01-15 10:12:03+00:00
- **Updated**: 2022-10-01 15:05:17+00:00
- **Authors**: Menglin Wang, Jiachen Li, Baisheng Lai, Xiaojin Gong, Xian-Sheng Hua
- **Comment**: Accepted to TIP
- **Journal**: None
- **Summary**: Recently, unsupervised person re-identification (Re-ID) has received increasing research attention due to its potential for label-free applications. A promising way to address unsupervised Re-ID is clustering-based, which generates pseudo labels by clustering and uses the pseudo labels to train a Re-ID model iteratively. However, most clustering-based methods take each cluster as a pseudo identity class, neglecting the intra-cluster variance mainly caused by the change of cameras. To address this issue, we propose to split each single cluster into multiple proxies according to camera views. The camera-aware proxies explicitly capture local structures within clusters, by which the intra-ID variance and inter-ID similarity can be better tackled. Assisted with the camera-aware proxies, we design two proxy-level contrastive learning losses that are, respectively, based on offline and online association results. The offline association directly associates proxies according to the clustering and splitting results, while the online strategy dynamically associates proxies in terms of up-to-date features to reduce the noise caused by the delayed update of pseudo labels. The combination of two losses enables us to train a desirable Re-ID model. Extensive experiments on three person Re-ID datasets and one vehicle Re-ID dataset show that our proposed approach demonstrates competitive performance with state-of-the-art methods. Code will be available at: https://github.com/Terminator8758/O2CAP.



### Multi-View representation learning in Multi-Task Scene
- **Arxiv ID**: http://arxiv.org/abs/2201.05829v1
- **DOI**: 10.1007/s00521-019-04577-z
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05829v1)
- **Published**: 2022-01-15 11:26:28+00:00
- **Updated**: 2022-01-15 11:26:28+00:00
- **Authors**: Run-kun Lu, Jian-wei Liu, Si-ming Lian, Xin Zuo
- **Comment**: 32 pages
- **Journal**: Neural Computing and Applications(2020)
- **Summary**: Over recent decades have witnessed considerable progress in whether multi-task learning or multi-view learning, but the situation that consider both learning scenes simultaneously has received not too much attention. How to utilize multiple views latent representation of each single task to improve each learning task performance is a challenge problem. Based on this, we proposed a novel semi-supervised algorithm, termed as Multi-Task Multi-View learning based on Common and Special Features (MTMVCSF). In general, multi-views are the different aspects of an object and every view includes the underlying common or special information of this object. As a consequence, we will mine multiple views jointly latent factor of each learning task which consists of each view special feature and the common feature of all views. By this way, the original multi-task multi-view data has degenerated into multi-task data, and exploring the correlations among multiple tasks enables to make an improvement on the performance of learning algorithm. Another obvious advantage of this approach is that we get latent representation of the set of unlabeled instances by the constraint of regression task with labeled instances. The performance of classification and semi-supervised clustering task in these latent representations perform obviously better than it in raw data. Furthermore, an anti-noise multi-task multi-view algorithm called AN-MTMVCSF is proposed, which has a strong adaptability to noise labels. The effectiveness of these algorithms is proved by a series of well-designed experiments on both real world and synthetic data.



### Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.05834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.05834v1)
- **Published**: 2022-01-15 12:02:28+00:00
- **Updated**: 2022-01-15 12:02:28+00:00
- **Authors**: Yi Zhang, Mingyuan Chen, Jundong Shen, Chongjun Wang
- **Comment**: To be published in AAAI 2022
- **Journal**: None
- **Summary**: Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various human emotions from heterogeneous visual, audio and text modalities. Previous methods mainly focus on projecting multiple modalities into a common latent space and learning an identical representation for all labels, which neglects the diversity of each modality and fails to capture richer semantic information for each label from different perspectives. Besides, associated relationships of modalities and labels have not been fully exploited. In this paper, we propose versaTile multi-modAl learning for multI-labeL emOtion Recognition (TAILOR), aiming to refine multi-modal representations and enhance discriminative capacity of each label. Specifically, we design an adversarial multi-modal refinement module to sufficiently explore the commonality among different modalities and strengthen the diversity of each modality. To further exploit label-modal dependence, we devise a BERT-like cross-modal encoder to gradually fuse private and common modality representations in a granularity descent way, as well as a label-guided decoder to adaptively generate a tailored representation for each label with the guidance of label semantics. In addition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in both aligned and unaligned settings, which demonstrate the superiority of TAILOR over the state-of-the-arts. Code is available at https://github.com/kniter1/TAILOR.



### Smart Parking Space Detection under Hazy conditions using Convolutional Neural Networks: A Novel Approach
- **Arxiv ID**: http://arxiv.org/abs/2201.05858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05858v1)
- **Published**: 2022-01-15 14:15:46+00:00
- **Updated**: 2022-01-15 14:15:46+00:00
- **Authors**: Gaurav Satyanath, Jajati Keshari Sahoo, Rajendra Kumar Roul
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Limited urban parking space combined with urbanization has necessitated the development of smart parking systems that can communicate the availability of parking slots to the end users. Towards this, various deep learning based solutions using convolutional neural networks have been proposed for parking space occupation detection. Though these approaches are robust to partial obstructions and lighting conditions, their performance is found to degrade in the presence of haze conditions. Looking in this direction, this paper investigates the use of dehazing networks that improves the performance of parking space occupancy classifier under hazy conditions. Additionally, training procedures are proposed for dehazing networks to maximize the performance of the system on both hazy and non-hazy conditions. The proposed system is deployable as part of existing smart parking systems where limited number of cameras are used to monitor hundreds of parking spaces. To validate our approach, we have developed a custom hazy parking system dataset from real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed approach is tested against existing state-of-the-art parking space detectors on CNRPark-EXT and hazy parking system datasets. Experimental results indicate that there is a significant accuracy improvement of the proposed approach on the hazy parking system dataset.



### SDT-DCSCN for Simultaneous Super-Resolution and Deblurring of Text Images
- **Arxiv ID**: http://arxiv.org/abs/2201.05865v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.05865v1)
- **Published**: 2022-01-15 14:51:50+00:00
- **Updated**: 2022-01-15 14:51:50+00:00
- **Authors**: Hala Neji, Mohamed Ben Halima, Javier Nogueras-Iso, Tarek. M. Hamdani, Abdulrahman M. Qahtani, Omar Almutiry, Habib Dhahri, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (Deep CNN) have achieved hopeful performance for single image super-resolution. In particular, the Deep CNN skip Connection and Network in Network (DCSCN) architecture has been successfully applied to natural images super-resolution. In this work we propose an approach called SDT-DCSCN that jointly performs super-resolution and deblurring of low-resolution blurry text images based on DCSCN. Our approach uses subsampled blurry images in the input and original sharp images as ground truth. The used architecture is consists of a higher number of filters in the input CNN layer to a better analysis of the text details. The quantitative and qualitative evaluation on different datasets prove the high performance of our model to reconstruct high-resolution and sharp text images. In addition, in terms of computational time, our proposed method gives competitive performance compared to state of the art methods.



### Prototype Guided Network for Anomaly Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.05869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05869v2)
- **Published**: 2022-01-15 15:07:38+00:00
- **Updated**: 2022-03-15 10:50:26+00:00
- **Authors**: Yiqing Hao, Yi Jin, Gaoyun An
- **Comment**: Need for edit,and improve the method for better performance
- **Journal**: None
- **Summary**: Semantic segmentation methods can not directly identify abnormal objects in images. Anomaly Segmentation algorithm from this realistic setting can distinguish between in-distribution objects and Out-Of-Distribution (OOD) objects and output the anomaly probability for pixels. In this paper, a Prototype Guided Anomaly segmentation Network (PGAN) is proposed to extract semantic prototypes for in-distribution training data from limited annotated images. In the model, prototypes are used to model the hierarchical category semantic information and distinguish OOD pixels. The proposed PGAN model includes a semantic segmentation network and a prototype extraction network. Similarity measures are adopted to optimize the prototypes. The learned semantic prototypes are used as category semantics to compare the similarity with features extracted from test images and then to generate semantic segmentation prediction. The proposed prototype extraction network can also be integrated into most semantic segmentation networks and recognize OOD pixels. On the StreetHazards dataset, the proposed PGAN model produced mIoU of 53.4% for anomaly segmentation. The experimental results demonstrate PGAN may achieve the SOTA performance in the anomaly segmentation tasks.



### Domain Adaptation via Bidirectional Cross-Attention Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.05887v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05887v2)
- **Published**: 2022-01-15 16:49:56+00:00
- **Updated**: 2022-10-03 10:22:26+00:00
- **Authors**: Xiyu Wang, Pengxin Guo, Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Adaptation (DA) aims to leverage the knowledge learned from a source domain with ample labeled data to a target domain with unlabeled data only. Most existing studies on DA contribute to learning domain-invariant feature representations for both domains by minimizing the domain gap based on convolution-based neural networks. Recently, vision transformers significantly improved performance in multiple vision tasks. Built on vision transformers, in this paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA with the aim to improve the performance. In the proposed BCAT, the attention mechanism can extract implicit source and target mixup feature representations to narrow the domain discrepancy. Specifically, in BCAT, we design a weight-sharing quadruple-branch transformer with a bidirectional cross-attention mechanism to learn domain-invariant feature representations. Extensive experiments demonstrate that the proposed BCAT model achieves superior performance on four benchmark datasets over existing state-of-the-art DA methods that are based on convolutions or transformers.



### SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/2201.05905v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.05905v2)
- **Published**: 2022-01-15 18:42:38+00:00
- **Updated**: 2022-03-28 21:41:48+00:00
- **Authors**: Minh Tran, Loi Ly, Binh-Son Hua, Ngan Le
- **Comment**: Accepted to ISBI 2022
- **Journal**: None
- **Summary**: Capsule network is a recent new deep network architecture that has been applied successfully for medical image segmentation tasks. This work extends capsule networks for volumetric medical image segmentation with self-supervised learning. To improve on the problem of weight initialization compared to previous capsule networks, we leverage self-supervised learning for capsule networks pre-training, where our pretext-task is optimized by self-reconstruction. Our capsule network, SS-3DCapsNet, has a UNet-based architecture with a 3D Capsule encoder and 3D CNNs decoder. Our experiments on multiple datasets including iSeg-2017, Hippocampus, and Cardiac demonstrate that our 3D capsule network with self-supervised pre-training considerably outperforms previous capsule networks and 3D-UNets.



### Towards Zero-shot Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.05914v1
- **DOI**: 10.1109/TPAMI.2022.3143074
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05914v1)
- **Published**: 2022-01-15 19:26:36+00:00
- **Updated**: 2022-01-15 19:26:36+00:00
- **Authors**: Yunus Can Bilge, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the problem of zero-shot sign language recognition (ZSSLR), where the goal is to leverage models learned over the seen sign classes to recognize the instances of unseen sign classes. In this context, readily available textual sign descriptions and attributes collected from sign language dictionaries are utilized as semantic class representations for knowledge transfer. For this novel problem setup, we introduce three benchmark datasets with their accompanying textual and attribute descriptions to analyze the problem in detail. Our proposed approach builds spatiotemporal models of body and hand regions. By leveraging the descriptive text and attribute embeddings along with these visual representations within a zero-shot learning framework, we show that textual and attribute based class definitions can provide effective knowledge for the recognition of previously unseen sign classes. We additionally introduce techniques to analyze the influence of binary attributes in correct and incorrect zero-shot predictions. We anticipate that the introduced approaches and the accompanying datasets will provide a basis for further exploration of zero-shot learning in sign language recognition.



### Multi-level Second-order Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.05916v1
- **DOI**: 10.1109/TMM.2022.3142955
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05916v1)
- **Published**: 2022-01-15 19:49:00+00:00
- **Updated**: 2022-01-15 19:49:00+00:00
- **Authors**: Hongguang Zhang, Hongdong Li, Piotr Koniusz
- **Comment**: IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: We propose a Multi-level Second-order (MlSo) few-shot learning network for supervised or unsupervised few-shot image classification and few-shot action recognition. We leverage so-called power-normalized second-order base learner streams combined with features that express multiple levels of visual abstraction, and we use self-supervised discriminating mechanisms. As Second-order Pooling (SoP) is popular in image recognition, we employ its basic element-wise variant in our pipeline. The goal of multi-level feature design is to extract feature representations at different layer-wise levels of CNN, realizing several levels of visual abstraction to achieve robust few-shot learning. As SoP can handle convolutional feature maps of varying spatial sizes, we also introduce image inputs at multiple spatial scales into MlSo. To exploit the discriminative information from multi-level and multi-scale features, we develop a Feature Matching (FM) module that reweights their respective branches. We also introduce a self-supervised step, which is a discriminator of the spatial level and the scale of abstraction. Our pipeline is trained in an end-to-end manner. With a simple architecture, we demonstrate respectable results on standard datasets such as Omniglot, mini-ImageNet, tiered-ImageNet, Open MIC, fine-grained datasets such as CUB Birds, Stanford Dogs and Cars, and action recognition datasets such as HMDB51, UCF101, and mini-MIT.



### ViTBIS: Vision Transformer for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.05920v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05920v1)
- **Published**: 2022-01-15 20:44:45+00:00
- **Updated**: 2022-01-15 20:44:45+00:00
- **Authors**: Abhinav Sagar
- **Comment**: Published at Clinical Image-Based Procedures, Distributed and
  Collaborative Learning, Artificial Intelligence for Combating COVID-19 and
  Secure and Privacy-Preserving Machine Learning workshop at MICCAI 2021
- **Journal**: Springer, Cham 2021
- **Summary**: In this paper, we propose a novel network named Vision Transformer for Biomedical Image Segmentation (ViTBIS). Our network splits the input feature maps into three parts with $1\times 1$, $3\times 3$ and $5\times 5$ convolutions in both encoder and decoder. Concat operator is used to merge the features before being fed to three consecutive transformer blocks with attention mechanism embedded inside it. Skip connections are used to connect encoder and decoder transformer blocks. Similarly, transformer blocks and multi scale architecture is used in decoder before being linearly projected to produce the output segmentation map. We test the performance of our network using Synapse multi-organ segmentation dataset, Automated cardiac diagnosis challenge dataset, Brain tumour MRI segmentation dataset and Spleen CT segmentation dataset. Without bells and whistles, our network outperforms most of the previous state of the art CNN and transformer based models using Dice score and the Hausdorff distance as the evaluation metrics.




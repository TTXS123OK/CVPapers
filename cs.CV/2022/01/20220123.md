# Arxiv Papers in cs.CV on 2022-01-23
### Generative Adversarial Network Applications in Creating a Meta-Universe
- **Arxiv ID**: http://arxiv.org/abs/2201.09152v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09152v1)
- **Published**: 2022-01-23 00:57:02+00:00
- **Updated**: 2022-01-23 00:57:02+00:00
- **Authors**: Soheyla Amirian, Thiab R. Taha, Khaled Rasheed, Hamid R. Arabnia
- **Comment**: Computational Science and Computational Intelligence; 2021
  International Conference on IEEE CPS (IEEE XPLORE, Scopus), IEEE, 2021
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are machine learning methods that are used in many important and novel applications. For example, in imaging science, GANs are effectively utilized in generating image datasets, photographs of human faces, image and video captioning, image-to-image translation, text-to-image translation, video prediction, and 3D object generation to name a few. In this paper, we discuss how GANs can be used to create an artificial world. More specifically, we discuss how GANs help to describe an image utilizing image/video captioning methods and how to translate the image to a new image using image-to-image translation frameworks in a theme we desire. We articulate how GANs impact creating a customized world.



### An Integrated Approach for Video Captioning and Applications
- **Arxiv ID**: http://arxiv.org/abs/2201.09153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09153v1)
- **Published**: 2022-01-23 01:06:00+00:00
- **Updated**: 2022-01-23 01:06:00+00:00
- **Authors**: Soheyla Amirian, Thiab R. Taha, Khaled Rasheed, Hamid R. Arabnia
- **Comment**: The 2021 World Congress in Computer Science, Computer Engineering,
  and Applied Computing (CSCE'21), IEEE, 2021
- **Journal**: None
- **Summary**: Physical computing infrastructure, data gathering, and algorithms have recently had significant advances to extract information from images and videos. The growth has been especially outstanding in image captioning and video captioning. However, most of the advancements in video captioning still take place in short videos. In this research, we caption longer videos only by using the keyframes, which are a small subset of the total video frames. Instead of processing thousands of frames, only a few frames are processed depending on the number of keyframes. There is a trade-off between the computation of many frames and the speed of the captioning process. The approach in this research is to allow the user to specify the trade-off between execution time and accuracy. In addition, we argue that linking images, videos, and natural language offers many practical benefits and immediate practical applications. From the modeling perspective, instead of designing and staging explicit algorithms to process videos and generate captions in complex processing pipelines, our contribution lies in designing hybrid deep learning architectures to apply in long videos by captioning video keyframes. We consider the technology and the methodology that we have developed as steps toward the applications discussed in this research.



### LSNet: Extremely Light-Weight Siamese Network For Change Detection in Remote Sensing Image
- **Arxiv ID**: http://arxiv.org/abs/2201.09156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09156v1)
- **Published**: 2022-01-23 01:42:49+00:00
- **Updated**: 2022-01-23 01:42:49+00:00
- **Authors**: Biyuan Liu, Huaixin Chen, Zhixi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The Siamese network is becoming the mainstream in change detection of remote sensing images (RSI). However, in recent years, the development of more complicated structure, module and training processe has resulted in the cumbersome model, which hampers their application in large-scale RSI processing. To this end, this paper proposes an extremely lightweight Siamese network (LSNet) for RSI change detection, which replaces standard convolution with depthwise separable atrous convolution, and removes redundant dense connections, retaining only valid feature flows while performing Siamese feature fusion, greatly compressing parameters and computation amount. Compared with the first-place model on the CCD dataset, the parameters and the computation amount of LSNet is greatly reduced by 90.35\% and 91.34\% respectively, with only a 1.5\% drops in accuracy.



### Pulmonary Fissure Segmentation in CT Images Based on ODoS Filter and Shape Features
- **Arxiv ID**: http://arxiv.org/abs/2201.09163v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09163v1)
- **Published**: 2022-01-23 02:43:03+00:00
- **Updated**: 2022-01-23 02:43:03+00:00
- **Authors**: Yuanyuan Peng, Pengpeng Luan, Hongbin Tu, Xiong Li, Ping Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Priori knowledge of pulmonary anatomy plays a vital role in diagnosis of lung diseases. In CT images, pulmonary fissure segmentation is a formidable mission due to various of factors. To address the challenge, an useful approach based on ODoS filter and shape features is presented for pulmonary fissure segmentation. Here, we adopt an ODoS filter by merging the orientation information and magnitude information to highlight structure features for fissure enhancement, which can effectively distinguish between pulmonary fissures and clutters. Motivated by the fact that pulmonary fissures appear as linear structures in 2D space and planar structures in 3D space in orientation field, an orientation curvature criterion and an orientation partition scheme are fused to separate fissure patches and other structures in different orientation partition, which can suppress parts of clutters. Considering the shape difference between pulmonary fissures and tubular structures in magnitude field, a shape measure approach and a 3D skeletonization model are combined to segment pulmonary fissures for clutters removal. When applying our scheme to 55 chest CT scans which acquired from a publicly available LOLA11 datasets, the median F1-score, False Discovery Rate (FDR), and False Negative Rate (FNR) respectively are 0.896, 0.109, and 0.100, which indicates that the presented method has a satisfactory pulmonary fissure segmentation performance.



### A Pre-trained Audio-Visual Transformer for Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.09165v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.09165v1)
- **Published**: 2022-01-23 03:09:16+00:00
- **Updated**: 2022-01-23 03:09:16+00:00
- **Authors**: Minh Tran, Mohammad Soleymani
- **Comment**: Accepted by IEEE ICASSP 2022
- **Journal**: None
- **Summary**: In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, fine-tuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.



### Mixed X-Ray Image Separation for Artworks with Concealed Designs
- **Arxiv ID**: http://arxiv.org/abs/2201.09167v1
- **DOI**: 10.1109/TIP.2022.3185488
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09167v1)
- **Published**: 2022-01-23 03:20:35+00:00
- **Updated**: 2022-01-23 03:20:35+00:00
- **Authors**: Wei Pu, Jun-Jie Huang, Barak Sober, Nathan Daly, Catherine Higgitt, Ingrid Daubechies, Pier Luigi Dragotti, Miguel Rodigues
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on X-ray images of paintings with concealed sub-surface designs (e.g., deriving from reuse of the painting support or revision of a composition by the artist), which include contributions from both the surface painting and the concealed features. In particular, we propose a self-supervised deep learning-based image separation approach that can be applied to the X-ray images from such paintings to separate them into two hypothetical X-ray images. One of these reconstructed images is related to the X-ray image of the concealed painting, while the second one contains only information related to the X-ray of the visible painting. The proposed separation network consists of two components: the analysis and the synthesis sub-networks. The analysis sub-network is based on learned coupled iterative shrinkage thresholding algorithms (LCISTA) designed using algorithm unrolling techniques, and the synthesis sub-network consists of several linear mappings. The learning algorithm operates in a totally self-supervised fashion without requiring a sample set that contains both the mixed X-ray images and the separated ones. The proposed method is demonstrated on a real painting with concealed content, Do\~na Isabel de Porcel by Francisco de Goya, to show its effectiveness.



### Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.09168v3
- **DOI**: 10.1109/TCSVT.2022.3150959
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.09168v3)
- **Published**: 2022-01-23 03:38:37+00:00
- **Updated**: 2022-03-03 11:39:51+00:00
- **Authors**: Jianfeng Dong, Yabing Wang, Xianke Chen, Xiaoye Qu, Xirong Li, Yuan He, Xun Wang
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology. Code is available at https://github.com/LiJiaBei-7/rivrl
- **Journal**: None
- **Summary**: This paper aims for the task of text-to-video retrieval, where given a query in the form of a natural-language sentence, it is asked to retrieve videos which are semantically relevant to the given query, from a great number of unlabeled videos. The success of this task depends on cross-modal representation learning that projects both videos and sentences into common spaces for semantic similarity computation. In this work, we concentrate on video representation learning, an essential component for text-to-video retrieval. Inspired by the reading strategy of humans, we propose a Reading-strategy Inspired Visual Representation Learning (RIVRL) to represent videos, which consists of two branches: a previewing branch and an intensive-reading branch. The previewing branch is designed to briefly capture the overview information of videos, while the intensive-reading branch is designed to obtain more in-depth information. Moreover, the intensive-reading branch is aware of the video overview captured by the previewing branch. Such holistic information is found to be useful for the intensive-reading branch to extract more fine-grained features. Extensive experiments on three datasets are conducted, where our model RIVRL achieves a new state-of-the-art on TGIF and VATEX. Moreover, on MSR-VTT, our model using two video features shows comparable performance to the state-of-the-art using seven video features and even outperforms models pre-trained on the large-scale HowTo100M dataset.



### Rich Action-semantic Consistent Knowledge for Early Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2201.09169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09169v2)
- **Published**: 2022-01-23 03:39:31+00:00
- **Updated**: 2023-01-20 02:57:36+00:00
- **Authors**: Xiaoli Liu, Jianqin Yin, Di Guo
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Early action prediction (EAP) aims to recognize human actions from a part of action execution in ongoing videos, which is an important task for many practical applications. Most prior works treat partial or full videos as a whole, ignoring rich action knowledge hidden in videos, i.e., semantic consistencies among different partial videos. In contrast, we partition original partial or full videos to form a new series of partial videos and mine the Action Semantic Consistent Knowledge (ASCK) among these new partial videos evolving in arbitrary progress levels. Moreover, a novel Rich Action-semantic Consistent Knowledge network (RACK) under the teacher-student framework is proposed for EAP. Firstly, we use a two-stream pre-trained model to extract features of videos. Secondly, we treat the RGB or flow features of the partial videos as nodes and their action semantic consistencies as edges. Next, we build a bi-directional semantic graph for the teacher network and a single-directional semantic graph for the student network to model rich ASCK among partial videos. The MSE and MMD losses are incorporated as our distillation loss to enrich the ASCK of partial videos from the teacher to the student network. Finally, we obtain the final prediction by summering the logits of different sub-networks and applying a softmax layer. Extensive experiments and ablative studies have been conducted, demonstrating the effectiveness of modeling rich ASCK for EAP. With the proposed RACK, we have achieved state-of-the-art performance on three benchmarks. The code will be released if the paper is accepted.



### Learning to Minimize the Remainder in Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09193v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09193v2)
- **Published**: 2022-01-23 06:31:23+00:00
- **Updated**: 2022-03-06 07:27:46+00:00
- **Authors**: Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, Qi Zhao
- **Comment**: Accepted to IEEE TMM
- **Journal**: None
- **Summary**: The learning process of deep learning methods usually updates the model's parameters in multiple iterations. Each iteration can be viewed as the first-order approximation of Taylor's series expansion. The remainder, which consists of higher-order terms, is usually ignored in the learning process for simplicity. This learning scheme empowers various multimedia based applications, such as image retrieval, recommendation system, and video search. Generally, multimedia data (e.g., images) are semantics-rich and high-dimensional, hence the remainders of approximations are possibly non-zero. In this work, we consider the remainder to be informative and study how it affects the learning process. To this end, we propose a new learning approach, namely gradient adjustment learning (GAL), to leverage the knowledge learned from the past training iterations to adjust vanilla gradients, such that the remainders are minimized and the approximations are improved. The proposed GAL is model- and optimizer-agnostic, and is easy to adapt to the standard learning framework. It is evaluated on three tasks, i.e., image classification, object detection, and regression, with state-of-the-art models and optimizers. The experiments show that the proposed GAL consistently enhances the evaluated models, whereas the ablation studies validate various aspects of the proposed GAL. The code is available at \url{https://github.com/luoyan407/gradient_adjustment.git}.



### Learning to Predict Gradients for Semi-Supervised Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09196v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09196v1)
- **Published**: 2022-01-23 06:45:47+00:00
- **Updated**: 2022-01-23 06:45:47+00:00
- **Authors**: Yan Luo, Yongkang Wong, Mohan Kankanhalli, Qi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: A key challenge for machine intelligence is to learn new visual concepts without forgetting the previously acquired knowledge. Continual learning is aimed towards addressing this challenge. However, there is a gap between existing supervised continual learning and human-like intelligence, where human is able to learn from both labeled and unlabeled data. How unlabeled data affects learning and catastrophic forgetting in the continual learning process remains unknown. To explore these issues, we formulate a new semi-supervised continual learning method, which can be generically applied to existing continual learning models. Specifically, a novel gradient learner learns from labeled data to predict gradients on unlabeled data. Hence, the unlabeled data could fit into the supervised continual learning method. Different from conventional semi-supervised settings, we do not hypothesize that the underlying classes, which are associated to the unlabeled data, are known to the learning process. In other words, the unlabeled data could be very distinct from the labeled data. We evaluate the proposed method on mainstream continual learning, adversarial continual learning, and semi-supervised learning tasks. The proposed method achieves state-of-the-art performance on classification accuracy and backward transfer in the continual learning setting while achieving desired performance on classification accuracy in the semi-supervised learning setting. This implies that the unlabeled images can enhance the generalizability of continual learning models on the predictive ability on unseen data and significantly alleviate catastrophic forgetting. The code is available at \url{https://github.com/luoyan407/grad_prediction.git}.



### Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2201.09201v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09201v2)
- **Published**: 2022-01-23 07:18:55+00:00
- **Updated**: 2023-08-10 18:34:17+00:00
- **Authors**: Ming Dai, Enhui Zheng, Zhenhua Feng, Jiedong Zhuang, Wankou Yang
- **Comment**: 13 pages,8 figures
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) rely on satellite systems for stable positioning. However, due to limited satellite coverage or communication disruptions, UAVs may lose signals from satellite-based positioning systems. In such situations, vision-based techniques can serve as an alternative, ensuring the self-positioning capability of UAVs. However, most of the existing datasets are developed for the geo-localization tasks of the objects identified by UAVs, rather than the self-positioning task of UAVs. Furthermore, the current UAV datasets use discrete sampling on synthetic data, such as Google Maps, thereby neglecting the crucial aspects of dense sampling and the uncertainties commonly experienced in real-world scenarios. To address these issues, this paper presents a new dataset, DenseUAV, which is the first publicly available dataset designed for the UAV self-positioning task. DenseUAV adopts dense sampling on UAV images obtained in low-altitude urban settings. In total, over 27K UAV-view and satellite-view images of 14 university campuses are collected and annotated, establishing a new benchmark. In terms of model development, we first verify the superiority of Transformers over CNNs in this task. Then, we incorporate metric learning into representation learning to enhance the discriminative capacity of the model and to lessen the modality discrepancy. Besides, to facilitate joint learning from both perspectives, we propose a mutually supervised learning approach. Last, we enhance the Recall@K metric and introduce a new measurement, SDM@K, to evaluate the performance of a trained model from both the retrieval and localization perspectives simultaneously. As a result, the proposed baseline method achieves a remarkable Recall@1 score of 83.05% and an SDM@1 score of 86.24% on DenseUAV. The dataset and code will be made publicly available on https://github.com/Dmmm1997/DenseUAV.



### Deeply Explain CNN via Hierarchical Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2201.09205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09205v1)
- **Published**: 2022-01-23 07:56:04+00:00
- **Updated**: 2022-01-23 07:56:04+00:00
- **Authors**: Ming-Ming Cheng, Peng-Tao Jiang, Ling-Hao Han, Liang Wang, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, some attribution methods for explaining CNNs attempt to study how the intermediate features affect the network prediction. However, they usually ignore the feature hierarchies among the intermediate features. This paper introduces a hierarchical decomposition framework to explain CNN's decision-making process in a top-down manner. Specifically, we propose a gradient-based activation propagation (gAP) module that can decompose any intermediate CNN decision to its lower layers and find the supporting features. Then we utilize the gAP module to iteratively decompose the network decision to the supporting evidence from different CNN layers. The proposed framework can generate a deep hierarchy of strongly associated supporting evidence for the network decision, which provides insight into the decision-making process. Moreover, gAP is effort-free for understanding CNN-based models without network architecture modification and extra training process. Experiments show the effectiveness of the proposed method. The code and interactive demo website will be made publicly available.



### A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization
- **Arxiv ID**: http://arxiv.org/abs/2201.09206v1
- **DOI**: 10.1109/TCSVT.2021.3135013
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09206v1)
- **Published**: 2022-01-23 08:01:42+00:00
- **Updated**: 2022-01-23 08:01:42+00:00
- **Authors**: Ming Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng
- **Comment**: 14 pages, 13 figures, IEEE Transactions on Circuits and Systems for
  Video Technology
- **Journal**: None
- **Summary**: Cross-view geo-localization is a task of matching the same geographic image from different views, e.g., unmanned aerial vehicle (UAV) and satellite. The most difficult challenges are the position shift and the uncertainty of distance and scale. Existing methods are mainly aimed at digging for more comprehensive fine-grained information. However, it underestimates the importance of extracting robust feature representation and the impact of feature alignment. The CNN-based methods have achieved great success in cross-view geo-localization. However it still has some limitations, e.g., it can only extract part of the information in the neighborhood and some scale reduction operations will make some fine-grained information lost. In particular, we introduce a simple and efficient transformer-based structure called Feature Segmentation and Region Alignment (FSRA) to enhance the model's ability to understand contextual information as well as to understand the distribution of instances. Without using additional supervisory information, FSRA divides regions based on the heat distribution of the transformer's feature map, and then aligns multiple specific regions in different views one on one. Finally, FSRA integrates each region into a set of feature representations. The difference is that FSRA does not divide regions manually, but automatically based on the heat distribution of the feature map. So that specific instances can still be divided and aligned when there are significant shifts and scale changes in the image. In addition, a multiple sampling strategy is proposed to overcome the disparity in the number of satellite images and that of images from other sources. Experiments show that the proposed method has superior performance and achieves the state-of-the-art in both tasks of drone view target localization and drone navigation. Code will be released at https://github.com/Dmmm1997/FSRA



### Visual Object Tracking on Multi-modal RGB-D Videos: A Review
- **Arxiv ID**: http://arxiv.org/abs/2201.09207v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09207v2)
- **Published**: 2022-01-23 08:02:49+00:00
- **Updated**: 2022-01-29 07:31:02+00:00
- **Authors**: Xue-Feng Zhu, Tianyang Xu, Xiao-Jun Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The development of visual object tracking has continued for decades. Recent years, as the wide accessibility of the low-cost RGBD sensors, the task of visual object tracking on RGB-D videos has drawn much attention. Compared to conventional RGB-only tracking, the RGB-D videos can provide more information that facilitates objecting tracking in some complicated scenarios. The goal of this review is to summarize the relative knowledge of the research filed of RGB-D tracking. To be specific, we will generalize the related RGB-D tracking benchmarking datasets as well as the corresponding performance measurements. Besides, the existing RGB-D tracking methods are summarized in the paper. Moreover, we discuss the possible future direction in the field of RGB-D tracking.



### Design of Sensor Fusion Driver Assistance System for Active Pedestrian Safety
- **Arxiv ID**: http://arxiv.org/abs/2201.09208v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2201.09208v1)
- **Published**: 2022-01-23 08:52:32+00:00
- **Updated**: 2022-01-23 08:52:32+00:00
- **Authors**: I-Hsi Kao, Ya-Zhu Yian, Jian-An Su, Yi-Horng Lai, Jau-Woei Perng, Tung-Li Hsieh, Yi-Shueh Tsai, Min-Shiu Hsieh
- **Comment**: The 14th International Conference on Automation Technology
  (Automation 2017), December 8-10, 2017, Kaohsiung, Taiwan
- **Journal**: None
- **Summary**: In this paper, we present a parallel architecture for a sensor fusion detection system that combines a camera and 1D light detection and ranging (lidar) sensor for object detection. The system contains two object detection methods, one based on an optical flow, and the other using lidar. The two sensors can effectively complement the defects of the other. The accurate longitudinal accuracy of the object's location and its lateral movement information can be achieved simultaneously. Using a spatio-temporal alignment and a policy of sensor fusion, we completed the development of a fusion detection system with high reliability at distances of up to 20 m. Test results show that the proposed system achieves a high level of accuracy for pedestrian or object detection in front of a vehicle, and has high robustness to special environments.



### FN-Net:Remove the Outliers by Filtering the Noise
- **Arxiv ID**: http://arxiv.org/abs/2201.09213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T06, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2201.09213v1)
- **Published**: 2022-01-23 09:09:27+00:00
- **Updated**: 2022-01-23 09:09:27+00:00
- **Authors**: Kai Lv
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Establishing the correspondence between two images is an important research direction of computer vision. When estimating the relationship between two images, it is often disturbed by outliers. In this paper, we propose a convolutional neural network that can filter the noise of outliers. It can output the probability that the pair of feature points is an inlier and regress the essential matrix representing the relative pose of the camera. The outliers are mainly caused by the noise introduced by the previous processing. The outliers rejection can be treated as a problem of noise elimination, and the soft threshold function has a very good effect on noise reduction. Therefore, we designed an adaptive denoising module based on soft threshold function to remove noise components in the outliers, to reduce the probability that the outlier is predicted to be an inlier. Experimental results on the YFCC100M dataset show that our method exceeds the state-of-the-art in relative pose estimation.



### Learning-Driven Lossy Image Compression; A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2201.09240v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.09240v1)
- **Published**: 2022-01-23 12:11:31+00:00
- **Updated**: 2022-01-23 12:11:31+00:00
- **Authors**: Sonain Jamil, Md. Jalil Piran, MuhibUrRahman
- **Comment**: None
- **Journal**: None
- **Summary**: In the realm of image processing and computer vision (CV), machine learning (ML) architectures are widely applied. Convolutional neural networks (CNNs) solve a wide range of image processing issues and can solve image compression problem. Compression of images is necessary due to bandwidth and memory constraints. Helpful, redundant, and irrelevant information are three different forms of information found in images. This paper aims to survey recent techniques utilizing mostly lossy image compression using ML architectures including different auto-encoders (AEs) such as convolutional auto-encoders (CAEs), variational auto-encoders (VAEs), and AEs with hyper-prior models, recurrent neural networks (RNNs), CNNs, generative adversarial networks (GANs), principal component analysis (PCA) and fuzzy means clustering. We divide all of the algorithms into several groups based on architecture. We cover still image compression in this survey. Various discoveries for the researchers are emphasized and possible future directions for researchers. The open research problems such as out of memory (OOM), striped region distortion (SRD), aliasing, and compatibility of the frameworks with central processing unit (CPU) and graphics processing unit (GPU) simultaneously are explained. The majority of the publications in the compression domain surveyed are from the previous five years and use a variety of approaches.



### Increasing the Cost of Model Extraction with Calibrated Proof of Work
- **Arxiv ID**: http://arxiv.org/abs/2201.09243v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09243v3)
- **Published**: 2022-01-23 12:21:28+00:00
- **Updated**: 2022-12-12 16:11:45+00:00
- **Authors**: Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot
- **Comment**: Published as a conference paper at ICLR 2022 (Spotlight - 5% of
  submitted papers)
- **Journal**: None
- **Summary**: In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.



### Face recognition via compact second order image gradient orientations
- **Arxiv ID**: http://arxiv.org/abs/2201.09246v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09246v2)
- **Published**: 2022-01-23 12:22:53+00:00
- **Updated**: 2022-01-29 06:57:20+00:00
- **Authors**: He-Feng Yin, Xiao-Jun Wu, Xiaoning Song
- **Comment**: 26 pages, 6 figures
- **Journal**: None
- **Summary**: Conventional subspace learning approaches based on image gradient orientations only employ the first-order gradient information. However, recent researches on human vision system (HVS) uncover that the neural image is a landscape or a surface whose geometric properties can be captured through the second order gradient information. The second order image gradient orientations (SOIGO) can mitigate the adverse effect of noises in face images. To reduce the redundancy of SOIGO, we propose compact SOIGO (CSOIGO) by applying linear complex principal component analysis (PCA) in SOIGO. Combined with collaborative representation based classification (CRC) algorithm, the classification performance of CSOIGO is further enhanced. CSOIGO is evaluated under real-world disguise, synthesized occlusion and mixed variations. Experimental results indicate that the proposed method is superior to its competing approaches with few training samples, and even outperforms some prevailing deep neural network based approaches. The source code of CSOIGO is available at https://github.com/yinhefeng/SOIGO.



### Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/2201.09267v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09267v1)
- **Published**: 2022-01-23 13:53:23+00:00
- **Updated**: 2022-01-23 13:53:23+00:00
- **Authors**: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley
- **Comment**: To appear as a part of an upcoming textbook on dimensionality
  reduction and manifold learning
- **Journal**: None
- **Summary**: This is a tutorial and survey paper on metric learning. Algorithms are divided into spectral, probabilistic, and deep metric learning. We first start with the definition of distance metric, Mahalanobis distance, and generalized Mahalanobis distance. In spectral methods, we start with methods using scatters of data, including the first spectral metric learning, relevant methods to Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric learning, locally linear metric adaptation, and adversarial metric learning are covered. We also explain several kernel spectral methods for metric learning in the feature space. We also introduce geometric metric learning methods on the Riemannian manifolds. In probabilistic methods, we start with collapsing classes in both input and feature spaces and then explain the neighborhood component analysis methods, Bayesian metric learning, information theoretic methods, and empirical risk minimization in metric learning. In deep learning methods, we first introduce reconstruction autoencoders and supervised loss functions for metric learning. Then, Siamese networks and its various loss functions, triplet mining, and triplet sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning, geometric metric learning by neural networks, and few-shot metric learning.



### Wavelet-Attention CNN for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.09271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09271v1)
- **Published**: 2022-01-23 14:00:33+00:00
- **Updated**: 2022-01-23 14:00:33+00:00
- **Authors**: Zhao Xiangyu
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: The feature learning methods based on convolutional neural network (CNN) have successfully produced tremendous achievements in image classification tasks. However, the inherent noise and some other factors may weaken the effectiveness of the convolutional feature statistics. In this paper, we investigate Discrete Wavelet Transform (DWT) in the frequency domain and design a new Wavelet-Attention (WA) block to only implement attention in the high-frequency domain. Based on this, we propose a Wavelet-Attention convolutional neural network (WA-CNN) for image classification. Specifically, WA-CNN decomposes the feature maps into low-frequency and high-frequency components for storing the structures of the basic objects, as well as the detailed information and noise, respectively. Then, the WA block is leveraged to capture the detailed information in the high-frequency domain with different attention factors but reserves the basic object structures in the low-frequency domain. Experimental results on CIFAR-10 and CIFAR-100 datasets show that our proposed WA-CNN achieves significant improvements in classification accuracy compared to other related networks. Specifically, based on MobileNetV2 backbones, WA-CNN achieves 1.26% Top-1 accuracy improvement on the CIFAR-10 benchmark and 1.54% Top-1 accuracy improvement on the CIFAR-100 benchmark.



### How to scale hyperparameters for quickshift image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.09286v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2201.09286v2)
- **Published**: 2022-01-23 15:05:54+00:00
- **Updated**: 2022-02-21 20:29:01+00:00
- **Authors**: Damien Garreau
- **Comment**: 33 pages, 16 figures. Accepted to AISTATS 2022
- **Journal**: None
- **Summary**: Quickshift is a popular algorithm for image segmentation, used as a preprocessing step in many applications. Unfortunately, it is quite challenging to understand the hyperparameters' influence on the number and shape of superpixels produced by the method. In this paper, we study theoretically a slightly modified version of the quickshift algorithm, with a particular emphasis on homogeneous image patches with i.i.d. pixel noise and sharp boundaries between such patches. Leveraging this analysis, we derive a simple heuristic to scale quickshift hyperparameters with respect to the image size, which we check empirically.



### A Survey for Deep RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.09296v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09296v2)
- **Published**: 2022-01-23 15:52:26+00:00
- **Updated**: 2022-01-29 05:51:53+00:00
- **Authors**: Zhangyong Tang, Tianyang Xu, Xiao-Jun Wu
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Visual object tracking with the visible (RGB) and thermal infrared (TIR) electromagnetic waves, shorted in RGBT tracking, recently draws increasing attention in the tracking community. Considering the rapid development of deep learning, a survey for the recent deep neural network based RGBT trackers is presented in this paper. Firstly, we give brief introduction for the RGBT trackers concluded into this category. Then, a comparison among the existing RGBT trackers on several challenging benchmarks is given statistically. Specifically, MDNet and Siamese architectures are the two mainstream frameworks in the RGBT community, especially the former. Trackers based on MDNet achieve higher performance while Siamese-based trackers satisfy the real-time requirement. In summary, since the large-scale dataset LasHeR is published, the integration of end-to-end framework, e.g., Siamese and Transformer, should be further considered to fulfil the real-time as well as more robust performance. Furthermore, the mathematical meaning should be more considered during designing the network. This survey can be treated as a look-up-table for researchers who are concerned about RGBT tracking.



### 1000x Faster Camera and Machine Vision with Ordinary Devices
- **Arxiv ID**: http://arxiv.org/abs/2201.09302v1
- **DOI**: 10.1016/j.eng.2022.01.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09302v1)
- **Published**: 2022-01-23 16:10:11+00:00
- **Updated**: 2022-01-23 16:10:11+00:00
- **Authors**: Tiejun Huang, Yajing Zheng, Zhaofei Yu, Rui Chen, Yuan Li, Ruiqin Xiong, Lei Ma, Junwei Zhao, Siwei Dong, Lin Zhu, Jianing Li, Shanshan Jia, Yihua Fu, Boxin Shi, Si Wu, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In digital cameras, we find a major limitation: the image and video form inherited from a film camera obstructs it from capturing the rapidly changing photonic world. Here, we present vidar, a bit sequence array where each bit represents whether the accumulation of photons has reached a threshold, to record and reconstruct the scene radiance at any moment. By employing only consumer-level CMOS sensors and integrated circuits, we have developed a vidar camera that is 1,000x faster than conventional cameras. By treating vidar as spike trains in biological vision, we have further developed a spiking neural network-based machine vision system that combines the speed of the machine and the mechanism of biological vision, achieving high-speed object detection and tracking 1,000x faster than human vision. We demonstrate the utility of the vidar camera and the super vision system in an assistant referee and target pointing system. Our study is expected to fundamentally revolutionize the image and video concepts and related industries, including photography, movies, and visual media, and to unseal a new spiking neural network-enabled speed-free machine vision era.



### Basket-based Softmax
- **Arxiv ID**: http://arxiv.org/abs/2201.09308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09308v1)
- **Published**: 2022-01-23 16:43:29+00:00
- **Updated**: 2022-01-23 16:43:29+00:00
- **Authors**: Qiang Meng, Xinqian Gu, Xiaqing Xu, Feng Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Softmax-based losses have achieved state-of-the-art performances on various tasks such as face recognition and re-identification. However, these methods highly relied on clean datasets with global labels, which limits their usage in many real-world applications. An important reason is that merging and organizing datasets from various temporal and spatial scenarios is usually not realistic, as noisy labels can be introduced and exponential-increasing resources are required. To address this issue, we propose a novel mining-during-training strategy called Basket-based Softmax (BBS) as well as its parallel version to effectively train models on multiple datasets in an end-to-end fashion. Specifically, for each training sample, we simultaneously adopt similarity scores as the clue to mining negative classes from other datasets, and dynamically add them to assist the learning of discriminative features. Experimentally, we demonstrate the efficiency and superiority of the BBS on the tasks of face recognition and re-identification, with both simulated and real-world datasets.



### Perceptual cGAN for MRI Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.09314v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09314v1)
- **Published**: 2022-01-23 16:58:56+00:00
- **Updated**: 2022-01-23 16:58:56+00:00
- **Authors**: Sahar Almahfouz Nasser, Saqib Shamsi, Valay Bundele, Bhavesh Garg, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: Capturing high-resolution magnetic resonance (MR) images is a time consuming process, which makes it unsuitable for medical emergencies and pediatric patients. Low-resolution MR imaging, by contrast, is faster than its high-resolution counterpart, but it compromises on fine details necessary for a more precise diagnosis. Super-resolution (SR), when applied to low-resolution MR images, can help increase their utility by synthetically generating high-resolution images with little additional time. In this paper, we present a SR technique for MR images that is based on generative adversarial networks (GANs), which have proven to be quite useful in generating sharp-looking details in SR. We introduce a conditional GAN with perceptual loss, which is conditioned upon the input low-resolution image, which improves the performance for isotropic and anisotropic MRI super-resolution.



### Sparse-view Cone Beam CT Reconstruction using Data-consistent Supervised and Adversarial Learning from Scarce Training Data
- **Arxiv ID**: http://arxiv.org/abs/2201.09318v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2201.09318v1)
- **Published**: 2022-01-23 17:08:52+00:00
- **Updated**: 2022-01-23 17:08:52+00:00
- **Authors**: Anish Lahiri, Marc Klasky, Jeffrey A. Fessler, Saiprasad Ravishankar
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of CT images from a limited set of projections through an object is important in several applications ranging from medical imaging to industrial settings. As the number of available projections decreases, traditional reconstruction techniques such as the FDK algorithm and model-based iterative reconstruction methods perform poorly. Recently, data-driven methods such as deep learning-based reconstruction have garnered a lot of attention in applications because they yield better performance when enough training data is available. However, even these methods have their limitations when there is a scarcity of available training data. This work focuses on image reconstruction in such settings, i.e., when both the number of available CT projections and the training data is extremely limited. We adopt a sequential reconstruction approach over several stages using an adversarially trained shallow network for 'destreaking' followed by a data-consistency update in each stage. To deal with the challenge of limited data, we use image subvolumes to train our method, and patch aggregation during testing. To deal with the computational challenge of learning on 3D datasets for 3D reconstruction, we use a hybrid 3D-to-2D mapping network for the 'destreaking' part. Comparisons to other methods over several test examples indicate that the proposed method has much potential, when both the number of projections and available training data are highly limited.



### Out of Distribution Detection on ImageNet-O
- **Arxiv ID**: http://arxiv.org/abs/2201.09352v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09352v1)
- **Published**: 2022-01-23 20:02:08+00:00
- **Updated**: 2022-01-23 20:02:08+00:00
- **Authors**: Anugya Srivastava, Shriya Jain, Mugdha Thigle
- **Comment**: None
- **Journal**: None
- **Summary**: Out of distribution (OOD) detection is a crucial part of making machine learning systems robust. The ImageNet-O dataset is an important tool in testing the robustness of ImageNet trained deep neural networks that are widely used across a variety of systems and applications. We aim to perform a comparative analysis of OOD detection methods on ImageNet-O, a first of its kind dataset with a label distribution different than that of ImageNet, that has been created to aid research in OOD detection for ImageNet models. As this dataset is fairly new, we aim to provide a comprehensive benchmarking of some of the current state of the art OOD detection methods on this novel dataset. This benchmarking covers a variety of model architectures, settings where we haves prior access to the OOD data versus when we don't, predictive score based approaches, deep generative approaches to OOD detection, and more.



### Survey and Systematization of 3D Object Detection Models and Methods
- **Arxiv ID**: http://arxiv.org/abs/2201.09354v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09354v2)
- **Published**: 2022-01-23 20:06:07+00:00
- **Updated**: 2023-05-05 09:19:03+00:00
- **Authors**: Moritz Drobnitzky, Jonas Friederich, Bernhard Egger, Patrick Zschech
- **Comment**: accepted at "The Visual Computer"
- **Journal**: None
- **Summary**: Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.



### Transformer-based SAR Image Despeckling
- **Arxiv ID**: http://arxiv.org/abs/2201.09355v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09355v1)
- **Published**: 2022-01-23 20:09:01+00:00
- **Updated**: 2022-01-23 20:09:01+00:00
- **Authors**: Malsha V. Perera, Wele Gedara Chaminda Bandara, Jeya Maria Jose Valanarasu, Vishal M. Patel
- **Comment**: Submitted to International Geoscience and Remote Sensing Symposium
  (IGARSS), 2022. Our code is available at
  https://github.com/malshaV/sar_transformer
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) images are usually degraded by a multiplicative noise known as speckle which makes processing and interpretation of SAR images difficult. In this paper, we introduce a transformer-based network for SAR image despeckling. The proposed despeckling network comprises of a transformer-based encoder which allows the network to learn global dependencies between different image regions - aiding in better despeckling. The network is trained end-to-end with synthetically generated speckled images using a composite loss function. Experiments show that the proposed method achieves significant improvements over traditional and convolutional neural network-based despeckling methods on both synthetic and real SAR images.



### POTHER: Patch-Voted Deep Learning-Based Chest X-ray Bias Analysis for COVID-19 Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.09360v5
- **DOI**: 10.1007/978-3-031-08754-7_51
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09360v5)
- **Published**: 2022-01-23 20:35:45+00:00
- **Updated**: 2022-08-08 15:11:48+00:00
- **Authors**: Tomasz Szczepaski, Arkadiusz Sitek, Tomasz Trzciski, Szymon Potka
- **Comment**: Accepted at International Conference on Computational Science (ICCS)
  2022, London
- **Journal**: None
- **Summary**: A critical step in the fight against COVID-19, which continues to have a catastrophic impact on peoples lives, is the effective screening of patients presented in the clinics with severe COVID-19 symptoms. Chest radiography is one of the promising screening approaches. Many studies reported detecting COVID-19 in chest X-rays accurately using deep learning. A serious limitation of many published approaches is insufficient attention paid to explaining decisions made by deep learning models. Using explainable artificial intelligence methods, we demonstrate that model decisions may rely on confounding factors rather than medical pathology. After an analysis of potential confounding factors found on chest X-ray images, we propose a novel method to minimise their negative impact. We show that our proposed method is more robust than previous attempts to counter confounding factors such as ECG leads in chest X-rays that often influence model classification decisions. In addition to being robust, our method achieves results comparable to the state-of-the-art. The source code and pre-trained weights are publicly available at (https://github.com/tomek1911/POTHER).



### Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch
- **Arxiv ID**: http://arxiv.org/abs/2201.09367v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09367v4)
- **Published**: 2022-01-23 21:09:59+00:00
- **Updated**: 2022-04-25 22:12:13+00:00
- **Authors**: Zhi Deng, Yang Liu, Hao Pan, Wassim Jabi, Juyong Zhang, Bailin Deng
- **Comment**: To appear in IEEE Transactions on Visualization and Computer Graphics
- **Journal**: None
- **Summary**: The freeform architectural modeling process often involves two important stages: concept design and digital modeling. In the first stage, architects usually sketch the overall 3D shape and the panel layout on a physical or digital paper briefly. In the second stage, a digital 3D model is created using the sketch as a reference. The digital model needs to incorporate geometric requirements for its components, such as the planarity of panels due to consideration of construction costs, which can make the modeling process more challenging. In this work, we present a novel sketch-based system to bridge the concept design and digital modeling of freeform roof-like shapes represented as planar quadrilateral (PQ) meshes. Our system allows the user to sketch the surface boundary and contour lines under axonometric projection and supports the sketching of occluded regions. In addition, the user can sketch feature lines to provide directional guidance to the PQ mesh layout. Given the 2D sketch input, we propose a deep neural network to infer in real-time the underlying surface shape along with a dense conjugate direction field, both of which are used to extract the final PQ mesh. To train and validate our network, we generate a large synthetic dataset that mimics architect sketching of freeform quadrilateral patches. The effectiveness and usability of our system are demonstrated with quantitative and qualitative evaluation as well as user studies.



### Unsupervised Severely Deformed Mesh Reconstruction (DMR) from a Single-View Image
- **Arxiv ID**: http://arxiv.org/abs/2201.09373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09373v1)
- **Published**: 2022-01-23 21:46:30+00:00
- **Updated**: 2022-01-23 21:46:30+00:00
- **Authors**: Jie Mei, Jingxi Yu, Suzanne Romain, Craig Rose, Kelsey Magrane, Graeme LeeSon, Jenq-Neng Hwang
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Much progress has been made in the supervised learning of 3D reconstruction of rigid objects from multi-view images or a video. However, it is more challenging to reconstruct severely deformed objects from a single-view RGB image in an unsupervised manner. Although training-based methods, such as specific category-level training, have been shown to successfully reconstruct rigid objects and slightly deformed objects like birds from a single-view image, they cannot effectively handle severely deformed objects and neither can be applied to some downstream tasks in the real world due to the inconsistent semantic meaning of vertices, which are crucial in defining the adopted 3D templates of objects to be reconstructed. In this work, we introduce a template-based method to infer 3D shapes from a single-view image and apply the reconstructed mesh to a downstream task, i.e., absolute length measurement. Without using 3D ground truth, our method faithfully reconstructs 3D meshes and achieves state-of-the-art accuracy in a length measurement task on a severely deformed fish dataset.



### ReconFormer: Accelerated MRI Reconstruction Using Recurrent Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.09376v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09376v2)
- **Published**: 2022-01-23 21:58:19+00:00
- **Updated**: 2022-01-28 01:44:51+00:00
- **Authors**: Pengfei Guo, Yiqun Mei, Jinyuan Zhou, Shanshan Jiang, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Accelerating magnetic resonance image (MRI) reconstruction process is a challenging ill-posed inverse problem due to the excessive under-sampling operation in k-space. In this paper, we propose a recurrent transformer model, namely ReconFormer, for MRI reconstruction which can iteratively reconstruct high fertility magnetic resonance images from highly under-sampled k-space data. In particular, the proposed architecture is built upon Recurrent Pyramid Transformer Layers (RPTL), which jointly exploits intrinsic multi-scale information at every architecture unit as well as the dependencies of the deep feature correlation through recurrent states. Moreover, the proposed ReconFormer is lightweight since it employs the recurrent structure for its parameter efficiency. We validate the effectiveness of ReconFormer on multiple datasets with different magnetic resonance sequences and show that it achieves significant improvements over the state-of-the-art methods with better parameter efficiency. Implementation code will be available in https://github.com/guopengf/ReconFormer.



### vCLIMB: A Novel Video Class Incremental Learning Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2201.09381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09381v2)
- **Published**: 2022-01-23 22:14:17+00:00
- **Updated**: 2022-04-06 05:25:45+00:00
- **Authors**: Andrs Villa, Kumail Alhamoud, Juan Len Alczar, Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem
- **Comment**: An updated version of our CVPR 2022 paper (oral); v2 adds minor text
  changes. The code of our benchmark can be found at:
  https://vclimb.netlify.app/
- **Journal**: None
- **Summary**: Continual learning (CL) is under-explored in the video domain. The few existing works contain splits with imbalanced class distributions over the tasks, or study the problem in unsuitable datasets. We introduce vCLIMB, a novel video continual learning benchmark. vCLIMB is a standardized test-bed to analyze catastrophic forgetting of deep models in video continual learning. In contrast to previous work, we focus on class incremental continual learning with models trained on a sequence of disjoint tasks, and distribute the number of classes uniformly across the tasks. We perform in-depth evaluations of existing CL methods in vCLIMB, and observe two unique challenges in video data. The selection of instances to store in episodic memory is performed at the frame level. Second, untrimmed training data influences the effectiveness of frame sampling strategies. We address these two challenges by proposing a temporal consistency regularization that can be applied on top of memory-based continual learning methods. Our approach significantly improves the baseline, by up to 24% on the untrimmed continual learning task.



### A Comprehensive Survey on Federated Learning: Concept and Applications
- **Arxiv ID**: http://arxiv.org/abs/2201.09384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09384v1)
- **Published**: 2022-01-23 22:33:23+00:00
- **Updated**: 2022-01-23 22:33:23+00:00
- **Authors**: Dhurgham Hassan Mahlool, Mohammed Hamzah Abed
- **Comment**: None
- **Journal**: Lecture Notes on Data Engineering and Communications Technologies
  2022
- **Summary**: This paper provides a comprehensive study of Federated Learning (FL) with an emphasis on components, challenges, applications and FL environment. FL can be applicable in multiple fields and domains in real-life models. in the medical system, the privacy of patients records and their medical condition is critical data, therefore collaborative learning or federated learning comes into the picture. On other hand build an intelligent system assist the medical staff without sharing the data lead into the FL concept and one of the applications that are used is a brain tumor diagnosis intelligent system based on AI methods that can efficiently work in a collaborative environment.this paper will introduce some of the applications and related work in the medical field and work under the FL concept then summarize them to introduce the main limitations of their work.



### A Survey on Patients Privacy Protection with Stganography and Visual Encryption
- **Arxiv ID**: http://arxiv.org/abs/2201.09388v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.09388v1)
- **Published**: 2022-01-23 22:44:45+00:00
- **Updated**: 2022-01-23 22:44:45+00:00
- **Authors**: Hussein K. Alzubaidy, Dhiah Al-Shammary, Mohammed Hamzah Abed
- **Comment**: None
- **Journal**: None
- **Summary**: In this survey, thirty models for steganography and visual encryption methods have been discussed to provide patients privacy protection.



### AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.09390v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09390v3)
- **Published**: 2022-01-23 22:48:36+00:00
- **Updated**: 2022-09-12 11:47:55+00:00
- **Authors**: Dmitrijs Kass, Ekta Vats
- **Comment**: 15th IAPR International Workshop on Document Analysis System (DAS)
- **Journal**: None
- **Summary**: This work proposes an attention-based sequence-to-sequence model for handwritten word recognition and explores transfer learning for data-efficient training of HTR systems. To overcome training data scarcity, this work leverages models pre-trained on scene text images as a starting point towards tailoring the handwriting recognition models. ResNet feature extraction and bidirectional LSTM-based sequence modeling stages together form an encoder. The prediction stage consists of a decoder and a content-based attention mechanism. The effectiveness of the proposed end-to-end HTR system has been empirically evaluated on a novel multi-writer dataset Imgur5K and the IAM dataset. The experimental results evaluate the performance of the HTR framework, further supported by an in-depth analysis of the error cases. Source code and pre-trained models are available at https://github.com/dmitrijsk/AttentionHTR.



### MISeval: a Metric Library for Medical Image Segmentation Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2201.09395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09395v1)
- **Published**: 2022-01-23 23:06:47+00:00
- **Updated**: 2022-01-23 23:06:47+00:00
- **Authors**: Dominik Mller, Dennis Hartmann, Philip Meyer, Florian Auer, Iaki Soto-Rey, Frank Kramer
- **Comment**: None
- **Journal**: None
- **Summary**: Correct performance assessment is crucial for evaluating modern artificial intelligence algorithms in medicine like deep-learning based medical image segmentation models. However, there is no universal metric library in Python for standardized and reproducible evaluation. Thus, we propose our open-source publicly available Python package MISeval: a metric library for Medical Image Segmentation Evaluation. The implemented metrics can be intuitively used and easily integrated into any performance assessment pipeline. The package utilizes modern CI/CD strategies to ensure functionality and stability. MISeval is available from PyPI (miseval) and GitHub: https://github.com/frankkramer-lab/miseval.



### Dynamic Label Assignment for Object Detection by Combining Predicted IoUs and Anchor IoUs
- **Arxiv ID**: http://arxiv.org/abs/2201.09396v2
- **DOI**: 10.3390/jimaging8070193
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09396v2)
- **Published**: 2022-01-23 23:14:07+00:00
- **Updated**: 2022-07-11 23:42:10+00:00
- **Authors**: Tianxiao Zhang, Bo Luo, Ajay Sharda, Guanghui Wang
- **Comment**: None
- **Journal**: Journal of Imaging 2022, 8(7), 193
- **Summary**: Label assignment plays a significant role in modern object detection models. Detection models may yield totally different performances with different label assignment strategies. For anchor-based detection models, the IoU (Intersection over Union) threshold between the anchors and their corresponding ground truth bounding boxes is the key element since the positive samples and negative samples are divided by the IoU threshold. Early object detectors simply utilize the fixed threshold for all training samples, while recent detection algorithms focus on adaptive thresholds based on the distribution of the IoUs to the ground truth boxes. In this paper, we introduce a simple while effective approach to perform label assignment dynamically based on the training status with predictions. By introducing the predictions in label assignment, more high-quality samples with higher IoUs to the ground truth objects are selected as the positive samples, which could reduce the discrepancy between the classification scores and the IoU scores, and generate more high-quality boundary boxes. Our approach shows improvements in the performance of the detection models with the adaptive label assignment algorithm and lower bounding box losses for those positive samples, indicating more samples with higher-quality predicted boxes are selected as positives.



### Fast MRI Reconstruction: How Powerful Transformers Are?
- **Arxiv ID**: http://arxiv.org/abs/2201.09400v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09400v2)
- **Published**: 2022-01-23 23:41:48+00:00
- **Updated**: 2022-04-04 17:02:50+00:00
- **Authors**: Jiahao Huang, Yinzhe Wu, Huanjun Wu, Guang Yang
- **Comment**: 5 pages, 5 figures, EMBC 2022
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is a widely used non-radiative and non-invasive method for clinical interrogation of organ structures and metabolism, with an inherently long scanning time. Methods by k-space undersampling and deep learning based reconstruction have been popularised to accelerate the scanning process. This work focuses on investigating how powerful transformers are for fast MRI by exploiting and comparing different novel network architectures. In particular, a generative adversarial network (GAN) based Swin transformer (ST-GAN) was introduced for the fast MRI reconstruction. To further preserve the edge and texture information, edge enhanced GAN based Swin transformer (EES-GAN) and texture enhanced GAN based Swin transformer (TES-GAN) were also developed, where a dual-discriminator GAN structure was applied. We compared our proposed GAN based transformers, standalone Swin transformer and other convolutional neural networks based GAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that transformers work well for the MRI reconstruction from different undersampling conditions. The utilisation of GAN's adversarial structure improves the quality of images reconstructed when undersampled for 30% or higher. The code is publicly available at https://github.com/ayanglab/SwinGANMR.




# Arxiv Papers in cs.CV on 2022-01-04
### External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2201.00942v1
- **DOI**: 10.1109/TMI.2021.3139637
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00942v1)
- **Published**: 2022-01-04 02:35:56+00:00
- **Updated**: 2022-01-04 02:35:56+00:00
- **Authors**: Yuyin Zhou, David Dreizin, Yan Wang, Fengze Liu, Wei Shen, Alan L. Yuille
- **Comment**: IEEE TMI
- **Journal**: None
- **Summary**: The spleen is one of the most commonly injured solid organs in blunt abdominal trauma. The development of automatic segmentation systems from multi-phase CT for splenic vascular injury can augment severity grading for improving clinical decision support and outcome prediction. However, accurate segmentation of splenic vascular injury is challenging for the following reasons: 1) Splenic vascular injury can be highly variant in shape, texture, size, and overall appearance; and 2) Data acquisition is a complex and expensive procedure that requires intensive efforts from both data scientists and radiologists, which makes large-scale well-annotated datasets hard to acquire in general.   In light of these challenges, we hereby design a novel framework for multi-phase splenic vascular injury segmentation, especially with limited data. On the one hand, we propose to leverage external data to mine pseudo splenic masks as the spatial attention, dubbed external attention, for guiding the segmentation of splenic vascular injury. On the other hand, we develop a synthetic phase augmentation module, which builds upon generative adversarial networks, for populating the internal data by fully leveraging the relation between different phases. By jointly enforcing external attention and populating internal data representation during training, our proposed method outperforms other competing methods and substantially improves the popular DeepLab-v3+ baseline by more than 7% in terms of average DSC, which confirms its effectiveness.



### HWRCNet: Handwritten Word Recognition in JPEG Compressed Domain using CNN-BiLSTM Network
- **Arxiv ID**: http://arxiv.org/abs/2201.00947v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00947v3)
- **Published**: 2022-01-04 02:52:56+00:00
- **Updated**: 2023-02-17 06:56:06+00:00
- **Authors**: Bulla Rajesh, Abhishek Kumar Gupta, Ayush Raj, Mohammed Javed, Shiv Ram Dubey
- **Comment**: Accepted in International Conference on Data Analytics and Learning,
  2022
- **Journal**: None
- **Summary**: Handwritten word recognition from document images using deep learning is an active research area in the field of Document Image Analysis and Recognition. In the present era of Big data, since more and more documents are being generated and archived in the compressed form to provide better storage and transmission efficiencies, the problem of word recognition in the respective compressed domain without decompression becomes very challenging. The traditional methods employ decompression and then apply learning algorithms over them, therefore, novel algorithms are to be designed in order to apply learning techniques directly in the compressed representations/domains. In this direction, this research paper proposes a novel HWRCNet model for handwritten word recognition directly in the compressed domain specifically focusing on JPEG format. The proposed model combines the Convolutional Neural Network (CNN) and Bi-Directional Long Short Term Memory (BiLSTM) based Recurrent Neural Network (RNN). Basically, we train the model using JPEG compressed word images and observe a very appealing performance with $89.05\%$ word recognition accuracy and $13.37\%$ character error rate.



### Stain Normalized Breast Histopathology Image Recognition using Convolutional Neural Networks for Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.00957v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00957v1)
- **Published**: 2022-01-04 03:09:40+00:00
- **Updated**: 2022-01-04 03:09:40+00:00
- **Authors**: Sruthi Krishna, Suganthi S. S, Shivsubramani Krishnamoorthy, Arnav Bhavsar
- **Comment**: 26 pages, 11 figures
- **Journal**: None
- **Summary**: Computer assisted diagnosis in digital pathology is becoming ubiquitous as it can provide more efficient and objective healthcare diagnostics. Recent advances have shown that the convolutional Neural Network (CNN) architectures, a well-established deep learning paradigm, can be used to design a Computer Aided Diagnostic (CAD) System for breast cancer detection. However, the challenges due to stain variability and the effect of stain normalization with such deep learning frameworks are yet to be well explored. Moreover, performance analysis with arguably more efficient network models, which may be important for high throughput screening, is also not well explored.To address this challenge, we consider some contemporary CNN models for binary classification of breast histopathology images that involves (1) the data preprocessing with stain normalized images using an adaptive colour deconvolution (ACD) based color normalization algorithm to handle the stain variabilities; and (2) applying transfer learning based training of some arguably more efficient CNN models, namely Visual Geometry Group Network (VGG16), MobileNet and EfficientNet. We have validated the trained CNN networks on a publicly available BreaKHis dataset, for 200x and 400x magnified histopathology images. The experimental analysis shows that pretrained networks in most cases yield better quality results on data augmented breast histopathology images with stain normalization, than the case without stain normalization. Further, we evaluated the performance and efficiency of popular lightweight networks using stain normalized images and found that EfficientNet outperforms VGG16 and MobileNet in terms of test accuracy and F1 Score. We observed that efficiency in terms of test time is better in EfficientNet than other networks; VGG Net, MobileNet, without much drop in the classification accuracy.



### AI visualization in Nanoscale Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2201.00966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, I.2.0; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2201.00966v1)
- **Published**: 2022-01-04 04:04:50+00:00
- **Updated**: 2022-01-04 04:04:50+00:00
- **Authors**: Rajagopal A, Nirmala V, Andrew J, Arun Muthuraj Vedamanickam.
- **Comment**: Best paper award at International Conference On Big Data, Machine
  Learning and Applications 2021. http://bigdml.nits.ac.in/ In Springer
  Proceedings 2021
- **Journal**: None
- **Summary**: Artificial Intelligence & Nanotechnology are promising areas for the future of humanity. While Deep Learning based Computer Vision has found applications in many fields from medicine to automotive, its application in nanotechnology can open doors for new scientific discoveries. Can we apply AI to explore objects that our eyes can't see such as nano scale sized objects? An AI platform to visualize nanoscale patterns learnt by a Deep Learning neural network can open new frontiers for nanotechnology. The objective of this paper is to develop a Deep Learning based visualization system on images of nanomaterials obtained by scanning electron microscope. This paper contributes an AI platform to enable any nanoscience researcher to use AI in visual exploration of nanoscale morphologies of nanomaterials. This AI is developed by a technique of visualizing intermediate activations of a Convolutional AutoEncoder. In this method, a nano scale specimen image is transformed into its feature representations by a Convolution Neural Network. The Convolutional AutoEncoder is trained on 100% SEM dataset, and then CNN visualization is applied. This AI generates various conceptual feature representations of the nanomaterial.   While Deep Learning based image classification of SEM images are widely published in literature, there are not much publications that have visualized Deep neural networks of nanomaterials. There is a significant opportunity to gain insights from the learnings extracted by machine learning. This paper unlocks the potential of applying Deep Learning based Visualization on electron microscopy to offer AI extracted features and architectural patterns of various nanomaterials. This is a contribution in Explainable AI in nano scale objects. This paper contributes an open source AI with reproducible results at URL (https://sites.google.com/view/aifornanotechnology)



### Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety
- **Arxiv ID**: http://arxiv.org/abs/2201.00969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2201.00969v1)
- **Published**: 2022-01-04 04:21:07+00:00
- **Updated**: 2022-01-04 04:21:07+00:00
- **Authors**: Rajagopal A, Nirmala V, Arun Muthuraj Vedamanickam
- **Comment**: In Springer Proceedings. International Conference On Big Data,
  Machine Learning and Applications 2021. http://bigdml.nits.ac.in/
- **Journal**: None
- **Summary**: There is amazing progress in Deep Learning based models for Image captioning and Low Light image enhancement. For the first time in literature, this paper develops a Deep Learning model that translates night scenes to sentences, opening new possibilities for AI applications in the safety of visually impaired women. Inspired by Image Captioning and Visual Question Answering, a novel Interactive Image Captioning is developed. A user can make the AI focus on any chosen person of interest by influencing the attention scoring. Attention context vectors are computed from CNN feature vectors and user-provided start word. The Encoder-Attention-Decoder neural network learns to produce captions from low brightness images. This paper demonstrates how women safety can be enabled by researching a novel AI capability in the Interactive Vision-Language model for perception of the environment in the night.



### StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams
- **Arxiv ID**: http://arxiv.org/abs/2201.00975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.00975v1)
- **Published**: 2022-01-04 04:44:05+00:00
- **Updated**: 2022-01-04 04:44:05+00:00
- **Authors**: Chengxi Li, Brent Harrison
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we build two automatic evaluation metrics for evaluating the association between a machine-generated caption and a ground truth stylized caption: OnlyStyle and StyleCIDEr.



### Underwater Object Classification and Detection: first results and open challenges
- **Arxiv ID**: http://arxiv.org/abs/2201.00977v1
- **DOI**: 10.1109/OCEANSChennai45887.2022.9775417
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.00977v1)
- **Published**: 2022-01-04 04:54:08+00:00
- **Updated**: 2022-01-04 04:54:08+00:00
- **Authors**: Andre Jesus, Claudio Zito, Claudio Tortorici, Eloy Roura, Giulia De Masi
- **Comment**: None
- **Journal**: In Proceedings of OCEANS 2022 Chennai, February 21-24, pp. 1-6
- **Summary**: This work reviews the problem of object detection in underwater environments. We analyse and quantify the shortcomings of conventional state-of-the-art (SOTA) algorithms in the computer vision community when applied to this challenging environment, as well as providing insights and general guidelines for future research efforts. First, we assessed if pretraining with the conventional ImageNet is beneficial when the object detector needs to be applied to environments that may be characterised by a different feature distribution. We then investigate whether two-stage detectors yields to better performance with respect to single-stage detectors, in terms of accuracy, intersection of union (IoU), floating operation per second (FLOPS), and inference time. Finally, we assessed the generalisation capability of each model to a lower quality dataset to simulate performance on a real scenario, in which harsher conditions ought to be expected. Our experimental results provide evidence that underwater object detection requires searching for "ad-hoc" architectures than merely training SOTA architectures on new data, and that pretraining is not beneficial.



### PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture
- **Arxiv ID**: http://arxiv.org/abs/2201.00978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00978v1)
- **Published**: 2022-01-04 04:56:57+00:00
- **Updated**: 2022-01-04 04:56:57+00:00
- **Authors**: Kai Han, Jianyuan Guo, Yehui Tang, Yunhe Wang
- **Comment**: Tech Report. An extension of "Transformer in Transformer"
  (arXiv:2103.00112)
- **Journal**: None
- **Summary**: Transformer networks have achieved great progress for computer vision tasks. Transformer-in-Transformer (TNT) architecture utilizes inner transformer and outer transformer to extract both local and global representations. In this work, we present new TNT baselines by introducing two advanced designs: 1) pyramid architecture, and 2) convolutional stem. The new "PyramidTNT" significantly improves the original TNT by establishing hierarchical representations. PyramidTNT achieves better performances than the previous state-of-the-art vision transformers such as Swin Transformer. We hope this new baseline will be helpful to the further research and application of vision transformer. Code will be available at https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch.



### Variational Stacked Local Attention Networks for Diverse Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2201.00985v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.00985v1)
- **Published**: 2022-01-04 05:14:34+00:00
- **Updated**: 2022-01-04 05:14:34+00:00
- **Authors**: Tonmoay Deb, Akib Sadmanee, Kishor Kumar Bhaumik, Amin Ahsan Ali, M Ashraful Amin, A K M Mahbubur Rahman
- **Comment**: To be published in Winter Conference on Applications of Computer
  Vision 2022
- **Journal**: None
- **Summary**: While describing Spatio-temporal events in natural language, video captioning models mostly rely on the encoder's latent visual representation. Recent progress on the encoder-decoder model attends encoder features mainly in linear interaction with the decoder. However, growing model complexity for visual data encourages more explicit feature interaction for fine-grained information, which is currently absent in the video captioning domain. Moreover, feature aggregations methods have been used to unveil richer visual representation, either by the concatenation or using a linear layer. Though feature sets for a video semantically overlap to some extent, these approaches result in objective mismatch and feature redundancy. In addition, diversity in captions is a fundamental component of expressing one event from several meaningful perspectives, currently missing in the temporal, i.e., video captioning domain. To this end, we propose Variational Stacked Local Attention Network (VSLAN), which exploits low-rank bilinear pooling for self-attentive feature interaction and stacking multiple video feature streams in a discount fashion. Each feature stack's learned attributes contribute to our proposed diversity encoding module, followed by the decoding query stage to facilitate end-to-end diverse and natural captions without any explicit supervision on attributes. We evaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity. The CIDEr score of VSLAN outperforms current off-the-shelf methods by $7.8\%$ on MSVD and $4.5\%$ on MSR-VTT, respectively. On the same datasets, VSLAN achieves competitive results in caption diversity metrics.



### Attention Mechanism Meets with Hybrid Dense Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.01001v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01001v1)
- **Published**: 2022-01-04 06:30:24+00:00
- **Updated**: 2022-01-04 06:30:24+00:00
- **Authors**: Muhammad Ahmad, Adil Mehmood Khan, Manuel Mazzara, Salvatore Distefano, Swalpa Kumar Roy, Xin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) are more suitable, indeed. However, fixed kernel sizes make traditional CNN too specific, neither flexible nor conducive to feature learning, thus impacting on the classification accuracy. The convolution of different kernel size networks may overcome this problem by capturing more discriminating and relevant information. In light of this, the proposed solution aims at combining the core idea of 3D and 2D Inception net with the Attention mechanism to boost the HSIC CNN performance in a hybrid scenario. The resulting \textit{attention-fused hybrid network} (AfNet) is based on three attention-fused parallel hybrid sub-nets with different kernels in each block repeatedly using high-level features to enhance the final ground-truth maps. In short, AfNet is able to selectively filter out the discriminative features critical for classification. Several tests on HSI datasets provided competitive results for AfNet compared to state-of-the-art models. The proposed pipeline achieved, indeed, an overall accuracy of 97\% for the Indian Pines, 100\% for Botswana, 99\% for Pavia University, Pavia Center, and Salinas datasets.



### Multi-Representation Adaptation Network for Cross-domain Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.01002v1
- **DOI**: 10.1016/j.neunet.2019.07.010
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.01002v1)
- **Published**: 2022-01-04 06:34:48+00:00
- **Updated**: 2022-01-04 06:34:48+00:00
- **Authors**: Yongchun Zhu, Fuzhen Zhuang, Jindong Wang, Jingwu Chen, Zhiping Shi, Wenjuan Wu, Qing He
- **Comment**: Neural Networks regular paper. Transfer Learning, Domain Adaptation
- **Journal**: None
- **Summary**: In image classification, it is often expensive and time-consuming to acquire sufficient labels. To solve this problem, domain adaptation often provides an attractive option given a large amount of labeled data from a similar nature but different domain. Existing approaches mainly align the distributions of representations extracted by a single structure and the representations may only contain partial information, e.g., only contain part of the saturation, brightness, and hue information. Along this line, we propose Multi-Representation Adaptation which can dramatically improve the classification accuracy for cross-domain image classification and specially aims to align the distributions of multiple representations extracted by a hybrid structure named Inception Adaptation Module (IAM). Based on this, we present Multi-Representation Adaptation Network (MRAN) to accomplish the cross-domain image classification task via multi-representation alignment which can capture the information from different aspects. In addition, we extend Maximum Mean Discrepancy (MMD) to compute the adaptation loss. Our approach can be easily implemented by extending most feed-forward models with IAM, and the network can be trained efficiently via back-propagation. Experiments conducted on three benchmark image datasets demonstrate the effectiveness of MRAN. The code has been available at https://github.com/easezyc/deep-transfer-learning.



### Aligning Domain-specific Distribution and Classifier for Cross-domain Classification from Multiple Sources
- **Arxiv ID**: http://arxiv.org/abs/2201.01003v1
- **DOI**: 10.1609/aaai.v33i01.33015989
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01003v1)
- **Published**: 2022-01-04 06:35:11+00:00
- **Updated**: 2022-01-04 06:35:11+00:00
- **Authors**: Yongchun Zhu, Fuzhen Zhuang, Deqing Wang
- **Comment**: AAAI 2019 long paper. Multi-source Domain Adaptation
- **Journal**: None
- **Summary**: While Unsupervised Domain Adaptation (UDA) algorithms, i.e., there are only labeled data from source domains, have been actively studied in recent years, most algorithms and theoretical results focus on Single-source Unsupervised Domain Adaptation (SUDA). However, in the practical scenario, labeled data can be typically collected from multiple diverse sources, and they might be different not only from the target domain but also from each other. Thus, domain adapters from multiple sources should not be modeled in the same way. Recent deep learning based Multi-source Unsupervised Domain Adaptation (MUDA) algorithms focus on extracting common domain-invariant representations for all domains by aligning distribution of all pairs of source and target domains in a common feature space. However, it is often very hard to extract the same domain-invariant representations for all domains in MUDA. In addition, these methods match distributions without considering domain-specific decision boundaries between classes. To solve these problems, we propose a new framework with two alignment stages for MUDA which not only respectively aligns the distributions of each pair of source and target domains in multiple specific feature spaces, but also aligns the outputs of classifiers by utilizing the domain-specific decision boundaries. Extensive experiments demonstrate that our method can achieve remarkable results on popular benchmark datasets for image classification.



### Learning to Generate Novel Classes for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.01008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.01008v1)
- **Published**: 2022-01-04 06:55:19+00:00
- **Updated**: 2022-01-04 06:55:19+00:00
- **Authors**: Kyungmoon Lee, Sungyeon Kim, Seunghoon Hong, Suha Kwak
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Deep metric learning aims to learn an embedding space where the distance between data reflects their class equivalence, even when their classes are unseen during training. However, the limited number of classes available in training precludes generalization of the learned embedding space. Motivated by this, we introduce a new data augmentation approach that synthesizes novel classes and their embedding vectors. Our approach can provide rich semantic information to an embedding model and improve its generalization by augmenting training data with novel classes unavailable in the original data. We implement this idea by learning and exploiting a conditional generative model, which, given a class label and a noise, produces a random embedding vector of the class. Our proposed generator allows the loss to use richer class relations by augmenting realistic and diverse classes, resulting in better generalization to unseen samples. Experimental results on public benchmark datasets demonstrate that our method clearly enhances the performance of proxy-based losses.



### Local Motion and Contrast Priors Driven Deep Network for Infrared Small Target Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.01014v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01014v5)
- **Published**: 2022-01-04 07:20:46+00:00
- **Updated**: 2023-04-04 14:25:10+00:00
- **Authors**: Xinyi Ying, Yingqian Wang, Longguang Wang, Weidong Sheng, Li Liu, Zaiping Lin, Shilin Zhou
- **Comment**: None
- **Journal**: JSTARS 2022
- **Summary**: Infrared small target super-resolution (SR) aims to recover reliable and detailed high-resolution image with high-contrast targets from its low-resolution counterparts. Since the infrared small target lacks color and fine structure information, it is significant to exploit the supplementary information among sequence images to enhance the target. In this paper, we propose the first infrared small target SR method named local motion and contrast prior driven deep network (MoCoPnet) to integrate the domain knowledge of infrared small target into deep network, which can mitigate the intrinsic feature scarcity of infrared small targets. Specifically, motivated by the local motion prior in the spatio-temporal dimension, we propose a local spatio-temporal attention module to perform implicit frame alignment and incorporate the local spatio-temporal information to enhance the local features (especially for small targets). Motivated by the local contrast prior in the spatial dimension, we propose a central difference residual group to incorporate the central difference convolution into the feature extraction backbone, which can achieve center-oriented gradient-aware feature extraction to further improve the target contrast. Extensive experiments have demonstrated that our method can recover accurate spatial dependency and improve the target contrast. Comparative results show that MoCoPnet can outperform the state-of-the-art video SR and single image SR methods in terms of both SR performance and target enhancement. Based on the SR results, we further investigate the influence of SR on infrared small target detection and the experimental results demonstrate that MoCoPnet promotes the detection performance. The code is available at https://github.com/XinyiYing/MoCoPnet.



### Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function
- **Arxiv ID**: http://arxiv.org/abs/2201.01016v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01016v2)
- **Published**: 2022-01-04 07:24:58+00:00
- **Updated**: 2022-05-05 05:34:55+00:00
- **Authors**: Yunze Xiao, Hao Zhu, Haotian Yang, Zhengyu Diao, Xiangju Lu, Xun Cao
- **Comment**: AAAI 2022 Oral, updated to camera ready version
- **Journal**: None
- **Summary**: Recovering detailed facial geometry from a set of calibrated multi-view images is valuable for its wide range of applications. Traditional multi-view stereo (MVS) methods adopt an optimization-based scheme to regularize the matching cost. Recently, learning-based methods integrate all these into an end-to-end neural network and show superiority of efficiency. In this paper, we propose a novel architecture to recover extremely detailed 3D faces within dozens of seconds. Unlike previous learning-based methods that regularize the cost volume via 3D CNN, we propose to learn an implicit function for regressing the matching cost. By fitting a 3D morphable model from multi-view images, the features of multiple images are extracted and aggregated in the mesh-attached UV space, which makes the implicit function more effective in recovering detailed facial shape. Our method outperforms SOTA learning-based MVS in accuracy by a large margin on the FaceScape dataset. The code and data are released in https://github.com/zhuhao-nju/mvfr.



### Weakly-supervised continual learning for class-incremental segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.01029v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01029v2)
- **Published**: 2022-01-04 08:13:59+00:00
- **Updated**: 2022-06-15 08:32:05+00:00
- **Authors**: Gaston Lenczner, Adrien Chan-Hon-Tong, Nicola Luminari, Bertrand Le Saux
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning is a powerful way to adapt existing deep learning models to new emerging use-cases in remote sensing. Starting from a neural network already trained for semantic segmentation, we propose to modify its label space to swiftly adapt it to new classes under weak supervision. To alleviate the background shift and the catastrophic forgetting problems inherent to this form of continual learning, we compare different regularization terms and leverage a pseudo-label strategy. We experimentally show the relevance of our approach on three public remote sensing datasets. Code is open-source and released in this repository: https://github.com/alteia-ai/ICSS}{https://github.com/alteia-ai/ICSS.



### A Robust Visual Sampling Model Inspired by Receptive Field
- **Arxiv ID**: http://arxiv.org/abs/2201.01030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01030v1)
- **Published**: 2022-01-04 08:14:56+00:00
- **Updated**: 2022-01-04 08:14:56+00:00
- **Authors**: Liwen Hu, Lei Ma, Dawei Weng, Tiejun Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Spike camera mimicking the retina fovea can report per-pixel luminance intensity accumulation by firing spikes. As a bio-inspired vision sensor with high temporal resolution, it has a huge potential for computer vision. However, the sampling model in current Spike camera is so susceptible to quantization and noise that it cannot capture the texture details of objects effectively. In this work, a robust visual sampling model inspired by receptive field (RVSM) is proposed where wavelet filter generated by difference of Gaussian (DoG) and Gaussian filter are used to simulate receptive field. Using corresponding method similar to inverse wavelet transform, spike data from RVSM can be converted into images. To test the performance, we also propose a high-speed motion spike dataset (HMD) including a variety of motion scenes. By comparing reconstructed images in HMD, we find RVSM can improve the ability of capturing information of Spike camera greatly. More importantly, due to mimicking receptive field mechanism to collect regional information, RVSM can filter high intensity noise effectively and improves the problem that Spike camera is sensitive to noise largely. Besides, due to the strong generalization of sampling structure, RVSM is also suitable for other neuromorphic vision sensor. Above experiments are finished in a Spike camera simulator.



### What Hinders Perceptual Quality of PSNR-oriented Methods?
- **Arxiv ID**: http://arxiv.org/abs/2201.01034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2201.01034v1)
- **Published**: 2022-01-04 08:30:09+00:00
- **Updated**: 2022-01-04 08:30:09+00:00
- **Authors**: Tianshuo Xu, Peng Mi, Xiawu Zheng, Lijiang Li, Fei Chao, Guannan Jiang, Wei Zhang, Yiyi Zhou, Rongrong Ji
- **Comment**: 10 pages,7 figures
- **Journal**: None
- **Summary**: In this paper, we discover two factors that inhibit POMs from achieving high perceptual quality: 1) center-oriented optimization (COO) problem and 2) model's low-frequency tendency. First, POMs tend to generate an SR image whose position in the feature space is closest to the distribution center of all potential high-resolution (HR) images, resulting in such POMs losing high-frequency details. Second, $90\%$ area of an image consists of low-frequency signals; in contrast, human perception relies on an image's high-frequency details. However, POMs apply the same calculation to process different-frequency areas, so that POMs tend to restore the low-frequency regions. Based on these two factors, we propose a Detail Enhanced Contrastive Loss (DECLoss), by combining a high-frequency enhancement module and spatial contrastive learning module, to reduce the influence of the COO problem and low-Frequency tendency. Experimental results show the efficiency and effectiveness when applying DECLoss on several regular SR models. E.g, in EDSR, our proposed method achieves 3.60$\times$ faster learning speed compared to a GAN-based method with a subtle degradation in visual quality. In addition, our final results show that an SR network equipped with our DECLoss generates more realistic and visually pleasing textures compared to state-of-the-art methods. %The source code of the proposed method is included in the supplementary material and will be made publicly available in the future.



### Sound and Visual Representation Learning with Multiple Pretraining Tasks
- **Arxiv ID**: http://arxiv.org/abs/2201.01046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.01046v1)
- **Published**: 2022-01-04 09:09:38+00:00
- **Updated**: 2022-01-04 09:09:38+00:00
- **Authors**: Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Different self-supervised tasks (SSL) reveal different features from the data. The learned feature representations can exhibit different performance for each downstream task. In this light, this work aims to combine Multiple SSL tasks (Multi-SSL) that generalizes well for all downstream tasks. Specifically, for this study, we investigate binaural sounds and image data in isolation. For binaural sounds, we propose three SSL tasks namely, spatial alignment, temporal synchronization of foreground objects and binaural audio and temporal gap prediction. We investigate several approaches of Multi-SSL and give insights into the downstream task performance on video retrieval, spatial sound super resolution, and semantic prediction on the OmniAudio dataset. Our experiments on binaural sound representations demonstrate that Multi-SSL via incremental learning (IL) of SSL tasks outperforms single SSL task models and fully supervised models in the downstream task performance. As a check of applicability on other modality, we also formulate our Multi-SSL models for image representation learning and we use the recently proposed SSL tasks, MoCov2 and DenseCL. Here, Multi-SSL surpasses recent methods such as MoCov2, DenseCL and DetCo by 2.06%, 3.27% and 1.19% on VOC07 classification and +2.83, +1.56 and +1.61 AP on COCO detection. Code will be made publicly available.



### DIAL: Deep Interactive and Active Learning for Semantic Segmentation in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2201.01047v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01047v1)
- **Published**: 2022-01-04 09:11:58+00:00
- **Updated**: 2022-01-04 09:11:58+00:00
- **Authors**: Gaston Lenczner, Adrien Chan-Hon-Tong, Bertrand Le Saux, Nicola Luminari, Guy Le Besnerais
- **Comment**: None
- **Journal**: None
- **Summary**: We propose in this article to build up a collaboration between a deep neural network and a human in the loop to swiftly obtain accurate segmentation maps of remote sensing images. In a nutshell, the agent iteratively interacts with the network to correct its initially flawed predictions. Concretely, these interactions are annotations representing the semantic labels. Our methodological contribution is twofold. First, we propose two interactive learning schemes to integrate user inputs into deep neural networks. The first one concatenates the annotations with the other network's inputs. The second one uses the annotations as a sparse ground-truth to retrain the network. Second, we propose an active learning strategy to guide the user towards the most relevant areas to annotate. To this purpose, we compare different state-of-the-art acquisition functions to evaluate the neural network uncertainty such as ConfidNet, entropy or ODIN. Through experiments on three remote sensing datasets, we show the effectiveness of the proposed methods. Notably, we show that active learning based on uncertainty estimation enables to quickly lead the user towards mistakes and that it is thus relevant to guide the user interventions.



### Towards Unsupervised Open World Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.01073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01073v2)
- **Published**: 2022-01-04 10:29:34+00:00
- **Updated**: 2022-09-12 14:35:00+00:00
- **Authors**: Svenja Uhlemeyer, Matthias Rottmann, Hanno Gottschalk
- **Comment**: UAI 2022, published in PMLR, Proceedings of the Thirty-Eighth
  Conference on Uncertainty in Artificial Intelligence
- **Journal**: None
- **Summary**: For the semantic segmentation of images, state-of-the-art deep neural networks (DNNs) achieve high segmentation accuracy if that task is restricted to a closed set of classes. However, as of now DNNs have limited ability to operate in an open world, where they are tasked to identify pixels belonging to unknown objects and eventually to learn novel classes, incrementally. Humans have the capability to say: I don't know what that is, but I've already seen something like that. Therefore, it is desirable to perform such an incremental learning task in an unsupervised fashion. We introduce a method where unknown objects are clustered based on visual similarity. Those clusters are utilized to define new classes and serve as training data for unsupervised incremental learning. More precisely, the connected components of a predicted semantic segmentation are assessed by a segmentation quality estimate. connected components with a low estimated prediction quality are candidates for a subsequent clustering. Additionally, the component-wise quality assessment allows for obtaining predicted segmentation masks for the image regions potentially containing unknown objects. The respective pixels of such masks are pseudo-labeled and afterwards used for re-training the DNN, i.e., without the use of ground truth generated by humans. In our experiments we demonstrate that, without access to ground truth and even with few data, a DNN's class space can be extended by a novel class, achieving considerable segmentation accuracy.



### Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.01080v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.01080v3)
- **Published**: 2022-01-04 10:58:59+00:00
- **Updated**: 2022-05-26 15:21:05+00:00
- **Authors**: Hui Liu, Bo Zhao, Yuefeng Peng, Weidong Li, Peng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are threatened by adversarial examples. Adversarial detection, which distinguishes adversarial images from benign images, is fundamental for robust DNN-based services. Image transformation is one of the most effective approaches to detect adversarial examples. During the last few years, a variety of image transformations have been studied and discussed to design reliable adversarial detectors. In this paper, we systematically synthesize the recent progress on adversarial detection via image transformations with a novel classification method. Then, we conduct extensive experiments to test the detection performance of image transformations against state-of-the-art adversarial attacks. Furthermore, we reveal that each individual transformation is not capable of detecting adversarial examples in a robust way, and propose a DNN-based approach referred to as \emph{AdvJudge}, which combines scores of 9 image transformations. Without knowing which individual scores are misleading or not misleading, AdvJudge can make the right judgment, and achieve a significant improvement in detection rate. Finally, we utilize an explainable AI tool to show the contribution of each image transformation to adversarial detection. Experimental results show that the contribution of image transformations to adversarial detection is significantly different, the combination of them can significantly improve the generic detection ability against state-of-the-art adversarial attacks.



### Identifying the exterior image of buildings on a 3D map and extracting elevation information using deep learning and digital image processing
- **Arxiv ID**: http://arxiv.org/abs/2201.01081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01081v1)
- **Published**: 2022-01-04 10:59:13+00:00
- **Updated**: 2022-01-04 10:59:13+00:00
- **Authors**: Donghwa Shon, Byeongjoon Noh, Nahyang Byun
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: Despite the fact that architectural administration information in Korea has been providing high-quality information for a long period of time, the level of utility of the information is not high because it focuses on administrative information. While this is the case, a three-dimensional (3D) map with higher resolution has emerged along with the technological development. However, it cannot function better than visual transmission, as it includes only image information focusing on the exterior of the building. If information related to the exterior of the building can be extracted or identified from a 3D map, it is expected that the utility of the information will be more valuable as the national architectural administration information can then potentially be extended to include such information regarding the building exteriors to the level of BIM(Building Information Modeling). This study aims to present and assess a basic method of extracting information related to the appearance of the exterior of a building for the purpose of 3D mapping using deep learning and digital image processing. After extracting and preprocessing images from the map, information was identified using the Fast R-CNN(Regions with Convolutional Neuron Networks) model. The information was identified using the Faster R-CNN model after extracting and preprocessing images from the map. As a result, it showed approximately 93% and 91% accuracy in terms of detecting the elevation and window parts of the building, respectively, as well as excellent performance in an experiment aimed at extracting the elevation information of the building. Nonetheless, it is expected that improved results will be obtained by supplementing the probability of mixing the false detection rate or noise data caused by the misunderstanding of experimenters in relation to the unclear boundaries of windows.



### Learning Quality-aware Representation for Multi-person Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2201.01087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01087v1)
- **Published**: 2022-01-04 11:10:28+00:00
- **Updated**: 2022-01-04 11:10:28+00:00
- **Authors**: Yabo Xiao, Dongdong Yu, Xiaojuan Wang, Lei Jin, Guoli Wang, Qian Zhang
- **Comment**: Accepted by AAAI2022; Slightly different compared with the
  camera-ready version
- **Journal**: None
- **Summary**: Off-the-shelf single-stage multi-person pose regression methods generally leverage the instance score (i.e., confidence of the instance localization) to indicate the pose quality for selecting the pose candidates. We consider that there are two gaps involved in existing paradigm:~1) The instance score is not well interrelated with the pose regression quality.~2) The instance feature representation, which is used for predicting the instance score, does not explicitly encode the structural pose information to predict the reasonable score that represents pose regression quality. To address the aforementioned issues, we propose to learn the pose regression quality-aware representation. Concretely, for the first gap, instead of using the previous instance confidence label (e.g., discrete {1,0} or Gaussian representation) to denote the position and confidence for person instance, we firstly introduce the Consistent Instance Representation (CIR) that unifies the pose regression quality score of instance and the confidence of background into a pixel-wise score map to calibrates the inconsistency between instance score and pose regression quality. To fill the second gap, we further present the Query Encoding Module (QEM) including the Keypoint Query Encoding (KQE) to encode the positional and semantic information for each keypoint and the Pose Query Encoding (PQE) which explicitly encodes the predicted structural pose information to better fit the Consistent Instance Representation (CIR). By using the proposed components, we significantly alleviate the above gaps. Our method outperforms previous single-stage regression-based even bottom-up methods and achieves the state-of-the-art result of 71.7 AP on MS COCO test-dev set.



### Short Range Correlation Transformer for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2201.01090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.01090v1)
- **Published**: 2022-01-04 11:12:39+00:00
- **Updated**: 2022-01-04 11:12:39+00:00
- **Authors**: Yunbin Zhao, Songhao Zhu, Dongsheng Wang, Zhiwei Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Occluded person re-identification is one of the challenging areas of computer vision, which faces problems such as inefficient feature representation and low recognition accuracy. Convolutional neural network pays more attention to the extraction of local features, therefore it is difficult to extract features of occluded pedestrians and the effect is not so satisfied. Recently, vision transformer is introduced into the field of re-identification and achieves the most advanced results by constructing the relationship of global features between patch sequences. However, the performance of vision transformer in extracting local features is inferior to that of convolutional neural network. Therefore, we design a partial feature transformer-based person re-identification framework named PFT. The proposed PFT utilizes three modules to enhance the efficiency of vision transformer. (1) Patch full dimension enhancement module. We design a learnable tensor with the same size as patch sequences, which is full-dimensional and deeply embedded in patch sequences to enrich the diversity of training samples. (2) Fusion and reconstruction module. We extract the less important part of obtained patch sequences, and fuse them with original patch sequence to reconstruct the original patch sequences. (3) Spatial Slicing Module. We slice and group patch sequences from spatial direction, which can effectively improve the short-range correlation of patch sequences. Experimental results over occluded and holistic re-identification datasets demonstrate that the proposed PFT network achieves superior performance consistently and outperforms the state-of-the-art methods.



### Towards Transferable Unrestricted Adversarial Examples with Minimum Changes
- **Arxiv ID**: http://arxiv.org/abs/2201.01102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01102v2)
- **Published**: 2022-01-04 12:03:20+00:00
- **Updated**: 2022-12-27 12:47:14+00:00
- **Authors**: Fangcheng Liu, Chao Zhang, Hongyang Zhang
- **Comment**: Accepted at SaTML 2023
- **Journal**: None
- **Summary**: Transfer-based adversarial example is one of the most important classes of black-box attacks. However, there is a trade-off between transferability and imperceptibility of the adversarial perturbation. Prior work in this direction often requires a fixed but large $\ell_p$-norm perturbation budget to reach a good transfer success rate, leading to perceptible adversarial perturbations. On the other hand, most of the current unrestricted adversarial attacks that aim to generate semantic-preserving perturbations suffer from weaker transferability to the target model. In this work, we propose a geometry-aware framework to generate transferable adversarial examples with minimum changes. Analogous to model selection in statistical machine learning, we leverage a validation model to select the best perturbation budget for each image under both the $\ell_{\infty}$-norm and unrestricted threat models. We propose a principled method for the partition of training and validation models by encouraging intra-group diversity while penalizing extra-group similarity. Extensive experiments verify the effectiveness of our framework on balancing imperceptibility and transferability of the crafted adversarial examples. The methodology is the foundation of our entry to the CVPR'21 Security AI Challenger: Unrestricted Adversarial Attacks on ImageNet, in which we ranked 1st place out of 1,559 teams and surpassed the runner-up submissions by 4.59% and 23.91% in terms of final score and average image quality level, respectively. Code is available at https://github.com/Equationliu/GA-Attack.



### Data Augmentation for Depression Detection Using Skeleton-Based Gait Information
- **Arxiv ID**: http://arxiv.org/abs/2201.01115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01115v1)
- **Published**: 2022-01-04 12:53:29+00:00
- **Updated**: 2022-01-04 12:53:29+00:00
- **Authors**: Jingjing Yang, Haifeng Lu, Chengming Li, Xiping Hu, Bin Hu
- **Comment**: 10 pages,10 figures
- **Journal**: None
- **Summary**: In recent years, the incidence of depression is rising rapidly worldwide, but large-scale depression screening is still challenging. Gait analysis provides a non-contact, low-cost, and efficient early screening method for depression. However, the early screening of depression based on gait analysis lacks sufficient effective sample data. In this paper, we propose a skeleton data augmentation method for assessing the risk of depression. First, we propose five techniques to augment skeleton data and apply them to depression and emotion datasets. Then, we divide augmentation methods into two types (non-noise augmentation and noise augmentation) based on the mutual information and the classification accuracy. Finally, we explore which augmentation strategies can capture the characteristics of human skeleton data more effectively. Experimental results show that the augmented training data set that retains more of the raw skeleton data properties determines the performance of the detection model. Specifically, rotation augmentation and channel mask augmentation make the depression detection accuracy reach 92.15% and 91.34%, respectively.



### DeepFGS: Fine-Grained Scalable Coding for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2201.01173v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01173v1)
- **Published**: 2022-01-04 15:03:13+00:00
- **Updated**: 2022-01-04 15:03:13+00:00
- **Authors**: Yi Ma, Yongqi Zhai, Ronggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Scalable coding, which can adapt to channel bandwidth variation, performs well in today's complex network environment. However, the existing scalable compression methods face two challenges: reduced compression performance and insufficient scalability. In this paper, we propose the first learned fine-grained scalable image compression model (DeepFGS) to overcome the above two shortcomings. Specifically, we introduce a feature separation backbone to divide the image information into basic and scalable features, then redistribute the features channel by channel through an information rearrangement strategy. In this way, we can generate a continuously scalable bitstream via one-pass encoding. In addition, we reuse the decoder to reduce the parameters and computational complexity of DeepFGS. Experiments demonstrate that our DeepFGS outperforms all learning-based scalable image compression models and conventional scalable image codecs in PSNR and MS-SSIM metrics. To the best of our knowledge, our DeepFGS is the first exploration of learned fine-grained scalable coding, which achieves the finest scalability compared with learning-based methods.



### The cluster structure function
- **Arxiv ID**: http://arxiv.org/abs/2201.01222v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01222v3)
- **Published**: 2022-01-04 16:05:59+00:00
- **Updated**: 2022-10-14 13:18:00+00:00
- **Authors**: Andrew R. Cohen, Paul M. B. Vitnyi
- **Comment**: None
- **Journal**: None
- **Summary**: For each partition of a data set into a given number of parts there is a partition such that every part is as much as possible a good model (an "algorithmic sufficient statistic") for the data in that part. Since this can be done for every number between one and the number of data, the result is a function, the cluster structure function. It maps the number of parts of a partition to values related to the deficiencies of being good models by the parts. Such a function starts with a value at least zero for no partition of the data set and descents to zero for the partition of the data set into singleton parts. The optimal clustering is the one chosen to minimize the cluster structure function. The theory behind the method is expressed in algorithmic information theory (Kolmogorov complexity). In practice the Kolmogorov complexities involved are approximated by a concrete compressor. We give examples using real data sets: the MNIST handwritten digits and the segmentation of real cells as used in stem cell research.



### Transfer Learning for Retinal Vascular Disease Detection: A Pilot Study with Diabetic Retinopathy and Retinopathy of Prematurity
- **Arxiv ID**: http://arxiv.org/abs/2201.01250v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01250v1)
- **Published**: 2022-01-04 17:14:42+00:00
- **Updated**: 2022-01-04 17:14:42+00:00
- **Authors**: Guan Wang, Yusuke Kikuchi, Jinglin Yi, Qiong Zou, Rui Zhou, Xin Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal vascular diseases affect the well-being of human body and sometimes provide vital signs of otherwise undetected bodily damage. Recently, deep learning techniques have been successfully applied for detection of diabetic retinopathy (DR). The main obstacle of applying deep learning techniques to detect most other retinal vascular diseases is the limited amount of data available. In this paper, we propose a transfer learning technique that aims to utilize the feature similarities for detecting retinal vascular diseases. We choose the well-studied DR detection as a source task and identify the early detection of retinopathy of prematurity (ROP) as the target task. Our experimental results demonstrate that our DR-pretrained approach dominates in all metrics the conventional ImageNet-pretrained transfer learning approach, currently adopted in medical image analysis. Moreover, our approach is more robust with respect to the stochasticity in the training process and with respect to reduced training samples. This study suggests the potential of our proposed transfer learning approach for a broad range of retinal vascular diseases or pathologies, where data is limited.



### Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2201.01266v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.01266v1)
- **Published**: 2022-01-04 18:01:34+00:00
- **Updated**: 2022-01-04 18:01:34+00:00
- **Authors**: Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger Roth, Daguang Xu
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular "U-shaped" network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase. Code: https://monai.io/research/swin-unetr



### Self-supervised Learning from 100 Million Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2201.01283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01283v1)
- **Published**: 2022-01-04 18:27:04+00:00
- **Updated**: 2022-01-04 18:27:04+00:00
- **Authors**: Florin C. Ghesu, Bogdan Georgescu, Awais Mansoor, Youngjin Yoo, Dominik Neumann, Pragneshkumar Patel, R. S. Vishwanath, James M. Balter, Yue Cao, Sasa Grbic, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Building accurate and robust artificial intelligence systems for medical image assessment requires not only the research and design of advanced deep learning models but also the creation of large and curated sets of annotated training examples. Constructing such datasets, however, is often very costly -- due to the complex nature of annotation tasks and the high level of expertise required for the interpretation of medical images (e.g., expert radiologists). To counter this limitation, we propose a method for self-supervised learning of rich image features based on contrastive learning and online feature clustering. For this purpose we leverage large training datasets of over 100,000,000 medical images of various modalities, including radiography, computed tomography (CT), magnetic resonance (MR) imaging and ultrasonography. We propose to use these features to guide model training in supervised and hybrid self-supervised/supervised regime on various downstream tasks. We highlight a number of advantages of this strategy on challenging image assessment problems in radiography, CT and MR: 1) Significant increase in accuracy compared to the state-of-the-art (e.g., AUC boost of 3-7% for detection of abnormalities from chest radiography scans and hemorrhage detection on brain CT); 2) Acceleration of model convergence during training by up to 85% compared to using no pretraining (e.g., 83% when training a model for detection of brain metastases in MR scans); 3) Increase in robustness to various image augmentations, such as intensity variations, rotations or scaling reflective of data variation seen in the field.



### A Transformer-Based Siamese Network for Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.01293v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01293v7)
- **Published**: 2022-01-04 18:55:22+00:00
- **Updated**: 2022-09-02 01:41:29+00:00
- **Authors**: Wele Gedara Chaminda Bandara, Vishal M. Patel
- **Comment**: Accepted to International Geoscience and Remote Sensing Symposium
  (IGARSS), 2022. 4 pages, 2 figures. Code & trained models are available at
  https://github.com/wgcban/ChangeFormer
- **Journal**: None
- **Summary**: This paper presents a transformer-based Siamese network architecture (abbreviated by ChangeFormer) for Change Detection (CD) from a pair of co-registered remote sensing images. Different from recent CD frameworks, which are based on fully convolutional networks (ConvNets), the proposed method unifies hierarchically structured transformer encoder with Multi-Layer Perception (MLP) decoder in a Siamese network architecture to efficiently render multi-scale long-range details required for accurate CD. Experiments on two CD datasets show that the proposed end-to-end trainable ChangeFormer architecture achieves better CD performance than previous counterparts. Our code is available at https://github.com/wgcban/ChangeFormer.



### 3DVSR: 3D EPI Volume-based Approach for Angular and Spatial Light field Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.01294v1
- **DOI**: 10.1016/j.sigpro.2021.108373
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01294v1)
- **Published**: 2022-01-04 18:57:00+00:00
- **Updated**: 2022-01-04 18:57:00+00:00
- **Authors**: Trung-Hieu Tran, Jan Berberich, Sven Simon
- **Comment**: None
- **Journal**: None
- **Summary**: Light field (LF) imaging, which captures both spatial and angular information of a scene, is undoubtedly beneficial to numerous applications. Although various techniques have been proposed for LF acquisition, achieving both angularly and spatially high-resolution LF remains a technology challenge. In this paper, a learning-based approach applied to 3D epipolar image (EPI) is proposed to reconstruct high-resolution LF. Through a 2-stage super-resolution framework, the proposed approach effectively addresses various LF super-resolution (SR) problems, i.e., spatial SR, angular SR, and angular-spatial SR. While the first stage provides flexible options to up-sample EPI volume to the desired resolution, the second stage, which consists of a novel EPI volume-based refinement network (EVRN), substantially enhances the quality of the high-resolution EPI volume. An extensive evaluation on 90 challenging synthetic and real-world light field scenes from 7 published datasets shows that the proposed approach outperforms state-of-the-art methods to a large extend for both spatial and angular super-resolution problem, i.e., an average peak signal to noise ratio improvement of more than 2.0 dB, 1.4 dB, and 3.14 dB in spatial SR $\times 2$, spatial SR $\times 4$, and angular SR respectively. The reconstructed 4D light field demonstrates a balanced performance distribution across all perspective images and presents superior visual quality compared to the previous works.



### Online Multi-Object Tracking with Unsupervised Re-Identification Learning and Occlusion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.01297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01297v1)
- **Published**: 2022-01-04 18:59:58+00:00
- **Updated**: 2022-01-04 18:59:58+00:00
- **Authors**: Qiankun Liu, Dongdong Chen, Qi Chu, Lu Yuan, Bin Liu, Lei Zhang, Nenghai Yu
- **Comment**: To Appear at Neurocomputing 2022
- **Journal**: None
- **Summary**: Occlusion between different objects is a typical challenge in Multi-Object Tracking (MOT), which often leads to inferior tracking results due to the missing detected objects. The common practice in multi-object tracking is re-identifying the missed objects after their reappearance. Though tracking performance can be boosted by the re-identification, the annotation of identity is required to train the model. In addition, such practice of re-identification still can not track those highly occluded objects when they are missed by the detector. In this paper, we focus on online multi-object tracking and design two novel modules, the unsupervised re-identification learning module and the occlusion estimation module, to handle these problems. Specifically, the proposed unsupervised re-identification learning module does not require any (pseudo) identity information nor suffer from the scalability issue. The proposed occlusion estimation module tries to predict the locations where occlusions happen, which are used to estimate the positions of missed objects by the detector. Our study shows that, when applied to state-of-the-art MOT methods, the proposed unsupervised re-identification learning is comparable to supervised re-identification learning, and the tracking performance is further improved by the proposed occlusion estimation module.



### Linear Variational State-Space Filtering
- **Arxiv ID**: http://arxiv.org/abs/2201.01353v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2201.01353v3)
- **Published**: 2022-01-04 21:28:32+00:00
- **Updated**: 2022-03-19 16:33:04+00:00
- **Authors**: Daniel Pfrommer, Nikolai Matni
- **Comment**: 18 pages, 6 figures. Fixed proof in appendix. For associated code,
  see https://github.com/pfrommerd/variational_state_space_models
- **Journal**: None
- **Summary**: We introduce Variational State-Space Filters (VSSF), a new method for unsupervised learning, identification, and filtering of latent Markov state space models from raw pixels. We present a theoretically sound framework for latent state space inference under heterogeneous sensor configurations. The resulting model can integrate an arbitrary subset of the sensor measurements used during training, enabling the learning of semi-supervised state representations, thus enforcing that certain components of the learned latent state space to agree with interpretable measurements. From this framework we derive L-VSSF, an explicit instantiation of this model with linear latent dynamics and Gaussian distribution parameterizations. We experimentally demonstrate L-VSSF's ability to filter in latent space beyond the sequence length of the training dataset across several different test environments.



### DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2201.01367v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01367v4)
- **Published**: 2022-01-04 22:26:14+00:00
- **Updated**: 2022-03-08 20:49:26+00:00
- **Authors**: Won Kyung Do, Monroe Kennedy III
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing the performance of tactile sensing in robots enables versatile, in-hand manipulation. Vision-based tactile sensors have been widely used as rich tactile feedback has been shown to be correlated with increased performance in manipulation tasks. Existing tactile sensor solutions with high resolution have limitations that include low accuracy, expensive components, or lack of scalability. In this paper, an inexpensive, scalable, and compact tactile sensor with high-resolution surface deformation modeling for surface reconstruction of the 3D sensor surface is proposed. By measuring the image from the fisheye camera, it is shown that the sensor can successfully estimate the surface deformation in real-time (1.8ms) by using deep convolutional neural networks. This sensor in its design and sensing abilities represents a significant step toward better object in-hand localization, classification, and surface estimation all enabled by high-resolution shape reconstruction.



### Image Processing Methods for Coronal Hole Segmentation, Matching, and Map Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.01380v1
- **DOI**: 10.1109/TIP.2019.2944057
- **Categories**: **eess.IV**, astro-ph.SR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01380v1)
- **Published**: 2022-01-04 23:19:53+00:00
- **Updated**: 2022-01-04 23:19:53+00:00
- **Authors**: V. Jatla, M. S. Pattichis, C. N. Arge
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 29 (2019): 1641-1653
- **Summary**: The paper presents the results from a multi-year effort to develop and validate image processing methods for selecting the best physical models based on solar image observations. The approach consists of selecting the physical models based on their agreement with coronal holes extracted from the images. Ultimately, the goal is to use physical models to predict geomagnetic storms. We decompose the problem into three subproblems: (i) coronal hole segmentation based on physical constraints, (ii) matching clusters of coronal holes between different maps, and (iii) physical map classification. For segmenting coronal holes, we develop a multi-modal method that uses segmentation maps from three different methods to initialize a level-set method that evolves the initial coronal hole segmentation to the magnetic boundary. Then, we introduce a new method based on Linear Programming for matching clusters of coronal holes. The final matching is then performed using Random Forests. The methods were carefully validated using consensus maps derived from multiple readers, manual clustering, manual map classification, and method validation for 50 maps. The proposed multi-modal segmentation method significantly outperformed SegNet, U-net, Henney-Harvey, and FCN by providing accurate boundary detection. Overall, the method gave a 95.5% map classification accuracy.




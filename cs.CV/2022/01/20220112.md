# Arxiv Papers in cs.CV on 2022-01-12
### Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound Sources in Unmapped 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2201.04279v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.04279v1)
- **Published**: 2022-01-12 03:08:03+00:00
- **Updated**: 2022-01-12 03:08:03+00:00
- **Authors**: Abdelrahman Younes
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work on audio-visual navigation targets a single static sound in noise-free audio environments and struggles to generalize to unheard sounds. We introduce the novel dynamic audio-visual navigation benchmark in which an embodied AI agent must catch a moving sound source in an unmapped environment in the presence of distractors and noisy sounds. We propose an end-to-end reinforcement learning approach that relies on a multi-modal architecture that fuses the spatial audio-visual information from a binaural audio signal and spatial occupancy maps to encode the features needed to learn a robust navigation policy for our new complex task settings. We demonstrate that our approach outperforms the current state-of-the-art with better generalization to unheard sounds and better robustness to noisy scenarios on the two challenging 3D scanned real-world datasets Replica and Matterport3D, for the static and dynamic audio-visual navigation benchmarks. Our novel benchmark will be made available at http://dav-nav.cs.uni-freiburg.de.



### Multiview Transformers for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.04288v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04288v4)
- **Published**: 2022-01-12 03:33:57+00:00
- **Updated**: 2022-05-31 06:19:59+00:00
- **Authors**: Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, Cordelia Schmid
- **Comment**: CVPR 2022; arXiv v4: update results on Epic-Kitchens-100
- **Journal**: None
- **Summary**: Video understanding requires reasoning at multiple spatiotemporal resolutions -- from short fine-grained motions to events taking place over longer durations. Although transformer architectures have recently advanced the state-of-the-art, they have not explicitly modelled different spatiotemporal resolutions. To this end, we present Multiview Transformers for Video Recognition (MTV). Our model consists of separate encoders to represent different views of the input video with lateral connections to fuse information across views. We present thorough ablation studies of our model and show that MTV consistently performs better than single-view counterparts in terms of accuracy and computational cost across a range of model sizes. Furthermore, we achieve state-of-the-art results on six standard datasets, and improve even further with large-scale pretraining. Code and checkpoints are available at: https://github.com/google-research/scenic/tree/main/scenic/projects/mtv.



### Robust Contrastive Learning against Noisy Views
- **Arxiv ID**: http://arxiv.org/abs/2201.04309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04309v1)
- **Published**: 2022-01-12 05:24:29+00:00
- **Updated**: 2022-01-12 05:24:29+00:00
- **Authors**: Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelka, Yale Song
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning relies on an assumption that positive pairs contain related views, e.g., patches of an image or co-occurring multimodal signals of a video, that share certain underlying information about an instance. But what if this assumption is violated? The literature suggests that contrastive learning produces suboptimal representations in the presence of noisy views, e.g., false positive pairs with no apparent shared information. In this work, we propose a new contrastive loss function that is robust against noisy views. We provide rigorous theoretical justifications by showing connections to robust symmetric losses for noisy binary classification and by establishing a new contrastive bound for mutual information maximization based on the Wasserstein distance measure. The proposed loss is completely modality-agnostic and a simple drop-in replacement for the InfoNCE loss, which makes it easy to apply to existing contrastive frameworks. We show that our approach provides consistent improvements over the state-of-the-art on image, video, and graph contrastive learning benchmarks that exhibit a variety of real-world noise patterns.



### Knee Cartilage Defect Assessment by Graph Representation and Surface Convolution
- **Arxiv ID**: http://arxiv.org/abs/2201.04318v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04318v1)
- **Published**: 2022-01-12 05:55:32+00:00
- **Updated**: 2022-01-12 05:55:32+00:00
- **Authors**: Zixu Zhuang, Liping Si, Sheng Wang, Kai Xuan, Xi Ouyang, Yiqiang Zhan, Zhong Xue, Lichi Zhang, Dinggang Shen, Weiwu Yao, Qian Wang
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Knee osteoarthritis (OA) is the most common osteoarthritis and a leading cause of disability. Cartilage defects are regarded as major manifestations of knee OA, which are visible by magnetic resonance imaging (MRI). Thus early detection and assessment for knee cartilage defects are important for protecting patients from knee OA. In this way, many attempts have been made on knee cartilage defect assessment by applying convolutional neural networks (CNNs) to knee MRI. However, the physiologic characteristics of the cartilage may hinder such efforts: the cartilage is a thin curved layer, implying that only a small portion of voxels in knee MRI can contribute to the cartilage defect assessment; heterogeneous scanning protocols further challenge the feasibility of the CNNs in clinical practice; the CNN-based knee cartilage evaluation results lack interpretability. To address these challenges, we model the cartilages structure and appearance from knee MRI into a graph representation, which is capable of handling highly diverse clinical data. Then, guided by the cartilage graph representation, we design a non-Euclidean deep learning network with the self-attention mechanism, to extract cartilage features in the local and global, and to derive the final assessment with a visualized result. Our comprehensive experiments show that the proposed method yields superior performance in knee cartilage defect assessment, plus its convenient 3D visualization for interpretability.



### Neural Residual Flow Fields for Efficient Video Representations
- **Arxiv ID**: http://arxiv.org/abs/2201.04329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04329v2)
- **Published**: 2022-01-12 06:22:09+00:00
- **Updated**: 2022-10-05 09:42:37+00:00
- **Authors**: Daniel Rho, Junwoo Cho, Jong Hwan Ko, Eunbyung Park
- **Comment**: Accepted for ACCV 2022, codes are available at
  https://github.com/daniel03c1/eff_video_representation
- **Journal**: None
- **Summary**: Neural fields have emerged as a powerful paradigm for representing various signals, including videos. However, research on improving the parameter efficiency of neural fields is still in its early stages. Even though neural fields that map coordinates to colors can be used to encode video signals, this scheme does not exploit the spatial and temporal redundancy of video signals. Inspired by standard video compression algorithms, we propose a neural field architecture for representing and compressing videos that deliberately removes data redundancy through the use of motion information across video frames. Maintaining motion information, which is typically smoother and less complex than color signals, requires a far fewer number of parameters. Furthermore, reusing color values through motion information further improves the network parameter efficiency. In addition, we suggest using more than one reference frame for video frame reconstruction and separate networks, one for optical flows and the other for residuals. Experimental results have shown that the proposed method outperforms the baseline methods by a significant margin. The code is available in https://github.com/daniel03c1/eff_video_representation



### MDS-Net: A Multi-scale Depth Stratification Based Monocular 3D Object Detection Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2201.04341v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04341v2)
- **Published**: 2022-01-12 07:11:18+00:00
- **Updated**: 2022-04-28 14:31:39+00:00
- **Authors**: Zhouzhen Xie, Yuying Song, Jingxuan Wu, Zecheng Li, Chunyi Song, Zhiwei Xu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Monocular 3D object detection is very challenging in autonomous driving due to the lack of depth information. This paper proposes a one-stage monocular 3D object detection algorithm based on multi-scale depth stratification, which uses the anchor-free method to detect 3D objects in a per-pixel prediction. In the proposed MDS-Net, a novel depth-based stratification structure is developed to improve the network's ability of depth prediction by establishing mathematical models between depth and image size of objects. A new angle loss function is then developed to further improve the accuracy of the angle prediction and increase the convergence speed of training. An optimized soft-NMS is finally applied in the post-processing stage to adjust the confidence of candidate boxes. Experiments on the KITTI benchmark show that the MDS-Net outperforms the existing monocular 3D detection methods in 3D detection and BEV detection tasks while fulfilling real-time requirements.



### Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.04358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04358v2)
- **Published**: 2022-01-12 08:40:23+00:00
- **Updated**: 2022-03-10 02:42:42+00:00
- **Authors**: Bin Xia, Yapeng Tian, Yucheng Hang, Wenming Yang, Qingmin Liao, Jie Zhou
- **Comment**: code is availavle at https://github.com/Zj-BinXia/AMSA
- **Journal**: AAAI2022
- **Summary**: Reference-based super-resolution (RefSR) has made significant progress in producing realistic textures using an external reference (Ref) image. However, existing RefSR methods obtain high-quality correspondence matchings consuming quadratic computation resources with respect to the input size, limiting its application. Moreover, these approaches usually suffer from scale misalignments between the low-resolution (LR) image and Ref image. In this paper, we propose an Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based Super-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch) and Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching efficiency, we design a novel Embedded PatchMacth scheme with random samples propagation, which involves end-to-end training with asymptotic linear computational cost to the input size. To further reduce computational cost and speed up convergence, we apply the coarse-to-fine strategy on Embedded PatchMacth constituting CFE-PatchMatch. To fully leverage reference information across multiple scales and enhance robustness to scale misalignment, we develop the MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation. The Dynamic Aggregation corrects minor scale misalignment by dynamically aggregating features, and the Multi-Scale Aggregation brings robustness to large scale misalignment by fusing multi-scale information. Experimental results show that the proposed AMSA achieves superior performance over state-of-the-art approaches on both quantitative and qualitative evaluations.



### SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.04364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04364v1)
- **Published**: 2022-01-12 08:59:12+00:00
- **Updated**: 2022-01-12 08:59:12+00:00
- **Authors**: Jiangning Zhang, Chao Xu, Jian Li, Yue Han, Yabiao Wang, Ying Tai, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In the practical application of restoring low-resolution gray-scale images, we generally need to run three separate processes of image colorization, super-resolution, and dows-sampling operation for the target device. However, this pipeline is redundant and inefficient for the independent processes, and some inner features could have been shared. Therefore, we present an efficient paradigm to perform {S}imultaneously Image {C}olorization and {S}uper-resolution (SCS) and propose an end-to-end SCSNet to achieve this goal. The proposed method consists of two parts: colorization branch for learning color information that employs the proposed plug-and-play \emph{Pyramid Valve Cross Attention} (PVCAttn) module to aggregate feature maps between source and reference images; and super-resolution branch for integrating color and texture information to predict target images, which uses the designed \emph{Continuous Pixel Mapping} (CPM) module to predict high-resolution images at continuous magnification. Furthermore, our SCSNet supports both automatic and referential modes that is more flexible for practical application. Abundant experiments demonstrate the superiority of our method for generating authentic images over state-of-the-art methods, e.g., averagely decreasing FID by 1.8$\downarrow$ and 5.1 $\downarrow$ compared with current best scores for automatic and referential modes, respectively, while owning fewer parameters (more than $\times$2$\downarrow$) and faster running speed (more than $\times$3$\uparrow$).



### Predicting Alzheimer's Disease Using 3DMgNet
- **Arxiv ID**: http://arxiv.org/abs/2201.04370v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04370v1)
- **Published**: 2022-01-12 09:08:08+00:00
- **Updated**: 2022-01-12 09:08:08+00:00
- **Authors**: Yelu Gao, Huang Huang, Lian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is an irreversible neurode generative disease of the brain.The disease may causes memory loss, difficulty communicating and disorientation. For the diagnosis of Alzheimer's disease, a series of scales are often needed to evaluate the diagnosis clinically, which not only increases the workload of doctors, but also makes the results of diagnosis highly subjective. Therefore, for Alzheimer's disease, imaging means to find early diagnostic markers has become a top priority.   In this paper, we propose a novel 3DMgNet architecture which is a unified framework of multigrid and convolutional neural network to diagnose Alzheimer's disease (AD). The model is trained using an open dataset (ADNI dataset) and then test with a smaller dataset of ours. Finally, the model achieved 92.133% accuracy for AD vs NC classification and significantly reduced the model parameters.



### Maximizing Self-supervision from Thermal Image for Effective Self-supervised Learning of Depth and Ego-motion
- **Arxiv ID**: http://arxiv.org/abs/2201.04387v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04387v2)
- **Published**: 2022-01-12 09:49:24+00:00
- **Updated**: 2022-06-15 10:07:23+00:00
- **Authors**: Ukcheol Shin, Kyunghyun Lee, Byeong-Uk Lee, In So Kweon
- **Comment**: 8 pages, Accepted by IEEE Robotics and Automation Letters (RA-L) with
  IROS 2022 option
- **Journal**: None
- **Summary**: Recently, self-supervised learning of depth and ego-motion from thermal images shows strong robustness and reliability under challenging scenarios. However, the inherent thermal image properties such as weak contrast, blurry edges, and noise hinder to generate effective self-supervision from thermal images. Therefore, most research relies on additional self-supervision sources such as well-lit RGB images, generative models, and Lidar information. In this paper, we conduct an in-depth analysis of thermal image characteristics that degenerates self-supervision from thermal images. Based on the analysis, we propose an effective thermal image mapping method that significantly increases image information, such as overall structure, contrast, and details, while preserving temporal consistency. The proposed method shows outperformed depth and pose results than previous state-of-the-art networks without leveraging additional RGB guidance.



### OCSampler: Compressing Videos to One Clip with Single-step Sampling
- **Arxiv ID**: http://arxiv.org/abs/2201.04388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04388v1)
- **Published**: 2022-01-12 09:50:38+00:00
- **Updated**: 2022-01-12 09:50:38+00:00
- **Authors**: Jintao Lin, Haodong Duan, Kai Chen, Dahua Lin, Limin Wang
- **Comment**: Video Understanding, Efficient Action Recognition
- **Journal**: None
- **Summary**: In this paper, we propose a framework named OCSampler to explore a compact yet effective video representation with one short clip for efficient video recognition. Recent works prefer to formulate frame sampling as a sequential decision task by selecting frames one by one according to their importance, while we present a new paradigm of learning instance-specific video condensation policies to select informative frames for representing the entire video only in a single step. Our basic motivation is that the efficient video recognition task lies in processing a whole sequence at once rather than picking up frames sequentially. Accordingly, these policies are derived from a light-weighted skim network together with a simple yet effective policy network within one step. Moreover, we extend the proposed method with a frame number budget, enabling the framework to produce correct predictions in high confidence with as few frames as possible. Experiments on four benchmarks, i.e., ActivityNet, Mini-Kinetics, FCVID, Mini-Sports1M, demonstrate the effectiveness of our OCSampler over previous methods in terms of accuracy, theoretical computational expense, actual inference speed. We also evaluate its generalization power across different classifiers, sampled frames, and search spaces. Especially, we achieve 76.9% mAP and 21.7 GFLOPs on ActivityNet with an impressive throughput: 123.9 Videos/s on a single TITAN Xp GPU.



### Towards Adversarially Robust Deep Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2201.04397v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04397v2)
- **Published**: 2022-01-12 10:23:14+00:00
- **Updated**: 2022-01-13 06:00:04+00:00
- **Authors**: Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan
- **Comment**: None
- **Journal**: None
- **Summary**: This work systematically investigates the adversarial robustness of deep image denoisers (DIDs), i.e, how well DIDs can recover the ground truth from noisy observations degraded by adversarial perturbations. Firstly, to evaluate DIDs' robustness, we propose a novel adversarial attack, namely Observation-based Zero-mean Attack ({\sc ObsAtk}), to craft adversarial zero-mean perturbations on given noisy images. We find that existing DIDs are vulnerable to the adversarial noise generated by {\sc ObsAtk}. Secondly, to robustify DIDs, we propose an adversarial training strategy, hybrid adversarial training ({\sc HAT}), that jointly trains DIDs with adversarial and non-adversarial noisy data to ensure that the reconstruction quality is high and the denoisers around non-adversarial data are locally smooth. The resultant DIDs can effectively remove various types of synthetic and adversarial noise. We also uncover that the robustness of DIDs benefits their generalization capability on unseen real-world noise. Indeed, {\sc HAT}-trained DIDs can recover high-quality clean images from real-world noise even without training on real noisy data. Extensive experiments on benchmark datasets, including Set68, PolyU, and SIDD, corroborate the effectiveness of {\sc ObsAtk} and {\sc HAT}.



### MoViDNN: A Mobile Platform for Evaluating Video Quality Enhancement with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.04402v1
- **DOI**: 10.1007/978-3-030-98355-0_40
- **Categories**: **cs.CV**, cs.MM, H.5.1; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2201.04402v1)
- **Published**: 2022-01-12 10:38:04+00:00
- **Updated**: 2022-01-12 10:38:04+00:00
- **Authors**: Ekrem Ã‡etinkaya, Minh Nguyen, Christian Timmerer
- **Comment**: 8 pages, 3 figures
- **Journal**: MMM 2022: MultiMedia Modeling pp 465-472
- **Summary**: Deep neural network (DNN) based approaches have been intensively studied to improve video quality thanks to their fast advancement in recent years. These approaches are designed mainly for desktop devices due to their high computational cost. However, with the increasing performance of mobile devices in recent years, it became possible to execute DNN based approaches in mobile devices. Despite having the required computational power, utilizing DNNs to improve the video quality for mobile devices is still an active research area. In this paper, we propose an open-source mobile platform, namely MoViDNN, to evaluate DNN based video quality enhancement methods, such as super-resolution, denoising, and deblocking. Our proposed platform can be used to evaluate the DNN based approaches both objectively and subjectively. For objective evaluation, we report common metrics such as execution time, PSNR, and SSIM. For subjective evaluation, Mean Score Opinion (MOS) is reported. The proposed platform is available publicly at https://github.com/cd-athena/MoViDNN



### Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents
- **Arxiv ID**: http://arxiv.org/abs/2201.04990v1
- **DOI**: 10.1145/3462244.3479932
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.0; I.6.5
- **Links**: [PDF](http://arxiv.org/pdf/2201.04990v1)
- **Published**: 2022-01-12 10:57:40+00:00
- **Updated**: 2022-01-12 10:57:40+00:00
- **Authors**: Junseok Park, Kwanyoung Park, Hyunseok Oh, Ganghun Lee, Minsu Lee, Youngki Lee, Byoung-Tak Zhang
- **Comment**: ICMI2021 Oral Presentation, 9 pages, 9 figures
- **Journal**: None
- **Summary**: Critical periods are phases during which a toddler's brain develops in spurts. To promote children's cognitive development, proper guidance is critical in this stage. However, it is not clear whether such a critical period also exists for the training of AI agents. Similar to human toddlers, well-timed guidance and multimodal interactions might significantly enhance the training efficiency of AI agents as well. To validate this hypothesis, we adapt this notion of critical periods to learning in AI agents and investigate the critical period in the virtual environment for AI agents. We formalize the critical period and Toddler-guidance learning in the reinforcement learning (RL) framework. Then, we built up a toddler-like environment with VECA toolkit to mimic human toddlers' learning characteristics. We study three discrete levels of mutual interaction: weak-mentor guidance (sparse reward), moderate mentor guidance (helper-reward), and mentor demonstration (behavioral cloning). We also introduce the EAVE dataset consisting of 30,000 real-world images to fully reflect the toddler's viewpoint. We evaluate the impact of critical periods on AI agents from two perspectives: how and when they are guided best in both uni- and multimodal learning. Our experimental results show that both uni- and multimodal agents with moderate mentor guidance and critical period on 1 million and 2 million training steps show a noticeable improvement. We validate these results with transfer learning on the EAVE dataset and find the performance advancement on the same critical period and the guidance.



### Optimizing Prediction of MGMT Promoter Methylation from MRI Scans using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.04416v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04416v2)
- **Published**: 2022-01-12 11:04:34+00:00
- **Updated**: 2022-01-27 03:54:42+00:00
- **Authors**: Sauman Das
- **Comment**: None
- **Journal**: None
- **Summary**: Glioblastoma Multiforme (GBM) is a malignant brain cancer forming around 48% of al brain and Central Nervous System (CNS) cancers. It is estimated that annually over 13,000 deaths occur in the US due to GBM, making it crucial to have early diagnosis systems that can lead to predictable and effective treatment. The most common treatment after GBM diagnosis is chemotherapy, which works by sending rapidly dividing cells to apoptosis. However, this form of treatment is not effective when the MGMT promoter sequence is methylated, and instead leads to severe side effects decreasing patient survivability. Therefore, it is important to be able to identify the MGMT promoter methylation status through non-invasive magnetic resonance imaging (MRI) based machine learning (ML) models. This is accomplished using the Brain Tumor Segmentation (BraTS) 2021 dataset, which was recently used for an international Kaggle competition. We developed four primary models - two radiomic models and two CNN models - each solving the binary classification task with progressive improvements. We built a novel ML model termed as the Intermediate State Generator which was used to normalize the slice thicknesses of all MRI scans. With further improvements, our best model was able to achieve performance significantly ($p < 0.05$) better than the best performing Kaggle model with a 6% increase in average cross-validation accuracy. This improvement could potentially lead to a more informed choice of chemotherapy as a treatment option, prolonging lives of thousands of patients with GBM each year.



### Beyond the Visible: A Survey on Cross-spectral Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.04435v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04435v2)
- **Published**: 2022-01-12 12:09:24+00:00
- **Updated**: 2022-05-06 00:27:11+00:00
- **Authors**: David Anghelone, Cunjian Chen, Arun Ross, Antitza Dantcheva
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-spectral face recognition (CFR) refers to recognizing individuals using face images stemming from different spectral bands, such as infrared vs. visible. While CFR is inherently more challenging than classical face recognition due to significant variation in facial appearance caused by the modality gap, it is useful in many scenarios including night-vision biometrics and detecting presentation attacks. Recent advances in convolutional neural networks (CNNs) have resulted in significant improvement in the performance of CFR systems. Given these developments, the contributions of this survey are three-fold. First, we provide an overview of CFR, by formalizing the CFR problem and presenting related applications. Secondly, we discuss the appropriate spectral bands for face recognition and discuss recent CFR methods, placing emphasis on deep neural networks. In particular we describe techniques that have been proposed to extract and compare heterogeneous features emerging from different spectral bands. We also discuss the datasets that have been used for evaluating CFR methods. Finally, we discuss the challenges and future lines of research on this topic.



### Real-Time Style Modelling of Human Locomotion via Feature-Wise Transformations and Local Motion Phases
- **Arxiv ID**: http://arxiv.org/abs/2201.04439v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04439v1)
- **Published**: 2022-01-12 12:25:57+00:00
- **Updated**: 2022-01-12 12:25:57+00:00
- **Authors**: Ian Mason, Sebastian Starke, Taku Komura
- **Comment**: None
- **Journal**: None
- **Summary**: Controlling the manner in which a character moves in a real-time animation system is a challenging task with useful applications. Existing style transfer systems require access to a reference content motion clip, however, in real-time systems the future motion content is unknown and liable to change with user input. In this work we present a style modelling system that uses an animation synthesis network to model motion content based on local motion phases. An additional style modulation network uses feature-wise transformations to modulate style in real-time. To evaluate our method, we create and release a new style modelling dataset, 100STYLE, containing over 4 million frames of stylised locomotion data in 100 different styles that present a number of challenges for existing systems. To model these styles, we extend the local phase calculation with a contact-free formulation. In comparison to other methods for real-time style modelling, we show our system is more robust and efficient in its style representation while improving motion quality.



### Globally Optimal Multi-Scale Monocular Hand-Eye Calibration Using Dual Quaternions
- **Arxiv ID**: http://arxiv.org/abs/2201.04473v1
- **DOI**: 10.1109/3DV53792.2021.00035
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04473v1)
- **Published**: 2022-01-12 13:48:04+00:00
- **Updated**: 2022-01-12 13:48:04+00:00
- **Authors**: Thomas Wodtko, Markus Horn, Michael Buchholz, Klaus Dietmayer
- **Comment**: None
- **Journal**: 2021 International Conference on 3D Vision (3DV)
- **Summary**: In this work, we present an approach for monocular hand-eye calibration from per-sensor ego-motion based on dual quaternions. Due to non-metrically scaled translations of monocular odometry, a scaling factor has to be estimated in addition to the rotation and translation calibration. For this, we derive a quadratically constrained quadratic program that allows a combined estimation of all extrinsic calibration parameters. Using dual quaternions leads to low run-times due to their compact representation. Our problem formulation further allows to estimate multiple scalings simultaneously for different sequences of the same sensor setup. Based on our problem formulation, we derive both, a fast local and a globally optimal solving approach. Finally, our algorithms are evaluated and compared to state-of-the-art approaches on simulated and real-world data, e.g., the EuRoC MAV dataset.



### Depth Estimation from Single-shot Monocular Endoscope Image Using Image Domain Adaptation And Edge-Aware Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.04485v1
- **DOI**: 10.1080/21681163.2021.2012835
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04485v1)
- **Published**: 2022-01-12 14:06:54+00:00
- **Updated**: 2022-01-12 14:06:54+00:00
- **Authors**: Masahiro Oda, Hayato Itoh, Kiyohito Tanaka, Hirotsugu Takabatake, Masaki Mori, Hiroshi Natori, Kensaku Mori
- **Comment**: Accepted paper as an oral presentation at Joint MICCAI workshop 2021,
  AE-CAI/CARE/OR2.0
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization, 2021
- **Summary**: We propose a depth estimation method from a single-shot monocular endoscopic image using Lambertian surface translation by domain adaptation and depth estimation using multi-scale edge loss. We employ a two-step estimation process including Lambertian surface translation from unpaired data and depth estimation. The texture and specular reflection on the surface of an organ reduce the accuracy of depth estimations. We apply Lambertian surface translation to an endoscopic image to remove these texture and reflections. Then, we estimate the depth by using a fully convolutional network (FCN). During the training of the FCN, improvement of the object edge similarity between an estimated image and a ground truth depth image is important for getting better results. We introduced a muti-scale edge loss function to improve the accuracy of depth estimation. We quantitatively evaluated the proposed method using real colonoscopic images. The estimated depth values were proportional to the real depth values. Furthermore, we applied the estimated depth images to automated anatomical location identification of colonoscopic images using a convolutional neural network. The identification accuracy of the network improved from 69.2% to 74.1% by using the estimated depth images.



### SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2201.04494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.04494v1)
- **Published**: 2022-01-12 14:48:11+00:00
- **Updated**: 2022-01-12 14:48:11+00:00
- **Authors**: Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, Andrew Markham
- **Comment**: Accepted by IJCV 2022
- **Journal**: None
- **Summary**: With the recent availability and affordability of commercial depth sensors and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets have been publicized to facilitate research in 3D computer vision. However, existing datasets either cover relatively small areas or have limited semantic annotations. Fine-grained understanding of urban-scale 3D scenes is still in its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV photogrammetry point cloud dataset consisting of nearly three billion points collected from three UK cities, covering 7.6 km^2. Each point in the dataset has been labelled with fine-grained semantic annotations, resulting in a dataset that is three times the size of the previous existing largest photogrammetric point cloud dataset. In addition to the more commonly encountered categories such as road and vegetation, urban-level categories including rail, bridge, and river are also included in our dataset. Based on this dataset, we further build a benchmark to evaluate the performance of state-of-the-art segmentation algorithms. In particular, we provide a comprehensive analysis and identify several key challenges limiting urban-scale point cloud understanding. The dataset is available at http://point-cloud-analysis.cs.ox.ac.uk.



### Early Diagnosis of Parkinsons Disease by Analyzing Magnetic Resonance Imaging Brain Scans and Patient Characteristics
- **Arxiv ID**: http://arxiv.org/abs/2201.04631v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04631v1)
- **Published**: 2022-01-12 15:51:54+00:00
- **Updated**: 2022-01-12 15:51:54+00:00
- **Authors**: Sabrina Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Parkinsons disease, PD, is a chronic condition that affects motor skills and includes symptoms like tremors and rigidity. The current diagnostic procedure uses patient assessments to evaluate symptoms and sometimes a magnetic resonance imaging or MRI scan. However, symptom variations cause inaccurate assessments, and the analysis of MRI scans requires experienced specialists. This research proposes to accurately diagnose PD severity with deep learning by combining symptoms data and MRI data from the Parkinsons Progression Markers Initiative database. A new hybrid model architecture was implemented to fully utilize both forms of clinical data, and models based on only symptoms and only MRI scans were also developed. The symptoms based model integrates a fully connected deep learning neural network, and the MRI scans and hybrid models integrate transfer learning based convolutional neural networks. Instead of performing only binary classification, all models diagnose patients into five severity categories, with stage zero representing healthy patients and stages four and five representing patients with PD. The symptoms only, MRI scans only, and hybrid models achieved accuracies of 0.77, 0.68, and 0.94, respectively. The hybrid model also had high precision and recall scores of 0.94 and 0.95. Real clinical cases confirm the strong performance of the hybrid, where patients were classified incorrectly with both other models but correctly by the hybrid. It is also consistent across the five severity stages, indicating accurate early detection. This is the first report to combine symptoms data and MRI scans with a machine learning approach on such a large scale.



### Structure and position-aware graph neural network for airway labeling
- **Arxiv ID**: http://arxiv.org/abs/2201.04532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04532v1)
- **Published**: 2022-01-12 16:05:19+00:00
- **Updated**: 2022-01-12 16:05:19+00:00
- **Authors**: Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Bram van Ginneken
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel graph-based approach for labeling the anatomical branches of a given airway tree segmentation. The proposed method formulates airway labeling as a branch classification problem in the airway tree graph, where branch features are extracted using convolutional neural networks (CNN) and enriched using graph neural networks. Our graph neural network is structure-aware by having each node aggregate information from its local neighbors and position-aware by encoding node positions in the graph.   We evaluated the proposed method on 220 airway trees from subjects with various severity stages of Chronic Obstructive Pulmonary Disease (COPD). The results demonstrate that our approach is computationally efficient and significantly improves branch classification performance than the baseline method. The overall average accuracy of our method reaches 91.18\% for labeling all 18 segmental airway branches, compared to 83.83\% obtained by the standard CNN method. We published our source code at https://github.com/DIAGNijmegen/spgnn. The proposed algorithm is also publicly available at https://grand-challenge.org/algorithms/airway-anatomical-labeling/.



### Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data
- **Arxiv ID**: http://arxiv.org/abs/2201.04569v3
- **DOI**: 10.1109/TNSE.2022.3188575
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04569v3)
- **Published**: 2022-01-12 17:01:19+00:00
- **Updated**: 2022-08-09 09:19:40+00:00
- **Authors**: Sunder Ali Khowaja, Ik Hyun Lee, Kapal Dev, Muhammad Aslam Jarwar, Nawab Muhammad Faseeh Qureshi
- **Comment**: 10 pages, 5 figures, 2 tables
- **Journal**: IEEE Transactions on Network Science and Engineering, 2022
- **Summary**: The past decade has seen a rapid adoption of Artificial Intelligence (AI), specifically the deep learning networks, in Internet of Medical Things (IoMT) ecosystem. However, it has been shown recently that the deep learning networks can be exploited by adversarial attacks that not only make IoMT vulnerable to the data theft but also to the manipulation of medical diagnosis. The existing studies consider adding noise to the raw IoMT data or model parameters which not only reduces the overall performance concerning medical inferences but also is ineffective to the likes of deep leakage from gradients method. In this work, we propose proximal gradient split learning (PSGL) method for defense against the model inversion attacks. The proposed method intentionally attacks the IoMT data when undergoing the deep neural network training process at client side. We propose the use of proximal gradient method to recover gradient maps and a decision-level fusion strategy to improve the recognition performance. Extensive analysis show that the PGSL not only provides effective defense mechanism against the model inversion attacks but also helps in improving the recognition performance on publicly available datasets. We report 14.0$\%$, 17.9$\%$, and 36.9$\%$ gains in accuracy over reconstructed and adversarial attacked images, respectively.



### ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.04584v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.04584v4)
- **Published**: 2022-01-12 17:21:28+00:00
- **Updated**: 2022-03-29 23:48:10+00:00
- **Authors**: Muhammad Asad, Lucas Fidon, Tom Vercauteren
- **Comment**: Accepted at MIDL 2022
- **Journal**: None
- **Summary**: Automatic segmentation of lung lesions associated with COVID-19 in CT images requires large amount of annotated volumes. Annotations mandate expert knowledge and are time-intensive to obtain through fully manual segmentation methods. Additionally, lung lesions have large inter-patient variations, with some pathologies having similar visual appearance as healthy lung tissues. This poses a challenge when applying existing semi-automatic interactive segmentation techniques for data labelling. To address these challenges, we propose an efficient convolutional neural networks (CNNs) that can be learned online while the annotator provides scribble-based interaction. To accelerate learning from only the samples labelled through user-interactions, a patch-based approach is used for training the network. Moreover, we use weighted cross-entropy loss to address the class imbalance that may result from user-interactions. During online inference, the learned network is applied to the whole input volume using a fully convolutional approach. We compare our proposed method with state-of-the-art using synthetic scribbles and show that it outperforms existing methods on the task of annotating lung lesions associated with COVID-19, achieving 16% higher Dice score while reducing execution time by 3$\times$ and requiring 9000 lesser scribbles-based labelled voxels. Due to the online learning aspect, our approach adapts quickly to user input, resulting in high quality segmentation labels. Source code for ECONet is available at: https://github.com/masadcv/ECONet-MONAILabel.



### SparseDet: Improving Sparsely Annotated Object Detection with Pseudo-positive Mining
- **Arxiv ID**: http://arxiv.org/abs/2201.04620v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04620v2)
- **Published**: 2022-01-12 18:57:04+00:00
- **Updated**: 2023-08-27 02:02:24+00:00
- **Authors**: Saksham Suri, Sai Saketh Rambhatla, Rama Chellappa, Abhinav Shrivastava
- **Comment**: Accepted at ICCV2023. Project webpage:
  https://www.cs.umd.edu/~sakshams/SparseDet. The first two authors contributed
  equally
- **Journal**: None
- **Summary**: Training with sparse annotations is known to reduce the performance of object detectors. Previous methods have focused on proxies for missing ground truth annotations in the form of pseudo-labels for unlabeled boxes. We observe that existing methods suffer at higher levels of sparsity in the data due to noisy pseudo-labels. To prevent this, we propose an end-to-end system that learns to separate the proposals into labeled and unlabeled regions using Pseudo-positive mining. While the labeled regions are processed as usual, self-supervised learning is used to process the unlabeled regions thereby preventing the negative effects of noisy pseudo-labels. This novel approach has multiple advantages such as improved robustness to higher sparsity when compared to existing methods. We conduct exhaustive experiments on five splits on the PASCAL-VOC and COCO datasets achieving state-of-the-art performance. We also unify various splits used across literature for this task and present a standardized benchmark. On average, we improve by $2.6$, $3.9$ and $9.6$ mAP over previous state-of-the-art methods on three splits of increasing sparsity on COCO. Our project is publicly available at https://www.cs.umd.edu/~sakshams/SparseDet.



### Virtual Elastic Objects
- **Arxiv ID**: http://arxiv.org/abs/2201.04623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2201.04623v1)
- **Published**: 2022-01-12 18:59:03+00:00
- **Updated**: 2022-01-12 18:59:03+00:00
- **Authors**: Hsiao-yu Chen, Edgar Tretschk, Tuur Stuyck, Petr Kadlecek, Ladislav Kavan, Etienne Vouga, Christoph Lassner
- **Comment**: None
- **Journal**: None
- **Summary**: We present Virtual Elastic Objects (VEOs): virtual objects that not only look like their real-world counterparts but also behave like them, even when subject to novel interactions. Achieving this presents multiple challenges: not only do objects have to be captured including the physical forces acting on them, then faithfully reconstructed and rendered, but also plausible material parameters found and simulated. To create VEOs, we built a multi-view capture system that captures objects under the influence of a compressed air stream. Building on recent advances in model-free, dynamic Neural Radiance Fields, we reconstruct the objects and corresponding deformation fields. We propose to use a differentiable, particle-based simulator to use these deformation fields to find representative material parameters, which enable us to run new simulations. To render simulated objects, we devise a method for integrating the simulation results with Neural Radiance Fields. The resulting method is applicable to a wide range of scenarios: it can handle objects composed of inhomogeneous material, with very different shapes, and it can simulate interactions with other virtual objects. We present our results using a newly collected dataset of 12 objects under a variety of force fields, which will be shared with the community.



### UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.04676v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04676v3)
- **Published**: 2022-01-12 20:02:32+00:00
- **Updated**: 2022-02-08 16:36:12+00:00
- **Authors**: Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, Yu Qiao
- **Comment**: Published as a conference paper at ICLR 2022; 19pages, 7 figures
- **Journal**: None
- **Summary**: It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer.



### BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations
- **Arxiv ID**: http://arxiv.org/abs/2201.04684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04684v1)
- **Published**: 2022-01-12 20:28:34+00:00
- **Updated**: 2022-01-12 20:28:34+00:00
- **Authors**: Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Adela Barriuso, Sanja Fidler, Antonio Torralba
- **Comment**: https://nv-tlabs.github.io/big-datasetgan/
- **Journal**: None
- **Summary**: Annotating images with pixel-wise labels is a time-consuming and costly process. Recently, DatasetGAN showcased a promising alternative - to synthesize a large labeled dataset via a generative adversarial network (GAN) by exploiting a small set of manually labeled, GAN-generated images. Here, we scale DatasetGAN to ImageNet scale of class diversity. We take image samples from the class-conditional generative model BigGAN trained on ImageNet, and manually annotate 5 images per class, for all 1k classes. By training an effective feature segmentation architecture on top of BigGAN, we turn BigGAN into a labeled dataset generator. We further show that VQGAN can similarly serve as a dataset generator, leveraging the already annotated data. We create a new ImageNet benchmark by labeling an additional set of 8k real images and evaluate segmentation performance in a variety of settings. Through an extensive ablation study we show big gains in leveraging a large generated dataset to train different supervised and self-supervised backbone models on pixel-wise tasks. Furthermore, we demonstrate that using our synthesized datasets for pre-training leads to improvements over standard ImageNet pre-training on several downstream datasets, such as PASCAL-VOC, MS-COCO, Cityscapes and chest X-ray, as well as tasks (detection, segmentation). Our benchmark will be made public and maintain a leaderboard for this challenging task. Project Page: https://nv-tlabs.github.io/big-datasetgan/



### Semantic Labeling of Human Action For Visually Impaired And Blind People Scene Interaction
- **Arxiv ID**: http://arxiv.org/abs/2201.04706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.04706v1)
- **Published**: 2022-01-12 21:21:05+00:00
- **Updated**: 2022-01-12 21:21:05+00:00
- **Authors**: Leyla Benhamida, Slimane Larabi
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this work is to contribute to the development of a tactile device for visually impaired and blind persons in order to let them to understand actions of the surrounding people and to interact with them. First, based on the state-of-the-art methods of human action recognition from RGB-D sequences, we use the skeleton information provided by Kinect, with the disentangled and unified multi-scale Graph Convolutional (MS-G3D) model to recognize the performed actions. We tested this model on real scenes and found some of constraints and limitations. Next, we apply a fusion between skeleton modality with MS-G3D and depth modality with CNN in order to bypass the discussed limitations. Third, the recognized actions are labeled semantically and will be mapped into an output device perceivable by the touch sense.



### Partial-Attribution Instance Segmentation for Astronomical Source Detection and Deblending
- **Arxiv ID**: http://arxiv.org/abs/2201.04714v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04714v1)
- **Published**: 2022-01-12 21:59:13+00:00
- **Updated**: 2022-01-12 21:59:13+00:00
- **Authors**: Ryan Hausen, Brant Robertson
- **Comment**: Accepted to the Fourth Workshop on Machine Learning and the Physical
  Sciences, NeurIPS 2021, 6 pages, 1 figure
- **Journal**: None
- **Summary**: Astronomical source deblending is the process of separating the contribution of individual stars or galaxies (sources) to an image comprised of multiple, possibly overlapping sources. Astronomical sources display a wide range of sizes and brightnesses and may show substantial overlap in images. Astronomical imaging data can further challenge off-the-shelf computer vision algorithms owing to its high dynamic range, low signal-to-noise ratio, and unconventional image format. These challenges make source deblending an open area of astronomical research, and in this work, we introduce a new approach called Partial-Attribution Instance Segmentation that enables source detection and deblending in a manner tractable for deep learning models. We provide a novel neural network implementation as a demonstration of the method.



### Adversarially Robust Classification by Conditional Generative Model Inversion
- **Arxiv ID**: http://arxiv.org/abs/2201.04733v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.04733v1)
- **Published**: 2022-01-12 23:11:16+00:00
- **Updated**: 2022-01-12 23:11:16+00:00
- **Authors**: Mitra Alirezaei, Tolga Tasdizen
- **Comment**: None
- **Journal**: None
- **Summary**: Most adversarial attack defense methods rely on obfuscating gradients. These methods are successful in defending against gradient-based attacks; however, they are easily circumvented by attacks which either do not use the gradient or by attacks which approximate and use the corrected gradient. Defenses that do not obfuscate gradients such as adversarial training exist, but these approaches generally make assumptions about the attack such as its magnitude. We propose a classification model that does not obfuscate gradients and is robust by construction without assuming prior knowledge about the attack. Our method casts classification as an optimization problem where we "invert" a conditional generator trained on unperturbed, natural images to find the class that generates the closest sample to the query image. We hypothesize that a potential source of brittleness against adversarial attacks is the high-to-low-dimensional nature of feed-forward classifiers which allows an adversary to find small perturbations in the input space that lead to large changes in the output space. On the other hand, a generative model is typically a low-to-high-dimensional mapping. While the method is related to Defense-GAN, the use of a conditional generative model and inversion in our model instead of the feed-forward classifier is a critical difference. Unlike Defense-GAN, which was shown to generate obfuscated gradients that are easily circumvented, we show that our method does not obfuscate gradients. We demonstrate that our model is extremely robust against black-box attacks and has improved robustness against white-box attacks compared to naturally trained, feed-forward classifiers.




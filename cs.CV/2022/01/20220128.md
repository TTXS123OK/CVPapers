# Arxiv Papers in cs.CV on 2022-01-28
### Classification of White Blood Cell Leukemia with Low Number of Interpretable and Explainable Features
- **Arxiv ID**: http://arxiv.org/abs/2201.11864v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11864v1)
- **Published**: 2022-01-28 00:08:56+00:00
- **Updated**: 2022-01-28 00:08:56+00:00
- **Authors**: William Franz Lamberti
- **Comment**: None
- **Journal**: None
- **Summary**: White Blood Cell (WBC) Leukaemia is detected through image-based classification. Convolutional Neural Networks are used to learn the features needed to classify images of cells a malignant or healthy. However, this type of model requires learning a large number of parameters and is difficult to interpret and explain. Explainable AI (XAI) attempts to alleviate this issue by providing insights to how models make decisions. Therefore, we present an XAI model which uses only 24 explainable and interpretable features and is highly competitive to other approaches by outperforming them by about 4.38\%. Further, our approach provides insight into which variables are the most important for the classification of the cells. This insight provides evidence that when labs treat the WBCs differently, the importance of various metrics changes substantially. Understanding the important features for classification is vital in medical imaging diagnosis and, by extension, understanding the AI models built in scientific pursuits.



### Calibrating Histopathology Image Classifiers using Label Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2201.11866v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11866v1)
- **Published**: 2022-01-28 00:13:09+00:00
- **Updated**: 2022-01-28 00:13:09+00:00
- **Authors**: Jerry Wei, Lorenzo Torresani, Jason Wei, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: The classification of histopathology images fundamentally differs from traditional image classification tasks because histopathology images naturally exhibit a range of diagnostic features, resulting in a diverse range of annotator agreement levels. However, examples with high annotator disagreement are often either assigned the majority label or discarded entirely when training histopathology image classifiers. This widespread practice often yields classifiers that do not account for example difficulty and exhibit poor model calibration. In this paper, we ask: can we improve model calibration by endowing histopathology image classifiers with inductive biases about example difficulty?   We propose several label smoothing methods that utilize per-image annotator agreement. Though our methods are simple, we find that they substantially improve model calibration, while maintaining (or even improving) accuracy. For colorectal polyp classification, a common yet challenging task in gastrointestinal pathology, we find that our proposed agreement-aware label smoothing methods reduce calibration error by almost 70%. Moreover, we find that using model confidence as a proxy for annotator agreement also improves calibration and accuracy, suggesting that datasets without multiple annotators can still benefit from our proposed label smoothing methods via our proposed confidence-aware label smoothing methods.   Given the importance of calibration (especially in histopathology image analysis), the improvements from our proposed techniques merit further exploration and potential implementation in other histopathology image classification tasks.



### Infrastructure-Based Object Detection and Tracking for Cooperative Driving Automation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2201.11871v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2201.11871v2)
- **Published**: 2022-01-28 00:55:24+00:00
- **Updated**: 2022-03-19 23:02:57+00:00
- **Authors**: Zhengwei Bai, Guoyuan Wu, Xuewei Qi, Yongkang Liu, Kentaro Oguchi, Matthew J. Barth
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection plays a fundamental role in enabling Cooperative Driving Automation (CDA), which is regarded as the revolutionary solution to addressing safety, mobility, and sustainability issues of contemporary transportation systems. Although current computer vision technologies could provide satisfactory object detection results in occlusion-free scenarios, the perception performance of onboard sensors could be inevitably limited by the range and occlusion. Owing to flexible position and pose for sensor installation, infrastructure-based detection and tracking systems can enhance the perception capability for connected vehicles and thus quickly become one of the most popular research topics. In this paper, we review the research progress for infrastructure-based object detection and tracking systems. Architectures of roadside perception systems based on different types of sensors are reviewed to show a high-level description of the workflows for infrastructure-based perception systems. Roadside sensors and different perception methodologies are reviewed and analyzed with detailed literature to provide a low-level explanation for specific methods followed by Datasets and Simulators to draw an overall landscape of infrastructure-based object detection and tracking methods. Discussions are conducted to point out current opportunities, open problems, and anticipated future trends.



### Indicative Image Retrieval: Turning Blackbox Learning into Grey
- **Arxiv ID**: http://arxiv.org/abs/2201.11898v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2201.11898v2)
- **Published**: 2022-01-28 02:21:09+00:00
- **Updated**: 2022-03-16 09:29:23+00:00
- **Authors**: Xulu Zhang, Zhenqun Yang, Hao Tian, Qing Li, Xiaoyong Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning became the game changer for image retrieval soon after it was introduced. It promotes the feature extraction (by representation learning) as the core of image retrieval, with the relevance/matching evaluation being degenerated into simple similarity metrics. In many applications, we need the matching evidence to be indicated rather than just have the ranked list (e.g., the locations of the target proteins/cells/lesions in medical images). It is like the matched words need to be highlighted in search engines. However, this is not easy to implement without explicit relevance/matching modeling. The deep representation learning models are not feasible because of their blackbox nature. In this paper, we revisit the importance of relevance/matching modeling in deep learning era with an indicative retrieval setting. The study shows that it is possible to skip the representation learning and model the matching evidence directly. By removing the dependency on the pre-trained models, it has avoided a lot of related issues (e.g., the domain gap between classification and retrieval, the detail-diffusion caused by convolution, and so on). More importantly, the study demonstrates that the matching can be explicitly modeled and backtracked later for generating the matching evidence indications. It can improve the explainability of deep inference. Our method obtains a best performance in literature on both Oxford-5k and Paris-6k, and sets a new record of 97.77% on Oxford-5k (97.81% on Paris-6k) without extracting any deep features.



### Low-rank features based double transformation matrices learning for image classification
- **Arxiv ID**: http://arxiv.org/abs/2201.12351v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12351v2)
- **Published**: 2022-01-28 04:31:18+00:00
- **Updated**: 2022-02-17 14:29:42+00:00
- **Authors**: Yu-Hong Cai, Xiao-Jun Wu, Zhe Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Linear regression is a supervised method that has been widely used in classification tasks. In order to apply linear regression to classification tasks, a technique for relaxing regression targets was proposed. However, methods based on this technique ignore the pressure on a single transformation matrix due to the complex information contained in the data. A single transformation matrix in this case is too strict to provide a flexible projection, thus it is necessary to adopt relaxation on transformation matrix. This paper proposes a double transformation matrices learning method based on latent low-rank feature extraction. The core idea is to use double transformation matrices for relaxation, and jointly projecting the learned principal and salient features from two directions into the label space, which can share the pressure of a single transformation matrix. Firstly, the low-rank features are learned by the latent low rank representation (LatLRR) method which processes the original data from two directions. In this process, sparse noise is also separated, which alleviates its interference on projection learning to some extent. Then, two transformation matrices are introduced to process the two features separately, and the information useful for the classification is extracted. Finally, the two transformation matrices can be easily obtained by alternate optimization methods. Through such processing, even when a large amount of redundant information is contained in samples, our method can also obtain projection results that are easy to classify. Experiments on multiple data sets demonstrate the effectiveness of our approach for classification, especially for complex scenarios.



### Stereo Matching with Cost Volume based Sparse Disparity Propagation
- **Arxiv ID**: http://arxiv.org/abs/2201.11937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11937v1)
- **Published**: 2022-01-28 05:20:41+00:00
- **Updated**: 2022-01-28 05:20:41+00:00
- **Authors**: Wei Xue, Xiaojiang Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo matching is crucial for binocular stereo vision. Existing methods mainly focus on simple disparity map fusion to improve stereo matching, which require multiple dense or sparse disparity maps. In this paper, we propose a simple yet novel scheme, termed feature disparity propagation, to improve general stereo matching based on matching cost volume and sparse matching feature points. Specifically, our scheme first calculates a reliable sparse disparity map by local feature matching, and then refines the disparity map by propagating reliable disparities to neighboring pixels in the matching cost domain. In addition, considering the gradient and multi-scale information of local disparity regions, we present a $\rho$-Census cost measure based on the well-known AD-Census, which guarantees the robustness of cost volume even without the cost aggregation step. Extensive experiments on Middlebury stereo benchmark V3 demonstrate that our scheme achieves promising performance comparable to state-of-the-art methods.



### DICP: Doppler Iterative Closest Point Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2201.11944v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11944v2)
- **Published**: 2022-01-28 05:51:07+00:00
- **Updated**: 2022-05-31 04:07:47+00:00
- **Authors**: Bruno Hexsel, Heethesh Vhavle, Yi Chen
- **Comment**: Accepted at Robotics: Science and Systems (RSS) 2022
- **Journal**: None
- **Summary**: In this paper, we present a novel algorithm for point cloud registration for range sensors capable of measuring per-return instantaneous radial velocity: Doppler ICP. Existing variants of ICP that solely rely on geometry or other features generally fail to estimate the motion of the sensor correctly in scenarios that have non-distinctive features and/or repetitive geometric structures such as hallways, tunnels, highways, and bridges. We propose a new Doppler velocity objective function that exploits the compatibility of each point's Doppler measurement and the sensor's current motion estimate. We jointly optimize the Doppler velocity objective function and the geometric objective function which sufficiently constrains the point cloud alignment problem even in feature-denied environments. Furthermore, the correspondence matches used for the alignment are improved by pruning away the points from dynamic targets which generally degrade the ICP solution. We evaluate our method on data collected from real sensors and from simulation. Our results show that with the added Doppler velocity residual terms, our method achieves a significant improvement in registration accuracy along with faster convergence, on average, when compared to classical point-to-plane ICP that solely relies on geometric residuals.



### Shuffle Augmentation of Features from Unlabeled Data for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2201.11963v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.5.1; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2201.11963v1)
- **Published**: 2022-01-28 07:11:05+00:00
- **Updated**: 2022-01-28 07:11:05+00:00
- **Authors**: Changwei Xu, Jianfei Yang, Haoran Tang, Han Zou, Cheng Lu, Tianshuo Zhang
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA), a branch of transfer learning where labels for target samples are unavailable, has been widely researched and developed in recent years with the help of adversarially trained models. Although existing UDA algorithms are able to guide neural networks to extract transferable and discriminative features, classifiers are merely trained under the supervision of labeled source data. Given the inevitable discrepancy between source and target domains, the classifiers can hardly be aware of the target classification boundaries. In this paper, Shuffle Augmentation of Features (SAF), a novel UDA framework, is proposed to address the problem by providing the classifier with supervisory signals from target feature representations. SAF learns from the target samples, adaptively distills class-aware target features, and implicitly guides the classifier to find comprehensive class borders. Demonstrated by extensive experiments, the SAF module can be integrated into any existing adversarial UDA models to achieve performance improvements.



### 3D Visualization and Spatial Data Mining for Analysis of LULC Images
- **Arxiv ID**: http://arxiv.org/abs/2202.00123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00123v1)
- **Published**: 2022-01-28 07:51:31+00:00
- **Updated**: 2022-01-28 07:51:31+00:00
- **Authors**: B. G. Kodge
- **Comment**: 5 pages, 7 figures and 3 tables
- **Journal**: International Journal of Electrical, Electronics and Computer
  Science Engineering, Vol. 4, Issue 6, Dec-2017, P-ISSN: 2454-1222, E-ISSN:
  2348-2273, pp-63-67
- **Summary**: The present study is an attempt made to create a new tool for the analysis of Land Use Land Cover (LUCL) images in 3D visualization. This study mainly uses spatial data mining techniques on high resolution LULC satellite imagery. Visualization of feature space allows exploration of patterns in the image data and insight into the classification process and related uncertainty. Visual Data Mining provides added value to image classifications as the user can be involved in the classification process providing increased confidence in and understanding of the results. In this study, we present a prototype of image segmentation, K-Means clustering and 3D visualization tool for visual data mining (VDM) of LUCL satellite imagery into volume visualization. This volume based representation divides feature space into spheres or voxels. The visualization tool is showcased in a classification study of high-resolution LULC imagery of Latur district (Maharashtra state, India) is used as sample data.



### Generalized Visual Quality Assessment of GAN-Generated Face Images
- **Arxiv ID**: http://arxiv.org/abs/2201.11975v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11975v1)
- **Published**: 2022-01-28 07:54:49+00:00
- **Updated**: 2022-01-28 07:54:49+00:00
- **Authors**: Yu Tian, Zhangkai Ni, Baoliang Chen, Shiqi Wang, Hanli Wang, Sam Kwong
- **Comment**: 12 pages, 8 figures, journal paper
- **Journal**: None
- **Summary**: Recent years have witnessed the dramatically increased interest in face generation with generative adversarial networks (GANs). A number of successful GAN algorithms have been developed to produce vivid face images towards different application scenarios. However, little work has been dedicated to automatic quality assessment of such GAN-generated face images (GFIs), even less have been devoted to generalized and robust quality assessment of GFIs generated with unseen GAN model. Herein, we make the first attempt to study the subjective and objective quality towards generalized quality assessment of GFIs. More specifically, we establish a large-scale database consisting of GFIs from four GAN algorithms, the pseudo labels from image quality assessment (IQA) measures, as well as the human opinion scores via subjective testing. Subsequently, we develop a quality assessment model that is able to deliver accurate quality predictions for GFIs from both available and unseen GAN algorithms based on meta-learning. In particular, to learn shared knowledge from GFIs pairs that are born of limited GAN algorithms, we develop the convolutional block attention (CBA) and facial attributes-based analysis (ABA) modules, ensuring that the learned knowledge tends to be consistent with human visual perception. Extensive experiments exhibit that the proposed model achieves better performance compared with the state-of-the-art IQA models, and is capable of retaining the effectiveness when evaluating GFIs from the unseen GAN algorithms.



### Computer-aided Recognition and Assessment of a Porous Bioelastomer on Ultrasound Images for Regenerative Medicine Applications
- **Arxiv ID**: http://arxiv.org/abs/2201.11987v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.11987v2)
- **Published**: 2022-01-28 08:42:58+00:00
- **Updated**: 2022-01-31 01:26:18+00:00
- **Authors**: Dun Wang, Kaixuan Guo, Yanying Zhu, Jia Sun, Aliona Dreglea, Jiao Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Biodegradable elastic scaffolds have attracted more and more attention in the field of soft tissue repair and tissue engineering. These scaffolds made of porous bioelastomers support tissue ingrowth along with their own degradation. It is necessary to develop a computer-aided analyzing method based on ultrasound images to identify the degradation performance of the scaffold, not only to obviate the need to do destructive testing, but also to monitor the scaffold's degradation and tissue ingrowth over time. It is difficult using a single traditional image processing algorithm to extract continuous and accurate contour of a porous bioelastomer. This paper proposes a joint algorithm for the bioelastomer's contour detection and a texture feature extraction method for monitoring the degradation behavior of the bioelastomer. Mean-shift clustering method is used to obtain the bioelastomer's and native tissue's clustering feature information. Then the OTSU image binarization method automatically selects the optimal threshold value to convert the grayscale ultrasound image into a binary image. The Canny edge detector is used to extract the complete bioelastomer's contour. The first-order and second-order statistical features of texture are extracted. The proposed joint algorithm not only achieves the ideal extraction of the bioelastomer's contours in ultrasound images, but also gives valuable feedback of the degradation behavior of the bioelastomer at the implant site based on the changes of texture characteristics and contour area. The preliminary results of this study suggest that the proposed computer-aided image processing techniques have values and potentials in the non-invasive analysis of tissue scaffolds in vivo based on ultrasound images and may help tissue engineers evaluate the tissue scaffold's degradation and cellular ingrowth progress and improve the scaffold designs.



### Hybrid Contrastive Learning with Cluster Ensemble for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2201.11995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11995v2)
- **Published**: 2022-01-28 09:15:20+00:00
- **Updated**: 2022-04-14 07:58:32+00:00
- **Authors**: He Sun, Mingkun Li, Chun-Guang Li
- **Comment**: accepted by ACPR2021
- **Journal**: None
- **Summary**: Unsupervised person re-identification (ReID) aims to match a query image of a pedestrian to the images in gallery set without supervision labels. The most popular approaches to tackle unsupervised person ReID are usually performing a clustering algorithm to yield pseudo labels at first and then exploit the pseudo labels to train a deep neural network. However, the pseudo labels are noisy and sensitive to the hyper-parameter(s) in clustering algorithm. In this paper, we propose a Hybrid Contrastive Learning (HCL) approach for unsupervised person ReID, which is based on a hybrid between instance-level and cluster-level contrastive loss functions. Moreover, we present a Multi-Granularity Clustering Ensemble based Hybrid Contrastive Learning (MGCE-HCL) approach, which adopts a multi-granularity clustering ensemble strategy to mine priority information among the pseudo positive sample pairs and defines a priority-weighted hybrid contrastive loss for better tolerating the noises in the pseudo positive samples. We conduct extensive experiments on two benchmark datasets Market-1501 and DukeMTMC-reID. Experimental results validate the effectiveness of our proposals.



### Deep Networks for Image and Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.11996v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11996v1)
- **Published**: 2022-01-28 09:15:21+00:00
- **Updated**: 2022-01-28 09:15:21+00:00
- **Authors**: Kuldeep Purohit, Srimanta Mandal, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: Efficiency of gradient propagation in intermediate layers of convolutional neural networks is of key importance for super-resolution task. To this end, we propose a deep architecture for single image super-resolution (SISR), which is built using efficient convolutional units we refer to as mixed-dense connection blocks (MDCB). The design of MDCB combines the strengths of both residual and dense connection strategies, while overcoming their limitations. To enable super-resolution for multiple factors, we propose a scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. This leads to improved performance and promotes parametric efficiency for higher factors. We train two versions of our network to enhance complementary image qualities using different loss configurations. We further employ our network for video super-resolution task, where our network learns to aggregate information from multiple frames and maintain spatio-temporal consistency. The proposed networks lead to qualitative and quantitative improvements over state-of-the-art techniques on image and video super-resolution benchmarks.



### Image Superresolution using Scale-Recurrent Dense Network
- **Arxiv ID**: http://arxiv.org/abs/2201.11998v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11998v1)
- **Published**: 2022-01-28 09:18:43+00:00
- **Updated**: 2022-01-28 09:18:43+00:00
- **Authors**: Kuldeep Purohit, Srimanta Mandal, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in the design of convolutional neural network (CNN) have yielded significant improvements in the performance of image super-resolution (SR). The boost in performance can be attributed to the presence of residual or dense connections within the intermediate layers of these networks. The efficient combination of such connections can reduce the number of parameters drastically while maintaining the restoration quality. In this paper, we propose a scale recurrent SR architecture built upon units containing series of dense connections within a residual block (Residual Dense Blocks (RDBs)) that allow extraction of abundant local features from the image. Our scale recurrent design delivers competitive performance for higher scale factors while being parametrically more efficient as compared to current state-of-the-art approaches. To further improve the performance of our network, we employ multiple residual connections in intermediate layers (referred to as Multi-Residual Dense Blocks), which improves gradient propagation in existing layers. Recent works have discovered that conventional loss functions can guide a network to produce results which have high PSNRs but are perceptually inferior. We mitigate this issue by utilizing a Generative Adversarial Network (GAN) based framework and deep feature (VGG) losses to train our network. We experimentally demonstrate that different weighted combinations of the VGG loss and the adversarial loss enable our network outputs to traverse along the perception-distortion curve. The proposed networks perform favorably against existing methods, both perceptually and objectively (PSNR-based) with fewer parameters.



### Dual Learning Music Composition and Dance Choreography
- **Arxiv ID**: http://arxiv.org/abs/2201.11999v1
- **DOI**: 10.1145/3474085.3475180
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.11999v1)
- **Published**: 2022-01-28 09:20:28+00:00
- **Updated**: 2022-01-28 09:20:28+00:00
- **Authors**: Shuang Wu, Zhenguang Li, Shijian Lu, Li Cheng
- **Comment**: ACMMM 2021 (Oral)
- **Journal**: None
- **Summary**: Music and dance have always co-existed as pillars of human activities, contributing immensely to the cultural, social, and entertainment functions in virtually all societies. Notwithstanding the gradual systematization of music and dance into two independent disciplines, their intimate connection is undeniable and one art-form often appears incomplete without the other. Recent research works have studied generative models for dance sequences conditioned on music. The dual task of composing music for given dances, however, has been largely overlooked. In this paper, we propose a novel extension, where we jointly model both tasks in a dual learning approach. To leverage the duality of the two modalities, we introduce an optimal transport objective to align feature embeddings, as well as a cycle consistency loss to foster overall consistency. Experimental results demonstrate that our dual learning framework improves individual task performance, delivering generated music compositions and dance choreographs that are realistic and faithful to the conditioned inputs.



### Unfolding a blurred image
- **Arxiv ID**: http://arxiv.org/abs/2201.12010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12010v1)
- **Published**: 2022-01-28 09:39:55+00:00
- **Updated**: 2022-01-28 09:39:55+00:00
- **Authors**: Kuldeep Purohit, Anshul Shah, A. N. Rajagopalan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1804.02913
- **Journal**: None
- **Summary**: We present a solution for the goal of extracting a video from a single motion blurred image to sequentially reconstruct the clear views of a scene as beheld by the camera during the time of exposure. We first learn motion representation from sharp videos in an unsupervised manner through training of a convolutional recurrent video autoencoder network that performs a surrogate task of video reconstruction. Once trained, it is employed for guided training of a motion encoder for blurred images. This network extracts embedded motion information from the blurred image to generate a sharp video in conjunction with the trained recurrent video decoder. As an intermediate step, we also design an efficient architecture that enables real-time single image deblurring and outperforms competing methods across all factors: accuracy, speed, and compactness. Experiments on real scenes and standard datasets demonstrate the superiority of our framework over the state-of-the-art and its ability to generate a plausible sequence of temporally consistent sharp frames.



### Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM
- **Arxiv ID**: http://arxiv.org/abs/2201.12047v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12047v2)
- **Published**: 2022-01-28 11:23:29+00:00
- **Updated**: 2023-03-13 07:02:14+00:00
- **Authors**: Ali Caglayan, Nevrez Imamoglu, Oguzhan Guclu, Ali Osman Serhatoglu, Weimin Wang, Ahmet Burak Can, Ryosuke Nakamura
- **Comment**: This article has been removed by arXiv administrators because the
  submitter did not have the authority to grant the license at the time of
  submission
- **Journal**: None
- **Summary**: Deep learning models as an emerging topic have shown great progress in various fields. Especially, visualization tools such as class activation mapping methods provided visual explanation on the reasoning of convolutional neural networks (CNNs). By using the gradients of the network layers, it is possible to demonstrate where the networks pay attention during a specific image recognition task. Moreover, these gradients can be integrated with CNN features for localizing more generalized task dependent attentive (salient) objects in scenes. Despite this progress, there is not much explicit usage of this gradient (network attention) information to integrate with CNN representations for object semantics. This can be very useful for visual tasks such as simultaneous localization and mapping (SLAM) where CNN representations of spatially attentive object locations may lead to improved performance. Therefore, in this work, we propose the use of task specific network attention for RGB-D indoor SLAM. To do so, we integrate layer-wise object attention information (layer gradients) with CNN layer representations to improve frame association performance in an RGB-D indoor SLAM method. Experiments show promising results with improved performance over the baseline.



### Detection of fake faces in videos
- **Arxiv ID**: http://arxiv.org/abs/2201.12051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12051v1)
- **Published**: 2022-01-28 11:29:07+00:00
- **Updated**: 2022-01-28 11:29:07+00:00
- **Authors**: M. Shamanth, Russel Mathias, Dr Vijayalakshmi MN
- **Comment**: 5 pages, 11 figures
- **Journal**: None
- **Summary**: : Deep learning methodologies have been used to create applications that can cause threats to privacy, democracy and national security and could be used to further amplify malicious activities. One of those deep learning-powered applications in recent times is synthesized videos of famous personalities. According to Forbes, Generative Adversarial Networks(GANs) generated fake videos growing exponentially every year and the organization known as Deeptrace had estimated an increase of deepfakes by 84% from the year 2018 to 2019. They are used to generate and modify human faces, where most of the existing fake videos are of prurient non-consensual nature, of which its estimates to be around 96% and some carried out impersonating personalities for cyber crime. In this paper, available video datasets are identified and a pretrained model BlazeFace is used to detect faces, and a ResNet and Xception ensembled architectured neural network trained on the dataset to achieve the goal of detection of fake faces in videos. The model is optimized over a loss value and log loss values and evaluated over its F1 score. Over a sample of data, it is observed that focal loss provides better accuracy, F1 score and loss as the gamma of the focal loss becomes a hyper parameter. This provides a k-folded accuracy of around 91% at its peak in a training cycle with the real world accuracy subjected to change over time as the model decays.



### You Only Cut Once: Boosting Data Augmentation with a Single Cut
- **Arxiv ID**: http://arxiv.org/abs/2201.12078v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12078v3)
- **Published**: 2022-01-28 12:34:40+00:00
- **Updated**: 2022-06-15 08:19:37+00:00
- **Authors**: Junlin Han, Pengfei Fang, Weihao Li, Jie Hong, Mohammad Ali Armin, Ian Reid, Lars Petersson, Hongdong Li
- **Comment**: ICML 2022, Code: https://github.com/JunlinHan/YOCO
- **Journal**: None
- **Summary**: We present You Only Cut Once (YOCO) for performing data augmentations. YOCO cuts one image into two pieces and performs data augmentations individually within each piece. Applying YOCO improves the diversity of the augmentation per sample and encourages neural networks to recognize objects from partial information. YOCO enjoys the properties of parameter-free, easy usage, and boosting almost all augmentations for free. Thorough experiments are conducted to evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly applied to varying data augmentations, neural network architectures, and brings performance gains on CIFAR and ImageNet classification tasks, sometimes surpassing conventional image-level augmentation by large margins. Moreover, we show YOCO benefits contrastive pre-training toward a more powerful representation that can be better transferred to multiple downstream tasks. Finally, we study a number of variants of YOCO and empirically analyze the performance for respective settings. Code is available at GitHub.



### DynaMixer: A Vision MLP Architecture with Dynamic Mixing
- **Arxiv ID**: http://arxiv.org/abs/2201.12083v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12083v3)
- **Published**: 2022-01-28 12:43:14+00:00
- **Updated**: 2022-06-18 03:38:04+00:00
- **Authors**: Ziyu Wang, Wenhao Jiang, Yiming Zhu, Li Yuan, Yibing Song, Wei Liu
- **Comment**: icml2022
- **Journal**: None
- **Summary**: Recently, MLP-like vision models have achieved promising performances on mainstream visual recognition tasks. In contrast with vision transformers and CNNs, the success of MLP-like models shows that simple information fusion operations among tokens and channels can yield a good representation power for deep recognition models. However, existing MLP-like models fuse tokens through static fusion operations, lacking adaptability to the contents of the tokens to be mixed. Thus, customary information fusion procedures are not effective enough. To this end, this paper presents an efficient MLP-like network architecture, dubbed DynaMixer, resorting to dynamic information fusion. Critically, we propose a procedure, on which the DynaMixer model relies, to dynamically generate mixing matrices by leveraging the contents of all the tokens to be mixed. To reduce the time complexity and improve the robustness, a dimensionality reduction technique and a multi-segment fusion mechanism are adopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\% top-1 accuracy on the ImageNet-1K dataset without extra training data, performing favorably against the state-of-the-art vision MLP models. When the number of parameters is reduced to 26M, it still achieves 82.7\% top-1 accuracy, surpassing the existing MLP-like models with a similar capacity. The code is available at \url{https://github.com/ziyuwwang/DynaMixer}.



### Psychophysical Evaluation of Human Performance in Detecting Digital Face Image Manipulations
- **Arxiv ID**: http://arxiv.org/abs/2201.12084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12084v1)
- **Published**: 2022-01-28 12:45:33+00:00
- **Updated**: 2022-01-28 12:45:33+00:00
- **Authors**: Robert Nichols, Christian Rathgeb, Pawel Drozdowski, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, increasing deployment of face recognition technology in security-critical settings, such as border control or law enforcement, has led to considerable interest in the vulnerability of face recognition systems to attacks utilising legitimate documents, which are issued on the basis of digitally manipulated face images. As automated manipulation and attack detection remains a challenging task, conventional processes with human inspectors performing identity verification remain indispensable. These circumstances merit a closer investigation of human capabilities in detecting manipulated face images, as previous work in this field is sparse and often concentrated only on specific scenarios and biometric characteristics.   This work introduces a web-based, remote visual discrimination experiment on the basis of principles adopted from the field of psychophysics and subsequently discusses interdisciplinary opportunities with the aim of examining human proficiency in detecting different types of digitally manipulated face images, specifically face swapping, morphing, and retouching. In addition to analysing appropriate performance measures, a possible metric of detectability is explored. Experimental data of 306 probands indicate that detection performance is widely distributed across the population and detection of certain types of face image manipulations is much more challenging than others.



### BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.12086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12086v2)
- **Published**: 2022-01-28 12:49:48+00:00
- **Updated**: 2022-02-15 05:43:32+00:00
- **Authors**: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.



### Label uncertainty-guided multi-stream model for disease screening
- **Arxiv ID**: http://arxiv.org/abs/2201.12089v1
- **DOI**: 10.1109/ISBI52829.2022.9761483
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12089v1)
- **Published**: 2022-01-28 12:53:18+00:00
- **Updated**: 2022-01-28 12:53:18+00:00
- **Authors**: Chi Liu, Zongyuan Ge, Mingguang He, Xiaotong Han
- **Comment**: To appear in ISBI 2022
- **Journal**: None
- **Summary**: The annotation of disease severity for medical image datasets often relies on collaborative decisions from multiple human graders. The intra-observer variability derived from individual differences always persists in this process, yet the influence is often underestimated. In this paper, we cast the intra-observer variability as an uncertainty problem and incorporate the label uncertainty information as guidance into the disease screening model to improve the final decision. The main idea is dividing the images into simple and hard cases by uncertainty information, and then developing a multi-stream network to deal with different cases separately. Particularly, for hard cases, we strengthen the network's capacity in capturing the correct disease features and resisting the interference of uncertainty. Experiments on a fundus image-based glaucoma screening case study show that the proposed model outperforms several baselines, especially in screening hard cases.



### Leveraging Inlier Correspondences Proportion for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2201.12094v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12094v2)
- **Published**: 2022-01-28 13:04:54+00:00
- **Updated**: 2022-11-21 13:57:14+00:00
- **Authors**: Lifa Zhu, Haining Guan, Changwei Lin, Renmin Han
- **Comment**: None
- **Journal**: None
- **Summary**: In feature-learning based point cloud registration, the correct correspondence construction is vital for the subsequent transformation estimation. However, it is still a challenge to extract discriminative features from point cloud, especially when the input is partial and composed by indistinguishable surfaces (planes, smooth surfaces, etc.). As a result, the proportion of inlier correspondences that precisely match points between two unaligned point clouds is beyond satisfaction. Motivated by this, we devise several techniques to promote feature-learning based point cloud registration performance by leveraging inlier correspondences proportion: a pyramid hierarchy decoder to characterize point features in multiple scales, a consistent voting strategy to maintain consistent correspondences and a geometry guided encoding module to take geometric characteristics into consideration. Based on the above techniques, We build our Geometry-guided Consistent Network (GCNet), and challenge GCNet by indoor, outdoor and object-centric synthetic datasets. Comprehensive experiments demonstrate that GCNet outperforms the state-of-the-art methods and the techniques used in GCNet is model-agnostic, which could be easily migrated to other feature-based deep learning or traditional registration methods, and dramatically improve the performance. The code is available at https://github.com/zhulf0804/NgeNet.



### Detecting Owner-member Relationship with Graph Convolution Network in Fisheye Camera System
- **Arxiv ID**: http://arxiv.org/abs/2201.12099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12099v1)
- **Published**: 2022-01-28 13:12:27+00:00
- **Updated**: 2022-01-28 13:12:27+00:00
- **Authors**: Zizhang Wu, Jason Wang, Tianhao Xu, Fan Wang
- **Comment**: Accepted by Pattern Recognition. arXiv admin note: substantial text
  overlap with arXiv:2103.16099
- **Journal**: None
- **Summary**: The owner-member relationship between wheels and vehicles contributes significantly to the 3D perception of vehicles, especially in embedded environments. However, to leverage this relationship we must face two major challenges: i) Traditional IoU-based heuristics have difficulty handling occluded traffic congestion scenarios. ii) The effectiveness and applicability of the solution in a vehicle-mounted system is difficult. To address these issues, we propose an innovative relationship prediction method, DeepWORD, by designing a graph convolutional network (GCN). Specifically, to improve the information richness, we use feature maps with local correlation as input to the nodes. Subsequently, we introduce a graph attention network (GAT) to dynamically correct the a priori estimation bias. Finally, we designed a dataset as a large-scale benchmark which has annotated owner-member relationship, called WORD. In the experiments we learned that the proposed method achieved state-of-the-art accuracy and real-time performance. The WORD dataset is made publicly available at https://github.com/NamespaceMain/ownermember-relationship-dataset.



### Feature Visualization within an Automated Design Assessment leveraging Explainable Artificial Intelligence Methods
- **Arxiv ID**: http://arxiv.org/abs/2201.12107v1
- **DOI**: 10.1016/j.procir.2021.05.075
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.12107v1)
- **Published**: 2022-01-28 13:31:42+00:00
- **Updated**: 2022-01-28 13:31:42+00:00
- **Authors**: Raoul Sch√∂nhof, Artem Werner, Jannes Elstner, Boldizsar Zopcsak, Ramez Awad, Marco Huber
- **Comment**: CIRP Design 2021, 10.1016/j.procir.2021.05.075
- **Journal**: 2021, Procedia CIRP 100(7):331-336
- **Summary**: Not only automation of manufacturing processes but also automation of automation procedures itself become increasingly relevant to automation research. In this context, automated capability assessment, mainly leveraged by deep learning systems driven from 3D CAD data, have been presented. Current assessment systems may be able to assess CAD data with regards to abstract features, e.g. the ability to automatically separate components from bulk goods, or the presence of gripping surfaces. Nevertheless, they suffer from the factor of black box systems, where an assessment can be learned and generated easily, but without any geometrical indicator about the reasons of the system's decision. By utilizing explainable AI (xAI) methods, we attempt to open up the black box. Explainable AI methods have been used in order to assess whether a neural network has successfully learned a given task or to analyze which features of an input might lead to an adversarial attack. These methods aim to derive additional insights into a neural network, by analyzing patterns from a given input and its impact to the network output. Within the NeuroCAD Project, xAI methods are used to identify geometrical features which are associated with a certain abstract feature. Within this work, a sensitivity analysis (SA), the layer-wise relevance propagation (LRP), the Gradient-weighted Class Activation Mapping (Grad-CAM) method as well as the Local Interpretable Model-Agnostic Explanations (LIME) have been implemented in the NeuroCAD environment, allowing not only to assess CAD models but also to identify features which have been relevant for the network decision. In the medium run, this might enable to identify regions of interest supporting product designers to optimize their models with regards to assembly processes.



### Rethinking Attention-Model Explainability through Faithfulness Violation Test
- **Arxiv ID**: http://arxiv.org/abs/2201.12114v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12114v3)
- **Published**: 2022-01-28 13:42:31+00:00
- **Updated**: 2022-07-05 06:42:28+00:00
- **Authors**: Yibing Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jing Li, Shiqi Wang
- **Comment**: Accepted to ICML 2022
- **Journal**: None
- **Summary**: Attention mechanisms are dominating the explainability of deep models. They produce probability distributions over the input, which are widely deemed as feature-importance indicators. However, in this paper, we find one critical limitation in attention explanations: weakness in identifying the polarity of feature impact. This would be somehow misleading -- features with higher attention weights may not faithfully contribute to model predictions; instead, they can impose suppression effects. With this finding, we reflect on the explainability of current attention-based techniques, such as Attentio$\odot$Gradient and LRP-based attention explanations. We first propose an actionable diagnostic methodology (henceforth faithfulness violation test) to measure the consistency between explanation weights and the impact polarity. Through the extensive experiments, we then show that most tested explanation methods are unexpectedly hindered by the faithfulness violation issue, especially the raw attention. Empirical analyses on the factors affecting violation issues further provide useful observations for adopting explanation methods in attention models.



### DELAUNAY: a dataset of abstract art for psychophysical and machine learning research
- **Arxiv ID**: http://arxiv.org/abs/2201.12123v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2201.12123v1)
- **Published**: 2022-01-28 13:57:32+00:00
- **Updated**: 2022-01-28 13:57:32+00:00
- **Authors**: Camille Gontier, Jakob Jordan, Mihai A. Petrovici
- **Comment**: None
- **Journal**: None
- **Summary**: Image datasets are commonly used in psychophysical experiments and in machine learning research. Most publicly available datasets are comprised of images of realistic and natural objects. However, while typical machine learning models lack any domain specific knowledge about natural objects, humans can leverage prior experience for such data, making comparisons between artificial and natural learning challenging. Here, we introduce DELAUNAY, a dataset of abstract paintings and non-figurative art objects labelled by the artists' names. This dataset provides a middle ground between natural images and artificial patterns and can thus be used in a variety of contexts, for example to investigate the sample efficiency of humans and artificial neural networks. Finally, we train an off-the-shelf convolutional neural network on DELAUNAY, highlighting several of its intriguing features.



### Development of a neural network to recognize standards and features from 3D CAD models
- **Arxiv ID**: http://arxiv.org/abs/2202.00573v1
- **DOI**: 10.1016/j.procir.2020.03.010
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00573v1)
- **Published**: 2022-01-28 14:01:35+00:00
- **Updated**: 2022-01-28 14:01:35+00:00
- **Authors**: Alexander Neb, Iyed Briki, Raoul Schoenhof
- **Comment**: None
- **Journal**: Procedia CIRP Volume 93, 2020, Pages 1429-1434
- **Summary**: Focus of this work is to recognize standards and further features directly from 3D CAD models. For this reason, a neural network was trained to recognize nine classes of machine elements. After the system identified a part as a standard, like a hexagon head screw after the DIN EN ISO 8676, it accesses the geometrical information of the CAD system via the Application Programming Interface (API). In the API, the system searches for necessary information to describe the part appropriately. Based on this information standardized parts can be recognized in detail and supplemented with further information.



### O-ViT: Orthogonal Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.12133v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12133v2)
- **Published**: 2022-01-28 14:18:52+00:00
- **Updated**: 2022-02-16 13:49:43+00:00
- **Authors**: Yanhong Fei, Yingjie Liu, Xian Wei, Mingsong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the tremendous success of the self-attention mechanism in natural language processing, the Vision Transformer (ViT) creatively applies it to image patch sequences and achieves incredible performance. However, the scaled dot-product self-attention of ViT brings about scale ambiguity to the structure of the original feature space. To address this problem, we propose a novel method named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the geometric perspective. O-ViT limits parameters of self-attention blocks to be on the norm-keeping orthogonal manifold, which can keep the geometry of the feature space. Moreover, O-ViT achieves both orthogonal constraints and cheap optimization overhead by adopting a surjective mapping between the orthogonal group and its Lie algebra.We have conducted comparative experiments on image recognition tasks to demonstrate O-ViT's validity and experiments show that O-ViT can boost the performance of ViT by up to 3.6%.



### Carotid artery wall segmentation in ultrasound image sequences using a deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2201.12152v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2201.12152v1)
- **Published**: 2022-01-28 14:37:20+00:00
- **Updated**: 2022-01-28 14:37:20+00:00
- **Authors**: Nolann Lain√©, Guillaume Zahnd, Herv √© Liebgott, Maciej Orkisz
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: The objective of this study is the segmentation of the intima-media complex of the common carotid artery, on longitudinal ultrasound images, to measure its thickness. We propose a fully automatic region-based segmentation method, involving a supervised region-based deep-learning approach based on a dilated U-net network. It was trained and evaluated using a 5-fold cross-validation on a multicenter database composed of 2176 images annotated by two experts. The resulting mean absolute difference (<120 um) compared to reference annotations was less than the inter-observer variability (180 um). With a 98.7% success rate, i.e., only 1.3% cases requiring manual correction, the proposed method has been shown to be robust and thus may be recommended for use in clinical practice.



### Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2201.12170v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12170v4)
- **Published**: 2022-01-28 15:11:34+00:00
- **Updated**: 2022-06-08 14:35:01+00:00
- **Authors**: Christoph Angermann, Matthias Schwab, Markus Haltmeier, Christian Laubichler, Steinbj√∂rn J√≥nsson
- **Comment**: arXiv admin note: text overlap with arXiv:2103.16938
- **Journal**: None
- **Summary**: Real-time estimation of actual object depth is an essential module for various autonomous system tasks such as 3D reconstruction, scene understanding and condition assessment. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks has yielded approaches that succeed in achieving realistic depth synthesis out of a simple RGB modality. Most of these models are based on paired RGB-depth data and/or the availability of video sequences and stereo images. The lack of sequences, stereo data and RGB-depth pairs makes depth estimation a fully unsupervised single-image transfer problem that has barely been explored so far. This study builds on recent advances in the field of generative neural networks in order to establish fully unsupervised single-shot depth estimation. Two generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance, a novel perceptual reconstruction term and hand-crafted image filters. We comprehensively evaluate the models using industrial surface depth data as well as the Texas 3D Face Recognition Database, the CelebAMask-HQ database of human portraits and the SURREAL dataset that records body depth. For each evaluation dataset the proposed method shows a significant increase in depth accuracy compared to state-of-the-art single-image transfer methods.



### Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks
- **Arxiv ID**: http://arxiv.org/abs/2201.12179v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12179v4)
- **Published**: 2022-01-28 15:25:50+00:00
- **Updated**: 2022-06-09 08:48:08+00:00
- **Authors**: Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, Kristian Kersting
- **Comment**: Accepted by ICML 2022
- **Journal**: None
- **Summary**: Model inversion attacks (MIAs) aim to create synthetic images that reflect the class-wise characteristics from a target classifier's private training data by exploiting the model's learned knowledge. Previous research has developed generative MIAs that use generative adversarial networks (GANs) as image priors tailored to a specific target model. This makes the attacks time- and resource-consuming, inflexible, and susceptible to distributional shifts between datasets. To overcome these drawbacks, we present Plug & Play Attacks, which relax the dependency between the target model and image prior, and enable the use of a single GAN to attack a wide range of targets, requiring only minor adjustments to the attack. Moreover, we show that powerful MIAs are possible even with publicly available pre-trained GANs and under strong distributional shifts, for which previous approaches fail to produce meaningful results. Our extensive evaluation confirms the improved robustness and flexibility of Plug & Play Attacks and their ability to create high-quality images revealing sensitive class characteristics.



### A tomographic workflow to enable deep learning for X-ray based foreign object detection
- **Arxiv ID**: http://arxiv.org/abs/2201.12184v1
- **DOI**: 10.1016/j.eswa.2022.117768
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12184v1)
- **Published**: 2022-01-28 15:33:20+00:00
- **Updated**: 2022-01-28 15:33:20+00:00
- **Authors**: Math√© T. Zeegers, Tristan van Leeuwen, Dani√´l M. Pelt, Sophia Bethany Coban, Robert van Liere, Kees Joost Batenburg
- **Comment**: This paper is under consideration at Expert Systems with
  Applications. 22 pages, 15 figures
- **Journal**: Expert Systems with Applications 206 (2022) 117768
- **Summary**: Detection of unwanted (`foreign') objects within products is a common procedure in many branches of industry for maintaining production quality. X-ray imaging is a fast, non-invasive and widely applicable method for foreign object detection. Deep learning has recently emerged as a powerful approach for recognizing patterns in radiographs (i.e., X-ray images), enabling automated X-ray based foreign object detection. However, these methods require a large number of training examples and manual annotation of these examples is a subjective and laborious task. In this work, we propose a Computed Tomography (CT) based method for producing training data for supervised learning of foreign object detection, with minimal labour requirements. In our approach, a few representative objects are CT scanned and reconstructed in 3D. The radiographs that have been acquired as part of the CT-scan data serve as input for the machine learning method. High-quality ground truth locations of the foreign objects are obtained through accurate 3D reconstructions and segmentations. Using these segmented volumes, corresponding 2D segmentations are obtained by creating virtual projections. We outline the benefits of objectively and reproducibly generating training data in this way compared to conventional radiograph annotation. In addition, we show how the accuracy depends on the number of objects used for the CT reconstructions. The results show that in this workflow generally only a relatively small number of representative objects (i.e., fewer than 10) are needed to achieve adequate detection performance in an industrial setting. Moreover, for real experimental data we show that the workflow leads to higher foreign object detection accuracies than with standard radiograph annotation.



### M√∂bius Convolutions for Spherical CNNs
- **Arxiv ID**: http://arxiv.org/abs/2201.12212v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, math.RT
- **Links**: [PDF](http://arxiv.org/pdf/2201.12212v2)
- **Published**: 2022-01-28 16:11:47+00:00
- **Updated**: 2022-05-12 18:59:15+00:00
- **Authors**: Thomas W. Mitchel, Noam Aigerman, Vladimir G. Kim, Michael Kazhdan
- **Comment**: SIGGRAPH 2022
- **Journal**: None
- **Summary**: M\"obius transformations play an important role in both geometry and spherical image processing - they are the group of conformal automorphisms of 2D surfaces and the spherical equivalent of homographies. Here we present a novel, M\"obius-equivariant spherical convolution operator which we call M\"obius convolution, and with it, develop the foundations for M\"obius-equivariant spherical CNNs. Our approach is based on a simple observation: to achieve equivariance, we only need to consider the lower-dimensional subgroup which transforms the positions of points as seen in the frames of their neighbors. To efficiently compute M\"obius convolutions at scale we derive an approximation of the action of the transformations on spherical filters, allowing us to compute our convolutions in the spectral domain with the fast Spherical Harmonic Transform. The resulting framework is both flexible and descriptive, and we demonstrate its utility by achieving promising results in both shape classification and image segmentation tasks.



### Self-paced learning to improve text row detection in historical documents with missing labels
- **Arxiv ID**: http://arxiv.org/abs/2201.12216v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12216v3)
- **Published**: 2022-01-28 16:17:48+00:00
- **Updated**: 2022-08-15 20:47:16+00:00
- **Authors**: Mihaela Gaman, Lida Ghadamiyan, Radu Tudor Ionescu, Marius Popescu
- **Comment**: Accepted at ECCV Workshop on Text in Everything (TiE 2022)
- **Journal**: None
- **Summary**: An important preliminary step of optical character recognition systems is the detection of text rows. To address this task in the context of historical data with missing labels, we propose a self-paced learning algorithm capable of improving the row detection performance. We conjecture that pages with more ground-truth bounding boxes are less likely to have missing annotations. Based on this hypothesis, we sort the training examples in descending order with respect to the number of ground-truth bounding boxes, and organize them into k batches. Using our self-paced learning method, we train a row detector over k iterations, progressively adding batches with less ground-truth annotations. At each iteration, we combine the ground-truth bounding boxes with pseudo-bounding boxes (bounding boxes predicted by the model itself) using non-maximum suppression, and we include the resulting annotations at the next training iteration. We demonstrate that our self-paced learning strategy brings significant performance gains on two data sets of historical documents, improving the average precision of YOLOv4 with more than 12% on one data set and 39% on the other.



### Continuous Deep Equilibrium Models: Training Neural ODEs faster by integrating them to Infinity
- **Arxiv ID**: http://arxiv.org/abs/2201.12240v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.DS
- **Links**: [PDF](http://arxiv.org/pdf/2201.12240v4)
- **Published**: 2022-01-28 16:51:54+00:00
- **Updated**: 2023-03-03 16:34:22+00:00
- **Authors**: Avik Pal, Alan Edelman, Christopher Rackauckas
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit models separate the definition of a layer from the description of its solution process. While implicit layers allow features such as depth to adapt to new scenarios and inputs automatically, this adaptivity makes its computational expense challenging to predict. In this manuscript, we increase the "implicitness" of the DEQ by redefining the method in terms of an infinite time neural ODE, which paradoxically decreases the training cost over a standard neural ODE by 2-4x. Additionally, we address the question: is there a way to simultaneously achieve the robustness of implicit layers while allowing the reduced computational expense of an explicit layer? To solve this, we develop Skip and Skip Reg. DEQ, an implicit-explicit (IMEX) layer that simultaneously trains an explicit prediction followed by an implicit correction. We show that training this explicit predictor is free and even decreases the training time by 1.11-3.19x. Together, this manuscript shows how bridging the dichotomy of implicit and explicit deep learning can combine the advantages of both techniques.



### A Review on Deep-Learning Algorithms for Fetal Ultrasound-Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2201.12260v1
- **DOI**: 10.1016/j.media.2022.102629
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12260v1)
- **Published**: 2022-01-28 17:22:44+00:00
- **Updated**: 2022-01-28 17:22:44+00:00
- **Authors**: Maria Chiara Fiorentino, Francesca Pia Villani, Mariachiara Di Cosmo, Emanuele Frontoni, Sara Moccia
- **Comment**: None
- **Journal**: Medical Image Analysis 2022
- **Summary**: Deep-learning (DL) algorithms are becoming the standard for processing ultrasound (US) fetal images. Despite a large number of survey papers already present in this field, most of them are focusing on a broader area of medical-image analysis or not covering all fetal US DL applications. This paper surveys the most recent work in the field, with a total of 145 research papers published after 2017. Each paper is analyzed and commented on from both the methodology and application perspective. We categorized the papers in (i) fetal standard-plane detection, (ii) anatomical-structure analysis, and (iii) biometry parameter estimation. For each category, main limitations and open issues are presented. Summary tables are included to facilitate the comparison among the different approaches. Publicly-available datasets and performance metrics commonly used to assess algorithm performance are summarized, too. This paper ends with a critical summary of the current state of the art on DL algorithms for fetal US image analysis and a discussion on current challenges that have to be tackled by researchers working in the field to translate the research methodology into the actual clinical practice.



### 3D-FlowNet: Event-based optical flow estimation with 3D representation
- **Arxiv ID**: http://arxiv.org/abs/2201.12265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.12265v1)
- **Published**: 2022-01-28 17:28:15+00:00
- **Updated**: 2022-01-28 17:28:15+00:00
- **Authors**: Haixin Sun, Minh-Quan Dao, Vincent Fremont
- **Comment**: None
- **Journal**: None
- **Summary**: Event-based cameras can overpass frame-based cameras limitations for important tasks such as high-speed motion detection during self-driving cars navigation in low illumination conditions. The event cameras' high temporal resolution and high dynamic range, allow them to work in fast motion and extreme light scenarios. However, conventional computer vision methods, such as Deep Neural Networks, are not well adapted to work with event data as they are asynchronous and discrete. Moreover, the traditional 2D-encoding representation methods for event data, sacrifice the time resolution. In this paper, we first improve the 2D-encoding representation by expanding it into three dimensions to better preserve the temporal distribution of the events. We then propose 3D-FlowNet, a novel network architecture that can process the 3D input representation and output optical flow estimations according to the new encoding methods. A self-supervised training strategy is adopted to compensate the lack of labeled datasets for the event-based camera. Finally, the proposed network is trained and evaluated with the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. The results show that our 3D-FlowNet outperforms state-of-the-art approaches with less training epoch (30 compared to 100 of Spike-FlowNet).



### HSADML: Hyper-Sphere Angular Deep Metric based Learning for Brain Tumor Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.12269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12269v1)
- **Published**: 2022-01-28 17:37:15+00:00
- **Updated**: 2022-01-28 17:37:15+00:00
- **Authors**: Aman Verma, Vibhav Prakash Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Brain Tumors are abnormal mass of clustered cells penetrating regions of brain. Their timely identification and classification help doctors to provide appropriate treatment. However, Classifi-cation of Brain Tumors is quite intricate because of high-intra class similarity and low-inter class variability. Due to morphological similarity amongst various MRI-Slices of different classes the challenge deepens more. This all leads to hampering generalizability of classification models. To this end, this paper proposes HSADML, a novel framework which enables deep metric learning (DML) using SphereFace Loss. SphereFace loss embeds the features into a hyperspheric-manifold and then imposes margin on the embeddings to enhance differentiability between the classes. With utilization of SphereFace loss based deep metric learning it is ensured that samples from class clustered together while the different ones are pushed apart. Results reflects the promi-nence in the approach, the proposed framework achieved state-of-the-art 98.69% validation accu-racy using k-NN (k=1) and this is significantly higher than normal SoftMax Loss training which though obtains 98.47% validation accuracy but that too with limited inter-class separability and intra-class closeness. Experimental analysis done over various classifiers and loss function set-tings suggests potential in the approach.



### Benchmarking Conventional Vision Models on Neuromorphic Fall Detection and Action Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2201.12285v1
- **DOI**: 10.1109/CCWC54503.2022.9720737
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.12285v1)
- **Published**: 2022-01-28 17:54:33+00:00
- **Updated**: 2022-01-28 17:54:33+00:00
- **Authors**: Karthik Sivarama Krishnan, Koushik Sivarama Krishnan
- **Comment**: 6 Pages, 2 Figures
- **Journal**: None
- **Summary**: Neuromorphic vision-based sensors are gaining popularity in recent years with their ability to capture Spatio-temporal events with low power sensing. These sensors record events or spikes over traditional cameras which helps in preserving the privacy of the subject being recorded. These events are captured as per-pixel brightness changes and the output data stream is encoded with time, location, and pixel intensity change information. This paper proposes and benchmarks the performance of fine-tuned conventional vision models on neuromorphic human action recognition and fall detection datasets. The Spatio-temporal event streams from the Dynamic Vision Sensing cameras are encoded into a standard sequence image frames. These video frames are used for benchmarking conventional deep learning-based architectures. In this proposed approach, we fine-tuned the state-of-the-art vision models for this Dynamic Vision Sensing (DVS) application and named these models as DVS-R2+1D, DVS-CSN, DVS-C2D, DVS-SlowFast, DVS-X3D, and DVS-MViT. Upon comparing the performance of these models, we see the current state-of-the-art MViT based architecture DVS-MViT outperforms all the other models with an accuracy of 0.958 and an F-1 score of 0.958. The second best is the DVS-C2D with an accuracy of 0.916 and an F-1 score of 0.916. Third and Fourth are DVS-R2+1D and DVS-SlowFast with an accuracy of 0.875 and 0.833 and F-1 score of 0.875 and 0.861 respectively. DVS-CSN and DVS-X3D were the least performing models with an accuracy of 0.708 and 0.625 and an F1 score of 0.722 and 0.625 respectively.



### VRT: A Video Restoration Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.12288v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12288v2)
- **Published**: 2022-01-28 17:54:43+00:00
- **Updated**: 2022-06-15 17:17:05+00:00
- **Authors**: Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, Luc Van Gool
- **Comment**: add results on VFI and STVSR; SOTA results (+up to 2.16dB) on video
  SR, video deblurring, video denoising, video frame interpolation and
  space-time video super-resolution. Code: https://github.com/JingyunLiang/VRT
- **Journal**: None
- **Summary**: Video restoration (e.g., video super-resolution) aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which either is restricted by frame-by-frame restoration or lacks long-range modelling ability. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction and long-range temporal dependency modelling abilities. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal mutual self attention (TMSA) and parallel warping. TMSA divides the video into small clips, on which mutual attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on five tasks, including video super-resolution, video deblurring, video denoising, video frame interpolation and space-time video super-resolution, demonstrate that VRT outperforms the state-of-the-art methods by large margins ($\textbf{up to 2.16dB}$) on fourteen benchmark datasets.



### Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions
- **Arxiv ID**: http://arxiv.org/abs/2201.12296v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12296v1)
- **Published**: 2022-01-28 18:01:42+00:00
- **Updated**: 2022-01-28 18:01:42+00:00
- **Authors**: Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, Z. Morley Mao
- **Comment**: Codebase and dataset are included in
  https://github.com/jiachens/ModelNet40-C
- **Journal**: None
- **Summary**: Deep neural networks on 3D point cloud data have been widely used in the real world, especially in safety-critical applications. However, their robustness against corruptions is less studied. In this paper, we present ModelNet40-C, the first comprehensive benchmark on 3D point cloud corruption robustness, consisting of 15 common and realistic corruptions. Our evaluation shows a significant gap between the performances on ModelNet40 and ModelNet40-C for state-of-the-art (SOTA) models. To reduce the gap, we propose a simple but effective method by combining PointCutMix-R and TENT after evaluating a wide range of augmentation and test-time adaptation strategies. We identify a number of critical insights for future studies on corruption robustness in point cloud recognition. For instance, we unveil that Transformer-based architectures with proper training recipes achieve the strongest robustness. We hope our in-depth analysis will motivate the development of robust training strategies or architecture designs in the 3D point cloud domain. Our codebase and dataset are included in https://github.com/jiachens/ModelNet40-C



### REET: Robustness Evaluation and Enhancement Toolbox for Computational Pathology
- **Arxiv ID**: http://arxiv.org/abs/2201.12311v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12311v1)
- **Published**: 2022-01-28 18:23:55+00:00
- **Updated**: 2022-01-28 18:23:55+00:00
- **Authors**: Alex Foote, Amina Asif, Nasir Rajpoot, Fayyaz Minhas
- **Comment**: None
- **Journal**: None
- **Summary**: Motivation: Digitization of pathology laboratories through digital slide scanners and advances in deep learning approaches for objective histological assessment have resulted in rapid progress in the field of computational pathology (CPath) with wide-ranging applications in medical and pharmaceutical research as well as clinical workflows. However, the estimation of robustness of CPath models to variations in input images is an open problem with a significant impact on the down-stream practical applicability, deployment and acceptability of these approaches. Furthermore, development of domain-specific strategies for enhancement of robustness of such models is of prime importance as well.   Implementation and Availability: In this work, we propose the first domain-specific Robustness Evaluation and Enhancement Toolbox (REET) for computational pathology applications. It provides a suite of algorithmic strategies for enabling robustness assessment of predictive models with respect to specialized image transformations such as staining, compression, focusing, blurring, changes in spatial resolution, brightness variations, geometric changes as well as pixel-level adversarial perturbations. Furthermore, REET also enables efficient and robust training of deep learning pipelines in computational pathology. REET is implemented in Python and is available at the following URL: https://github.com/alexjfoote/reetoolbox.   Contact: Fayyaz.minhas@warwick.ac.uk



### DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR
- **Arxiv ID**: http://arxiv.org/abs/2201.12329v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12329v4)
- **Published**: 2022-01-28 18:51:09+00:00
- **Updated**: 2022-03-30 16:04:38+00:00
- **Authors**: Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \url{https://github.com/SlongLiu/DAB-DETR}.



### Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices
- **Arxiv ID**: http://arxiv.org/abs/2201.12382v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, I.2; I.5.4; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2201.12382v2)
- **Published**: 2022-01-28 19:24:30+00:00
- **Updated**: 2022-04-19 18:41:45+00:00
- **Authors**: Miko≈Çaj Ma≈Çki≈Ñski, Jacek Ma≈Ñdziuk
- **Comment**: None
- **Journal**: None
- **Summary**: Abstract visual reasoning (AVR) domain encompasses problems solving which requires the ability to reason about relations among entities present in a given scene. While humans, generally, solve AVR tasks in a "natural" way, even without prior experience, this type of problems has proven difficult for current machine learning systems. The paper summarises recent progress in applying deep learning methods to solving AVR problems, as a proxy for studying machine intelligence. We focus on the most common type of AVR tasks -- the Raven's Progressive Matrices (RPMs) -- and provide a comprehensive review of the learning methods and deep neural models applied to solve RPMs, as well as, the RPM benchmark sets. Performance analysis of the state-of-the-art approaches to solving RPMs leads to formulation of certain insights and remarks on the current and future trends in this area. We conclude the paper by demonstrating how real-world problems can benefit from the discoveries of RPM studies.



### Developing a Machine-Learning Algorithm to Diagnose Age-Related Macular Degeneration
- **Arxiv ID**: http://arxiv.org/abs/2201.12384v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12384v1)
- **Published**: 2022-01-28 19:25:36+00:00
- **Updated**: 2022-01-28 19:25:36+00:00
- **Authors**: Ananya Dua, Pham Hung Minh, Sajid Fahmid, Shikhar Gupta, Sophia Zheng, Vanessa Moyo, Yanran Elisa Xue
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Today, more than 12 million people over the age of 40 suffer from ocular diseases. Most commonly, older patients are susceptible to age related macular degeneration, an eye disease that causes blurring of the central vision due to the deterioration of the retina. The former can only be detected through complex and expensive imaging software, markedly a visual field test; this leaves a significant population with untreated eye disease and holds them at risk for complete vision loss. The use of machine learning algorithms has been proposed for treating eye disease. However, the development of these models is limited by a lack of understanding regarding appropriate model and training parameters to maximize model performance. In our study, we address these points by generating 6 models, each with a learning rate of 1 * 10^n where n is 0, -1, -2, ... -6, and calculated a f1 score for each of the models. Our analysis shows that sample imbalance is a key challenge in training of machine learning models and can result in deceptive improvements in training cost which does not translate to true improvements in model predictive performance. Considering the wide ranging impact of the disease and its adverse effects, we developed a machine learning algorithm to treat the same. We trained our model on varying eye disease datasets consisting of over 5000 patients, and the pictures of their infected eyes. In the future, we hope this model is used extensively, especially in areas that are under-resourced, to better diagnose eye disease and improve well being for humanity.



### A deep Q-learning method for optimizing visual search strategies in backgrounds of dynamic noise
- **Arxiv ID**: http://arxiv.org/abs/2201.12385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2201.12385v1)
- **Published**: 2022-01-28 19:26:45+00:00
- **Updated**: 2022-01-28 19:26:45+00:00
- **Authors**: Weimin Zhou, Miguel P. Eckstein
- **Comment**: SPIE Medical Imaging 2022
- **Journal**: None
- **Summary**: Humans process visual information with varying resolution (foveated visual system) and explore images by orienting through eye movements the high-resolution fovea to points of interest. The Bayesian ideal searcher (IS) that employs complete knowledge of task-relevant information optimizes eye movement strategy and achieves the optimal search performance. The IS can be employed as an important tool to evaluate the optimality of human eye movements, and potentially provide guidance to improve human observer visual search strategies. Najemnik and Geisler (2005) derived an IS for backgrounds of spatial 1/f noise. The corresponding template responses follow Gaussian distributions and the optimal search strategy can be analytically determined. However, the computation of the IS can be intractable when considering more realistic and complex backgrounds such as medical images. Modern reinforcement learning methods, successfully applied to obtain optimal policy for a variety of tasks, do not require complete knowledge of the background generating functions and can be potentially applied to anatomical backgrounds. An important first step is to validate the optimality of the reinforcement learning method. In this study, we investigate the ability of a reinforcement learning method that employs Q-network to approximate the IS. We demonstrate that the search strategy corresponding to the Q-network is consistent with the IS search strategy. The findings show the potential of the reinforcement learning with Q-network approach to estimate optimal eye movement planning with real anatomical backgrounds.



### Few-shot Unsupervised Domain Adaptation for Multi-modal Cardiac Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.12386v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12386v1)
- **Published**: 2022-01-28 19:28:48+00:00
- **Updated**: 2022-01-28 19:28:48+00:00
- **Authors**: Mingxuan Gu, Sulaiman Vesal, Ronak Kosti, Andreas Maier
- **Comment**: Accepted t0 BVM2022
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) methods intend to reduce the gap between source and target domains by using unlabeled target domain and labeled source domain data, however, in the medical domain, target domain data may not always be easily available, and acquiring new samples is generally time-consuming. This restricts the development of UDA methods for new domains. In this paper, we explore the potential of UDA in a more challenging while realistic scenario where only one unlabeled target patient sample is available. We call it Few-shot Unsupervised Domain adaptation (FUDA). We first generate target-style images from source images and explore diverse target styles from a single target patient with Random Adaptive Instance Normalization (RAIN). Then, a segmentation network is trained in a supervised manner with the generated target images. Our experiments demonstrate that FUDA improves the segmentation performance by 0.33 of Dice score on the target domain compared with the baseline, and it also gives 0.28 of Dice score improvement in a more rigorous one-shot setting. Our code is available at \url{https://github.com/MingxuanGu/Few-shot-UDA}.



### DoubleU-Net++: Architecture with Exploit Multiscale Features for Vertebrae Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.12389v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12389v1)
- **Published**: 2022-01-28 19:42:31+00:00
- **Updated**: 2022-01-28 19:42:31+00:00
- **Authors**: Simindokht Jahangard, Mahdi Bonyani, Abbas Khosravi
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of the vertebra is an important prerequisite in various medical applications (E.g. tele surgery) to assist surgeons. Following the successful development of deep neural networks, recent studies have focused on the essential rule of vertebral segmentation. Prior works contain a large number of parameters, and their segmentation is restricted to only one view. Inspired by DoubleU-Net, we propose a novel model named DoubleU-Net++ in which DensNet as feature extractor, special attention module from Convolutional Block Attention on Module (CBAM) and, Pyramid Squeeze Attention (PSA) module are employed to improve extracted features. We evaluate our proposed model on three different views (sagittal, coronal, and axial) of VerSe2020 and xVertSeg datasets. Compared with state-of-the-art studies, our architecture is trained faster and achieves higher precision, recall, and F1-score as evaluation (imporoved by 4-6%) and the result of above 94% for sagittal view and above 94% for both coronal view and above 93% axial view were gained for VerSe2020 dataset, respectively. Also, for xVertSeg dataset, we achieved precision, recall,and F1-score of above 97% for sagittal view, above 93% for coronal view ,and above 96% for axial view.



### Syfer: Neural Obfuscation for Private Data Release
- **Arxiv ID**: http://arxiv.org/abs/2201.12406v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12406v1)
- **Published**: 2022-01-28 20:32:04+00:00
- **Updated**: 2022-01-28 20:32:04+00:00
- **Authors**: Adam Yala, Victor Quach, Homa Esfahanizadeh, Rafael G. L. D'Oliveira, Ken R. Duffy, Muriel M√©dard, Tommi S. Jaakkola, Regina Barzilay
- **Comment**: None
- **Journal**: None
- **Summary**: Balancing privacy and predictive utility remains a central challenge for machine learning in healthcare. In this paper, we develop Syfer, a neural obfuscation method to protect against re-identification attacks. Syfer composes trained layers with random neural networks to encode the original data (e.g. X-rays) while maintaining the ability to predict diagnoses from the encoded data. The randomness in the encoder acts as the private key for the data owner. We quantify privacy as the number of attacker guesses required to re-identify a single image (guesswork). We propose a contrastive learning algorithm to estimate guesswork. We show empirically that differentially private methods, such as DP-Image, obtain privacy at a significant loss of utility. In contrast, Syfer achieves strong privacy while preserving utility. For example, X-ray classifiers built with DP-image, Syfer, and original data achieve average AUCs of 0.53, 0.78, and 0.86, respectively.



### CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture
- **Arxiv ID**: http://arxiv.org/abs/2201.12425v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12425v1)
- **Published**: 2022-01-28 21:30:42+00:00
- **Updated**: 2022-01-28 21:30:42+00:00
- **Authors**: Ruofan Liang, Hongyi Sun, Nandita Vijaykumar
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit neural representations with multi-layer perceptrons (MLPs) have recently gained prominence for a wide variety of tasks such as novel view synthesis and 3D object representation and rendering. However, a significant challenge with these representations is that both training and inference with an MLP over a large number of input coordinates to learn and represent an image, video, or 3D object, require large amounts of computation and incur long processing times. In this work, we aim to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX. With CoordX, the initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP. This approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal. Our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads. We demonstrate a speedup of up to 2.92x compared to the baseline model for image, video, and 3D shape representation and rendering tasks.



### Mobile Robot Manipulation using Pure Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.12437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.12437v2)
- **Published**: 2022-01-28 21:52:05+00:00
- **Updated**: 2022-10-17 13:46:23+00:00
- **Authors**: Brent Griffin
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: This paper addresses the problem of mobile robot manipulation using object detection. Our approach uses detection and control as complimentary functions that learn from real-world interactions. We develop an end-to-end manipulation method based solely on detection and introduce Task-focused Few-shot Object Detection (TFOD) to learn new objects and settings. Our robot collects its own training data and automatically determines when to retrain detection to improve performance across various subtasks (e.g., grasping). Notably, detection training is low-cost, and our robot learns to manipulate new objects using as few as four clicks of annotation. In physical experiments, our robot learns visual control from a single click of annotation and a novel update formulation, manipulates new objects in clutter and other mobile settings, and achieves state-of-the-art results on an existing visual servo control and depth estimation benchmark. Finally, we develop a TFOD Benchmark to support future object detection research for robotics: https://github.com/griffbr/tfod.




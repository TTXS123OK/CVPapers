# Arxiv Papers in cs.CV on 2022-08-28
### Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.13113v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13113v1)
- **Published**: 2022-08-28 01:43:21+00:00
- **Updated**: 2022-08-28 01:43:21+00:00
- **Authors**: Youbao Tang, Ning Zhang, Yirui Wang, Shenghua He, Mei Han, Jing Xiao, Ruei-Sung Lin
- **Comment**: One of a series of works about lesion RECIST diameter prediction and
  weakly-supervised lesion segmentation (MICCAI 2022)
- **Journal**: None
- **Summary**: Automatically measuring lesion/tumor size with RECIST (Response Evaluation Criteria In Solid Tumors) diameters and segmentation is important for computer-aided diagnosis. Although it has been studied in recent years, there is still space to improve its accuracy and robustness, such as (1) enhancing features by incorporating rich contextual information while keeping a high spatial resolution and (2) involving new tasks and losses for joint optimization. To reach this goal, this paper proposes a transformer-based network (MeaFormer, Measurement transFormer) for lesion RECIST diameter prediction and segmentation (LRDPS). It is formulated as three correlative and complementary tasks: lesion segmentation, heatmap prediction, and keypoint regression. To the best of our knowledge, it is the first time to use keypoint regression for RECIST diameter prediction. MeaFormer can enhance high-resolution features by employing transformers to capture their long-range dependencies. Two consistency losses are introduced to explicitly build relationships among these tasks for better optimization. Experiments show that MeaFormer achieves the state-of-the-art performance of LRDPS on the large-scale DeepLesion dataset and produces promising results of two downstream clinic-relevant tasks, i.e., 3D lesion segmentation and RECIST assessment in longitudinal studies.



### Delving into the Continuous Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.13121v1
- **DOI**: 10.1145/3503161.3548222
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13121v1)
- **Published**: 2022-08-28 02:32:25+00:00
- **Updated**: 2022-08-28 02:32:25+00:00
- **Authors**: Yinsong Xu, Zhuqing Jiang, Aidong Men, Yang Liu, Qingchao Chen
- **Comment**: Accepted by ACM MM22
- **Journal**: None
- **Summary**: Existing domain adaptation methods assume that domain discrepancies are caused by a few discrete attributes and variations, e.g., art, real, painting, quickdraw, etc. We argue that this is not realistic as it is implausible to define the real-world datasets using a few discrete attributes. Therefore, we propose to investigate a new problem namely the Continuous Domain Adaptation (CDA) through the lens where infinite domains are formed by continuously varying attributes. Leveraging knowledge of two labeled source domains and several observed unlabeled target domains data, the objective of CDA is to learn a generalized model for whole data distribution with the continuous attribute. Besides the contributions of formulating a new problem, we also propose a novel approach as a strong CDA baseline. To be specific, firstly we propose a novel alternating training strategy to reduce discrepancies among multiple domains meanwhile generalize to unseen target domains. Secondly, we propose a continuity constraint when estimating the cross-domain divergence measurement. Finally, to decouple the discrepancy from the mini-batch size, we design a domain-specific queue to maintain the global view of the source domain that further boosts the adaptation performances. Our method is proven to achieve the state-of-the-art in CDA problem using extensive experiments. The code is available at https://github.com/SPIresearch/CDA.



### Removing Rain Streaks via Task Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.13133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13133v1)
- **Published**: 2022-08-28 03:32:17+00:00
- **Updated**: 2022-08-28 03:32:17+00:00
- **Authors**: Yinglong Wang, Chao Ma, Jianzhuang Liu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Due to the difficulty in collecting paired real-world training data, image deraining is currently dominated by supervised learning with synthesized data generated by e.g., Photoshop rendering. However, the generalization to real rainy scenes is usually limited due to the gap between synthetic and real-world data. In this paper, we first statistically explore why the supervised deraining models cannot generalize well to real rainy cases, and find the substantial difference of synthetic and real rainy data. Inspired by our studies, we propose to remove rain by learning favorable deraining representations from other connected tasks. In connected tasks, the label for real data can be easily obtained. Hence, our core idea is to learn representations from real data through task transfer to improve deraining generalization. We thus term our learning strategy as \textit{task transfer learning}. If there are more than one connected tasks, we propose to reduce model size by knowledge distillation. The pretrained models for the connected tasks are treated as teachers, all their knowledge is distilled to a student network, so that we reduce the model size, meanwhile preserve effective prior representations from all the connected tasks. At last, the student network is fine-tuned with minority of paired synthetic rainy data to guide the pretrained prior representations to remove rain. Extensive experiments demonstrate that proposed task transfer learning strategy is surprisingly successful and compares favorably with state-of-the-art supervised learning methods and apparently surpass other semi-supervised deraining methods on synthetic data. Particularly, it shows superior generalization over them to real-world scenes.



### An Access Control Method with Secret Key for Semantic Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/2208.13135v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13135v1)
- **Published**: 2022-08-28 04:09:36+00:00
- **Updated**: 2022-08-28 04:09:36+00:00
- **Authors**: Teru Nagamori, Ryota Iijima, Hitoshi Kiya
- **Comment**: To appear in the International Conference on Machine Learning and
  Cybernetics 2022 (ICMLC 2022)
- **Journal**: None
- **Summary**: A novel method for access control with a secret key is proposed to protect models from unauthorized access in this paper. We focus on semantic segmentation models with the vision transformer (ViT), called segmentation transformer (SETR). Most existing access control methods focus on image classification tasks, or they are limited to CNNs. By using a patch embedding structure that ViT has, trained models and test images can be efficiently encrypted with a secret key, and then semantic segmentation tasks are carried out in the encrypted domain. In an experiment, the method is confirmed to provide the same accuracy as that of using plain images without any encryption to authorized users with a correct key and also to provide an extremely degraded accuracy to unauthorized users.



### Efficient Motion Modelling with Variable-sized blocks from Hierarchical Cuboidal Partitioning
- **Arxiv ID**: http://arxiv.org/abs/2208.13137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.13137v1)
- **Published**: 2022-08-28 04:13:58+00:00
- **Updated**: 2022-08-28 04:13:58+00:00
- **Authors**: Priyabrata Karmakar, Manzur Murshed, Manoranjan Paul, David Taubman
- **Comment**: None
- **Journal**: None
- **Summary**: Motion modelling with block-based architecture has been widely used in video coding where a frame is divided into fixed-sized blocks that are motion compensated independently. This often leads to coding inefficiency as fixed-sized blocks hardly align with the object boundaries. Although hierarchical block-partitioning has been introduced to address this, the increased number of motion vectors limits the benefit. Recently, approximate segmentation of images with cuboidal partitioning has gained popularity. Not only are the variable-sized rectangular segments (cuboids) readily amenable to block-based image/video coding techniques, but they are also capable of aligning well with the object boundaries. This is because cuboidal partitioning is based on a homogeneity constraint, minimising the sum of squared errors (SSE). In this paper, we have investigated the potential of cuboids in motion modelling against the fixed-sized blocks used in scalable video coding. Specifically, we have constructed motion-compensated current frame using the cuboidal partitioning information of the anchor frame in a group-of-picture (GOP). The predicted current frame has then been used as the base layer while encoding the current frame as an enhancement layer using the scalable HEVC encoder. Experimental results confirm 6.71%-10.90% bitrate savings on 4K video sequences.



### ClusTR: Exploring Efficient Self-attention via Clustering for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.13138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13138v1)
- **Published**: 2022-08-28 04:18:27+00:00
- **Updated**: 2022-08-28 04:18:27+00:00
- **Authors**: Yutong Xie, Jianpeng Zhang, Yong Xia, Anton van den Hengel, Qi Wu
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Although Transformers have successfully transitioned from their language modelling origins to image-based applications, their quadratic computational complexity remains a challenge, particularly for dense prediction. In this paper we propose a content-based sparse attention method, as an alternative to dense self-attention, aiming to reduce the computation complexity while retaining the ability to model long-range dependencies. Specifically, we cluster and then aggregate key and value tokens, as a content-based method of reducing the total token count. The resulting clustered-token sequence retains the semantic diversity of the original signal, but can be processed at a lower computational cost. Besides, we further extend the clustering-guided attention from single-scale to multi-scale, which is conducive to dense prediction tasks. We label the proposed Transformer architecture ClusTR, and demonstrate that it achieves state-of-the-art performance on various vision tasks but at lower computational cost and with fewer parameters. For instance, our ClusTR small model with 22.7M parameters achieves 83.2\% Top-1 accuracy on ImageNet. Source code and ImageNet models will be made publicly available.



### Automatic Infectious Disease Classification Analysis with Concept Discovery
- **Arxiv ID**: http://arxiv.org/abs/2209.02415v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.02415v2)
- **Published**: 2022-08-28 05:33:44+00:00
- **Updated**: 2022-11-14 20:54:19+00:00
- **Authors**: Elena Sizikova, Joshua Vendrow, Xu Cao, Rachel Grotheer, Jamie Haddock, Lara Kassab, Alona Kryshchenko, Thomas Merkh, R. W. M. A. Madushani, Kenny Moise, Annie Ulichney, Huy V. Vo, Chuntian Wang, Megan Coffee, Kathryn Leonard, Deanna Needell
- **Comment**: Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2022, November 28th, 2022, New Orleans, United States & Virtual,
  http://www.ml4h.cc, 13 pages
- **Journal**: None
- **Summary**: Automatic infectious disease classification from images can facilitate needed medical diagnoses. Such an approach can identify diseases, like tuberculosis, which remain under-diagnosed due to resource constraints and also novel and emerging diseases, like monkeypox, which clinicians have little experience or acumen in diagnosing. Avoiding missed or delayed diagnoses would prevent further transmission and improve clinical outcomes. In order to understand and trust neural network predictions, analysis of learned representations is necessary. In this work, we argue that automatic discovery of concepts, i.e., human interpretable attributes, allows for a deep understanding of learned information in medical image analysis tasks, generalizing beyond the training labels or protocols. We provide an overview of existing concept discovery approaches in medical image and computer vision communities, and evaluate representative methods on tuberculosis (TB) prediction and monkeypox prediction tasks. Finally, we propose NMFx, a general NMF formulation of interpretability by concept discovery that works in a unified way in unsupervised, weakly supervised, and supervised scenarios.



### Generative Modelling of the Ageing Heart with Cross-Sectional Imaging and Clinical Data
- **Arxiv ID**: http://arxiv.org/abs/2208.13146v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13146v2)
- **Published**: 2022-08-28 06:14:39+00:00
- **Updated**: 2022-10-10 11:00:32+00:00
- **Authors**: Mengyun Qiao, Berke Doga Basaran, Huaqi Qiu, Shuo Wang, Yi Guo, Yuanyuan Wang, Paul M. Matthews, Daniel Rueckert, Wenjia Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Cardiovascular disease, the leading cause of death globally, is an age-related disease. Understanding the morphological and functional changes of the heart during ageing is a key scientific question, the answer to which will help us define important risk factors of cardiovascular disease and monitor disease progression. In this work, we propose a novel conditional generative model to describe the changes of 3D anatomy of the heart during ageing. The proposed model is flexible and allows integration of multiple clinical factors (e.g. age, gender) into the generating process. We train the model on a large-scale cross-sectional dataset of cardiac anatomies and evaluate on both cross-sectional and longitudinal datasets. The model demonstrates excellent performance in predicting the longitudinal evolution of the ageing heart and modelling its data distribution. The codes are available at https://github.com/MengyunQ/AgeHeart.



### Face Anti-Spoofing from the Perspective of Data Sampling
- **Arxiv ID**: http://arxiv.org/abs/2208.13164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13164v1)
- **Published**: 2022-08-28 07:54:30+00:00
- **Updated**: 2022-08-28 07:54:30+00:00
- **Authors**: Usman Muhammad, Mourad Oussalah
- **Comment**: None
- **Journal**: None
- **Summary**: Without deploying face anti-spoofing countermeasures, face recognition systems can be spoofed by presenting a printed photo, a video, or a silicon mask of a genuine user. Thus, face presentation attack detection (PAD) plays a vital role in providing secure facial access to digital devices. Most existing video-based PAD countermeasures lack the ability to cope with long-range temporal variations in videos. Moreover, the key-frame sampling prior to the feature extraction step has not been widely studied in the face anti-spoofing domain. To mitigate these issues, this paper provides a data sampling approach by proposing a video processing scheme that models the long-range temporal variations based on Gaussian Weighting Function. Specifically, the proposed scheme encodes the consecutive t frames of video sequences into a single RGB image based on a Gaussian-weighted summation of the t frames. Using simply the data sampling scheme alone, we demonstrate that state-of-the-art performance can be achieved without any bells and whistles in both intra-database and inter-database testing scenarios for the three public benchmark datasets; namely, Replay-Attack, MSU-MFSD, and CASIA-FASD. In particular, the proposed scheme provides a much lower error (from 15.2% to 6.7% on CASIA-FASD and 5.9% to 4.9% on Replay-Attack) compared to baselines in cross-database scenarios.



### Towards Real-World Video Deblurring by Exploring Blur Formation Process
- **Arxiv ID**: http://arxiv.org/abs/2208.13184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13184v1)
- **Published**: 2022-08-28 09:24:52+00:00
- **Updated**: 2022-08-28 09:24:52+00:00
- **Authors**: Mingdeng Cao, Zhihang Zhong, Yanbo Fan, Jiahao Wang, Yong Zhang, Jue Wang, Yujiu Yang, Yinqiang Zheng
- **Comment**: ECCV AIM Workshop 2022
- **Journal**: None
- **Summary**: This paper aims at exploring how to synthesize close-to-real blurs that existing video deblurring models trained on them can generalize well to real-world blurry videos. In recent years, deep learning-based approaches have achieved promising success on video deblurring task. However, the models trained on existing synthetic datasets still suffer from generalization problems over real-world blurry scenarios with undesired artifacts. The factors accounting for the failure remain unknown. Therefore, we revisit the classical blur synthesis pipeline and figure out the possible reasons, including shooting parameters, blur formation space, and image signal processor~(ISP). To analyze the effects of these potential factors, we first collect an ultra-high frame-rate (940 FPS) RAW video dataset as the data basis to synthesize various kinds of blurs. Then we propose a novel realistic blur synthesis pipeline termed as RAW-Blur by leveraging blur formation cues. Through numerous experiments, we demonstrate that synthesizing blurs in the RAW space and adopting the same ISP as the real-world testing data can effectively eliminate the negative effects of synthetic data. Furthermore, the shooting parameters of the synthesized blurry video, e.g., exposure time and frame-rate play significant roles in improving the performance of deblurring models. Impressively, the models trained on the blurry data synthesized by the proposed RAW-Blur pipeline can obtain more than 5dB PSNR gain against those trained on the existing synthetic blur datasets. We believe the novel realistic synthesis pipeline and the corresponding RAW video dataset can help the community to easily construct customized blur datasets to improve real-world video deblurring performance largely, instead of laboriously collecting real data pairs.



### Grounded Affordance from Exocentric View
- **Arxiv ID**: http://arxiv.org/abs/2208.13196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13196v2)
- **Published**: 2022-08-28 10:32:47+00:00
- **Updated**: 2023-05-25 05:47:29+00:00
- **Authors**: Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao
- **Comment**: arXiv admin note: text overlap with arXiv:2203.09905
- **Journal**: None
- **Summary**: Affordance grounding aims to locate objects' "action possibilities" regions, which is an essential step toward embodied intelligence. Due to the diversity of interactive affordance, the uniqueness of different individuals leads to diverse interactions, which makes it difficult to establish an explicit link between object parts and affordance labels. Human has the ability that transforms the various exocentric interactions into invariant egocentric affordance to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. However, there is some "interaction bias" between personas, mainly regarding different regions and different views. To this end, we devise a cross-view affordance knowledge transfer framework that extracts affordance-specific features from exocentric interactions and transfers them to the egocentric view. Specifically, the perception of affordance regions is enhanced by preserving affordance co-relations. In addition, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from $36$ affordance categories. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality. Code is released at https://github.com/lhc1224/Cross-view-affordance-grounding.



### Leachable Component Clustering
- **Arxiv ID**: http://arxiv.org/abs/2208.13217v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.13217v1)
- **Published**: 2022-08-28 13:13:17+00:00
- **Updated**: 2022-08-28 13:13:17+00:00
- **Authors**: Miao Cheng, Xinge You
- **Comment**: 7 pages, 24 figures
- **Journal**: None
- **Summary**: Clustering attempts to partition data instances into several distinctive groups, while the similarities among data belonging to the common partition can be principally reserved. Furthermore, incomplete data frequently occurs in many realworld applications, and brings perverse influence on pattern analysis. As a consequence, the specific solutions to data imputation and handling are developed to conduct the missing values of data, and independent stage of knowledge exploitation is absorbed for information understanding. In this work, a novel approach to clustering of incomplete data, termed leachable component clustering, is proposed. Rather than existing methods, the proposed method handles data imputation with Bayes alignment, and collects the lost patterns in theory. Due to the simple numeric computation of equations, the proposed method can learn optimized partitions while the calculation efficiency is held. Experiments on several artificial incomplete data sets demonstrate that, the proposed method is able to present superior performance compared with other state-of-the-art algorithms.



### Visualizing high-dimensional loss landscapes with Hessian directions
- **Arxiv ID**: http://arxiv.org/abs/2208.13219v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.13219v1)
- **Published**: 2022-08-28 13:18:47+00:00
- **Updated**: 2022-08-28 13:18:47+00:00
- **Authors**: Lucas BÃ¶ttcher, Gregory Wheeler
- **Comment**: 21 pages, 7 figures
- **Journal**: None
- **Summary**: Analyzing geometric properties of high-dimensional loss functions, such as local curvature and the existence of other optima around a certain point in loss space, can help provide a better understanding of the interplay between neural network structure, implementation attributes, and learning performance. In this work, we combine concepts from high-dimensional probability and differential geometry to study how curvature properties in lower-dimensional loss representations depend on those in the original loss space. We show that saddle points in the original space are rarely correctly identified as such in lower-dimensional representations if random projections are used. In such projections, the expected curvature in a lower-dimensional representation is proportional to the mean curvature in the original loss space. Hence, the mean curvature in the original loss space determines if saddle points appear, on average, as either minima, maxima, or almost flat regions. We use the connection between expected curvature and mean curvature (i.e., the normalized Hessian trace) to estimate the trace of Hessians without calculating the Hessian or Hessian-vector products as in Hutchinson's method. Because random projections are not able to correctly identify saddle information, we propose to study projections along Hessian directions that are associated with the largest and smallest principal curvatures. We connect our findings to the ongoing debate on loss landscape flatness and generalizability. Finally, we illustrate our method in numerical experiments on different image classifiers with up to about $7\times 10^6$ parameters.



### Deep learning for automatic head and neck lymph node level delineation provides expert-level accuracy
- **Arxiv ID**: http://arxiv.org/abs/2208.13224v2
- **DOI**: 10.3389/fonc.2023.1115258
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13224v2)
- **Published**: 2022-08-28 13:58:54+00:00
- **Updated**: 2023-03-01 19:54:06+00:00
- **Authors**: Thomas Weissmann, Yixing Huang, Stefan Fischer, Johannes Roesch, Sina Mansoorian, Horacio Ayala Gaona, Antoniu-Oreste Gostian, Markus Hecht, Sebastian Lettmaier, Lisa Deloch, Benjamin Frey, Udo S. Gaipl, Luitpold V. Distel, Andreas Maier, Heinrich Iro, Sabine Semrau, Christoph Bert, Rainer Fietkau, Florian Putz
- **Comment**: 14 pages, 6 figures, published in Frontiers in Oncology
- **Journal**: Front. Oncol. 13:1115258
- **Summary**: Background: Deep learning (DL)-based head and neck lymph node level (HN_LNL) autodelineation is of high relevance to radiotherapy research and clinical treatment planning but still underinvestigated in academic literature. Methods: An expert-delineated cohort of 35 planning CTs was used for training of an nnU-net 3D-fullres/2D-ensemble model for autosegmentation of 20 different HN_LNL. A second cohort acquired at the same institution later in time served as the test set (n=20). In a completely blinded evaluation, 3 clinical experts rated the quality of DL autosegmentations in a head-to-head comparison with expert-created contours. For a subgroup of 10 cases, intraobserver variability was compared to the average DL autosegmentation accuracy on the original and recontoured set of expert segmentations. A postprocessing step to adjust craniocaudal boundaries of level autosegmentations to the CT slice plane was introduced and the effect on geometric accuracy and expert rating was investigated. Results: Blinded expert ratings for DL segmentations and expert-created contours were not significantly different. DL segmentations with slice plane adjustment were rated numerically higher (mean, 81.0 vs. 79.6,p=0.185) and DL segmentations without slice plane adjustment were rated numerically lower (77.2 vs. 79.6,p=0.167) than manually drawn contours. DL segmentations with CT slice plane adjustment were rated significantly better than DL contours without slice plane adjustment (81.0 vs. 77.2,p=0.004). Geometric accuracy of DL segmentations was not different from intraobserver variability (mean, 0.76 vs. 0.77, p=0.307). Conclusions: We show that a nnU-net 3D-fullres/2D-ensemble model can be used for highly accurate autodelineation of HN_LNL using only a limited training dataset that is ideally suited for large-scale standardized autodelineation of HN_LNL in the research setting.



### Towards Accurate Reconstruction of 3D Scene Shape from A Single Monocular Image
- **Arxiv ID**: http://arxiv.org/abs/2208.13241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.13241v2)
- **Published**: 2022-08-28 16:20:14+00:00
- **Updated**: 2022-09-04 14:22:51+00:00
- **Authors**: Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Simon Chen, Yifan Liu, Chunhua Shen
- **Comment**: 20 pages. Journal version of the conference paper "Learning to
  Recover 3D Scene Shape from a Single Image". arXiv admin note: substantial
  text overlap with arXiv:2012.09365
- **Journal**: None
- **Summary**: Despite significant progress made in the past few years, challenges remain for depth estimation using a single monocular image. First, it is nontrivial to train a metric-depth prediction model that can generalize well to diverse scenes mainly due to limited training data. Thus, researchers have built large-scale relative depth datasets that are much easier to collect. However, existing relative depth estimation models often fail to recover accurate 3D scene shapes due to the unknown depth shift caused by training with the relative depth data. We tackle this problem here and attempt to estimate accurate scene shapes by training on large-scale relative depth data, and estimating the depth shift. To do so, we propose a two-stage framework that first predicts depth up to an unknown scale and shift from a single monocular image, and then exploits 3D point cloud data to predict the depth shift and the camera's focal length that allow us to recover 3D scene shapes. As the two modules are trained separately, we do not need strictly paired training data. In addition, we propose an image-level normalized regression loss and a normal-based geometry loss to improve training with relative depth annotation. We test our depth model on nine unseen datasets and achieve state-of-the-art performance on zero-shot evaluation. Code is available at: https://git.io/Depth



### FFCNN: Fast FPGA based Acceleration for Convolution neural network inference
- **Arxiv ID**: http://arxiv.org/abs/2208.13250v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.13250v1)
- **Published**: 2022-08-28 16:55:25+00:00
- **Updated**: 2022-08-28 16:55:25+00:00
- **Authors**: F. Keddous, H-N. Nguyen, A. Nakib
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new efficient OpenCL-based Accelerator for large scale Convolutional Neural Networks called Fast Inference on FPGAs for Convolution Neural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels architecture. As pointed out before, high-level synthesis tools such as the OpenCL framework can easily port codes originally designed for CPUs and GPUs to FPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs. This work aims to propose an efficient FPGA implementation of OpenCL High-Performance Computing Applications. To do so, a Data reuse and task mapping techniques are also presented to improve design efficiency. In addition, the following motivations were taken into account when developing FFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK based FPGA design flow. 2) In FFFCN, different techniques have been integrated to improve the memory band with and throughput. A performance analysis is conducted on two deep CNN for Large-Scale Images classification. The obtained results, and the comparison with other works designed to accelerate the same types of architectures, show the efficiency and the competitiveness of the proposed accelerator design by significantly improved performance and resource utilization.



### Detection and Classification of Brain tumors Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.13264v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2208.13264v1)
- **Published**: 2022-08-28 18:24:22+00:00
- **Updated**: 2022-08-28 18:24:22+00:00
- **Authors**: Gopinath Balaji, Ranit Sen, Harsh Kirty
- **Comment**: 11 pages, 14 figures, Presented this paper and published only
  abstract in the 'International conference on Machine learning Big data
  management Cloud and Computing (ICMBDC)' on 1st January 2022, Proceeding
  Detail available in https://digitalxplore.org/proceeding.php?pid=1210
- **Journal**: None
- **Summary**: Abnormal development of tissues in the body as a result of swelling and morbid enlargement is known as a tumor. They are mainly classified as Benign and Malignant. Tumour in the brain is fatal as it may be cancerous, so it can feed on healthy cells nearby and keep increasing in size. This may affect the soft tissues, nerve cells, and small blood vessels in the brain. Hence there is a need to detect and classify them during the early stages with utmost precision. There are different sizes and locations of brain tumors which makes it difficult to understand their nature. The process of detection and classification of brain tumors can prove to be an onerous task even with advanced MRI (Magnetic Resonance Imaging) techniques due to the similarities between the healthy cells nearby and the tumor. In this paper, we have used Keras and Tensorflow to implement state-of-the-art Convolutional Neural Network (CNN) architectures, like EfficientNetB0, ResNet50, Xception, MobileNetV2, and VGG16, using Transfer Learning to detect and classify three types of brain tumors namely - Glioma, Meningioma, and Pituitary. The dataset we used consisted of 3264 2-D magnetic resonance images and 4 classes. Due to the small size of the dataset, various data augmentation techniques were used to increase the size of the dataset. Our proposed methodology not only consists of data augmentation, but also various image denoising techniques, skull stripping, cropping, and bias correction. In our proposed work EfficientNetB0 architecture performed the best giving an accuracy of 97.61%. The aim of this paper is to differentiate between normal and abnormal pixels and also classify them with better accuracy.



### JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/2208.13266v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.13266v3)
- **Published**: 2022-08-28 18:30:46+00:00
- **Updated**: 2022-09-07 14:43:22+00:00
- **Authors**: Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Di, Xuehai He, Xin Eric Wang
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.



### Efficient liver segmentation with 3D CNN using computed tomography scans
- **Arxiv ID**: http://arxiv.org/abs/2208.13271v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13271v1)
- **Published**: 2022-08-28 19:02:39+00:00
- **Updated**: 2022-08-28 19:02:39+00:00
- **Authors**: Khaled Humady, Yasmeen Al-Saeed, Nabila Eladawi, Ahmed Elgarayhi, Mohammed Elmogy, Mohammed Sallah
- **Comment**: 21 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: The liver is one of the most critical metabolic organs in vertebrates due to its vital functions in the human body, such as detoxification of the blood from waste products and medications. Liver diseases due to liver tumors are one of the most common mortality reasons around the globe. Hence, detecting liver tumors in the early stages of tumor development is highly required as a critical part of medical treatment. Many imaging modalities can be used as aiding tools to detect liver tumors. Computed tomography (CT) is the most used imaging modality for soft tissue organs such as the liver. This is because it is an invasive modality that can be captured relatively quickly. This paper proposed an efficient automatic liver segmentation framework to detect and segment the liver out of CT abdomen scans using the 3D CNN DeepMedic network model. Segmenting the liver region accurately and then using the segmented liver region as input to tumors segmentation method is adopted by many studies as it reduces the false rates resulted from segmenting abdomen organs as tumors. The proposed 3D CNN DeepMedic model has two pathways of input rather than one pathway, as in the original 3D CNN model. In this paper, the network was supplied with multiple abdomen CT versions, which helped improve the segmentation quality. The proposed model achieved 94.36%, 94.57%, 91.86%, and 93.14% for accuracy, sensitivity, specificity, and Dice similarity score, respectively. The experimental results indicate the applicability of the proposed method.



### Unsupervised diffeomorphic cardiac image registration using parameterization of the deformation field
- **Arxiv ID**: http://arxiv.org/abs/2208.13275v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.13275v1)
- **Published**: 2022-08-28 19:34:10+00:00
- **Updated**: 2022-08-28 19:34:10+00:00
- **Authors**: Ameneh Sheikhjafari, Deepa Krishnaswamy, Michelle Noga, Nilanjan Ray, Kumaradevan Punithakumar
- **Comment**: 12 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: This study proposes an end-to-end unsupervised diffeomorphic deformable registration framework based on moving mesh parameterization. Using this parameterization, a deformation field can be modeled with its transformation Jacobian determinant and curl of end velocity field. The new model of the deformation field has three important advantages; firstly, it relaxes the need for an explicit regularization term and the corresponding weight in the cost function. The smoothness is implicitly embedded in the solution which results in a physically plausible deformation field. Secondly, it guarantees diffeomorphism through explicit constraints applied to the transformation Jacobian determinant to keep it positive. Finally, it is suitable for cardiac data processing, since the nature of this parameterization is to define the deformation field in terms of the radial and rotational components. The effectiveness of the algorithm is investigated by evaluating the proposed method on three different data sets including 2D and 3D cardiac MRI scans. The results demonstrate that the proposed framework outperforms existing learning-based and non-learning-based methods while generating diffeomorphic transformations.




# Arxiv Papers in cs.CV on 2022-08-03
### Style Transfer of Black and White Silhouette Images using CycleGAN and a Randomly Generated Dataset
- **Arxiv ID**: http://arxiv.org/abs/2208.04140v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.04140v1)
- **Published**: 2022-08-03 01:02:12+00:00
- **Updated**: 2022-08-03 01:02:12+00:00
- **Authors**: Worasait Suwannik
- **Comment**: None
- **Journal**: None
- **Summary**: CycleGAN can be used to transfer an artistic style to an image. It does not require pairs of source and stylized images to train a model. Taking this advantage, we propose using randomly generated data to train a machine learning model that can transfer traditional art style to a black and white silhouette image. The result is noticeably better than the previous neural style transfer methods. However, there are some areas for improvement, such as removing artifacts and spikes from the transformed image.



### Compressive Self-localization Using Relative Attribute Embedding
- **Arxiv ID**: http://arxiv.org/abs/2208.08863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.08863v1)
- **Published**: 2022-08-03 01:42:49+00:00
- **Updated**: 2022-08-03 01:42:49+00:00
- **Authors**: Ryogo Yamamoto, Kanji Tanaka
- **Comment**: 3 pages, 4 figures, An extended abstract version of a manuscript
  submitted to an international conference
- **Journal**: None
- **Summary**: The use of relative attribute (e.g., beautiful, safe, convenient) -based image embeddings in visual place recognition, as a domain-adaptive compact image descriptor that is orthogonal to the typical approach of absolute attribute (e.g., color, shape, texture) -based image embeddings, is explored in this paper.



### A comprehensive survey on computer-aided diagnostic systems in diabetic retinopathy screening
- **Arxiv ID**: http://arxiv.org/abs/2208.01810v1
- **DOI**: 10.1088/978-0-7503-2060-3ch12
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2208.01810v1)
- **Published**: 2022-08-03 02:11:42+00:00
- **Updated**: 2022-08-03 02:11:42+00:00
- **Authors**: Meysam Tavakoli, Patrick Kelley
- **Comment**: 65 pages, 7 figures, 9 tables
- **Journal**: None
- **Summary**: Diabetes Mellitus (DM) can lead to significant microvasculature disruptions that eventually causes diabetic retinopathy (DR), or complications in the eye due to diabetes. If left unchecked, this disease can increase over time and eventually cause complete vision loss. The general method to detect such optical developments is through examining the vessels, optic nerve head, microaneurysms, haemorrhage, exudates, etc. from retinal images. Ultimately this is limited by the number of experienced ophthalmologists and the vastly growing number of DM cases. To enable earlier and efficient DR diagnosis, the field of ophthalmology requires robust computer aided diagnosis (CAD) systems. Our review is intended for anyone, from student to established researcher, who wants to understand what can be accomplished with CAD systems and their algorithms to modeling and where the field of retinal image processing in computer vision and pattern recognition is headed. For someone just getting started, we place a special emphasis on the logic, strengths and shortcomings of different databases and algorithms frameworks with a focus on very recent approaches.



### TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.01813v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01813v3)
- **Published**: 2022-08-03 02:18:09+00:00
- **Updated**: 2022-10-08 00:33:02+00:00
- **Authors**: Jun Wang, Mingfei Gao, Yuqian Hu, Ramprasaath R. Selvaraju, Chetan Ramaiah, Ran Xu, Joseph F. JaJa, Larry S. Davis
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Text-VQA aims at answering questions that require understanding the textual cues in an image. Despite the great progress of existing Text-VQA methods, their performance suffers from insufficient human-labeled question-answer (QA) pairs. However, we observe that, in general, the scene text is not fully exploited in the existing datasets -- only a small portion of the text in each image participates in the annotated QA activities. This results in a huge waste of useful information. To address this deficiency, we develop a new method to generate high-quality and diverse QA pairs by explicitly utilizing the existing rich text available in the scene context of each image. Specifically, we propose, TAG, a text-aware visual question-answer generation architecture that learns to produce meaningful, and accurate QA samples using a multimodal transformer. The architecture exploits underexplored scene text information and enhances scene understanding of Text-VQA models by combining the generated QA pairs with the initial training data. Extensive experimental results on two well-known Text-VQA benchmarks (TextVQA and ST-VQA) demonstrate that our proposed TAG effectively enlarges the training data that helps improve the Text-VQA performance without extra labeling effort. Moreover, our model outperforms state-of-the-art approaches that are pre-trained with extra large-scale data. Code is available at https://github.com/HenryJunW/TAG.



### Neural Contourlet Network for Monocular 360 Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.01817v1
- **DOI**: 10.1109/TCSVT.2022.3192283
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01817v1)
- **Published**: 2022-08-03 02:25:55+00:00
- **Updated**: 2022-08-03 02:25:55+00:00
- **Authors**: Zhijie Shen, Chunyu Lin, Lang Nie, Kang Liao, Yao Zhao
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology
- **Journal**: None
- **Summary**: For a monocular 360 image, depth estimation is a challenging because the distortion increases along the latitude. To perceive the distortion, existing methods devote to designing a deep and complex network architecture. In this paper, we provide a new perspective that constructs an interpretable and sparse representation for a 360 image. Considering the importance of the geometric structure in depth estimation, we utilize the contourlet transform to capture an explicit geometric cue in the spectral domain and integrate it with an implicit cue in the spatial domain. Specifically, we propose a neural contourlet network consisting of a convolutional neural network and a contourlet transform branch. In the encoder stage, we design a spatial-spectral fusion module to effectively fuse two types of cues. Contrary to the encoder, we employ the inverse contourlet transform with learned low-pass subbands and band-pass directional subbands to compose the depth in the decoder. Experiments on the three popular panoramic image datasets demonstrate that the proposed approach outperforms the state-of-the-art schemes with faster convergence. Code is available at https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE.



### Statistical Attention Localization (SAL): Methodology and Application to Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.01823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01823v1)
- **Published**: 2022-08-03 02:55:11+00:00
- **Updated**: 2022-08-03 02:55:11+00:00
- **Authors**: Yijing Yang, Vasileios Magoulianitis, Xinyu Wang, C. -C. Jay Kuo
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: A statistical attention localization (SAL) method is proposed to facilitate the object classification task in this work. SAL consists of three steps: 1) preliminary attention window selection via decision statistics, 2) attention map refinement, and 3) rectangular attention region finalization. SAL computes soft-decision scores of local squared windows and uses them to identify salient regions in Step 1. To accommodate object of various sizes and shapes, SAL refines the preliminary result and obtain an attention map of more flexible shape in Step 2. Finally, SAL yields a rectangular attention region using the refined attention map and bounding box regularization in Step 3. As an application, we adopt E-PixelHop, which is an object classification solution based on successive subspace learning (SSL), as the baseline. We apply SAL so as to obtain a cropped-out and resized attention region as an alternative input. Classification results of the whole image as well as the attention region are ensembled to achieve the highest classification accuracy. Experiments on the CIFAR-10 dataset are given to demonstrate the advantage of the SAL-assisted object classification method.



### Medical image registration using unsupervised deep neural network: A scoping literature review
- **Arxiv ID**: http://arxiv.org/abs/2208.01825v1
- **DOI**: 10.1016/j.bspc.2021.103444
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.01825v1)
- **Published**: 2022-08-03 03:11:34+00:00
- **Updated**: 2022-08-03 03:11:34+00:00
- **Authors**: Samaneh Abbasi, Meysam Tavakoli, Hamid Reza Boveiri, Mohammad Amin Mosleh Shirazi, Raouf Khayami, Hedieh Khorasani, Reza Javidan, Alireza Mehdizadeh
- **Comment**: None
- **Journal**: Biomedical Signal Processing and Control 2021
- **Summary**: In medicine, image registration is vital in image-guided interventions and other clinical applications. However, it is a difficult subject to be addressed which by the advent of machine learning, there have been considerable progress in algorithmic performance has recently been achieved for medical image registration in this area. The implementation of deep neural networks provides an opportunity for some medical applications such as conducting image registration in less time with high accuracy, playing a key role in countering tumors during the operation. The current study presents a comprehensive scoping review on the state-of-the-art literature of medical image registration studies based on unsupervised deep neural networks is conducted, encompassing all the related studies published in this field to this date. Here, we have tried to summarize the latest developments and applications of unsupervised deep learning-based registration methods in the medical field. Fundamental and main concepts, techniques, statistical analysis from different viewpoints, novelties, and future directions are elaborately discussed and conveyed in the current comprehensive scoping review. Besides, this review hopes to help those active readers, who are riveted by this field, achieve deep insight into this exciting field.



### Fast Hierarchical Deep Unfolding Network for Image Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/2208.01827v1
- **DOI**: 10.1145/3503161.3548389
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.01827v1)
- **Published**: 2022-08-03 03:27:32+00:00
- **Updated**: 2022-08-03 03:27:32+00:00
- **Authors**: Wenxue Cui, Shaohui Liu, Debin Zhao
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: By integrating certain optimization solvers with deep neural network, deep unfolding network (DUN) has attracted much attention in recent years for image compressed sensing (CS). However, there still exist several issues in existing DUNs: 1) For each iteration, a simple stacked convolutional network is usually adopted, which apparently limits the expressiveness of these models. 2) Once the training is completed, most hyperparameters of existing DUNs are fixed for any input content, which significantly weakens their adaptability. In this paper, by unfolding the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), a novel fast hierarchical DUN, dubbed FHDUN, is proposed for image compressed sensing, in which a well-designed hierarchical unfolding architecture is developed to cooperatively explore richer contextual prior information in multi-scale spaces. To further enhance the adaptability, series of hyperparametric generation networks are developed in our framework to dynamically produce the corresponding optimal hyperparameters according to the input content. Furthermore, due to the accelerated policy in FISTA, the newly embedded acceleration module makes the proposed FHDUN save more than 50% of the iterative loops against recent DUNs. Extensive CS experiments manifest that the proposed FHDUN outperforms existing state-of-the-art CS methods, while maintaining fewer iterations.



### Integrating Object-aware and Interaction-aware Knowledge for Weakly Supervised Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.01834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01834v1)
- **Published**: 2022-08-03 04:20:17+00:00
- **Updated**: 2022-08-03 04:20:17+00:00
- **Authors**: Xingchen Li, Long Chen, Wenbo Ma, Yi Yang, Jun Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, increasing efforts have been focused on Weakly Supervised Scene Graph Generation (WSSGG). The mainstream solution for WSSGG typically follows the same pipeline: they first align text entities in the weak image-level supervisions (e.g., unlocalized relation triplets or captions) with image regions, and then train SGG models in a fully-supervised manner with aligned instance-level "pseudo" labels. However, we argue that most existing WSSGG works only focus on object-consistency, which means the grounded regions should have the same object category label as text entities. While they neglect another basic requirement for an ideal alignment: interaction-consistency, which means the grounded region pairs should have the same interactions (i.e., visual relations) as text entity pairs. Hence, in this paper, we propose to enhance a simple grounding module with both object-aware and interaction-aware knowledge to acquire more reliable pseudo labels. To better leverage these two types of knowledge, we regard them as two teachers and fuse their generated targets to guide the training process of our grounding module. Specifically, we design two different strategies to adaptively assign weights to different teachers by assessing their reliability on each training sample. Extensive experiments have demonstrated that our method consistently improves WSSGG performance on various kinds of weak supervision.



### EMC2A-Net: An Efficient Multibranch Cross-channel Attention Network for SAR Target Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.01836v1
- **DOI**: 10.1109/TGRS.2023.3285037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01836v1)
- **Published**: 2022-08-03 04:31:52+00:00
- **Updated**: 2022-08-03 04:31:52+00:00
- **Authors**: Xiang Yu, Zhe Geng, Xiaohua Huang, Qinglu Wang, Daiyin Zhu
- **Comment**: 15 pages, 9 figures, Submitted to IEEE Transactions on Geoscience and
  Remote Sensing, 2022
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) have shown great potential in synthetic aperture radar (SAR) target recognition. SAR images have a strong sense of granularity and have different scales of texture features, such as speckle noise, target dominant scatterers and target contours, which are rarely considered in the traditional CNN model. This paper proposed two residual blocks, namely EMC2A blocks with multiscale receptive fields(RFs), based on a multibranch structure and then designed an efficient isotopic architecture deep CNN (DCNN), EMC2A-Net. EMC2A blocks utilize parallel dilated convolution with different dilation rates, which can effectively capture multiscale context features without significantly increasing the computational burden. To further improve the efficiency of multiscale feature fusion, this paper proposed a multiscale feature cross-channel attention module, namely the EMC2A module, adopting a local multiscale feature interaction strategy without dimensionality reduction. This strategy adaptively adjusts the weights of each channel through efficient one-dimensional (1D)-circular convolution and sigmoid function to guide attention at the global channel wise level. The comparative results on the MSTAR dataset show that EMC2A-Net outperforms the existing available models of the same type and has relatively lightweight network structure. The ablation experiment results show that the EMC2A module significantly improves the performance of the model by using only a few parameters and appropriate cross-channel interactions.



### Learning Prior Feature and Attention Enhanced Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2208.01837v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01837v2)
- **Published**: 2022-08-03 04:32:53+00:00
- **Updated**: 2023-08-26 03:30:36+00:00
- **Authors**: Chenjie Cao, Qiaole Dong, Yanwei Fu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Many recent inpainting works have achieved impressive results by leveraging Deep Neural Networks (DNNs) to model various prior information for image restoration. Unfortunately, the performance of these methods is largely limited by the representation ability of vanilla Convolutional Neural Networks (CNNs) backbones.On the other hand, Vision Transformers (ViT) with self-supervised pre-training have shown great potential for many visual recognition and object detection tasks. A natural question is whether the inpainting task can be greatly benefited from the ViT backbone? However, it is nontrivial to directly replace the new backbones in inpainting networks, as the inpainting is an inverse problem fundamentally different from the recognition tasks. To this end, this paper incorporates the pre-training based Masked AutoEncoder (MAE) into the inpainting model, which enjoys richer informative priors to enhance the inpainting process. Moreover, we propose to use attention priors from MAE to make the inpainting model learn more long-distance dependencies between masked and unmasked regions. Sufficient ablations have been discussed about the inpainting and the self-supervised pre-training models in this paper. Besides, experiments on both Places2 and FFHQ demonstrate the effectiveness of our proposed model. Codes and pre-trained models are released in https://github.com/ewrfcas/MAE-FAR.



### Re-Attention Transformer for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2208.01838v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01838v2)
- **Published**: 2022-08-03 04:34:28+00:00
- **Updated**: 2023-02-26 03:54:20+00:00
- **Authors**: Hui Su, Yue Ye, Zhiwei Chen, Mingli Song, Lechao Cheng
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Weakly supervised object localization is a challenging task which aims to localize objects with coarse annotations such as image categories. Existing deep network approaches are mainly based on class activation map, which focuses on highlighting discriminative local region while ignoring the full object. In addition, the emerging transformer-based techniques constantly put a lot of emphasis on the backdrop that impedes the ability to identify complete objects. To address these issues, we present a re-attention mechanism termed token refinement transformer (TRT) that captures the object-level semantics to guide the localization well. Specifically, TRT introduces a novel module named token priority scoring module (TPSM) to suppress the effects of background noise while focusing on the target object. Then, we incorporate the class activation map as the semantically aware input to restrain the attention map to the target object. Extensive experiments on two benchmarks showcase the superiority of our proposed method against existing methods with image category annotations. Source code is available in \url{https://github.com/su-hui-zz/ReAttentionTransformer}.



### 'Labelling the Gaps': A Weakly Supervised Automatic Eye Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.01840v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01840v2)
- **Published**: 2022-08-03 04:51:56+00:00
- **Updated**: 2022-08-12 04:01:02+00:00
- **Authors**: Shreya Ghosh, Abhinav Dhall, Jarrod Knibbe, Munawar Hayat
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past few years, there has been an increasing interest to interpret gaze direction in an unconstrained environment with limited supervision. Owing to data curation and annotation issues, replicating gaze estimation method to other platforms, such as unconstrained outdoor or AR/VR, might lead to significant drop in performance due to insufficient availability of accurately annotated data for model training. In this paper, we explore an interesting yet challenging problem of gaze estimation method with a limited amount of labelled data. The proposed method distills knowledge from the labelled subset with visual features; including identity-specific appearance, gaze trajectory consistency and motion features. Given a gaze trajectory, the method utilizes label information of only the start and the end frames of a gaze sequence. An extension of the proposed method further reduces the requirement of labelled frames to only the start frame with a minor drop in the generated label's quality. We evaluate the proposed method on four benchmark datasets (CAVE, TabletGaze, MPII and Gaze360) as well as web-crawled YouTube videos. Our proposed method reduces the annotation effort to as low as 2.67%, with minimal impact on performance; indicating the potential of our model enabling gaze estimation 'in-the-wild' setup.



### DALLE-URBAN: Capturing the urban design expertise of large text to image transformers
- **Arxiv ID**: http://arxiv.org/abs/2208.04139v2
- **DOI**: 10.1109/DICTA56598.2022.10034603
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.04139v2)
- **Published**: 2022-08-03 04:59:16+00:00
- **Updated**: 2022-10-03 08:21:46+00:00
- **Authors**: Sachith Seneviratne, Damith Senanayake, Sanka Rasnayaka, Rajith Vidanaarachchi, Jason Thompson
- **Comment**: Accepted to DICTA 2022, released 11000+ environmental scene images
  generated by Stable Diffusion and 1000+ images generated by DALLE-2
- **Journal**: None
- **Summary**: Automatically converting text descriptions into images using transformer architectures has recently received considerable attention. Such advances have implications for many applied design disciplines across fashion, art, architecture, urban planning, landscape design and the future tools available to such disciplines. However, a detailed analysis capturing the capabilities of such models, specifically with a focus on the built environment, has not been performed to date. In this work, we investigate the capabilities and biases of such text-to-image methods as it applies to the built environment in detail. We use a systematic grammar to generate queries related to the built environment and evaluate resulting generated images. We generate 1020 different images and find that text to image transformers are robust at generating realistic images across different domains for this use-case. Generated imagery can be found at the github: https://github.com/sachith500/DALLEURBAN



### Multi-Feature Vision Transformer via Self-Supervised Representation Learning for Improvement of COVID-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2208.01843v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.01843v1)
- **Published**: 2022-08-03 05:02:47+00:00
- **Updated**: 2022-08-03 05:02:47+00:00
- **Authors**: Xiao Qi, David J. Foran, John L. Nosher, Ilker Hacihaliloglu
- **Comment**: Accepted to the 2022 MICCAI Workshop on Medical Image Learning with
  Limited and Noisy Data
- **Journal**: None
- **Summary**: The role of chest X-ray (CXR) imaging, due to being more cost-effective, widely available, and having a faster acquisition time compared to CT, has evolved during the COVID-19 pandemic. To improve the diagnostic performance of CXR imaging a growing number of studies have investigated whether supervised deep learning methods can provide additional support. However, supervised methods rely on a large number of labeled radiology images, which is a time-consuming and complex procedure requiring expert clinician input. Due to the relative scarcity of COVID-19 patient data and the costly labeling process, self-supervised learning methods have gained momentum and has been proposed achieving comparable results to fully supervised learning approaches. In this work, we study the effectiveness of self-supervised learning in the context of diagnosing COVID-19 disease from CXR images. We propose a multi-feature Vision Transformer (ViT) guided architecture where we deploy a cross-attention mechanism to learn information from both original CXR images and corresponding enhanced local phase CXR images. We demonstrate the performance of the baseline self-supervised learning models can be further improved by leveraging the local phase-based enhanced CXR images. By using 10\% labeled CXR scans, the proposed model achieves 91.10\% and 96.21\% overall accuracy tested on total 35,483 CXR images of healthy (8,851), regular pneumonia (6,045), and COVID-19 (18,159) scans and shows significant improvement over state-of-the-art techniques. Code is available https://github.com/endiqq/Multi-Feature-ViT



### Multiclass ASMA vs Targeted PGD Attack in Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.01844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01844v1)
- **Published**: 2022-08-03 05:05:30+00:00
- **Updated**: 2022-08-03 05:05:30+00:00
- **Authors**: Johnson Vo, Jiabao Xie, Sahil Patel
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning networks have demonstrated high performance in a large variety of applications, such as image classification, speech recognition, and natural language processing. However, there exists a major vulnerability exploited by the use of adversarial attacks. An adversarial attack imputes images by altering the input image very slightly, making it nearly undetectable to the naked eye, but results in a very different classification by the network. This paper explores the projected gradient descent (PGD) attack and the Adaptive Mask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model using two types of architectures: MobileNetV3 and ResNet50, It was found that PGD was very consistent in changing the segmentation to be its target while the generalization of ASMA to a multiclass target was not as effective. The existence of such attack however puts all of image classification deep learning networks in danger of exploitation.



### Pyramidal Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2208.01864v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.01864v3)
- **Published**: 2022-08-03 06:26:18+00:00
- **Updated**: 2022-09-30 06:06:06+00:00
- **Authors**: Dohoon Ryu, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, diffusion model have demonstrated impressive image generation performances, and have been extensively studied in various computer vision tasks. Unfortunately, training and evaluating diffusion models consume a lot of time and computational resources. To address this problem, here we present a novel pyramidal diffusion model that can generate high resolution images starting from much coarser resolution images using a {\em single} score function trained with a positional embedding. This enables a neural network to be much lighter and also enables time-efficient image generation without compromising its performances. Furthermore, we show that the proposed approach can be also efficiently used for multi-scale super-resolution problem using a single score function.



### Leveraging Smartphone Sensors for Detecting Abnormal Gait for Smart Wearable Mobile Technologies
- **Arxiv ID**: http://arxiv.org/abs/2208.01876v1
- **DOI**: 10.3991/ijim.v15i24.25891.
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.01876v1)
- **Published**: 2022-08-03 07:00:16+00:00
- **Updated**: 2022-08-03 07:00:16+00:00
- **Authors**: Md Shahriar Tasjid, Ahmed Al Marouf
- **Comment**: None
- **Journal**: International Journal of Interactive Mobile Technologies (iJIM);
  Volume 15 Number 24; Page 167-175; 2021
- **Summary**: Walking is one of the most common modes of terrestrial locomotion for humans. Walking is essential for humans to perform most kinds of daily activities. When a person walks, there is a pattern in it, and it is known as gait. Gait analysis is used in sports and healthcare. We can analyze this gait in different ways, like using video captured by the surveillance cameras or depth image cameras in the lab environment. It also can be recognized by wearable sensors. e.g., accelerometer, force sensors, gyroscope, flexible goniometer, magneto resistive sensors, electromagnetic tracking system, force sensors, and electromyography (EMG). Analysis through these sensors required a lab condition, or users must wear these sensors. For detecting abnormality in gait action of a human, we need to incorporate the sensors separately. We can know about one's health condition by abnormal human gait after detecting it. Understanding a regular gait vs. abnormal gait may give insights to the health condition of the subject using the smart wearable technologies. Therefore, in this paper, we proposed a way to analyze abnormal human gait through smartphone sensors. Though smart devices like smartphones and smartwatches are used by most of the person nowadays. So, we can track down their gait using sensors of these intelligent wearable devices.



### Graph Signal Processing for Heterogeneous Change Detection Part I: Vertex Domain Filtering
- **Arxiv ID**: http://arxiv.org/abs/2208.01881v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.01881v2)
- **Published**: 2022-08-03 07:22:45+00:00
- **Updated**: 2022-08-08 02:32:15+00:00
- **Authors**: Yuli Sun, Lin Lei, Dongdong Guan, Gangyao Kuang, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a new strategy for the Heterogeneous Change Detection (HCD) problem: solving HCD from the perspective of Graph Signal Processing (GSP). We construct a graph for each image to capture the structure information, and treat each image as the graph signal. In this way, we convert the HCD into a GSP problem: a comparison of the responses of the two signals on different systems defined on the two graphs, which attempts to find structural differences (Part I) and signal differences (Part II) due to the changes between heterogeneous images. In this first part, we analyze the HCD with GSP from the vertex domain. We first show that for the unchanged images, their structures are consistent, and then the outputs of the same signal on systems defined on the two graphs are similar. However, once a region has changed, the local structure of the image changes, i.e., the connectivity of the vertex containing this region changes. Then, we can compare the output signals of the same input graph signal passing through filters defined on the two graphs to detect changes. We design different filters from the vertex domain, which can flexibly explore the high-order neighborhood information hidden in original graphs. We also analyze the detrimental effects of changing regions on the change detection results from the viewpoint of signal propagation. Experiments conducted on seven real data sets show the effectiveness of the vertex domain filtering based HCD method.



### Combined CNN Transformer Encoder for Enhanced Fine-grained Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.01897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01897v1)
- **Published**: 2022-08-03 08:01:55+00:00
- **Updated**: 2022-08-03 08:01:55+00:00
- **Authors**: Mei Chee Leong, Haosong Zhang, Hui Li Tan, Liyuan Li, Joo Hwee Lim
- **Comment**: The Ninth Workshop on Fine-Grained Visual Categorization (FGVC9) @
  CVPR2022
- **Journal**: None
- **Summary**: Fine-grained action recognition is a challenging task in computer vision. As fine-grained datasets have small inter-class variations in spatial and temporal space, fine-grained action recognition model requires good temporal reasoning and discrimination of attribute action semantics. Leveraging on CNN's ability in capturing high level spatial-temporal feature representations and Transformer's modeling efficiency in capturing latent semantics and global dependencies, we investigate two frameworks that combine CNN vision backbone and Transformer Encoder to enhance fine-grained action recognition: 1) a vision-based encoder to learn latent temporal semantics, and 2) a multi-modal video-text cross encoder to exploit additional text input and learn cross association between visual and text semantics. Our experimental results show that both our Transformer encoder frameworks effectively learn latent temporal semantics and cross-modality association, with improved recognition performance over CNN vision model. We achieve new state-of-the-art performance on the FineGym benchmark dataset for both proposed architectures.



### XCon: Learning with Experts for Fine-grained Category Discovery
- **Arxiv ID**: http://arxiv.org/abs/2208.01898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01898v1)
- **Published**: 2022-08-03 08:03:12+00:00
- **Updated**: 2022-08-03 08:03:12+00:00
- **Authors**: Yixin Fei, Zhongkai Zhao, Siwei Yang, Bingchen Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of generalized category discovery (GCD) in this paper, i.e. clustering the unlabeled images leveraging the information from a set of seen classes, where the unlabeled images could contain both seen classes and unseen classes. The seen classes can be seen as an implicit criterion of classes, which makes this setting different from unsupervised clustering where the cluster criteria may be ambiguous. We mainly concern the problem of discovering categories within a fine-grained dataset since it is one of the most direct applications of category discovery, i.e. helping experts discover novel concepts within an unlabeled dataset using the implicit criterion set forth by the seen classes. State-of-the-art methods for generalized category discovery leverage contrastive learning to learn the representations, but the large inter-class similarity and intra-class variance pose a challenge for the methods because the negative examples may contain irrelevant cues for recognizing a category so the algorithms may converge to a local-minima. We present a novel method called Expert-Contrastive Learning (XCon) to help the model to mine useful information from the images by first partitioning the dataset into sub-datasets using k-means clustering and then performing contrastive learning on each of the sub-datasets to learn fine-grained discriminative features. Experiments on fine-grained datasets show a clear improved performance over the previous best methods, indicating the effectiveness of our method.



### Graph Signal Processing for Heterogeneous Change Detection Part II: Spectral Domain Analysis
- **Arxiv ID**: http://arxiv.org/abs/2208.01905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01905v2)
- **Published**: 2022-08-03 08:11:24+00:00
- **Updated**: 2022-08-08 03:00:43+00:00
- **Authors**: Yuli Sun, Lin Lei, Dongdong Guan, Gangyao Kuang, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This is the second part of the paper that provides a new strategy for the heterogeneous change detection (HCD) problem, that is, solving HCD from the perspective of graph signal processing (GSP). We construct a graph to represent the structure of each image, and treat each image as a graph signal defined on the graph. In this way, we can convert the HCD problem into a comparison of responses of signals on systems defined on the graphs. In the part I, the changes are measured by comparing the structure difference between the graphs from the vertex domain. In this part II, we analyze the GSP for HCD from the spectral domain. We first analyze the spectral properties of the different images on the same graph, and show that their spectra exhibit commonalities and dissimilarities. Specially, it is the change that leads to the dissimilarities of their spectra. Then, we propose a regression model for the HCD, which decomposes the source signal into the regressed signal and changed signal, and requires the regressed signal have the same spectral property as the target signal on the same graph. With the help of graph spectral analysis, the proposed regression model is flexible and scalable. Experiments conducted on seven real data sets show the effectiveness of the proposed method.



### Rethinking the Evaluation of Unbiased Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.01909v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01909v2)
- **Published**: 2022-08-03 08:23:51+00:00
- **Updated**: 2022-10-11 02:04:18+00:00
- **Authors**: Xingchen Li, Long Chen, Jian Shao, Shaoning Xiao, Songyang Zhang, Jun Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Current Scene Graph Generation (SGG) methods tend to predict frequent predicate categories and fail to recognize rare ones due to the severe imbalanced distribution of predicates. To improve the robustness of SGG models on different predicate categories, recent research has focused on unbiased SGG and adopted mean Recall@K (mR@K) as the main evaluation metric. However, we discovered two overlooked issues about this de facto standard metric, which makes current unbiased SGG evaluation vulnerable and unfair: 1) mR@K neglects the correlations among predicates and unintentionally breaks category independence when ranking all the triplet predictions together regardless of the predicate categories. 2) mR@K neglects the compositional diversity of different predicates and assigns excessively high weights to some oversimple category samples with limited composable relation triplet types. In addition, we investigate the under-explored correlation between objects and predicates, which can serve as a simple but strong baseline for unbiased SGG. In this paper, we refine mR@K and propose two complementary evaluation metrics for unbiased SGG: Independent Mean Recall (MR) and weighted IMR (wIMR). These two metrics are designed by considering the category independence and diversity of composable relation triplets, respectively. We compare the proposed metrics with the de facto standard metrics through extensive experiments and discuss the solutions to evaluate unbiased SGG in a more trustworthy way.



### Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Activities of Daily Living
- **Arxiv ID**: http://arxiv.org/abs/2208.01910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01910v1)
- **Published**: 2022-08-03 08:28:33+00:00
- **Updated**: 2022-08-03 08:28:33+00:00
- **Authors**: Zdravko Marinov, David Schneider, Alina Roitberg, Rainer Stiefelhagen
- **Comment**: 8 pages, 7 figures, to be published in IROS 2022
- **Journal**: None
- **Summary**: Domain shifts, such as appearance changes, are a key challenge in real-world applications of activity recognition models, which range from assistive robotics and smart homes to driver observation in intelligent vehicles. For example, while simulations are an excellent way of economical data collection, a Synthetic-to-Real domain shift leads to a > 60% drop in accuracy when recognizing activities of Daily Living (ADLs). We tackle this challenge and introduce an activity domain generation framework which creates novel ADL appearances (novel domains) from different existing activity modalities (source domains) inferred from video training data. Our framework computes human poses, heatmaps of body joints, and optical flow maps and uses them alongside the original RGB videos to learn the essence of source domains in order to generate completely new ADL domains. The model is optimized by maximizing the distance between the existing source appearances and the generated novel appearances while ensuring that the semantics of an activity is preserved through an additional classification loss. While source data multimodality is an important concept in this design, our setup does not rely on multi-sensor setups, (i.e., all source modalities are inferred from a single video only.) The newly created activity domains are then integrated in the training of the ADL classification networks, resulting in models far less susceptible to changes in data distributions. Extensive experiments on the Synthetic-to-Real benchmark Sims4Action demonstrate the potential of the domain generation paradigm for cross-domain ADL recognition, setting new state-of-the-art results. Our code is publicly available at https://github.com/Zrrr1997/syn2real_DG



### N-RPN: Hard Example Learning for Region Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.01916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01916v1)
- **Published**: 2022-08-03 08:48:33+00:00
- **Updated**: 2022-08-03 08:48:33+00:00
- **Authors**: MyeongAh Cho, Tae-young Chung, Hyeongmin Lee, Sangyoun Lee
- **Comment**: None
- **Journal**: None
- **Summary**: The region proposal task is to generate a set of candidate regions that contain an object. In this task, it is most important to propose as many candidates of ground-truth as possible in a fixed number of proposals. In a typical image, however, there are too few hard negative examples compared to the vast number of easy negatives, so region proposal networks struggle to train on hard negatives. Because of this problem, networks tend to propose hard negatives as candidates, while failing to propose ground-truth candidates, which leads to poor performance. In this paper, we propose a Negative Region Proposal Network(nRPN) to improve Region Proposal Network(RPN). The nRPN learns from the RPN's false positives and provide hard negative examples to the RPN. Our proposed nRPN leads to a reduction in false positives and better RPN performance. An RPN trained with an nRPN achieves performance improvements on the PASCAL VOC 2007 dataset.



### Per-Clip Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.01924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01924v1)
- **Published**: 2022-08-03 09:02:29+00:00
- **Updated**: 2022-08-03 09:02:29+00:00
- **Authors**: Kwanyong Park, Sanghyun Woo, Seoung Wug Oh, In So Kweon, Joon-Young Lee
- **Comment**: CVPR 2022; Code is available at https://github.com/pkyong95/PCVOS
- **Journal**: None
- **Summary**: Recently, memory-based approaches show promising results on semi-supervised video object segmentation. These methods predict object masks frame-by-frame with the help of frequently updated memory of the previous mask. Different from this per-frame inference, we investigate an alternative perspective by treating video object segmentation as clip-wise mask propagation. In this per-clip inference scheme, we update the memory with an interval and simultaneously process a set of consecutive frames (i.e. clip) between the memory updates. The scheme provides two potential benefits: accuracy gain by clip-level optimization and efficiency gain by parallel computation of multiple frames. To this end, we propose a new method tailored for the per-clip inference. Specifically, we first introduce a clip-wise operation to refine the features based on intra-clip correlation. In addition, we employ a progressive matching mechanism for efficient information-passing within a clip. With the synergy of two modules and a newly proposed per-clip based training, our network achieves state-of-the-art performance on Youtube-VOS 2018/2019 val (84.6% and 84.6%) and DAVIS 2016/2017 val (91.9% and 86.1%). Furthermore, our model shows a great speed-accuracy trade-off with varying memory update intervals, which leads to huge flexibility.



### SuperLine3D: Self-supervised Line Segmentation and Description for LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2208.01925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01925v1)
- **Published**: 2022-08-03 09:06:14+00:00
- **Updated**: 2022-08-03 09:06:14+00:00
- **Authors**: Xiangrui Zhao, Sheng Yang, Tianxin Huang, Jun Chen, Teng Ma, Mingyang Li, Yong Liu
- **Comment**: 17 pages, ECCV 2022 Accepted
- **Journal**: None
- **Summary**: Poles and building edges are frequently observable objects on urban roads, conveying reliable hints for various computer vision tasks. To repetitively extract them as features and perform association between discrete LiDAR frames for registration, we propose the first learning-based feature segmentation and description model for 3D lines in LiDAR point cloud. To train our model without the time consuming and tedious data labeling process, we first generate synthetic primitives for the basic appearance of target lines, and build an iterative line auto-labeling process to gradually refine line labels on real LiDAR scans. Our segmentation model can extract lines under arbitrary scale perturbations, and we use shared EdgeConv encoder layers to train the two segmentation and descriptor heads jointly. Base on the model, we can build a highly-available global registration module for point cloud registration, in conditions without initial transformation hints. Experiments have demonstrated that our line-based registration method is highly competitive to state-of-the-art point-based approaches. Our code is available at https://github.com/zxrzju/SuperLine3D.git.



### PalQuant: Accelerating High-precision Networks on Low-precision Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2208.01944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01944v1)
- **Published**: 2022-08-03 09:44:13+00:00
- **Updated**: 2022-08-03 09:44:13+00:00
- **Authors**: Qinghao Hu, Gang Li, Qiman Wu, Jian Cheng
- **Comment**: accepted by ECCV2022
- **Journal**: None
- **Summary**: Recently low-precision deep learning accelerators (DLAs) have become popular due to their advantages in chip area and energy consumption, yet the low-precision quantized models on these DLAs bring in severe accuracy degradation. One way to achieve both high accuracy and efficient inference is to deploy high-precision neural networks on low-precision DLAs, which is rarely studied. In this paper, we propose the PArallel Low-precision Quantization (PalQuant) method that approximates high-precision computations via learning parallel low-precision representations from scratch. In addition, we present a novel cyclic shuffle module to boost the cross-group information communication between parallel low-precision groups. Extensive experiments demonstrate that PalQuant has superior performance to state-of-the-art quantization methods in both accuracy and inference speed, e.g., for ResNet-18 network quantization, PalQuant can obtain 0.52\% higher accuracy and 1.78$\times$ speedup simultaneously over their 4-bit counter-part on a state-of-the-art 2-bit accelerator. Code is available at \url{https://github.com/huqinghao/PalQuant}.



### Decay2Distill: Leveraging spatial perturbation and regularization for self-supervised image denoising
- **Arxiv ID**: http://arxiv.org/abs/2208.01948v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.01948v2)
- **Published**: 2022-08-03 09:54:05+00:00
- **Updated**: 2022-08-04 14:27:59+00:00
- **Authors**: Manisha Das Chaity, Masud An Nur Islam Fahim
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired image denoising has achieved promising development over the last few years. Regardless of the performance, methods tend to heavily rely on underlying noise properties or any assumption which is not always practical. Alternatively, if we can ground the problem from a structural perspective rather than noise statistics, we can achieve a more robust solution. with such motivation, we propose a self-supervised denoising scheme that is unpaired and relies on spatial degradation followed by a regularized refinement. Our method shows considerable improvement over previous methods and exhibited consistent performance over different data domains.



### Negative Frames Matter in Egocentric Visual Query 2D Localization
- **Arxiv ID**: http://arxiv.org/abs/2208.01949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01949v1)
- **Published**: 2022-08-03 09:54:51+00:00
- **Updated**: 2022-08-03 09:54:51+00:00
- **Authors**: Mengmeng Xu, Cheng-Yang Fu, Yanghao Li, Bernard Ghanem, Juan-Manuel Perez-Rua, Tao Xiang
- **Comment**: First place winning solution for VQ2D task in CVPR-2022 Ego4D
  Challenge. Our code is publicly available at
  https://github.com/facebookresearch/vq2d_cvpr
- **Journal**: None
- **Summary**: The recently released Ego4D dataset and benchmark significantly scales and diversifies the first-person visual perception data. In Ego4D, the Visual Queries 2D Localization task aims to retrieve objects appeared in the past from the recording in the first-person view. This task requires a system to spatially and temporally localize the most recent appearance of a given object query, where query is registered by a single tight visual crop of the object in a different scene.   Our study is based on the three-stage baseline introduced in the Episodic Memory benchmark. The baseline solves the problem by detection and tracking: detect the similar objects in all the frames, then run a tracker from the most confident detection result. In the VQ2D challenge, we identified two limitations of the current baseline. (1) The training configuration has redundant computation. Although the training set has millions of instances, most of them are repetitive and the number of unique object is only around 14.6k. The repeated gradient computation of the same object lead to an inefficient training; (2) The false positive rate is high on background frames. This is due to the distribution gap between training and evaluation. During training, the model is only able to see the clean, stable, and labeled frames, but the egocentric videos also have noisy, blurry, or unlabeled background frames. To this end, we developed a more efficient and effective solution. Concretely, we bring the training loop from ~15 days to less than 24 hours, and we achieve 0.17% spatial-temporal AP, which is 31% higher than the baseline. Our solution got the first ranking on the public leaderboard. Our code is publicly available at https://github.com/facebookresearch/vq2d_cvpr.



### Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2208.01954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01954v1)
- **Published**: 2022-08-03 10:00:49+00:00
- **Updated**: 2022-08-03 10:00:49+00:00
- **Authors**: Juncheng Li, Junlin Xie, Linchao Zhu, Long Qian, Siliang Tang, Wenqiao Zhang, Haochen Shi, Shengyu Zhang, Longhui Wei, Qi Tian, Yueting Zhuang
- **Comment**: Accepted by ACM Multimedia 2022
- **Journal**: None
- **Summary**: Understanding human emotions is a crucial ability for intelligent robots to provide better human-robot interactions. The existing works are limited to trimmed video-level emotion classification, failing to locate the temporal window corresponding to the emotion. In this paper, we introduce a new task, named Temporal Emotion Localization in videos~(TEL), which aims to detect human emotions and localize their corresponding temporal boundaries in untrimmed videos with aligned subtitles. TEL presents three unique challenges compared to temporal action localization: 1) The emotions have extremely varied temporal dynamics; 2) The emotion cues are embedded in both appearances and complex plots; 3) The fine-grained temporal annotations are complicated and labor-intensive. To address the first two challenges, we propose a novel dilated context integrated network with a coarse-fine two-stream architecture. The coarse stream captures varied temporal dynamics by modeling multi-granularity temporal contexts. The fine stream achieves complex plots understanding by reasoning the dependency between the multi-granularity temporal contexts from the coarse stream and adaptively integrates them into fine-grained video segment features. To address the third challenge, we introduce a cross-modal consensus learning paradigm, which leverages the inherent semantic consensus between the aligned video and subtitle to achieve weakly-supervised learning. We contribute a new testing set with 3,000 manually-annotated temporal boundaries so that future research on the TEL problem can be quantitatively evaluated. Extensive experiments show the effectiveness of our approach on temporal emotion localization. The repository of this work is at https://github.com/YYJMJC/Temporal-Emotion-Localization-in-Videos.



### Augmentation Learning for Semi-Supervised Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.01956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01956v1)
- **Published**: 2022-08-03 10:06:51+00:00
- **Updated**: 2022-08-03 10:06:51+00:00
- **Authors**: Tim Frommknecht, Pedro Alves Zipf, Quanfu Fan, Nina Shvetsova, Hilde Kuehne
- **Comment**: Accepted to GCPR 2022, 13 pages with 4 figures
- **Journal**: None
- **Summary**: Recently, a number of new Semi-Supervised Learning methods have emerged. As the accuracy for ImageNet and similar datasets increased over time, the performance on tasks beyond the classification of natural images is yet to be explored. Most Semi-Supervised Learning methods rely on a carefully manually designed data augmentation pipeline that is not transferable for learning on images of other domains. In this work, we propose a Semi-Supervised Learning method that automatically selects the most effective data augmentation policy for a particular dataset. We build upon the Fixmatch method and extend it with meta-learning of augmentations. The augmentation is learned in additional training before the classification training and makes use of bi-level optimization, to optimize the augmentation policy and maximize accuracy. We evaluate our approach on two domain-specific datasets, containing satellite images and hand-drawn sketches, and obtain state-of-the-art results. We further investigate in an ablation the different parameters relevant for learning augmentation policies and show how policy learning can be used to adapt augmentations to datasets beyond ImageNet.



### PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?
- **Arxiv ID**: http://arxiv.org/abs/2208.01957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.01957v1)
- **Published**: 2022-08-03 10:06:56+00:00
- **Updated**: 2022-08-03 10:06:56+00:00
- **Authors**: Aleksandr Kim, Guillem Bras, Aljoa Oep, Laura Leal-Taix
- **Comment**: ECCV 2022, 17 pages, 5 pages of supplementary, 3 figures
- **Journal**: None
- **Summary**: Most (3D) multi-object tracking methods rely on appearance-based cues for data association. By contrast, we investigate how far we can get by only encoding geometric relationships between objects in 3D space as cues for data-driven data association. We encode 3D detections as nodes in a graph, where spatial and temporal pairwise relations among objects are encoded via localized polar coordinates on graph edges. This representation makes our geometric relations invariant to global transformations and smooth trajectory changes, especially under non-holonomic motion. This allows our graph neural network to learn to effectively encode temporal and spatial interactions and fully leverage contextual and motion cues to obtain final scene interpretation by posing data association as edge classification. We establish a new state-of-the-art on nuScenes dataset and, more importantly, show that our method, PolarMOT, generalizes remarkably well across different locations (Boston, Singapore, Karlsruhe) and datasets (nuScenes and KITTI).



### Learning Object Manipulation Skills from Video via Approximate Differentiable Physics
- **Arxiv ID**: http://arxiv.org/abs/2208.01960v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.01960v1)
- **Published**: 2022-08-03 10:21:47+00:00
- **Updated**: 2022-08-03 10:21:47+00:00
- **Authors**: Vladimir Petrik, Mohammad Nomaan Qureshi, Josef Sivic, Makarand Tapaswi
- **Comment**: Accepted for IROS2022, code at
  https://github.com/petrikvladimir/video_skills_learning_with_approx_physics,
  project page at
  https://data.ciirc.cvut.cz/public/projects/2022Real2SimPhysics/
- **Journal**: None
- **Summary**: We aim to teach robots to perform simple object manipulation tasks by watching a single video demonstration. Towards this goal, we propose an optimization approach that outputs a coarse and temporally evolving 3D scene to mimic the action demonstrated in the input video. Similar to previous work, a differentiable renderer ensures perceptual fidelity between the 3D scene and the 2D video. Our key novelty lies in the inclusion of a differentiable approach to solve a set of Ordinary Differential Equations (ODEs) that allows us to approximately model laws of physics such as gravity, friction, and hand-object or object-object interactions. This not only enables us to dramatically improve the quality of estimated hand and object states, but also produces physically admissible trajectories that can be directly translated to a robot without the need for costly reinforcement learning. We evaluate our approach on a 3D reconstruction task that consists of 54 video demonstrations sourced from 9 actions such as pull something from right to left or put something in front of something. Our approach improves over previous state-of-the-art by almost 30%, demonstrating superior quality on especially challenging actions involving physical interactions of two objects such as put something onto something. Finally, we showcase the learned skills on a Franka Emika Panda robot.



### Localization and Classification of Parasitic Eggs in Microscopic Images Using an EfficientDet Detector
- **Arxiv ID**: http://arxiv.org/abs/2208.01963v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.1; I.4.5; I.4.9; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2208.01963v1)
- **Published**: 2022-08-03 10:28:18+00:00
- **Updated**: 2022-08-03 10:28:18+00:00
- **Authors**: Nouar AlDahoul, Hezerul Abdul Karim, Shaira Limson Kee, Myles Joshua Toledo Tan
- **Comment**: 6 pages, 7 figures, to be published in IEEE International Conference
  on Image Processing 2022
- **Journal**: None
- **Summary**: IPIs caused by protozoan and helminth parasites are among the most common infections in humans in LMICs. They are regarded as a severe public health concern, as they cause a wide array of potentially detrimental health conditions. Researchers have been developing pattern recognition techniques for the automatic identification of parasite eggs in microscopic images. Existing solutions still need improvements to reduce diagnostic errors and generate fast, efficient, and accurate results. Our paper addresses this and proposes a multi-modal learning detector to localize parasitic eggs and categorize them into 11 categories. The experiments were conducted on the novel Chula-ParasiteEgg-11 dataset that was used to train both EfficientDet model with EfficientNet-v2 backbone and EfficientNet-B7+SVM. The dataset has 11,000 microscopic training images from 11 categories. Our results show robust performance with an accuracy of 92%, and an F1 score of 93%. Additionally, the IOU distribution illustrates the high localization capability of the detector.



### Adaptive Domain Generalization via Online Disagreement Minimization
- **Arxiv ID**: http://arxiv.org/abs/2208.01996v2
- **DOI**: 10.1109/TIP.2023.3295739
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.01996v2)
- **Published**: 2022-08-03 11:51:11+00:00
- **Updated**: 2023-07-09 13:23:14+00:00
- **Authors**: Xin Zhang, Ying-Cong Chen
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samples into a domain-invariant space, and the multiple classifiers capture the distinct decision boundaries that each of them relates to a specific source domain. During testing, distribution differences between target and source domains could be effectively measured by leveraging prediction disagreement among source classifiers. By fine-tuning source models to minimize the disagreement at test time, target domain features are well aligned to the invariant feature space. We verify AdaODM on two popular DG methods, namely ERM and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and TerraIncognita. The results show AdaODM stably improves the generalization capacity on unseen domains and achieves state-of-the-art performance.



### Convolutional Fine-Grained Classification with Self-Supervised Target Relation Regularization
- **Arxiv ID**: http://arxiv.org/abs/2208.01997v1
- **DOI**: 10.1109/TIP.2022.3197931
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.01997v1)
- **Published**: 2022-08-03 11:51:53+00:00
- **Updated**: 2022-08-03 11:51:53+00:00
- **Authors**: Kangjun Liu, Ke Chen, Kui Jia
- **Comment**: Accepted by TIP, 15 pages, 8 figures, 11 tables
- **Journal**: IEEE Transactions on Image Processing 2022
- **Summary**: Fine-grained visual classification can be addressed by deep representation learning under supervision of manually pre-defined targets (e.g., one-hot or the Hadamard codes). Such target coding schemes are less flexible to model inter-class correlation and are sensitive to sparse and imbalanced data distribution as well. In light of this, this paper introduces a novel target coding scheme -- dynamic target relation graphs (DTRG), which, as an auxiliary feature regularization, is a self-generated structural output to be mapped from input images. Specifically, online computation of class-level feature centers is designed to generate cross-category distance in the representation space, which can thus be depicted by a dynamic graph in a non-parametric manner. Explicitly minimizing intra-class feature variations anchored on those class-level centers can encourage learning of discriminative features. Moreover, owing to exploiting inter-class dependency, the proposed target graphs can alleviate data sparsity and imbalanceness in representation learning. Inspired by recent success of the mixup style data augmentation, this paper introduces randomness into soft construction of dynamic target relation graphs to further explore relation diversity of target classes. Experimental results can demonstrate the effectiveness of our method on a number of diverse benchmarks of multiple visual classification tasks, especially achieving the state-of-the-art performance on popular fine-grained object benchmarks and superior robustness against sparse and imbalanced data. Source codes are made publicly available at https://github.com/AkonLau/DTRG.



### Gradient-based Uncertainty for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.02005v1
- **DOI**: 10.1007/978-3-031-20044-1_35
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02005v1)
- **Published**: 2022-08-03 12:21:02+00:00
- **Updated**: 2022-08-03 12:21:02+00:00
- **Authors**: Julia Hornauer, Vasileios Belagiannis
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: In monocular depth estimation, disturbances in the image context, like moving objects or reflecting materials, can easily lead to erroneous predictions. For that reason, uncertainty estimates for each pixel are necessary, in particular for safety-critical applications such as automated driving. We propose a post hoc uncertainty estimation approach for an already trained and thus fixed depth estimation model, represented by a deep neural network. The uncertainty is estimated with the gradients which are extracted with an auxiliary loss function. To avoid relying on ground-truth information for the loss definition, we present an auxiliary loss function based on the correspondence of the depth prediction for an image and its horizontally flipped counterpart. Our approach achieves state-of-the-art uncertainty estimation results on the KITTI and NYU Depth V2 benchmarks without the need to retrain the neural network. Models and code are publicly available at https://github.com/jhornauer/GrUMoDepth.



### Maintaining Performance with Less Data
- **Arxiv ID**: http://arxiv.org/abs/2208.02007v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02007v1)
- **Published**: 2022-08-03 12:22:18+00:00
- **Updated**: 2022-08-03 12:22:18+00:00
- **Authors**: Dominic Sanderson, Tatiana Kalgonova
- **Comment**: 12 pages, 8 figures, 11 tables
- **Journal**: None
- **Summary**: We propose a novel method for training a neural network for image classification to reduce input data dynamically, in order to reduce the costs of training a neural network model. As Deep Learning tasks become more popular, their computational complexity increases, leading to more intricate algorithms and models which have longer runtimes and require more input data. The result is a greater cost on time, hardware, and environmental resources. By using data reduction techniques, we reduce the amount of work performed, and therefore the environmental impact of AI techniques, and with dynamic data reduction we show that accuracy may be maintained while reducing runtime by up to 50%, and reducing carbon emission proportionally.



### Vision-Based Safety System for Barrierless Human-Robot Collaboration
- **Arxiv ID**: http://arxiv.org/abs/2208.02010v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2208.02010v1)
- **Published**: 2022-08-03 12:31:03+00:00
- **Updated**: 2022-08-03 12:31:03+00:00
- **Authors**: Lina Mara Amaya-Meja, Nicols Duque-Surez, Daniel Jaramillo-Ramrez, Carol Martinez
- **Comment**: Accepted for publication at the 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Human safety has always been the main priority when working near an industrial robot. With the rise of Human-Robot Collaborative environments, physical barriers to avoiding collisions have been disappearing, increasing the risk of accidents and the need for solutions that ensure a safe Human-Robot Collaboration. This paper proposes a safety system that implements Speed and Separation Monitoring (SSM) type of operation. For this, safety zones are defined in the robot's workspace following current standards for industrial collaborative robots. A deep learning-based computer vision system detects, tracks, and estimates the 3D position of operators close to the robot. The robot control system receives the operator's 3D position and generates 3D representations of them in a simulation environment. Depending on the zone where the closest operator was detected, the robot stops or changes its operating speed. Three different operation modes in which the human and robot interact are presented. Results show that the vision-based system can correctly detect and classify in which safety zone an operator is located and that the different proposed operation modes ensure that the robot's reaction and stop time are within the required time limits to guarantee safety.



### Character Generation through Self-Supervised Vectorization
- **Arxiv ID**: http://arxiv.org/abs/2208.02012v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02012v1)
- **Published**: 2022-08-03 12:31:55+00:00
- **Updated**: 2022-08-03 12:31:55+00:00
- **Authors**: Gokcen Gokceoglu, Emre Akbas
- **Comment**: None
- **Journal**: None
- **Summary**: The prevalent approach in self-supervised image generation is to operate on pixel level representations. While this approach can produce high quality images, it cannot benefit from the simplicity and innate quality of vectorization. Here we present a drawing agent that operates on stroke-level representation of images. At each time step, the agent first assesses the current canvas and decides whether to stop or keep drawing. When a 'draw' decision is made, the agent outputs a program indicating the stroke to be drawn. As a result, it produces a final raster image by drawing the strokes on a canvas, using a minimal number of strokes and dynamically deciding when to stop. We train our agent through reinforcement learning on MNIST and Omniglot datasets for unconditional generation and parsing (reconstruction) tasks. We utilize our parsing agent for exemplar generation and type conditioned concept generation in Omniglot challenge without any further training. We present successful results on all three generation tasks and the parsing task. Crucially, we do not need any stroke-level or vector supervision; we only use raster images for training.



### YOLO-FaceV2: A Scale and Occlusion Aware Face Detector
- **Arxiv ID**: http://arxiv.org/abs/2208.02019v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02019v2)
- **Published**: 2022-08-03 12:40:00+00:00
- **Updated**: 2022-08-04 16:29:08+00:00
- **Authors**: Ziping Yu, Hongbo Huang, Weijun Chen, Yongxin Su, Yahui Liu, Xiuying Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, face detection algorithms based on deep learning have made great progress. These algorithms can be generally divided into two categories, i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO. Because of the better balance between accuracy and speed, one-stage detectors have been widely used in many applications. In this paper, we propose a real-time face detector based on the one-stage detector YOLOv5, named YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to enhance receptive field of small face, and use NWD Loss to make up for the sensitivity of IoU to the location deviation of tiny objects. For face occlusion, we present an attention module named SEAM and introduce Repulsion Loss to solve it. Moreover, we use a weight function Slide to solve the imbalance between easy and hard samples and use the information of the effective receptive field to design the anchor. The experimental results on WiderFace dataset show that our face detector outperforms YOLO and its variants can be find in all easy, medium and hard subsets. Source code in https://github.com/Krasjet-Yu/YOLO-FaceV2



### SSformer: A Lightweight Transformer for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.02034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02034v1)
- **Published**: 2022-08-03 12:57:00+00:00
- **Updated**: 2022-08-03 12:57:00+00:00
- **Authors**: Wentao Shi, Jing Xu, Pan Gao
- **Comment**: None
- **Journal**: None
- **Summary**: It is well believed that Transformer performs better in semantic segmentation compared to convolutional neural networks. Nevertheless, the original Vision Transformer may lack of inductive biases of local neighborhoods and possess a high time complexity. Recently, Swin Transformer sets a new record in various vision tasks by using hierarchical architecture and shifted windows while being more efficient. However, as Swin Transformer is specifically designed for image classification, it may achieve suboptimal performance on dense prediction-based segmentation task. Further, simply combing Swin Transformer with existing methods would lead to the boost of model size and parameters for the final segmentation model. In this paper, we rethink the Swin Transformer for semantic segmentation, and design a lightweight yet effective transformer model, called SSformer. In this model, considering the inherent hierarchical design of Swin Transformer, we propose a decoder to aggregate information from different layers, thus obtaining both local and global attentions. Experimental results show the proposed SSformer yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and lower compute.



### Template matching with white balance adjustment under multiple illuminants
- **Arxiv ID**: http://arxiv.org/abs/2208.02035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02035v1)
- **Published**: 2022-08-03 12:57:18+00:00
- **Updated**: 2022-08-03 12:57:18+00:00
- **Authors**: Teruaki Akazawa, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: \c{opyright} 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: In this paper, we propose a novel template matching method with a white balancing adjustment, called N-white balancing, which was proposed for multi-illuminant scenes. To reduce the influence of lighting effects, N-white balancing is applied to images for multi-illumination color constancy, and then a template matching method is carried out by using adjusted images. In experiments, the effectiveness of the proposed method is demonstrated to be effective in object detection tasks under various illumination conditions.



### AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy
- **Arxiv ID**: http://arxiv.org/abs/2208.02049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02049v1)
- **Published**: 2022-08-03 13:17:23+00:00
- **Updated**: 2022-08-03 13:17:23+00:00
- **Authors**: Ziyi Wang, Bo Lu, Yonghao Long, Fangxun Zhong, Tak-Hong Cheung, Qi Dou, Yunhui Liu
- **Comment**: Accepted at MICCAI 2022
- **Journal**: None
- **Summary**: Computer-assisted minimally invasive surgery has great potential in benefiting modern operating theatres. The video data streamed from the endoscope provides rich information to support context-awareness for next-generation intelligent surgical systems. To achieve accurate perception and automatic manipulation during the procedure, learning based technique is a promising way, which enables advanced image analysis and scene understanding in recent years. However, learning such models highly relies on large-scale, high-quality, and multi-task labelled data. This is currently a bottleneck for the topic, as available public dataset is still extremely limited in the field of CAI. In this paper, we present and release the first integrated dataset (named AutoLaparo) with multiple image-based perception tasks to facilitate learning-based automation in hysterectomy surgery. Our AutoLaparo dataset is developed based on full-length videos of entire hysterectomy procedures. Specifically, three different yet highly correlated tasks are formulated in the dataset, including surgical workflow recognition, laparoscope motion prediction, and instrument and key anatomy segmentation. In addition, we provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset. The dataset is available at https://autolaparo.github.io.



### AstroVision: Towards Autonomous Feature Detection and Description for Missions to Small Bodies Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.02053v1
- **DOI**: 10.1016/j.actaastro.2023.01.009
- **Categories**: **astro-ph.IM**, astro-ph.EP, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.02053v1)
- **Published**: 2022-08-03 13:18:44+00:00
- **Updated**: 2022-08-03 13:18:44+00:00
- **Authors**: Travis Driver, Katherine Skinner, Mehregan Dor, Panagiotis Tsiotras
- **Comment**: None
- **Journal**: None
- **Summary**: Missions to small celestial bodies rely heavily on optical feature tracking for characterization of and relative navigation around the target body. While deep learning has led to great advancements in feature detection and description, training and validating data-driven models for space applications is challenging due to the limited availability of large-scale, annotated datasets. This paper introduces AstroVision, a large-scale dataset comprised of 115,970 densely annotated, real images of 16 different small bodies captured during past and ongoing missions. We leverage AstroVision to develop a set of standardized benchmarks and conduct an exhaustive evaluation of both handcrafted and data-driven feature detection and description methods. Next, we employ AstroVision for end-to-end training of a state-of-the-art, deep feature detection and description network and demonstrate improved performance on multiple benchmarks. The full benchmarking pipeline and the dataset will be made publicly available to facilitate the advancement of computer vision algorithms for space applications.



### Evaluation and comparison of eight popular Lidar and Visual SLAM algorithms
- **Arxiv ID**: http://arxiv.org/abs/2208.02063v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02063v1)
- **Published**: 2022-08-03 13:31:09+00:00
- **Updated**: 2022-08-03 13:31:09+00:00
- **Authors**: Bharath Garigipati, Nataliya Strokina, Reza Ghabcheloo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we evaluate eight popular and open-source 3D Lidar and visual SLAM (Simultaneous Localization and Mapping) algorithms, namely LOAM, Lego LOAM, LIO SAM, HDL Graph, ORB SLAM3, Basalt VIO, and SVO2. We have devised experiments both indoor and outdoor to investigate the effect of the following items: i) effect of mounting positions of the sensors, ii) effect of terrain type and vibration, iii) effect of motion (variation in linear and angular speed). We compare their performance in terms of relative and absolute pose error. We also provide comparison on their required computational resources. We thoroughly analyse and discuss the results and identify the best performing system for the environment cases with our multi-camera and multi-Lidar indoor and outdoor datasets. We hope our findings help one to choose a sensor and the corresponding SLAM algorithm combination suiting their needs, based on their target environment.



### A Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2208.02080v1
- **DOI**: 10.1145/3503161.3548365
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02080v1)
- **Published**: 2022-08-03 14:05:20+00:00
- **Updated**: 2022-08-03 14:05:20+00:00
- **Authors**: Alex Falcon, Giuseppe Serra, Oswald Lanz
- **Comment**: Accepted for presentation at 30th ACM International Conference on
  Multimedia (ACM MM)
- **Journal**: None
- **Summary**: Every hour, huge amounts of visual contents are posted on social media and user-generated content platforms. To find relevant videos by means of a natural language query, text-video retrieval methods have received increased attention over the past few years. Data augmentation techniques were introduced to increase the performance on unseen test examples by creating new training samples with the application of semantics-preserving techniques, such as color space or geometric transformations on images. Yet, these techniques are usually applied on raw data, leading to more resource-demanding solutions and also requiring the shareability of the raw data, which may not always be true, e.g. copyright issues with clips from movies or TV series. To address this shortcoming, we propose a multimodal data augmentation technique which works in the feature space and creates new videos and captions by mixing semantically similar samples. We experiment our solution on a large scale public dataset, EPIC-Kitchens-100, and achieve considerable improvements over a baseline method, improved state-of-the-art performance, while at the same time performing multiple ablation studies. We release code and pretrained models on Github at https://github.com/aranciokov/FSMMDA_VideoRetrieval.



### Unsupervised Discovery of Semantic Concepts in Satellite Imagery with Style-based Wavelet-driven Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2208.02089v1
- **DOI**: 10.1145/3549737.3549777
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02089v1)
- **Published**: 2022-08-03 14:19:24+00:00
- **Updated**: 2022-08-03 14:19:24+00:00
- **Authors**: Nikos Kostagiolas, Mihalis A. Nicolaou, Yannis Panagakis
- **Comment**: 11 pages, 5 figures, accepted at SETN 2022
- **Journal**: None
- **Summary**: In recent years, considerable advancements have been made in the area of Generative Adversarial Networks (GANs), particularly with the advent of style-based architectures that address many key shortcomings - both in terms of modeling capabilities and network interpretability. Despite these improvements, the adoption of such approaches in the domain of satellite imagery is not straightforward. Typical vision datasets used in generative tasks are well-aligned and annotated, and exhibit limited variability. In contrast, satellite imagery exhibits great spatial and spectral variability, wide presence of fine, high-frequency details, while the tedious nature of annotating satellite imagery leads to annotation scarcity - further motivating developments in unsupervised learning. In this light, we present the first pre-trained style- and wavelet-based GAN model that can readily synthesize a wide gamut of realistic satellite images in a variety of settings and conditions - while also preserving high-frequency information. Furthermore, we show that by analyzing the intermediate activations of our network, one can discover a multitude of interpretable semantic directions that facilitate the guided synthesis of satellite images in terms of high-level concepts (e.g., urbanization) without using any form of supervision. Via a set of qualitative and quantitative experiments we demonstrate the efficacy of our framework, in terms of suitability for downstream tasks (e.g., data augmentation), quality of synthetic imagery, as well as generalization capabilities to unseen datasets.



### Edge-Based Self-Supervision for Semi-Supervised Few-Shot Microscopy Image Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.02105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02105v1)
- **Published**: 2022-08-03 14:35:00+00:00
- **Updated**: 2022-08-03 14:35:00+00:00
- **Authors**: Youssef Dawoud, Katharina Ernst, Gustavo Carneiro, Vasileios Belagiannis
- **Comment**: Accepted by MOVI 2022
- **Journal**: None
- **Summary**: Deep neural networks currently deliver promising results for microscopy image cell segmentation, but they require large-scale labelled databases, which is a costly and time-consuming process. In this work, we relax the labelling requirement by combining self-supervised with semi-supervised learning. We propose the prediction of edge-based maps for self-supervising the training of the unlabelled images, which is combined with the supervised training of a small number of labelled images for learning the segmentation task. In our experiments, we evaluate on a few-shot microscopy image cell segmentation benchmark and show that only a small number of annotated images, e.g. 10% of the original training set, is enough for our approach to reach similar performance as with the fully annotated databases on 1- to 10-shots. Our code and trained models is made publicly available



### Pedestrian-Robot Interactions on Autonomous Crowd Navigation: Reactive Control Methods and Evaluation Metrics
- **Arxiv ID**: http://arxiv.org/abs/2208.02121v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2208.02121v1)
- **Published**: 2022-08-03 14:56:03+00:00
- **Updated**: 2022-08-03 14:56:03+00:00
- **Authors**: Diego Paez-Granados, Yujie He, David Gonon, Dan Jia, Bastian Leibe, Kenji Suzuki, Aude Billard
- **Comment**: \c{opyright}IEEE All rights reserved. IEEE-IROS-2022, Oct.23-27.
  Kyoto, Japan
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS-2022)
- **Summary**: Autonomous navigation in highly populated areas remains a challenging task for robots because of the difficulty in guaranteeing safe interactions with pedestrians in unstructured situations. In this work, we present a crowd navigation control framework that delivers continuous obstacle avoidance and post-contact control evaluated on an autonomous personal mobility vehicle. We propose evaluation metrics for accounting efficiency, controller response and crowd interactions in natural crowds. We report the results of over 110 trials in different crowd types: sparse, flows, and mixed traffic, with low- (< 0.15 ppsm), mid- (< 0.65 ppsm), and high- (< 1 ppsm) pedestrian densities. We present comparative results between two low-level obstacle avoidance methods and a baseline of shared control. Results show a 10% drop in relative time to goal on the highest density tests, and no other efficiency metric decrease. Moreover, autonomous navigation showed to be comparable to shared-control navigation with a lower relative jerk and significantly higher fluency in commands indicating high compatibility with the crowd. We conclude that the reactive controller fulfils a necessary task of fast and continuous adaptation to crowd navigation, and it should be coupled with high-level planners for environmental and situational awareness.



### LSSANet: A Long Short Slice-Aware Network for Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.02122v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02122v1)
- **Published**: 2022-08-03 14:57:42+00:00
- **Updated**: 2022-08-03 14:57:42+00:00
- **Authors**: Rui Xu, Yong Luo, Bo Du, Kaiming Kuang, Jiancheng Yang
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been demonstrated to be highly effective in the field of pulmonary nodule detection. However, existing CNN based pulmonary nodule detection methods lack the ability to capture long-range dependencies, which is vital for global information extraction. In computer vision tasks, non-local operations have been widely utilized, but the computational cost could be very high for 3D computed tomography (CT) images. To address this issue, we propose a long short slice-aware network (LSSANet) for the detection of pulmonary nodules. In particular, we develop a new non-local mechanism termed long short slice grouping (LSSG), which splits the compact non-local embeddings into a short-distance slice grouped one and a long-distance slice grouped counterpart. This not only reduces the computational burden, but also keeps long-range dependencies among any elements across slices and in the whole feature map. The proposed LSSG is easy-to-use and can be plugged into many pulmonary nodule detection networks. To verify the performance of LSSANet, we compare with several recently proposed and competitive detection approaches based on 2D/3D CNN. Promising evaluation results on the large-scale PN9 dataset demonstrate the effectiveness of our method. Code is at https://github.com/Ruixxxx/LSSANet.



### SC6D: Symmetry-agnostic and Correspondence-free 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.02129v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02129v3)
- **Published**: 2022-08-03 15:08:27+00:00
- **Updated**: 2022-09-18 07:24:50+00:00
- **Authors**: Dingding Cai, Janne Heikkil, Esa Rahtu
- **Comment**: 3DV 2022
- **Journal**: None
- **Summary**: This paper presents an efficient symmetry-agnostic and correspondence-free framework, referred to as SC6D, for 6D object pose estimation from a single monocular RGB image. SC6D requires neither the 3D CAD model of the object nor any prior knowledge of the symmetries. The pose estimation is decomposed into three sub-tasks: a) object 3D rotation representation learning and matching; b) estimation of the 2D location of the object center; and c) scale-invariant distance estimation (the translation along the z-axis) via classification. SC6D is evaluated on three benchmark datasets, T-LESS, YCB-V, and ITODD, and results in state-of-the-art performance on the T-LESS dataset. Moreover, SC6D is computationally much more efficient than the previous state-of-the-art method SurfEmb. The implementation and pre-trained models are publicly available at https://github.com/dingdingcai/SC6D-pose.



### Masked Vision and Language Modeling for Multi-modal Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.02131v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02131v2)
- **Published**: 2022-08-03 15:11:01+00:00
- **Updated**: 2023-03-14 23:51:53+00:00
- **Authors**: Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, Stefano Soatto
- **Comment**: International Conference on Learning Representations (ICLR) 2023
- **Journal**: None
- **Summary**: In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, achieves state-of-the-art performance in the regime of millions of pre-training data. Also, we outperforms the other competitors by a significant margin in limited data scenarios.



### Subject-Specific Lesion Generation and Pseudo-Healthy Synthesis for Multiple Sclerosis Brain Images
- **Arxiv ID**: http://arxiv.org/abs/2208.02135v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02135v1)
- **Published**: 2022-08-03 15:12:55+00:00
- **Updated**: 2022-08-03 15:12:55+00:00
- **Authors**: Berke Doga Basaran, Mengyun Qiao, Paul M. Matthews, Wenjia Bai
- **Comment**: 13 pages, 6 figures, 2022 MICCAI SASHIMI (Simulation and Synthesis in
  Medical Imaging) Workshop paper
- **Journal**: None
- **Summary**: Understanding the intensity characteristics of brain lesions is key for defining image-based biomarkers in neurological studies and for predicting disease burden and outcome. In this work, we present a novel foreground-based generative method for modelling the local lesion characteristics that can both generate synthetic lesions on healthy images and synthesize subject-specific pseudo-healthy images from pathological images. Furthermore, the proposed method can be used as a data augmentation module to generate synthetic images for training brain image segmentation networks. Experiments on multiple sclerosis (MS) brain images acquired on magnetic resonance imaging (MRI) demonstrate that the proposed method can generate highly realistic pseudo-healthy and pseudo-pathological brain images. Data augmentation using the synthetic images improves the brain image segmentation performance compared to traditional data augmentation methods as well as a recent lesion-aware data augmentation technique, CarveMix. The code will be released at https://github.com/dogabasaran/lesion-synthesis.



### GPPF: A General Perception Pre-training Framework via Sparsely Activated Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.02148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02148v2)
- **Published**: 2022-08-03 15:34:35+00:00
- **Updated**: 2022-08-04 04:39:23+00:00
- **Authors**: Benyuan Sun, Jin Dai, Zihao Liang, Congying Liu, Yi Yang, Bo Bai
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: Pre-training over mixtured multi-task, multi-domain, and multi-modal data remains an open challenge in vision perception pre-training. In this paper, we propose GPPF, a General Perception Pre-training Framework, that pre-trains a task-level dynamic network, which is composed by knowledge "legos" in each layers, on labeled multi-task and multi-domain datasets. By inspecting humans' innate ability to learn in complex environment, we recognize and transfer three critical elements to deep networks: (1) simultaneous exposure to diverse cross-task and cross-domain information in each batch. (2) partitioned knowledge storage in separate lego units driven by knowledge sharing. (3) sparse activation of a subset of lego units for both pre-training and downstream tasks. Noteworthy, the joint training of disparate vision tasks is non-trivial due to their differences in input shapes, loss functions, output formats, data distributions, etc. Therefore, we innovatively develop a plug-and-play multi-task training algorithm, which supports Single Iteration Multiple Tasks (SIMT) concurrently training. SIMT lays the foundation of pre-training with large-scale multi-task multi-domain datasets and is proved essential for stable training in our GPPF experiments. Excitingly, the exhaustive experiments show that, our GPPF-R50 model achieves significant improvements of 2.5-5.8 over a strong baseline of the 8 pre-training tasks in GPPF-15M and harvests a range of SOTAs over the 22 downstream tasks with similar computation budgets. We also validate the generalization ability of GPPF to SOTA vision transformers with consistent improvements. These solid experimental results fully prove the effective knowledge learning, storing, sharing, and transfer provided by our novel GPPF framework.



### KD-SCFNet: Towards More Accurate and Efficient Salient Object Detection via Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2208.02178v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02178v6)
- **Published**: 2022-08-03 16:03:11+00:00
- **Updated**: 2022-11-21 17:13:31+00:00
- **Authors**: Jin Zhang, Qiuwei Liang, Yanjiao Shi
- **Comment**: There are some important mistakes in the article that need to be
  modified
- **Journal**: None
- **Summary**: Most existing salient object detection (SOD) models are difficult to apply due to the complex and huge model structures. Although some lightweight models are proposed, the accuracy is barely satisfactory. In this paper, we design a novel semantics-guided contextual fusion network (SCFNet) that focuses on the interactive fusion of multi-level features for accurate and efficient salient object detection. Furthermore, we apply knowledge distillation to SOD task and provide a sizeable dataset KD-SOD80K. In detail, we transfer the rich knowledge from a seasoned teacher to the untrained SCFNet through unlabeled images, enabling SCFNet to learn a strong generalization ability to detect salient objects more accurately. The knowledge distillation based SCFNet (KDSCFNet) achieves comparable accuracy to the state-of-the-art heavyweight methods with less than 1M parameters and 174 FPS real-time detection speed. Extensive experiments demonstrate the robustness and effectiveness of the proposed distillation method and SOD framework. Code and data: https://github.com/zhangjinCV/KD-SCFNet.



### RealPatch: A Statistical Matching Framework for Model Patching with Real Samples
- **Arxiv ID**: http://arxiv.org/abs/2208.02192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02192v1)
- **Published**: 2022-08-03 16:22:30+00:00
- **Updated**: 2022-08-03 16:22:30+00:00
- **Authors**: Sara Romiti, Christopher Inskip, Viktoriia Sharmanska, Novi Quadrianto
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning classifiers are typically trained to minimise the average error across a dataset. Unfortunately, in practice, this process often exploits spurious correlations caused by subgroup imbalance within the training data, resulting in high average performance but highly variable performance across subgroups. Recent work to address this problem proposes model patching with CAMEL. This previous approach uses generative adversarial networks to perform intra-class inter-subgroup data augmentations, requiring (a) the training of a number of computationally expensive models and (b) sufficient quality of model's synthetic outputs for the given domain. In this work, we propose RealPatch, a framework for simpler, faster, and more data-efficient data augmentation based on statistical matching. Our framework performs model patching by augmenting a dataset with real samples, mitigating the need to train generative models for the target task. We demonstrate the effectiveness of RealPatch on three benchmark datasets, CelebA, Waterbirds and a subset of iWildCam, showing improvements in worst-case subgroup performance and in subgroup performance gap in binary classification. Furthermore, we conduct experiments with the imSitu dataset with 211 classes, a setting where generative model-based patching such as CAMEL is impractical. We show that RealPatch can successfully eliminate dataset leakage while reducing model leakage and maintaining high utility. The code for RealPatch can be found at https://github.com/wearepal/RealPatch.



### Large-scale Building Damage Assessment using a Novel Hierarchical Transformer Architecture on Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2208.02205v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02205v3)
- **Published**: 2022-08-03 16:41:39+00:00
- **Updated**: 2023-02-04 20:48:21+00:00
- **Authors**: Navjot Kaur, Cheng-Chun Lee, Ali Mostafavi, Ali Mahdavi-Amiri
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents \dahitra, a novel deep-learning model with hierarchical transformers to classify building damages based on satellite images in the aftermath of natural disasters. Satellite imagery provides real-time and high-coverage information and offers opportunities to inform large-scale post-disaster building damage assessment, which is critical for rapid emergency response. In this work, a novel transformer-based network is proposed for assessing building damage. This network leverages hierarchical spatial features of multiple resolutions and captures the temporal differences in the feature domain after applying a transformer encoder on the spatial features. The proposed network achieves state-of-the-art performance when tested on a large-scale disaster damage dataset (xBD) for building localization and damage classification, as well as on LEVIR-CD dataset for change detection tasks. In addition, this work introduces a new high-resolution satellite imagery dataset, Ida-BD (related to 2021 Hurricane Ida in Louisiana in 2021) for domain adaptation. Further, it demonstrates an approach of using this dataset by adapting the model with limited fine-tuning and hence applying the model to newly damaged areas with scarce data.



### Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control
- **Arxiv ID**: http://arxiv.org/abs/2208.02210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02210v1)
- **Published**: 2022-08-03 16:46:08+00:00
- **Updated**: 2022-08-03 16:46:08+00:00
- **Authors**: Michail Christos Doukas, Evangelos Ververas, Viktoriia Sharmanska, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: We present Free-HeadGAN, a person-generic neural talking head synthesis system. We show that modeling faces with sparse 3D facial landmarks are sufficient for achieving state-of-the-art generative performance, without relying on strong statistical priors of the face, such as 3D Morphable Models. Apart from 3D pose and facial expressions, our method is capable of fully transferring the eye gaze, from a driving actor to a source identity. Our complete pipeline consists of three components: a canonical 3D key-point estimator that regresses 3D pose and expression-related deformations, a gaze estimation network and a generator that is built upon the architecture of HeadGAN. We further experiment with an extension of our generator to accommodate few-shot learning using an attention mechanism, in case more than one source images are available. Compared to the latest models for reenactment and motion transfer, our system achieves higher photo-realism combined with superior identity preservation, while offering explicit gaze control.



### The Importance of the Instantaneous Phase in Detecting Faces with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.01638v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.01638v1)
- **Published**: 2022-08-03 17:10:54+00:00
- **Updated**: 2022-08-03 17:10:54+00:00
- **Authors**: Luis Sanchez Tapia
- **Comment**: Master Thesis
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have provided new and accurate methods for processing digital images and videos. Yet, training CNNs is extremely demanding in terms of computational resources. Also, for specific applications, the standard use of transfer learning also tends to require far more resources than what may be needed. Furthermore, the final systems tend to operate as black boxes that are difficult to interpret. The current thesis considers the problem of detecting faces from the AOLME video dataset. The AOLME dataset consists of a large video collection of group interactions that are recorded in unconstrained classroom environments. For the thesis, still image frames were extracted at every minute from 18 24-minute videos. Then, each video frame was divided into 9x5 blocks with 50x50 pixels each. For each of the 19440 blocks, the percentage of face pixels was set as ground truth. Face detection was then defined as a regression problem for determining the face pixel percentage for each block. For testing different methods, 12 videos were used for training and validation. The remaining 6 videos were used for testing. The thesis examines the impact of using the instantaneous phase for the AOLME block-based face detection application. For comparison, the thesis compares the use of the Frequency Modulation image based on the instantaneous phase, the use of the instantaneous amplitude, and the original gray scale image. To generate the FM and AM inputs, the thesis uses dominant component analysis that aims to decrease the training overhead while maintaining interpretability.



### MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training
- **Arxiv ID**: http://arxiv.org/abs/2208.02245v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.02245v1)
- **Published**: 2022-08-03 17:50:42+00:00
- **Updated**: 2022-08-03 17:50:42+00:00
- **Authors**: De-An Huang, Zhiding Yu, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose MinVIS, a minimal video instance segmentation (VIS) framework that achieves state-of-the-art VIS performance with neither video-based architectures nor training procedures. By only training a query-based image instance segmentation model, MinVIS outperforms the previous best result on the challenging Occluded VIS dataset by over 10% AP. Since MinVIS treats frames in training videos as independent images, we can drastically sub-sample the annotated frames in training videos without any modifications. With only 1% of labeled frames, MinVIS outperforms or is comparable to fully-supervised state-of-the-art approaches on YouTube-VIS 2019/2021. Our key observation is that queries trained to be discriminative between intra-frame object instances are temporally consistent and can be used to track instances without any manually designed heuristics. MinVIS thus has the following inference pipeline: we first apply the trained query-based image instance segmentation to video frames independently. The segmented instances are then tracked by bipartite matching of the corresponding queries. This inference is done in an online fashion and does not need to process the whole video at once. MinVIS thus has the practical advantages of reducing both the labeling costs and the memory requirements, while not sacrificing the VIS performance. Code is available at: https://github.com/NVlabs/MinVIS



### Content-Based Landmark Retrieval Combining Global and Local Features using Siamese Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.04201v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.04201v1)
- **Published**: 2022-08-03 18:11:36+00:00
- **Updated**: 2022-08-03 18:11:36+00:00
- **Authors**: Tianyi Hu, Monika Kwiatkowski, Simon Matern, Olaf Hellwich
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a method for landmark retrieval that utilizes global and local features. A Siamese network is used for global feature extraction and metric learning, which gives an initial ranking of the landmark search. We utilize the extracted feature maps from the Siamese architecture as local descriptors, the search results are then further refined using a cosine similarity between local descriptors. We conduct a deeper analysis of the Google Landmark Dataset, which is used for evaluation, and augment the dataset to handle various intra-class variances. Furthermore, we conduct several experiments to compare the effects of transfer learning and metric learning, as well as experiments using other local descriptors. We show that a re-ranking using local features can improve the search results. We believe that the proposed local feature extraction using cosine similarity is a simple approach that can be extended to many other retrieval tasks.



### Unsupervised Flow Refinement near Motion Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2208.02305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.02305v1)
- **Published**: 2022-08-03 18:44:39+00:00
- **Updated**: 2022-08-03 18:44:39+00:00
- **Authors**: Shuzhi Yu, Hannah Halin Kim, Shuai Yuan, Carlo Tomasi
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised optical flow estimators based on deep learning have attracted increasing attention due to the cost and difficulty of annotating for ground truth. Although performance measured by average End-Point Error (EPE) has improved over the years, flow estimates are still poorer along motion boundaries (MBs), where the flow is not smooth, as is typically assumed, and where features computed by neural networks are contaminated by multiple motions. To improve flow in the unsupervised settings, we design a framework that detects MBs by analyzing visual changes along boundary candidates and replaces motions close to detections with motions farther away. Our proposed algorithm detects boundaries more accurately than a baseline method with the same inputs and can improve estimates from any flow predictor without additional training.



### Counterfactual Image Synthesis for Discovery of Personalized Predictive Image Markers
- **Arxiv ID**: http://arxiv.org/abs/2208.02311v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02311v1)
- **Published**: 2022-08-03 18:58:45+00:00
- **Updated**: 2022-08-03 18:58:45+00:00
- **Authors**: Amar Kumar, Anjun Hu, Brennan Nichyporuk, Jean-Pierre R. Falet, Douglas L. Arnold, Sotirios Tsaftaris, Tal Arbel
- **Comment**: Accepted to the MIABID workshop at MICCAI 2022
- **Journal**: None
- **Summary**: The discovery of patient-specific imaging markers that are predictive of future disease outcomes can help us better understand individual-level heterogeneity of disease evolution. In fact, deep learning models that can provide data-driven personalized markers are much more likely to be adopted in medical practice. In this work, we demonstrate that data-driven biomarker discovery can be achieved through a counterfactual synthesis process. We show how a deep conditional generative model can be used to perturb local imaging features in baseline images that are pertinent to subject-specific future disease evolution and result in a counterfactual image that is expected to have a different future outcome. Candidate biomarkers, therefore, result from examining the set of features that are perturbed in this process. Through several experiments on a large-scale, multi-scanner, multi-center multiple sclerosis (MS) clinical trial magnetic resonance imaging (MRI) dataset of relapsing-remitting (RRMS) patients, we demonstrate that our model produces counterfactuals with changes in imaging features that reflect established clinical markers predictive of future MRI lesional activity at the population level. Additional qualitative results illustrate that our model has the potential to discover novel and subject-specific predictive markers of future activity.



### Image-based Detection of Surface Defects in Concrete during Construction
- **Arxiv ID**: http://arxiv.org/abs/2208.02313v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02313v2)
- **Published**: 2022-08-03 19:05:12+00:00
- **Updated**: 2022-12-06 15:19:33+00:00
- **Authors**: Dominik Kuhnke, Monika Kwiatkowski, Olaf Hellwich
- **Comment**: None
- **Journal**: None
- **Summary**: Defects increase the cost and duration of construction projects as they require significant inspection and documentation efforts. Automating defect detection could significantly reduce these efforts. This work focuses on detecting honeycombs, a substantial defect in concrete structures that may affect structural integrity. We compared honeycomb images scraped from the web with images obtained from real construction inspections. We found that web images do not capture the complete variance found in real-case scenarios and that there is still a lack of data in this domain. Our dataset is therefore freely available for further research. A Mask R-CNN and EfficientNet-B0 were trained for honeycomb detection. The Mask R-CNN model allows detecting honeycombs based on instance segmentation, whereas the EfficientNet-B0 model allows a patch-based classification. Our experiments demonstrate that both approaches are suitable for solving and automating honeycomb detection. In the future, this solution can be incorporated into defect documentation systems.



### Human Saliency-Driven Patch-based Matching for Interpretable Post-mortem Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.03138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.03138v1)
- **Published**: 2022-08-03 19:40:44+00:00
- **Updated**: 2022-08-03 19:40:44+00:00
- **Authors**: Aidan Boyd, Daniel Moreira, Andrey Kuehlkamp, Kevin Bowyer, Adam Czajka
- **Comment**: None
- **Journal**: None
- **Summary**: Forensic iris recognition, as opposed to live iris recognition, is an emerging research area that leverages the discriminative power of iris biometrics to aid human examiners in their efforts to identify deceased persons. As a machine learning-based technique in a predominantly human-controlled task, forensic recognition serves as "back-up" to human expertise in the task of post-mortem identification. As such, the machine learning model must be (a) interpretable, and (b) post-mortem-specific, to account for changes in decaying eye tissue. In this work, we propose a method that satisfies both requirements, and that approaches the creation of a post-mortem-specific feature extractor in a novel way employing human perception. We first train a deep learning-based feature detector on post-mortem iris images, using annotations of image regions highlighted by humans as salient for their decision making. In effect, the method learns interpretable features directly from humans, rather than purely data-driven features. Second, regional iris codes (again, with human-driven filtering kernels) are used to pair detected iris patches, which are translated into pairwise, patch-based comparison scores. In this way, our method presents human examiners with human-understandable visual cues in order to justify the identification decision and corresponding confidence score. When tested on a dataset of post-mortem iris images collected from 259 deceased subjects, the proposed method places among the three best iris matchers, demonstrating better results than the commercial (non-human-interpretable) VeriEye approach. We propose a unique post-mortem iris recognition method trained with human saliency to give fully-interpretable comparison outcomes for use in the context of forensic examination, achieving state-of-the-art recognition performance.



### Towards Generating Large Synthetic Phytoplankton Datasets for Efficient Monitoring of Harmful Algal Blooms
- **Arxiv ID**: http://arxiv.org/abs/2208.02332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02332v1)
- **Published**: 2022-08-03 20:15:55+00:00
- **Updated**: 2022-08-03 20:15:55+00:00
- **Authors**: Nitpreet Bamra, Vikram Voleti, Alexander Wong, Jason Deglint
- **Comment**: None
- **Journal**: None
- **Summary**: Climate change is increasing the frequency and severity of harmful algal blooms (HABs), which cause significant fish deaths in aquaculture farms. This contributes to ocean pollution and greenhouse gas (GHG) emissions since dead fish are either dumped into the ocean or taken to landfills, which in turn negatively impacts the climate. Currently, the standard method to enumerate harmful algae and other phytoplankton is to manually observe and count them under a microscope. This is a time-consuming, tedious and error-prone process, resulting in compromised management decisions by farmers. Hence, automating this process for quick and accurate HAB monitoring is extremely helpful. However, this requires large and diverse datasets of phytoplankton images, and such datasets are hard to produce quickly. In this work, we explore the feasibility of generating novel high-resolution photorealistic synthetic phytoplankton images, containing multiple species in the same image, given a small dataset of real images. To this end, we employ Generative Adversarial Networks (GANs) to generate synthetic images. We evaluate three different GAN architectures: ProjectedGAN, FastGAN, and StyleGANv2 using standard image quality metrics. We empirically show the generation of high-fidelity synthetic phytoplankton images using a training dataset of only 961 real images. Thus, this work demonstrates the ability of GANs to create large synthetic datasets of phytoplankton from small training datasets, accomplishing a key step towards sustainable systematic monitoring of harmful algal blooms.



### Estimating Visual Information From Audio Through Manifold Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.02337v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2208.02337v2)
- **Published**: 2022-08-03 20:47:11+00:00
- **Updated**: 2022-09-13 17:45:36+00:00
- **Authors**: Fabrizio Pedersoli, Dryden Wiebe, Amin Banitalebi, Yong Zhang, George Tzanetakis, Kwang Moo Yi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new framework for extracting visual information about a scene only using audio signals. Audio-based methods can overcome some of the limitations of vision-based methods i.e., they do not require "line-of-sight", are robust to occlusions and changes in illumination, and can function as a backup in case vision/lidar sensors fail. Therefore, audio-based methods can be useful even for applications in which only visual information is of interest Our framework is based on Manifold Learning and consists of two steps. First, we train a Vector-Quantized Variational Auto-Encoder to learn the data manifold of the particular visual modality we are interested in. Second, we train an Audio Transformation network to map multi-channel audio signals to the latent representation of the corresponding visual sample. We show that our method is able to produce meaningful images from audio using a publicly available audio/visual dataset. In particular, we consider the prediction of the following visual modalities from audio: depth and semantic segmentation. We hope the findings of our work can facilitate further research in visual information extraction from audio. Code is available at: https://github.com/ubc-vision/audio_manifold.



### Word-Level Fine-Grained Story Visualization
- **Arxiv ID**: http://arxiv.org/abs/2208.02341v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.02341v3)
- **Published**: 2022-08-03 21:01:47+00:00
- **Updated**: 2022-09-22 08:11:44+00:00
- **Authors**: Bowen Li, Thomas Lukasiewicz
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story with a global consistency across dynamic scenes and characters. Current works still struggle with output images' quality and consistency, and rely on additional semantic information or auxiliary captioning networks. To address these challenges, we first introduce a new sentence representation, which incorporates word information from all story sentences to mitigate the inconsistency problem. Then, we propose a new discriminator with fusion features and further extend the spatial attention to improve image quality and story consistency. Extensive experiments on different datasets and human evaluation demonstrate the superior performance of our approach, compared to state-of-the-art methods, neither using segmentation masks nor auxiliary captioning networks.



### Graph Neural Networks Extract High-Resolution Cultivated Land Maps from Sentinel-2 Image Series
- **Arxiv ID**: http://arxiv.org/abs/2208.02349v1
- **DOI**: 10.1109/LGRS.2022.3185407
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02349v1)
- **Published**: 2022-08-03 21:19:06+00:00
- **Updated**: 2022-08-03 21:19:06+00:00
- **Authors**: Lukasz Tulczyjew, Michal Kawulok, Nicolas Longp, Bertrand Le Saux, Jakub Nalepa
- **Comment**: 7 pages (including supplementary material), published in IEEE
  Geoscience and Remote Sensing Letters
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 1-5,
  2022, Art no. 5513105
- **Summary**: Maintaining farm sustainability through optimizing the agricultural management practices helps build more planet-friendly environment. The emerging satellite missions can acquire multi- and hyperspectral imagery which captures more detailed spectral information concerning the scanned area, hence allows us to benefit from subtle spectral features during the analysis process in agricultural applications. We introduce an approach for extracting 2.5 m cultivated land maps from 10 m Sentinel-2 multispectral image series which benefits from a compact graph convolutional neural network. The experiments indicate that our models not only outperform classical and deep machine learning techniques through delivering higher-quality segmentation maps, but also dramatically reduce the memory footprint when compared to U-Nets (almost 8k trainable parameters of our models, with up to 31M parameters of U-Nets). Such memory frugality is pivotal in the missions which allow us to uplink a model to the AI-powered satellite once it is in orbit, as sending large nets is impossible due to the time constraints.



### A Multibranch Convolutional Neural Network for Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2208.02361v1
- **DOI**: 10.1109/LGRS.2022.3185449
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02361v1)
- **Published**: 2022-08-03 21:59:03+00:00
- **Updated**: 2022-08-03 21:59:03+00:00
- **Authors**: Lukasz Tulczyjew, Michal Kawulok, Nicolas Longp, Bertrand Le Saux, Jakub Nalepa
- **Comment**: 14 pages (including supplementary material), published in IEEE
  Geoscience and Remote Sensing Letters
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 1-5,
  2022, Art no. 6011105
- **Summary**: Hyperspectral unmixing remains one of the most challenging tasks in the analysis of such data. Deep learning has been blooming in the field and proved to outperform other classic unmixing techniques, and can be effectively deployed onboard Earth observation satellites equipped with hyperspectral imagers. In this letter, we follow this research pathway and propose a multi-branch convolutional neural network that benefits from fusing spectral, spatial, and spectral-spatial features in the unmixing process. The results of our experiments, backed up with the ablation study, revealed that our techniques outperform others from the literature and lead to higher-quality fractional abundance estimation. Also, we investigated the influence of reducing the training sets on the capabilities of all algorithms and their robustness against noise, as capturing large and representative ground-truth sets is time-consuming and costly in practice, especially in emerging Earth observation scenarios.




# Arxiv Papers in cs.CV on 2022-03-09
### Probabilistic Rotation Representation With an Efficiently Computable Bingham Loss Function and Its Application to Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.04456v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.04456v1)
- **Published**: 2022-03-09 00:38:28+00:00
- **Updated**: 2022-03-09 00:38:28+00:00
- **Authors**: Hiroya Sato, Takuya Ikeda, Koichi Nishiwaki
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: In recent years, a deep learning framework has been widely used for object pose estimation. While quaternion is a common choice for rotation representation of 6D pose, it cannot represent an uncertainty of the observation. In order to handle the uncertainty, Bingham distribution is one promising solution because this has suitable features, such as a smooth representation over SO(3), in addition to the ambiguity representation. However, it requires the complex computation of the normalizing constants. This is the bottleneck of loss computation in training neural networks based on Bingham representation. As such, we propose a fast-computable and easy-to-implement loss function for Bingham distribution. We also show not only to examine the parametrization of Bingham distribution but also an application based on our loss function.



### Human Attention Detection Using AM-FM Representations
- **Arxiv ID**: http://arxiv.org/abs/2203.07093v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.07093v1)
- **Published**: 2022-03-09 00:51:37+00:00
- **Updated**: 2022-03-09 00:51:37+00:00
- **Authors**: Wenjing Shi
- **Comment**: Masters thesis
- **Journal**: None
- **Summary**: Human activity detection from digital videos presents many challenges to the computer vision and image processing communities. Recently, many methods have been developed to detect human activities with varying degree of success. Yet, the general human activity detection problem remains very challenging, especially when the methods need to work 'in the wild' (e.g., without having precise control over the imaging geometry). The thesis explores phase-based solutions for (i) detecting faces, (ii) back of the heads, (iii) joint detection of faces and back of the heads, and (iv) whether the head is looking to the left or the right, using standard video cameras without any control on the imaging geometry. The proposed phase-based approach is based on the development of simple and robust methods that rely on the use of Amplitude Modulation- Frequency Modulation (AM-FM) models. The approach is validated using video frames extracted from the Advancing Out-of-school Learning in Mathematics and Engineering (AOLME) project. The dataset consisted of 13,265 images from ten students looking at the camera, and 6,122 images from five students looking away from the camera. For the students facing the camera, the method was able to correctly classify 97.1% of them looking to the left and 95.9% of them looking to the right. For the students facing the back of the camera, the method was able to correctly classify 87.6% of them looking to the left and 93.3% of them looking to the right. The results indicate that AM-FM based methods hold great promise for analyzing human activity videos.



### Autonomous Mosquito Habitat Detection Using Satellite Imagery and Convolutional Neural Networks for Disease Risk Mapping
- **Arxiv ID**: http://arxiv.org/abs/2203.04463v2
- **DOI**: 10.1002/essoar.10508221.1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04463v2)
- **Published**: 2022-03-09 00:54:59+00:00
- **Updated**: 2022-03-12 04:29:16+00:00
- **Authors**: Sriram Elango, Nandini Ramachandran, Russanne Low
- **Comment**: 8 pages, 11 figures
- **Journal**: None
- **Summary**: Mosquitoes are known vectors for disease transmission that cause over one million deaths globally each year. The majority of natural mosquito habitats are areas containing standing water that are challenging to detect using conventional ground-based technology on a macro scale. Contemporary approaches, such as drones, UAVs, and other aerial imaging technology are costly when implemented and are only most accurate on a finer spatial scale whereas the proposed convolutional neural network(CNN) approach can be applied for disease risk mapping and further guide preventative efforts on a more global scale. By assessing the performance of autonomous mosquito habitat detection technology, the transmission of mosquito-borne diseases can be prevented in a cost-effective manner. This approach aims to identify the spatiotemporal distribution of mosquito habitats in extensive areas that are difficult to survey using ground-based technology by employing computer vision on satellite imagery for proof of concept. The research presents an evaluation and the results of 3 different CNN models to determine their accuracy of predicting large-scale mosquito habitats. For this approach, a dataset was constructed containing a variety of geographical features. Larger land cover variables such as ponds/lakes, inlets, and rivers were utilized to classify mosquito habitats while minute sites were omitted for higher accuracy on a larger scale. Using the dataset, multiple CNN networks were trained and evaluated for accuracy of habitat prediction. Utilizing a CNN-based approach on readily available satellite imagery is cost-effective and scalable, unlike most aerial imaging technology. Testing revealed that YOLOv4 obtained greater accuracy in mosquito habitat detection for identifying large-scale mosquito habitats.



### The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.04466v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04466v3)
- **Published**: 2022-03-09 00:58:04+00:00
- **Updated**: 2022-06-19 23:53:13+00:00
- **Authors**: Xin Yu, Thiago Serra, Srikumar Ramalingam, Shandian Zhe
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks tend to achieve better accuracy with training if they are larger -- even if the resulting models are overparameterized. Nevertheless, carefully removing such excess parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value -- even though magnitude is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, as well as a systematic update of the remaining weights. Our selection method outperforms other methods under high sparsity, and the weight update is advantageous even when combined with the other methods.



### Part-level Action Parsing via a Pose-guided Coarse-to-Fine Framework
- **Arxiv ID**: http://arxiv.org/abs/2203.04476v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04476v2)
- **Published**: 2022-03-09 01:30:57+00:00
- **Updated**: 2022-09-02 01:52:25+00:00
- **Authors**: Xiaodong Chen, Xinchen Liu, Wu Liu, Kun Liu, Dong Wu, Yongdong Zhang, Tao Mei
- **Comment**: Accepted by IEEE ISCAS 2022, 5 pages, 2 figures. arXiv admin note:
  text overlap with arXiv:2110.03368
- **Journal**: None
- **Summary**: Action recognition from videos, i.e., classifying a video into one of the pre-defined action types, has been a popular topic in the communities of artificial intelligence, multimedia, and signal processing. However, existing methods usually consider an input video as a whole and learn models, e.g., Convolutional Neural Networks (CNNs), with coarse video-level class labels. These methods can only output an action class for the video, but cannot provide fine-grained and explainable cues to answer why the video shows a specific action. Therefore, researchers start to focus on a new task, Part-level Action Parsing (PAP), which aims to not only predict the video-level action but also recognize the frame-level fine-grained actions or interactions of body parts for each person in the video. To this end, we propose a coarse-to-fine framework for this challenging task. In particular, our framework first predicts the video-level class of the input video, then localizes the body parts and predicts the part-level action. Moreover, to balance the accuracy and computation in part-level action parsing, we propose to recognize the part-level actions by segment-level features. Furthermore, to overcome the ambiguity of body parts, we propose a pose-guided positional embedding method to accurately localize body parts. Through comprehensive experiments on a large-scale dataset, i.e., Kinetics-TPS, our framework achieves state-of-the-art performance and outperforms existing methods over a 31.10% ROC score.



### 3SD: Self-Supervised Saliency Detection With No Labels
- **Arxiv ID**: http://arxiv.org/abs/2203.04478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04478v1)
- **Published**: 2022-03-09 01:40:28+00:00
- **Updated**: 2022-03-09 01:40:28+00:00
- **Authors**: Rajeev Yasarla, Renliang Weng, Wongun Choi, Vishal Patel, Amir Sadeghian
- **Comment**: None
- **Journal**: None
- **Summary**: We present a conceptually simple self-supervised method for saliency detection. Our method generates and uses pseudo-ground truth labels for training. The generated pseudo-GT labels don't require any kind of human annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent works show that features extracted from classification tasks provide important saliency cues like structure and semantic information of salient objects in the image. Our method, called 3SD, exploits this idea by adding a branch for a self-supervised classification task in parallel with salient object detection, to obtain class activation maps (CAM maps). These CAM maps along with the edges of the input image are used to generate the pseudo-GT saliency maps to train our 3SD network. Specifically, we propose a contrastive learning-based training on multiple image patches for the classification task. We show the multi-patch classification with contrastive loss improves the quality of the CAM maps compared to naive classification on the entire image. Experiments on six benchmark datasets demonstrate that without any labels, our 3SD method outperforms all existing weakly supervised and unsupervised methods, and its performance is on par with the fully-supervised methods. Code is available at :https://github.com/rajeevyasarla/3SD



### Image Steganography based on Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2203.04500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2203.04500v1)
- **Published**: 2022-03-09 02:58:29+00:00
- **Updated**: 2022-03-09 02:58:29+00:00
- **Authors**: Donghui Hu, Yu Zhang, Cong Yu, Jian Wang, Yaofei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image steganography is the art and science of using images as cover for covert communications. With the development of neural networks, traditional image steganography is more likely to be detected by deep learning-based steganalysis. To improve upon this, we propose image steganography network based on style transfer, and the embedding of secret messages can be disguised as image stylization. We embed secret information while transforming the content image style. In latent space, the secret information is integrated into the latent representation of the cover image to generate the stego images, which are indistinguishable from normal stylized images. It is an end-to-end unsupervised model without pre-training. Extensive experiments on the benchmark dataset demonstrate the reliability, quality and security of stego images generated by our steganographic network.



### An error correction scheme for improved air-tissue boundary in real-time MRI video for speech production
- **Arxiv ID**: http://arxiv.org/abs/2203.06004v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2203.06004v1)
- **Published**: 2022-03-09 03:17:24+00:00
- **Updated**: 2022-03-09 03:17:24+00:00
- **Authors**: Anwesha Roy, Varun Belagali, Prasanta Kumar Ghosh
- **Comment**: accepted for ICASSP 2022
- **Journal**: None
- **Summary**: The best performance in Air-tissue boundary (ATB) segmentation of real-time Magnetic Resonance Imaging (rtMRI) videos in speech production is known to be achieved by a 3-dimensional convolutional neural network (3D-CNN) model. However, the evaluation of this model, as well as other ATB segmentation techniques reported in the literature, is done using Dynamic Time Warping (DTW) distance between the entire original and predicted contours. Such an evaluation measure may not capture local errors in the predicted contour. Careful analysis of predicted contours reveals errors in regions like the velum part of contour1 (ATB comprising of upper lip, hard palate, and velum) and tongue base section of contour2 (ATB covering jawline, lower lip, tongue base, and epiglottis), which are not captured in a global evaluation metric like DTW distance. In this work, we automatically detect such errors and propose a correction scheme for the same. We also propose two new evaluation metrics for ATB segmentation separately in contour1 and contour2 to explicitly capture two types of errors in these contours. The proposed detection and correction strategies result in an improvement of these two evaluation metrics by 61.8% and 61.4% for contour1 and by 67.8% and 28.4% for contour2. Traditional DTW distance, on the other hand, improves by 44.6% for contour1 and 4.0% for contour2.



### Update Compression for Deep Neural Networks on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2203.04516v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04516v2)
- **Published**: 2022-03-09 04:20:43+00:00
- **Updated**: 2022-04-21 07:44:53+00:00
- **Authors**: Bo Chen, Ali Bakhshi, Gustavo Batista, Brian Ng, Tat-Jun Chin
- **Comment**: CVPR 2022 Mobile AI Workshop
- **Journal**: None
- **Summary**: An increasing number of artificial intelligence (AI) applications involve the execution of deep neural networks (DNNs) on edge devices. Many practical reasons motivate the need to update the DNN model on the edge device post-deployment, such as refining the model, concept drift, or outright change in the learning task. In this paper, we consider the scenario where retraining can be done on the server side based on a copy of the DNN model, with only the necessary data transmitted to the edge to update the deployed model. However, due to bandwidth constraints, we want to minimise the transmission required to achieve the update. We develop a simple approach based on matrix factorisation to compress the model update -- this differs from compressing the model itself. The key idea is to preserve existing knowledge in the current model and optimise only small additional parameters for the update which can be used to reconstitute the model on the edge. We compared our method to similar techniques used in federated learning; our method usually requires less than half of the update size of existing methods to achieve the same accuracy.



### Fast Road Segmentation via Uncertainty-aware Symmetric Network
- **Arxiv ID**: http://arxiv.org/abs/2203.04537v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04537v1)
- **Published**: 2022-03-09 06:11:29+00:00
- **Updated**: 2022-03-09 06:11:29+00:00
- **Authors**: Yicong Chang, Feng Xue, Fei Sheng, Wenteng Liang, Anlong Ming
- **Comment**: Accepted by ICRA 2022
- **Journal**: None
- **Summary**: The high performance of RGB-D based road segmentation methods contrasts with their rare application in commercial autonomous driving, which is owing to two reasons: 1) the prior methods cannot achieve high inference speed and high accuracy in both ways; 2) the different properties of RGB and depth data are not well-exploited, limiting the reliability of predicted road. In this paper, based on the evidence theory, an uncertainty-aware symmetric network (USNet) is proposed to achieve a trade-off between speed and accuracy by fully fusing RGB and depth data. Firstly, cross-modal feature fusion operations, which are indispensable in the prior RGB-D based methods, are abandoned. We instead separately adopt two light-weight subnetworks to learn road representations from RGB and depth inputs. The light-weight structure guarantees the real-time inference of our method. Moreover, a multiscale evidence collection (MEC) module is designed to collect evidence in multiple scales for each modality, which provides sufficient evidence for pixel class determination. Finally, in uncertainty-aware fusion (UAF) module, the uncertainty of each modality is perceived to guide the fusion of the two subnetworks. Experimental results demonstrate that our method achieves a state-of-the-art accuracy with real-time inference speed of 43+ FPS. The source code is available at https://github.com/morancyc/USNet.



### Monocular Depth Distribution Alignment with Low Computation
- **Arxiv ID**: http://arxiv.org/abs/2203.04538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04538v1)
- **Published**: 2022-03-09 06:18:26+00:00
- **Updated**: 2022-03-09 06:18:26+00:00
- **Authors**: Fei Sheng, Feng Xue, Yicong Chang, Wenteng Liang, Anlong Ming
- **Comment**: Accepted by ICRA 2022
- **Journal**: None
- **Summary**: The performance of monocular depth estimation generally depends on the amount of parameters and computational cost. It leads to a large accuracy contrast between light-weight networks and heavy-weight networks, which limits their application in the real world. In this paper, we model the majority of accuracy contrast between them as the difference of depth distribution, which we call "Distribution drift". To this end, a distribution alignment network (DANet) is proposed. We firstly design a pyramid scene transformer (PST) module to capture inter-region interaction in multiple scales. By perceiving the difference of depth features between every two regions, DANet tends to predict a reasonable scene structure, which fits the shape of distribution to ground truth. Then, we propose a local-global optimization (LGO) scheme to realize the supervision of global range of scene depth. Thanks to the alignment of depth distribution shape and scene depth range, DANet sharply alleviates the distribution drift, and achieves a comparable performance with prior heavy-weight methods, but uses only 1% floating-point operations per second (FLOPs) of them. The experiments on two datasets, namely the widely used NYUDv2 dataset and the more challenging iBims-1 dataset, demonstrate the effectiveness of our method. The source code is available at https://github.com/YiLiM1/DANet.



### ChiTransformer:Towards Reliable Stereo from Cues
- **Arxiv ID**: http://arxiv.org/abs/2203.04554v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04554v3)
- **Published**: 2022-03-09 07:19:58+00:00
- **Updated**: 2022-08-09 14:04:30+00:00
- **Authors**: Qing Su, Shihao Ji
- **Comment**: 11 pages, 3 figures, CVPR2022
- **Journal**: None
- **Summary**: Current stereo matching techniques are challenged by restricted searching space, occluded regions and sheer size. While single image depth estimation is spared from these challenges and can achieve satisfactory results with the extracted monocular cues, the lack of stereoscopic relationship renders the monocular prediction less reliable on its own, especially in highly dynamic or cluttered environments. To address these issues in both scenarios, we present an optic-chiasm-inspired self-supervised binocular depth estimation method, wherein a vision transformer (ViT) with gated positional cross-attention (GPCA) layers is designed to enable feature-sensitive pattern retrieval between views while retaining the extensive context information aggregated through self-attentions. Monocular cues from a single view are thereafter conditionally rectified by a blending layer with the retrieved pattern pairs. This crossover design is biologically analogous to the optic-chasma structure in the human visual system and hence the name, ChiTransformer. Our experiments show that this architecture yields substantial improvements over state-of-the-art self-supervised stereo approaches by 11%, and can be used on both rectilinear and non-rectilinear (e.g., fisheye) images. Project is available at https://github.com/ISL-CV/ChiTransformer.



### Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.04559v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04559v4)
- **Published**: 2022-03-09 07:33:36+00:00
- **Updated**: 2022-07-11 06:57:19+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Haozhi Cao, Keyu Wu, Wu Min, Zhenghua Chen
- **Comment**: Accepted by ECCV 2022, update to camera-ready version with updated
  title. 22 pages, 5 figures, 7 tables
- **Journal**: None
- **Summary**: Video-based Unsupervised Domain Adaptation (VUDA) methods improve the robustness of video models, enabling them to be applied to action recognition tasks across different environments. However, these methods require constant access to source data during the adaptation process. Yet in many real-world applications, subjects and scenes in the source video domain should be irrelevant to those in the target video domain. With the increasing emphasis on data privacy, such methods that require source data access would raise serious privacy issues. Therefore, to cope with such concern, a more practical domain adaptation scenario is formulated as the Source-Free Video-based Domain Adaptation (SFVDA). Though there are a few methods for Source-Free Domain Adaptation (SFDA) on image data, these methods yield degenerating performance in SFVDA due to the multi-modality nature of videos, with the existence of additional temporal features. In this paper, we propose a novel Attentive Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal consistency, guaranteed by two novel consistency objectives, namely feature consistency and source prediction consistency, performed across local temporal features. ATCoN further constructs effective overall temporal features by attending to local temporal features based on prediction confidence. Empirical results demonstrate the state-of-the-art performance of ATCoN across various cross-domain action recognition benchmarks.



### Training from a Better Start Point: Active Self-Semi-Supervised Learning for Few Labeled Samples
- **Arxiv ID**: http://arxiv.org/abs/2203.04560v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04560v2)
- **Published**: 2022-03-09 07:45:05+00:00
- **Updated**: 2022-10-05 03:17:56+00:00
- **Authors**: Ziting Wen, Oscar Pizarro, Stefan Williams
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Training with fewer annotations is a key issue for applying deep models to various practical domains. To date, semi-supervised learning has achieved great success in training with few annotations. However, confirmation bias increases dramatically as the number of annotations decreases making it difficult to continue reducing the number of annotations. Based on the observation that the quality of pseudo-labels early in semi-supervised training plays an important role in mitigating confirmation bias, in this paper we propose an active self-semi-supervised learning (AS3L) framework. AS3L bootstraps semi-supervised models with prior pseudo-labels (PPL), where PPL is obtained by label propagation over self-supervised features. We illustrate that the accuracy of PPL is not only affected by the quality of features, but also by the selection of the labeled samples. We develop active learning and label propagation strategies to obtain better PPL. Consequently, our framework can significantly improve the performance of models in the case of few annotations while reducing the training time. Experiments on four semi-supervised learning benchmarks demonstrate the effectiveness of the proposed methods. Our method outperforms the baseline method by an average of 7\% on the four datasets and outperforms the baseline method in accuracy while taking about 1/3 of the training time.



### MLNav: Learning to Safely Navigate on Martian Terrains
- **Arxiv ID**: http://arxiv.org/abs/2203.04563v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04563v1)
- **Published**: 2022-03-09 07:53:15+00:00
- **Updated**: 2022-03-09 07:53:15+00:00
- **Authors**: Shreyansh Daftry, Neil Abcouwer, Tyler Del Sesto, Siddarth Venkatraman, Jialin Song, Lucas Igel, Amos Byon, Ugo Rosolia, Yisong Yue, Masahiro Ono
- **Comment**: IEEE Robotics and Automation Letters (RA-L) and ICRA 2022
- **Journal**: None
- **Summary**: We present MLNav, a learning-enhanced path planning framework for safety-critical and resource-limited systems operating in complex environments, such as rovers navigating on Mars. MLNav makes judicious use of machine learning to enhance the efficiency of path planning while fully respecting safety constraints. In particular, the dominant computational cost in such safety-critical settings is running a model-based safety checker on the proposed paths. Our learned search heuristic can simultaneously predict the feasibility for all path options in a single run, and the model-based safety checker is only invoked on the top-scoring paths. We validate in high-fidelity simulations using both real Martian terrain data collected by the Perseverance rover, as well as a suite of challenging synthetic terrains. Our experiments show that: (i) compared to the baseline ENav path planner on board the Perserverance rover, MLNav can provide a significant improvement in multiple key metrics, such as a 10x reduction in collision checks when navigating real Martian terrains, despite being trained with synthetic terrains; and (ii) MLNav can successfully navigate highly challenging terrains where the baseline ENav fails to find a feasible path before timing out.



### Region-Aware Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2203.04564v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04564v2)
- **Published**: 2022-03-09 07:55:45+00:00
- **Updated**: 2022-03-18 02:18:20+00:00
- **Authors**: Chao Xu, Jiangning Zhang, Miao Hua, Qian He, Zili Yi, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to achieve identity-consistent harmonious high-resolution face generation in a local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch augments local identity-relevant features by introducing the Transformer to effectively model misaligned cross-scale semantic interaction. \textbf{2)} Global Source Feature-Adaptive (SFA) branch further complements global identity-relevant cues for generating identity-consistent swapped faces. Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated with StyleGAN2 to predict identity-relevant soft facial masks in an unsupervised manner that is more practical for generating harmonious high-resolution faces. Abundant experiments qualitatively and quantitatively demonstrate the superiority of our method for generating more identity-consistent high-resolution swapped faces over SOTA methods, \eg, obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.



### All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators
- **Arxiv ID**: http://arxiv.org/abs/2203.04566v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.04566v2)
- **Published**: 2022-03-09 08:03:07+00:00
- **Updated**: 2022-03-13 07:51:46+00:00
- **Authors**: Brijen Thananjeyan, Justin Kerr, Huang Huang, Joseph E. Gonzalez, Ken Goldberg
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale semantic image annotation is a significant challenge for learning-based perception systems in robotics. Current approaches often rely on human labelers, which can be expensive, or simulation data, which can visually or physically differ from real data. This paper proposes Labels from UltraViolet (LUV), a novel framework that enables rapid, labeled data collection in real manipulation environments without human labeling. LUV uses transparent, ultraviolet-fluorescent paint with programmable ultraviolet LEDs to collect paired images of a scene in standard lighting and UV lighting to autonomously extract segmentation masks and keypoints via color segmentation. We apply LUV to a suite of diverse robot perception tasks to evaluate its labeling quality, flexibility, and data collection rate. Results suggest that LUV is 180-2500 times faster than a human labeler across the tasks. We show that LUV provides labels consistent with human annotations on unpainted test images. The networks trained on these labels are used to smooth and fold crumpled towels with 83% success rate and achieve 1.7mm position error with respect to human labels on a surgical needle pose estimation task. The low cost of LUV makes it ideal as a lightweight replacement for human labeling systems, with the one-time setup costs at $300 equivalent to the cost of collecting around 200 semantic segmentation labels on Amazon Mechanical Turk. Code, datasets, visualizations, and supplementary material can be found at https://sites.google.com/berkeley.edu/luv



### PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.04568v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04568v3)
- **Published**: 2022-03-09 08:06:56+00:00
- **Updated**: 2022-07-23 13:04:23+00:00
- **Authors**: Wentao Liu, Tong Tian, Weijin Xu, Huihua Yang, Xipeng Pan, Songlin Yan, Lemeng Wang
- **Comment**: 10 pages, 3 figures
- **Journal**: MICCAI2022
- **Summary**: The success of Transformer in computer vision has attracted increasing attention in the medical imaging community. Especially for medical image segmentation, many excellent hybrid architectures based on convolutional neural networks (CNNs) and Transformer have been presented and achieve impressive performance. However, most of these methods, which embed modular Transformer into CNNs, struggle to reach their full potential. In this paper, we propose a novel hybrid architecture for medical image segmentation called PHTrans, which parallelly hybridizes Transformer and CNN in main building blocks to produce hierarchical representations from global and local features and adaptively aggregate them, aiming to fully exploit their strengths to obtain better segmentation performance. Specifically, PHTrans follows the U-shaped encoder-decoder design and introduces the parallel hybird module in deep stages, where convolution blocks and the modified 3D Swin Transformer learn local features and global dependencies separately, then a sequence-to-volume operation unifies the dimensions of the outputs to achieve feature aggregation. Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its effectiveness, consistently outperforming state-of-the-art methods. The code is available at: https://github.com/lseventeen/PHTrans.



### CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.04570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04570v1)
- **Published**: 2022-03-09 08:15:14+00:00
- **Updated**: 2022-03-09 08:15:14+00:00
- **Authors**: Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng Jing, Xiaoyao Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices.   We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition.   In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning.   Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\% patches, our CP-ViT method reduces over 40\% FLOPs while maintaining accuracy loss within 1\%.



### A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices
- **Arxiv ID**: http://arxiv.org/abs/2203.04571v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04571v2)
- **Published**: 2022-03-09 08:29:21+00:00
- **Updated**: 2023-03-03 14:09:50+00:00
- **Authors**: Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, Abbas Rahimi
- **Comment**: Updated version with additional NVSA end-to-end training,
  generalization experiments, and PGM experiments
- **Journal**: None
- **Summary**: Neither deep neural networks nor symbolic AI alone has approached the kind of intelligence expressed in humans. This is mainly because neural networks are not able to decompose joint representations to obtain distinct objects (the so-called binding problem), while symbolic AI suffers from exhaustive rule searches, among other problems. These two problems are still pronounced in neuro-symbolic AI which aims to combine the best of the two paradigms. Here, we show that the two problems can be addressed with our proposed neuro-vector-symbolic architecture (NVSA) by exploiting its powerful operators on high-dimensional distributed representations that serve as a common language between neural networks and symbolic AI. The efficacy of NVSA is demonstrated by solving the Raven's progressive matrices datasets. Compared to state-of-the-art deep neural network and neuro-symbolic approaches, end-to-end training of NVSA achieves a new record of 87.7% average accuracy in RAVEN, and 88.1% in I-RAVEN datasets. Moreover, compared to the symbolic reasoning within the neuro-symbolic approaches, the probabilistic reasoning of NVSA with less expensive operations on the distributed representations is two orders of magnitude faster. Our code is available at https://github.com/IBM/neuro-vector-symbolic-architectures.



### Multi-modal Brain Tumor Segmentation via Missing Modality Synthesis and Modality-level Attention Fusion
- **Arxiv ID**: http://arxiv.org/abs/2203.04586v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04586v1)
- **Published**: 2022-03-09 09:08:48+00:00
- **Updated**: 2022-03-09 09:08:48+00:00
- **Authors**: Ziqi Huang, Li Lin, Pujin Cheng, Linkai Peng, Xiaoying Tang
- **Comment**: 6 pages, 5 figures, submitted to ICPR 2022
- **Journal**: None
- **Summary**: Multi-modal magnetic resonance (MR) imaging provides great potential for diagnosing and analyzing brain gliomas. In clinical scenarios, common MR sequences such as T1, T2 and FLAIR can be obtained simultaneously in a single scanning process. However, acquiring contrast enhanced modalities such as T1ce requires additional time, cost, and injection of contrast agent. As such, it is clinically meaningful to develop a method to synthesize unavailable modalities which can also be used as additional inputs to downstream tasks (e.g., brain tumor segmentation) for performance enhancing. In this work, we propose an end-to-end framework named Modality-Level Attention Fusion Network (MAF-Net), wherein we innovatively conduct patchwise contrastive learning for extracting multi-modal latent features and dynamically assigning attention weights to fuse different modalities. Through extensive experiments on BraTS2020, our proposed MAF-Net is found to yield superior T1ce synthesis performance (SSIM of 0.8879 and PSNR of 22.78) and accurate brain tumor segmentation (mean Dice scores of 67.9%, 41.8% and 88.0% on segmenting the tumor core, enhancing tumor and whole tumor).



### Mapping global dynamics of benchmark creation and saturation in artificial intelligence
- **Arxiv ID**: http://arxiv.org/abs/2203.04592v4
- **DOI**: 10.1038/s41467-022-34591-0
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04592v4)
- **Published**: 2022-03-09 09:16:49+00:00
- **Updated**: 2022-10-07 13:45:59+00:00
- **Authors**: Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, Matthias Samwald
- **Comment**: This version includes more recent data and additional analyses
- **Journal**: Nature Communications volume 13, Article number: 6793 (2022)
- **Summary**: Benchmarks are crucial to measuring and steering progress in artificial intelligence (AI). However, recent studies raised concerns over the state of AI benchmarking, reporting issues such as benchmark overfitting, benchmark saturation and increasing centralization of benchmark dataset creation. To facilitate monitoring of the health of the AI benchmarking ecosystem, we introduce methodologies for creating condensed maps of the global dynamics of benchmark creation and saturation. We curated data for 3765 benchmarks covering the entire domains of computer vision and natural language processing, and show that a large fraction of benchmarks quickly trended towards near-saturation, that many benchmarks fail to find widespread utilization, and that benchmark performance gains for different AI tasks were prone to unforeseen bursts. We analyze attributes associated with benchmark popularity, and conclude that future benchmarks should emphasize versatility, breadth and real-world utility.



### Domain Generalization using Pretrained Models without Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/2203.04600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04600v1)
- **Published**: 2022-03-09 09:33:59+00:00
- **Updated**: 2022-03-09 09:33:59+00:00
- **Authors**: Ziyue Li, Kan Ren, Xinyang Jiang, Bo Li, Haipeng Zhang, Dongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-tuning pretrained models is a common practice in domain generalization (DG) tasks. However, fine-tuning is usually computationally expensive due to the ever-growing size of pretrained models. More importantly, it may cause over-fitting on source domain and compromise their generalization ability as shown in recent works. Generally, pretrained models possess some level of generalization ability and can achieve decent performance regarding specific domains and samples. However, the generalization performance of pretrained models could vary significantly over different test domains even samples, which raises challenges for us to best leverage pretrained models in DG tasks. In this paper, we propose a novel domain generalization paradigm to better leverage various pretrained models, named specialized ensemble learning for domain generalization (SEDGE). It first trains a linear label space adapter upon fixed pretrained models, which transforms the outputs of the pretrained model to the label space of the target domain. Then, an ensemble network aware of model specialty is proposed to dynamically dispatch proper pretrained models to predict each test sample. Experimental studies on several benchmarks show that SEDGE achieves significant performance improvements comparing to strong baselines including state-of-the-art method in DG tasks and reduces the trainable parameters by ~99% and the training time by ~99.5%.



### Attention-effective multiple instance learning on weakly stem cell colony segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.04606v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04606v1)
- **Published**: 2022-03-09 09:49:02+00:00
- **Updated**: 2022-03-09 09:49:02+00:00
- **Authors**: Novanto Yudistira, Muthu Subash Kavitha, Jeny Rajan, Takio Kurita
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of induced pluripotent stem cell (iPSC) colonies often needs the precise extraction of the colony features. However, existing computerized systems relied on segmentation of contours by preprocessing for classifying the colony conditions were task-extensive. To maximize the efficiency in categorizing colony conditions, we propose a multiple instance learning (MIL) in weakly supervised settings. It is designed in a single model to produce weak segmentation and classification of colonies without using finely labeled samples. As a single model, we employ a U-net-like convolution neural network (CNN) to train on binary image-level labels for MIL colonies classification. Furthermore, to specify the object of interest we used a simple post-processing method. The proposed approach is compared over conventional methods using five-fold cross-validation and receiver operating characteristic (ROC) curve. The maximum accuracy of the MIL-net is 95%, which is 15 % higher than the conventional methods. Furthermore, the ability to interpret the location of the iPSC colonies based on the image level label without using a pixel-wise ground truth image is more appealing and cost-effective in colony condition recognition.



### Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/2203.04607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04607v2)
- **Published**: 2022-03-09 09:51:00+00:00
- **Updated**: 2022-11-12 09:38:05+00:00
- **Authors**: Qilong Zhang, Chaoning Zhang, Chaoqun Li, Jingkuan Song, Lianli Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the adversarial vulnerability of deep neural networks (DNNs) has raised increasing attention. Among all the threat models, no-box attacks are the most practical but extremely challenging since they neither rely on any knowledge of the target model or similar substitute model, nor access the dataset for training a new substitute model. Although a recent method has attempted such an attack in a loose sense, its performance is not good enough and computational overhead of training is expensive. In this paper, we move a step forward and show the existence of a \textbf{training-free} adversarial perturbation under the no-box threat model, which can be successfully used to attack different DNNs in real-time. Motivated by our observation that high-frequency component (HFC) domains in low-level features and plays a crucial role in classification, we attack an image mainly by manipulating its frequency components. Specifically, the perturbation is manipulated by suppression of the original HFC and adding of noisy HFC. We empirically and experimentally analyze the requirements of effective noisy HFC and show that it should be regionally homogeneous, repeating and dense. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our proposed no-box method. It attacks ten well-known models with a success rate of \textbf{98.13\%} on average, which outperforms state-of-the-art no-box attacks by \textbf{29.39\%}. Furthermore, our method is even competitive to mainstream transfer-based black-box attacks.



### Object-Based Visual Camera Pose Estimation From Ellipsoidal Model and 3D-Aware Ellipse Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.04613v1
- **DOI**: 10.1007/s11263-022-01585-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04613v1)
- **Published**: 2022-03-09 10:00:52+00:00
- **Updated**: 2022-03-09 10:00:52+00:00
- **Authors**: Matthieu Zins, Gilles Simon, Marie-Odile Berger
- **Comment**: International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: In this paper, we propose a method for initial camera pose estimation from just a single image which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoids in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method. This is achieved with very little effort in terms of training data acquisition - a few hundred calibrated images of which only three need manual object annotation. Code and models are released at https://gitlab.inria.fr/tangram/3d-aware-ellipses-for-visual-localization



### Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.04614v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04614v2)
- **Published**: 2022-03-09 10:02:00+00:00
- **Updated**: 2022-03-12 05:32:06+00:00
- **Authors**: Zhiyuan Cai, Li Lin, Huaqing He, Xiaoying Tang
- **Comment**: https://github.com/Davidczy/Uni4Eye
- **Journal**: None
- **Summary**: A large-scale labeled dataset is a key factor for the success of supervised deep learning in computer vision. However, a limited number of annotated data is very common, especially in ophthalmic image analysis, since manual annotation is time-consuming and labor-intensive. Self-supervised learning (SSL) methods bring huge opportunities for better utilizing unlabeled data, as they do not need massive annotations. With an attempt to use as many as possible unlabeled ophthalmic images, it is necessary to break the dimension barrier, simultaneously making use of both 2D and 3D images. In this paper, we propose a universal self-supervised Transformer framework, named Uni4Eye, to discover the inherent image property and capture domain-specific feature embedding in ophthalmic images. Uni4Eye can serve as a global feature extractor, which builds its basis on a Masked Image Modeling task with a Vision Transformer (ViT) architecture. We employ a Unified Patch Embedding module to replace the origin patch embedding module in ViT for jointly processing both 2D and 3D input images. Besides, we design a dual-branch multitask decoder module to simultaneously perform two reconstruction tasks on the input image and its gradient map, delivering discriminative representations for better convergence. We evaluate the performance of our pre-trained Uni4Eye encoder by fine-tuning it on six downstream ophthalmic image classification tasks. The superiority of Uni4Eye is successfully established through comparisons to other state-of-the-art SSL pre-training methods.



### Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.04623v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04623v2)
- **Published**: 2022-03-09 10:21:40+00:00
- **Updated**: 2022-03-10 03:14:03+00:00
- **Authors**: Xiao Yang, Yinpeng Dong, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have revealed the vulnerability of face recognition models against physical adversarial patches, which raises security concerns about the deployed face recognition systems. However, it is still challenging to ensure the reproducibility for most attack algorithms under complex physical conditions, which leads to the lack of a systematic evaluation of the existing methods. It is therefore imperative to develop a framework that can enable a comprehensive evaluation of the vulnerability of face recognition in the physical world. To this end, we propose to simulate the complex transformations of faces in the physical world via 3D-face modeling, which serves as a digital counterpart of physical faces. The generic framework allows us to control different face variations and physical conditions to conduct reproducible evaluations comprehensively. With this digital simulator, we further propose a Face3DAdv method considering the 3D face transformations and realistic physical variations. Extensive experiments validate that Face3DAdv can significantly improve the effectiveness of diverse physically realizable adversarial patches in both simulated and physical environments, against various white-box and black-box face recognition models.



### 3D Dense Face Alignment with Fused Features by Aggregating CNNs and GCNs
- **Arxiv ID**: http://arxiv.org/abs/2203.04643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04643v1)
- **Published**: 2022-03-09 11:07:10+00:00
- **Updated**: 2022-03-09 11:07:10+00:00
- **Authors**: Yanda Meng, Xu Chen, Dongxu Gao, Yitian Zhao, Xiaoyun Yang, Yihong Qiao, Xiaowei Huang, Yalin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel multi-level aggregation network to regress the coordinates of the vertices of a 3D face from a single 2D image in an end-to-end manner. This is achieved by seamlessly combining standard convolutional neural networks (CNNs) with Graph Convolution Networks (GCNs). By iteratively and hierarchically fusing the features across different layers and stages of the CNNs and GCNs, our approach can provide a dense face alignment and 3D face reconstruction simultaneously for the benefit of direct feature learning of 3D face mesh. Experiments on several challenging datasets demonstrate that our method outperforms state-of-the-art approaches on both 2D and 3D face alignment tasks.



### Normal and Visibility Estimation of Human Face from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2203.04647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04647v1)
- **Published**: 2022-03-09 11:20:42+00:00
- **Updated**: 2022-03-09 11:20:42+00:00
- **Authors**: Fuzhi Zhong, Rui Wang, Yuchi Huo, Hujun Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work on the intrinsic image of humans starts to consider the visibility of incident illumination and encodes the light transfer function by spherical harmonics. In this paper, we show that such a light transfer function can be further decomposed into visibility and cosine terms related to surface normal. Such decomposition allows us to recover the surface normal in addition to visibility. We propose a deep learning-based approach with a reconstruction loss for training on real-world images. Results show that compared with previous works, the reconstruction of human face from our method better reveals the surface normal and shading details especially around regions where visibility effect is strong.



### Learning the Degradation Distribution for Blind Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2203.04962v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04962v1)
- **Published**: 2022-03-09 11:30:38+00:00
- **Updated**: 2022-03-09 11:30:38+00:00
- **Authors**: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan
- **Comment**: Accepted to CVRP2022
- **Journal**: None
- **Summary**: Synthetic high-resolution (HR) \& low-resolution (LR) pairs are widely used in existing super-resolution (SR) methods. To avoid the domain gap between synthetic and test images, most previous methods try to adaptively learn the synthesizing (degrading) process via a deterministic model. However, some degradations in real scenarios are stochastic and cannot be determined by the content of the image. These deterministic models may fail to model the random factors and content-independent parts of degradations, which will limit the performance of the following SR models. In this paper, we propose a probabilistic degradation model (PDM), which studies the degradation $\mathbf{D}$ as a random variable, and learns its distribution by modeling the mapping from a priori random variable $\mathbf{z}$ to $\mathbf{D}$. Compared with previous deterministic degradation models, PDM could model more diverse degradations and generate HR-LR pairs that may better cover the various degradations of test images, and thus prevent the SR model from over-fitting to specific ones. Extensive experiments have demonstrated that our degradation model can help the SR model achieve better performance on different datasets. The source codes are released at \url{git@github.com:greatlog/UnpairedSR.git}.



### Ray Tracing-Guided Design of Plenoptic Cameras
- **Arxiv ID**: http://arxiv.org/abs/2203.04660v1
- **DOI**: 10.1109/3DV53792.2021.00120
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.04660v1)
- **Published**: 2022-03-09 11:57:00+00:00
- **Updated**: 2022-03-09 11:57:00+00:00
- **Authors**: Tim Michels, Reinhard Koch
- **Comment**: 9 pages, 9 figures. Accepted at 3DV 2021. 2021 International
  Conference on 3D Vision (3DV). IEEE, 2021
- **Journal**: None
- **Summary**: The design of a plenoptic camera requires the combination of two dissimilar optical systems, namely a main lens and an array of microlenses. And while the construction process of a conventional camera is mainly concerned with focusing the image onto a single plane, in the case of plenoptic cameras there can be additional requirements such as a predefined depth of field or a desired range of disparities in neighboring microlens images. Due to this complexity, the manual creation of multiple plenoptic camera setups is often a time-consuming task. In this work we assume a simulation framework as well as the main lens data given and present a method to calculate the remaining aperture, sensor and microlens array parameters under different sets of constraints. Our ray tracing-based approach is shown to result in models outperforming their pendants generated with the commonly used paraxial approximations in terms of image quality, while still meeting the desired constraints. Both the implementation and evaluation setup including 30 plenoptic camera designs are made publicly available.



### Creating Realistic Ground Truth Data for the Evaluation of Calibration Methods for Plenoptic and Conventional Cameras
- **Arxiv ID**: http://arxiv.org/abs/2203.04661v1
- **DOI**: 10.1109/3DV.2019.00055
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.04661v1)
- **Published**: 2022-03-09 11:58:00+00:00
- **Updated**: 2022-03-09 11:58:00+00:00
- **Authors**: Tim Michels, Arne Petersen, Reinhard Koch
- **Comment**: 9 pages, 8 figures. Accepted at 3DV 2019
- **Journal**: 2019 International Conference on 3D Vision (3DV). IEEE, 2019
- **Summary**: Camera calibration methods usually consist of capturing images of known calibration patterns and using the detected correspondences to optimize the parameters of the assumed camera model. A meaningful evaluation of these methods relies on the availability of realistic synthetic data. In previous works concerned with conventional cameras the synthetic data was mainly created by rendering perfect images with a pinhole camera and subsequently adding distortions and aberrations to the renderings and correspondences according to the assumed camera model. This method can bias the evaluation since not every camera perfectly complies with an assumed model. Furthermore, in the field of plenoptic camera calibration there is no synthetic ground truth data available at all. We address these problems by proposing a method based on backward ray tracing to create realistic ground truth data that can be used for an unbiased evaluation of calibration methods for both types of cameras.



### Simulation of Plenoptic Cameras
- **Arxiv ID**: http://arxiv.org/abs/2203.04662v1
- **DOI**: 10.1109/3DTV.2018.8478432
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.04662v1)
- **Published**: 2022-03-09 11:58:31+00:00
- **Updated**: 2022-03-09 11:58:31+00:00
- **Authors**: Tim Michels, Arne Petersen, Luca Palmieri, Reinhard Koch
- **Comment**: None
- **Journal**: 2018-3DTV-Conference: The True Vision-Capture, Transmission and
  Display of 3D Video (3DTV-CON). IEEE, 2018
- **Summary**: Plenoptic cameras enable the capturing of spatial as well as angular color information which can be used for various applications among which are image refocusing and depth calculations. However, these cameras are expensive and research in this area currently lacks data for ground truth comparisons. In this work we describe a flexible, easy-to-use Blender model for the different plenoptic camera types which is on the one hand able to provide the ground truth data for research and on the other hand allows an inexpensive assessment of the cameras usefulness for the desired applications. Furthermore we show that the rendering results exhibit the same image degradation effects as real cameras and make our simulation publicly available.



### Towards Inadequately Pre-trained Models in Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.04668v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04668v3)
- **Published**: 2022-03-09 12:15:55+00:00
- **Updated**: 2023-08-17 03:27:39+00:00
- **Authors**: Andong Deng, Xingjian Li, Di Hu, Tianyang Wang, Haoyi Xiong, Chengzhong Xu
- **Comment**: Accepted by ICCV'2023
- **Journal**: None
- **Summary**: Pre-training has been a popular learning paradigm in deep learning era, especially in annotation-insufficient scenario. Better ImageNet pre-trained models have been demonstrated, from the perspective of architecture, by previous research to have better transferability to downstream tasks. However, in this paper, we found that during the same pre-training process, models at middle epochs, which is inadequately pre-trained, can outperform fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance. This reveals that there is not a solid positive correlation between top-1 accuracy on ImageNet and the transferring result on target data. Based on the contradictory phenomenon between FE and FT that better feature extractor fails to be fine-tuned better accordingly, we conduct comprehensive analyses on features before softmax layer to provide insightful explanations. Our discoveries suggest that, during pre-training, models tend to first learn spectral components corresponding to large singular values and the residual components contribute more when fine-tuning.



### Structure-Aware Flow Generation for Human Body Reshaping
- **Arxiv ID**: http://arxiv.org/abs/2203.04670v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04670v2)
- **Published**: 2022-03-09 12:22:38+00:00
- **Updated**: 2022-03-11 03:38:21+00:00
- **Authors**: Jianqiang Ren, Yuan Yao, Biwen Lei, Miaomiao Cui, Xuansong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Body reshaping is an important procedure in portrait photo retouching. Due to the complicated structure and multifarious appearance of human bodies, existing methods either fall back on the 3D domain via body morphable model or resort to keypoint-based image deformation, leading to inefficiency and unsatisfied visual quality. In this paper, we address these limitations by formulating an end-to-end flow generation architecture under the guidance of body structural priors, including skeletons and Part Affinity Fields, and achieve unprecedentedly controllable performance under arbitrary poses and garments. A compositional attention mechanism is introduced for capturing both visual perceptual correlations and structural associations of the human body to reinforce the manipulation consistency among related parts. For a comprehensive evaluation, we construct the first large-scale body reshaping dataset, namely BR-5K, which contains 5,000 portrait photos as well as professionally retouched targets. Extensive experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods in terms of visual performance, controllability, and efficiency. The dataset is available at our website: https://github.com/JianqiangRen/FlowBasedBodyReshaping.



### Align-Deform-Subtract: An Interventional Framework for Explaining Object Differences
- **Arxiv ID**: http://arxiv.org/abs/2203.04694v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.04694v2)
- **Published**: 2022-03-09 13:13:55+00:00
- **Updated**: 2022-07-20 15:11:57+00:00
- **Authors**: Cian Eastwood, Li Nanbo, Christopher K. I. Williams
- **Comment**: ICLR 2022 Workshop on Objects, Structure and Causality
- **Journal**: None
- **Summary**: Given two object images, how can we explain their differences in terms of the underlying object properties? To address this question, we propose Align-Deform-Subtract (ADS) -- an interventional framework for explaining object differences. By leveraging semantic alignments in image-space as counterfactual interventions on the underlying object properties, ADS iteratively quantifies and removes differences in object properties. The result is a set of "disentangled" error measures which explain object differences in terms of the underlying properties. Experiments on real and synthetic data illustrate the efficacy of the framework.



### FlexIT: Towards Flexible Semantic Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2203.04705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04705v1)
- **Published**: 2022-03-09 13:34:38+00:00
- **Updated**: 2022-03-09 13:34:38+00:00
- **Authors**: Guillaume Couairon, Asya Grechka, Jakob Verbeek, Holger Schwenk, Matthieu Cord
- **Comment**: accepted at CVPR 2022
- **Journal**: None
- **Summary**: Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. Based on this success, recent work on image editing proceeds by projecting images to the GAN latent space and manipulating the latent vector. However, these approaches are limited in that only images from a narrow domain can be transformed, and with only a limited number of editing operations. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an auto-encoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet. Code will be made publicly available.



### A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.04708v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04708v2)
- **Published**: 2022-03-09 13:35:19+00:00
- **Updated**: 2022-03-11 07:37:37+00:00
- **Authors**: Yukun Su, Jingliang Deng, Ruizhou Sun, Guosheng Lin, Qingyao Wu
- **Comment**: Code: https://github.com/suyukun666/UFO
- **Journal**: None
- **Summary**: Humans tend to mine objects by learning from a group of images or several frames of video since we live in a dynamic world. In the computer vision area, many researches focus on co-segmentation (CoS), co-saliency detection (CoSD) and video salient object detection (VSOD) to discover the co-occurrent objects. However, previous approaches design different networks on these similar tasks separately, and they are difficult to apply to each other, which lowers the upper bound of the transferability of deep learning frameworks. Besides, they fail to take full advantage of the cues among inter- and intra-feature within a group of images. In this paper, we introduce a unified framework to tackle these issues, term as UFO (Unified Framework for Co-Object Segmentation). Specifically, we first introduce a transformer block, which views the image feature as a patch token and then captures their long-range dependencies through the self-attention mechanism. This can help the network to excavate the patch structured similarities among the relevant objects. Furthermore, we propose an intra-MLP learning module to produce self-mask to enhance the network to avoid partial activation. Extensive experiments on four CoS benchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks (Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal and SegV2) show that our method outperforms other state-of-the-arts on three different tasks in both accuracy and speed by using the same network architecture , which can reach 140 FPS in real-time.



### Defending Black-box Skeleton-based Human Activity Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2203.04713v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04713v6)
- **Published**: 2022-03-09 13:46:10+00:00
- **Updated**: 2022-12-02 09:54:22+00:00
- **Authors**: He Wang, Yunfeng Diao, Zichang Tan, Guodong Guo
- **Comment**: Accepted in AAAI 2023
- **Journal**: None
- **Summary**: Skeletal motions have been heavily replied upon for human activity recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR has been identified across a variety of classifiers and data, calling for mitigation. To this end, we propose the first black-box defense method for skeleton-based HAR to our best knowledge. Our method is featured by full Bayesian treatments of the clean data, the adversaries and the classifier, leading to (1) a new Bayesian Energy-based formulation of robust discriminative classifiers, (2) a new adversary sampling scheme based on natural motion manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is straightforward but elegant, which turns vulnerable black-box classifiers into robust ones without sacrificing accuracy. It demonstrates surprising and universal effectiveness across a wide range of skeletal HAR classifiers and datasets, under various attacks. Code is available at https://github.com/realcrane/RobustActionRecogniser.



### SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters
- **Arxiv ID**: http://arxiv.org/abs/2203.04746v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04746v2)
- **Published**: 2022-03-09 14:26:10+00:00
- **Updated**: 2022-04-06 17:47:29+00:00
- **Authors**: Albert Mosella-Montoro, Javier Ruiz-Hidalgo
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: This work presents SkinningNet, an end-to-end Two-Stream Graph Neural Network architecture that computes skinning weights from an input mesh and its associated skeleton, without making any assumptions on shape class and structure of the provided mesh. Whereas previous methods pre-compute handcrafted features that relate the mesh and the skeleton or assume a fixed topology of the skeleton, the proposed method extracts this information in an end-to-end learnable fashion by jointly learning the best relationship between mesh vertices and skeleton joints. The proposed method exploits the benefits of the novel Multi-Aggregator Graph Convolution that combines the results of different aggregators during the summarizing step of the Message-Passing scheme, helping the operation to generalize for unseen topologies. Experimental results demonstrate the effectiveness of the contributions of our novel architecture, with SkinningNet outperforming current state-of-the-art alternatives.



### Human Gaze Guided Attention for Surgical Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.04752v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04752v2)
- **Published**: 2022-03-09 14:28:00+00:00
- **Updated**: 2022-11-13 14:51:13+00:00
- **Authors**: Abdishakour Awale, Duygu Sarikaya
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling and automatically recognizing surgical activities are fundamental steps toward automation in surgery and play important roles in providing timely feedback to surgeons. Accurately recognizing surgical activities in video poses a challenging problem that requires an effective means of learning both spatial and temporal dynamics. Human gaze and visual saliency carry important information about visual attention and can be used to extract more relevant features that better reflect these spatial and temporal dynamics. In this study, we propose to use human gaze with a spatio-temporal attention mechanism for activity recognition in surgical videos. Our model consists of an I3D-based architecture, learns spatio-temporal features using 3D convolutions, as well as learns an attention map using human gaze as supervision. We evaluate our model on the Suturing task of JIGSAWS which is a publicly available surgical video understanding dataset. To our knowledge, we are the first to use human gaze for surgical activity recognition. Our results and ablation studies support the contribution of using human gaze to guide attention by outperforming state-of-the art models with an accuracy of 85.4%.



### Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.04771v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04771v4)
- **Published**: 2022-03-09 14:42:26+00:00
- **Updated**: 2022-03-21 03:02:37+00:00
- **Authors**: Sen Jia, Yifan Wang
- **Comment**: 8 pages, 26 figures, conference paper
- **Journal**: None
- **Summary**: Hyperspectral images (HSI) not only have a broad macroscopic field of view but also contain rich spectral information, and the types of surface objects can be identified through spectral information, which is one of the main applications in hyperspectral image related research.In recent years, more and more deep learning methods have been proposed, among which convolutional neural networks (CNN) are the most influential. However, CNN-based methods are difficult to capture long-range dependencies, and also require a large amount of labeled data for model training.Besides, most of the self-supervised training methods in the field of HSI classification are based on the reconstruction of input samples, and it is difficult to achieve effective use of unlabeled samples. To address the shortcomings of CNN networks, we propose a noval multi-scale convolutional embedding module for HSI to realize effective extraction of spatial-spectral information, which can be better combined with Transformer network.In order to make more efficient use of unlabeled data, we propose a new self-supervised pretask. Similar to Mask autoencoder, but our pre-training method only masks the corresponding token of the central pixel in the encoder, and inputs the remaining token into the decoder to reconstruct the spectral information of the central pixel.Such a pretask can better model the relationship between the central feature and the domain feature, and obtain more stable training results.



### Neural Data-Dependent Transform for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2203.04963v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04963v2)
- **Published**: 2022-03-09 14:56:48+00:00
- **Updated**: 2022-03-30 05:47:48+00:00
- **Authors**: Dezhao Wang, Wenhan Yang, Yueyu Hu, Jiaying Liu
- **Comment**: Accepted by CVPR 2022. Project page:
  https://dezhao-wang.github.io/Neural-Syntax-Website/
- **Journal**: None
- **Summary**: Learned image compression has achieved great success due to its excellent modeling capacity, but seldom further considers the Rate-Distortion Optimization (RDO) of each input image. To explore this potential in the learned codec, we make the first attempt to build a neural data-dependent transform and introduce a continuous online mode decision mechanism to jointly optimize the coding efficiency for each individual image. Specifically, apart from the image content stream, we employ an additional model stream to generate the transform parameters at the decoder side. The presence of a model stream enables our model to learn more abstract neural-syntax, which helps cluster the latent representations of images more compactly. Beyond the transform stage, we also adopt neural-syntax based post-processing for the scenarios that require higher quality reconstructions regardless of extra decoding overhead. Moreover, the involvement of the model stream further makes it possible to optimize both the representation and the decoder in an online way, i.e. RDO at the testing time. It is equivalent to a continuous online mode decision, like coding modes in the traditional codecs, to improve the coding efficiency based on the individual input image. The experimental results show the effectiveness of the proposed neural-syntax design and the continuous online mode decision mechanism, demonstrating the superiority of our method in coding efficiency compared to the latest conventional standard Versatile Video Coding (VVC) and other state-of-the-art learning-based methods.



### How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2203.04781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04781v1)
- **Published**: 2022-03-09 15:05:39+00:00
- **Updated**: 2022-03-09 15:05:39+00:00
- **Authors**: Alessio Monti, Angelo Porrello, Simone Calderara, Pasquale Coscia, Lamberto Ballan, Rita Cucchiara
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Accurate prediction of future human positions is an essential task for modern video-surveillance systems. Current state-of-the-art models usually rely on a "history" of past tracked locations (e.g., 3 to 5 seconds) to predict a plausible sequence of future locations (e.g., up to the next 5 seconds). We feel that this common schema neglects critical traits of realistic applications: as the collection of input trajectories involves machine perception (i.e., detection and tracking), incorrect detection and fragmentation errors may accumulate in crowded scenes, leading to tracking drifts. On this account, the model would be fed with corrupted and noisy input data, thus fatally affecting its prediction performance.   In this regard, we focus on delivering accurate predictions when only few input observations are used, thus potentially lowering the risks associated with automatic perception. To this end, we conceive a novel distillation strategy that allows a knowledge transfer from a teacher network to a student one, the latter fed with fewer observations (just two ones). We show that a properly defined teacher supervision allows a student network to perform comparably to state-of-the-art approaches that demand more observations. Besides, extensive experiments on common trajectory forecasting datasets highlight that our student network better generalizes to unseen scenarios.



### Evaluation of YOLO Models with Sliced Inference for Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.04799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04799v1)
- **Published**: 2022-03-09 15:24:30+00:00
- **Updated**: 2022-03-09 15:24:30+00:00
- **Authors**: Muhammed Can Keles, Batuhan Salmanoglu, Mehmet Serdar Guzel, Baran Gursoy, Gazi Erkan Bostanci
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Small object detection has major applications in the fields of UAVs, surveillance, farming and many others. In this work we investigate the performance of state of the art Yolo based object detection models for the task of small object detection as they are one of the most popular and easy to use object detection models. We evaluated YOLOv5 and YOLOX models in this study. We also investigate the effects of slicing aided inference and fine-tuning the model for slicing aided inference. We used the VisDrone2019Det dataset for training and evaluating our models. This dataset is challenging in the sense that most objects are relatively small compared to the image sizes. This work aims to benchmark the YOLOv5 and YOLOX models for small object detection. We have seen that sliced inference increases the AP50 score in all experiments, this effect was greater for the YOLOv5 models compared to the YOLOX models. The effects of sliced fine-tuning and sliced inference combined produced substantial improvement for all models. The highest AP50 score was achieved by the YOLOv5- Large model on the VisDrone2019Det test-dev subset with the score being 48.8.



### NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.04802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04802v1)
- **Published**: 2022-03-09 15:28:02+00:00
- **Updated**: 2022-03-09 15:28:02+00:00
- **Authors**: Fu Li, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, Slobodan Ilic
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation of 3D objects in monocular images is a fundamental and long-standing problem in computer vision. Existing deep learning approaches for 6D pose estimation typically rely on the assumption of availability of 3D object models and 6D pose annotations. However, precise annotation of 6D poses in real data is intricate, time-consuming and not scalable, while synthetic data scales well but lacks realism. To avoid these problems, we present a weakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs only 2D object segmentation and known relative camera poses during training. Following the first-reconstruct-then-regress idea, we first reconstruct the objects from multiple views in the form of an implicit neural representation. Then, we train a pose regression network to predict pixel-wise 2D-3D correspondences between images and the reconstructed model. At inference, the approach only needs a single image as input. A NeRF-enabled PnP+RANSAC algorithm is used to estimate stable and accurate pose from the predicted correspondences. Experiments on LineMod and LineMod-Occlusion show that the proposed method has state-of-the-art accuracy in comparison to the best 6D pose estimation methods in spite of being trained only with weak labels. Besides, we extend the Homebrewed DB dataset with more real training images to support the weakly supervised task and achieve compelling results on this dataset. The extended dataset and code will be released soon.



### A high-precision self-supervised monocular visual odometry in foggy weather based on robust cycled generative adversarial networks and multi-task learning aided depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.04812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04812v1)
- **Published**: 2022-03-09 15:41:57+00:00
- **Updated**: 2022-03-09 15:41:57+00:00
- **Authors**: Xiuyuan Li, Jiangang Yu, Fengchao Li, Guowen An
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a high-precision self-supervised monocular VO, which is specifically designed for navigation in foggy weather. A cycled generative adversarial network is designed to obtain high-quality self-supervised loss via forcing the forward and backward half-cycle to output consistent estimation. Moreover, gradient-based loss and perceptual loss are introduced to eliminate the interference of complex photometric change on self-supervised loss in foggy weather. To solve the ill-posed problem of depth estimation, a self-supervised multi-task learning aided depth estimation module is designed based on the strong correlation between the depth estimation and transmission map calculation of hazy images in foggy weather. The experimental results on the synthetic foggy KITTI dataset show that the proposed self-supervised monocular VO performs better in depth and pose estimation than other state-of-the-art monocular VO in the literature, indicating the designed method is more suitable for foggy weather.



### Text-DIAE: A Self-Supervised Degradation Invariant Autoencoders for Text Recognition and Document Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2203.04814v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04814v4)
- **Published**: 2022-03-09 15:44:36+00:00
- **Updated**: 2022-08-18 14:29:56+00:00
- **Authors**: Mohamed Ali Souibgui, Sanket Biswas, Andres Mafla, Ali Furkan Biten, Alicia Forns, Yousri Kessentini, Josep Llads, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: Preprint
- **Journal**: None
- **Summary**: In this paper, we propose a Text-Degradation Invariant Auto Encoder (Text-DIAE), a self-supervised model designed to tackle two tasks, text recognition (handwritten or scene-text) and document image enhancement. We start by employing a transformer-based architecture that incorporates three pretext tasks as learning objectives to be optimized during pre-training without the usage of labeled data. Each of the pretext objectives is specifically tailored for the final downstream tasks. We conduct several ablation experiments that confirm the design choice of the selected pretext tasks. Importantly, the proposed model does not exhibit limitations of previous state-of-the-art methods based on contrastive losses, while at the same time requiring substantially fewer data samples to converge. Finally, we demonstrate that our method surpasses the state-of-the-art in existing supervised and self-supervised settings in handwritten and scene text recognition and document image enhancement. Our code and trained models will be made publicly available at~\url{ http://Upon_Acceptance}.



### A high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network
- **Arxiv ID**: http://arxiv.org/abs/2203.04822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04822v1)
- **Published**: 2022-03-09 15:54:00+00:00
- **Updated**: 2022-03-09 15:54:00+00:00
- **Authors**: Xiuyuan Li, Fengchao Li, Jiangang Yu, Guowen An
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based underwater object detection (UOD) remains a major challenge due to the degraded visibility and difficulty to obtain sufficient underwater object images captured from various perspectives for training. To address these issues, this paper presents a high-precision UOD based on joint self-supervised deblurring and improved spatial transformer network. A self-supervised deblurring subnetwork is introduced into the designed multi-task learning aided object detection architecture to force the shared feature extraction module to output clean features for detection subnetwork. Aiming at alleviating the limitation of insufficient photos from different perspectives, an improved spatial transformer network is designed based on perspective transformation, adaptively enriching image features within the network. The experimental results show that the proposed UOD approach achieved 47.9 mAP in URPC2017 and 70.3 mAP in URPC2018, outperforming many state-of-the-art UOD methods and indicating the designed method is more suitable for UOD.



### CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2203.04838v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04838v4)
- **Published**: 2022-03-09 16:12:08+00:00
- **Updated**: 2023-07-29 13:47:17+00:00
- **Authors**: Jiaming Zhang, Huayao Liu, Kailun Yang, Xinxin Hu, Ruiping Liu, Rainer Stiefelhagen
- **Comment**: Accepted to IEEE Transactions on Intelligent Transportation Systems
  (T-ITS). The source code of CMX is publicly available at
  https://github.com/huaaaliu/RGBX_Semantic_Segmentation
- **Journal**: None
- **Summary**: Scene understanding based on image segmentation is a crucial component of autonomous vehicles. Pixel-wise semantic segmentation of RGB images can be advanced by exploiting complementary features from the supplementary modality (X-modality). However, covering a wide variety of sensors with a modality-agnostic model remains an unresolved problem due to variations in sensor characteristics among different modalities. Unlike previous modality-specific methods, in this work, we propose a unified fusion framework, CMX, for RGB-X semantic segmentation. To generalize well across different modalities, that often include supplements as well as uncertainties, a unified cross-modal interaction is crucial for modality fusion. Specifically, we design a Cross-Modal Feature Rectification Module (CM-FRM) to calibrate bi-modal features by leveraging the features from one modality to rectify the features of the other modality. With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to perform sufficient exchange of long-range contexts before mixing. To verify CMX, for the first time, we unify five modalities complementary to RGB, i.e., depth, thermal, polarization, event, and LiDAR. Extensive experiments show that CMX generalizes well to diverse multi-modal fusion, achieving state-of-the-art performances on five RGB-Depth benchmarks, as well as RGB-Thermal, RGB-Polarization, and RGB-LiDAR datasets. Besides, to investigate the generalizability to dense-sparse data fusion, we establish an RGB-Event semantic segmentation benchmark based on the EventScape dataset, on which CMX sets the new state-of-the-art. The source code of CMX is publicly available at https://github.com/huaaaliu/RGBX_Semantic_Segmentation.



### Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2203.04845v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04845v3)
- **Published**: 2022-03-09 16:17:47+00:00
- **Updated**: 2022-07-11 02:42:31+00:00
- **Authors**: Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, Luc Van Gool
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Many algorithms have been developed to solve the inverse problem of coded aperture snapshot spectral imaging (CASSI), i.e., recovering the 3D hyperspectral images (HSIs) from a 2D compressive measurement. In recent years, learning-based methods have demonstrated promising performance and dominated the mainstream research direction. However, existing CNN-based methods show limitations in capturing long-range dependencies and non-local self-similarity. Previous Transformer-based methods densely sample tokens, some of which are uninformative, and calculate the multi-head self-attention (MSA) between some tokens that are unrelated in content. This does not fit the spatially sparse nature of HSI signals and limits the model scalability. In this paper, we propose a novel Transformer-based method, coarse-to-fine sparse Transformer (CST), firstly embedding HSI sparsity into deep learning for HSI reconstruction. In particular, CST uses our proposed spectra-aware screening mechanism (SASM) for coarse patch selecting. Then the selected patches are fed into our customized spectra-aggregation hashing multi-head self-attention (SAH-MSA) for fine pixel clustering and self-similarity capturing. Comprehensive experiments show that our CST significantly outperforms state-of-the-art methods while requiring cheaper computational costs. The code and models will be released at https://github.com/caiyuanhao1998/MST



### CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering
- **Arxiv ID**: http://arxiv.org/abs/2203.04873v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04873v2)
- **Published**: 2022-03-09 16:51:15+00:00
- **Updated**: 2022-03-14 00:34:11+00:00
- **Authors**: Nicholas Soucy, Salimeh Yasaei Sekeh
- **Comment**: 11 Pages, 5 Tables, 1 Algorithm, 5 Figures
- **Journal**: None
- **Summary**: Most semantic segmentation approaches of Hyperspectral images (HSIs) use and require preprocessing steps in the form of patching to accurately classify diversified land cover in remotely sensed images. These approaches use patching to incorporate the rich neighborhood information in images and exploit the simplicity and segmentability of the most common HSI datasets. In contrast, most landmasses in the world consist of overlapping and diffused classes, making neighborhood information weaker than what is seen in common HSI datasets. To combat this issue and generalize the segmentation models to more complex and diverse HSI datasets, in this work, we propose our novel flagship model: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to combine spectral information extracted from convolutional neural network (CNN) training on a cluster of landscape pixels. Our CEU-Net model outperforms existing state-of-the-art HSI semantic segmentation methods and gets competitive performance with and without patching when compared to baseline models. We highlight CEU-Net's high performance across Botswana, KSC, and Salinas datasets compared to HybridSN and AeroRIT methods.



### VGQ-CNN: Moving Beyond Fixed Cameras and Top-Grasps for Grasp Quality Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.04874v2
- **DOI**: 10.1109/IJCNN55064.2022.9892763
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04874v2)
- **Published**: 2022-03-09 16:52:32+00:00
- **Updated**: 2022-06-23 17:31:28+00:00
- **Authors**: A. Konrad, J. McDonald, R. Villing
- **Comment**: Accepted for International Joint Conference on Neural Networks
  (IJCNN) 2022
- **Journal**: None
- **Summary**: We present the Versatile Grasp Quality Convolutional Neural Network (VGQ-CNN), a grasp quality prediction network for 6-DOF grasps. VGQ-CNN can be used when evaluating grasps for objects seen from a wide range of camera poses or mobile robots without the need to retrain the network. By defining the grasp orientation explicitly as an input to the network, VGQ-CNN can evaluate 6-DOF grasp poses, moving beyond the 4-DOF grasps used in most image-based grasp evaluation methods like GQ-CNN. To train VGQ-CNN, we generate the new Versatile Grasp dataset (VG-dset) containing 6-DOF grasps observed from a wide range of camera poses. VGQ-CNN achieves a balanced accuracy of 82.1% on our test-split while generalising to a variety of camera poses. Meanwhile, it achieves competitive performance for overhead cameras and top-grasps with a balanced accuracy of 74.2% compared to GQ-CNN's 76.6%. We also propose a modified network architecture, FAST-VGQ-CNN, that speeds up inference using a shared encoder architecture and can make 128 grasp quality predictions in 12ms on a CPU. Code and data are available at https://aucoroboticsmu.github.io/vgq-cnn/.



### Metastatic Cancer Outcome Prediction with Injective Multiple Instance Pooling
- **Arxiv ID**: http://arxiv.org/abs/2203.04964v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04964v1)
- **Published**: 2022-03-09 16:58:03+00:00
- **Updated**: 2022-03-09 16:58:03+00:00
- **Authors**: Jianan Chen, Anne L. Martel
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer stage is a large determinant of patient prognosis and management in many cancer types, and is often assessed using medical imaging modalities, such as CT and MRI. These medical images contain rich information that can be explored to stratify patients within each stage group to further improve prognostic algorithms. Although the majority of cancer deaths result from metastatic and multifocal disease, building imaging biomarkers for patients with multiple tumors has been a challenging task due to the lack of annotated datasets and standard study framework. In this paper, we process two public datasets to set up a benchmark cohort of 341 patient in total for studying outcome prediction of multifocal metastatic cancer. We identify the lack of expressiveness in common multiple instance classification networks and propose two injective multiple instance pooling functions that are better suited to outcome prediction. Our results show that multiple instance learning with injective pooling functions can achieve state-of-the-art performance in the non-small-cell lung cancer CT and head and neck CT outcome prediction benchmarking tasks. We will release the processed multifocal datasets, our code and the intermediate files i.e. extracted radiomic features to support further transparent and reproducible research.



### Efficient Image Representation Learning with Federated Sampled Softmax
- **Arxiv ID**: http://arxiv.org/abs/2203.04888v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04888v1)
- **Published**: 2022-03-09 17:00:32+00:00
- **Updated**: 2022-03-09 17:00:32+00:00
- **Authors**: Sagar M. Waghmare, Hang Qi, Huizhong Chen, Mikhail Sirotenko, Tomer Meron
- **Comment**: 15 pages, 10 figures, 4 tables and 1 algorithm
- **Journal**: None
- **Summary**: Learning image representations on decentralized data can bring many benefits in cases where data cannot be aggregated across data silos. Softmax cross entropy loss is highly effective and commonly used for learning image representations. Using a large number of classes has proven to be particularly beneficial for the descriptive power of such representations in centralized learning. However, doing so on decentralized data with Federated Learning is not straightforward as the demand on FL clients' computation and communication increases proportionally to the number of classes. In this work we introduce federated sampled softmax (FedSS), a resource-efficient approach for learning image representation with Federated Learning. Specifically, the FL clients sample a set of classes and optimize only the corresponding model parameters with respect to a sampled softmax objective that approximates the global full softmax objective. We examine the loss formulation and empirically show that our method significantly reduces the number of parameters transferred to and optimized by the client devices, while performing on par with the standard full softmax method. This work creates a possibility for efficiently learning image representations on decentralized data with a large number of classes under the federated setting.



### Low-light Image and Video Enhancement via Selective Manipulation of Chromaticity
- **Arxiv ID**: http://arxiv.org/abs/2203.04889v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.04889v1)
- **Published**: 2022-03-09 17:01:28+00:00
- **Updated**: 2022-03-09 17:01:28+00:00
- **Authors**: Sumit Shekhar, Max Reimann, Amir Semmo, Sebastian Pasewaldt, Jrgen Dllner, Matthias Trapp
- **Comment**: None
- **Journal**: None
- **Summary**: Image acquisition in low-light conditions suffers from poor quality and significant degradation in visual aesthetics. This affects the visual perception of the acquired image and the performance of various computer vision and image processing algorithms applied after acquisition. Especially for videos, the additional temporal domain makes it more challenging, wherein we need to preserve quality in a temporally coherent manner. We present a simple yet effective approach for low-light image and video enhancement. To this end, we introduce "Adaptive Chromaticity", which refers to an adaptive computation of image chromaticity. The above adaptivity allows us to avoid the costly step of low-light image decomposition into illumination and reflectance, employed by many existing techniques. All stages in our method consist of only point-based operations and high-pass or low-pass filtering, thereby ensuring that the amount of temporal incoherence is negligible when applied on a per-frame basis for videos. Our results on standard lowlight image datasets show the efficacy of our algorithm and its qualitative and quantitative superiority over several state-of-the-art techniques. For videos captured in the wild, we perform a user study to demonstrate the preference for our method in comparison to state-of-the-art approaches.



### Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction
- **Arxiv ID**: http://arxiv.org/abs/2203.04895v2
- **DOI**: 10.1109/TIP.2022.3222641
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04895v2)
- **Published**: 2022-03-09 17:20:18+00:00
- **Updated**: 2022-11-08 02:39:23+00:00
- **Authors**: Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: Benefiting from color independence, illumination invariance and location discrimination attributed by the depth map, it can provide important supplemental information for extracting salient objects in complex environments. However, high-quality depth sensors are expensive and can not be widely applied. While general depth sensors produce the noisy and sparse depth information, which brings the depth-based networks with irreversible interference. In this paper, we propose a novel multi-task and multi-modal filtered transformer (MMFT) network for RGB-D salient object detection (SOD). Specifically, we unify three complementary tasks: depth estimation, salient object detection and contour estimation. The multi-task mechanism promotes the model to learn the task-aware features from the auxiliary tasks. In this way, the depth information can be completed and purified. Moreover, we introduce a multi-modal filtered transformer (MFT) module, which equips with three modality-specific filters to generate the transformer-enhanced feature for each modality. The proposed model works in a depth-free style during the testing phase. Experiments show that it not only significantly surpasses the depth-based RGB-D SOD methods on multiple datasets, but also precisely predicts a high-quality depth map and salient contour at the same time. And, the resulted depth map can help existing RGB-D SOD methods obtain significant performance gain. The source code will be publicly available at https://github.com/Xiaoqi-Zhao-DLUT/MMFT.



### Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.04904v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04904v3)
- **Published**: 2022-03-09 17:26:53+00:00
- **Updated**: 2022-07-15 04:16:21+00:00
- **Authors**: Zhenhailong Wang, Hang Yu, Manling Li, Han Zhao, Heng Ji
- **Comment**: 4 pages, 4 figures, under review
- **Journal**: None
- **Summary**: Despite achieving state-of-the-art zero-shot performance, existing vision-language models still fall short of few-shot transfer ability on domain-specific problems. Classical fine-tuning often fails to prevent highly expressive models from exploiting spurious correlations. Although model-agnostic meta-learning (MAML) presents as a natural alternative for few-shot transfer learning, the expensive computation due to implicit second-order optimization limits its use on large-scale vision-language models such as CLIP. While much literature has been devoted to exploring alternative optimization strategies, we identify another essential aspect towards effective few-shot transfer learning, task sampling, which is previously only be viewed as part of data pre-processing in MAML. To show the impact of task sampling, we propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which differentiates classical fine-tuning only on uniformly sampling multiple tasks. Despite its simplicity, we show that MAMF consistently outperforms classical fine-tuning on five few-shot vision-language classification tasks. We further show that the effectiveness of the bi-level optimization in MAML is highly sensitive to the zero-shot performance of a task in the context of few-shot vision-language classification. The goal of this paper is to provide new insights on what makes few-shot learning work, and encourage more research into investigating better task sampling strategies.



### What Matters For Meta-Learning Vision Regression Tasks?
- **Arxiv ID**: http://arxiv.org/abs/2203.04905v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04905v1)
- **Published**: 2022-03-09 17:28:16+00:00
- **Updated**: 2022-03-09 17:28:16+00:00
- **Authors**: Ning Gao, Hanna Ziesche, Ngo Anh Vien, Michael Volpp, Gerhard Neumann
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: Meta-learning is widely used in few-shot classification and function regression due to its ability to quickly adapt to unseen tasks. However, it has not yet been well explored on regression tasks with high dimensional inputs such as images. This paper makes two main contributions that help understand this barely explored area. \emph{First}, we design two new types of cross-category level vision regression tasks, namely object discovery and pose estimation of unprecedented complexity in the meta-learning domain for computer vision. To this end, we (i) exhaustively evaluate common meta-learning techniques on these tasks, and (ii) quantitatively analyze the effect of various deep learning techniques commonly used in recent meta-learning algorithms in order to strengthen the generalization capability: data augmentation, domain randomization, task augmentation and meta-regularization. Finally, we (iii) provide some insights and practical recommendations for training meta-learning algorithms on vision regression tasks. \emph{Second}, we propose the addition of functional contrastive learning (FCL) over the task representations in Conditional Neural Processes (CNPs) and train in an end-to-end fashion. The experimental results show that the results of prior work are misleading as a consequence of a poor choice of the loss function as well as too small meta-training sets. Specifically, we find that CNPs outperform MAML on most tasks without fine-tuning. Furthermore, we observe that naive task augmentation without a tailored design results in underfitting.



### KPE: Keypoint Pose Encoding for Transformer-based Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.04907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.04907v2)
- **Published**: 2022-03-09 17:38:03+00:00
- **Updated**: 2022-10-06 10:00:48+00:00
- **Authors**: Soon Yau Cheong, Armin Mustafa, Andrew Gilbert
- **Comment**: None
- **Journal**: British Machine Vision Conference (BMVC) 2022
- **Summary**: Transformers have recently been shown to generate high quality images from text input. However, the existing method of pose conditioning using skeleton image tokens is computationally inefficient and generate low quality images. Therefore we propose a new method; Keypoint Pose Encoding (KPE); KPE is 10 times more memory efficient and over 73% faster at generating high quality images from text input conditioned on the pose. The pose constraint improves the image quality and reduces errors on body extremities such as arms and legs. The additional benefits include invariance to changes in the target image domain and image resolution, making it easily scalable to higher resolution images. We demonstrate the versatility of KPE by generating photorealistic multiperson images derived from the DeepFashion dataset. We also introduce a evaluation method People Count Error (PCE) that is effective in detecting error in generated human images.



### Rethinking data-driven point spread function modeling with a differentiable optical model
- **Arxiv ID**: http://arxiv.org/abs/2203.04908v2
- **DOI**: 10.1088/1361-6420/acb664
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04908v2)
- **Published**: 2022-03-09 17:39:18+00:00
- **Updated**: 2023-01-23 10:07:36+00:00
- **Authors**: Tobias Liaudat, Jean-Luc Starck, Martin Kilbinger, Pierre-Antoine Frugier
- **Comment**: Submitted. Without appendix: 42 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: In astronomy, upcoming space telescopes with wide-field optical instruments have a spatially varying point spread function (PSF). Specific scientific goals require a high-fidelity estimation of the PSF at target positions where no direct measurement of the PSF is provided. Even though observations of the PSF are available at some positions of the field of view (FOV), they are undersampled, noisy, and integrated into wavelength in the instrument's passband. PSF modeling represents a challenging ill-posed problem, as it requires building a model from degraded observations that can infer a super-resolved PSF at any wavelength and position in the FOV. Our model, coined WaveDiff, proposes a paradigm shift in the data-driven modeling of the point spread function field of telescopes. We change the data-driven modeling space from the pixels to the wavefront by adding a differentiable optical forward model into the modeling framework. This change allows the transfer of complexity from the instrumental response into the forward model. The proposed model relies on stochastic gradient descent to estimate its parameters. Our framework paves the way to building powerful, physically motivated models that do not require special calibration data. This paper demonstrates the WaveDiff model in a simplified setting of a space telescope. The proposed framework represents a performance breakthrough with respect to the existing state-of-the-art data-driven approach. The pixel reconstruction errors decrease 6-fold at observation resolution and 44-fold for a 3x super-resolution. The ellipticity errors are reduced at least 20 times, and the size error is reduced more than 250 times. By only using noisy broad-band in-focus observations, we successfully capture the PSF chromatic variations due to diffraction. Code available at https://github.com/tobias-liaudat/wf-psf.



### Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2203.04913v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04913v2)
- **Published**: 2022-03-09 17:48:33+00:00
- **Updated**: 2022-03-31 21:33:05+00:00
- **Authors**: Dominik Zietlow, Michael Lohaus, Guha Balakrishnan, Matthus Kleindessner, Francesco Locatello, Bernhard Schlkopf, Chris Russell
- **Comment**: None
- **Journal**: None
- **Summary**: Algorithmic fairness is frequently motivated in terms of a trade-off in which overall performance is decreased so as to improve performance on disadvantaged groups where the algorithm would otherwise be less accurate. Contrary to this, we find that applying existing fairness approaches to computer vision improve fairness by degrading the performance of classifiers across all groups (with increased degradation on the best performing groups).   Extending the bias-variance decomposition for classification to fairness, we theoretically explain why the majority of fairness classifiers designed for low capacity models should not be used in settings involving high-capacity models, a scenario common to computer vision. We corroborate this analysis with extensive experimental support that shows that many of the fairness heuristics used in computer vision also degrade performance on the most disadvantaged groups. Building on these insights, we propose an adaptive augmentation strategy that, uniquely, of all methods tested, improves performance for the disadvantaged groups.



### Triangular Character Animation Sampling with Motion, Emotion, and Relation
- **Arxiv ID**: http://arxiv.org/abs/2203.04930v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04930v1)
- **Published**: 2022-03-09 18:19:03+00:00
- **Updated**: 2022-03-09 18:19:03+00:00
- **Authors**: Yizhou Zhao, Liang Qiu, Wensi Ai, Pan Lu, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Dramatic progress has been made in animating individual characters. However, we still lack automatic control over activities between characters, especially those involving interactions. In this paper, we present a novel energy-based framework to sample and synthesize animations by associating the characters' body motions, facial expressions, and social relations. We propose a Spatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode the contextual relationship between motion, emotion, and relation, forming a triangle in a conditional random field. We train our model from a labeled dataset of two-character interactions. Experiments demonstrate that our method can recognize the social relation between two characters and sample new scenes of vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the social relation. Thus, our method can provide animators with an automatic way to generate 3D character animations, help synthesize interactions between Non-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in virtual reality (VR).



### Do better ImageNet classifiers assess perceptual similarity better?
- **Arxiv ID**: http://arxiv.org/abs/2203.04946v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04946v3)
- **Published**: 2022-03-09 18:45:41+00:00
- **Updated**: 2022-10-29 09:59:38+00:00
- **Authors**: Manoj Kumar, Neil Houlsby, Nal Kalchbrenner, Ekin D. Cubuk
- **Comment**: TMLR 2022 (https://openreview.net/forum?id=qrGKGZZvH0)
- **Journal**: None
- **Summary**: Perceptual distances between images, as measured in the space of pre-trained deep features, have outperformed prior low-level, pixel-based metrics on assessing perceptual similarity. While the capabilities of older and less accurate models such as AlexNet and VGG to capture perceptual similarity are well known, modern and more accurate models are less studied. In this paper, we present a large-scale empirical study to assess how well ImageNet classifiers perform on perceptual similarity. First, we observe a inverse correlation between ImageNet accuracy and Perceptual Scores of modern networks such as ResNets, EfficientNets, and Vision Transformers: that is better classifiers achieve worse Perceptual Scores. Then, we examine the ImageNet accuracy/Perceptual Score relationship on varying the depth, width, number of training steps, weight decay, label smoothing, and dropout. Higher accuracy improves Perceptual Score up to a certain point, but we uncover a Pareto frontier between accuracies and Perceptual Score in the mid-to-high accuracy regime. We explore this relationship further using a number of plausible hypotheses such as distortion invariance, spatial frequency sensitivity, and alternative perceptual functions. Interestingly we discover shallow ResNets and ResNets trained for less than 5 epochs only on ImageNet, whose emergent Perceptual Score matches the prior best networks trained directly on supervised human perceptual judgements. The checkpoints for the models in our study are available at https://console.cloud.google.com/storage/browser/gresearch/perceptual_similarity.



### UNeXt: MLP-based Rapid Medical Image Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2203.04967v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04967v1)
- **Published**: 2022-03-09 18:58:22+00:00
- **Updated**: 2022-03-09 18:58:22+00:00
- **Authors**: Jeya Maria Jose Valanarasu, Vishal M. Patel
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: UNet and its latest extensions like TransUNet have been the leading medical image segmentation methods in recent years. However, these networks cannot be effectively adopted for rapid image segmentation in point-of-care applications as they are parameter-heavy, computationally complex and slow to use. To this end, we propose UNeXt which is a Convolutional multilayer perceptron (MLP) based network for image segmentation. We design UNeXt in an effective way with an early convolutional stage and a MLP stage in the latent stage. We propose a tokenized MLP block where we efficiently tokenize and project the convolutional features and use MLPs to model the representation. To further boost the performance, we propose shifting the channels of the inputs while feeding in to MLPs so as to focus on learning local dependencies. Using tokenized MLPs in latent space reduces the number of parameters and computational complexity while being able to result in a better representation to help segmentation. The network also consists of skip connections between various levels of encoder and decoder. We test UNeXt on multiple medical image segmentation datasets and show that we reduce the number of parameters by 72x, decrease the computational complexity by 68x, and improve the inference speed by 10x while also obtaining better segmentation performance over the state-of-the-art medical image segmentation architectures. Code is available at https://github.com/jeya-maria-jose/UNeXt-pytorch



### Resource-Efficient Invariant Networks: Exponential Gains by Unrolled Optimization
- **Arxiv ID**: http://arxiv.org/abs/2203.05006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2203.05006v1)
- **Published**: 2022-03-09 19:04:08+00:00
- **Updated**: 2022-03-09 19:04:08+00:00
- **Authors**: Sam Buchanan, Jingkai Yan, Ellie Haber, John Wright
- **Comment**: None
- **Journal**: None
- **Summary**: Achieving invariance to nuisance transformations is a fundamental challenge in the construction of robust and reliable vision systems. Existing approaches to invariance scale exponentially with the dimension of the family of transformations, making them unable to cope with natural variabilities in visual data such as changes in pose and perspective. We identify a common limitation of these approaches--they rely on sampling to traverse the high-dimensional space of transformations--and propose a new computational primitive for building invariant networks based instead on optimization, which in many scenarios provides a provably more efficient method for high-dimensional exploration than sampling. We provide empirical and theoretical corroboration of the efficiency gains and soundness of our proposed method, and demonstrate its utility in constructing an efficient invariant network for a simple hierarchical object detection task when combined with unrolled optimization. Code for our networks and experiments is available at https://github.com/sdbuch/refine.



### Dynamic Instance Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2203.05028v2
- **DOI**: 10.1109/TIP.2022.3186531
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.05028v2)
- **Published**: 2022-03-09 20:05:54+00:00
- **Updated**: 2022-06-19 16:08:36+00:00
- **Authors**: Zhongying Deng, Kaiyang Zhou, Da Li, Junjun He, Yi-Zhe Song, Tao Xiang
- **Comment**: Accepted to IEEE T-IP. Code available at
  https://github.com/Zhongying-Deng/DIDA
- **Journal**: None
- **Summary**: Most existing studies on unsupervised domain adaptation (UDA) assume that each domain's training samples come with domain labels (e.g., painting, photo). Samples from each domain are assumed to follow the same distribution and the domain labels are exploited to learn domain-invariant features via feature alignment. However, such an assumption often does not hold true -- there often exist numerous finer-grained domains (e.g., dozens of modern painting styles have been developed, each differing dramatically from those of the classic styles). Therefore, forcing feature distribution alignment across each artificially-defined and coarse-grained domain can be ineffective. In this paper, we address both single-source and multi-source UDA from a completely different perspective, which is to view each instance as a fine domain. Feature alignment across domains is thus redundant. Instead, we propose to perform dynamic instance domain adaptation (DIDA). Concretely, a dynamic neural network with adaptive convolutional kernels is developed to generate instance-adaptive residuals to adapt domain-agnostic deep features to each individual instance. This enables a shared classifier to be applied to both source and target domain data without relying on any domain annotation. Further, instead of imposing intricate feature alignment losses, we adopt a simple semi-supervised learning paradigm using only a cross-entropy loss for both labeled source and pseudo labeled target data. Our model, dubbed DIDA-Net, achieves state-of-the-art performance on several commonly used single-source and multi-source UDA datasets including Digits, Office-Home, DomainNet, Digit-Five, and PACS.



### Adaptive Trajectory Prediction via Transferable GNN
- **Arxiv ID**: http://arxiv.org/abs/2203.05046v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.05046v2)
- **Published**: 2022-03-09 21:08:47+00:00
- **Updated**: 2022-03-26 19:12:45+00:00
- **Authors**: Yi Xu, Lichen Wang, Yizhou Wang, Yun Fu
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is an essential component in a wide range of AI applications such as autonomous driving and robotics. Existing methods usually assume the training and testing motions follow the same pattern while ignoring the potential distribution differences (e.g., shopping mall and street). This issue results in inevitable performance decrease. To address this issue, we propose a novel Transferable Graph Neural Network (T-GNN) framework, which jointly conducts trajectory prediction as well as domain alignment in a unified framework. Specifically, a domain-invariant GNN is proposed to explore the structural motion knowledge where the domain-specific knowledge is reduced. Moreover, an attention-based adaptive knowledge learning module is further proposed to explore fine-grained individual-level feature representations for knowledge transfer. By this way, disparities across different trajectory domains will be better alleviated. More challenging while practical trajectory prediction experiments are designed, and the experimental results verify the superior performance of our proposed model. To the best of our knowledge, our work is the pioneer which fills the gap in benchmarks and techniques for practical pedestrian trajectory prediction across different domains.



### Evaluating Proposed Fairness Models for Face Recognition Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2203.05051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.05051v1)
- **Published**: 2022-03-09 21:16:43+00:00
- **Updated**: 2022-03-09 21:16:43+00:00
- **Authors**: John J. Howard, Eli J. Laird, Yevgeniy B. Sirotin, Rebecca E. Rubin, Jerry L. Tipton, Arun R. Vemury
- **Comment**: None
- **Journal**: None
- **Summary**: The development of face recognition algorithms by academic and commercial organizations is growing rapidly due to the onset of deep learning and the widespread availability of training data. Though tests of face recognition algorithm performance indicate yearly performance gains, error rates for many of these systems differ based on the demographic composition of the test set. These "demographic differentials" in algorithm performance can contribute to unequal or unfair outcomes for certain groups of people, raising concerns with increased worldwide adoption of face recognition systems. Consequently, regulatory bodies in both the United States and Europe have proposed new rules requiring audits of biometric systems for "discriminatory impacts" (European Union Artificial Intelligence Act) and "fairness" (U.S. Federal Trade Commission). However, no standard for measuring fairness in biometric systems yet exists. This paper characterizes two proposed measures of face recognition algorithm fairness (fairness measures) from scientists in the U.S. and Europe. We find that both proposed methods are challenging to interpret when applied to disaggregated face recognition error rates as they are commonly experienced in practice. To address this, we propose a set of interpretability criteria, termed the Functional Fairness Measure Criteria (FFMC), that outlines a set of properties desirable in a face recognition algorithm fairness measure. We further develop a new fairness measure, the Gini Aggregation Rate for Biometric Equitability (GARBE), and show how, in conjunction with the Pareto optimization, this measure can be used to select among alternative algorithms based on the accuracy/fairness trade-space. Finally, we have open-sourced our dataset of machine-readable, demographically disaggregated error rates. We believe this is currently the largest open-source dataset of its kind.



### Optical Flow Training under Limited Label Budget via Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.05053v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.05053v2)
- **Published**: 2022-03-09 21:23:21+00:00
- **Updated**: 2022-09-28 15:36:45+00:00
- **Authors**: Shuai Yuan, Xian Sun, Hannah Kim, Shuzhi Yu, Carlo Tomasi
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Supervised training of optical flow predictors generally yields better accuracy than unsupervised training. However, the improved performance comes at an often high annotation cost. Semi-supervised training trades off accuracy against annotation cost. We use a simple yet effective semi-supervised training method to show that even a small fraction of labels can improve flow accuracy by a significant margin over unsupervised training. In addition, we propose active learning methods based on simple heuristics to further reduce the number of labels required to achieve the same target accuracy. Our experiments on both synthetic and real optical flow datasets show that our semi-supervised networks generally need around 50% of the labels to achieve close to full-label accuracy, and only around 20% with active learning on Sintel. We also analyze and show insights on the factors that may influence active learning performance. Code is available at https://github.com/duke-vision/optical-flow-active-learning-release.



### SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2203.05056v5
- **DOI**: 10.1109/LRA.2022.3188106
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.05056v5)
- **Published**: 2022-03-09 21:30:52+00:00
- **Updated**: 2023-01-02 08:31:35+00:00
- **Authors**: Ahmed Rida Sekkat, Yohan Dupuis, Varun Ravi Kumar, Hazem Rashed, Senthil Yogamani, Pascal Vasseur, Paul Honeine
- **Comment**: IEEE Robotics and Automation Letters (RA-L) and IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2022). An
  initial sample of the dataset is released in
  https://drive.google.com/drive/folders/1N5rrySiw1uh9kLeBuOblMbXJ09YsqO7I
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 7, Issue: 3, July
  2022)
- **Summary**: Surround-view cameras are a primary sensor for automated driving, used for near-field perception. It is one of the most commonly used sensors in commercial vehicles primarily used for parking visualization and automated parking. Four fisheye cameras with a 190{\deg} field of view cover the 360{\deg} around the vehicle. Due to its high radial distortion, the standard algorithms do not extend easily. Previously, we released the first public fisheye surround-view dataset named WoodScape. In this work, we release a synthetic version of the surround-view dataset, covering many of its weaknesses and extending it. Firstly, it is not possible to obtain ground truth for pixel-wise optical flow and depth. Secondly, WoodScape did not have all four cameras annotated simultaneously in order to sample diverse frames. However, this means that multi-camera algorithms cannot be designed to obtain a unified output in birds-eye space, which is enabled in the new dataset. We implemented surround-view fisheye geometric projections in CARLA Simulator matching WoodScape's configuration and created SynWoodScape. We release 80k images from the synthetic dataset with annotations for 10+ tasks. We also release the baseline code and supporting scripts.



### Artificial Intelligence Solution for Effective Treatment Planning for Glioblastoma Patients
- **Arxiv ID**: http://arxiv.org/abs/2203.05563v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.05563v1)
- **Published**: 2022-03-09 22:29:48+00:00
- **Updated**: 2022-03-09 22:29:48+00:00
- **Authors**: Vikram Goddla
- **Comment**: The paper includes cover page, table of contents and references
  spanning 23 pages overall
- **Journal**: None
- **Summary**: Glioblastomas are the most common malignant brain tumors in adults. Approximately 200000 people die each year from Glioblastoma in the world. Glioblastoma patients have a median survival of 12 months with optimal therapy and about 4 months without treatment. Glioblastomas appear as heterogeneous necrotic masses with irregular peripheral enhancement, surrounded by vasogenic edema. The current standard of care includes surgical resection, radiotherapy and chemotherapy, which require accurate segmentation of brain tumor subregions. For effective treatment planning, it is vital to identify the methylation status of the promoter of Methylguanine Methyltransferase (MGMT), a positive prognostic factor for chemotherapy. However, current methods for brain tumor segmentation are tedious, subjective and not scalable, and current techniques to determine the methylation status of MGMT promoter involve surgically invasive procedures, which are expensive and time consuming. Hence there is a pressing need to develop automated tools to segment brain tumors and non-invasive methods to predict methylation status of MGMT promoter, to facilitate better treatment planning and improve survival rate. I created an integrated diagnostics solution powered by Artificial Intelligence to automatically segment brain tumor subregions and predict MGMT promoter methylation status, using brain MRI scans. My AI solution is proven on large datasets with performance exceeding current standards and field tested with data from teaching files of local neuroradiologists. With my solution, physicians can submit brain MRI images, and get segmentation and methylation predictions in minutes, and guide brain tumor patients with effective treatment planning and ultimately improve survival time.



### The Transitive Information Theory and its Application to Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2203.05074v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.05074v2)
- **Published**: 2022-03-09 22:35:02+00:00
- **Updated**: 2022-03-28 21:15:47+00:00
- **Authors**: Trung Ngo, Najwa Laabid, Ville Hautamki, Merja Heinniemi
- **Comment**: None
- **Journal**: None
- **Summary**: Paradoxically, a Variational Autoencoder (VAE) could be pushed in two opposite directions, utilizing powerful decoder model for generating realistic images but collapsing the learned representation, or increasing regularization coefficient for disentangling representation but ultimately generating blurry examples. Existing methods narrow the issues to the rate-distortion trade-off between compression and reconstruction. We argue that a good reconstruction model does learn high capacity latents that encode more details, however, its use is hindered by two major issues: the prior is random noise which is completely detached from the posterior and allow no controllability in the generation; mean-field variational inference doesn't enforce hierarchy structure which makes the task of recombining those units into plausible novel output infeasible. As a result, we develop a system that learns a hierarchy of disentangled representation together with a mechanism for recombining the learned representation for generalization. This is achieved by introducing a minimal amount of inductive bias to learn controllable prior for the VAE. The idea is supported by here developed transitive information theory, that is, the mutual information between two target variables could alternately be maximized through the mutual information to the third variable, thus bypassing the rate-distortion bottleneck in VAE design. In particular, we show that our model, named SemafoVAE (inspired by the similar concept in computer science), could generate high-quality examples in a controllable manner, perform smooth traversals of the disentangled factors and intervention at a different level of representation hierarchy.



### NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2203.05081v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.05081v1)
- **Published**: 2022-03-09 22:57:15+00:00
- **Updated**: 2022-03-09 22:57:15+00:00
- **Authors**: Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15$\times$ faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a self-evaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.



### HDL: Hybrid Deep Learning for the Synthesis of Myocardial Velocity Maps in Digital Twins for Cardiac Analysis
- **Arxiv ID**: http://arxiv.org/abs/2203.05564v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2203.05564v1)
- **Published**: 2022-03-09 23:22:56+00:00
- **Updated**: 2022-03-09 23:22:56+00:00
- **Authors**: Xiaodan Xing, Javier Del Ser, Yinzhe Wu, Yang Li, Jun Xia, Lei Xu, David Firmin, Peter Gatehouse, Guang Yang
- **Comment**: 9 pages, 14 figures, Accepted by IEEE JBHI
- **Journal**: None
- **Summary**: Synthetic digital twins based on medical data accelerate the acquisition, labelling and decision making procedure in digital healthcare. A core part of digital healthcare twins is model-based data synthesis, which permits the generation of realistic medical signals without requiring to cope with the modelling complexity of anatomical and biochemical phenomena producing them in reality. Unfortunately, algorithms for cardiac data synthesis have been so far scarcely studied in the literature. An important imaging modality in the cardiac examination is three-directional CINE multi-slice myocardial velocity mapping (3Dir MVM), which provides a quantitative assessment of cardiac motion in three orthogonal directions of the left ventricle. The long acquisition time and complex acquisition produce make it more urgent to produce synthetic digital twins of this imaging modality. In this study, we propose a hybrid deep learning (HDL) network, especially for synthetic 3Dir MVM data. Our algorithm is featured by a hybrid UNet and a Generative Adversarial Network with a foreground-background generation scheme. The experimental results show that from temporally down-sampled magnitude CINE images (six times), our proposed algorithm can still successfully synthesise high temporal resolution 3Dir MVM CMR data (PSNR=42.32) with precise left ventricle segmentation (DICE=0.92). These performance scores indicate that our proposed HDL algorithm can be implemented in real-world digital twins for myocardial velocity mapping data simulation. To the best of our knowledge, this work is the first one in the literature investigating digital twins of the 3Dir MVM CMR, which has shown great potential for improving the efficiency of clinical studies via synthesised cardiac data.



### Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice
- **Arxiv ID**: http://arxiv.org/abs/2203.05962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.05962v1)
- **Published**: 2022-03-09 23:55:24+00:00
- **Updated**: 2022-03-09 23:55:24+00:00
- **Authors**: Peihao Wang, Wenqing Zheng, Tianlong Chen, Zhangyang Wang
- **Comment**: International Conference on Learning Representations (ICLR), 2022
- **Journal**: None
- **Summary**: Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains "for free" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing.




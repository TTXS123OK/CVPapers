# Arxiv Papers in cs.CV on 2022-03-03
### Polar Transformation Based Multiple Instance Learning Assisting Weakly Supervised Image Segmentation With Loose Bounding Box Annotations
- **Arxiv ID**: http://arxiv.org/abs/2203.06000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.06000v1)
- **Published**: 2022-03-03 00:44:40+00:00
- **Updated**: 2022-03-03 00:44:40+00:00
- **Authors**: Juan Wang, Bin Xia
- **Comment**: under review
- **Journal**: None
- **Summary**: This study investigates weakly supervised image segmentation using loose bounding box supervision. It presents a multiple instance learning strategy based on polar transformation to assist image segmentation when loose bounding boxes are employed as supervision. In this strategy, weighted smooth maximum approximation is introduced to incorporate the observation that pixels closer to the origin of the polar transformation are more likely to belong to the object in the bounding box. The proposed approach was evaluated on a public medical dataset using Dice coefficient. The results demonstrate its superior performance. The codes are available at \url{https://github.com/wangjuan313/wsis-polartransform}.



### Quality or Quantity: Toward a Unified Approach for Multi-organ Segmentation in Body CT
- **Arxiv ID**: http://arxiv.org/abs/2203.01934v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01934v1)
- **Published**: 2022-03-03 00:48:54+00:00
- **Updated**: 2022-03-03 00:48:54+00:00
- **Authors**: Fakrul Islam Tushar, Husam Nujaim, Wanyi Fu, Ehsan Abadi, Maciej A. Mazurowski, Ehsan Samei, William P. Segars, Joseph Y. Lo
- **Comment**: 6 pages, 3 figures, 2 tables, Accepted and Presented at SPIE Medical
  Imaging 2022
- **Journal**: None
- **Summary**: Organ segmentation of medical images is a key step in virtual imaging trials. However, organ segmentation datasets are limited in terms of quality (because labels cover only a few organs) and quantity (since case numbers are limited). In this study, we explored the tradeoffs between quality and quantity. Our goal is to create a unified approach for multi-organ segmentation of body CT, which will facilitate the creation of large numbers of accurate virtual phantoms. Initially, we compared two segmentation architectures, 3D-Unet and DenseVNet, which were trained using XCAT data that is fully labeled with 22 organs, and chose the 3D-Unet as the better performing model. We used the XCAT-trained model to generate pseudo-labels for the CT-ORG dataset that has only 7 organs segmented. We performed two experiments: First, we trained 3D-UNet model on the XCAT dataset, representing quality data, and tested it on both XCAT and CT-ORG datasets. Second, we trained 3D-UNet after including the CT-ORG dataset into the training set to have more quantity. Performance improved for segmentation in the organs where we have true labels in both datasets and degraded when relying on pseudo-labels. When organs were labeled in both datasets, Exp-2 improved Average DSC in XCAT and CT-ORG by 1. This demonstrates that quality data is the key to improving the model's performance.



### Nuclei instance segmentation and classification in histopathology images with StarDist
- **Arxiv ID**: http://arxiv.org/abs/2203.02284v3
- **DOI**: 10.1109/ISBIC56247.2022.9854534
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02284v3)
- **Published**: 2022-03-03 01:00:26+00:00
- **Updated**: 2022-08-19 12:48:56+00:00
- **Authors**: Martin Weigert, Uwe Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation and classification of nuclei is an important task in computational pathology. We show that StarDist, a deep learning nuclei segmentation method originally developed for fluorescence microscopy, can be extended and successfully applied to histopathology images. This is substantiated by conducting experiments on the Lizard dataset, and through entering the Colon Nuclei Identification and Counting (CoNIC) challenge 2022, where our approach achieved the first spot on the leaderboard for the segmentation and classification task for both the preliminary and final test phase.



### Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.01474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01474v3)
- **Published**: 2022-03-03 01:20:24+00:00
- **Updated**: 2022-07-09 12:32:05+00:00
- **Authors**: Chongyang Zhong, Lei Hu, Zihao Zhang, Yongjing Ye, Shihong Xia
- **Comment**: We declare that the evaluation metric we used is following STSGCN,
  that is, the average error over frames (denoted by *), which we only found
  when we checked their test code on July 9, 2022
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2022
- **Summary**: Predicting future motion based on historical motion sequence is a fundamental problem in computer vision, and it has wide applications in autonomous driving and robotics. Some recent works have shown that Graph Convolutional Networks(GCN) are instrumental in modeling the relationship between different joints. However, considering the variants and diverse action types in human motion data, the cross-dependency of the spatio-temporal relationships will be difficult to depict due to the decoupled modeling strategy, which may also exacerbate the problem of insufficient generalization. Therefore, we propose the Spatio-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex spatio-temporal dependencies over diverse action types. Specifically, we adopt gating networks to enhance the generalization of GCN via the trainable adaptive adjacency matrix obtained by blending the candidate spatio-temporal adjacency matrices. Moreover, GAGCN addresses the cross-dependency of space and time by balancing the weights of spatio-temporal modeling and fusing the decoupled spatio-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW demonstrate that GAGCN achieves state-of-the-art performance in both short-term and long-term predictions.



### CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision
- **Arxiv ID**: http://arxiv.org/abs/2203.01475v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01475v2)
- **Published**: 2022-03-03 01:22:11+00:00
- **Updated**: 2022-03-12 03:50:10+00:00
- **Authors**: Ke Zhang, Xiahai Zhuang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Curating a large set of fully annotated training data can be costly, especially for the tasks of medical image segmentation. Scribble, a weaker form of annotation, is more obtainable in practice, but training segmentation models from limited supervision of scribbles is still challenging. To address the difficulties, we propose a new framework for scribble learning-based medical image segmentation, which is composed of mix augmentation and cycle consistency and thus is referred to as CycleMix. For augmentation of supervision, CycleMix adopts the mixup strategy with a dedicated design of random occlusion, to perform increments and decrements of scribbles. For regularization of supervision, CycleMix intensifies the training objective with consistency losses to penalize inconsistent segmentation, which results in significant improvement of segmentation performance. Results on two open datasets, i.e., ACDC and MSCMRseg, showed that the proposed method achieved exhilarating performance, demonstrating comparable or even better accuracy than the fully-supervised methods. The code and expert-made scribble annotations for MSCMRseg are publicly available at https://github.com/BWGZK/CycleMix.



### MetaDT: Meta Decision Tree with Class Hierarchy for Interpretable Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.01482v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01482v2)
- **Published**: 2022-03-03 01:53:47+00:00
- **Updated**: 2023-07-26 00:49:29+00:00
- **Authors**: Baoquan Zhang, Hao Jiang, Xutao Li, Shanshan Feng, Yunming Ye, Rui Ye
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel classes with few examples. Recently, lots of methods have been proposed from the perspective of meta-learning and representation learning. However, few works focus on the interpretability of FSL decision process. In this paper, we take a step towards the interpretable FSL by proposing a novel meta-learning based decision tree framework, namely, MetaDT. In particular, the FSL interpretability is achieved from two aspects, i.e., a concept aspect and a visual aspect. On the concept aspect, we first introduce a tree-like concept hierarchy as FSL prior. Then, resorting to the prior, we split each few-shot task to a set of subtasks with different concept levels and then perform class prediction via a model of decision tree. The advantage of such design is that a sequence of high-level concept decisions that lead up to a final class prediction can be obtained, which clarifies the FSL decision process. On the visual aspect, a set of subtask-specific classifiers with visual attention mechanism is designed to perform decision at each node of the decision tree. As a result, a subtask-specific heatmap visualization can be obtained to achieve the decision interpretability of each tree node. At last, to alleviate the data scarcity issue of FSL, we regard the prior of concept hierarchy as an undirected graph, and then design a graph convolution-based decision tree inference network as our meta-learner to infer parameters of the decision tree. Extensive experiments on performance comparison and interpretability analysis show superiority of our MetaDT.



### PetsGAN: Rethinking Priors for Single Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.01488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01488v1)
- **Published**: 2022-03-03 02:31:50+00:00
- **Updated**: 2022-03-03 02:31:50+00:00
- **Authors**: Zicheng Zhang, Yinglu Liu, Congying Han, Hailin Shi, Tiande Guo, Bowen Zhou
- **Comment**: AAAI 2022 (oral)
- **Journal**: None
- **Summary**: Single image generation (SIG), described as generating diverse samples that have similar visual content with the given single image, is first introduced by SinGAN which builds a pyramid of GANs to progressively learn the internal patch distribution of the single image. It also shows great potentials in a wide range of image manipulation tasks. However, the paradigm of SinGAN has limitations in terms of generation quality and training time. Firstly, due to the lack of high-level information, SinGAN cannot handle the object images well as it does on the scene and texture images. Secondly, the separate progressive training scheme is time-consuming and easy to cause artifact accumulation. To tackle these problems, in this paper, we dig into the SIG problem and improve SinGAN by fully-utilization of internal and external priors. The main contributions of this paper include: 1) We introduce to SIG a regularized latent variable model. To the best of our knowledge, it is the first time to give a clear formulation and optimization goal of SIG, and all the existing methods for SIG can be regarded as special cases of this model. 2) We design a novel Prior-based end-to-end training GAN (PetsGAN) to overcome the problems of SinGAN. Our method gets rid of the time-consuming progressive training scheme and can be trained end-to-end. 3) We construct abundant qualitative and quantitative experiments to show the superiority of our method on both generated image quality, diversity, and the training speed. Moreover, we apply our method to other image manipulation tasks (e.g., style transfer, harmonization), and the results further prove the effectiveness and efficiency of our method.



### E-CIR: Event-Enhanced Continuous Intensity Recovery
- **Arxiv ID**: http://arxiv.org/abs/2203.01935v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01935v1)
- **Published**: 2022-03-03 03:20:32+00:00
- **Updated**: 2022-03-03 03:20:32+00:00
- **Authors**: Chen Song, Qixing Huang, Chandrajit Bajaj
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: A camera begins to sense light the moment we press the shutter button. During the exposure interval, relative motion between the scene and the camera causes motion blur, a common undesirable visual artifact. This paper presents E-CIR, which converts a blurry image into a sharp video represented as a parametric function from time to intensity. E-CIR leverages events as an auxiliary input. We discuss how to exploit the temporal event structure to construct the parametric bases. We demonstrate how to train a deep learning model to predict the function coefficients. To improve the appearance consistency, we further introduce a refinement module to propagate visual features among consecutive frames. Compared to state-of-the-art event-enhanced deblurring approaches, E-CIR generates smoother and more realistic results. The implementation of E-CIR is available at https://github.com/chensong1995/E-CIR.



### Optical Flow Based Motion Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2203.11693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.11693v1)
- **Published**: 2022-03-03 03:24:14+00:00
- **Updated**: 2022-03-03 03:24:14+00:00
- **Authors**: Ka Man Lo
- **Comment**: This is an undergraduate research project
- **Journal**: None
- **Summary**: Motion detection is a fundamental but challenging task for autonomous driving. In particular scenes like highway, remote objects have to be paid extra attention for better controlling decision. Aiming at distant vehicles, we train a neural network model to classify the motion status using optical flow field information as the input. The experiments result in high accuracy, showing that our idea is viable and promising. The trained model also achieves an acceptable performance for nearby vehicles. Our work is implemented in PyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used. Visualization videos are available at https://www.youtube.com/playlist?list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .



### NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.01502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01502v2)
- **Published**: 2022-03-03 03:27:20+00:00
- **Updated**: 2022-06-06 08:55:02+00:00
- **Authors**: Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, Ping Tan
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Estimating the accurate depth from a single image is challenging since it is inherently ambiguous and ill-posed. While recent works design increasingly complicated and powerful networks to directly regress the depth map, we take the path of CRFs optimization. Due to the expensive computation, CRFs are usually performed between neighborhoods rather than the whole graph. To leverage the potential of fully-connected CRFs, we split the input into windows and perform the FC-CRFs optimization within each window, which reduces the computation complexity and makes FC-CRFs feasible. To better capture the relationships between nodes in the graph, we exploit the multi-head attention mechanism to compute a multi-head potential function, which is fed to the networks to output an optimized depth map. Then we build a bottom-up-top-down structure, where this neural window FC-CRFs module serves as the decoder, and a vision transformer serves as the encoder. The experiments demonstrate that our method significantly improves the performance across all metrics on both the KITTI and NYUv2 datasets, compared to previous methods. Furthermore, the proposed method can be directly applied to panorama images and outperforms all previous panorama methods on the MatterPort3D dataset. Project page: https://weihaosky.github.io/newcrfs.



### SoftGroup for 3D Instance Segmentation on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2203.01509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01509v1)
- **Published**: 2022-03-03 04:35:37+00:00
- **Updated**: 2022-03-03 04:35:37+00:00
- **Authors**: Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan Thanh Nguyen, Chang D. Yoo
- **Comment**: To appear in CVPR 2022
- **Journal**: None
- **Summary**: Existing state-of-the-art 3D instance segmentation methods perform semantic segmentation followed by grouping. The hard predictions are made when performing semantic segmentation such that each point is associated with a single class. However, the errors stemming from hard decision propagate into grouping that results in (1) low overlaps between the predicted instance with the ground truth and (2) substantial false positives. To address the aforementioned problems, this paper proposes a 3D instance segmentation method referred to as SoftGroup by performing bottom-up soft grouping followed by top-down refinement. SoftGroup allows each point to be associated with multiple classes to mitigate the problems stemming from semantic prediction errors and suppresses false positive instances by learning to categorize them as background. Experimental results on different datasets and multiple evaluation metrics demonstrate the efficacy of SoftGroup. Its performance surpasses the strongest prior method by a significant margin of +6.2% on the ScanNet v2 hidden test set and +6.8% on S3DIS Area 5 in terms of AP_50. SoftGroup is also fast, running at 345ms per scan with a single Titan X on ScanNet v2 dataset. The source code and trained models for both datasets are available at \url{https://github.com/thangvubk/SoftGroup.git}.



### Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2203.01516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68T07(Primary), I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2203.01516v1)
- **Published**: 2022-03-03 05:00:32+00:00
- **Updated**: 2022-03-03 05:00:32+00:00
- **Authors**: Changhong Fu, Sihang Li, Xinnan Yuan, Junjie Ye, Ziang Cao, Fangqiang Ding
- **Comment**: 7 pages, 7 figures, accepted by ICRA 2022
- **Journal**: None
- **Summary**: Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related applications, which leads to a highly demanding requirement on the robustness of UAV trackers. However, adding imperceptible perturbations can easily fool the tracker and cause tracking failures. This risk is often overlooked and rarely researched at present. Therefore, to help increase awareness of the potential risk and the robustness of UAV tracking, this work proposes a novel adaptive adversarial attack approach, i.e., Ad$^2$Attack, against UAV object tracking. Specifically, adversarial examples are generated online during the resampling of the search patch image, which leads trackers to lose the target in the following frames. Ad$^2$Attack is composed of a direct downsampling module and a super-resolution upsampling module with adaptive stages. A novel optimization function is proposed for balancing the imperceptibility and efficiency of the attack. Comprehensive experiments on several well-known benchmarks and real-world conditions show the effectiveness of our attack method, which dramatically reduces the performance of the most advanced Siamese trackers.



### BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.01522v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01522v2)
- **Published**: 2022-03-03 05:31:33+00:00
- **Updated**: 2022-03-29 12:48:13+00:00
- **Authors**: Zhi Hou, Baosheng Yu, Dacheng Tao
- **Comment**: Camera Ready, CVPR2022
- **Journal**: None
- **Summary**: Despite the success of deep neural networks, there are still many challenges in deep representation learning due to the data scarcity issues such as data imbalance, unseen distribution, and domain shift. To address the above-mentioned issues, a variety of methods have been devised to explore the sample relationships in a vanilla way (i.e., from the perspectives of either the input or the loss function), failing to explore the internal structure of deep neural networks for learning with sample relationships. Inspired by this, we propose to enable deep neural networks themselves with the ability to learn the sample relationships from each mini-batch. Specifically, we introduce a batch transformer module or BatchFormer, which is then applied into the batch dimension of each mini-batch to implicitly explore sample relationships during training. By doing this, the proposed method enables the collaboration of different samples, e.g., the head-class samples can also contribute to the learning of the tail classes for long-tailed recognition. Furthermore, to mitigate the gap between training and testing, we share the classifier between with or without the BatchFormer during training, which can thus be removed during testing. We perform extensive experiments on over ten datasets and the proposed method achieves significant improvements on different data scarcity applications without any bells and whistles, including the tasks of long-tailed recognition, compositional zero-shot learning, domain generalization, and contrastive learning. Code will be made publicly available at https://github.com/zhihou7/BatchFormer.



### CAFE: Learning to Condense Dataset by Aligning Features
- **Arxiv ID**: http://arxiv.org/abs/2203.01531v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01531v2)
- **Published**: 2022-03-03 05:58:49+00:00
- **Updated**: 2022-03-27 17:13:08+00:00
- **Authors**: Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, Yang You
- **Comment**: The manuscript has been accepted by CVPR-2022!
- **Journal**: None
- **Summary**: Dataset condensation aims at reducing the network training effort through condensing a cumbersome training set into a compact synthetic one. State-of-the-art approaches largely rely on learning the synthetic data by matching the gradients between the real and synthetic data batches. Despite the intuitive motivation and promising results, such gradient-based methods, by nature, easily overfit to a biased set of samples that produce dominant gradients, and thus lack global supervision of data distribution. In this paper, we propose a novel scheme to Condense dataset by Aligning FEatures (CAFE), which explicitly attempts to preserve the real-feature distribution as well as the discriminant power of the resulting synthetic set, lending itself to strong generalization capability to various architectures. At the heart of our approach is an effective strategy to align features from the real and synthetic data across various scales, while accounting for the classification of real samples. Our scheme is further backed up by a novel dynamic bi-level optimization, which adaptively adjusts parameter updates to prevent over-/under-fitting. We validate the proposed CAFE across various datasets, and demonstrate that it generally outperforms the state of the art: on the SVHN dataset, for example, the performance gain is up to 11%. Extensive experiments and analyses verify the effectiveness and necessity of proposed designs.



### Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks
- **Arxiv ID**: http://arxiv.org/abs/2203.01532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01532v1)
- **Published**: 2022-03-03 05:59:29+00:00
- **Updated**: 2022-03-03 05:59:29+00:00
- **Authors**: Chanyong Jung, Gihyun Kwon, Jong Chul Ye
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Recently, contrastive learning-based image translation methods have been proposed, which contrasts different spatial locations to enhance the spatial correspondence. However, the methods often ignore the diverse semantic relation within the images. To address this, here we propose a novel semantic relation consistency (SRC) regularization along with the decoupled contrastive learning, which utilize the diverse semantics by focusing on the heterogeneous semantics between the image patches of a single image. To further improve the performance, we present a hard negative mining by exploiting the semantic relation. We verified our method for three tasks: single-modal and multi-modal image translations, and GAN compression task for image translation. Experimental results confirmed the state-of-art performance of our method in all the three tasks.



### Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work
- **Arxiv ID**: http://arxiv.org/abs/2203.01536v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.01536v4)
- **Published**: 2022-03-03 06:17:03+00:00
- **Updated**: 2022-08-23 00:39:40+00:00
- **Authors**: Khawar Islam
- **Comment**: Added AAAI 2022 methods and working on ICLR 2022 methods and ICML
  2022
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) are becoming more popular and dominating technique for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a demanding technique in computer vision, ViTs have been successfully solved various vision problems while focusing on long-range relationships. In this paper, we begin by introducing the fundamental concepts and background of the self-attention mechanism. Next, we provide a comprehensive overview of recent top-performing ViT methods describing in terms of strength and weakness, computational cost as well as training and testing dataset. We thoroughly compare the performance of various ViT algorithms and most representative CNN methods on popular benchmark datasets. Finally, we explore some limitations with insightful observations and provide further research direction. The project page along with the collections of papers are available at https://github.com/khawar512/ViT-Survey



### ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor Search on ARM
- **Arxiv ID**: http://arxiv.org/abs/2203.02505v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2203.02505v1)
- **Published**: 2022-03-03 06:19:51+00:00
- **Updated**: 2022-03-03 06:19:51+00:00
- **Authors**: Yusuke Matsui, Yoshiki Imaizumi, Naoya Miyamoto, Naoki Yoshifuji
- **Comment**: ICASSP 2022
- **Journal**: None
- **Summary**: We accelerate the 4-bit product quantization (PQ) on the ARM architecture. Notably, the drastic performance of the conventional 4-bit PQ strongly relies on x64-specific SIMD register, such as AVX2; hence, we cannot yet achieve such good performance on ARM. To fill this gap, we first bundle two 128-bit registers as one 256-bit component. We then apply shuffle operations for each using the ARM-specific NEON instruction. By making this simple but critical modification, we achieve a dramatic speedup for the 4-bit PQ on an ARM architecture. Experiments show that the proposed method consistently achieves a 10x improvement over the naive PQ with the same accuracy.



### Self-supervised Transparent Liquid Segmentation for Robotic Pouring
- **Arxiv ID**: http://arxiv.org/abs/2203.01538v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01538v1)
- **Published**: 2022-03-03 06:26:04+00:00
- **Updated**: 2022-03-03 06:26:04+00:00
- **Authors**: Gautham Narayan Narasimhan, Kai Zhang, Ben Eisner, Xingyu Lin, David Held
- **Comment**: Accepted at ICRA 2022
- **Journal**: 2022 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: Liquid state estimation is important for robotics tasks such as pouring; however, estimating the state of transparent liquids is a challenging problem. We propose a novel segmentation pipeline that can segment transparent liquids such as water from a static, RGB image without requiring any manual annotations or heating of the liquid for training. Instead, we use a generative model that is capable of translating images of colored liquids into synthetically generated transparent liquid images, trained only on an unpaired dataset of colored and transparent liquid images. Segmentation labels of colored liquids are obtained automatically using background subtraction. Our experiments show that we are able to accurately predict a segmentation mask for transparent liquids without requiring any manual annotations. We demonstrate the utility of transparent liquid segmentation in a robotic pouring task that controls pouring by perceiving the liquid height in a transparent cup. Accompanying video and supplementary materials can be found



### Curriculum-style Local-to-global Adaptation for Cross-domain Remote Sensing Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01539v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01539v1)
- **Published**: 2022-03-03 06:33:46+00:00
- **Updated**: 2022-03-03 06:33:46+00:00
- **Authors**: Bo Zhang, Tao Chen, Bin Wang
- **Comment**: Accepted for publication by IEEE T-GRS, code is available at
  https://github.com/BOBrown/CCDA_LGFA
- **Journal**: None
- **Summary**: Although domain adaptation has been extensively studied in natural image-based segmentation task, the research on cross-domain segmentation for very high resolution (VHR) remote sensing images (RSIs) still remains underexplored. The VHR RSIs-based cross-domain segmentation mainly faces two critical challenges: 1) Large area land covers with many diverse object categories bring severe local patch-level data distribution deviations, thus yielding different adaptation difficulties for different local patches; 2) Different VHR sensor types or dynamically changing modes cause the VHR images to go through intensive data distribution differences even for the same geographical location, resulting in different global feature-level domain gap. To address these challenges, we propose a curriculum-style local-to-global cross-domain adaptation framework for the segmentation of VHR RSIs. The proposed curriculum-style adaptation performs the adaptation process in an easy-to-hard way according to the adaptation difficulties that can be obtained using an entropy-based score for each patch of the target domain, and thus well aligns the local patches in a domain image. The proposed local-to-global adaptation performs the feature alignment process from the locally semantic to globally structural feature discrepancies, and consists of a semantic-level domain classifier and an entropy-level domain classifier that can reduce the above cross-domain feature discrepancies. Extensive experiments have been conducted in various cross-domain scenarios, including geographic location variations and imaging mode variations, and the experimental results demonstrate that the proposed method can significantly boost the domain adaptability of segmentation networks for VHR RSIs. Our code is available at: https://github.com/BOBrown/CCDA_LGFA.



### SegTAD: Precise Temporal Action Detection via Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01542v1)
- **Published**: 2022-03-03 06:52:13+00:00
- **Updated**: 2022-03-03 06:52:13+00:00
- **Authors**: Chen Zhao, Merey Ramazanova, Mengmeng Xu, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action detection (TAD) is an important yet challenging task in video analysis. Most existing works draw inspiration from image object detection and tend to reformulate it as a proposal generation - classification problem. However, there are two caveats with this paradigm. First, proposals are not equipped with annotated labels, which have to be empirically compiled, thus the information in the annotations is not necessarily precisely employed in the model training process. Second, there are large variations in the temporal scale of actions, and neglecting this fact may lead to deficient representation in the video features. To address these issues and precisely model temporal action detection, we formulate the task of temporal action detection in a novel perspective of semantic segmentation. Owing to the 1-dimensional property of TAD, we are able to convert the coarse-grained detection annotations to fine-grained semantic segmentation annotations for free. We take advantage of them to provide precise supervision so as to mitigate the impact induced by the imprecise proposal labels. We propose an end-to-end framework SegTAD composed of a 1D semantic segmentation network (1D-SSN) and a proposal detection network (PDN).



### Rethinking the role of normalization and residual blocks for spiking neural networks
- **Arxiv ID**: http://arxiv.org/abs/2203.01544v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01544v1)
- **Published**: 2022-03-03 07:13:39+00:00
- **Updated**: 2022-03-03 07:13:39+00:00
- **Authors**: Shin-ichi Ikegawa, Ryuji Saiin, Yoshihide Sawada, Naotake Natori
- **Comment**: 14 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Biologically inspired spiking neural networks (SNNs) are widely used to realize ultralow-power energy consumption. However, deep SNNs are not easy to train due to the excessive firing of spiking neurons in the hidden layers. To tackle this problem, we propose a novel but simple normalization technique called postsynaptic potential normalization. This normalization removes the subtraction term from the standard normalization and uses the second raw moment instead of the variance as the division term. The spike firing can be controlled, enabling the training to proceed appropriating, by conducting this simple normalization to the postsynaptic potential. The experimental results show that SNNs with our normalization outperformed other models using other normalizations. Furthermore, through the pre-activation residual blocks, the proposed model can train with more than 100 layers without other special techniques dedicated to SNNs.



### A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions
- **Arxiv ID**: http://arxiv.org/abs/2203.02281v2
- **DOI**: 10.3390/app12094429
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02281v2)
- **Published**: 2022-03-03 07:49:21+00:00
- **Updated**: 2022-03-23 04:39:12+00:00
- **Authors**: Banoth Thulasya Naik, Mohammad Farukh Hashmi, Neeraj Dhanraj Bokde
- **Comment**: None
- **Journal**: applsci-1674306/2022
- **Summary**: Recent developments in video analysis of sports and computer vision techniques have achieved significant improvements to enable a variety of critical operations. To provide enhanced information, such as detailed complex analysis in sports like soccer, basketball, cricket, badminton, etc., studies have focused mainly on computer vision techniques employed to carry out different tasks. This paper presents a comprehensive review of sports video analysis for various applications high-level analysis such as detection and classification of players, tracking player or ball in sports and predicting the trajectories of player or ball, recognizing the teams strategies, classifying various events in sports. The paper further discusses published works in a variety of application-specific tasks related to sports and the present researchers views regarding them. Since there is a wide research scope in sports for deploying computer vision techniques in various sports, some of the publicly available datasets related to a particular sport have been provided. This work reviews a detailed discussion on some of the artificial intelligence(AI)applications in sports vision, GPU-based work stations, and embedded platforms. Finally, this review identifies the research directions, probable challenges, and future trends in the area of visual recognition in sports.



### Addressing the Shape-Radiance Ambiguity in View-Dependent Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2203.01553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01553v1)
- **Published**: 2022-03-03 07:59:09+00:00
- **Updated**: 2022-03-03 07:59:09+00:00
- **Authors**: Sverker Rasmuson, Erik Sintorn, Ulf Assarsson
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for handling view-dependent information in radiance fields to help with convergence and quality of 3D reconstruction. Radiance fields with view-dependence suffers from the so called shape-radiance ambiguity, which can lead to incorrect geometry given a high angular resolution of view-dependent colors. We propose the addition of a difference plane in front of each camera, with the purpose of separating view-dependent and Lambertian components during training. We also propose an additional step where we train, but do not store, a low-resolution view-dependent function that helps to isolate the surface if such a separation is proven difficult. These additions have a small impact on performance and memory usage but enables reconstruction of scenes with highly specular components without any other explicit handling of view-dependence such as Spherical Harmonics.



### BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.01937v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01937v5)
- **Published**: 2022-03-03 08:04:59+00:00
- **Updated**: 2023-08-09 08:16:47+00:00
- **Authors**: Yuanhong Chen, Fengbei Liu, Hu Wang, Chong Wang, Yu Tian, Yuyuan Liu, Gustavo Carneiro
- **Comment**: Code is available at https://github.com/cyh-0/BoMD
- **Journal**: None
- **Summary**: Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their similarity with the semantic descriptors produced by BERT models from the multi-label image annotation. Our experiments on diverse noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness in many CXR multi-label classification benchmarks.



### Self-Supervised Ego-Motion Estimation Based on Multi-Layer Fusion of RGB and Inferred Depth
- **Arxiv ID**: http://arxiv.org/abs/2203.01557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01557v1)
- **Published**: 2022-03-03 08:12:32+00:00
- **Updated**: 2022-03-03 08:12:32+00:00
- **Authors**: Zijie Jiang, Hajime Taira, Naoyuki Miyashita, Masatoshi Okutomi
- **Comment**: Accepted to ICRA 2022. Code will be available at
  https://github.com/Beniko95J/MLF-VO
- **Journal**: None
- **Summary**: In existing self-supervised depth and ego-motion estimation methods, ego-motion estimation is usually limited to only leveraging RGB information. Recently, several methods have been proposed to further improve the accuracy of self-supervised ego-motion estimation by fusing information from other modalities, e.g., depth, acceleration, and angular velocity. However, they rarely focus on how different fusion strategies affect performance. In this paper, we investigate the effect of different fusion strategies for ego-motion estimation and propose a new framework for self-supervised learning of depth and ego-motion estimation, which performs ego-motion estimation by leveraging RGB and inferred depth information in a Multi-Layer Fusion manner. As a result, we have achieved state-of-the-art performance among learning-based methods on the KITTI odometry benchmark. Detailed studies on the design choices of leveraging inferred depth information and fusion strategies have also been carried out, which clearly demonstrate the advantages of our proposed framework.



### ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.01562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01562v2)
- **Published**: 2022-03-03 08:23:20+00:00
- **Updated**: 2022-03-14 10:44:06+00:00
- **Authors**: Zuheng Ming, Zitong Yu, Musab Al-Ghadi, Muriel Visani, Muhammad MuzzamilLuqman, Jean-Christophe Burie
- **Comment**: None
- **Journal**: None
- **Summary**: Face Presentation Attack Detection (PAD) is an important measure to prevent spoof attacks for face biometric systems. Many works based on Convolution Neural Networks (CNNs) for face PAD formulate the problem as an image-level binary classification task without considering the context. Alternatively, Vision Transformers (ViT) using self-attention to attend the context of an image become the mainstreams in face PAD. Inspired by ViT, we propose a Video-based Transformer for face PAD (ViTransPAD) with short/long-range spatio-temporal attention which can not only focus on local details with short attention within a frame but also capture long-range dependencies over frames. Instead of using coarse image patches with single-scale as in ViT, we propose the Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate multi-scale patch partitions of Q, K, V feature maps to the heads of transformer in a coarse-to-fine manner, which enables to learn a fine-grained representation to perform pixel-level discrimination for face PAD. Due to lack inductive biases of convolutions in pure transformers, we also introduce convolutions to the proposed ViTransPAD to integrate the desirable properties of CNNs by using convolution patch embedding and convolution projection. The extensive experiments show the effectiveness of our proposed ViTransPAD with a preferable accuracy-computation balance, which can serve as a new backbone for face PAD.



### Occlusion-Aware Cost Constructor for Light Field Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.01576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01576v1)
- **Published**: 2022-03-03 08:58:29+00:00
- **Updated**: 2022-03-03 08:58:29+00:00
- **Authors**: Yingqian Wang, Longguang Wang, Zhengyu Liang, Jungang Yang, Wei An, Yulan Guo
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Matching cost construction is a key step in light field (LF) depth estimation, but was rarely studied in the deep learning era. Recent deep learning-based LF depth estimation methods construct matching cost by sequentially shifting each sub-aperture image (SAI) with a series of predefined offsets, which is complex and time-consuming. In this paper, we propose a simple and fast cost constructor to construct matching cost for LF depth estimation. Our cost constructor is composed by a series of convolutions with specifically designed dilation rates. By applying our cost constructor to SAI arrays, pixels under predefined disparities can be integrated and matching cost can be constructed without using any shifting operation. More importantly, the proposed cost constructor is occlusion-aware and can handle occlusions by dynamically modulating pixels from different views. Based on the proposed cost constructor, we develop a deep network for LF depth estimation. Our network ranks first on the commonly used 4D LF benchmark in terms of the mean square error (MSE), and achieves a faster running time than other state-of-the-art methods.



### HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2203.01577v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01577v3)
- **Published**: 2022-03-03 09:02:52+00:00
- **Updated**: 2022-04-08 08:34:00+00:00
- **Authors**: Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, Li Yi
- **Comment**: None
- **Journal**: CVPR2022
- **Summary**: We present HOI4D, a large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds. With HOI4D, we establish three benchmarking tasks to promote category-level HOI from 4D visual signals including semantic segmentation of 4D dynamic point cloud sequences, category-level object pose tracking, and egocentric action segmentation with diverse interaction targets. In-depth analysis shows HOI4D poses great challenges to existing methods and produces great research opportunities.



### Towards Universal Backward-Compatible Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.01583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01583v2)
- **Published**: 2022-03-03 09:23:51+00:00
- **Updated**: 2022-03-18 06:10:15+00:00
- **Authors**: Binjie Zhang, Yixiao Ge, Yantao Shen, Shupeng Su, Fanzi Wu, Chun Yuan, Xuyuan Xu, Yexin Wang, Ying Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional model upgrades for visual search systems require offline refresh of gallery features by feeding gallery images into new models (dubbed as "backfill"), which is time-consuming and expensive, especially in large-scale applications. The task of backward-compatible representation learning is therefore introduced to support backfill-free model upgrades, where the new query features are interoperable with the old gallery features. Despite the success, previous works only investigated a close-set training scenario (i.e., the new training set shares the same classes as the old one), and are limited by more realistic and challenging open-set scenarios. To this end, we first introduce a new problem of universal backward-compatible representation learning, covering all possible data split in model upgrades. We further propose a simple yet effective method, dubbed as Universal Backward-Compatible Training (UniBCT) with a novel structural prototype refinement algorithm, to learn compatible representations in all kinds of model upgrading benchmarks in a unified manner. Comprehensive experiments on the large-scale face recognition datasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.



### Multi-Tailed Vision Transformer for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2203.01587v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01587v2)
- **Published**: 2022-03-03 09:30:55+00:00
- **Updated**: 2023-06-26 13:13:08+00:00
- **Authors**: Yunke Wang, Bo Du, Wenyuan Wang, Chang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Vision Transformer (ViT) has achieved promising performance in image recognition and gradually serves as a powerful backbone in various vision tasks. To satisfy the sequential input of Transformer, the tail of ViT first splits each image into a sequence of visual tokens with a fixed length. Then the following self-attention layers constructs the global relationship between tokens to produce useful representation for the downstream tasks. Empirically, representing the image with more tokens leads to better performance, yet the quadratic computational complexity of self-attention layer to the number of tokens could seriously influence the efficiency of ViT's inference. For computational reduction, a few pruning methods progressively prune uninformative tokens in the Transformer encoder, while leaving the number of tokens before the Transformer untouched. In fact, fewer tokens as the input for the Transformer encoder can directly reduce the following computational cost. In this spirit, we propose a Multi-Tailed Vision Transformer (MT-ViT) in the paper. MT-ViT adopts multiple tails to produce visual sequences of different lengths for the following Transformer encoder. A tail predictor is introduced to decide which tail is the most efficient for the image to produce accurate prediction. Both modules are optimized in an end-to-end fashion, with the Gumbel-Softmax trick. Experiments on ImageNet-1K demonstrate that MT-ViT can achieve a significant reduction on FLOPs with no degradation of the accuracy and outperform other compared methods in both accuracy and FLOPs.



### 3D Human Motion Prediction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2203.01593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01593v2)
- **Published**: 2022-03-03 09:46:43+00:00
- **Updated**: 2022-03-07 09:18:05+00:00
- **Authors**: Kedi Lyu, Haipeng Chen, Zhenguang Liu, Beiqi Zhang, Ruili Wang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D human motion prediction, predicting future poses from a given sequence, is an issue of great significance and challenge in computer vision and machine intelligence, which can help machines in understanding human behaviors. Due to the increasing development and understanding of Deep Neural Networks (DNNs) and the availability of large-scale human motion datasets, the human motion prediction has been remarkably advanced with a surge of interest among academia and industrial community. In this context, a comprehensive survey on 3D human motion prediction is conducted for the purpose of retrospecting and analyzing relevant works from existing released literature. In addition, a pertinent taxonomy is constructed to categorize these existing approaches for 3D human motion prediction. In this survey, relevant methods are categorized into three categories: human pose representation, network structure design, and \textit{prediction target}. We systematically review all relevant journal and conference papers in the field of human motion prediction since 2015, which are presented in detail based on proposed categorizations in this survey. Furthermore, the outline for the public benchmark datasets, evaluation criteria, and performance comparisons are respectively presented in this paper. The limitations of the state-of-the-art methods are discussed as well, hoping for paving the way for future explorations.



### A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2203.01594v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01594v1)
- **Published**: 2022-03-03 09:47:59+00:00
- **Updated**: 2022-03-03 09:47:59+00:00
- **Authors**: Rashid Khan, M Shujah Islam, Khadija Kanwal, Mansoor Iqbal, Md. Imran Hossain, Zhongfu Ye
- **Comment**: 16 PAGES, 8 figures, 1 TABLE
- **Journal**: Information Technology and Control 2022
- **Summary**: Image captioning is a fast-growing research field of computer vision and natural language processing that involves creating text explanations for images. This study aims to develop a system that uses a pre-trained convolutional neural network (CNN) to extract features from an image, integrates the features with an attention mechanism, and creates captions using a recurrent neural network (RNN). To encode an image into a feature vector as graphical attributes, we employed multiple pre-trained convolutional neural networks. Following that, a language model known as GRU is chosen as the decoder to construct the descriptive sentence. In order to increase performance, we merge the Bahdanau attention model with GRU to allow learning to be focused on a specific portion of the image. On the MSCOCO dataset, the experimental results achieve competitive performance against state-of-the-art approaches.



### Syntax-Aware Network for Handwritten Mathematical Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.01601v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01601v4)
- **Published**: 2022-03-03 09:57:19+00:00
- **Updated**: 2022-06-07 06:41:12+00:00
- **Authors**: Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, Xiang Bai
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Handwritten mathematical expression recognition (HMER) is a challenging task that has many potential applications. Recent methods for HMER have achieved outstanding performance with an encoder-decoder architecture. However, these methods adhere to the paradigm that the prediction is made "from one character to another", which inevitably yields prediction errors due to the complicated structures of mathematical expressions or crabbed handwritings. In this paper, we propose a simple and efficient method for HMER, which is the first to incorporate syntax information into an encoder-decoder network. Specifically, we present a set of grammar rules for converting the LaTeX markup sequence of each expression into a parsing tree; then, we model the markup sequence prediction as a tree traverse process with a deep neural network. In this way, the proposed method can effectively describe the syntax context of expressions, alleviating the structure prediction errors of HMER. Experiments on three benchmark datasets demonstrate that our method achieves better recognition performance than prior arts. To further validate the effectiveness of our method, we create a large-scale dataset consisting of 100k handwritten mathematical expression images acquired from ten thousand writers. The source code, new dataset, and pre-trained models of this work will be publicly available.



### Color Space-based HoVer-Net for Nuclei Instance Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.01940v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01940v1)
- **Published**: 2022-03-03 10:27:36+00:00
- **Updated**: 2022-03-03 10:27:36+00:00
- **Authors**: Hussam Azzuni, Muhammad Ridzuan, Min Xu, Mohammad Yaqub
- **Comment**: 3 pages, 1 figure, and 1 table
- **Journal**: None
- **Summary**: Nuclei segmentation and classification is the first and most crucial step that is utilized for many different microscopy medical analysis applications. However, it suffers from many issues such as the segmentation of small objects, imbalance, and fine-grained differences between types of nuclei. In this paper, multiple different contributions were done tackling these problems present. Firstly, the recently released "ConvNeXt" was used as the encoder for HoVer-Net model since it leverages the key components of transformers that make them perform well. Secondly, to enhance the visual differences between nuclei, a multi-channel color space-based approach is used to aid the model in extracting distinguishing features. Thirdly, Unified Focal loss (UFL) was used to tackle the background-foreground imbalance. Finally, Sharpness-Aware Minimization (SAM) was used to ensure generalizability of the model. Overall, we were able to outperform the current state-of-the-art (SOTA), HoVer-Net, on the preliminary test set of the CoNiC Challenge 2022 by 12.489% mPQ+.



### Adaptive Path Planning for UAVs for Multi-Resolution Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01642v1)
- **Published**: 2022-03-03 11:03:28+00:00
- **Updated**: 2022-03-03 11:03:28+00:00
- **Authors**: Felix Stache, Jonas Westheider, Federico Magistri, Cyrill Stachniss, Marija Popovi
- **Comment**: 10 pages, 7 figures, Submission to Robotics and Autonomous Systems
  journal. arXiv admin note: substantial text overlap with arXiv:2108.01884
- **Journal**: None
- **Summary**: Efficient data collection methods play a major role in helping us better understand the Earth and its ecosystems. In many applications, the usage of unmanned aerial vehicles (UAVs) for monitoring and remote sensing is rapidly gaining momentum due to their high mobility, low cost, and flexible deployment. A key challenge is planning missions to maximize the value of acquired data in large environments given flight time limitations. This is, for example, relevant for monitoring agricultural fields. This paper addresses the problem of adaptive path planning for accurate semantic segmentation of using UAVs. We propose an online planning algorithm which adapts the UAV paths to obtain high-resolution semantic segmentations necessary in areas with fine details as they are detected in incoming images. This enables us to perform close inspections at low altitudes only where required, without wasting energy on exhaustive mapping at maximum image resolution. A key feature of our approach is a new accuracy model for deep learning-based architectures that captures the relationship between UAV altitude and semantic segmentation accuracy. We evaluate our approach on different domains using real-world data, proving the efficacy and generability of our solution.



### Selective Residual M-Net for Real Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2203.01645v1
- **DOI**: 10.23919/EUSIPCO55093.2022.9909521
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.01645v1)
- **Published**: 2022-03-03 11:10:30+00:00
- **Updated**: 2022-03-03 11:10:30+00:00
- **Authors**: Chi-Mao Fan, Tsung-Jung Liu, Kuan-Hsien Liu
- **Comment**: arXiv admin note: text overlap with arXiv:2203.01296
- **Journal**: None
- **Summary**: Image restoration is a low-level vision task which is to restore degraded images to noise-free images. With the success of deep neural networks, the convolutional neural networks surpass the traditional restoration methods and become the mainstream in the computer vision area. To advance the performanceof denoising algorithms, we propose a blind real image denoising network (SRMNet) by employing a hierarchical architecture improved from U-Net. Specifically, we use a selective kernel with residual block on the hierarchical structure called M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet has competitive performance results on two synthetic and two real-world noisy datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/TentativeGitHub/SRMNet.



### Autoregressive Image Generation using Residual Quantization
- **Arxiv ID**: http://arxiv.org/abs/2203.01941v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01941v2)
- **Published**: 2022-03-03 11:44:46+00:00
- **Updated**: 2022-03-09 09:11:16+00:00
- **Authors**: Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, Wook-Shin Han
- **Comment**: 30 pages, 24 figures, accepted by CVPR 2022, the code is available at
  https://github.com/kakaobrain/rq-vae-transformer
- **Journal**: None
- **Summary**: For autoregressive (AR) modeling of high-resolution images, vector quantization (VQ) represents an image as a sequence of discrete codes. A short sequence length is important for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code sequence and generate high-fidelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a fixed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes. Then, RQ-Transformer learns to predict the quantized feature vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256$\times$256 image as 8$\times$8 resolution of the feature map, and RQ-Transformer can efficiently reduce the computational costs. Consequently, our framework outperforms the existing AR models on various benchmarks of unconditional and conditional image generation. Our approach also has a significantly faster sampling speed than previous AR models to generate high-quality images.



### Correlation-Aware Deep Tracking
- **Arxiv ID**: http://arxiv.org/abs/2203.01666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01666v1)
- **Published**: 2022-03-03 11:53:54+00:00
- **Updated**: 2022-03-03 11:53:54+00:00
- **Authors**: Fei Xie, Chunyu Wang, Guangting Wang, Yue Cao, Wankou Yang, Wenjun Zeng
- **Comment**: accepted by CVPR2022
- **Journal**: None
- **Summary**: Robustness and discrimination power are two fundamental requirements in visual object tracking. In most tracking paradigms, we find that the features extracted by the popular Siamese-like networks cannot fully discriminatively model the tracked targets and distractor objects, hindering them from simultaneously meeting these two requirements. While most methods focus on designing robust correlation operations, we propose a novel target-dependent feature network inspired by the self-/cross-attention scheme. In contrast to the Siamese-like feature extraction, our network deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it is able to suppress non-target features, resulting in instance-varying feature extraction. The output features of the search image can be directly used for predicting target locations without extra correlation step. Moreover, our model can be flexibly pre-trained on abundant unpaired images, leading to notably faster convergence than the existing methods. Extensive experiments show our method achieves the state-of-the-art results while running at real-time. Our feature networks also can be applied to existing tracking pipelines seamlessly to raise the tracking performance. Code will be available.



### Translational Lung Imaging Analysis Through Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2203.01668v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01668v1)
- **Published**: 2022-03-03 11:56:20+00:00
- **Updated**: 2022-03-03 11:56:20+00:00
- **Authors**: Pedro M. Gordaliza, Juan Jos Vaquero, Arrate Muoz-Barrutia
- **Comment**: None
- **Journal**: None
- **Summary**: The development of new treatments often requires clinical trials with translational animal models using (pre)-clinical imaging to characterize inter-species pathological processes. Deep Learning (DL) models are commonly used to automate retrieving relevant information from the images. Nevertheless, they typically suffer from low generability and explainability as a product of their entangled design, resulting in a specific DL model per animal model. Consequently, it is not possible to take advantage of the high capacity of DL to discover statistical relationships from inter-species images.   To alleviate this problem, in this work, we present a model capable of extracting disentangled information from images of different animal models and the mechanisms that generate the images. Our method is located at the intersection between deep generative models, disentanglement and causal representation learning. It is optimized from images of pathological lung infected by Tuberculosis and is able: a) from an input slice, infer its position in a volume, the animal model to which it belongs, the damage present and even more, generate a mask covering the whole lung (similar overlap measures to the nnU-Net), b) generate realistic lung images by setting the above variables and c) generate counterfactual images, namely, healthy versions of a damaged input slice.



### Constrained unsupervised anomaly segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01671v2
- **DOI**: 10.1016/j.media.2022.102526
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01671v2)
- **Published**: 2022-03-03 12:05:35+00:00
- **Updated**: 2022-07-12 08:15:40+00:00
- **Authors**: Julio Silva-Rodrguez, Valery Naranjo, Jose Dolz
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2109.00482
  Accepted at Medical Image Analysis
- **Journal**: None
- **Summary**: Current unsupervised anomaly localization approaches rely on generative models to learn the distribution of normal images, which is later used to identify potential anomalous regions derived from errors on the reconstructed images. However, a main limitation of nearly all prior literature is the need of employing anomalous images to set a class-specific threshold to locate the anomalies. This limits their usability in realistic scenarios, where only normal data is typically accessible. Despite this major drawback, only a handful of works have addressed this limitation, by integrating supervision on attention maps during training. In this work, we propose a novel formulation that does not require accessing images with abnormalities to define the threshold. Furthermore, and in contrast to very recent work, the proposed constraint is formulated in a more principled manner, leveraging well-known knowledge in constrained optimization. In particular, the equality constraint on the attention maps in prior work is replaced by an inequality constraint, which allows more flexibility. In addition, to address the limitations of penalty-based functions we employ an extension of the popular log-barrier methods to handle the constraint. Last, we propose an alternative regularization term that maximizes the Shannon entropy of the attention maps, reducing the amount of hyperparameters of the proposed model. Comprehensive experiments on two publicly available datasets on brain lesion segmentation demonstrate that the proposed approach substantially outperforms relevant literature, establishing new state-of-the-art results for unsupervised lesion segmentation, and without the need to access anomalous images.



### Cross-Modality Earth Mover's Distance for Visible Thermal Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2203.01675v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2203.01675v1)
- **Published**: 2022-03-03 12:26:59+00:00
- **Updated**: 2022-03-03 12:26:59+00:00
- **Authors**: Yongguo Ling, Zhun Zhong, Donglin Cao, Zhiming Luo, Yaojin Lin, Shaozi Li, Nicu Sebe
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Visible thermal person re-identification (VT-ReID) suffers from the inter-modality discrepancy and intra-identity variations. Distribution alignment is a popular solution for VT-ReID, which, however, is usually restricted to the influence of the intra-identity variations. In this paper, we propose the Cross-Modality Earth Mover's Distance (CM-EMD) that can alleviate the impact of the intra-identity variations during modality alignment. CM-EMD selects an optimal transport strategy and assigns high weights to pairs that have a smaller intra-identity variation. In this manner, the model will focus on reducing the inter-modality discrepancy while paying less attention to intra-identity variations, leading to a more effective modality alignment. Moreover, we introduce two techniques to improve the advantage of CM-EMD. First, the Cross-Modality Discrimination Learning (CM-DL) is designed to overcome the discrimination degradation problem caused by modality alignment. By reducing the ratio between intra-identity and inter-identity variances, CM-DL leads the model to learn more discriminative representations. Second, we construct the Multi-Granularity Structure (MGS), enabling us to align modalities from both coarse- and fine-grained levels with the proposed CM-EMD. Extensive experiments show the benefits of the proposed CM-EMD and its auxiliary techniques (CM-DL and MGS). Our method achieves state-of-the-art performance on two VT-ReID benchmarks.



### Bridging the Source-to-target Gap for Cross-domain Person Re-Identification with Intermediate Domains
- **Arxiv ID**: http://arxiv.org/abs/2203.01682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01682v1)
- **Published**: 2022-03-03 12:44:56+00:00
- **Updated**: 2022-03-03 12:44:56+00:00
- **Authors**: Yongxing Dai, Yifan Sun, Jun Liu, Zekun Tong, Yi Yang, Ling-Yu Duan
- **Comment**: This work is a journal extension of our ICCV 2021 Oral paper
  https://openaccess.thecvf.com/content/ICCV2021/html/Dai_IDM_An_Intermediate_Domain_Module_for_Domain_Adaptive_Person_Re-ID_ICCV_2021_paper.html
- **Journal**: None
- **Summary**: Cross-domain person re-identification (re-ID), such as unsupervised domain adaptive (UDA) re-ID, aims to transfer the identity-discriminative knowledge from the source to the target domain. Existing methods commonly consider the source and target domains are isolated from each other, i.e., no intermediate status is modeled between both domains. Directly transferring the knowledge between two isolated domains can be very difficult, especially when the domain gap is large. From a novel perspective, we assume these two domains are not completely isolated, but can be connected through intermediate domains. Instead of directly aligning the source and target domains against each other, we propose to align the source and target domains against their intermediate domains for a smooth knowledge transfer. To discover and utilize these intermediate domains, we propose an Intermediate Domain Module (IDM) and a Mirrors Generation Module (MGM). IDM has two functions: 1) it generates multiple intermediate domains by mixing the hidden-layer features from source and target domains and 2) it dynamically reduces the domain gap between the source / target domain features and the intermediate domain features. While IDM achieves good domain alignment, it introduces a side effect, i.e., the mix-up operation may mix the identities into a new identity and lose the original identities. To compensate this, MGM is introduced by mapping the features into the IDM-generated intermediate domains without changing their original identity. It allows to focus on minimizing domain variations to promote the alignment between the source / target domain and intermediate domains, which reinforces IDM into IDM++. We extensively evaluate our method under both the UDA and domain generalization (DG) scenarios and observe that IDM++ yields consistent performance improvement for cross-domain re-ID, achieving new state of the art.



### Relative distance matters for one-shot landmark detection
- **Arxiv ID**: http://arxiv.org/abs/2203.01687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01687v2)
- **Published**: 2022-03-03 12:50:56+00:00
- **Updated**: 2022-03-04 05:54:47+00:00
- **Authors**: Qingsong Yao, Jianji Wang, Yihua Sun, Quan Quan, Heqin Zhu, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning based methods such as cascade comparing to detect (CC2D) have shown great potential for one-shot medical landmark detection. However, the important cue of relative distance between landmarks is ignored in CC2D. In this paper, we upgrade CC2D to version II by incorporating a simple-yet-effective relative distance bias in the training stage, which is theoretically proved to encourage the encoder to project the relatively distant landmarks to the embeddings with low similarities. As consequence, CC2Dv2 is less possible to detect a wrong point far from the correct landmark. Furthermore, we present an open-source, landmark-labeled dataset for the measurement of biomechanical parameters of the lower extremity to alleviate the burden of orthopedic surgeons. The effectiveness of CC2Dv2 is evaluated on the public dataset from the ISBI 2015 Grand-Challenge of cephalometric radiographs and our new dataset, which greatly outperforms the state-of-the-art one-shot landmark detection approaches.



### Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2203.01703v3
- **DOI**: 10.1007/978-3-031-16440-8_15
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.01703v3)
- **Published**: 2022-03-03 13:18:21+00:00
- **Updated**: 2022-09-16 08:34:15+00:00
- **Authors**: Dominik J. E. Waibel, Scott Atwell, Matthias Meier, Carsten Marr, Bastian Rieck
- **Comment**: Accepted at the 25th International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI)
- **Journal**: None
- **Summary**: Reconstructing 3D objects from 2D images is both challenging for our brains and machine learning algorithms. To support this spatial reasoning task, contextual information about the overall shape of an object is critical. However, such information is not captured by established loss terms (e.g. Dice loss). We propose to complement geometrical shape information by including multi-scale topological features, such as connected components, cycles, and voids, in the reconstruction loss. Our method uses cubical complexes to calculate topological features of 3D volume data and employs an optimal transport distance to guide the reconstruction process. This topology-aware loss is fully differentiable, computationally efficient, and can be added to any neural network. We demonstrate the utility of our loss by incorporating it into SHAPR, a model for predicting the 3D cell shape of individual cells based on 2D microscopy images. Using a hybrid loss that leverages both geometrical and topological information of single objects to assess their shape, we find that topological information substantially improves the quality of reconstructions, thus highlighting its ability to extract more relevant features from image datasets.



### Weakly Supervised Object Localization as Domain Adaption
- **Arxiv ID**: http://arxiv.org/abs/2203.01714v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01714v3)
- **Published**: 2022-03-03 13:50:22+00:00
- **Updated**: 2022-03-25 03:32:31+00:00
- **Authors**: Lei Zhu, Qi She, Qian Chen, Yunfei You, Boyu Wang, Yanye Lu
- **Comment**: Accept by CVPR 2022 Conference
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) focuses on localizing objects only with the supervision of image-level classification masks. Most previous WSOL methods follow the classification activation map (CAM) that localizes objects based on the classification structure with the multi-instance learning (MIL) mechanism. However, the MIL mechanism makes CAM only activate discriminative object parts rather than the whole object, weakening its performance for localizing objects. To avoid this problem, this work provides a novel perspective that models WSOL as a domain adaption (DA) task, where the score estimator trained on the source/image domain is tested on the target/pixel domain to locate objects. Under this perspective, a DA-WSOL pipeline is designed to better engage DA approaches into WSOL to enhance localization performance. It utilizes a proposed target sampling strategy to select different types of target samples. Based on these types of target samples, domain adaption localization (DAL) loss is elaborated. It aligns the feature distribution between the two domains by DA and makes the estimator perceive target domain cues by Universum regularization. Experiments show that our pipeline outperforms SOTA methods on multi benchmarks. Code are released at \url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.



### Detecting High-Quality GAN-Generated Face Images using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.01716v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01716v1)
- **Published**: 2022-03-03 13:53:27+00:00
- **Updated**: 2022-03-03 13:53:27+00:00
- **Authors**: Ehsan Nowroozi, Mauro Conti, Yassine Mekdad
- **Comment**: 16 Pages, 6 figures
- **Journal**: None
- **Summary**: In the past decades, the excessive use of the last-generation GAN (Generative Adversarial Networks) models in computer vision has enabled the creation of artificial face images that are visually indistinguishable from genuine ones. These images are particularly used in adversarial settings to create fake social media accounts and other fake online profiles. Such malicious activities can negatively impact the trustworthiness of users identities. On the other hand, the recent development of GAN models may create high-quality face images without evidence of spatial artifacts. Therefore, reassembling uniform color channel correlations is a challenging research problem. To face these challenges, we need to develop efficient tools able to differentiate between fake and authentic face images. In this chapter, we propose a new strategy to differentiate GAN-generated images from authentic images by leveraging spectral band discrepancies, focusing on artificial face image synthesis. In particular, we enable the digital preservation of face images using the Cross-band co-occurrence matrix and spatial co-occurrence matrix. Then, we implement these techniques and feed them to a Convolutional Neural Networks (CNN) architecture to identify the real from artificial faces. Additionally, we show that the performance boost is particularly significant and achieves more than 92% in different post-processing environments. Finally, we provide several research observations demonstrating that this strategy improves a comparable detection method based only on intra-band spatial co-occurrences.



### Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2203.01723v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01723v2)
- **Published**: 2022-03-03 14:14:51+00:00
- **Updated**: 2022-03-16 14:14:21+00:00
- **Authors**: Jiawei Liu, Zhipeng Huang, Liang Li, Kecheng Zheng, Zheng-Jun Zha
- **Comment**: 9 pages, 2 figures, AAAI 2022
- **Journal**: None
- **Summary**: Generalizable person re-identification aims to learn a model with only several labeled source domains that can perform well on unseen domains. Without access to the unseen domain, the feature statistics of the batch normalization (BN) layer learned from a limited number of source domains is doubtlessly biased for unseen domain. This would mislead the feature representation learning for unseen domain and deteriorate the generalizaiton ability of the model. In this paper, we propose a novel Debiased Batch Normalization via Gaussian Process approach (GDNorm) for generalizable person re-identification, which models the feature statistic estimation from BN layers as a dynamically self-refining Gaussian process to alleviate the bias to unseen domain for improving the generalization. Specifically, we establish a lightweight model with multiple set of domain-specific BN layers to capture the discriminability of individual source domain, and learn the corresponding parameters of the domain-specific BN layers. These parameters of different source domains are employed to deduce a Gaussian process. We randomly sample several paths from this Gaussian process served as the BN estimations of potential new domains outside of existing source domains, which can further optimize these learned parameters from source domains, and estimate more accurate Gaussian process by them in return, tending to real data distribution. Even without a large number of source domains, GDNorm can still provide debiased BN estimation by using the mean path of the Gaussian process, while maintaining low computational cost during testing. Extensive experiments demonstrate that our GDNorm effectively improves the generalization ability of the model on unseen domain.



### Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology
- **Arxiv ID**: http://arxiv.org/abs/2203.01726v3
- **DOI**: 10.1038/s41598-022-21910-0
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01726v3)
- **Published**: 2022-03-03 14:16:22+00:00
- **Updated**: 2022-09-29 12:15:31+00:00
- **Authors**: S. Kyathanahally, T. Hardeman, M. Reyes, E. Merz, T. Bulas, P. Brun, F. Pomati, M. Baity-Jesi
- **Comment**: To appear in Scientific Reports
- **Journal**: Scientific Reports 12, 18590 (2022)
- **Summary**: Monitoring biodiversity is paramount to manage and protect natural resources. Collecting images of organisms over large temporal or spatial scales is a promising practice to monitor the biodiversity of natural ecosystems, providing large amounts of data with minimal interference with the environment. Deep learning models are currently used to automate classification of organisms into taxonomic units. However, imprecision in these classifiers introduces a measurement noise that is difficult to control and can significantly hinder the analysis and interpretation of data. {We overcome this limitation through ensembles of Data-efficient image Transformers (DeiTs), which not only are easy to train and implement, but also significantly outperform} the previous state of the art (SOTA). We validate our results on ten ecological imaging datasets of diverse origin, ranging from plankton to birds. On all the datasets, we achieve a new SOTA, with a reduction of the error with respect to the previous SOTA ranging from 29.35% to 100.00%, and often achieving performances very close to perfect classification. Ensembles of DeiTs perform better not because of superior single-model performances but rather due to smaller overlaps in the predictions by independent models and lower top-1 probabilities. This increases the benefit of ensembling, especially when using geometric averages to combine individual learners. While we only test our approach on biodiversity image datasets, our approach is generic and can be applied to any kind of images.



### Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2203.01730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01730v1)
- **Published**: 2022-03-03 14:20:10+00:00
- **Updated**: 2022-03-03 14:20:10+00:00
- **Authors**: Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang Cui, Zhen Li
- **Comment**: To appear in CVPR2022
- **Journal**: None
- **Summary**: 3D single object tracking (3D SOT) in LiDAR point clouds plays a crucial role in autonomous driving. Current approaches all follow the Siamese paradigm based on appearance matching. However, LiDAR point clouds are usually textureless and incomplete, which hinders effective appearance matching. Besides, previous methods greatly overlook the critical motion clues among targets. In this work, beyond 3D Siamese tracking, we introduce a motion-centric paradigm to handle 3D SOT from a new perspective. Following this paradigm, we propose a matching-free two-stage tracker M^2-Track. At the 1^st-stage, M^2-Track localizes the target within successive frames via motion transformation. Then it refines the target box through motion-assisted shape completion at the 2^nd-stage. Extensive experiments confirm that M^2-Track significantly outperforms previous state-of-the-arts on three large-scale datasets while running at 57FPS (~8%, ~17%, and ~22%) precision gains on KITTI, NuScenes, and Waymo Open Dataset respectively). Further analysis verifies each component's effectiveness and shows the motion-centric paradigm's promising potential when combined with appearance matching.



### Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2203.01735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01735v2)
- **Published**: 2022-03-03 14:26:49+00:00
- **Updated**: 2022-03-16 14:10:26+00:00
- **Authors**: Zhipeng Huang, Jiawei Liu, Liang Li, Kecheng Zheng, Zheng-Jun Zha
- **Comment**: 9 pages, 2 figures, AAAI 2022
- **Journal**: None
- **Summary**: RGB-infrared person re-identification is an emerging cross-modality re-identification task, which is very challenging due to significant modality discrepancy between RGB and infrared images. In this work, we propose a novel modality-adaptive mixup and invariant decomposition (MID) approach for RGB-infrared person re-identification towards learning modality-invariant and discriminative representations. MID designs a modality-adaptive mixup scheme to generate suitable mixed modality images between RGB and infrared images for mitigating the inherent modality discrepancy at the pixel-level. It formulates modality mixup procedure as Markov decision process, where an actor-critic agent learns dynamical and local linear interpolation policy between different regions of cross-modality images under a deep reinforcement learning framework. Such policy guarantees modality-invariance in a more continuous latent space and avoids manifold intrusion by the corrupted mixed modality samples. Moreover, to further counter modality discrepancy and enforce invariant visual semantics at the feature-level, MID employs modality-adaptive convolution decomposition to disassemble a regular convolution layer into modality-specific basis layers and a modality-shared coefficient layer. Extensive experimental results on two challenging benchmarks demonstrate superior performance of MID over state-of-the-art methods.



### PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence
- **Arxiv ID**: http://arxiv.org/abs/2203.01754v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01754v2)
- **Published**: 2022-03-03 15:04:55+00:00
- **Updated**: 2022-04-08 16:19:05+00:00
- **Authors**: Zijian Dong, Chen Guo, Jie Song, Xu Chen, Andreas Geiger, Otmar Hilliges
- **Comment**: CVPR'2022; Video: https://youtu.be/oGpKUuD54Qk | Project page:
  https://zj-dong.github.io/pina/ | Supplementary Material:
  https://ait.ethz.ch/projects/2022/pina/downloads/supp.pdf
- **Journal**: None
- **Summary**: We present a novel method to learn Personalized Implicit Neural Avatars (PINA) from a short RGB-D sequence. This allows non-expert users to create a detailed and personalized virtual copy of themselves, which can be animated with realistic clothing deformations. PINA does not require complete scans, nor does it require a prior learned from large datasets of clothed humans. Learning a complete avatar in this setting is challenging, since only few depth observations are available, which are noisy and incomplete (i.e. only partial visibility of the body per frame). We propose a method to learn the shape and non-rigid deformations via a pose-conditioned implicit surface and a deformation field, defined in canonical space. This allows us to fuse all partial observations into a single consistent canonical representation. Fusion is formulated as a global optimization problem over the pose, shape and skinning parameters. The method can learn neural avatars from real noisy RGB-D sequences for a diverse set of people and clothing styles and these avatars can be animated given unseen motion sequences.



### NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2203.01762v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2203.01762v2)
- **Published**: 2022-03-03 15:13:29+00:00
- **Updated**: 2022-06-18 02:49:47+00:00
- **Authors**: Shanyan Guan, Huayu Deng, Yunbo Wang, Xiaokang Yang
- **Comment**: ICML 2022, the project page: https://syguan96.github.io/NeuroFluid/
- **Journal**: None
- **Summary**: Deep learning has shown great potential for modeling the physical dynamics of complex particle systems such as fluids. Existing approaches, however, require the supervision of consecutive particle properties, including positions and velocities. In this paper, we consider a partially observable scenario known as fluid dynamics grounding, that is, inferring the state transitions and interactions within the fluid particle systems from sequential visual observations of the fluid surface. We propose a differentiable two-stage network named NeuroFluid. Our approach consists of (i) a particle-driven neural renderer, which involves fluid physical properties into the volume rendering function, and (ii) a particle transition model optimized to reduce the differences between the rendered and the observed images. NeuroFluid provides the first solution to unsupervised learning of particle-based fluid dynamics by training these two models jointly. It is shown to reasonably estimate the underlying physics of fluids with different initial shapes, viscosity, and densities.



### A multi-stream convolutional neural network for classification of progressive MCI in Alzheimer's disease using structural MRI images
- **Arxiv ID**: http://arxiv.org/abs/2203.01944v1
- **DOI**: 10.1109/JBHI.2022.3155705
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01944v1)
- **Published**: 2022-03-03 15:14:13+00:00
- **Updated**: 2022-03-03 15:14:13+00:00
- **Authors**: Mona Ashtari-Majlan, Abbas Seifi, Mohammad Mahdi Dehshibi
- **Comment**: None
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 26 (8), 2022,
  pp. 3918 - 3926
- **Summary**: Early diagnosis of Alzheimer's disease and its prodromal stage, also known as mild cognitive impairment (MCI), is critical since some patients with progressive MCI will develop the disease. We propose a multi-stream deep convolutional neural network fed with patch-based imaging data to classify stable MCI and progressive MCI. First, we compare MRI images of Alzheimer's disease with cognitively normal subjects to identify distinct anatomical landmarks using a multivariate statistical test. These landmarks are then used to extract patches that are fed into the proposed multi-stream convolutional neural network to classify MRI images. Next, we train the architecture in a separate scenario using samples from Alzheimer's disease images, which are anatomically similar to the progressive MCI ones and cognitively normal images to compensate for the lack of progressive MCI training data. Finally, we transfer the trained model weights to the proposed architecture in order to fine-tune the model using progressive MCI and stable MCI data. Experimental results on the ADNI-1 dataset indicate that our method outperforms existing methods for MCI classification, with an F1-score of 85.96%.



### Random Quantum Neural Networks (RQNN) for Noisy Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.01764v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01764v1)
- **Published**: 2022-03-03 15:15:29+00:00
- **Updated**: 2022-03-03 15:15:29+00:00
- **Authors**: Debanjan Konar, Erol Gelenbe, Soham Bhandary, Aditya Das Sarma, Attila Cangi
- **Comment**: None
- **Journal**: None
- **Summary**: Classical Random Neural Networks (RNNs) have demonstrated effective applications in decision making, signal processing, and image recognition tasks. However, their implementation has been limited to deterministic digital systems that output probability distributions in lieu of stochastic behaviors of random spiking signals. We introduce the novel class of supervised Random Quantum Neural Networks (RQNNs) with a robust training strategy to better exploit the random nature of the spiking RNN. The proposed RQNN employs hybrid classical-quantum algorithms with superposition state and amplitude encoding features, inspired by quantum information theory and the brain's spatial-temporal stochastic spiking property of neuron information encoding. We have extensively validated our proposed RQNN model, relying on hybrid classical-quantum algorithms via the PennyLane Quantum simulator with a limited number of \emph{qubits}. Experiments on the MNIST, FashionMNIST, and KMNIST datasets demonstrate that the proposed RQNN model achieves an average classification accuracy of $94.9\%$. Additionally, the experimental findings illustrate the proposed RQNN's effectiveness and resilience in noisy settings, with enhanced image classification accuracy when compared to the classical counterparts (RNNs), classical Spiking Neural Networks (SNNs), and the classical convolutional neural network (AlexNet). Furthermore, the RQNN can deal with noise, which is useful for various applications, including computer vision in NISQ devices. The PyTorch code (https://github.com/darthsimpus/RQN) is made available on GitHub to reproduce the results reported in this manuscript.



### Revisiting Click-based Interactive Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01784v2
- **DOI**: 10.1109/ICIP46576.2022.9897460
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01784v2)
- **Published**: 2022-03-03 15:55:14+00:00
- **Updated**: 2022-06-07 15:25:55+00:00
- **Authors**: Stephane Vujasinovic, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen
- **Comment**: ICIP 2022. 5 pages = 4 pages of content + 1 page of references
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2022,
  pp. 2756-2760
- **Summary**: While current methods for interactive Video Object Segmentation (iVOS) rely on scribble-based interactions to generate precise object masks, we propose a Click-based interactive Video Object Segmentation (CiVOS) framework to simplify the required user workload as much as possible. CiVOS builds on de-coupled modules reflecting user interaction and mask propagation. The interaction module converts click-based interactions into an object mask, which is then inferred to the remaining frames by the propagation module. Additional user interactions allow for a refinement of the object mask. The approach is extensively evaluated on the popular interactive~DAVIS dataset, but with an inevitable adaptation of scribble-based interactions with click-based counterparts. We consider several strategies for generating clicks during our evaluation to reflect various user inputs and adjust the DAVIS performance metric to perform a hardware-independent comparison. The presented CiVOS pipeline achieves competitive results, although requiring a lower user workload.



### On Learning Contrastive Representations for Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2203.01785v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01785v3)
- **Published**: 2022-03-03 15:58:05+00:00
- **Updated**: 2022-07-23 21:54:03+00:00
- **Authors**: Li Yi, Sheng Liu, Qi She, A. Ian McLeod, Boyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are able to memorize noisy labels easily with a softmax cross-entropy (CE) loss. Previous studies attempted to address this issue focus on incorporating a noise-robust loss function to the CE loss. However, the memorization issue is alleviated but still remains due to the non-robust CE loss. To address this issue, we focus on learning robust contrastive representations of data on which the classifier is hard to memorize the label noise under the CE loss. We propose a novel contrastive regularization function to learn such representations over noisy data where label noise does not dominate the representation learning. By theoretically investigating the representations induced by the proposed regularization function, we reveal that the learned representations keep information related to true labels and discard information related to corrupted labels. Moreover, our theoretical results also indicate that the learned representations are robust to the label noise. The effectiveness of this method is demonstrated with experiments on benchmark datasets.



### Automatic Facial Paralysis Estimation with Facial Action Units
- **Arxiv ID**: http://arxiv.org/abs/2203.01800v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01800v2)
- **Published**: 2022-03-03 16:14:49+00:00
- **Updated**: 2022-03-30 11:44:37+00:00
- **Authors**: Xuri Ge, Joemon M. Jose, Pengcheng Wang, Arunachalam Iyer, Xiao Liu, Hu Han
- **Comment**: 12 pages, 5 figures, resubmitted to IEEE Transactions on Affective
  Computing
- **Journal**: None
- **Summary**: Facial palsy is unilateral facial nerve weakness or paralysis of rapid onset with unknown causes. Automatically estimating facial palsy severeness can be helpful for the diagnosis and treatment of people suffering from it across the world. In this work, we develop and experiment with a novel model for estimating facial palsy severity. For this, an effective Facial Action Units (AU) detection technique is incorporated into our model, where AUs refer to a unique set of facial muscle movements used to describe almost every anatomically possible facial expression. In this paper, we propose a novel Adaptive Local-Global Relational Network (ALGRNet) for facial AU detection and use it to classify facial paralysis severity. ALGRNet mainly consists of three main novel structures: (i) an adaptive region learning module that learns the adaptive muscle regions based on the detected landmarks; (ii) a skip-BiLSTM that models the latent relationships among local AUs; and (iii) a feature fusion&refining module that investigates the complementary between the local and global face. Quantitative results on two AU benchmarks, i.e., BP4D and DISFA, demonstrate our ALGRNet can achieve promising AU detection accuracy. We further demonstrate the effectiveness of its application to facial paralysis estimation by migrating ALGRNet to a facial paralysis dataset collected and annotated by medical professionals.



### Intensity Image-based LiDAR Fiducial Marker System
- **Arxiv ID**: http://arxiv.org/abs/2203.01816v1
- **DOI**: 10.1109/LRA.2022.3174971
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01816v1)
- **Published**: 2022-03-03 16:25:19+00:00
- **Updated**: 2022-03-03 16:25:19+00:00
- **Authors**: Yibo Liu, Hunter Schofield, Jinjun Shan
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The fiducial marker system for LiDAR is crucial for the robotic application but it is still rare to date. In this paper, an Intensity Image-based LiDAR Fiducial Marker (IILFM) system is developed. This system only requires an unstructured point cloud with intensity as the input and it has no restriction on marker placement and shape. A marker detection method that locates the predefined 3D fiducials in the point cloud through the intensity image is introduced. Then, an approach that utilizes the detected 3D fiducials to estimate the LiDAR 6-DOF pose that describes the transmission from the world coordinate system to the LiDAR coordinate system is developed. Moreover, all these processes run in real-time (approx 40 Hz on Livox Mid-40 and approx 143 Hz on VLP-16). Qualitative and quantitative experiments are conducted to demonstrate that the proposed system has similar convenience and accuracy as the conventional visual fiducial marker system. The codes and results are available at: https://github.com/York-SDCNLab/IILFM.



### LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/2203.01824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01824v2)
- **Published**: 2022-03-03 16:28:10+00:00
- **Updated**: 2022-03-25 16:14:33+00:00
- **Authors**: Zhigang Jiang, Zhongzheng Xiang, Jinhua Xu, Ming Zhao
- **Comment**: To Appear in CVPR 2022
- **Journal**: None
- **Summary**: 3D room layout estimation by a single panorama using deep neural networks has made great progress. However, previous approaches can not obtain efficient geometry awareness of room layout with the only latitude of boundaries or horizon-depth. We present that using horizon-depth along with room height can obtain omnidirectional-geometry awareness of room layout in both horizontal and vertical directions. In addition, we propose a planar-geometry aware loss function with normals and gradients of normals to supervise the planeness of walls and turning of corners. We propose an efficient network, LGT-Net, for room layout estimation, which contains a novel Transformer architecture called SWG-Transformer to model geometry relations. SWG-Transformer consists of (Shifted) Window Blocks and Global Blocks to combine the local and global geometry relations. Moreover, we design a novel relative position embedding of Transformer to enhance the spatial identification ability for the panorama. Experiments show that the proposed LGT-Net achieves better performance than current state-of-the-arts (SOTA) on benchmark datasets.



### STUN: Self-Teaching Uncertainty Estimation for Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.01851v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01851v2)
- **Published**: 2022-03-03 16:59:42+00:00
- **Updated**: 2022-09-13 07:17:10+00:00
- **Authors**: Kaiwen Cai, Chris Xiaoxuan Lu, Xiaowei Huang
- **Comment**: To appear at the 35th IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS2022)
- **Journal**: None
- **Summary**: Place recognition is key to Simultaneous Localization and Mapping (SLAM) and spatial perception. However, a place recognition in the wild often suffers from erroneous predictions due to image variations, e.g., changing viewpoints and street appearance. Integrating uncertainty estimation into the life cycle of place recognition is a promising method to mitigate the impact of variations on place recognition performance. However, existing uncertainty estimation approaches in this vein are either computationally inefficient (e.g., Monte Carlo dropout) or at the cost of dropped accuracy. This paper proposes STUN, a self-teaching framework that learns to simultaneously predict the place and estimate the prediction uncertainty given an input image. To this end, we first train a teacher net using a standard metric learning pipeline to produce embedding priors. Then, supervised by the pretrained teacher net, a student net with an additional variance branch is trained to finetune the embedding priors and estimate the uncertainty sample by sample. During the online inference phase, we only use the student net to generate a place prediction in conjunction with the uncertainty. When compared with place recognition systems that are ignorant to the uncertainty, our framework features the uncertainty estimation for free without sacrificing any prediction accuracy. Our experimental results on the large-scale Pittsburgh30k dataset demonstrate that STUN outperforms the state-of-the-art methods in both recognition accuracy and the quality of uncertainty estimation.



### Efficient Video Instance Segmentation via Tracklet Query and Proposal
- **Arxiv ID**: http://arxiv.org/abs/2203.01853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01853v1)
- **Published**: 2022-03-03 17:00:11+00:00
- **Updated**: 2022-03-03 17:00:11+00:00
- **Authors**: Jialian Wu, Sudhir Yarram, Hui Liang, Tian Lan, Junsong Yuan, Jayan Eledath, Gerard Medioni
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Video Instance Segmentation (VIS) aims to simultaneously classify, segment, and track multiple object instances in videos. Recent clip-level VIS takes a short video clip as input each time showing stronger performance than frame-level VIS (tracking-by-segmentation), as more temporal context from multiple frames is utilized. Yet, most clip-level methods are neither end-to-end learnable nor real-time. These limitations are addressed by the recent VIS transformer (VisTR) which performs VIS end-to-end within a clip. However, VisTR suffers from long training time due to its frame-wise dense attention. In addition, VisTR is not fully end-to-end learnable in multiple video clips as it requires a hand-crafted data association to link instance tracklets between successive clips. This paper proposes EfficientVIS, a fully end-to-end framework with efficient training and inference. At the core are tracklet query and tracklet proposal that associate and segment regions-of-interest (RoIs) across space and time by an iterative query-video interaction. We further propose a correspondence learning that makes tracklets linking between clips end-to-end learnable. Compared to VisTR, EfficientVIS requires 15x fewer training epochs while achieving state-of-the-art accuracy on the YouTube-VIS benchmark. Meanwhile, our method enables whole video instance segmentation in a single end-to-end pass without data association at all.



### A study on the distribution of social biases in self-supervised learning visual models
- **Arxiv ID**: http://arxiv.org/abs/2203.01854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01854v1)
- **Published**: 2022-03-03 17:03:21+00:00
- **Updated**: 2022-03-03 17:03:21+00:00
- **Authors**: Kirill Sirotkin, Pablo Carballeira, Marcos Escudero-Violo
- **Comment**: 10 pages, 6 figures, accepted in CVPR2022
- **Journal**: None
- **Summary**: Deep neural networks are efficient at learning the data distribution if it is sufficiently sampled. However, they can be strongly biased by non-relevant factors implicitly incorporated in the training data. These include operational biases, such as ineffective or uneven data sampling, but also ethical concerns, as the social biases are implicitly present\textemdash even inadvertently, in the training data or explicitly defined in unfair training schedules. In tasks having impact on human processes, the learning of social biases may produce discriminatory, unethical and untrustworthy consequences. It is often assumed that social biases stem from supervised learning on labelled data, and thus, Self-Supervised Learning (SSL) wrongly appears as an efficient and bias-free solution, as it does not require labelled data. However, it was recently proven that a popular SSL method also incorporates biases. In this paper, we study the biases of a varied set of SSL visual models, trained using ImageNet data, using a method and dataset designed by psychological experts to measure social biases. We show that there is a correlation between the type of the SSL model and the number of biases that it incorporates. Furthermore, the results also suggest that this number does not strictly depend on the model's accuracy and changes throughout the network. Finally, we conclude that a careful SSL model selection process can reduce the number of social biases in the deployed model, whilst keeping high performance.



### Robustness and Adaptation to Hidden Factors of Variation
- **Arxiv ID**: http://arxiv.org/abs/2203.01864v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01864v1)
- **Published**: 2022-03-03 17:20:28+00:00
- **Updated**: 2022-03-03 17:20:28+00:00
- **Authors**: William Paul, Philippe Burlina
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle here a specific, still not widely addressed aspect, of AI robustness, which consists of seeking invariance / insensitivity of model performance to hidden factors of variations in the data. Towards this end, we employ a two step strategy that a) does unsupervised discovery, via generative models, of sensitive factors that cause models to under-perform, and b) intervenes models to make their performance invariant to these sensitive factors' influence. We consider 3 separate interventions for robustness, including: data augmentation, semantic consistency, and adversarial alignment. We evaluate our method using metrics that measure trade offs between invariance (insensitivity) and overall performance (utility) and show the benefits of our method for 3 settings (unsupervised, semi-supervised and generalization).



### LatentFormer: Multi-Agent Transformer-Based Interaction Modeling and Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.01880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01880v1)
- **Published**: 2022-03-03 17:44:58+00:00
- **Updated**: 2022-03-03 17:44:58+00:00
- **Authors**: Elmira Amirloo, Amir Rasouli, Peter Lakner, Mohsen Rohani, Jun Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-agent trajectory prediction is a fundamental problem in autonomous driving. The key challenges in prediction are accurately anticipating the behavior of surrounding agents and understanding the scene context. To address these problems, we propose LatentFormer, a transformer-based model for predicting future vehicle trajectories. The proposed method leverages a novel technique for modeling interactions among dynamic objects in the scene. Contrary to many existing approaches which model cross-agent interactions during the observation time, our method additionally exploits the future states of the agents. This is accomplished using a hierarchical attention mechanism where the evolving states of the agents autoregressively control the contributions of past trajectories and scene encodings in the final prediction. Furthermore, we propose a multi-resolution map encoding scheme that relies on a vision transformer module to effectively capture both local and global scene context to guide the generation of more admissible future trajectories. We evaluate the proposed method on the nuScenes benchmark dataset and show that our approach achieves state-of-the-art performance and improves upon trajectory metrics by up to 40%. We further investigate the contributions of various components of the proposed technique via extensive ablation studies.



### Panoptic segmentation with highly imbalanced semantic labels
- **Arxiv ID**: http://arxiv.org/abs/2203.11692v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.11692v4)
- **Published**: 2022-03-03 17:46:21+00:00
- **Updated**: 2022-04-19 08:37:38+00:00
- **Authors**: Josef Lorenz Rumberger, Elias Baumann, Peter Hirsch, Andrew Janowczyk, Inti Zlobec, Dagmar Kainmueller
- **Comment**: None
- **Journal**: None
- **Summary**: We describe here the panoptic segmentation method we devised for our participation in the CoNIC: Colon Nuclei Identification and Counting Challenge at ISBI 2022. Key features of our method are a weighted loss specifically engineered for semantic segmentation of highly imbalanced cell types, and a state-of-the art nuclei instance segmentation model, which we combine in a Hovernet-like architecture.



### Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features
- **Arxiv ID**: http://arxiv.org/abs/2203.01881v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01881v4)
- **Published**: 2022-03-03 17:48:23+00:00
- **Updated**: 2023-05-04 21:49:21+00:00
- **Authors**: Neha Kalibhat, Kanika Narang, Hamed Firooz, Maziar Sanjabi, Soheil Feizi
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on any pre-trained self-supervised model to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear classification performance of state-of-the-art self-supervised models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and enhancing these features through Q-score regularization makes representations more interpretable across all self-supervised models.



### DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae
- **Arxiv ID**: http://arxiv.org/abs/2203.01882v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01882v3)
- **Published**: 2022-03-03 17:49:40+00:00
- **Updated**: 2022-03-21 09:02:21+00:00
- **Authors**: Juan P. Vigueras-Guilln, Jeroen van Rooij, Bart T. H. van Dooren, Hans G. Lemij, Esma Islamaj, Lucas J. van Vliet, Koenraad A. Vermeer
- **Comment**: 9 pages, 7 figures, 2 tables. Code:
  https://github.com/jpviguerasguillen/feedback-non-local-attention-fNLA
- **Journal**: None
- **Summary**: To estimate the corneal endothelial parameters from specular microscopy images depicting cornea guttata (Fuchs dystrophy), we propose a new deep learning methodology that includes a novel attention mechanism named feedback non-local attention (fNLA). Our approach first infers the cell edges, then selects the cells that are well detected, and finally applies a postprocessing method to correct mistakes and provide the binary segmentation from which the corneal parameters are estimated (cell density [ECD], coefficient of variation [CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which contained guttae. Manual segmentation was performed in all images. We compared the results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with fNLA provided the best performance, with a mean absolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6 times smaller than the error obtained by Topcon's built-in software. Our approach handled the cells affected by guttae remarkably well, detecting cell edges occluded by small guttae while discarding areas covered by large guttae. Overall, the proposed method obtained accurate estimations in extremely challenging specular images.



### ROCT-Net: A new ensemble deep convolutional model with improved spatial resolution learning for detecting common diseases from retinal OCT images
- **Arxiv ID**: http://arxiv.org/abs/2203.01883v1
- **DOI**: 10.1109/ICCKE54056.2021.9721471
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01883v1)
- **Published**: 2022-03-03 17:51:01+00:00
- **Updated**: 2022-03-03 17:51:01+00:00
- **Authors**: Mohammad Rahimzadeh, Mahmoud Reza Mohammadi
- **Comment**: This is a preprint of an article published in the ICCKE 2021
  conference. The final authenticated version is available online at
  https://doi.org/10.1109/ICCKE54056.2021.9721471. The code of this paper is
  shared at https://github.com/mr7495/OCT-classification
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) imaging is a well-known technology for visualizing retinal layers and helps ophthalmologists to detect possible diseases. Accurate and early diagnosis of common retinal diseases can prevent the patients from suffering critical damages to their vision. Computer-aided diagnosis (CAD) systems can significantly assist ophthalmologists in improving their examinations. This paper presents a new enhanced deep ensemble convolutional neural network for detecting retinal diseases from OCT images. Our model generates rich and multi-resolution features by employing the learning architectures of two robust convolutional models. Spatial resolution is a critical factor in medical images, especially the OCT images that contain tiny essential points. To empower our model, we apply a new post-architecture model to our ensemble model for enhancing spatial resolution learning without increasing computational costs. The introduced post-architecture model can be deployed to any feature extraction model to improve the utilization of the feature map's spatial values. We have collected two open-source datasets for our experiments to make our models capable of detecting six crucial retinal diseases: Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), Diabetic Retinopathy (DR), Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen alongside the normal cases. Our experiments on two datasets and comparing our model with some other well-known deep convolutional neural networks have proven that our architecture can increase the classification accuracy up to 5%. We hope that our proposed methods create the next step of CAD systems development and help future researches. The code of this paper is shared at https://github.com/mr7495/OCT-classification.



### TCTrack: Temporal Contexts for Aerial Tracking
- **Arxiv ID**: http://arxiv.org/abs/2203.01885v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01885v3)
- **Published**: 2022-03-03 18:04:20+00:00
- **Updated**: 2022-03-28 07:35:29+00:00
- **Authors**: Ziang Cao, Ziyuan Huang, Liang Pan, Shiwei Zhang, Ziwei Liu, Changhong Fu
- **Comment**: To appear in CVPR2022. Code:
  https://github.com/vision4robotics/TCTrack
- **Journal**: None
- **Summary**: Temporal contexts among consecutive frames are far from being fully utilized in existing visual trackers. In this work, we present TCTrack, a comprehensive framework to fully exploit temporal contexts for aerial tracking. The temporal contexts are incorporated at \textbf{two levels}: the extraction of \textbf{features} and the refinement of \textbf{similarity maps}. Specifically, for feature extraction, an online temporally adaptive convolution is proposed to enhance the spatial features using temporal information, which is achieved by dynamically calibrating the convolution weights according to the previous frames. For similarity map refinement, we propose an adaptive temporal transformer, which first effectively encodes temporal knowledge in a memory-efficient way, before the temporal knowledge is decoded for accurate adjustment of the similarity map. TCTrack is effective and efficient: evaluation on four aerial tracking benchmarks shows its impressive performance; real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX Xavier.



### Instance Segmentation for Autonomous Log Grasping in Forestry Operations
- **Arxiv ID**: http://arxiv.org/abs/2203.01902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01902v2)
- **Published**: 2022-03-03 18:29:25+00:00
- **Updated**: 2022-10-18 21:57:32+00:00
- **Authors**: Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Franois Pomerleau, Philippe Gigure
- **Comment**: 8 pages, 6 figures, accepted at IROS 2022
- **Journal**: None
- **Summary**: Wood logs picking is a challenging task to automate. Indeed, logs usually come in cluttered configurations, randomly orientated and overlapping. Recent work on log picking automation usually assume that the logs' pose is known, with little consideration given to the actual perception problem. In this paper, we squarely address the latter, using a data-driven approach. First, we introduce a novel dataset, named TimberSeg 1.0, that is densely annotated, i.e., that includes both bounding boxes and pixel-level mask annotations for logs. This dataset comprises 220 images with 2500 individually segmented logs. Using our dataset, we then compare three neural network architectures on the task of individual logs detection and segmentation; two region-based methods and one attention-based method. Unsurprisingly, our results show that axis-aligned proposals, failing to take into account the directional nature of logs, underperform with 19.03 mAP. A rotation-aware proposal method significantly improve results to 31.83 mAP. More interestingly, a Transformer-based approach, without any inductive bias on rotations, outperformed the two others, achieving a mAP of 57.53 on our dataset. Our use case demonstrates the limitations of region-based approaches for cluttered, elongated objects. It also highlights the potential of attention-based methods on this specific task, as they work directly at the pixel-level. These encouraging results indicate that such a perception system could be used to assist the operators on the short-term, or to fully automate log picking operations in the future.



### Computer Vision Aided Blockage Prediction in Real-World Millimeter Wave Deployments
- **Arxiv ID**: http://arxiv.org/abs/2203.01907v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01907v1)
- **Published**: 2022-03-03 18:38:10+00:00
- **Updated**: 2022-03-03 18:38:10+00:00
- **Authors**: Gouranga Charan, Ahmed Alkhateeb
- **Comment**: The dataset and code files will be available on the DeepSense 6G
  dataset website https://deepsense6g.net/
- **Journal**: None
- **Summary**: This paper provides the first real-world evaluation of using visual (RGB camera) data and machine learning for proactively predicting millimeter wave (mmWave) dynamic link blockages before they happen. Proactively predicting line-of-sight (LOS) link blockages enables mmWave/sub-THz networks to make proactive network management decisions, such as proactive beam switching and hand-off) before a link failure happens. This can significantly enhance the network reliability and latency while efficiently utilizing the wireless resources. To evaluate this gain in reality, this paper (i) develops a computer vision based solution that processes the visual data captured by a camera installed at the infrastructure node and (ii) studies the feasibility of the proposed solution based on the large-scale real-world dataset, DeepSense 6G, that comprises multi-modal sensing and communication data. Based on the adopted real-world dataset, the developed solution achieves $\approx 90\%$ accuracy in predicting blockages happening within the future $0.1$s and $\approx 80\%$ for blockages happening within $1$s, which highlights a promising solution for mmWave/sub-THz communication networks.



### NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2203.01913v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01913v2)
- **Published**: 2022-03-03 18:49:57+00:00
- **Updated**: 2022-04-27 16:55:51+00:00
- **Authors**: Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, Phillip Isola
- **Comment**: ICRA 2022, Website: https://yenchenlin.me/nerf-supervision/
- **Journal**: None
- **Summary**: Thin, reflective objects such as forks and whisks are common in our daily lives, but they are particularly challenging for robot perception because it is hard to reconstruct them using commodity RGB-D cameras or multi-view stereo techniques. While traditional pipelines struggle with objects like these, Neural Radiance Fields (NeRFs) have recently been shown to be remarkably effective for performing view synthesis on objects with thin structures or reflective materials. In this paper we explore the use of NeRF as a new source of supervision for robust robot vision systems. In particular, we demonstrate that a NeRF representation of a scene can be used to train dense object descriptors. We use an optimized NeRF to extract dense correspondences between multiple views of an object, and then use these correspondences as training data for learning a view-invariant representation of the object. NeRF's usage of a density field allows us to reformulate the correspondence problem with a novel distribution-of-depths formulation, as opposed to the conventional approach of using a depth map. Dense correspondence models supervised with our method significantly outperform off-the-shelf learned descriptors by 106% (PCK@3px metric, more than doubling performance) and outperform our baseline supervised with multi-view stereo by 29%. Furthermore, we demonstrate the learned dense descriptors enable robots to perform accurate 6-degree of freedom (6-DoF) pick and place of thin and reflective objects.



### Playable Environments: Video Manipulation in Space and Time
- **Arxiv ID**: http://arxiv.org/abs/2203.01914v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.01914v2)
- **Published**: 2022-03-03 18:51:05+00:00
- **Updated**: 2022-03-15 18:13:26+00:00
- **Authors**: Willi Menapace, Stphane Lathuilire, Aliaksandr Siarohin, Christian Theobalt, Sergey Tulyakov, Vladislav Golyanik, Elisa Ricci
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: We present Playable Environments - a new representation for interactive video generation and manipulation in space and time. With a single image at inference time, our novel framework allows the user to move objects in 3D while generating a video by providing a sequence of desired actions. The actions are learnt in an unsupervised manner. The camera can be controlled to get the desired viewpoint. Our method builds an environment state for each frame, which can be manipulated by our proposed action module and decoded back to the image space with volumetric rendering. To support diverse appearances of objects, we extend neural radiance fields with style-based modulation. Our method trains on a collection of various monocular videos requiring only the estimated camera parameters and 2D object locations. To set a challenging benchmark, we introduce two large scale video datasets with significant camera movements. As evidenced by our experiments, playable environments enable several creative applications not attainable by prior video synthesis works, including playable 3D video generation, stylization and manipulation. Further details, code and examples are available at https://willi-menapace.github.io/playable-environments-website



### Investigating the limited performance of a deep-learning-based SPECT denoising approach: An observer-study-based characterization
- **Arxiv ID**: http://arxiv.org/abs/2203.01918v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01918v1)
- **Published**: 2022-03-03 18:51:59+00:00
- **Updated**: 2022-03-03 18:51:59+00:00
- **Authors**: Zitong Yu, Md Ashequr Rahman, Abhinav K. Jha
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple objective assessment of image-quality-based studies have reported that several deep-learning-based denoising methods show limited performance on signal-detection tasks. Our goal was to investigate the reasons for this limited performance. To achieve this goal, we conducted a task-based characterization of a DL-based denoising approach for individual signal properties. We conducted this study in the context of evaluating a DL-based approach for denoising SPECT images. The training data consisted of signals of different sizes and shapes within a clustered-lumpy background, imaged with a 2D parallel-hole-collimator SPECT system. The projections were generated at normal and 20% low count level, both of which were reconstructed using an OSEM algorithm. A CNN-based denoiser was trained to process the low-count images. The performance of this CNN was characterized for five different signal sizes and four different SBR by designing each evaluation as an SKE/BKS signal-detection task. Performance on this task was evaluated using an anthropomorphic CHO. As in previous studies, we observed that the DL-based denoising method did not improve performance on signal-detection tasks. Evaluation using the idea of observer-study-based characterization demonstrated that the DL-based denoising approach did not improve performance on the signal-detection task for any of the signal types. Overall, these results provide new insights on the performance of the DL-based denoising approach as a function of signal size and contrast. More generally, the observer study-based characterization provides a mechanism to evaluate the sensitivity of the method to specific object properties and may be explored as analogous to characterizations such as modulation transfer function for linear systems. Finally, this work underscores the need for objective task-based evaluation of DL-based denoising approaches.



### NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification
- **Arxiv ID**: http://arxiv.org/abs/2203.01921v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01921v2)
- **Published**: 2022-03-03 18:53:49+00:00
- **Updated**: 2022-03-04 20:12:12+00:00
- **Authors**: Shreyas Fadnavis, Jens Sjlund, Anders Eklund, Eleftherios Garyfallidis
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue micro-architecture, which can, in turn, be used to reconstruct tissue microstructure and white matter pathways. The accuracy of such tasks is hampered by the low signal-to-noise ratio in dMRI. Today, the noise is characterized mainly by visual inspection of residual maps and estimated standard deviation. However, it is hard to estimate the impact of noise on downstream tasks based only on such qualitative assessments. To address this issue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for quantitative image quality analysis in the absence of a ground truth reference image. NUQ uses a recent Bayesian formulation of dMRI models to estimate the uncertainty of microstructural measures. Specifically, NUQ uses the maximum mean discrepancy metric to compute a pooled quality score by comparing samples drawn from the posterior distribution of the microstructure measures. We show that NUQ allows a fine-grained analysis of noise, capturing details that are visually imperceptible. We perform qualitative and quantitative comparisons on real datasets, showing that NUQ generates consistent scores across different denoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics and controls, we quantify the substantial impact of denoising on group differences.



### Vision-Language Intelligence: Tasks, Representation Learning, and Large Models
- **Arxiv ID**: http://arxiv.org/abs/2203.01922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.01922v1)
- **Published**: 2022-03-03 18:54:59+00:00
- **Updated**: 2022-03-03 18:54:59+00:00
- **Authors**: Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, PengChuan Zhang, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a comprehensive survey of vision-language (VL) intelligence from the perspective of time. This survey is inspired by the remarkable progress in both computer vision and natural language processing, and recent trends shifting from single modality processing to multiple modality comprehension. We summarize the development in this field into three time periods, namely task-specific methods, vision-language pre-training (VLP) methods, and larger models empowered by large-scale weakly-labeled data. We first take some common VL tasks as examples to introduce the development of task-specific methods. Then we focus on VLP methods and comprehensively review key components of the model structures and training methods. After that, we show how recent work utilizes large-scale raw image-text data to learn language-aligned visual representations that generalize better on zero or few shot learning tasks. Finally, we discuss some potential future trends towards modality cooperation, unified representation, and knowledge incorporation. We believe that this review will be of help for researchers and practitioners of AI and ML, especially those interested in computer vision and natural language processing.



### Recovering 3D Human Mesh from Monocular Images: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2203.01923v5
- **DOI**: 10.1109/TPAMI.2023.3298850
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.01923v5)
- **Published**: 2022-03-03 18:56:08+00:00
- **Updated**: 2023-08-25 07:30:32+00:00
- **Authors**: Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang
- **Comment**: Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,
  Project page: https://github.com/tinatiansjz/hmr-survey
- **Journal**: None
- **Summary**: Estimating human pose and shape from monocular images is a long-standing problem in computer vision. Since the release of statistical body models, 3D human mesh recovery has been drawing broader attention. With the same goal of obtaining well-aligned and physically plausible mesh results, two paradigms have been developed to overcome challenges in the 2D-to-3D lifting process: i) an optimization-based paradigm, where different data terms and regularization terms are exploited as optimization objectives; and ii) a regression-based paradigm, where deep learning techniques are embraced to solve the problem in an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving the quality of 3D mesh labels for a wide range of datasets. Though remarkable progress has been achieved in the past decade, the task is still challenging due to flexible body motions, diverse appearances, complex environments, and insufficient in-the-wild annotations. To the best of our knowledge, this is the first survey that focuses on the task of monocular 3D human mesh recovery. We start with the introduction of body models and then elaborate recovery frameworks and training objectives by providing in-depth analyses of their strengths and weaknesses. We also summarize datasets, evaluation metrics, and benchmark results. Open issues and future directions are discussed in the end, hoping to motivate researchers and facilitate their research in this area. A regularly updated project page can be found at https://github.com/tinatiansjz/hmr-survey.



### Label-Only Model Inversion Attacks via Boundary Repulsion
- **Arxiv ID**: http://arxiv.org/abs/2203.01925v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01925v1)
- **Published**: 2022-03-03 18:57:57+00:00
- **Updated**: 2022-03-03 18:57:57+00:00
- **Authors**: Mostafa Kahla, Si Chen, Hoang Anh Just, Ruoxi Jia
- **Comment**: Accepted at CVPR2022
- **Journal**: None
- **Summary**: Recent studies show that the state-of-the-art deep neural networks are vulnerable to model inversion attacks, in which access to a model is abused to reconstruct private training data of any given target class. Existing attacks rely on having access to either the complete target model (whitebox) or the model's soft-labels (blackbox). However, no prior work has been done in the harder but more practical scenario, in which the attacker only has access to the model's predicted label, without a confidence measure. In this paper, we introduce an algorithm, Boundary-Repelling Model Inversion (BREP-MI), to invert private training data using only the target model's predicted labels. The key idea of our algorithm is to evaluate the model's predicted labels over a sphere and then estimate the direction to reach the target class's centroid. Using the example of face recognition, we show that the images reconstructed by BREP-MI successfully reproduce the semantics of the private training data for various datasets and target model architectures. We compare BREP-MI with the state-of-the-art whitebox and blackbox model inversion attacks and the results show that despite assuming less knowledge about the target model, BREP-MI outperforms the blackbox attack and achieves comparable results to the whitebox attack.



### CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.01929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01929v1)
- **Published**: 2022-03-03 18:59:04+00:00
- **Updated**: 2022-03-03 18:59:04+00:00
- **Authors**: Muhammad Zubair Irshad, Thomas Kollar, Michael Laskey, Kevin Stone, Zsolt Kira
- **Comment**: Accepted to ICRA 2022, Project page with videos:
  https://zubair-irshad.github.io/projects/CenterSnap.html
- **Journal**: None
- **Summary**: This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instance-level pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple one-stage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this per-pixel representation, our approach can reconstruct in real-time (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach significantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6% absolute improvement in mAP for 6D pose for novel real-world object instances.



### Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining
- **Arxiv ID**: http://arxiv.org/abs/2203.01969v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01969v3)
- **Published**: 2022-03-03 19:18:28+00:00
- **Updated**: 2023-01-04 21:56:42+00:00
- **Authors**: Benjamin Billot, Magdamo Colin, Sean E. Arnold, Sudeshna Das, Juan. E. Iglesias
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Retrospective analysis of brain MRI scans acquired in the clinic has the potential to enable neuroimaging studies with sample sizes much larger than those found in research datasets. However, analysing such clinical images "in the wild" is challenging, since subjects are scanned with highly variable protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent advances in convolutional neural networks (CNNs) and domain randomisation for image segmentation, best represented by the publicly available method SynthSeg, may enable morphometry of clinical MRI at scale. In this work, we first evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000 scans acquired at Massachusetts General Hospital. We show that SynthSeg is generally robust, but frequently falters on scans with low signal-to-noise ratio or poor tissue contrast. Next, we propose SynthSeg+, a novel method that greatly mitigates these problems using a hierarchy of conditional segmentation and denoising CNNs. We show that this method is considerably more robust than SynthSeg, while also outperforming cascaded networks and state-of-the-art segmentation denoising methods. Finally, we apply our approach to a proof-of-concept volumetric study of ageing, where it closely replicates atrophy patterns observed in research studies conducted on high-quality, 1mm, T1-weighted scans. The code and trained model are publicly available at https://github.com/BBillot/SynthSeg.



### Towards Rich, Portable, and Large-Scale Pedestrian Data Collection
- **Arxiv ID**: http://arxiv.org/abs/2203.01974v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01974v1)
- **Published**: 2022-03-03 19:28:10+00:00
- **Updated**: 2022-03-03 19:28:10+00:00
- **Authors**: Allan Wang, Abhijat Biswas, Henny Admoni, Aaron Steinfeld
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Recently, pedestrian behavior research has shifted towards machine learning based methods and converged on the topic of modeling pedestrian interactions. For this, a large-scale dataset that contains rich information is needed. We propose a data collection system that is portable, which facilitates accessible large-scale data collection in diverse environments. We also couple the system with a semi-autonomous labeling pipeline for fast trajectory label production. We demonstrate the effectiveness of our system by further introducing a dataset we have collected -- the TBD pedestrian dataset. Compared with existing pedestrian datasets, our dataset contains three components: human verified labels grounded in the metric space, a combination of top-down and perspective views, and naturalistic human behavior in the presence of a socially appropriate "robot". In addition, the TBD pedestrian dataset is larger in quantity compared to similar existing datasets and contains unique pedestrian behavior.



### Audio-Visual Object Classification for Human-Robot Collaboration
- **Arxiv ID**: http://arxiv.org/abs/2203.01977v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2203.01977v1)
- **Published**: 2022-03-03 19:37:12+00:00
- **Updated**: 2022-03-03 19:37:12+00:00
- **Authors**: A. Xompero, Y. L. Pang, T. Patten, A. Prabhakar, B. Calli, A. Cavallaro
- **Comment**: 5 pages, 2 figures, 1 table; accepted at ICASSP 2022; Challenge
  webpage, see https://corsmal.eecs.qmul.ac.uk/challenge.html
- **Journal**: None
- **Summary**: Human-robot collaboration requires the contactless estimation of the physical properties of containers manipulated by a person, for example while pouring content in a cup or moving a food box. Acoustic and visual signals can be used to estimate the physical properties of such objects, which may vary substantially in shape, material and size, and also be occluded by the hands of the person. To facilitate comparisons and stimulate progress in solving this problem, we present the CORSMAL challenge and a dataset to assess the performance of the algorithms through a set of well-defined performance scores. The tasks of the challenge are the estimation of the mass, capacity, and dimensions of the object (container), and the classification of the type and amount of its content. A novel feature of the challenge is our real-to-simulation framework for visualising and assessing the impact of estimation errors in human-to-robot handovers.



### Region-of-Interest Based Neural Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2203.01978v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01978v2)
- **Published**: 2022-03-03 19:37:52+00:00
- **Updated**: 2022-11-02 15:32:39+00:00
- **Authors**: Yura Perugachi-Diaz, Guillaume Sautire, Davide Abati, Yang Yang, Amirhossein Habibian, Taco S Cohen
- **Comment**: Updated arxiv version to the camera-ready version after acceptance at
  British Machine Vision Conference (BMVC) 2022
- **Journal**: None
- **Summary**: Humans do not perceive all parts of a scene with the same resolution, but rather focus on few regions of interest (ROIs). Traditional Object-Based codecs take advantage of this biological intuition, and are capable of non-uniform allocation of bits in favor of salient regions, at the expense of increased distortion the remaining areas: such a strategy allows a boost in perceptual quality under low rate constraints. Recently, several neural codecs have been introduced for video compression, yet they operate uniformly over all spatial locations, lacking the capability of ROI-based processing. In this paper, we introduce two models for ROI-based neural video coding. First, we propose an implicit model that is fed with a binary ROI mask and it is trained by de-emphasizing the distortion of the background. Secondly, we design an explicit latent scaling method, that allows control over the quantization binwidth for different spatial regions of latent variables, conditioned on the ROI mask. By extensive experiments, we show that our methods outperform all our baselines in terms of Rate-Distortion (R-D) performance in the ROI. Moreover, they can generalize to different datasets and to any arbitrary ROI at inference time. Finally, they do not require expensive pixel-level annotations during training, as synthetic ROI masks can be used with little to no degradation in performance. To the best of our knowledge, our proposals are the first solutions that integrate ROI-based capabilities into neural video compression models.



### Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values
- **Arxiv ID**: http://arxiv.org/abs/2203.01993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01993v2)
- **Published**: 2022-03-03 20:16:49+00:00
- **Updated**: 2022-05-06 08:45:47+00:00
- **Authors**: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk
- **Comment**: 20 pages, 16 figures, CVPR 2022 Oral, Camera Ready
- **Journal**: None
- **Summary**: We present Polarity Sampling, a theoretically justified plug-and-play method for controlling the generation quality and diversity of pre-trained deep generative networks DGNs). Leveraging the fact that DGNs are, or can be approximated by, continuous piecewise affine splines, we derive the analytical DGN output space distribution as a function of the product of the DGN's Jacobian singular values raised to a power $\rho$. We dub $\rho$ the $\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on the modes ($\rho < 0$) or anti-modes ($\rho > 0$) of the DGN output-space distribution. We demonstrate that nonzero polarity values achieve a better precision-recall (quality-diversity) Pareto frontier than standard methods, such as truncation, for a number of state-of-the-art DGNs. We also present quantitative and qualitative results on the improvement of overall generation quality (e.g., in terms of the Frechet Inception Distance) for a number of state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different conditional and unconditional image generation tasks. In particular, Polarity Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-samp



### Fast Neural Architecture Search for Lightweight Dense Prediction Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.01994v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01994v3)
- **Published**: 2022-03-03 20:17:10+00:00
- **Updated**: 2022-03-09 17:14:50+00:00
- **Authors**: Lam Huynh, Esa Rahtu, Jiri Matas, Janne Heikkila
- **Comment**: 15 pages, 11 figures, 8 tables. arXiv admin note: substantial text
  overlap with arXiv:2108.11105
- **Journal**: None
- **Summary**: We present LDP, a lightweight dense prediction neural architecture search (NAS) framework. Starting from a pre-defined generic backbone, LDP applies the novel Assisted Tabu Search for efficient architecture exploration. LDP is fast and suitable for various dense estimation problems, unlike previous NAS methods that are either computational demanding or deployed only for a single subtask. The performance of LPD is evaluated on monocular depth estimation, semantic segmentation, and image super-resolution tasks on diverse datasets, including NYU-Depth-v2, KITTI, Cityscapes, COCO-stuff, DIV2K, Set5, Set14, BSD100, Urban100. Experiments show that the proposed framework yields consistent improvements on all tested dense prediction tasks, while being $5\%-315\%$ more compact in terms of the number of model parameters than prior arts.



### Counting Molecules: Python based scheme for automated enumeration and categorization of molecules in scanning tunneling microscopy images
- **Arxiv ID**: http://arxiv.org/abs/2203.01998v1
- **DOI**: 10.1016/j.simpa.2022.100301
- **Categories**: **cond-mat.mes-hall**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01998v1)
- **Published**: 2022-03-03 20:27:45+00:00
- **Updated**: 2022-03-03 20:27:45+00:00
- **Authors**: Jack Hellerstedt, Ale Cahlk, Martin vec, Oleksandr Stetsovych, Tyler Hennen
- **Comment**: None
- **Journal**: None
- **Summary**: Scanning tunneling and atomic force microscopies (STM/nc-AFM) are rapidly progressing to offer unprecedented spatial resolution of a diverse array of chemical species. In particular, they are employed to characterize on-surface chemical reactions by directly examining precursors and products. Chiral effects and self-assembled structures can also be investigated. This open source, modular, python based scheme automates the categorization of a variety of molecules present in medium sized (10$\times$10 to 100$\times$100 nm) scanned probe images.



### Why adversarial training can hurt robust accuracy
- **Arxiv ID**: http://arxiv.org/abs/2203.02006v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.02006v1)
- **Published**: 2022-03-03 20:41:38+00:00
- **Updated**: 2022-03-03 20:41:38+00:00
- **Authors**: Jacob Clarysse, Julia Hrrmann, Fanny Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite may be true -- Even though adversarial training helps when enough data is available, it may hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Our proof provides explanatory insights that may also transfer to feature learning models. Further, we observe in experiments on standard image datasets that the same behavior occurs for perceptible attacks that effectively reduce class information such as mask attacks and object corruptions.



### DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations
- **Arxiv ID**: http://arxiv.org/abs/2203.02013v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02013v1)
- **Published**: 2022-03-03 20:52:47+00:00
- **Updated**: 2022-03-03 20:52:47+00:00
- **Authors**: Yiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan Salakhutdinov, Louis-Philippe Morency
- **Comment**: Code available at https://github.com/lvyiwei1/DIME
- **Journal**: None
- **Summary**: The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. Code for our experiments can be found at https://github.com/lvyiwei1/DIME.



### Anomaly Detection-Inspired Few-Shot Medical Image Segmentation Through Self-Supervision With Supervoxels
- **Arxiv ID**: http://arxiv.org/abs/2203.02048v1
- **DOI**: 10.1016/j.media.2022.102385
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02048v1)
- **Published**: 2022-03-03 22:36:39+00:00
- **Updated**: 2022-03-03 22:36:39+00:00
- **Authors**: Stine Hansen, Srishti Gautam, Robert Jenssen, Michael Kampffmeyer
- **Comment**: Accepted in Medical Image Analysis
- **Journal**: None
- **Summary**: Recent work has shown that label-efficient few-shot learning through self-supervision can achieve promising medical image segmentation results. However, few-shot segmentation models typically rely on prototype representations of the semantic classes, resulting in a loss of local information that can degrade performance. This is particularly problematic for the typically large and highly heterogeneous background class in medical image segmentation problems. Previous works have attempted to address this issue by learning additional prototypes for each class, but since the prototypes are based on a limited number of slices, we argue that this ad-hoc solution is insufficient to capture the background properties. Motivated by this, and the observation that the foreground class (e.g., one organ) is relatively homogeneous, we propose a novel anomaly detection-inspired approach to few-shot medical image segmentation in which we refrain from modeling the background explicitly. Instead, we rely solely on a single foreground prototype to compute anomaly scores for all query pixels. The segmentation is then performed by thresholding these anomaly scores using a learned threshold. Assisted by a novel self-supervision task that exploits the 3D structure of medical images through supervoxels, our proposed anomaly detection-inspired few-shot medical image segmentation model outperforms previous state-of-the-art approaches on two representative MRI datasets for the tasks of abdominal organ segmentation and cardiac segmentation.



### Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.02053v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.02053v2)
- **Published**: 2022-03-03 22:53:54+00:00
- **Updated**: 2022-10-19 20:39:13+00:00
- **Authors**: Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, James Zou
- **Comment**: Published at NeurIPS 2022. Code and data are available at
  https://modalitygap.readthedocs.io/
- **Journal**: None
- **Summary**: We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (e.g. images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone. As a consequence, in a multi-modal model with two encoders, the representations of the two modalities are clearly apart when the model is initialized. During optimization, contrastive learning keeps the different modalities separate by a certain distance, which is influenced by the temperature parameter in the loss function. Our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model's downstream zero-shot classification performance and fairness. Our code and data are available at https://modalitygap.readthedocs.io/



### Sim2Real Instance-Level Style Transfer for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.02069v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.02069v1)
- **Published**: 2022-03-03 23:46:47+00:00
- **Updated**: 2022-03-03 23:46:47+00:00
- **Authors**: Takuya Ikeda, Suomi Tanishige, Ayako Amma, Michael Sudano, Herv Audren, Koichi Nishiwaki
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: In recent years, synthetic data has been widely used in the training of 6D pose estimation networks, in part because it automatically provides perfect annotation at low cost. However, there are still non-trivial domain gaps, such as differences in textures/materials, between synthetic and real data. These gaps have a measurable impact on performance. To solve this problem, we introduce a simulation to reality (sim2real) instance-level style transfer for 6D pose estimation network training. Our approach transfers the style of target objects individually, from synthetic to real, without human intervention. This improves the quality of synthetic data for training pose estimation networks. We also propose a complete pipeline from data collection to the training of a pose estimation network and conduct extensive evaluation on a real-world robotic platform. Our evaluation shows significant improvement achieved by our method in both pose estimation performance and the realism of images adapted by the style transfer.




# Arxiv Papers in cs.CV on 2022-03-08
### PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/2203.03796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03796v2)
- **Published**: 2022-03-08 01:36:26+00:00
- **Updated**: 2022-05-09 11:34:14+00:00
- **Authors**: Yunhao Du, Zhihang Tong, Junfeng Wan, Binyu Zhang, Yanyun Zhao
- **Comment**: ICME 2022 Workshop
- **Journal**: None
- **Summary**: Activity detection in surveillance videos is a challenging task caused by small objects, complex activity categories, its untrimmed nature, etc. Existing methods are generally limited in performance due to inaccurate proposals, poor classifiers or inadequate post-processing method. In this work, we propose a comprehensive and effective activity detection system in untrimmed surveillance videos for person-centered and vehicle-centered activities. It consists of four modules, i.e., object localizer, proposal filter, activity classifier and activity refiner. For person-centered activities, a novel part-attention mechanism is proposed to explore detailed features in different body parts. As for vehicle-centered activities, we propose a localization masking method to jointly encode motion and foreground attention features. We conduct experiments on the large-scale activity detection datasets VIRAT, and achieve the best results for both groups of activities. Furthermore, our team won the 1st place in the TRECVID 2021 ActEV challenge.



### Unknown-Aware Object Detection: Learning What You Don't Know from Videos in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2203.03800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03800v1)
- **Published**: 2022-03-08 01:44:03+00:00
- **Updated**: 2022-03-08 01:44:03+00:00
- **Authors**: Xuefeng Du, Xin Wang, Gabriel Gozum, Yixuan Li
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Building reliable object detectors that can detect out-of-distribution (OOD) objects is critical yet underexplored. One of the key challenges is that models lack supervision signals from unknown data, producing overconfident predictions on OOD objects. We propose a new unknown-aware object detection framework through Spatial-Temporal Unknown Distillation (STUD), which distills unknown objects from videos in the wild and meaningfully regularizes the model's decision boundary. STUD first identifies the unknown candidate object proposals in the spatial dimension, and then aggregates the candidates across multiple video frames to form a diverse set of unknown objects near the decision boundary. Alongside, we employ an energy-based uncertainty regularization loss, which contrastively shapes the uncertainty space between the in-distribution and distilled unknown objects. STUD establishes the state-of-the-art performance on OOD detection tasks for object detection, reducing the FPR95 score by over 10% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/stud.



### Panoramic Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.03806v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.03806v2)
- **Published**: 2022-03-08 02:00:17+00:00
- **Updated**: 2022-11-07 02:25:07+00:00
- **Authors**: Ruize Han, Haomin Yan, Jiacheng Li, Songmiao Wang, Wei Feng, Song Wang
- **Comment**: 17 pages
- **Journal**: European Conference on Computer Vision (ECCV 2022)
- **Summary**: To obtain a more comprehensive activity understanding for a crowded scene, in this paper, we propose a new problem of panoramic human activity recognition (PAR), which aims to simultaneous achieve the individual action, social group activity, and global activity recognition. This is a challenging yet practical problem in real-world applications. For this problem, we develop a novel hierarchical graph neural network to progressively represent and model the multi-granularity human activities and mutual social relations for a crowd of people. We further build a benchmark to evaluate the proposed method and other existing related methods. Experimental results verify the rationality of the proposed PAR problem, the effectiveness of our method and the usefulness of the benchmark. We will release the source code and benchmark to the public for promoting the study on this problem.



### Image Search with Text Feedback by Additive Attention Compositional Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.03809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.03809v1)
- **Published**: 2022-03-08 02:03:49+00:00
- **Updated**: 2022-03-08 02:03:49+00:00
- **Authors**: Yuxin Tian, Shawn Newsam, Kofi Boakye
- **Comment**: None
- **Journal**: None
- **Summary**: Effective image retrieval with text feedback stands to impact a range of real-world applications, such as e-commerce. Given a source image and text feedback that describes the desired modifications to that image, the goal is to retrieve the target images that resemble the source yet satisfy the given modifications by composing a multi-modal (image-text) query. We propose a novel solution to this problem, Additive Attention Compositional Learning (AACL), that uses a multi-modal transformer-based architecture and effectively models the image-text contexts. Specifically, we propose a novel image-text composition module based on additive attention that can be seamlessly plugged into deep neural networks. We also introduce a new challenging benchmark derived from the Shopping100k dataset. AACL is evaluated on three large-scale datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong baselines. Extensive experiments show that AACL achieves new state-of-the-art results on all three datasets.



### Towards performant and reliable undersampled MR reconstruction via diffusion model sampling
- **Arxiv ID**: http://arxiv.org/abs/2203.04292v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04292v2)
- **Published**: 2022-03-08 02:25:38+00:00
- **Updated**: 2022-03-11 02:00:26+00:00
- **Authors**: Cheng Peng, Pengfei Guo, S. Kevin Zhou, Vishal Patel, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance (MR) image reconstruction from under-sampled acquisition promises faster scanning time. To this end, current State-of-The-Art (SoTA) approaches leverage deep neural networks and supervised training to learn a recovery model. While these approaches achieve impressive performances, the learned model can be fragile on unseen degradation, e.g. when given a different acceleration factor. These methods are also generally deterministic and provide a single solution to an ill-posed problem; as such, it can be difficult for practitioners to understand the reliability of the reconstruction. We introduce DiffuseRecon, a novel diffusion model-based MR reconstruction method. DiffuseRecon guides the generation process based on the observed signals and a pre-trained diffusion model, and does not require additional training on specific acceleration factors. DiffuseRecon is stochastic in nature and generates results from a distribution of fully-sampled MR images; as such, it allows us to explicitly visualize different potential reconstruction solutions. Lastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo sampling scheme to approximate the most likely reconstruction candidate. The proposed DiffuseRecon achieves SoTA performances reconstructing from raw acquisition signals in fastMRI and SKM-TEA. Code will be open-sourced at www.github.com/cpeng93/DiffuseRecon.



### Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers
- **Arxiv ID**: http://arxiv.org/abs/2203.03814v1
- **DOI**: 10.1109/CVPR52688.2022.00257
- **Categories**: **eess.IV**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.03814v1)
- **Published**: 2022-03-08 02:29:32+00:00
- **Updated**: 2022-03-08 02:29:32+00:00
- **Authors**: Han Joo Chae, Seunghwan Lee, Hyewon Son, Seungyeob Han, Taebin Lim
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We introduce AiD Regen, a novel system that generates 3D wound models combining 2D semantic segmentation with 3D reconstruction so that they can be printed via 3D bio-printers during the surgery to treat diabetic foot ulcers (DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D image capturing, semantic segmentation, boundary-guided point-cloud processing, 3D model reconstruction, and 3D printable G-code generation, into a single system that can be used out of the box. We developed a multi-stage data preprocessing method to handle small and unbalanced DFU image datasets. AiD Regen's human-in-the-loop machine learning interface enables clinicians to not only create 3D regenerative patches with just a few touch interactions but also customize and confirm wound boundaries. As evidenced by our experiments, our model outperforms prior wound segmentation models and our reconstruction algorithm is capable of generating 3D wound models with compelling accuracy. We further conducted a case study on a real DFU patient and demonstrated the effectiveness of AiD Regen in treating DFU wounds.



### Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon
- **Arxiv ID**: http://arxiv.org/abs/2203.03818v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03818v3)
- **Published**: 2022-03-08 02:40:18+00:00
- **Updated**: 2022-03-23 02:02:05+00:00
- **Authors**: Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji
- **Comment**: This paper has been accepted by CVPR2022. Code:
  https://github.com/hncszyq/ShadowAttack
- **Journal**: None
- **Summary**: Estimating the risk level of adversarial examples is essential for safely deploying machine learning models in the real world. One popular approach for physical-world attacks is to adopt the "sticker-pasting" strategy, which however suffers from some limitations, including difficulties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artificial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily noticed by humans. In this paper, we study a new type of optical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adversarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simulated and real-world environments. Experimental results on traffic sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets respectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mechanism of this attack.



### Table Structure Recognition with Conditional Attention
- **Arxiv ID**: http://arxiv.org/abs/2203.03819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2203.03819v1)
- **Published**: 2022-03-08 02:44:58+00:00
- **Updated**: 2022-03-08 02:44:58+00:00
- **Authors**: Bin Xiao, Murat Simsek, Burak Kantarci, Ala Abu Alkheir
- **Comment**: IJDAR under review
- **Journal**: None
- **Summary**: Tabular data in digital documents is widely used to express compact and important information for readers. However, it is challenging to parse tables from unstructured digital documents, such as PDFs and images, into machine-readable format because of the complexity of table structures and the missing of meta-information. Table Structure Recognition (TSR) problem aims to recognize the structure of a table and transform the unstructured tables into a structured and machine-readable format so that the tabular data can be further analysed by the down-stream tasks, such as semantic modeling and information retrieval. In this study, we hypothesize that a complicated table structure can be represented by a graph whose vertices and edges represent the cells and association between cells, respectively. Then we define the table structure recognition problem as a cell association classification problem and propose a conditional attention network (CATT-Net). The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods on various datasets. Besides, we investigate whether the alignment of a cell bounding box or a text-focused approach has more impact on the model performance. Due to the lack of public dataset annotations based on these two approaches, we further annotate the ICDAR2013 dataset providing both types of bounding boxes, which can be a new benchmark dataset for evaluating the methods in this field. Experimental results show that the alignment of a cell bounding box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the Macro-average F1 score from 0.787 to 0.923.



### CF-ViT: A General Coarse-to-Fine Method for Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2203.03821v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03821v5)
- **Published**: 2022-03-08 02:57:49+00:00
- **Updated**: 2022-11-21 09:47:20+00:00
- **Authors**: Mengzhao Chen, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, Rongrong Ji
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: Vision Transformers (ViT) have made many breakthroughs in computer vision tasks. However, considerable redundancy arises in the spatial dimension of an input image, leading to massive computational costs. Therefore, We propose a coarse-to-fine vision transformer (CF-ViT) to relieve computational burden while retaining performance in this paper. Our proposed CF-ViT is motivated by two important observations in modern ViT models: (1) The coarse-grained patch splitting can locate informative regions of an input image. (2) Most images can be well recognized by a ViT model in a small-length token sequence. Therefore, our CF-ViT implements network inference in a two-stage manner. At coarse inference stage, an input image is split into a small-length patch sequence for a computationally economical classification. If not well recognized, the informative patches are identified and further re-split in a fine-grained granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For example, without any compromise on performance, CF-ViT reduces 53% FLOPs of LV-ViT, and also achieves 2.01x throughput.



### Deep Rectangling for Image Stitching: A Learning Baseline
- **Arxiv ID**: http://arxiv.org/abs/2203.03831v4
- **DOI**: 10.1109/CVPR52688.2022.00565
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03831v4)
- **Published**: 2022-03-08 03:34:10+00:00
- **Updated**: 2022-03-29 01:25:15+00:00
- **Authors**: Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao
- **Comment**: Accepted by CVPR2022 (oral); Codes and dataset:
  https://github.com/nie-lang/DeepRectangling
- **Journal**: None
- **Summary**: Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant irregular boundaries. To deal with this problem, existing image rectangling methods devote to searching an initial mesh and optimizing a target mesh to form the mesh deformation in two stages. Then rectangular images can be generated by warping stitched images. However, these solutions only work for images with rich linear structures, leading to noticeable distortions for portraits and landscapes with non-linear objects. In this paper, we address these issues by proposing the first deep learning solution to image rectangling. Concretely, we predefine a rigid target mesh and only estimate an initial mesh to form the mesh deformation, contributing to a compact one-stage solution. The initial mesh is predicted using a fully convolutional network with a residual progressive regression strategy. To obtain results with high content fidelity, a comprehensive objective function is proposed to simultaneously encourage the boundary rectangular, mesh shape-preserving, and content perceptually natural. Besides, we build the first image stitching rectangling dataset with a large diversity in irregular boundaries and scenes. Experiments demonstrate our superiority over traditional methods both quantitatively and qualitatively.



### Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap
- **Arxiv ID**: http://arxiv.org/abs/2203.03833v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.03833v2)
- **Published**: 2022-03-08 03:44:49+00:00
- **Updated**: 2022-07-19 06:57:13+00:00
- **Authors**: Yongwei Chen, Zihao Wang, Longkun Zou, Ke Chen, Kui Jia
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Semantic analyses of object point clouds are largely driven by releasing of benchmarking datasets, including synthetic ones whose instances are sampled from object CAD models. However, learning from synthetic data may not generalize to practical scenarios, where point clouds are typically incomplete, non-uniformly distributed, and noisy. Such a challenge of Simulation-to-Reality (Sim2Real) domain gap could be mitigated via learning algorithms of domain adaptation; however, we argue that generation of synthetic point clouds via more physically realistic rendering is a powerful alternative, as systematic non-uniform noise patterns can be captured. To this end, we propose an integrated scheme consisting of physically realistic synthesis of object point clouds via rendering stereo images via projection of speckle patterns onto CAD models and a novel quasi-balanced self-training designed for more balanced data distribution by sparsity-driven selection of pseudo labeled samples for long tailed classes. Experiment results can verify the effectiveness of our method as well as both of its modules for unsupervised domain adaptation on point cloud classification, achieving the state-of-the-art performance. Source codes and the SpeckleNet synthetic dataset are available at https://github.com/Gorilla-Lab-SCUT/QS3.



### Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2203.03838v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.03838v1)
- **Published**: 2022-03-08 04:01:08+00:00
- **Updated**: 2022-03-08 04:01:08+00:00
- **Authors**: Shentong Mo, Daizong Liu, Wei Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Query-based video grounding is an important yet challenging task in video understanding, which aims to localize the target segment in an untrimmed video according to a sentence query. Most previous works achieve significant progress by addressing this task in a fully-supervised manner with segment-level labels, which require high labeling cost. Although some recent efforts develop weakly-supervised methods that only need the video-level knowledge, they generally match multiple pre-defined segment proposals with query and select the best one, which lacks fine-grained frame-level details for distinguishing frames with high repeatability and similarity within the entire video. To alleviate the above limitations, we propose a self-contrastive learning framework to address the query-based video grounding task under a weakly-supervised setting. Firstly, instead of utilizing redundant segment proposals, we propose a new grounding scheme that learns frame-wise matching scores referring to the query semantic to predict the possible foreground frames by only using the video-level annotations. Secondly, since some predicted frames (i.e., boundary frames) are relatively coarse and exhibit similar appearance to their adjacent frames, we propose a coarse-to-fine contrastive learning paradigm to learn more discriminative frame-wise representations for distinguishing the false positive frames. In particular, we iteratively explore multi-scale hard negative samples that are close to positive samples in the representation space for distinguishing fine-grained frame-wise details, thus enforcing more accurate segment grounding. Extensive experiments on two challenging benchmarks demonstrate the superiority of our proposed method compared with the state-of-the-art methods.



### NaviAirway: a Bronchiole-sensitive Deep Learning-based Airway Segmentation Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2203.04294v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04294v3)
- **Published**: 2022-03-08 04:23:47+00:00
- **Updated**: 2023-06-16 18:29:12+00:00
- **Authors**: Andong Wang, Terence Chi Chun Tam, Ho Ming Poon, Kun-Chang Yu, Wei-Ning Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Airway segmentation is essential for chest CT image analysis. Different from natural image segmentation, which pursues high pixel-wise accuracy, airway segmentation focuses on topology. The task is challenging not only because of its complex tree-like structure but also the severe pixel imbalance among airway branches of different generations. To tackle the problems, we present a NaviAirway method which consists of a bronchiole-sensitive loss function for airway topology preservation and an iterative training strategy for accurate model learning across different airway generations. To supplement the features of airway branches learned by the model, we distill the knowledge from numerous unlabeled chest CT images in a teacher-student manner. Experimental results show that NaviAirway outperforms existing methods, particularly in the identification of higher-generation bronchioles and robustness to new CT scans. Moreover, NaviAirway is general enough to be combined with different backbone models to significantly improve their performance. NaviAirway can generate an airway roadmap for Navigation Bronchoscopy and can also be applied to other scenarios when segmenting fine and long tubular structures in biomedical images. The code is publicly available on https://github.com/AntonotnaWang/NaviAirway.



### Self-supervised Social Relation Representation for Human Group Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.03843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03843v2)
- **Published**: 2022-03-08 04:26:07+00:00
- **Updated**: 2022-11-07 02:29:29+00:00
- **Authors**: Jiacheng Li, Ruize Han, Haomin Yan, Zekun Qian, Wei Feng, Song Wang
- **Comment**: 17 pages
- **Journal**: European Conference on Computer Vision (ECCV 2022, Oral)
- **Summary**: Human group detection, which splits crowd of people into groups, is an important step for video-based human social activity analysis. The core of human group detection is the human social relation representation and division.In this paper, we propose a new two-stage multi-head framework for human group detection. In the first stage, we propose a human behavior simulator head to learn the social relation feature embedding, which is self-supervisely trained by leveraging the socially grounded multi-person behavior relationship. In the second stage, based on the social relation embedding, we develop a self-attention inspired network for human group detection. Remarkable performance on two state-of-the-art large-scale benchmarks, i.e., PANDA and JRDB-Group, verifies the effectiveness of the proposed framework. Benefiting from the self-supervised social relation embedding, our method can provide promising results with very few (labeled) training data. We will release the source code to the public.



### Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.03844v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03844v3)
- **Published**: 2022-03-08 04:26:18+00:00
- **Updated**: 2022-07-04 02:17:05+00:00
- **Authors**: Yunshan Zhong, Mingbao Lin, Xunchao Li, Ke Li, Yunhang Shen, Fei Chao, Yongjian Wu, Rongrong Ji
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Light-weight super-resolution (SR) models have received considerable attention for their serviceability in mobile devices. Many efforts employ network quantization to compress SR models. However, these methods suffer from severe performance degradation when quantizing the SR models to ultra-low precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In this paper, we identify that the performance drop comes from the contradiction between the layer-wise symmetric quantizer and the highly asymmetric activation distribution in SR models. This discrepancy leads to either a waste on the quantization levels or detail loss in reconstructed images. Therefore, we propose a novel activation quantizer, referred to as Dynamic Dual Trainable Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically, DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower bounds to tackle the highly asymmetric activations. 2) A dynamic gate controller to adaptively adjust the upper and lower bounds at runtime to overcome the drastically varying activation ranges over different samples.To reduce the extra overhead, the dynamic gate controller is quantized to 2-bit and applied to only part of the SR networks according to the introduced dynamic intensity. Extensive experiments demonstrate that our DDTB exhibits significant performance improvements in ultra-low precision. For example, our DDTB achieves a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and scaling up output images to x4. Code is at \url{https://github.com/zysxmu/DDTB}.



### Region Specific Optimization (RSO)-based Deep Interactive Registration
- **Arxiv ID**: http://arxiv.org/abs/2203.04295v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04295v1)
- **Published**: 2022-03-08 04:35:08+00:00
- **Updated**: 2022-03-08 04:35:08+00:00
- **Authors**: Ti Bai, Muhan Lin, Xiao Liang, Biling Wang, Michael Dohopolski, Bin Cai, Dan Nguyen, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image registration is a fundamental and vital task which will affect the efficacy of many downstream clinical tasks. Deep learning (DL)-based deformable image registration (DIR) methods have been investigated, showing state-of-the-art performance. A test time optimization (TTO) technique was proposed to further improve the DL models' performance. Despite the substantial accuracy improvement with this TTO technique, there still remained some regions that exhibited large registration errors even after many TTO iterations. To mitigate this challenge, we firstly identified the reason why the TTO technique was slow, or even failed, to improve those regions' registration results. We then proposed a two-levels TTO technique, i.e., image-specific optimization (ISO) and region-specific optimization (RSO), where the region can be interactively indicated by the clinician during the registration result reviewing process. For both efficiency and accuracy, we further envisioned a three-step DL-based image registration workflow. Experimental results showed that our proposed method outperformed the conventional method qualitatively and quantitatively.



### Where Does the Performance Improvement Come From? -- A Reproducibility Concern about Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2203.03853v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03853v3)
- **Published**: 2022-03-08 05:01:43+00:00
- **Updated**: 2022-08-27 16:59:48+00:00
- **Authors**: Jun Rao, Fei Wang, Liang Ding, Shuhan Qi, Yibing Zhan, Weifeng Liu, Dacheng Tao
- **Comment**: SIGIR 2022
- **Journal**: None
- **Summary**: This article aims to provide the information retrieval community with some reflections on recent advances in retrieval learning by analyzing the reproducibility of image-text retrieval models. Due to the increase of multimodal data over the last decade, image-text retrieval has steadily become a major research direction in the field of information retrieval. Numerous researchers train and evaluate image-text retrieval algorithms using benchmark datasets such as MS-COCO and Flickr30k. Research in the past has mostly focused on performance, with multiple state-of-the-art methodologies being suggested in a variety of ways. According to their assertions, these techniques provide improved modality interactions and hence more precise multimodal representations. In contrast to previous works, we focus on the reproducibility of the approaches and the examination of the elements that lead to improved performance by pretrained and nonpretrained models in retrieving images and text. To be more specific, we first examine the related reproducibility concerns and explain why our focus is on image-text retrieval tasks. Second, we systematically summarize the current paradigm of image-text retrieval models and the stated contributions of those approaches. Third, we analyze various aspects of the reproduction of pretrained and nonpretrained retrieval models. To complete this, we conducted ablation experiments and obtained some influencing factors that affect retrieval recall more than the improvement claimed in the original paper. Finally, we present some reflections and challenges that the retrieval community should consider in the future. Our source code is publicly available at https://github.com/WangFei-2019/Image-text-Retrieval.



### A New 27 Class Sign Language Dataset Collected from 173 Individuals
- **Arxiv ID**: http://arxiv.org/abs/2203.03859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03859v1)
- **Published**: 2022-03-08 05:30:03+00:00
- **Updated**: 2022-03-08 05:30:03+00:00
- **Authors**: Arda Mavi, Zeynep Dikle
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: After the interviews, it has been comprehended that speech-impaired individuals who use sign languages have difficulty communicating with other people who do not know sign language. Due to the communication problems, the sense of independence of speech-impaired individuals could be damaged and lead them to socialize less with society. To contribute to the development of technologies, that can reduce the communication problems of speech-impaired persons, a new dataset was presented with this paper. The dataset was created by processing American Sign Language-based photographs collected from 173 volunteers, published as 27 Class Sign Language Dataset on the Kaggle Datasets web page.



### Weakly Supervised Semantic Segmentation using Out-of-Distribution Data
- **Arxiv ID**: http://arxiv.org/abs/2203.03860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03860v1)
- **Published**: 2022-03-08 05:33:35+00:00
- **Updated**: 2022-03-08 05:33:35+00:00
- **Authors**: Jungbeom Lee, Seong Joon Oh, Sangdoo Yun, Junsuk Choe, Eunji Kim, Sungroh Yoon
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation (WSSS) methods are often built on pixel-level localization maps obtained from a classifier. However, training on class labels only, classifiers suffer from the spurious correlation between foreground and background cues (e.g. train and rail), fundamentally bounding the performance of WSSS. There have been previous endeavors to address this issue with additional supervision. We propose a novel source of information to distinguish foreground from the background: Out-of-Distribution (OoD) data, or images devoid of foreground object classes. In particular, we utilize the hard OoDs that the classifier is likely to make false-positive predictions. These samples typically carry key visual features on the background (e.g. rail) that the classifiers often confuse as foreground (e.g. train), so these cues let classifiers correctly suppress spurious background cues. Acquiring such hard OoDs does not require an extensive amount of annotation efforts; it only incurs a few additional image-level labeling costs on top of the original efforts to collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs. W-OoD achieves state-of-the-art performance on Pascal VOC 2012.



### Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective
- **Arxiv ID**: http://arxiv.org/abs/2203.03871v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.03871v3)
- **Published**: 2022-03-08 06:16:33+00:00
- **Updated**: 2022-07-21 04:50:38+00:00
- **Authors**: Quan Cui, Bingchen Zhao, Zhao-Min Chen, Borui Zhao, Renjie Song, Jiajun Liang, Boyan Zhou, Osamu Yoshie
- **Comment**: Accepted by ECCV 2022, Quan Cui and Bingchen Zhao contributed equally
  to this work
- **Journal**: None
- **Summary**: This work simultaneously considers the discriminability and transferability properties of deep representations in the typical supervised learning task, i.e., image classification. By a comprehensive temporal analysis, we observe a trade-off between these two properties. The discriminability keeps increasing with the training progressing while the transferability intensely diminishes in the later training period.   From the perspective of information-bottleneck theory, we reveal that the incompatibility between discriminability and transferability is attributed to the over-compression of input information. More importantly, we investigate why and how the InfoNCE loss can alleviate the over-compression, and further present a learning framework, named contrastive temporal coding~(CTC), to counteract the over-compression and alleviate the incompatibility. Extensive experiments validate that CTC successfully mitigates the incompatibility, yielding discriminative and transferable representations. Noticeable improvements are achieved on the image classification task and challenging transfer learning tasks. We hope that this work will raise the significance of the transferability property in the conventional supervised learning setting. Code is available at https://github.com/DTennant/dt-tradeoff.



### Visual anomaly detection in video by variational autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2203.03872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.03872v1)
- **Published**: 2022-03-08 06:22:04+00:00
- **Updated**: 2022-03-08 06:22:04+00:00
- **Authors**: Faraz Waseem, Rafael Perez Martinez, Chris Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomalies detection is the intersection of anomaly detection and visual intelligence. It has commercial applications in surveillance, security, self-driving cars and crop monitoring. Videos can capture a variety of anomalies. Due to efforts needed to label training data, unsupervised approaches to train anomaly detection models for videos is more practical An autoencoder is a neural network that is trained to recreate its input using latent representation of input also called a bottleneck layer. Variational autoencoder uses distribution (mean and variance) as compared to latent vector as bottleneck layer and can have better regularization effect. In this paper we have demonstrated comparison between performance of convolutional LSTM versus a variation convolutional LSTM autoencoder



### Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels
- **Arxiv ID**: http://arxiv.org/abs/2203.03884v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03884v2)
- **Published**: 2022-03-08 07:16:23+00:00
- **Updated**: 2022-03-14 11:11:39+00:00
- **Authors**: Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, Xinyi Le
- **Comment**: Accepted to CVPR 2022. Project:
  https://haochen-wang409.github.io/U2PL/
- **Journal**: None
- **Summary**: The crux of semi-supervised semantic segmentation is to assign adequate pseudo-labels to the pixels of unlabeled images. A common practice is to select the highly confident predictions as the pseudo ground-truth, but it leads to a problem that most pixels may be left unused due to their unreliability. We argue that every pixel matters to the model training, even its prediction is ambiguous. Intuitively, an unreliable prediction may get confused among the top classes (i.e., those with the highest probabilities), however, it should be confident about the pixel not belonging to the remaining classes. Hence, such a pixel can be convincingly treated as a negative sample to those most unlikely categories. Based on this insight, we develop an effective pipeline to make sufficient use of unlabeled data. Concretely, we separate reliable and unreliable pixels via the entropy of predictions, push each unreliable pixel to a category-wise queue that consists of negative samples, and manage to train the model with all candidate pixels. Considering the training evolution, where the prediction becomes more and more accurate, we adaptively adjust the threshold for the reliable-unreliable partition. Experimental results on various benchmarks and training settings demonstrate the superiority of our approach over the state-of-the-art alternatives.



### Boosting Mask R-CNN Performance for Long, Thin Forensic Traces with Pre-Segmentation and IoU Region Merging
- **Arxiv ID**: http://arxiv.org/abs/2203.03886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.03886v1)
- **Published**: 2022-03-08 07:18:16+00:00
- **Updated**: 2022-03-08 07:18:16+00:00
- **Authors**: Moritz Zink, Martin Schiele, Pengcheng Fan, Stephan Gasterst√§dt
- **Comment**: 9 Pages, 19 Figures
- **Journal**: None
- **Summary**: Mask R-CNN has recently achieved great success in the field of instance segmentation. However, weaknesses of the algorithm have been repeatedly pointed out as well, especially in the segmentation of long, sparse objects whose orientation is not exclusively horizontal or vertical. We present here an approach that significantly improves the performance of the algorithm by first pre-segmenting the images with a PSPNet algorithm. To further improve its prediction, we have developed our own cost functions and heuristics in the form of training strategies, which can prevent so-called (early) overfitting and achieve a more targeted convergence. Furthermore, due to the high variance of the images, especially for PSPNet, we aimed to develop strategies for a high robustness and generalization, which are also presented here.



### ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via Adversarial Rotation
- **Arxiv ID**: http://arxiv.org/abs/2203.03888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03888v1)
- **Published**: 2022-03-08 07:20:16+00:00
- **Updated**: 2022-03-08 07:20:16+00:00
- **Authors**: Robin Wang, Yibo Yang, Dacheng Tao
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Point cloud classifiers with rotation robustness have been widely discussed in the 3D deep learning community. Most proposed methods either use rotation invariant descriptors as inputs or try to design rotation equivariant networks. However, robust models generated by these methods have limited performance under clean aligned datasets due to modifications on the original classifiers or input space. In this study, for the first time, we show that the rotation robustness of point cloud classifiers can also be acquired via adversarial training with better performance on both rotated and clean datasets. Specifically, our proposed framework named ART-Point regards the rotation of the point cloud as an attack and improves rotation robustness by training the classifier on inputs with Adversarial RoTations. We contribute an axis-wise rotation attack that uses back-propagated gradients of the pre-trained model to effectively find the adversarial rotations. To avoid model over-fitting on adversarial inputs, we construct rotation pools that leverage the transferability of adversarial rotations among samples to increase the diversity of training data. Moreover, we propose a fast one-step optimization to efficiently reach the final robust model. Experiments show that our proposed rotation attack achieves a high success rate and ART-Point can be used on most existing classifiers to improve the rotation robustness while showing better performance on clean datasets than state-of-the-art methods.



### ClearPose: Large-scale Transparent Object Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2203.03890v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.03890v2)
- **Published**: 2022-03-08 07:29:31+00:00
- **Updated**: 2022-07-21 02:33:01+00:00
- **Authors**: Xiaotong Chen, Huijie Zhang, Zeren Yu, Anthony Opipari, Odest Chadwicke Jenkins
- **Comment**: ECCV 2022 accepted paper
- **Journal**: None
- **Summary**: Transparent objects are ubiquitous in household settings and pose distinct challenges for visual sensing and perception systems. The optical properties of transparent objects leave conventional 3D sensors alone unreliable for object depth and pose estimation. These challenges are highlighted by the shortage of large-scale RGB-Depth datasets focusing on transparent objects in real-world settings. In this work, we contribute a large-scale real-world RGB-Depth transparent object dataset named ClearPose to serve as a benchmark dataset for segmentation, scene-level depth completion and object-centric pose estimation tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth frames and 5M instance annotations covering 63 household objects. The dataset includes object categories commonly used in daily life under various lighting and occluding conditions as well as challenging test scenarios such as cases of occlusion by opaque or translucent objects, non-planar orientations, presence of liquids, etc. We benchmark several state-of-the-art depth completion and object pose estimation deep neural networks on ClearPose. The dataset and benchmarking source code is available at https://github.com/opipari/ClearPose.



### Geodesic Multi-Modal Mixup for Robust Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/2203.03897v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.03897v2)
- **Published**: 2022-03-08 07:34:52+00:00
- **Updated**: 2022-10-19 07:42:56+00:00
- **Authors**: Junhyuk So, Changdae Oh, Yongtaek Lim, Hoyoon Byun, Minchul Shin, Kyungwoo Song
- **Comment**: None
- **Journal**: None
- **Summary**: Pre-trained large-scale models provide a transferable embedding, and they show promising performance on diverse downstream tasks. However, the analysis of learned embedding has not been explored well, and the transferability for cross-modal tasks can be improved. This paper provides a perspective to understand multi-modal embedding in terms of uniformity and alignment. We newly find that the representation learned by multi-modal learning models such as CLIP has two separated embedding spaces for each heterogeneous dataset with less alignment. Besides, there are unexplored large intermediate areas between the two modalities with less uniformity. As a result, lack of alignment and uniformity might restrict the robustness and transferability of the representation for the downstream task. To this end, we provide a new end-to-end fine-tuning method for robust representation that encourages better uniformity and alignment score. First, we propose a \textit{Geodesic Multi-Modal Mixup} that mixes the representation of image and text to generate the hard negative samples on the hyperspherical embedding space. Second, we fine-tune the multi-modal model on hard negative samples as well as normal negatives and positive samples with contrastive loss. Through extensive experiments on retrieval, classification, and structure-awareness task, we demonstrate that our geodesic multi-modal Mixup learns a robust representation and provides improved performance on various downstream tasks.



### End-to-end system for object detection from sub-sampled radar data
- **Arxiv ID**: http://arxiv.org/abs/2203.03905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03905v1)
- **Published**: 2022-03-08 08:02:33+00:00
- **Updated**: 2022-03-08 08:02:33+00:00
- **Authors**: Madhumitha Sakthi, Ahmed Tewfik, Marius Arvinte, Haris Vikalo
- **Comment**: Submitted to EUSIPCO 2022
- **Journal**: None
- **Summary**: Robust and accurate sensing is of critical importance for advancing autonomous automotive systems. The need to acquire situational awareness in complex urban conditions using sensors such as radar has motivated research on power and latency-efficient signal acquisition methods. In this paper, we present an end-to-end signal processing pipeline, capable of operating in extreme weather conditions, that relies on sub-sampled radar data to perform object detection in vehicular settings. The results of the object detection are further utilized to sub-sample forthcoming radar data, which stands in contrast to prior work where the sub-sampling relies on image information. We show robust detection based on radar data reconstructed using 20% of samples under extreme weather conditions such as snow or fog, and on low-illuminated nights. Additionally, we generate 20% sampled radar data in a fine-tuning set and show 1.1% gain in AP50 across scenes and 3% AP50 gain in motorway condition.



### Language Matters: A Weakly Supervised Vision-Language Pre-training Approach for Scene Text Detection and Spotting
- **Arxiv ID**: http://arxiv.org/abs/2203.03911v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03911v3)
- **Published**: 2022-03-08 08:10:45+00:00
- **Updated**: 2022-11-14 08:17:26+00:00
- **Authors**: Chuhui Xue, Wenqing Zhang, Yu Hao, Shijian Lu, Philip Torr, Song Bai
- **Comment**: Accepted by ECCV2022 for oral presentation
- **Journal**: None
- **Summary**: Recently, Vision-Language Pre-training (VLP) techniques have greatly benefited various vision-language tasks by jointly learning visual and textual representations, which intuitively helps in Optical Character Recognition (OCR) tasks due to the rich visual and textual information in scene text images. However, these methods cannot well cope with OCR tasks because of the difficulty in both instance-level text encoding and image-text pair acquisition (i.e. images and captured texts in them). This paper presents a weakly supervised pre-training method, oCLIP, which can acquire effective scene text representations by jointly learning and aligning visual and textual information. Our network consists of an image encoder and a character-aware text encoder that extract visual and textual features, respectively, as well as a visual-textual decoder that models the interaction among textual and visual features for learning effective scene text representations. With the learning of textual features, the pre-trained model can attend texts in images well with character awareness. Besides, these designs enable the learning from weakly annotated texts (i.e. partial texts in images without text bounding boxes) which mitigates the data annotation constraint greatly. Experiments over the weakly annotated images in ICDAR2019-LSVT show that our pre-trained model improves F-score by +2.5\% and +4.8\% while transferring its weights to other text detection and spotting networks, respectively. In addition, the proposed method outperforms existing pre-training techniques consistently across multiple public datasets (e.g., +3.2\% and +1.3\% for Total-Text and CTW1500).



### Globally-Optimal Event Camera Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.03914v1
- **DOI**: 10.1007/978-3-030-58574-7_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03914v1)
- **Published**: 2022-03-08 08:24:22+00:00
- **Updated**: 2022-03-08 08:24:22+00:00
- **Authors**: Xin Peng, Yifu Wang, Ling Gao, Laurent Kneip
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), 2020
- **Summary**: Event cameras are bio-inspired sensors that perform well in HDR conditions and have high temporal resolution. However, different from traditional frame-based cameras, event cameras measure asynchronous pixel-level brightness changes and return them in a highly discretised format, hence new algorithms are needed. The present paper looks at fronto-parallel motion estimation of an event camera. The flow of the events is modeled by a general homographic warping in a space-time volume, and the objective is formulated as a maximisation of contrast within the image of unwarped events. However, in stark contrast to prior art, we derive a globally optimal solution to this generally non-convex problem, and thus remove the dependency on a good initial guess. Our algorithm relies on branch-and-bound optimisation for which we derive novel, recursive upper and lower bounds for six different contrast estimation functions. The practical validity of our approach is supported by a highly successful application to AGV motion estimation with a downward facing event camera, a challenging scenario in which the sensor experiences fronto-parallel motion in front of noisy, fast moving textures.



### PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2203.03931v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03931v3)
- **Published**: 2022-03-08 08:54:00+00:00
- **Updated**: 2022-07-20 03:43:24+00:00
- **Authors**: Kuan Zhu, Haiyun Guo, Tianyi Yan, Yousong Zhu, Jinqiao Wang, Ming Tang
- **Comment**: Accepted by ECCV2022. Codes are available at
  https://github.com/CASIA-IVA-Lab/PASS-reID
- **Journal**: None
- **Summary**: In person re-identification (ReID), very recent researches have validated pre-training the models on unlabelled person images is much better than on ImageNet. However, these researches directly apply the existing self-supervised learning (SSL) methods designed for image classification to ReID without any adaption in the framework. These SSL methods match the outputs of local views (e.g., red T-shirt, blue shorts) to those of the global views at the same time, losing lots of details. In this paper, we propose a ReID-specific pre-training method, Part-Aware Self-Supervised pre-training (PASS), which can generate part-level features to offer fine-grained information and is more suitable for ReID. PASS divides the images into several local areas, and the local views randomly cropped from each area are assigned with a specific learnable [PART] token. On the other hand, the [PART]s of all local areas are also appended to the global views. PASS learns to match the output of the local views and global views on the same [PART]. That is, the learned [PART] of the local views from a local area is only matched with the corresponding [PART] learned from the global views. As a result, each [PART] can focus on a specific local area of the image and extracts fine-grained information of this area. Experiments show PASS sets the new state-of-the-art performances on Market1501 and MSMT17 on various ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves 92.2\%/90.2\%/88.5\% mAP accuracy on Market1501 for supervised/UDA/USL ReID. Our codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.



### Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention
- **Arxiv ID**: http://arxiv.org/abs/2203.03937v4
- **DOI**: None
- **Categories**: **cs.CV**, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2203.03937v4)
- **Published**: 2022-03-08 09:01:41+00:00
- **Updated**: 2022-05-27 02:33:14+00:00
- **Authors**: Kai Liu, Tianyi Wu, Cong Liu, Guodong Guo
- **Comment**: 11 pages, 6 figures. Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by each query attending to all keys/values, various methods have constrained the range of attention within local regions, where each query only attends to keys/values within a hand-crafted window. However, these hand-crafted window partition mechanisms are data-agnostic and ignore their input content, so it is likely that one query maybe attends to irrelevant keys/values. To address this issue, we propose a Dynamic Group Attention (DG-Attention), which dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group. Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention. Built on the DG-Attention, we develop a general vision transformer backbone named Dynamic Group Transformer (DGT). Extensive experiments show that our models can outperform the state-of-the-art methods on multiple common vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation.



### Plug-and-play Shape Refinement Framework for Multi-site and Lifespan Brain Skull Stripping
- **Arxiv ID**: http://arxiv.org/abs/2203.04299v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04299v3)
- **Published**: 2022-03-08 09:11:08+00:00
- **Updated**: 2022-12-22 20:18:47+00:00
- **Authors**: Yunxiang Li, Ruilong Dan, Shuai Wang, Yifan Cao, Xiangde Luo, Chenghao Tan, Gangyong Jia, Huiyu Zhou, You Zhang, Yaqi Wang, Li Wang
- **Comment**: 11 page
- **Journal**: None
- **Summary**: Skull stripping is a crucial prerequisite step in the analysis of brain magnetic resonance images (MRI). Although many excellent works or tools have been proposed, they suffer from low generalization capability. For instance, the model trained on a dataset with specific imaging parameters cannot be well applied to other datasets with different imaging parameters. Especially, for the lifespan datasets, the model trained on an adult dataset is not applicable to an infant dataset due to the large domain difference. To address this issue, numerous methods have been proposed, where domain adaptation based on feature alignment is the most common. Unfortunately, this method has some inherent shortcomings, which need to be retrained for each new domain and requires concurrent access to the input images of both domains. In this paper, we design a plug-and-play shape refinement (PSR) framework for multi-site and lifespan skull stripping. To deal with the domain shift between multi-site lifespan datasets, we take advantage of the brain shape prior, which is invariant to imaging parameters and ages. Experiments demonstrate that our framework can outperform the state-of-the-art methods on multi-site lifespan datasets.



### An Online Semantic Mapping System for Extending and Enhancing Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2203.03944v1
- **DOI**: 10.1016/j.engappai.2022.104830
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03944v1)
- **Published**: 2022-03-08 09:14:37+00:00
- **Updated**: 2022-03-08 09:14:37+00:00
- **Authors**: Thorsten Hempel, Ayoub Al-Hamadi
- **Comment**: Accepted by Engineering Applications of Artificial Intelligence,
  Elsevier, 7 Mar 2022
- **Journal**: Engineering Applications of Artificial Intelligence, Volume 111,
  May 2022, 104830
- **Summary**: We present a real-time semantic mapping approach for mobile vision systems with a 2D to 3D object detection pipeline and rapid data association for generated landmarks. Besides the semantic map enrichment the associated detections are further introduced as semantic constraints into a simultaneous localization and mapping (SLAM) system for pose correction purposes. This way, we are able generate additional meaningful information that allows to achieve higher-level tasks, while simultaneously leveraging the view-invariance of object detections to improve the accuracy and the robustness of the odometry estimation. We propose tracklets of locally associated object observations to handle ambiguous and false predictions and an uncertainty-based greedy association scheme for an accelerated processing time. Our system reaches real-time capabilities with an average iteration duration of 65~ms and is able to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a public dataset. Additionally, we implemented our approach as a modular ROS package that makes it straightforward for integration in arbitrary graph-based SLAM methods.



### Multi-trial Neural Architecture Search with Lottery Tickets
- **Arxiv ID**: http://arxiv.org/abs/2203.04300v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2203.04300v3)
- **Published**: 2022-03-08 09:14:37+00:00
- **Updated**: 2022-12-03 03:02:42+00:00
- **Authors**: Zimian Wei, Hengyue Pan, Lujun Li, Menglong Lu, Xin Niu, Peijie Dong, Dongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has brought significant progress in recent image recognition tasks. Most existing NAS methods apply restricted search spaces, which limits the upper-bound performance of searched models. To address this issue, we propose a new search space named MobileNet3-MT. By reducing human-prior knowledge in omni dimensions of networks, MobileNet3-MT accommodates more potential candidates. For searching in this challenging search space, we present an efficient Multi-trial Evolution-based NAS method termed MENAS. Specifically, we accelerate the evolutionary search process by gradually pruning models in the population. Each model is trained with an early stop and replaced by its Lottery Tickets (the explored optimal pruned network).In this way, the full training pipeline of cumbersome networks is prevented and more efficient networks are automatically generated. Extensive experimental results on ImageNet-1K, CIFAR-10, and CIFAR-100 demonstrate that MENAS achieves state-of-the-art performance.



### RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2203.03949v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03949v4)
- **Published**: 2022-03-08 09:24:05+00:00
- **Updated**: 2022-08-21 18:09:48+00:00
- **Authors**: Di Chang, Alja≈æ Bo≈æiƒç, Tong Zhang, Qingsong Yan, Yingcong Chen, Sabine S√ºsstrunk, Matthias Nie√üner
- **Comment**: Accepted by ECCV 2022, Project Page:
  https://boese0601.github.io/rc-mvsnet/
- **Journal**: None
- **Summary**: Finding accurate correspondences among different views is the Achilles' heel of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the assumption that corresponding pixels share similar photometric features. However, multi-view images in real scenarios observe non-Lambertian surfaces and experience occlusions. In this work, we propose a novel approach with neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences among views. Specifically, we impose a depth rendering consistency loss to constrain the geometry features close to the object surface to alleviate occlusions. Concurrently, we introduce a reference view synthesis loss to generate consistent supervision, even for non-Lambertian surfaces. Extensive experiments on DTU and Tanks\&Temples benchmarks demonstrate that our RC-MVSNet approach achieves state-of-the-art performance over unsupervised MVS frameworks and competitive performance to many supervised methods.The code is released at https://github.com/Boese0601/RC-MVSNet



### Efficient and Accurate Hyperspectral Pansharpening Using 3D VolumeNet and 2.5D Texture Transfer
- **Arxiv ID**: http://arxiv.org/abs/2203.03951v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03951v1)
- **Published**: 2022-03-08 09:24:12+00:00
- **Updated**: 2022-03-08 09:24:12+00:00
- **Authors**: Yinao Li, Yutaro Iwamoto, Ryousuke Nakamura, Lanfen Lin, Ruofeng Tong, Yen-Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNN) have obtained promising results in single-image SR for hyperspectral pansharpening. However, enhancing CNNs' representation ability with fewer parameters and a shorter prediction time is a challenging and critical task. In this paper, we propose a novel multi-spectral image fusion method using a combination of the previously proposed 3D CNN model VolumeNet and 2.5D texture transfer method using other modality high resolution (HR) images. Since a multi-spectral (MS) image consists of several bands and each band is a 2D image slice, MS images can be seen as 3D data. Thus, we use the previously proposed VolumeNet to fuse HR panchromatic (PAN) images and bicubic interpolated MS images. Because the proposed 3D VolumeNet can effectively improve the accuracy by expanding the receptive field of the model, and due to its lightweight structure, we can achieve better performance against the existing method without purchasing a large number of remote sensing images for training. In addition, VolumeNet can restore the high-frequency information lost in the HR MR image as much as possible, reducing the difficulty of feature extraction in the following step: 2.5D texture transfer. As one of the latest technologies, deep learning-based texture transfer has been demonstrated to effectively and efficiently improve the visual performance and quality evaluation indicators of image reconstruction. Different from the texture transfer processing of RGB image, we use HR PAN images as the reference images and perform texture transfer for each frequency band of MS images, which is named 2.5D texture transfer. The experimental results show that the proposed method outperforms the existing methods in terms of objective accuracy assessment, method efficiency, and visual subjective evaluation.



### ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer
- **Arxiv ID**: http://arxiv.org/abs/2203.03952v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03952v5)
- **Published**: 2022-03-08 09:25:17+00:00
- **Updated**: 2022-07-26 12:11:15+00:00
- **Authors**: Haokui Zhang, Wenze Hu, Xiaoyu Wang
- **Comment**: This paper has been acccpted by ECCV 2022. Code is available at
  https://github.com/hkzhang91/ParC-Net
- **Journal**: None
- **Summary**: Recently, vision transformers started to show impressive results which outperform large convolution based models significantly. However, in the area of small models for mobile or resource constrained devices, ConvNet still has its own advantages in both performance and model complexity. We propose ParC-Net, a pure ConvNet based backbone model that further strengthens these advantages by fusing the merits of vision transformers into ConvNets. Specifically, we propose position aware circular convolution (ParC), a light-weight convolution op which boasts a global receptive field while producing location sensitive features as in local convolutions. We combine the ParCs and squeeze-exictation ops to form a meta-former like model block, which further has the attention mechanism like transformers. The aforementioned block can be used in plug-and-play manner to replace relevant blocks in ConvNets or transformers. Experiment results show that the proposed ParC-Net achieves better performance than popular light-weight ConvNets and vision transformer based models in common vision tasks and datasets, while having fewer parameters and faster inference speed. For classification on ImageNet-1k, ParC-Net achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7% accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, ParC-Net also shows better performance. Source code is available at https://github.com/hkzhang91/ParC-Net



### Generative Cooperative Learning for Unsupervised Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.03962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03962v1)
- **Published**: 2022-03-08 09:36:51+00:00
- **Updated**: 2022-03-08 09:36:51+00:00
- **Authors**: Muhammad Zaigham Zaheer, Arif Mahmood, Muhammad Haris Khan, Mattia Segu, Fisher Yu, Seung-Ik Lee
- **Comment**: Accepted to the Conference on Computer Vision and Pattern Recognition
  CVPR 2022
- **Journal**: None
- **Summary**: Video anomaly detection is well investigated in weakly-supervised and one-class classification (OCC) settings. However, unsupervised video anomaly detection methods are quite sparse, likely because anomalies are less frequent in occurrence and usually not well-defined, which when coupled with the absence of ground truth supervision, could adversely affect the performance of the learning algorithms. This problem is challenging yet rewarding as it can completely eradicate the costs of obtaining laborious annotations and enable such systems to be deployed without human intervention. To this end, we propose a novel unsupervised Generative Cooperative Learning (GCL) approach for video anomaly detection that exploits the low frequency of anomalies towards building a cross-supervision between a generator and a discriminator. In essence, both networks get trained in a cooperative fashion, thereby allowing unsupervised learning. We conduct extensive experiments on two large-scale video anomaly detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over the existing state-of-the-art unsupervised and OCC methods corroborate the effectiveness of our approach.



### Live Laparoscopic Video Retrieval with Compressed Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2203.04301v2
- **DOI**: 10.1016/j.media.2023.102866
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04301v2)
- **Published**: 2022-03-08 09:37:57+00:00
- **Updated**: 2023-06-12 12:51:28+00:00
- **Authors**: Tong Yu, Pietro Mascagni, Juan Verde, Jacques Marescaux, Didier Mutter, Nicolas Padoy
- **Comment**: 16 pages, 13 figures
- **Journal**: Medical Image Analysis 88 (2023) 102866
- **Summary**: Searching through large volumes of medical data to retrieve relevant information is a challenging yet crucial task for clinical care. However the primitive and most common approach to retrieval, involving text in the form of keywords, is severely limited when dealing with complex media formats. Content-based retrieval offers a way to overcome this limitation, by using rich media as the query itself. Surgical video-to-video retrieval in particular is a new and largely unexplored research problem with high clinical value, especially in the real-time case: using real-time video hashing, search can be achieved directly inside of the operating room. Indeed, the process of hashing converts large data entries into compact binary arrays or hashes, enabling large-scale search operations at a very fast rate. However, due to fluctuations over the course of a video, not all bits in a given hash are equally reliable. In this work, we propose a method capable of mitigating this uncertainty while maintaining a light computational footprint. We present superior retrieval results (3-4 % top 10 mean average precision) on a multi-task evaluation protocol for surgery, using cholecystectomy phases, bypass phases, and coming from an entirely new dataset introduced here, critical events across six different surgery types. Success on this multi-task benchmark shows the generalizability of our approach for surgical video retrieval.



### GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework
- **Arxiv ID**: http://arxiv.org/abs/2203.03966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03966v2)
- **Published**: 2022-03-08 09:49:48+00:00
- **Updated**: 2022-10-09 11:03:28+00:00
- **Authors**: Ming Wang, Beibei Lin, Xianda Guo, Lincheng Li, Zheng Zhu, Jiande Sun, Shunli Zhang, Xin Yu
- **Comment**: Accepted to ACCV2022
- **Journal**: None
- **Summary**: Many gait recognition methods first partition the human gait into N-parts and then combine them to establish part-based feature representations. Their gait recognition performance is often affected by partitioning strategies, which are empirically chosen in different datasets. However, we observe that strips as the basic component of parts are agnostic against different partitioning strategies. Motivated by this observation, we present a strip-based multi-level gait recognition network, named GaitStrip, to extract comprehensive gait information at different levels. To be specific, our high-level branch explores the context of gait sequences and our low-level one focuses on detailed posture changes. We introduce a novel StriP-Based feature extractor (SPB) to learn the strip-based feature representations by directly taking each strip of the human body as the basic unit. Moreover, we propose a novel multi-branch structure, called Enhanced Convolution Module (ECM), to extract different representations of gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the Frame-Level feature extractor (FL) and SPB, and has two obvious advantages: First, each branch focuses on a specific representation, which can be used to improve the robustness of the network. Specifically, ST aims to extract spatial-temporal features of gait sequences, while FL is used to generate the feature representation of each frame. Second, the parameters of the ECM can be reduced in test by introducing a structural re-parameterization technique. Extensive experimental results demonstrate that our GaitStrip achieves state-of-the-art performance in both normal walking and complex conditions.



### On Generalizing Beyond Domains in Cross-Domain Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.03970v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03970v1)
- **Published**: 2022-03-08 09:57:48+00:00
- **Updated**: 2022-03-08 09:57:48+00:00
- **Authors**: Christian Simon, Masoud Faraki, Yi-Hsuan Tsai, Xiang Yu, Samuel Schulter, Yumin Suh, Mehrtash Harandi, Manmohan Chandraker
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks often suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent methods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distributions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful features by equipping the classifier with class similarity metrics as learning parameters which are obtained through Mahalanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demonstrate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multiple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfficeHome.



### Universal Prototype Transport for Zero-Shot Action Recognition and Localization
- **Arxiv ID**: http://arxiv.org/abs/2203.03971v2
- **DOI**: 10.1007/s11263-023-01846-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03971v2)
- **Published**: 2022-03-08 09:58:40+00:00
- **Updated**: 2023-08-01 09:21:58+00:00
- **Authors**: Pascal Mettes
- **Comment**: None
- **Journal**: International Journal of Computer Vision (2023)
- **Summary**: This work addresses the problem of recognizing action categories in videos when no training examples are available. The current state-of-the-art enables such a zero-shot recognition by learning universal mappings from videos to a semantic space, either trained on large-scale seen actions or on objects. While effective, we find that universal action and object mappings are biased to specific regions in the semantic space. These biases lead to a fundamental problem: many unseen action categories are simply never inferred during testing. For example on UCF-101, a quarter of the unseen actions are out of reach with a state-of-the-art universal action model. To that end, this paper introduces universal prototype transport for zero-shot action recognition. The main idea is to re-position the semantic prototypes of unseen actions by matching them to the distribution of all test videos. For universal action models, we propose to match distributions through a hyperspherical optimal transport from unseen action prototypes to the set of all projected test videos. The resulting transport couplings in turn determine the target prototype for each unseen action. Rather than directly using the target prototype as final result, we re-position unseen action prototypes along the geodesic spanned by the original and target prototypes as a form of semantic regularization. For universal object models, we outline a variant that defines target prototypes based on an optimal transport between unseen action prototypes and object prototypes. Empirically, we show that universal prototype transport diminishes the biased selection of unseen action prototypes and boosts both universal action and object models for zero-shot classification and spatio-temporal localization.



### GaitEdge: Beyond Plain End-to-end Gait Recognition for Better Practicality
- **Arxiv ID**: http://arxiv.org/abs/2203.03972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03972v2)
- **Published**: 2022-03-08 09:58:46+00:00
- **Updated**: 2022-07-18 03:28:43+00:00
- **Authors**: Junhao Liang, Chao Fan, Saihui Hou, Chuanfu Shen, Yongzhen Huang, Shiqi Yu
- **Comment**: 16 pages, 7 figures, accepted by ECCV2022
- **Journal**: None
- **Summary**: Gait is one of the most promising biometrics to identify individuals at a long distance. Although most previous methods have focused on recognizing the silhouettes, several end-to-end methods that extract gait features directly from RGB images perform better. However, we demonstrate that these end-to-end methods may inevitably suffer from the gait-irrelevant noises, i.e., low-level texture and colorful information. Experimentally, we design the cross-domain evaluation to support this view. In this work, we propose a novel end-to-end framework named GaitEdge which can effectively block gait-irrelevant information and release end-to-end training potential. Specifically, GaitEdge synthesizes the output of the pedestrian segmentation network and then feeds it to the subsequent recognition network, where the synthetic silhouettes consist of trainable edges of bodies and fixed interiors to limit the information that the recognition network receives. Besides, GaitAlign for aligning silhouettes is embedded into the GaitEdge without losing differentiability. Experimental results on CASIA-B and our newly built TTG-200 indicate that GaitEdge significantly outperforms the previous methods and provides a more practical end-to-end paradigm. All the source code are available at https://github.com/ShiqiYu/OpenGait.



### End-to-end Multiple Instance Learning with Gradient Accumulation
- **Arxiv ID**: http://arxiv.org/abs/2203.03981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.03981v1)
- **Published**: 2022-03-08 10:14:51+00:00
- **Updated**: 2022-03-08 10:14:51+00:00
- **Authors**: Axel Andersson, Nadezhda Koriakina, Nata≈°a Sladoje, Joakim Lindblad
- **Comment**: None
- **Journal**: None
- **Summary**: Being able to learn on weakly labeled data, and provide interpretability, are two of the main reasons why attention-based deep multiple instance learning (ABMIL) methods have become particularly popular for classification of histopathological images. Such image data usually come in the form of gigapixel-sized whole-slide-images (WSI) that are cropped into smaller patches (instances). However, the sheer size of the data makes training of ABMIL models challenging. All the instances from one WSI cannot be processed at once by conventional GPUs. Existing solutions compromise training by relying on pre-trained models, strategic sampling or selection of instances, or self-supervised learning. We propose a training strategy based on gradient accumulation that enables direct end-to-end training of ABMIL models without being limited by GPU memory. We conduct experiments on both QMNIST and Imagenette to investigate the performance and training time, and compare with the conventional memory-expensive baseline and a recent sampled-based approach. This memory-efficient approach, although slower, reaches performance indistinguishable from the memory-expensive baseline.



### Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2203.03984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03984v1)
- **Published**: 2022-03-08 10:18:25+00:00
- **Updated**: 2022-03-08 10:18:25+00:00
- **Authors**: Ganglai Wang, Peng Zhang, Lei Xie, Wei Huang, Yufei Zha
- **Comment**: None
- **Journal**: None
- **Summary**: Talking face generation with great practical significance has attracted more attention in recent audio-visual studies. How to achieve accurate lip synchronization is a long-standing challenge to be further investigated. Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by incorporating spatial attention module and channel attention module into lip-syncing strategy. Rather than focusing on the unimportant regions of the face image, the proposed AttnWav2Lip model is able to pay more attention on the lip region reconstruction. To our limited knowledge, this is the first attempt to introduce attention mechanism to the scheme of talking face generation. An extensive experiments have been conducted to evaluate the effectiveness of the proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a superior performance has been demonstrated on the benchmark lip synthesis datasets, including LRW, LRS2 and LRS3.



### SuperPoint features in endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2203.04302v2
- **DOI**: 10.1007/978-3-031-21083-9_5
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04302v2)
- **Published**: 2022-03-08 10:19:08+00:00
- **Updated**: 2023-01-09 16:40:21+00:00
- **Authors**: O. L. Barbed, F. Chadebecq, J. Morlana, J. M. Mart√≠nez-Montiel, A. C. Murillo
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: There is often a significant gap between research results and applicability in routine medical practice. This work studies the performance of well-known local features on a medical dataset captured during routine colonoscopy procedures. Local feature extraction and matching is a key step for many computer vision applications, specially regarding 3D modelling. In the medical domain, handcrafted local features such as SIFT, with public pipelines such as COLMAP, are still a predominant tool for this kind of tasks. We explore the potential of the well known self-supervised approach SuperPoint, present an adapted variation for the endoscopic domain and propose a challenging evaluation framework. SuperPoint based models achieve significantly higher matching quality than commonly used local features in this domain. Our adapted model avoids features within specularity regions, a frequent and problematic artifact in endoscopic images, with consequent benefits for matching and reconstruction results.



### SimpleTrack: Rethinking and Improving the JDE Approach for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2203.03985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03985v1)
- **Published**: 2022-03-08 10:19:35+00:00
- **Updated**: 2022-03-08 10:19:35+00:00
- **Authors**: Jiaxin Li, Yan Ding, Hualiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Joint detection and embedding (JDE) based methods usually estimate bounding boxes and embedding features of objects with a single network in Multi-Object Tracking (MOT). In the tracking stage, JDE-based methods fuse the target motion information and appearance information by applying the same rule, which could fail when the target is briefly lost or blocked. To overcome this problem, we propose a new association matrix, the Embedding and Giou matrix, which combines embedding cosine distance and Giou distance of objects. To further improve the performance of data association, we develop a simple, effective tracker named SimpleTrack, which designs a bottom-up fusion method for Re-identity and proposes a new tracking strategy based on our EG matrix. The experimental results indicate that SimpleTrack has powerful data association capability, e.g., 61.6 HOTA and 76.3 IDF1 on MOT17. In addition, we apply the EG matrix to 5 different state-of-the-art JDE-based methods and achieve significant improvements in IDF1, HOTA and IDsw metrics, and increase the tracking speed of these methods by about 20%.



### Skating-Mixer: Long-Term Sport Audio-Visual Modeling with MLPs
- **Arxiv ID**: http://arxiv.org/abs/2203.03990v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.03990v4)
- **Published**: 2022-03-08 10:36:55+00:00
- **Updated**: 2022-12-17 06:50:32+00:00
- **Authors**: Jingfei Xia, Mingchen Zhuge, Tiantian Geng, Shun Fan, Yuantai Wei, Zhenyu He, Feng Zheng
- **Comment**: Our code is available at
  https://github.com/AndyFrancesco29/Audio-Visual-Figure-Skating
- **Journal**: None
- **Summary**: Figure skating scoring is challenging because it requires judging the technical moves of the players as well as their coordination with the background music. Most learning-based methods cannot solve it well for two reasons: 1) each move in figure skating changes quickly, hence simply applying traditional frame sampling will lose a lot of valuable information, especially in 3 to 5 minutes long videos; 2) prior methods rarely considered the critical audio-visual relationship in their models. Due to these reasons, we introduce a novel architecture, named Skating-Mixer. It extends the MLP framework into a multimodal fashion and effectively learns long-term representations through our designed memory recurrent unit (MRU). Aside from the model, we collected a high-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8 types of programs with 7 different rating metrics, overtaking other datasets in both quantity and diversity. Experiments show the proposed method achieves SOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In addition, we include an analysis applying our method to the recent competitions in Beijing 2022 Winter Olympic Games, proving our method has strong applicability.



### DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos
- **Arxiv ID**: http://arxiv.org/abs/2203.03996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.03996v1)
- **Published**: 2022-03-08 10:54:00+00:00
- **Updated**: 2022-03-08 10:54:00+00:00
- **Authors**: Mathias Parger, Chengcheng Tang, Christopher D. Twigg, Cem Keskin, Robert Wang, Markus Steinberger
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Convolutional neural network inference on video data requires powerful hardware for real-time processing. Given the inherent coherence across consecutive frames, large parts of a video typically change little. By skipping identical image regions and truncating insignificant pixel updates, computational redundancy can in theory be reduced significantly. However, these theoretical savings have been difficult to translate into practice, as sparse updates hamper computational consistency and memory access coherence; which are key for efficiency on real hardware. With DeltaCNN, we present a sparse convolutional neural network framework that enables sparse frame-by-frame updates to accelerate video inference in practice. We provide sparse implementations for all typical CNN layers and propagate sparse feature updates end-to-end - without accumulating errors over time. DeltaCNN is applicable to all convolutional neural networks without retraining. To the best of our knowledge, we are the first to significantly outperform the dense reference, cuDNN, in practical settings, achieving speedups of up to 7x with only marginal differences in accuracy.



### Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration
- **Arxiv ID**: http://arxiv.org/abs/2203.04006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.04006v1)
- **Published**: 2022-03-08 11:01:24+00:00
- **Updated**: 2022-03-08 11:01:24+00:00
- **Authors**: Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang
- **Comment**: Accepted by ACL 2022
- **Journal**: None
- **Summary**: Vision-language navigation (VLN) is a challenging task due to its large searching space in the environment. To address this problem, previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets. However, the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration capabilities in environments, which hinders their generalization of unseen scenes. To improve the ability of fast cross-domain adaptation, we propose Prompt-based Environmental Self-exploration (ProbES), which can self-explore the environments by sampling trajectories and automatically generates structured instructions via a large-scale cross-modal pretrained model (CLIP). Our method fully utilizes the knowledge learned from CLIP to build an in-domain dataset by self-exploration without human labeling. Unlike the conventional approach of fine-tuning, we introduce prompt-based learning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge. By automatically synthesizing trajectory-instruction pairs in any environment without human supervision and efficient prompt-based learning, our model can adapt to diverse vision-language navigation tasks, including VLN and REVERIE. Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.



### DuMLP-Pin: A Dual-MLP-dot-product Permutation-invariant Network for Set Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2203.04007v2
- **DOI**: 10.1609/aaai.v36i1.19939
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04007v2)
- **Published**: 2022-03-08 11:01:41+00:00
- **Updated**: 2022-08-31 03:43:28+00:00
- **Authors**: Jiajun Fei, Ziyu Zhu, Wenlei Liu, Zhidong Deng, Mingyang Li, Huanjun Deng, Shuo Zhang
- **Comment**: 16 pages, accepted by AAAI 2022
  (https://ojs.aaai.org/index.php/AAAI/article/view/19939), with technical
  appendix
- **Journal**: None
- **Summary**: Existing permutation-invariant methods can be divided into two categories according to the aggregation scope, i.e. global aggregation and local one. Although the global aggregation methods, e. g., PointNet and Deep Sets, get involved in simpler structures, their performance is poorer than the local aggregation ones like PointNet++ and Point Transformer. It remains an open problem whether there exists a global aggregation method with a simple structure, competitive performance, and even much fewer parameters. In this paper, we propose a novel global aggregation permutation-invariant network based on dual MLP dot-product, called DuMLP-Pin, which is capable of being employed to extract features for set inputs, including unordered or unstructured pixel, attribute, and point cloud data sets. We strictly prove that any permutation-invariant function implemented by DuMLP-Pin can be decomposed into two or more permutation-equivariant ones in a dot-product way as the cardinality of the given input set is greater than a threshold. We also show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints under certain conditions. The performance of DuMLP-Pin is evaluated on several different tasks with diverse data sets. The experimental results demonstrate that our DuMLP-Pin achieves the best results on the two classification problems for pixel sets and attribute sets. On both the point cloud classification and the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far best-performing local aggregation method with only a 1-2% difference, while the number of required parameters is significantly reduced by more than 85% in classification and 69% in segmentation, respectively. The code is publicly available on https://github.com/JaronTHU/DuMLP-Pin.



### Evolutionary Neural Cascade Search across Supernetworks
- **Arxiv ID**: http://arxiv.org/abs/2203.04011v2
- **DOI**: 10.1145/3512290.3528749
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2203.04011v2)
- **Published**: 2022-03-08 11:06:01+00:00
- **Updated**: 2022-04-28 16:01:47+00:00
- **Authors**: Alexander Chebykin, Tanja Alderliesten, Peter A. N. Bosman
- **Comment**: 13 pages, 13 figures. Accepted at GECCO 2022
- **Journal**: None
- **Summary**: To achieve excellent performance with modern neural networks, having the right network architecture is important. Neural Architecture Search (NAS) concerns the automatic discovery of task-specific network architectures. Modern NAS approaches leverage supernetworks whose subnetworks encode candidate neural network architectures. These subnetworks can be trained simultaneously, removing the need to train each network from scratch, thereby increasing the efficiency of NAS. A recent method called Neural Architecture Transfer (NAT) further improves the efficiency of NAS for computer vision tasks by using a multi-objective evolutionary algorithm to find high-quality subnetworks of a supernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS - Evolutionary Neural Cascade Search. ENCAS can be used to search over multiple pretrained supernetworks to achieve a trade-off front of cascades of different neural network architectures, maximizing accuracy while minimizing FLOPs count. We test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100, ImageNet) and achieve Pareto dominance over previous state-of-the-art NAS models up to 1.5 GFLOPs. Additionally, applying ENCAS to a pool of 518 publicly available ImageNet classifiers leads to Pareto dominance in all computation regimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied by an 18\% decrease in computation effort from 362 to 296 GFLOPs. Our code is available at https://github.com/AwesomeLemon/ENCAS



### Mutual Contrastive Low-rank Learning to Disentangle Whole Slide Image Representations for Glioma Grading
- **Arxiv ID**: http://arxiv.org/abs/2203.04013v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04013v2)
- **Published**: 2022-03-08 11:08:44+00:00
- **Updated**: 2022-10-04 15:16:46+00:00
- **Authors**: Lipei Zhang, Yiran Wei, Ying Fu, Stephen Price, Carola-Bibiane Sch√∂nlieb, Chao Li
- **Comment**: 14 pages, 4 figures, 3 tables; This paper was accepted by BMVC 2022
- **Journal**: None
- **Summary**: Whole slide images (WSI) provide valuable phenotypic information for histological assessment and malignancy grading of tumors. The WSI-based grading promises to provide rapid diagnostic support and facilitate digital health. Currently, the most commonly used WSIs are derived from formalin-fixed paraffin-embedded (FFPE) and Frozen section. The majority of automatic tumor grading models are developed based on FFPE sections, which could be affected by the artifacts introduced by tissue processing. The frozen section exists problems such as low quality that might influence training within single modality as well. To overcome this problem in a single modal training and achieve better multi-modal and discriminative representation disentanglement in brain tumor, we propose a mutual contrastive low-rank learning (MCL) scheme to integrate FFPE and frozen sections for glioma grading. We first design a mutual learning scheme to jointly optimize the model training based on FFPE and frozen sections. In this proposed scheme, we design a normalized modality contrastive loss (NMC-loss), which could promote to disentangle multi-modality complementary representation of FFPE and frozen sections from the same patient. To reduce intra-class variance, and increase inter-class margin at intra- and inter-patient levels, we conduct a low-rank (LR) loss. Our experiments show that the proposed scheme achieves better performance than the model trained based on each single modality or mixed modalities and even improves the feature extraction in classical attention-based multiple instances learning methods (MIL). The combination of NMC-loss and low-rank loss outperforms other typical contrastive loss functions.



### Dynamic Dual-Output Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2203.04304v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04304v2)
- **Published**: 2022-03-08 11:20:40+00:00
- **Updated**: 2022-03-15 10:11:01+00:00
- **Authors**: Yaniv Benny, Lior Wolf
- **Comment**: To be presented at CVPR 2022
- **Journal**: None
- **Summary**: Iterative denoising-based generation, also known as denoising diffusion models, has recently been shown to be comparable in quality to other classes of generative models, and even surpass them. Including, in particular, Generative Adversarial Networks, which are currently the state of the art in many sub-tasks of image generation. However, a major drawback of this method is that it requires hundreds of iterations to produce a competitive result. Recent works have proposed solutions that allow for faster generation with fewer iterations, but the image quality gradually deteriorates with increasingly fewer iterations being applied during generation. In this paper, we reveal some of the causes that affect the generation quality of diffusion models, especially when sampling with few iterations, and come up with a simple, yet effective, solution to mitigate them. We consider two opposite equations for the iterative denoising, the first predicts the applied noise, and the second predicts the image directly. Our solution takes the two options and learns to dynamically alternate between them through the denoising process. Our proposed solution is general and can be applied to any existing diffusion model. As we show, when applied to various SOTA architectures, our solution immediately improves their generation quality, with negligible added complexity and parameters. We experiment on multiple datasets and configurations and run an extensive ablation study to support these findings.



### Data augmentation with mixtures of max-entropy transformations for filling-level classification
- **Arxiv ID**: http://arxiv.org/abs/2203.04027v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.04027v1)
- **Published**: 2022-03-08 11:41:38+00:00
- **Updated**: 2022-03-08 11:41:38+00:00
- **Authors**: Apostolos Modas, Andrea Cavallaro, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of distribution shifts in test-time data with a principled data augmentation scheme for the task of content-level classification. In such a task, properties such as shape or transparency of test-time containers (cup or drinking glass) may differ from those represented in the training data. Dealing with such distribution shifts using standard augmentation schemes is challenging and transforming the training images to cover the properties of the test-time instances requires sophisticated image manipulations. We therefore generate diverse augmentations using a family of max-entropy transformations that create samples with new shapes, colors and spectral characteristics. We show that such a principled augmentation scheme, alone, can replace current approaches that use transfer learning or can be used in combination with transfer learning to improve its performance.



### Stage-Aware Feature Alignment Network for Real-Time Semantic Segmentation of Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/2203.04031v1
- **DOI**: 10.1109/TCSVT.2021.3121680
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04031v1)
- **Published**: 2022-03-08 11:46:41+00:00
- **Updated**: 2022-03-08 11:46:41+00:00
- **Authors**: Xi Weng, Yan Yan, Si Chen, Jing-Hao Xue, Hanzi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past few years, deep convolutional neural network-based methods have made great progress in semantic segmentation of street scenes. Some recent methods align feature maps to alleviate the semantic gap between them and achieve high segmentation accuracy. However, they usually adopt the feature alignment modules with the same network configuration in the decoder and thus ignore the different roles of stages of the decoder during feature aggregation, leading to a complex decoder structure. Such a manner greatly affects the inference speed. In this paper, we present a novel Stage-aware Feature Alignment Network (SFANet) based on the encoder-decoder structure for real-time semantic segmentation of street scenes. Specifically, a Stage-aware Feature Alignment module (SFA) is proposed to align and aggregate two adjacent levels of feature maps effectively. In the SFA, by taking into account the unique role of each stage in the decoder, a novel stage-aware Feature Enhancement Block (FEB) is designed to enhance spatial details and contextual information of feature maps from the encoder. In this way, we are able to address the misalignment problem with a very simple and efficient multi-branch decoder structure. Moreover, an auxiliary training strategy is developed to explicitly alleviate the multi-scale object problem without bringing additional computational costs during the inference phase. Experimental results show that the proposed SFANet exhibits a good balance between accuracy and speed for real-time semantic segmentation of street scenes. In particular, based on ResNet-18, SFANet respectively obtains 78.1% and 74.7% mean of class-wise Intersection-over-Union (mIoU) at inference speeds of 37 FPS and 96 FPS on the challenging Cityscapes and CamVid test datasets by using only a single GTX 1080Ti GPU.



### StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2203.04036v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04036v2)
- **Published**: 2022-03-08 12:06:12+00:00
- **Updated**: 2022-03-17 02:19:35+00:00
- **Authors**: Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, Yujiu Yang
- **Comment**: Project Page is at http://feiiyin.github.io/StyleHEAT/
- **Journal**: None
- **Summary**: One-shot talking face generation aims at synthesizing a high-quality talking face video from an arbitrary portrait image, driven by a video or an audio segment. One challenging quality factor is the resolution of the output video: higher resolution conveys more details. In this work, we investigate the latent feature space of a pre-trained StyleGAN and discover some excellent spatial transformation properties. Upon the observation, we explore the possibility of using a pre-trained StyleGAN to break through the resolution limit of training datasets. We propose a novel unified framework based on a pre-trained StyleGAN that enables a set of powerful functionalities, i.e., high-resolution video generation, disentangled control by driving video or audio, and flexible face editing. Our framework elevates the resolution of the synthesized talking face to 1024*1024 for the first time, even though the training dataset has a lower resolution. We design a video-based motion generation module and an audio-based one, which can be plugged into the framework either individually or jointly to drive the video generation. The predicted motion is used to transform the latent features of StyleGAN for visual animation. To compensate for the transformation distortion, we propose a calibration network as well as a domain loss to refine the features. Moreover, our framework allows two types of facial editing, i.e., global editing via GAN inversion and intuitive editing based on 3D morphable models. Comprehensive experiments show superior video quality, flexible controllability, and editability over state-of-the-art methods.



### Deep Multi-Branch Aggregation Network for Real-Time Semantic Segmentation in Street Scenes
- **Arxiv ID**: http://arxiv.org/abs/2203.04037v1
- **DOI**: 10.1109/TITS.2022.3150350
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04037v1)
- **Published**: 2022-03-08 12:07:32+00:00
- **Updated**: 2022-03-08 12:07:32+00:00
- **Authors**: Xi Weng, Yan Yan, Genshun Dong, Chang Shu, Biao Wang, Hanzi Wang, Ji Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time semantic segmentation, which aims to achieve high segmentation accuracy at real-time inference speed, has received substantial attention over the past few years. However, many state-of-the-art real-time semantic segmentation methods tend to sacrifice some spatial details or contextual information for fast inference, thus leading to degradation in segmentation quality. In this paper, we propose a novel Deep Multi-branch Aggregation Network (called DMA-Net) based on the encoder-decoder structure to perform real-time semantic segmentation in street scenes. Specifically, we first adopt ResNet-18 as the encoder to efficiently generate various levels of feature maps from different stages of convolutions. Then, we develop a Multi-branch Aggregation Network (MAN) as the decoder to effectively aggregate different levels of feature maps and capture the multi-scale information. In MAN, a lattice enhanced residual block is designed to enhance feature representations of the network by taking advantage of the lattice structure. Meanwhile, a feature transformation block is introduced to explicitly transform the feature map from the neighboring branch before feature aggregation. Moreover, a global context block is used to exploit the global contextual information. These key components are tightly combined and jointly optimized in a unified network. Extensive experimental results on the challenging Cityscapes and CamVid datasets demonstrate that our proposed DMA-Net respectively obtains 77.0% and 73.6% mean Intersection over Union (mIoU) at the inference speed of 46.7 FPS and 119.8 FPS by only using a single NVIDIA GTX 1080Ti GPU. This shows that DMA-Net provides a good tradeoff between segmentation quality and speed for semantic segmentation in street scenes.



### Gait Recognition with Mask-based Regularization
- **Arxiv ID**: http://arxiv.org/abs/2203.04038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04038v1)
- **Published**: 2022-03-08 12:13:29+00:00
- **Updated**: 2022-03-08 12:13:29+00:00
- **Authors**: Chuanfu Shen, Beibei Lin, Shunli Zhang, George Q. Huang, Shiqi Yu, Xin Yu
- **Comment**: 14 pages,4 figues
- **Journal**: None
- **Summary**: Most gait recognition methods exploit spatial-temporal representations from static appearances and dynamic walking patterns. However, we observe that many part-based methods neglect representations at boundaries. In addition, the phenomenon of overfitting on training data is relatively common in gait recognition, which is perhaps due to insufficient data and low-informative gait silhouettes. Motivated by these observations, we propose a novel mask-based regularization method named ReverseMask. By injecting perturbation on the feature map, the proposed regularization method helps convolutional architecture learn the discriminative representations and enhances generalization. Also, we design an Inception-like ReverseMask Block, which has three branches composed of a global branch, a feature dropping branch, and a feature scaling branch. Precisely, the dropping branch can extract fine-grained representations when partial activations are zero-outed. Meanwhile, the scaling branch randomly scales the feature map, keeping structural information of activations and preventing overfitting. The plug-and-play Inception-like ReverseMask block is simple and effective to generalize networks, and it also improves the performance of many state-of-the-art methods. Extensive experiments demonstrate that the ReverseMask regularization help baseline achieves higher accuracy and better generalization. Moreover, the baseline with Inception-like Block significantly outperforms state-of-the-art methods on the two most popular datasets, CASIA-B and OUMVLP. The source code will be released.



### Shape-invariant 3D Adversarial Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2203.04041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04041v2)
- **Published**: 2022-03-08 12:21:35+00:00
- **Updated**: 2022-03-22 14:43:14+00:00
- **Authors**: Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: Adversary and invisibility are two fundamental but conflict characters of adversarial perturbations. Previous adversarial attacks on 3D point cloud recognition have often been criticized for their noticeable point outliers, since they just involve an "implicit constrain" like global distance loss in the time-consuming optimization to limit the generated noise. While point cloud is a highly structured data format, it is hard to constrain its perturbation with a simple loss or metric properly. In this paper, we propose a novel Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility of point perturbations. This map reveals the vulnerability of point cloud recognition models when encountering shape-invariant adversarial noises. These noises are designed along the shape surface with an "explicit constrain" instead of extra distance loss. Specifically, we first apply a reversible coordinate transformation on each point of the point cloud input, to reduce one degree of point freedom and limit its movement on the tangent plane. Then we calculate the best attacking direction with the gradients of the transformed point cloud obtained on the white-box model. Finally we assign each point with a non-negative score to construct the sensitivity map, which benefits both white-box adversarial invisibility and black-box query-efficiency extended in our work. Extensive evaluations prove that our method can achieve the superior performance on various point cloud recognition models, with its satisfying adversarial imperceptibility and strong resistance to different point cloud defense settings. Our code is available at: https://github.com/shikiw/SI-Adv.



### Abandoning the Bayer-Filter to See in the Dark
- **Arxiv ID**: http://arxiv.org/abs/2203.04042v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04042v2)
- **Published**: 2022-03-08 12:22:31+00:00
- **Updated**: 2022-03-22 11:27:33+00:00
- **Authors**: Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang, Zhe Jin, Andrew Beng Jin Teoh, Jiajun Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light image enhancement - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from the colored raw image. Next, a fully convolutional network is proposed to achieve the low-light image enhancement by fusing colored raw data with synthesized monochrome raw data. Channel-wise attention is also introduced to the fusion process to establish a complementary interaction between features from colored and monochrome raw images. To train the convolutional networks, we propose a dataset with monochrome and color raw pairs named Mono-Colored Raw paired dataset (MCR) collected by using a monochrome camera without Bayer-Filter and a color camera with Bayer-Filter. The proposed pipeline take advantages of the fusion of the virtual monochrome and the color raw images and our extensive experiments indicate that significant improvement can be achieved by leveraging raw sensor data and data-driven learning.



### Diffusion Models for Medical Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.04306v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04306v2)
- **Published**: 2022-03-08 12:35:07+00:00
- **Updated**: 2022-10-05 13:54:33+00:00
- **Authors**: Julia Wolleb, Florentin Bieder, Robin Sandk√ºhler, Philippe C. Cattin
- **Comment**: None
- **Journal**: None
- **Summary**: In medical applications, weakly supervised anomaly detection methods are of great interest, as only image-level annotations are required for training. Current anomaly detection methods mainly rely on generative adversarial networks or autoencoder models. Those models are often complicated to train or have difficulties to preserve fine details in the image. We present a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. We combine the deterministic iterative noising and denoising scheme with classifier guidance for image-to-image translation between diseased and healthy subjects. Our method generates very detailed anomaly maps without the need for a complex training procedure. We evaluate our method on the BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.



### Graph Attention Transformer Network for Multi-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.04049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04049v1)
- **Published**: 2022-03-08 12:39:05+00:00
- **Updated**: 2022-03-08 12:39:05+00:00
- **Authors**: Jin Yuan, Shikai Chen, Yao Zhang, Zhongchao Shi, Xin Geng, Jianping Fan, Yong Rui
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label classification aims to recognize multiple objects or attributes from images. However, it is challenging to learn from proper label graphs to effectively characterize such inter-label correlations or dependencies. Current methods often use the co-occurrence probability of labels based on the training set as the adjacency matrix to model this correlation, which is greatly limited by the dataset and affects the model's generalization ability. In this paper, we propose a Graph Attention Transformer Network (GATN), a general framework for multi-label image classification that can effectively mine complex inter-label relationships. First, we use the cosine similarity based on the label word embedding as the initial correlation matrix, which can represent rich semantic information. Subsequently, we design the graph attention transformer layer to transfer this adjacency matrix to adapt to the current domain. Our extensive experiments have demonstrated that our proposed methods can achieve state-of-the-art performance on three datasets.



### BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs
- **Arxiv ID**: http://arxiv.org/abs/2203.04050v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04050v3)
- **Published**: 2022-03-08 12:39:51+00:00
- **Updated**: 2022-08-17 03:28:41+00:00
- **Authors**: Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang, Erkang Cheng
- **Comment**: Accepted at the IEEE Winter Conference on Applications of Computer
  Vision, WACV 2023
- **Journal**: None
- **Summary**: Semantic segmentation in bird's eye view (BEV) is an important task for autonomous driving. Though this task has attracted a large amount of research efforts, it is still challenging to flexibly cope with arbitrary (single or multiple) camera sensors equipped on the autonomous vehicle. In this paper, we present BEVSegFormer, an effective transformer-based method for BEV semantic segmentation from arbitrary camera rigs. Specifically, our method first encodes image features from arbitrary cameras with a shared backbone. These image features are then enhanced by a deformable transformer-based encoder. Moreover, we introduce a BEV transformer decoder module to parse BEV semantic segmentation results. An efficient multi-camera deformable attention unit is designed to carry out the BEV-to-image view transformation. Finally, the queries are reshaped according the layout of grids in the BEV, and upsampled to produce the semantic segmentation result in a supervised manner. We evaluate the proposed algorithm on the public nuScenes dataset and a self-collected dataset. Experimental results show that our method achieves promising performance on BEV semantic segmentation from arbitrary camera rigs. We also demonstrate the effectiveness of each component via ablation study.



### Counting with Adaptive Auxiliary Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.04061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04061v1)
- **Published**: 2022-03-08 13:10:17+00:00
- **Updated**: 2022-03-08 13:10:17+00:00
- **Authors**: Yanda Meng, Joshua Bridge, Meng Wei, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Xiaowei Huang, Yalin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an adaptive auxiliary task learning based approach for object counting problems. Unlike existing auxiliary task learning based methods, we develop an attention-enhanced adaptively shared backbone network to enable both task-shared and task-tailored features learning in an end-to-end manner. The network seamlessly combines standard Convolution Neural Network (CNN) and Graph Convolution Network (GCN) for feature extraction and feature reasoning among different domains of tasks. Our approach gains enriched contextual information by iteratively and hierarchically fusing the features across different task branches of the adaptive CNN backbone. The whole framework pays special attention to the objects' spatial locations and varied density levels, informed by object (or crowd) segmentation and density level segmentation auxiliary tasks. In particular, thanks to the proposed dilated contrastive density loss function, our network benefits from individual and regional context supervision in terms of pixel-independent and pixel-dependent feature learning mechanisms, along with strengthened robustness. Experiments on seven challenging multi-domain datasets demonstrate that our method achieves superior performance to the state-of-the-art auxiliary task learning based counting methods. Our code is made publicly available at: https://github.com/smallmax00/Counting_With_Adaptive_Auxiliary



### Analyzing General-Purpose Deep-Learning Detection and Segmentation Models with Images from a Lidar as a Camera Sensor
- **Arxiv ID**: http://arxiv.org/abs/2203.04064v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04064v1)
- **Published**: 2022-03-08 13:14:43+00:00
- **Updated**: 2022-03-08 13:14:43+00:00
- **Authors**: Yu Xianjia, Sahar Salimpour, Jorge Pe√±a Queralta, Tomi Westerlund
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last decade, robotic perception algorithms have significantly benefited from the rapid advances in deep learning (DL). Indeed, a significant amount of the autonomy stack of different commercial and research platforms relies on DL for situational awareness, especially vision sensors. This work explores the potential of general-purpose DL perception algorithms, specifically detection and segmentation neural networks, for processing image-like outputs of advanced lidar sensors. Rather than processing the three-dimensional point cloud data, this is, to the best of our knowledge, the first work to focus on low-resolution images with 360\textdegree field of view obtained with lidar sensors by encoding either depth, reflectivity, or near-infrared light in the image pixels. We show that with adequate preprocessing, general-purpose DL models can process these images, opening the door to their usage in environmental conditions where vision sensors present inherent limitations. We provide both a qualitative and quantitative analysis of the performance of a variety of neural network architectures. We believe that using DL models built for visual cameras offers significant advantages due to the much wider availability and maturity compared to point cloud-based perception.



### Geolocation estimation of target vehicles using image processing and geometric computation
- **Arxiv ID**: http://arxiv.org/abs/2203.10938v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.10938v1)
- **Published**: 2022-03-08 13:15:29+00:00
- **Updated**: 2022-03-08 13:15:29+00:00
- **Authors**: Elnaz Namazi, Rudolf Mester, Chaoru Lu, Jingyue Li
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating vehicles' locations is one of the key components in intelligent traffic management systems (ITMSs) for increasing traffic scene awareness. Traditionally, stationary sensors have been employed in this regard. The development of advanced sensing and communication technologies on modern vehicles (MVs) makes it feasible to use such vehicles as mobile sensors to estimate the traffic data of observed vehicles. This study aims to explore the capabilities of a monocular camera mounted on an MV in order to estimate the geolocation of the observed vehicle in a global positioning system (GPS) coordinate system. We proposed a new methodology by integrating deep learning, image processing, and geometric computation to address the observed-vehicle localization problem. To evaluate our proposed methodology, we developed new algorithms and tested them using real-world traffic data. The results indicated that our proposed methodology and algorithms could effectively estimate the observed vehicle's latitude and longitude dynamically.



### Lane Detection with Versatile AtrousFormer and Local Semantic Guidance
- **Arxiv ID**: http://arxiv.org/abs/2203.04067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04067v1)
- **Published**: 2022-03-08 13:25:35+00:00
- **Updated**: 2022-03-08 13:25:35+00:00
- **Authors**: Jiaxing Yang, Lihe Zhang, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based techniques. A few have a try on incorporating the recent adorable, the seq2seq Transformer \cite{transformer}. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately, in which the predicted Gaussian map of the starting point of each lane serves to guide the process. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.



### E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.04074v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04074v1)
- **Published**: 2022-03-08 13:36:23+00:00
- **Updated**: 2022-03-08 13:36:23+00:00
- **Authors**: Tao Zhang, Shiqing Wei, Shunping Ji
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Contour-based instance segmentation methods have developed rapidly recently but feature rough and hand-crafted front-end contour initialization, which restricts the model performance, and an empirical and fixed backend predicted-label vertex pairing, which contributes to the learning difficulty. In this paper, we introduce a novel contour-based method, named E2EC, for high-quality instance segmentation. Firstly, E2EC applies a novel learnable contour initialization architecture instead of hand-crafted contour initialization. This consists of a contour initialization module for constructing more explicit learning goals and a global contour deformation module for taking advantage of all of the vertices' features better. Secondly, we propose a novel label sampling scheme, named multi-direction alignment, to reduce the learning difficulty. Thirdly, to improve the quality of the boundary details, we dynamically match the most appropriate predicted-ground truth vertex pairs and propose the corresponding loss function named dynamic matching loss. The experiments showed that E2EC can achieve a state-of-the-art performance on the KITTI INStance (KINS) dataset, the Semantic Boundaries Dataset (SBD), the Cityscapes and the COCO dataset. E2EC is also efficient for use in real-time applications, with an inference speed of 36 fps for 512*512 images on an NVIDIA A6000 GPU. Code will be released at https://github.com/zhang-tao-whu/e2ec.



### Semantic Distillation Guided Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.04076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.04076v1)
- **Published**: 2022-03-08 13:40:51+00:00
- **Updated**: 2022-03-08 13:40:51+00:00
- **Authors**: Bo Xu, Guanze Liu, Han Huang, Cheng Lu, Yandong Guo
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: Most existing CNN-based salient object detection methods can identify local segmentation details like hair and animal fur, but often misinterpret the real saliency due to the lack of global contextual information caused by the subjectiveness of the SOD task and the locality of convolution layers. Moreover, due to the unrealistically expensive labeling costs, the current existing SOD datasets are insufficient to cover the real data distribution. The limitation and bias of the training data add additional difficulty to fully exploring the semantic association between object-to-object and object-to-environment in a given image. In this paper, we propose a semantic distillation guided SOD (SDG-SOD) method that produces accurate results by fusing semantically distilled knowledge from generated image captioning into the Vision-Transformer-based SOD framework. SDG-SOD can better uncover inter-objects and object-to-environment saliency and cover the gap between the subjective nature of SOD and its expensive labeling. Comprehensive experiments on five benchmark datasets demonstrate that the SDG-SOD outperforms the state-of-the-art approaches on four evaluation metrics, and largely improves the model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.



### Breast cancer detection using artificial intelligence techniques: A systematic literature review
- **Arxiv ID**: http://arxiv.org/abs/2203.04308v1
- **DOI**: 10.1016/j.artmed.2022.102276
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2203.04308v1)
- **Published**: 2022-03-08 13:51:17+00:00
- **Updated**: 2022-03-08 13:51:17+00:00
- **Authors**: Ali Bou Nassif, Manar Abu Talib, Qassim Nasir, Yaman Afadar, Omar Elgendy
- **Comment**: None
- **Journal**: Artificial Intelligence in Medicine, Elsevier, Vol 127, May 2022
- **Summary**: Cancer is one of the most dangerous diseases to humans, and yet no permanent cure has been developed for it. Breast cancer is one of the most common cancer types. According to the National Breast Cancer foundation, in 2020 alone, more than 276,000 new cases of invasive breast cancer and more than 48,000 non-invasive cases were diagnosed in the US. To put these figures in perspective, 64% of these cases are diagnosed early in the disease's cycle, giving patients a 99% chance of survival. Artificial intelligence and machine learning have been used effectively in detection and treatment of several dangerous diseases, helping in early diagnosis and treatment, and thus increasing the patient's chance of survival. Deep learning has been designed to analyze the most important features affecting detection and treatment of serious diseases. For example, breast cancer can be detected using genes or histopathological imaging. Analysis at the genetic level is very expensive, so histopathological imaging is the most common approach used to detect breast cancer. In this research work, we systematically reviewed previous work done on detection and treatment of breast cancer using genetic sequencing or histopathological imaging with the help of deep learning and machine learning. We also provide recommendations to researchers who will work in this field



### Contrastive Enhancement Using Latent Prototype for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.04095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04095v1)
- **Published**: 2022-03-08 14:02:32+00:00
- **Updated**: 2022-03-08 14:02:32+00:00
- **Authors**: Xiaoyu Zhao, Xiaoqian Chen, Zhiqiang Gong, Wen Yao, Yunyang Zhang, Xiaohu Zheng
- **Comment**: 18 pages, 4 figures
- **Journal**: None
- **Summary**: Few-shot segmentation enables the model to recognize unseen classes with few annotated examples. Most existing methods adopt prototype learning architecture, where support prototype vectors are expanded and concatenated with query features to perform conditional segmentation. However, such framework potentially focuses more on query features while may neglect the similarity between support and query features. This paper proposes a contrastive enhancement approach using latent prototypes to leverage latent classes and raise the utilization of similarity information between prototype and query features. Specifically, a latent prototype sampling module is proposed to generate pseudo-mask and novel prototypes based on features similarity. The module conveniently conducts end-to-end learning and has no strong dependence on clustering numbers like cluster-based method. Besides, a contrastive enhancement module is developed to drive models to provide different predictions with the same query features. Our method can be used as an auxiliary module to flexibly integrate into other baselines for a better segmentation performance. Extensive experiments show our approach remarkably improves the performance of state-of-the-art methods for 1-shot and 5-shot segmentation, especially outperforming baseline by 5.9% and 7.3% for 5-shot task on Pascal-5^i and COCO-20^i. Source code is available at https://github.com/zhaoxiaoyu1995/CELP-Pytorch



### VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer
- **Arxiv ID**: http://arxiv.org/abs/2203.04099v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2203.04099v2)
- **Published**: 2022-03-08 14:08:47+00:00
- **Updated**: 2022-07-19 16:54:03+00:00
- **Authors**: Juan F. Montesinos, Venkatesh S. Kadandale, Gloria Haro
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: This paper presents an audio-visual approach for voice separation which produces state-of-the-art results at a low latency in two scenarios: speech and singing voice. The model is based on a two-stage network. Motion cues are obtained with a lightweight graph convolutional network that processes face landmarks. Then, both audio and motion features are fed to an audio-visual transformer which produces a fairly good estimation of the isolated target source. In a second stage, the predominant voice is enhanced with an audio-only network. We present different ablation studies and comparison to state-of-the-art methods. Finally, we explore the transferability of models trained for speech separation in the task of singing voice separation. The demos, code, and weights are available in https://ipcv.github.io/VoViT/



### Predicting conversion of mild cognitive impairment to Alzheimer's disease
- **Arxiv ID**: http://arxiv.org/abs/2203.04725v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04725v1)
- **Published**: 2022-03-08 14:13:54+00:00
- **Updated**: 2022-03-08 14:13:54+00:00
- **Authors**: Yiran Wei, Stephen J. Price, Carola-Bibiane Sch√∂nlieb, Chao Li
- **Comment**: Under review
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is the most common age-related dementia. Mild cognitive impairment (MCI) is the early stage of cognitive decline before AD. It is crucial to predict the MCI-to-AD conversion for precise management, which remains challenging due to the diversity of patients. Previous evidence shows that the brain network generated from diffusion MRI promises to classify dementia using deep learning. However, the limited availability of diffusion MRI challenges the model training. In this study, we develop a self-supervised contrastive learning approach to generate structural brain networks from routine anatomical MRI under the guidance of diffusion MRI. The generated brain networks are applied to train a learning framework for predicting the MCI-to-AD conversion. Instead of directly modelling the AD brain networks, we train a graph encoder and a variational autoencoder to model the healthy ageing trajectories from brain networks of healthy controls. To predict the MCI-to-AD conversion, we further design a recurrent neural networks based approach to model the longitudinal deviation of patients' brain networks from the healthy ageing trajectory. Numerical results show that the proposed methods outperform the benchmarks in the prediction task. We also visualize the model interpretation to explain the prediction and identify abnormal changes of white matter tracts.



### Quantification of Occlusion Handling Capability of a 3D Human Pose Estimation Framework
- **Arxiv ID**: http://arxiv.org/abs/2203.04113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04113v1)
- **Published**: 2022-03-08 14:35:46+00:00
- **Updated**: 2022-03-08 14:35:46+00:00
- **Authors**: Mehwish Ghafoor, Arif Mahmood
- **Comment**: Accepted for publication in IEEE Transaction Multimedia, 2022
- **Journal**: None
- **Summary**: 3D human pose estimation using monocular images is an important yet challenging task. Existing 3D pose detection methods exhibit excellent performance under normal conditions however their performance may degrade due to occlusion. Recently some occlusion aware methods have also been proposed, however, the occlusion handling capability of these networks has not yet been thoroughly investigated. In the current work, we propose an occlusion-guided 3D human pose estimation framework and quantify its occlusion handling capability by using different protocols. The proposed method estimates more accurate 3D human poses using 2D skeletons with missing joints as input. Missing joints are handled by introducing occlusion guidance that provides extra information about the absence or presence of a joint. Temporal information has also been exploited to better estimate the missing joints. A large number of experiments are performed for the quantification of occlusion handling capability of the proposed method on three publicly available datasets in various settings including random missing joints, fixed body parts missing, and complete frames missing, using mean per joint position error criterion. In addition to that, the quality of the predicted 3D poses is also evaluated using action classification performance as a criterion. 3D poses estimated by the proposed method achieved significantly improved action recognition performance in the presence of missing joints. Our experiments demonstrate the effectiveness of the proposed framework for handling the missing joints as well as quantification of the occlusion handling capability of the deep neural networks.



### An Efficient Polyp Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2203.04118v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04118v2)
- **Published**: 2022-03-08 14:42:29+00:00
- **Updated**: 2022-04-22 00:44:25+00:00
- **Authors**: Tugberk Erol, Duygu Sarikaya
- **Comment**: 4 pages, in Turkish language, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Cancer is a disease that occurs as a result of the uncontrolled division and proliferation of cells. Colon cancer is one of the most common types of cancer in the world. Polyps that can be seen in the large intestine can cause cancer if not removed with early intervention. Deep learning and image segmentation techniques are used to minimize the number of polyps that goes unnoticed by the experts during these interventions. Although these techniques perform well in terms of accuracy, they require too many parameters. We propose a new model to address this problem. Our proposed model requires fewer parameters as well as outperforms the state-of-the-art models. We use EfficientNetB0 for the encoder part, as it performs well in various tasks while requiring fewer parameters. We use partial decoder, which is used to reduce the number of parameters while achieving high accuracy in segmentation. Since polyps have variable appearances and sizes, we use an asymmetric convolution block instead of a classic convolution block. Then, we weight each feature map using a squeeze and excitation block to improve our segmentation results. We used different splits of Kvasir and CVC-ClinicDB datasets for training, validation, and testing, while we use CVC- ColonDB, ETIS, and Endoscene datasets for testing. Our model outperforms state-of-art models with a Dice metric of %71.8 on the ColonDB test dataset, %89.3 on the EndoScene test dataset, and %74.8 on the ETIS test dataset while requiring fewer parameters. Our model requires 2.626.337 parameters in total while the closest model in the state-of-the-art is U-Net++ with 9.042.177 parameters.



### YouTube-GDD: A challenging gun detection dataset with rich contextual information
- **Arxiv ID**: http://arxiv.org/abs/2203.04129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.04129v1)
- **Published**: 2022-03-08 14:55:10+00:00
- **Updated**: 2022-03-08 14:55:10+00:00
- **Authors**: Yongxiang Gu, Xingbin Liao, Xiaolin Qin
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: An automatic gun detection system can detect potential gun-related violence at an early stage that is of paramount importance for citizens security. In the whole system, object detection algorithm is the key to perceive the environment so that the system can detect dangerous objects such as pistols and rifles. However, mainstream deep learning-based object detection algorithms depend heavily on large-scale high-quality annotated samples, and the existing gun datasets are characterized by low resolution, little contextual information and little data volume. To promote the development of security, this work presents a new challenging dataset called YouTube Gun Detection Dataset (YouTube-GDD). Our dataset is collected from 343 high-definition YouTube videos and contains 5000 well-chosen images, in which 16064 instances of gun and 9046 instances of person are annotated. Compared to other datasets, YouTube-GDD is "dynamic", containing rich contextual information and recording shape changes of the gun during shooting. To build a baseline for gun detection, we evaluate YOLOv5 on YouTube-GDD and analyze the influence of additional related annotated information on gun detection. YouTube-GDD and subsequent updates will be released at https://github.com/UCAS-GYX/YouTube-GDD.



### NeReF: Neural Refractive Field for Fluid Surface Reconstruction and Implicit Representation
- **Arxiv ID**: http://arxiv.org/abs/2203.04130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04130v1)
- **Published**: 2022-03-08 14:56:16+00:00
- **Updated**: 2022-03-08 14:56:16+00:00
- **Authors**: Ziyu Wang, Wei Yang, Junming Cao, Lan Xu, Junqing Yu, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing neural reconstruction schemes such as Neural Radiance Field (NeRF) are largely focused on modeling opaque objects. We present a novel neural refractive field(NeReF) to recover wavefront of transparent fluids by simultaneously estimating the surface position and normal of the fluid front. Unlike prior arts that treat the reconstruction target as a single layer of the surface, NeReF is specifically formulated to recover a volumetric normal field with its corresponding density field. A query ray will be refracted by NeReF according to its accumulated refractive point and normal, and we employ the correspondences and uniqueness of refracted ray for NeReF optimization. We show NeReF, as a global optimization scheme, can more robustly tackle refraction distortions detrimental to traditional methods for correspondence matching. Furthermore, the continuous NeReF representation of wavefront enables view synthesis as well as normal integration. We validate our approach on both synthetic and real data and show it is particularly suitable for sparse multi-view acquisition. We hence build a small light field array and experiment on various surface shapes to demonstrate high fidelity NeReF reconstruction.



### Motron: Multimodal Probabilistic Human Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2203.04132v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.04132v3)
- **Published**: 2022-03-08 14:58:41+00:00
- **Updated**: 2022-03-25 08:33:57+00:00
- **Authors**: Tim Salzmann, Marco Pavone, Markus Ryll
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Autonomous systems and humans are increasingly sharing the same space. Robots work side by side or even hand in hand with humans to balance each other's limitations. Such cooperative interactions are ever more sophisticated. Thus, the ability to reason not just about a human's center of gravity position, but also its granular motion is an important prerequisite for human-robot interaction. Though, many algorithms ignore the multimodal nature of humans or neglect uncertainty in their motion forecasts. We present Motron, a multimodal, probabilistic, graph-structured model, that captures human's multimodality using probabilistic methods while being able to output deterministic maximum-likelihood motions and corresponding confidence values for each mode. Our model aims to be tightly integrated with the robotic planning-control-interaction loop; outputting physically feasible human motions and being computationally efficient. We demonstrate the performance of our model on several challenging real-world motion forecasting datasets, outperforming a wide array of generative/variational methods while providing state-of-the-art single-output motions if required. Both using significantly less computational power than state-of-the art algorithms.



### Multi-Scale Adaptive Network for Single Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2203.04313v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04313v2)
- **Published**: 2022-03-08 15:13:20+00:00
- **Updated**: 2022-10-29 07:57:34+00:00
- **Authors**: Yuanbiao Gou, Peng Hu, Jiancheng Lv, Joey Tianyi Zhou, Xi Peng
- **Comment**: None
- **Journal**: the Thirty-Sixth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2022)
- **Summary**: Multi-scale architectures have shown effectiveness in a variety of tasks thanks to appealing cross-scale complementarity. However, existing architectures treat different scale features equally without considering the scale-specific characteristics, \textit{i.e.}, the within-scale characteristics are ignored in the architecture design. In this paper, we reveal this missing piece for multi-scale architecture design and accordingly propose a novel Multi-Scale Adaptive Network (MSANet) for single image denoising. Specifically, MSANet simultaneously embraces the within-scale characteristics and the cross-scale complementarity thanks to three novel neural blocks, \textit{i.e.}, adaptive feature block (AFeB), adaptive multi-scale block (AMB), and adaptive fusion block (AFuB). In brief, AFeB is designed to adaptively preserve image details and filter noises, which is highly expected for the features with mixed details and noises. AMB could enlarge the receptive field and aggregate the multi-scale information, which meets the need of contextually informative features. AFuB devotes to adaptively sampling and transferring the features from one scale to another scale, which fuses the multi-scale features with varying characteristics from coarse to fine. Extensive experiments on both three real and six synthetic noisy image datasets show the superiority of MSANet compared with 12 methods. The code could be accessed from https://github.com/XLearning-SCU/2022-NeurIPS-MSANet.



### Easy Ensemble: Simple Deep Ensemble Learning for Sensor-Based Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.04153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04153v1)
- **Published**: 2022-03-08 15:30:32+00:00
- **Updated**: 2022-03-08 15:30:32+00:00
- **Authors**: Tatsuhito Hasegawa, Kazuma Kondo
- **Comment**: 15 pages, 11 figures, this paper is a pre-print to submit to IEEE
  Internet of Things journal
- **Journal**: None
- **Summary**: Sensor-based human activity recognition (HAR) is a paramount technology in the Internet of Things services. HAR using representation learning, which automatically learns a feature representation from raw data, is the mainstream method because it is difficult to interpret relevant information from raw sensor data to design meaningful features. Ensemble learning is a robust approach to improve generalization performance; however, deep ensemble learning requires various procedures, such as data partitioning and training multiple models, which are time-consuming and computationally expensive. In this study, we propose Easy Ensemble (EE) for HAR, which enables the easy implementation of deep ensemble learning in a single model. In addition, we propose input masking as a method for diversifying the input for EE. Experiments on a benchmark dataset for HAR demonstrated the effectiveness of EE and input masking and their characteristics compared with conventional ensemble learning methods.



### Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2203.04156v1
- **DOI**: 10.1109/TKDE.2021.3112815
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04156v1)
- **Published**: 2022-03-08 15:37:44+00:00
- **Updated**: 2022-03-08 15:37:44+00:00
- **Authors**: Wenwen Qiang, Jiangmeng Li, Changwen Zheng, Bing Su, Hui Xiong
- **Comment**: Accepted by IEEE Transactions on Knowledge and Data Engineering
  (TKDE) 2022; Refer to https://ieeexplore.ieee.org/document/9540279
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) requires source domain samples with clean ground truth labels during training. Accurately labeling a large number of source domain samples is time-consuming and laborious. An alternative is to utilize samples with noisy labels for training. However, training with noisy labels can greatly reduce the performance of UDA. In this paper, we address the problem that learning UDA models only with access to noisy labels and propose a novel method called robust local preserving and global aligning network (RLPGA). RLPGA improves the robustness of the label noise from two aspects. One is learning a classifier by a robust informative-theoretic-based loss function. The other is constructing two adjacency weight matrices and two negative weight matrices by the proposed local preserving module to preserve the local topology structures of input data. We conduct theoretical analysis on the robustness of the proposed RLPGA and prove that the robust informative-theoretic-based loss and the local preserving module are beneficial to reduce the empirical risk of the target domain. A series of empirical studies show the effectiveness of our proposed RLPGA.



### PyNET-QxQ: An Efficient PyNET Variant for QxQ Bayer Pattern Demosaicing in CMOS Image Sensors
- **Arxiv ID**: http://arxiv.org/abs/2203.04314v2
- **DOI**: 10.1109/ACCESS.2023.3272665
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04314v2)
- **Published**: 2022-03-08 15:43:26+00:00
- **Updated**: 2023-05-05 08:39:04+00:00
- **Authors**: Minhyeok Cho, Haechang Lee, Hyunwoo Je, Kijeong Kim, Dongil Ryu, Albert No
- **Comment**: Accepted by IEEE Access
- **Journal**: None
- **Summary**: Deep learning-based image signal processor (ISP) models for mobile cameras can generate high-quality images that rival those of professional DSLR cameras. However, their computational demands often make them unsuitable for mobile settings. Additionally, modern mobile cameras employ non-Bayer color filter arrays (CFA) such as Quad Bayer, Nona Bayer, and QxQ Bayer to enhance image quality, yet most existing deep learning-based ISP (or demosaicing) models focus primarily on standard Bayer CFAs. In this study, we present PyNET-QxQ, a lightweight demosaicing model specifically designed for QxQ Bayer CFA patterns, which is derived from the original PyNET. We also propose a knowledge distillation method called progressive distillation to train the reduced network more effectively. Consequently, PyNET-QxQ contains less than 2.5% of the parameters of the original PyNET while preserving its performance. Experiments using QxQ images captured by a proto type QxQ camera sensor show that PyNET-QxQ outperforms existing conventional algorithms in terms of texture and edge reconstruction, despite its significantly reduced parameter count.



### Understanding Person Identification through Gait
- **Arxiv ID**: http://arxiv.org/abs/2203.04179v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04179v4)
- **Published**: 2022-03-08 16:09:54+00:00
- **Updated**: 2022-10-18 13:57:06+00:00
- **Authors**: Simon Hanisch, Evelyn Muschter, Admantini Hatzipanayioti, Shu-Chen Li, Thorsten Strufe
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition is the process of identifying humans from their bipedal locomotion such as walking or running. As such, gait data is privacy sensitive information and should be anonymized where possible. With the rise of higher quality gait recording techniques, such as depth cameras or motion capture suits, an increasing amount of detailed gait data is captured and processed. The introduction and rise of the Metaverse is an example of a potentially popular application scenario in which the gait of users is transferred onto digital avatars. As a first step towards developing effective anonymization techniques for high-quality gait data, we study different aspects of movement data to quantify their contribution to gait recognition. We first extract categories of features from the literature on human gait perception and then design experiments for each category to assess how much the information they contain contributes to recognition success. We evaluated the utility of gait perturbation by means of naturalness ratings in a user study. Our results show that gait anonymization will be challenging, as the data is highly redundant and inter-dependent.



### Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP)
- **Arxiv ID**: http://arxiv.org/abs/2203.04180v3
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04180v3)
- **Published**: 2022-03-08 16:11:41+00:00
- **Updated**: 2022-04-28 08:42:20+00:00
- **Authors**: Charles Millard, Mark Chiew, Jared Tanner, Aaron T. Hess, Boris Mailhe
- **Comment**: 10 pages, 9 figures, submitted to IEEE TMI on 26th April 2022
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) has excellent soft tissue contrast but is hindered by an inherently slow data acquisition process. Compressed sensing, which reconstructs sparse signals from incoherently sampled data, has been widely applied to accelerate MRI acquisitions. Compressed sensing MRI requires one or more model parameters to be tuned, which is usually done by hand, giving sub-optimal tuning in general. To address this issue, we build on previous work by the authors on the single-coil Variable Density Approximate Message Passing (VDAMP) algorithm, extending the framework to multiple receiver coils to propose the Parallel VDAMP (P-VDAMP) algorithm. For Bernoulli random variable density sampling, P-VDAMP obeys a "state evolution", where the intermediate per-iteration image estimate is distributed according to the ground truth corrupted by a zero-mean Gaussian vector with approximately known covariance. To our knowledge, P-VDAMP is the first algorithm for multi-coil MRI data that obeys a state evolution with accurately tracked parameters. We leverage state evolution to automatically tune sparse parameters on-the-fly with Stein's Unbiased Risk Estimate (SURE). P-VDAMP is evaluated on brain, knee and angiogram datasets and compared with four variants of the Fast Iterative Shrinkage-Thresholding algorithm (FISTA), including two tuning-free variants from the literature. The proposed method is found to have a similar reconstruction quality and time to convergence as FISTA with an optimally tuned sparse weighting and offers substantial robustness and reconstruction quality improvements over competing tuning-free methods.



### Selective-Supervised Contrastive Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2203.04181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04181v1)
- **Published**: 2022-03-08 16:12:08+00:00
- **Updated**: 2022-03-08 16:12:08+00:00
- **Authors**: Shikun Li, Xiaobo Xia, Shiming Ge, Tongliang Liu
- **Comment**: Accepted to CVPR 2022. 12 pages, 5 figure, and 10 tables
- **Journal**: None
- **Summary**: Deep networks have strong capacities of embedding data into latent representations and finishing following tasks. However, the capacities largely come from high-quality annotated labels, which are expensive to collect. Noisy labels are more affordable, but result in corrupted representations, leading to poor generalization performance. To learn robust representations and handle noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in this paper. Specifically, Sel-CL extend supervised contrastive learning (Sup-CL), which is powerful in representation learning, but is degraded when there are noisy labels. Sel-CL tackles the direct cause of the problem of Sup-CL. That is, as Sup-CL works in a \textit{pair-wise} manner, noisy pairs built by noisy labels mislead representation learning. To alleviate the issue, we select confident pairs out of noisy ones for Sup-CL without knowing noise rates. In the selection process, by measuring the agreement between learned representations and given labels, we first identify confident examples that are exploited to build confident pairs. Then, the representation similarity distribution in the built confident pairs is exploited to identify more confident pairs out of noisy pairs. All obtained confident pairs are finally used for Sup-CL to enhance representations. Experiments on multiple noisy datasets demonstrate the robustness of the learned representations by our method, following the state-of-the-art performance. Source codes are available at https://github.com/ShikunLi/Sel-CL



### RankSeg: Adaptive Pixel Classification with Image Category Ranking for Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.04187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04187v2)
- **Published**: 2022-03-08 16:25:30+00:00
- **Updated**: 2022-07-20 13:22:39+00:00
- **Authors**: Haodi He, Yuhui Yuan, Xiangyu Yue, Han Hu
- **Comment**: Accepted at ECCV 2022, Code will be available at:
  https://github.com/openseg-group/RankSeg
- **Journal**: None
- **Summary**: The segmentation task has traditionally been formulated as a complete-label pixel classification task to predict a class for each pixel from a fixed number of predefined semantic categories shared by all images or videos. Yet, following this formulation, standard architectures will inevitably encounter various challenges under more realistic settings where the scope of categories scales up (e.g., beyond the level of 1k). On the other hand, in a typical image or video, only a few categories, i.e., a small subset of the complete label are present. Motivated by this intuition, in this paper, we propose to decompose segmentation into two sub-problems: (i) image-level or video-level multi-label classification and (ii) pixel-level rank-adaptive selected-label classification. Given an input image or video, our framework first conducts multi-label classification over the complete label, then sorts the complete label and selects a small subset according to their class confidence scores. We then use a rank-adaptive pixel classifier to perform the pixel-wise classification over only the selected labels, which uses a set of rank-oriented learnable temperature parameters to adjust the pixel classifications scores. Our approach is conceptually general and can be used to improve various existing segmentation frameworks by simply using a lightweight multi-label classification head and rank-adaptive pixel classifier. We demonstrate the effectiveness of our framework with competitive experimental results across four tasks, including image semantic segmentation, image panoptic segmentation, video instance segmentation, and video semantic segmentation. Especially, with our RankSeg, Mask2Former gains +0.8%/+0.7%/+0.7% on ADE20K panoptic segmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic segmentation benchmarks respectively.



### A Gating Model for Bias Calibration in Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.04195v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04195v1)
- **Published**: 2022-03-08 16:41:06+00:00
- **Updated**: 2022-03-08 16:41:06+00:00
- **Authors**: Gukyeong Kwon, Ghassan AlRegib
- **Comment**: IEEE Transactions on Image Processing, 2022. Code is available at
  https://github.com/gukyeongkwon/gating-ae
- **Journal**: None
- **Summary**: Generalized zero-shot learning (GZSL) aims at training a model that can generalize to unseen class data by only using auxiliary information. One of the main challenges in GZSL is a biased model prediction toward seen classes caused by overfitting on only available seen class data during training. To overcome this issue, we propose a two-stream autoencoder-based gating model for GZSL. Our gating model predicts whether the query data is from seen classes or unseen classes, and utilizes separate seen and unseen experts to predict the class independently from each other. This framework avoids comparing the biased prediction scores for seen classes with the prediction scores for unseen classes. In particular, we measure the distance between visual and attribute representations in the latent space and the cross-reconstruction space of the autoencoder. These distances are utilized as complementary features to characterize unseen classes at different levels of data abstraction. Also, the two-stream autoencoder works as a unified framework for the gating model and the unseen expert, which makes the proposed method computationally efficient. We validate our proposed method in four benchmark image recognition datasets. In comparison with other state-of-the-art methods, we achieve the best harmonic mean accuracy in SUN and AWA2, and the second best in CUB and AWA1. Furthermore, our base model requires at least 20% less number of model parameters than state-of-the-art methods relying on generative models.



### Trustable Co-label Learning from Multiple Noisy Annotators
- **Arxiv ID**: http://arxiv.org/abs/2203.04199v1
- **DOI**: 10.1109/TMM.2021.3137752
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04199v1)
- **Published**: 2022-03-08 16:57:00+00:00
- **Updated**: 2022-03-08 16:57:00+00:00
- **Authors**: Shikun Li, Tongliang Liu, Jiyong Tan, Dan Zeng, Shiming Ge
- **Comment**: Accepted by IEEE TMM. 13 pages, 9 figures and 6 tables
- **Journal**: None
- **Summary**: Supervised deep learning depends on massive accurately annotated examples, which is usually impractical in many real-world scenarios. A typical alternative is learning from multiple noisy annotators. Numerous earlier works assume that all labels are noisy, while it is usually the case that a few trusted samples with clean labels are available. This raises the following important question: how can we effectively use a small amount of trusted data to facilitate robust classifier learning from multiple annotators? This paper proposes a data-efficient approach, called \emph{Trustable Co-label Learning} (TCL), to learn deep classifiers from multiple noisy annotators when a small set of trusted data is available. This approach follows the coupled-view learning manner, which jointly learns the data classifier and the label aggregator. It effectively uses trusted data as a guide to generate trustable soft labels (termed co-labels). A co-label learning can then be performed by alternately reannotating the pseudo labels and refining the classifiers. In addition, we further improve TCL for a special complete data case, where each instance is labeled by all annotators and the label aggregator is represented by multilayer neural networks to enhance model capacity. Extensive experiments on synthetic and real datasets clearly demonstrate the effectiveness and robustness of the proposed approach. Source code is available at https://github.com/ShikunLi/TCL



### AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant
- **Arxiv ID**: http://arxiv.org/abs/2203.04203v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04203v5)
- **Published**: 2022-03-08 17:07:09+00:00
- **Updated**: 2022-07-20 15:45:22+00:00
- **Authors**: Benita Wong, Joya Chen, You Wu, Stan Weixian Lei, Dongxing Mao, Difei Gao, Mike Zheng Shou
- **Comment**: Accepted by ECCV 2022. Equal contribution: Benita Wong, Joya Chen,
  You Wu; Corresponding author: Mike Zheng Shou
- **Journal**: None
- **Summary**: A long-standing goal of intelligent assistants such as AR glasses/robots has been to assist users in affordance-centric real-world scenarios, such as "how can I run the microwave for 1 minute?". However, there is still no clear task definition and suitable benchmarks. In this paper, we define a new task called Affordance-centric Question-driven Task Completion, where the AI assistant should learn from instructional videos to provide step-by-step help in the user's view. To support the task, we constructed AssistQ, a new dataset comprising 531 question-answer samples from 100 newly filmed instructional videos. We also developed a novel Question-to-Actions (Q2A) model to address the AQTC task and validate it on the AssistQ dataset. The results show that our model significantly outperforms several VQA-related baselines while still having large room for improvement. We expect our task and dataset to advance Egocentric AI Assistant's development. Our project page is available at: https://showlab.github.io/assistq/.



### Lightweight Monocular Depth Estimation through Guided Decoding
- **Arxiv ID**: http://arxiv.org/abs/2203.04206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.04206v1)
- **Published**: 2022-03-08 17:11:36+00:00
- **Updated**: 2022-03-08 17:11:36+00:00
- **Authors**: Michael Rudolph, Youssef Dawoud, Ronja G√ºldenring, Lazaros Nalpantidis, Vasileios Belagiannis
- **Comment**: Accepted to ICRA 2022
- **Journal**: None
- **Summary**: We present a lightweight encoder-decoder architecture for monocular depth estimation, specifically designed for embedded platforms. Our main contribution is the Guided Upsampling Block (GUB) for building the decoder of our model. Motivated by the concept of guided image filtering, GUB relies on the image to guide the decoder on upsampling the feature representation and the depth map reconstruction, achieving high resolution results with fine-grained details. Based on multiple GUBs, our model outperforms the related methods on the NYU Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano and 102.9 fps on the Xavier NX. Our code and models are made publicly available.



### Towards Universal Texture Synthesis by Combining Texton Broadcasting with Noise Injection in StyleGAN-2
- **Arxiv ID**: http://arxiv.org/abs/2203.04221v1
- **DOI**: 10.1016/j.prime.2022.100092
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04221v1)
- **Published**: 2022-03-08 17:44:35+00:00
- **Updated**: 2022-03-08 17:44:35+00:00
- **Authors**: Jue Lin, Gaurav Sharma, Thrasyvoulos N. Pappas
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new approach for universal texture synthesis by incorporating a multi-scale texton broadcasting module in the StyleGAN-2 framework. The texton broadcasting module introduces an inductive bias, enabling generation of broader range of textures, from those with regular structures to completely stochastic ones. To train and evaluate the proposed approach, we construct a comprehensive high-resolution dataset that captures the diversity of natural textures as well as stochastic variations within each perceptually uniform texture. Experimental results demonstrate that the proposed approach yields significantly better quality textures than the state of the art. The ultimate goal of this work is a comprehensive understanding of texture space.



### Neural Face Identification in a 2D Wireframe Projection of a Manifold Object
- **Arxiv ID**: http://arxiv.org/abs/2203.04229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04229v1)
- **Published**: 2022-03-08 17:47:51+00:00
- **Updated**: 2022-03-08 17:47:51+00:00
- **Authors**: Kehan Wang, Jia Zheng, Zihan Zhou
- **Comment**: To Appear in CVPR 2022. The project page is at
  https://manycore-research.github.io/faceformer
- **Journal**: None
- **Summary**: In computer-aided design (CAD) systems, 2D line drawings are commonly used to illustrate 3D object designs. To reconstruct the 3D models depicted by a single 2D line drawing, an important key is finding the edge loops in the line drawing which correspond to the actual faces of the 3D object. In this paper, we approach the classical problem of face identification from a novel data-driven point of view. We cast it as a sequence generation problem: starting from an arbitrary edge, we adopt a variant of the popular Transformer model to predict the edges associated with the same face in a natural order. This allows us to avoid searching the space of all possible edge loops with various hand-crafted rules and heuristics as most existing methods do, deal with challenging cases such as curved surfaces and nested edge loops, and leverage additional cues such as face types. We further discuss how possibly imperfect predictions can be used for 3D object reconstruction.



### A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2203.04232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04232v2)
- **Published**: 2022-03-08 17:49:07+00:00
- **Updated**: 2023-02-11 17:40:17+00:00
- **Authors**: Yan Xia, Qiangqiang Wu, Wei Li, Antoni B. Chan, Uwe Stilla
- **Comment**: Accepted by IEEE Transactions on Intelligent Transportation Systems
  2023
- **Journal**: None
- **Summary**: Recent works on 3D single object tracking treat the task as a target-specific 3D detection task, where an off-the-shelf 3D detector is commonly employed for the tracking. However, it is non-trivial to perform accurate target-specific detection since the point cloud of objects in raw LiDAR scans is usually sparse and incomplete. In this paper, we address this issue by explicitly leveraging temporal motion cues and propose DMT, a Detector-free Motion-prediction-based 3D Tracking network that completely removes the usage of complicated 3D detectors and is lighter, faster, and more accurate than previous trackers. Specifically, the motion prediction module is first introduced to estimate a potential target center of the current frame in a point-cloud-free manner. Then, an explicit voting module is proposed to directly regress the 3D box from the estimated target center. Extensive experiments on KITTI and NuScenes datasets demonstrate that our DMT can still achieve better performance (~10% improvement over the NuScenes dataset) and a faster tracking speed (i.e., 72 FPS) than state-of-the-art approaches without applying any complicated 3D detectors. Our code is released at \url{https://github.com/jimmy-dq/DMT}



### MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent
- **Arxiv ID**: http://arxiv.org/abs/2203.04317v2
- **DOI**: 10.1016/j.compmedimag.2023.102267
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2203.04317v2)
- **Published**: 2022-03-08 18:07:47+00:00
- **Updated**: 2023-07-26 13:43:04+00:00
- **Authors**: Soumick Chatterjee, Himanshi Bajaj, Istiyak H. Siddiquee, Nandish Bandi Subbarayappa, Steve Simon, Suraj Bangalore Shashidhar, Oliver Speck, Andreas N√ºrnberge
- **Comment**: None
- **Journal**: Computerized Medical Imaging and Graphics (2023): 102267
- **Summary**: Image registration is the process of bringing different images into a common coordinate system - a technique widely used in various applications of computer vision, such as remote sensing, image retrieval, and, most commonly, medical imaging. Deep learning based techniques have been applied successfully to tackle various complex medical image processing problems, including medical image registration. Over the years, several image registration techniques have been proposed using deep learning. Deformable image registration techniques such as Voxelmorph have been successful in capturing finer changes and providing smoother deformations. However, Voxelmorph, as well as ICNet and FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical view of the supplied image) and, therefore, cannot track large deformations. In order to tackle the aforementioned problems, this paper extends the Voxelmorph approach in three different ways. To improve the performance in case of small as well as large deformations, supervision of the model at different resolutions has been integrated using a multi-scale UNet. To support the network to learn and encode the minute structural co-relations of the given image-pairs, a self-constructing graph network (SCGNet) has been used as the latent of the multi-scale UNet - which can improve the learning process of the model and help the model to generalise better. And finally, to make the deformations inverse-consistent, cycle consistency loss has been employed. On the task of registration of brain MRIs, the proposed method achieved significant improvements over ANTs and VoxelMorph, obtaining a Dice score of 0.8013 \pm 0.0243 for intramodal and 0.6211 \pm 0.0309 for intermodal, while VoxelMorph achieved 0.7747 \pm 0.0260 and 0.6071 \pm 0.0510, respectively



### End-to-End Semi-Supervised Learning for Video Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.04251v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04251v3)
- **Published**: 2022-03-08 18:11:25+00:00
- **Updated**: 2022-07-01 05:36:23+00:00
- **Authors**: Akash Kumar, Yogesh Singh Rawat
- **Comment**: CVPR'22
- **Journal**: None
- **Summary**: In this work, we focus on semi-supervised learning for video action detection which utilizes both labeled as well as unlabeled data. We propose a simple end-to-end consistency based approach which effectively utilizes the unlabeled data. Video action detection requires both, action class prediction as well as a spatio-temporal localization of actions. Therefore, we investigate two types of constraints, classification consistency, and spatio-temporal consistency. The presence of predominant background and static regions in a video makes it challenging to utilize spatio-temporal consistency for action detection. To address this, we propose two novel regularization constraints for spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness. Both these aspects exploit the temporal continuity of action in videos and are found to be effective for utilizing unlabeled videos for action detection. We demonstrate the effectiveness of the proposed approach on two different action detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show the effectiveness of the proposed approach for video object segmentation on the Youtube-VOS which demonstrates its generalization capability The proposed approach achieves competitive performance by using merely 20% of annotations on UCF101-24 when compared with recent fully supervised methods. On UCF101-24, it improves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively, compared to supervised approach.



### Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap
- **Arxiv ID**: http://arxiv.org/abs/2203.04275v6
- **DOI**: 10.1016/j.asr.2023.03.036
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04275v6)
- **Published**: 2022-03-08 18:49:34+00:00
- **Updated**: 2023-08-17 22:45:06+00:00
- **Authors**: Tae Ha Park, Simone D'Amico
- **Comment**: Accepted to Advances in Space Research; fixed error on reporting
  translation from heatmaps
- **Journal**: None
- **Summary**: This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target domain images online at deployment. Specifically, ODR performs self-supervised entropy minimization of the predicted satellite foreground, thereby improving the CNN's performance on the target domain images without their pose labels and with minimal computational efforts. The GitHub repository for SPNv2 is available at https://github.com/tpark94/spnv2.



### Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2203.04279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04279v1)
- **Published**: 2022-03-08 18:55:11+00:00
- **Updated**: 2022-03-08 18:55:11+00:00
- **Authors**: Prune Truong, Martin Danelljan, Fisher Yu, Luc Van Gool
- **Comment**: Accepted at CVPR 2022 code:
  https://github.com/PruneTruong/DenseMatching
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2022
- **Summary**: We propose Probabilistic Warp Consistency, a weakly-supervised learning objective for semantic matching. Our approach directly supervises the dense matching scores predicted by the network, encoded as a conditional probability distribution. We first construct an image triplet by applying a known warp to one of the images in a pair depicting different instances of the same object class. Our probabilistic learning objectives are then derived using the constraints arising from the resulting image triplet. We further account for occlusion and background clutter present in real image pairs by extending our probabilistic output space with a learnable unmatched state. To supervise it, we design an objective between image pairs depicting different object classes. We validate our method by applying it to four recent semantic matching architectures. Our weakly-supervised approach sets a new state-of-the-art on four challenging semantic matching benchmarks. Lastly, we demonstrate that our objective also brings substantial improvements in the strongly-supervised regime, when combined with keypoint annotations.



### A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation
- **Arxiv ID**: http://arxiv.org/abs/2203.04287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04287v2)
- **Published**: 2022-03-08 18:59:56+00:00
- **Updated**: 2023-03-23 02:54:23+00:00
- **Authors**: Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, Stephen Lin
- **Comment**: Accepted by CVPR 2022. Code and models are available at:
  https://github.com/FangyunWei/SLRT
- **Journal**: None
- **Summary**: This paper proposes a simple transfer learning baseline for sign language translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily) contain only about 10K-20K pairs of sign videos, gloss annotations and texts, which are an order of magnitude smaller than typical parallel data for training spoken language translation models. Data is thus a bottleneck for training effective sign language translation models. To mitigate this problem, we propose to progressively pretrain the model from general-domain datasets that include a large amount of external supervision to within-domain datasets. Concretely, we pretrain the sign-to-gloss visual network on the general domain of human actions and the within-domain of a sign-to-gloss dataset, and pretrain the gloss-to-text translation network on the general domain of a multilingual corpus and the within-domain of a gloss-to-text corpus. The joint model is fine-tuned with an additional module named the visual-language mapper that connects the two networks. This simple baseline surpasses the previous state-of-the-art results on two sign language translation benchmarks, demonstrating the effectiveness of transfer learning. With its simplicity and strong performance, this approach can serve as a solid baseline for future research. Code and models are available at: https://github.com/FangyunWei/SLRT.



### Unrolled Primal-Dual Networks for Lensless Cameras
- **Arxiv ID**: http://arxiv.org/abs/2203.04353v1
- **DOI**: 10.1364/OE.475521
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04353v1)
- **Published**: 2022-03-08 19:21:39+00:00
- **Updated**: 2022-03-08 19:21:39+00:00
- **Authors**: Oliver Kingshott, Nick Antipa, Emrah Bostan, Kaan Ak≈üit
- **Comment**: 8 pages, 5 figures, not published at any conference
- **Journal**: None
- **Summary**: Conventional image reconstruction models for lensless cameras often assume that each measurement results from convolving a given scene with a single experimentally measured point-spread function. These image reconstruction models fall short in simulating lensless cameras truthfully as these models are not sophisticated enough to account for optical aberrations or scenes with depth variations. Our work shows that learning a supervised primal-dual reconstruction method results in image quality matching state of the art in the literature without demanding a large network capacity. This improvement stems from our primary finding that embedding learnable forward and adjoint models in a learned primal-dual optimization framework can even improve the quality of reconstructed images (+5dB PSNR) compared to works that do not correct for the model error. In addition, we built a proof-of-concept lensless camera prototype that uses a pseudo-random phase mask to demonstrate our point. Finally, we share the extensive evaluation of our learned model based on an open dataset and a dataset from our proof-of-concept lensless camera prototype.



### Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.04961v2
- **DOI**: 10.1117/12.2625781
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, I.2.0; I.4.0; I.5.0; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2203.04961v2)
- **Published**: 2022-03-08 19:37:08+00:00
- **Updated**: 2022-04-15 18:14:11+00:00
- **Authors**: Zuzanna Szafranowska, Richard Osuala, Bennet Breier, Kaisar Kushibar, Karim Lekadir, Oliver Diaz
- **Comment**: Draft accepted as oral presentation at International Workshop on
  Breast Imaging (IWBI) 2022. 9 pages, 3 figures
- **Journal**: 16th International Workshop on Breast Imaging (IWBI2022). 12286.
  2022. 169 -- 177
- **Summary**: Early detection of breast cancer in mammography screening via deep-learning based computer-aided detection systems shows promising potential in improving the curability and mortality rates of breast cancer. However, many clinical centres are restricted in the amount and heterogeneity of available data to train such models to (i) achieve promising performance and to (ii) generalise well across acquisition protocols and domains. As sharing data between centres is restricted due to patient privacy concerns, we propose a potential solution: sharing trained generative models between centres as substitute for real patient data. In this work, we use three well known mammography datasets to simulate three different centres, where one centre receives the trained generator of Generative Adversarial Networks (GANs) from the two remaining centres in order to augment the size and heterogeneity of its training dataset. We evaluate the utility of this approach on mammography patch classification on the test set of the GAN-receiving centre using two different classification models, (a) a convolutional neural network and (b) a transformer neural network. Our experiments demonstrate that shared GANs notably increase the performance of both transformer and convolutional classification models and highlight this approach as a viable alternative to inter-centre data sharing.



### Regularized Training of Intermediate Layers for Generative Models for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2203.04382v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04382v1)
- **Published**: 2022-03-08 20:30:49+00:00
- **Updated**: 2022-03-08 20:30:49+00:00
- **Authors**: Sean Gunn, Jorio Cocola, Paul Hand
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors when solving inverse problems. One challenge of using them is overcoming representation error, the fundamental limitation of the network in representing any particular signal. Recently, multiple proposed inversion algorithms reduce representation error by optimizing over intermediate layer representations. These methods are typically applied to generative models that were trained agnostic of the downstream inversion algorithm. In our work, we introduce a principle that if a generative model is intended for inversion using an algorithm based on optimization of intermediate layers, it should be trained in a way that regularizes those intermediate layers. We instantiate this principle for two notable recent inversion algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of these inversion algorithms, we introduce a new regularized GAN training algorithm and demonstrate that the learned generative model results in lower reconstruction errors across a wide range of under sampling ratios when solving compressed sensing, inpainting, and super-resolution problems.



### The Flag Median and FlagIRLS
- **Arxiv ID**: http://arxiv.org/abs/2203.04437v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.MG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2203.04437v1)
- **Published**: 2022-03-08 23:06:58+00:00
- **Updated**: 2022-03-08 23:06:58+00:00
- **Authors**: Nathan Mankovich, Emily King, Chris Peterson, Michael Kirby
- **Comment**: None
- **Journal**: None
- **Summary**: Finding prototypes (e.g., mean and median) for a dataset is central to a number of common machine learning algorithms. Subspaces have been shown to provide useful, robust representations for datasets of images, videos and more. Since subspaces correspond to points on a Grassmann manifold, one is led to consider the idea of a subspace prototype for a Grassmann-valued dataset. While a number of different subspace prototypes have been described, the calculation of some of these prototypes has proven to be computationally expensive while other prototypes are affected by outliers and produce highly imperfect clustering on noisy data. This work proposes a new subspace prototype, the flag median, and introduces the FlagIRLS algorithm for its calculation. We provide evidence that the flag median is robust to outliers and can be used effectively in algorithms like Linde-Buzo-Grey (LBG) to produce improved clusterings on Grassmannians. Numerical experiments include a synthetic dataset, the MNIST handwritten digits dataset, the Mind's Eye video dataset and the UCF YouTube action dataset. The flag median is compared the other leading algorithms for computing prototypes on the Grassmannian, namely, the $\ell_2$-median and to the flag mean. We find that using FlagIRLS to compute the flag median converges in $4$ iterations on a synthetic dataset. We also see that Grassmannian LBG with a codebook size of $20$ and using the flag median produces at least a $10\%$ improvement in cluster purity over Grassmannian LBG using the flag mean or $\ell_2$-median on the Mind's Eye dataset.



### Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4
- **Arxiv ID**: http://arxiv.org/abs/2203.06649v3
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2203.06649v3)
- **Published**: 2022-03-08 23:08:35+00:00
- **Updated**: 2022-10-17 20:49:10+00:00
- **Authors**: William Berrios, Arturo Deza
- **Comment**: Under review
- **Journal**: None
- **Summary**: Modern high-scoring models of vision in the brain score competition do not stem from Vision Transformers. However, in this paper, we provide evidence against the unexpected trend of Vision Transformers (ViT) being not perceptually aligned with human visual representations by showing how a dual-stream Transformer, a CrossViT$~\textit{a la}$ Chen et al. (2021), under a joint rotationally-invariant and adversarial optimization procedure yields 2nd place in the aggregate Brain-Score 2022 competition(Schrimpf et al., 2020b) averaged across all visual categories, and at the time of the competition held 1st place for the highest explainable variance of area V4. In addition, our current Transformer-based model also achieves greater explainable variance for areas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that integrates a frontal V1-like computation module (Dapello et al.,2020). To assess the contribution of the optimization scheme with respect to the CrossViT architecture, we perform several additional experiments on differently optimized CrossViT's regarding adversarial robustness, common corruption benchmarks, mid-ventral stimuli interpretation and feature inversion. Against our initial expectations, our family of results provides tentative support for an $\textit{"All roads lead to Rome"}$ argument enforced via a joint optimization rule even for non biologically-motivated models of vision such as Vision Transformers. Code is available at https://github.com/williamberrios/BrainScore-Transformers



### Pointillism: Accurate 3D bounding box estimation with multi-radars
- **Arxiv ID**: http://arxiv.org/abs/2203.04440v1
- **DOI**: 10.1145/3384419.3430783
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04440v1)
- **Published**: 2022-03-08 23:09:58+00:00
- **Updated**: 2022-03-08 23:09:58+00:00
- **Authors**: Kshitiz Bansal, Keshav Rungta, Siyuan Zhu, Dinesh Bharadia
- **Comment**: Accepted in SenSys '20. Dataset has been made publicly available
- **Journal**: Proceedings of the 18th Conference on Embedded Networked Sensor
  Systems. Pages 340-353, 2020
- **Summary**: Autonomous perception requires high-quality environment sensing in the form of 3D bounding boxes of dynamic objects. The primary sensors used in automotive systems are light-based cameras and LiDARs. However, they are known to fail in adverse weather conditions. Radars can potentially solve this problem as they are barely affected by adverse weather conditions. However, specular reflections of wireless signals cause poor performance of radar point clouds. We introduce Pointillism, a system that combines data from multiple spatially separated radars with an optimal separation to mitigate these problems. We introduce a novel concept of Cross Potential Point Clouds, which uses the spatial diversity induced by multiple radars and solves the problem of noise and sparsity in radar point clouds. Furthermore, we present the design of RP-net, a novel deep learning architecture, designed explicitly for radar's sparse data distribution, to enable accurate 3D bounding box estimation. The spatial techniques designed and proposed in this paper are fundamental to radars point cloud distribution and would benefit other radar sensing applications.



### Self-Supervision, Remote Sensing and Abstraction: Representation Learning Across 3 Million Locations
- **Arxiv ID**: http://arxiv.org/abs/2203.04445v1
- **DOI**: 10.1109/DICTA52665.2021.9647061
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.04445v1)
- **Published**: 2022-03-08 23:32:33+00:00
- **Updated**: 2022-03-08 23:32:33+00:00
- **Authors**: Sachith Seneviratne, Kerry A. Nice, Jasper S. Wijnands, Mark Stevenson, Jason Thompson
- **Comment**: Preprint of https://ieeexplore.ieee.org/document/9647061
- **Journal**: None
- **Summary**: Self-supervision based deep learning classification approaches have received considerable attention in academic literature. However, the performance of such methods on remote sensing imagery domains remains under-explored. In this work, we explore contrastive representation learning methods on the task of imagery-based city classification, an important problem in urban computing. We use satellite and map imagery across 2 domains, 3 million locations and more than 1500 cities. We show that self-supervised methods can build a generalizable representation from as few as 200 cities, with representations achieving over 95\% accuracy in unseen cities with minimal additional training. We also find that the performance discrepancy of such methods, when compared to supervised methods, induced by the domain discrepancy between natural imagery and abstract imagery is significant for remote sensing imagery. We compare all analysis against existing supervised models from academic literature and open-source our models for broader usage and further criticism.



### Self-Supervised Domain Calibration and Uncertainty Estimation for Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.04446v3
- **DOI**: 10.1109/LRA.2022.3232033
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.04446v3)
- **Published**: 2022-03-08 23:35:04+00:00
- **Updated**: 2023-03-14 01:13:36+00:00
- **Authors**: Pierre-Yves Lajoie, Giovanni Beltrame
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 8 (2023) 792 - 799
- **Summary**: Visual place recognition techniques based on deep learning, which have imposed themselves as the state-of-the-art in recent years, do not generalize well to environments visually different from the training set. Thus, to achieve top performance, it is sometimes necessary to fine-tune the networks to the target environment. To this end, we propose a self-supervised domain calibration procedure based on robust pose graph optimization from Simultaneous Localization and Mapping (SLAM) as the supervision signal without requiring GPS or manual labeling. Moreover, we leverage the procedure to improve uncertainty estimation for place recognition matches which is important in safety critical applications. We show that our approach can improve the performance of a state-of-the-art technique on a target environment dissimilar from its training set and that we can obtain uncertainty estimates. We believe that this approach will help practitioners to deploy robust place recognition solutions in real-world applications. Our code is available publicly: https://github.com/MISTLab/vpr-calibration-and-uncertainty



### How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?
- **Arxiv ID**: http://arxiv.org/abs/2203.04450v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.04450v3)
- **Published**: 2022-03-08 23:44:01+00:00
- **Updated**: 2023-04-15 07:25:57+00:00
- **Authors**: Yifei Ming, Yiyou Sun, Ousmane Dia, Yixuan Li
- **Comment**: Published at ICLR 2023
- **Journal**: The Eleventh International Conference on Learning Representations,
  2023
- **Summary**: Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hyperspherical space, and demonstrate the importance of dispersion and compactness. CIDER establishes superior performance, outperforming the latest rival by 19.36% in FPR95. Code is available at https://github.com/deeplearning-wisc/cider.




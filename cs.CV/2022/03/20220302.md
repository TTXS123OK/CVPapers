# Arxiv Papers in cs.CV on 2022-03-02
### NESTANets: Stable, accurate and efficient neural networks for analysis-sparse inverse problems
- **Arxiv ID**: http://arxiv.org/abs/2203.00804v2
- **DOI**: 10.1007/s43670-022-00043-5
- **Categories**: **cs.LG**, cs.CV, cs.IT, cs.NA, eess.IV, math.IT, math.NA, E.4; H.1.1; G.1.6; I.2.6; I.4.5; I.5.1; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2203.00804v2)
- **Published**: 2022-03-02 00:44:25+00:00
- **Updated**: 2022-10-20 21:25:25+00:00
- **Authors**: Maksym Neyra-Nesterenko, Ben Adcock
- **Comment**: None
- **Journal**: Sampl. Theory Signal Process. Data Anal. 21, 4 (2023)
- **Summary**: Solving inverse problems is a fundamental component of science, engineering and mathematics. With the advent of deep learning, deep neural networks have significant potential to outperform existing state-of-the-art, model-based methods for solving inverse problems. However, it is known that current data-driven approaches face several key issues, notably hallucinations, instabilities and unpredictable generalization, with potential impact in critical tasks such as medical imaging. This raises the key question of whether or not one can construct deep neural networks for inverse problems with explicit stability and accuracy guarantees. In this work, we present a novel construction of accurate, stable and efficient neural networks for inverse problems with general analysis-sparse models, termed NESTANets. To construct the network, we first unroll NESTA, an accelerated first-order method for convex optimization. The slow convergence of this method leads to deep networks with low efficiency. Therefore, to obtain shallow, and consequently more efficient, networks we combine NESTA with a novel restart scheme. We then use compressed sensing techniques to demonstrate accuracy and stability. We showcase this approach in the case of Fourier imaging, and verify its stability and performance via a series of numerical experiments. The key impact of this work is demonstrating the construction of efficient neural networks based on unrolling with guaranteed stability and accuracy.



### InCloud: Incremental Learning for Point Cloud Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.00807v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00807v3)
- **Published**: 2022-03-02 00:56:49+00:00
- **Updated**: 2022-11-29 07:24:13+00:00
- **Authors**: Joshua Knights, Peyman Moghadam, Milad Ramezani, Sridha Sridharan, Clinton Fookes
- **Comment**: 2022 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2022)
- **Journal**: None
- **Summary**: Place recognition is a fundamental component of robotics, and has seen tremendous improvements through the use of deep learning models in recent years. Networks can experience significant drops in performance when deployed in unseen or highly dynamic environments, and require additional training on the collected data. However naively fine-tuning on new training distributions can cause severe degradation of performance on previously visited domains, a phenomenon known as catastrophic forgetting. In this paper we address the problem of incremental learning for point cloud place recognition and introduce InCloud, a structure-aware distillation-based approach which preserves the higher-order structure of the network's embedding space. We introduce several challenging new benchmarks on four popular and large-scale LiDAR datasets (Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud place recognition performance over a variety of network architectures. To the best of our knowledge, this work is the first to effectively apply incremental learning for point cloud place recognition. Data pre-processing, training and evaluation code for this paper can be found at https://github.com/csiro-robotics/InCloud.



### Instance-aware multi-object self-supervision for monocular depth prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.00809v2
- **DOI**: 10.1109/LRA.2022.3194951
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00809v2)
- **Published**: 2022-03-02 00:59:25+00:00
- **Updated**: 2022-08-09 06:46:57+00:00
- **Authors**: Houssem Boulahbal, Adrian Voicila, Andrew Comport
- **Comment**: IROS 2022 and RAL
- **Journal**: None
- **Summary**: This paper proposes a self-supervised monocular image-to-depth prediction framework that is trained with an end-to-end photometric loss that handles not only 6-DOF camera motion but also 6-DOF moving object instances. Self-supervision is performed by warping the images across a video sequence using depth and scene motion including object instances. One novelty of the proposed method is the use of the multi-head attention of the transformer network that matches moving objects across time and models their interaction and dynamics. This enables accurate and robust pose estimation for each object instance. Most image-to-depth predication frameworks make the assumption of rigid scenes, which largely degrades their performance with respect to dynamic objects. Only a few SOTA papers have accounted for dynamic objects. The proposed method is shown to outperform these methods on standard benchmarks and the impact of the dynamic motion on these benchmarks is exposed. Furthermore, the proposed image-to-depth prediction framework is also shown to be competitive with SOTA video-to-depth prediction frameworks.



### Robust Seatbelt Detection and Usage Recognition for Driver Monitoring Systems
- **Arxiv ID**: http://arxiv.org/abs/2203.00810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00810v1)
- **Published**: 2022-03-02 01:04:03+00:00
- **Updated**: 2022-03-02 01:04:03+00:00
- **Authors**: Feng Hu
- **Comment**: AAAI 2022 Workshop on Trustworthy Autonomous Systems Engineering 2022
  (https://jinghany.github.io/trase2022/program/)
- **Journal**: None
- **Summary**: Wearing a seatbelt appropriately while driving can reduce serious crash-related injuries or deaths by about half. However, current seatbelt reminder system has multiple shortcomings, such as can be easily fooled by a "Seatbelt Warning Stopper", and cannot recognize incorrect usages for example seating in front of a buckled seatbelt or wearing a seatbelt under the arm. General seatbelt usage recognition has many challenges, to name a few, lacking of color information in Infrared (IR) cameras, strong distortion caused by wide Field of View (FoV) fisheye lens, low contrast between belt and its background, occlusions caused by hands or hair, and imaging blurry. In this paper, we introduce a novel general seatbelt detection and usage recognition framework to resolve the above challenges. Our method consists of three components: a local predictor, a global assembler, and a shape modeling process. Our approach can be applied to the driver in the Driver Monitoring System (DMS) or general passengers in the Occupant Monitoring System (OMS) for various camera modalities. Experiment results on both DMS and OMS are provided to demonstrate the accuracy and robustness of the proposed approach.



### 3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.00828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00828v1)
- **Published**: 2022-03-02 02:42:14+00:00
- **Updated**: 2022-03-02 02:42:14+00:00
- **Authors**: Dening Lu, Qian Xie, Linlin Xu, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Although accurate and fast point cloud classification is a fundamental task in 3D applications, it is difficult to achieve this purpose due to the irregularity and disorder of point clouds that make it challenging to achieve effective and efficient global discriminative feature learning. Lately, 3D Transformers have been adopted to improve point cloud processing. Nevertheless, massive Transformer layers tend to incur huge computational and memory costs. This paper presents a novel hierarchical framework that incorporates convolution with Transformer for point cloud classification, named 3D Convolution-Transformer Network (3DCTN), to combine the strong and efficient local feature learning ability of convolution with the remarkable global context modeling capability of Transformer. Our method has two main modules operating on the downsampling point sets, and each module consists of a multi-scale local feature aggregating (LFA) block and a global feature learning (GFL) block, which are implemented by using Graph Convolution and Transformer respectively. We also conduct a detailed investigation on a series of Transformer variants to explore better performance for our network. Various experiments on ModelNet40 demonstrate that our method achieves state-of-the-art classification performance, in terms of both accuracy and efficiency.



### Adaptive Discriminative Regularization for Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.00833v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00833v3)
- **Published**: 2022-03-02 02:52:23+00:00
- **Updated**: 2023-01-11 14:06:36+00:00
- **Authors**: Qingsong Zhao, Yi Wang, Shuguang Dou, Chen Gong, Yin Wang, Cairong Zhao
- **Comment**: We submit it to a Journal this time
- **Journal**: None
- **Summary**: How to improve discriminative feature learning is central in classification. Existing works address this problem by explicitly increasing inter-class separability and intra-class similarity, whether by constructing positive and negative pairs for contrastive learning or posing tighter class separating margins. These methods do not exploit the similarity between different classes as they adhere to i.i.d. assumption in data. In this paper, we embrace the real-world data distribution setting that some classes share semantic overlaps due to their similar appearances or concepts. Regarding this hypothesis, we propose a novel regularization to improve discriminative learning. We first calibrate the estimated highest likelihood of one sample based on its semantically neighboring classes, then encourage the overall likelihood predictions to be deterministic by imposing an adaptive exponential penalty. As the gradient of the proposed method is roughly proportional to the uncertainty of the predicted likelihoods, we name it adaptive discriminative regularization (ADR), trained along with a standard cross entropy loss in classification. Extensive experiments demonstrate that it can yield consistent and non-trivial performance improvements in a variety of visual classification tasks (over 10 benchmarks). Furthermore, we find it is robust to long-tailed and noisy label data distribution. Its flexible design enables its compatibility with mainstream classification architectures and losses.



### OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion
- **Arxiv ID**: http://arxiv.org/abs/2203.00838v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00838v2)
- **Published**: 2022-03-02 03:19:49+00:00
- **Updated**: 2022-03-29 02:33:16+00:00
- **Authors**: Yuyan Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Ye Duan, Liu Ren
- **Comment**: CVPR 2022, accepted as Oral
- **Journal**: None
- **Summary**: A well-known challenge in applying deep-learning methods to omnidirectional images is spherical distortion. In dense regression tasks such as depth estimation, where structural details are required, using a vanilla CNN layer on the distorted 360 image results in undesired information loss. In this paper, we propose a 360 monocular depth estimation pipeline, OmniFusion, to tackle the spherical distortion issue. Our pipeline transforms a 360 image into less-distorted perspective patches (i.e. tangent images) to obtain patch-wise predictions via CNN, and then merge the patch-wise results for final output. To handle the discrepancy between patch-wise predictions which is a major issue affecting the merging quality, we propose a new framework with the following key components. First, we propose a geometry-aware feature fusion mechanism that combines 3D geometric features with 2D image features to compensate for the patch-wise discrepancy. Second, we employ the self-attention-based transformer architecture to conduct a global aggregation of patch-wise information, which further improves the consistency. Last, we introduce an iterative depth refinement mechanism, to further refine the estimated depth based on the more accurate geometric features. Experiments show that our method greatly mitigates the distortion issue, and achieves state-of-the-art performances on several 360 monocular depth estimation benchmark datasets.



### X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning
- **Arxiv ID**: http://arxiv.org/abs/2203.00843v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00843v3)
- **Published**: 2022-03-02 03:35:37+00:00
- **Updated**: 2022-04-06 11:55:04+00:00
- **Authors**: Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Zhen Li, Shuguang Cui
- **Comment**: To appear in CVPR2022
- **Journal**: None
- **Summary**: 3D dense captioning aims to describe individual objects by natural language in 3D scenes, where 3D scenes are usually represented as RGB-D scans or point clouds. However, only exploiting single modal information, e.g., point cloud, previous approaches fail to produce faithful descriptions. Though aggregating 2D features into point clouds may be beneficial, it introduces an extra computational burden, especially in inference phases. In this study, we investigate a cross-modal knowledge transfer using Transformer for 3D dense captioning, X-Trans2Cap, to effectively boost the performance of single-modal 3D caption through knowledge distillation using a teacher-student framework. In practice, during the training phase, the teacher network exploits auxiliary 2D modality and guides the student network that only takes point clouds as input through the feature consistency constraints. Owing to the well-designed cross-modal feature fusion module and the feature alignment in the training phase, X-Trans2Cap acquires rich appearance information embedded in 2D images with ease. Thus, a more faithful caption can be generated only using point clouds during the inference. Qualitative and quantitative results confirm that X-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e., about +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets, respectively.



### Can No-reference features help in Full-reference image quality estimation?
- **Arxiv ID**: http://arxiv.org/abs/2203.00845v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00845v1)
- **Published**: 2022-03-02 03:39:28+00:00
- **Updated**: 2022-03-02 03:39:28+00:00
- **Authors**: Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah
- **Comment**: Code to be updated on: https://github.com/saikatdutta/nr-in-friqa
- **Journal**: None
- **Summary**: Development of perceptual image quality assessment (IQA) metrics has been of significant interest to computer vision community. The aim of these metrics is to model quality of an image as perceived by humans. Recent works in Full-reference IQA research perform pixelwise comparison between deep features corresponding to query and reference images for quality prediction. However, pixelwise feature comparison may not be meaningful if distortion present in query image is severe. In this context, we explore utilization of no-reference features in Full-reference IQA task. Our model consists of both full-reference and no-reference branches. Full-reference branches use both distorted and reference images, whereas No-reference branch only uses distorted image. Our experiments show that use of no-reference features boosts performance of image quality assessment. Our model achieves higher SRCC and KRCC scores than a number of state-of-the-art algorithms on KADID-10K and PIPAL datasets.



### Recent, rapid advancement in visual question answering architecture: a review
- **Arxiv ID**: http://arxiv.org/abs/2203.01322v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.01322v4)
- **Published**: 2022-03-02 03:39:53+00:00
- **Updated**: 2022-07-09 23:19:58+00:00
- **Authors**: Venkat Kodali, Daniel Berleant
- **Comment**: 8 pages. Accepted to EIT2022 conference and posted on ArXiv in
  accordance with IEEE policy
- **Journal**: None
- **Summary**: Understanding visual question answering is going to be crucial for numerous human activities. However, it presents major challenges at the heart of the artificial intelligence endeavor. This paper presents an update on the rapid advancements in visual question answering using images that have occurred in the last couple of years. Tremendous growth in research on improving visual question answering system architecture has been published recently, showing the importance of multimodal architectures. Several points on the benefits of visual question answering are mentioned in the review paper by Manmadhan et al. (2020), on which the present article builds, including subsequent updates in the field.



### Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation
- **Arxiv ID**: http://arxiv.org/abs/2203.01323v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.01323v1)
- **Published**: 2022-03-02 03:53:21+00:00
- **Updated**: 2022-03-02 03:53:21+00:00
- **Authors**: Wei Dai, Daniel Berleant
- **Comment**: 10 pages. arXiv admin note: text overlap with arXiv:2103.03102
- **Journal**: None
- **Summary**: Accuracies of deep learning (DL) classifiers are often unstable in that they may change significantly when retested on adversarial images, imperfect images, or perturbed images. This paper adds to the fundamental body of work on benchmarking the robustness of DL classifiers on defective images. To measure robust DL classifiers, previous research reported on single-factor corruption. We created comprehensive 69 benchmarking image sets, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. The state-of-the-art two-factor perturbation includes (a) two digital perturbations (salt & pepper noise and Gaussian noise) applied in both sequences, and (b) one digital perturbation (salt & pepper noise) and a geometric perturbation (rotation) applied in both sequences. Previous research evaluating DL classifiers has often used top-1/top-5 accuracy. We innovate a new two-dimensional, statistical matrix to evaluating robustness of DL classifiers. Also, we introduce a new visualization tool, including minimum accuracy, maximum accuracy, mean accuracies, and coefficient of variation (CV), for benchmarking robustness of DL classifiers. Comparing with single factor corruption, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. All source codes and related image sets are shared on the Website at http://cslinux.semo.edu/david/data to support future academic research and industry projects.



### Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2203.00858v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00858v2)
- **Published**: 2022-03-02 04:19:16+00:00
- **Updated**: 2022-07-13 07:15:27+00:00
- **Authors**: Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang
- **Comment**: Accepted by ACM MultiMedia 2022
- **Journal**: None
- **Summary**: Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the sensor data and perceive the environment. However, DNN models are vulnerable to different types of adversarial attacks, which pose significant risks to the security and safety of the vehicles and passengers. One prominent threat is the backdoor attack, where the adversary can compromise the DNN model by poisoning the training samples. Although lots of effort has been devoted to the investigation of the backdoor attack to conventional computer vision tasks, its practicality and applicability to the autonomous driving scenario is rarely explored, especially in the physical world.   In this paper, we target the lane detection system, which is an indispensable module for many autonomous driving tasks, e.g., navigation, lane switching. We design and realize the first physical backdoor attacks to such system. Our attacks are comprehensively effective against different types of lane detection algorithms. Specifically, we introduce two attack methodologies (poison-annotation and clean-annotation) to generate poisoned samples. With those samples, the trained lane detection model will be infected with the backdoor, and can be activated by common objects (e.g., traffic cones) to make wrong detections, leading the vehicle to drive off the road or onto the opposite lane. Extensive evaluations on public datasets and physical autonomous vehicles demonstrate that our backdoor attacks are effective, stealthy and robust against various defense solutions. Our codes and experimental videos can be found in https://sites.google.com/view/lane-detection-attack/lda.



### MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video
- **Arxiv ID**: http://arxiv.org/abs/2203.00859v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00859v4)
- **Published**: 2022-03-02 04:20:59+00:00
- **Updated**: 2022-04-25 08:24:27+00:00
- **Authors**: Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, Junsong Yuan
- **Comment**: CVPR2022 Accepted Paper
- **Journal**: None
- **Summary**: Recent transformer-based solutions have been introduced to estimate 3D human pose from 2D keypoint sequence by considering body joints among all frames globally to learn spatio-temporal correlation. We observe that the motions of different joints differ significantly. However, the previous methods cannot efficiently model the solid inter-frame correspondence of each joint, leading to insufficient learning of spatial-temporal correlation. We propose MixSTE (Mixed Spatio-Temporal Encoder), which has a temporal transformer block to separately model the temporal motion of each joint and a spatial transformer block to learn inter-joint spatial correlation. These two blocks are utilized alternately to obtain better spatio-temporal feature encoding. In addition, the network output is extended from the central frame to entire frames of the input video, thereby improving the coherence between the input and output sequences. Extensive experiments are conducted on three benchmarks (Human3.6M, MPI-INF-3DHP, and HumanEva). The results show that our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is available at https://github.com/JinluZhang1126/MixSTE.



### D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention
- **Arxiv ID**: http://arxiv.org/abs/2203.00860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00860v1)
- **Published**: 2022-03-02 04:21:12+00:00
- **Updated**: 2022-03-02 04:21:12+00:00
- **Authors**: Junyu Lin, Xiaofeng Mao, Yuefeng Chen, Lei Xu, Yuan He, Hui Xue
- **Comment**: None
- **Journal**: None
- **Summary**: DETR is the first fully end-to-end detector that predicts a final set of predictions without post-processing. However, it suffers from problems such as low performance and slow convergence. A series of works aim to tackle these issues in different ways, but the computational cost is yet expensive due to the sophisticated encoder-decoder architecture. To alleviate this issue, we propose a decoder-only detector called D^2ETR. In the absence of encoder, the decoder directly attends to the fine-fused feature maps generated by the Transformer backbone with a novel computationally efficient cross-scale attention module. D^2ETR demonstrates low computational complexity and high detection accuracy in evaluations on the COCO benchmark, outperforming DETR and its variants.



### Styleverse: Towards Identity Stylization across Heterogeneous Domains
- **Arxiv ID**: http://arxiv.org/abs/2203.00861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00861v1)
- **Published**: 2022-03-02 04:23:01+00:00
- **Updated**: 2022-03-02 04:23:01+00:00
- **Authors**: Jia Li, Jie Cao, JunXian Duan, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new challenging task namely IDentity Stylization (IDS) across heterogeneous domains. IDS focuses on stylizing the content identity, rather than completely swapping it using the reference identity. We use an effective heterogeneous-network-based framework $Styleverse$ that uses a single domain-aware generator to exploit the Metaverse of diverse heterogeneous faces, based on the proposed dataset FS13 with limited data. FS13 means 13 kinds of Face Styles considering diverse lighting conditions, art representations and life dimensions. Previous similar tasks, \eg, image style transfer can handle textural style transfer based on a reference image. This task usually ignores the high structure-aware facial area and high-fidelity preservation of the content. However, Styleverse intends to controllably create topology-aware faces in the Parallel Style Universe, where the source facial identity is adaptively styled via AdaIN guided by the domain-aware and reference-aware style embeddings from heterogeneous pretrained models. We first establish the IDS quantitative benchmark as well as the qualitative Styleverse matrix. Extensive experiments demonstrate that Styleverse achieves higher-fidelity identity stylization compared with other state-of-the-art methods.



### SEA: Bridging the Gap Between One- and Two-stage Detector Distillation via SEmantic-aware Alignment
- **Arxiv ID**: http://arxiv.org/abs/2203.00862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00862v1)
- **Published**: 2022-03-02 04:24:05+00:00
- **Updated**: 2022-03-02 04:24:05+00:00
- **Authors**: Yixin Chen, Zhuotao Tian, Pengguang Chen, Shu Liu, Jiaya Jia
- **Comment**: Preprint Version
- **Journal**: None
- **Summary**: We revisit the one- and two-stage detector distillation tasks and present a simple and efficient semantic-aware framework to fill the gap between them. We address the pixel-level imbalance problem by designing the category anchor to produce a representative pattern for each category and regularize the topological distance between pixels and category anchors to further tighten their semantic bonds. We name our method SEA (SEmantic-aware Alignment) distillation given the nature of abstracting dense fine-grained information by semantic reliance to well facilitate distillation efficacy. SEA is well adapted to either detection pipeline and achieves new state-of-the-art results on the challenging COCO object detection task on both one- and two-stage detectors. Its superior performance on instance segmentation further manifests the generalization ability. Both 2x-distilled RetinaNet and FCOS with ResNet50-FPN outperform their corresponding 3x ResNet101-FPN teacher, arriving 40.64 and 43.06 AP, respectively. Code will be made publicly available.



### Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding
- **Arxiv ID**: http://arxiv.org/abs/2203.00867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00867v2)
- **Published**: 2022-03-02 04:27:27+00:00
- **Updated**: 2022-03-16 10:34:06+00:00
- **Authors**: Qiaole Dong, Chenjie Cao, Yanwei Fu
- **Comment**: This paper has been accepted in CVPR2022
- **Journal**: None
- **Summary**: Image inpainting has made significant advances in recent years. However, it is still challenging to recover corrupted images with both vivid textures and reasonable structures. Some specific methods only tackle regular textures while losing holistic structures due to the limited receptive fields of convolutional neural networks (CNNs). On the other hand, attention-based models can learn better long-range dependency for the structure recovery, but they are limited by the heavy computation for inference with large image sizes. To address these issues, we propose to leverage an additional structure restorer to facilitate the image inpainting incrementally. The proposed model restores holistic image structures with a powerful attention-based transformer model in a fixed low-resolution sketch space. Such a grayscale space is easy to be upsampled to larger scales to convey correct structural information. Our structure restorer can be integrated with other pretrained inpainting models efficiently with the zero-initialized residual addition. Furthermore, a masking positional encoding strategy is utilized to improve the performance with large irregular masks. Extensive experiments on various datasets validate the efficacy of our model compared with other competitors. Our codes are released in https://github.com/DQiaole/ZITS_inpainting.



### Hybrid Optimized Deep Convolution Neural Network based Learning Model for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.00869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00869v1)
- **Published**: 2022-03-02 04:39:37+00:00
- **Updated**: 2022-03-02 04:39:37+00:00
- **Authors**: Venkata Beri
- **Comment**: 23 Pages, 7 Figures
- **Journal**: None
- **Summary**: Object identification is one of the most fundamental and difficult issues in computer vision. It aims to discover object instances in real pictures from a huge number of established categories. In recent years, deep learning-based object detection techniques that developed from computer vision have grabbed the public's interest. Object recognition methods based on deep learning frameworks have quickly become a popular way to interpret moving images acquired by various sensors. Due to its vast variety of applications for various computer vision tasks such as activity or event detection, content-based image retrieval, and scene understanding, academics have spent decades attempting to solve this problem. With this goal in mind, a unique deep learning classification technique is used to create an autonomous object detecting system. The noise destruction and normalising operations, which are carried out using gaussian filter and contrast normalisation techniques, respectively, are the first steps in the study activity. The pre-processed picture is next subjected to entropy-based segmentation algorithms, which separate the image's significant areas in order to distinguish between distinct occurrences. The classification challenge is completed by the suggested Hybrid Optimized Dense Convolutional Neural Network (HODCNN). The major goal of this framework is to aid in the precise recognition of distinct items from the gathered input frames. The suggested system's performance is assessed by comparing it to existing machine learning and deep learning methodologies. The experimental findings reveal that the suggested framework has a detection accuracy of 0.9864, which is greater than current techniques. As a result, the suggested object detection model outperforms other current methods.



### Dense Voxel Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.00871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00871v2)
- **Published**: 2022-03-02 04:51:31+00:00
- **Updated**: 2022-10-27 15:22:24+00:00
- **Authors**: Anas Mahmoud, Jordan S. K. Hu, Steven L. Waslander
- **Comment**: Accepted in WACV 2023
- **Journal**: None
- **Summary**: Camera and LiDAR sensor modalities provide complementary appearance and geometric information useful for detecting 3D objects for autonomous vehicle applications. However, current end-to-end fusion methods are challenging to train and underperform state-of-the-art LiDAR-only detectors. Sequential fusion methods suffer from a limited number of pixel and point correspondences due to point cloud sparsity, or their performance is strictly capped by the detections of one of the modalities. Our proposed solution, Dense Voxel Fusion (DVF) is a sequential fusion method that generates multi-scale dense voxel feature representations, improving expressiveness in low point density regions. To enhance multi-modal learning, we train directly with projected ground truth 3D bounding box labels, avoiding noisy, detector-specific 2D predictions. Both DVF and the multi-modal training approach can be applied to any voxel-based LiDAR backbone. DVF ranks 3rd among published fusion methods on KITTI 3D car detection benchmark without introducing additional trainable parameters, nor requiring stereo images or dense depth labels. In addition, DVF significantly improves 3D vehicle detection performance of voxel-based methods on the Waymo Open Dataset.



### Split Semantic Detection in Sandplay Images
- **Arxiv ID**: http://arxiv.org/abs/2203.00907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00907v2)
- **Published**: 2022-03-02 07:35:45+00:00
- **Updated**: 2023-01-05 11:25:26+00:00
- **Authors**: Xiaokun Feng, Xiaotang Chen, Jian Jia, Kaiqi Huang
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Sandplay image, as an important psychoanalysis carrier, is a visual scene constructed by the client selecting and placing sand objects (e.g., sand, river, human figures, animals, vegetation, buildings, etc.). As the projection of the client's inner world, it contains high-level semantic information reflecting the client's subjective psychological states, which is different from the common natural image scene that only contains the objective basic semantics (e.g., object's name, attribute, bounding box, etc.). In this work, we take "split" which is a typical psychological semantics related to many emotional and personality problems as the research goal, and we propose an automatic detection model, which can replace the time-consuming and expensive manual analysis process. To achieve that, we design a distribution map generation method projecting the semantic judgment problem into a visual problem, and a feature dimensionality reduction and extraction algorithm which can provide a good representation of split semantics. Besides, we built a sandplay datasets by collecting one sample from each client and inviting 5 therapists to label each sample, which has a large data cost. Experimental results demonstrated the effectiveness of our proposed method.



### Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence
- **Arxiv ID**: http://arxiv.org/abs/2203.00911v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00911v2)
- **Published**: 2022-03-02 07:42:15+00:00
- **Updated**: 2022-03-08 00:19:47+00:00
- **Authors**: Zhihong Pan, Baopu Li, Dongliang He, Mingde Yao, Wenhao Wu, Tianwei Lin, Xin Li, Errui Ding
- **Comment**: To appear at CVPR 2022
- **Journal**: None
- **Summary**: Deep learning based single image super-resolution models have been widely studied and superb results are achieved in upscaling low-resolution images with fixed scale factor and downscaling degradation kernel. To improve real world applicability of such models, there are growing interests to develop models optimized for arbitrary upscaling factors. Our proposed method is the first to treat arbitrary rescaling, both upscaling and downscaling, as one unified process. Using joint optimization of both directions, the proposed model is able to learn upscaling and downscaling simultaneously and achieve bidirectional arbitrary image rescaling. It improves the performance of current arbitrary upscaling models by a large margin while at the same time learns to maintain visual perception quality in downscaled images. The proposed model is further shown to be robust in cycle idempotence test, free of severe degradations in reconstruction accuracy when the downscaling-to-upscaling cycle is applied repetitively. This robustness is beneficial for image rescaling in the wild when this cycle could be applied to one image for multiple times. It also performs well on tests with arbitrary large scales and asymmetric scales, even when the model is not trained with such tasks. Extensive experiments are conducted to demonstrate the superior performance of our model.



### A Principled Design of Image Representation: Towards Forensic Tasks
- **Arxiv ID**: http://arxiv.org/abs/2203.00913v4
- **DOI**: 10.1109/TPAMI.2022.3204971
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.00913v4)
- **Published**: 2022-03-02 07:46:52+00:00
- **Updated**: 2022-10-06 05:18:10+00:00
- **Authors**: Shuren Qi, Yushu Zhang, Chao Wang, Jiantao Zhou, Xiaochun Cao
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence, 2022, https://ieeexplore.ieee.org/document/9881995/
- **Journal**: None
- **Summary**: Image forensics is a rising topic as the trustworthy multimedia content is critical for modern society. Like other vision-related applications, forensic analysis relies heavily on the proper image representation. Despite the importance, current theoretical understanding for such representation remains limited, with varying degrees of neglect for its key role. For this gap, we attempt to investigate the forensic-oriented image representation as a distinct problem, from the perspectives of theory, implementation, and application. Our work starts from the abstraction of basic principles that the representation for forensics should satisfy, especially revealing the criticality of robustness, interpretability, and coverage. At the theoretical level, we propose a new representation framework for forensics, called Dense Invariant Representation (DIR), which is characterized by stable description with mathematical guarantees. At the implementation level, the discrete calculation problems of DIR are discussed, and the corresponding accurate and fast solutions are designed with generic nature and constant complexity. We demonstrate the above arguments on the dense-domain pattern detection and matching experiments, providing comparison results with state-of-the-art descriptors. Also, at the application level, the proposed DIR is initially explored in passive and active forensics, namely copy-move forgery detection and perceptual hashing, exhibiting the benefits in fulfilling the requirements of such forensic tasks.



### PUFA-GAN: A Frequency-Aware Generative Adversarial Network for 3D Point Cloud Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2203.00914v1
- **DOI**: 10.1109/TIP.2022.3222918
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00914v1)
- **Published**: 2022-03-02 07:47:46+00:00
- **Updated**: 2022-03-02 07:47:46+00:00
- **Authors**: Hao Liu, Hui Yuan, Junhui Hou, Raouf Hamzaoui, Wei Gao
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a generative adversarial network for point cloud upsampling, which can not only make the upsampled points evenly distributed on the underlying surface but also efficiently generate clean high frequency regions. The generator of our network includes a dynamic graph hierarchical residual aggregation unit and a hierarchical residual aggregation unit for point feature extraction and upsampling, respectively. The former extracts multiscale point-wise descriptive features, while the latter captures rich feature details with hierarchical residuals. To generate neat edges, our discriminator uses a graph filter to extract and retain high frequency points. The generated high resolution point cloud and corresponding high frequency points help the discriminator learn the global and high frequency properties of the point cloud. We also propose an identity distribution loss function to make sure that the upsampled points remain on the underlying surface of the input low resolution point cloud. To assess the regularity of the upsampled points in high frequency regions, we introduce two evaluation metrics. Objective and subjective results demonstrate that the visual quality of the upsampled point clouds generated by our method is better than that of the state-of-the-art methods.



### Translation Invariant Global Estimation of Heading Angle Using Sinogram of LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2203.00924v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00924v1)
- **Published**: 2022-03-02 08:11:27+00:00
- **Updated**: 2022-03-02 08:11:27+00:00
- **Authors**: Xiaqing Ding, Xuecheng Xu, Sha Lu, Yanmei Jiao, Mengwen Tan, Rong Xiong, Huanjun Deng, Mingyang Li, Yue Wang
- **Comment**: Paper accepted in the 2022 IEEE International Conference on Robotics
  and Automation (ICRA)
- **Journal**: None
- **Summary**: Global point cloud registration is an essential module for localization, of which the main difficulty exists in estimating the rotation globally without initial value. With the aid of gravity alignment, the degree of freedom in point cloud registration could be reduced to 4DoF, in which only the heading angle is required for rotation estimation. In this paper, we propose a fast and accurate global heading angle estimation method for gravity-aligned point clouds. Our key idea is that we generate a translation invariant representation based on Radon Transform, allowing us to solve the decoupled heading angle globally with circular cross-correlation. Besides, for heading angle estimation between point clouds with different distributions, we implement this heading angle estimator as a differentiable module to train a feature extraction network end- to-end. The experimental results validate the effectiveness of the proposed method in heading angle estimation and show better performance compared with other methods.



### Parameterized Image Quality Score Distribution Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.00926v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00926v1)
- **Published**: 2022-03-02 08:13:33+00:00
- **Updated**: 2022-03-02 08:13:33+00:00
- **Authors**: Yixuan Gao, Xiongkuo Min, Wenhan Zhu, Xiao-Ping Zhang, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, image quality has been generally describedby a mean opinion score (MOS). However, we observe that thequality scores of an image given by a group of subjects are verysubjective and diverse. Thus it is not enough to use a MOS todescribe the image quality. In this paper, we propose to describeimage quality using a parameterized distribution rather thana MOS, and an objective method is also proposed to predictthe image quality score distribution (IQSD). At first, the LIVEdatabase is re-recorded. Specifically, we have invited a largegroup of subjects to evaluate the quality of all images in theLIVE database, and each image is evaluated by a large numberof subjects (187 valid subjects), whose scores can form a reliableIQSD. By analyzing the obtained subjective quality scores, wefind that the IQSD can be well modeled by an alpha stable model,and it can reflect much more information than a single MOS, suchas the skewness of opinion score, the subject diversity and themaximum probability score for an image. Therefore, we proposeto model the IQSD using the alpha stable model. Moreover, wepropose a framework and an algorithm to predict the alphastable model based IQSD, where quality features are extractedfrom each image based on structural information and statisticalinformation, and support vector regressors are trained to predictthe alpha stable model parameters. Experimental results verifythe feasibility of using alpha stable model to describe the IQSD,and prove the effectiveness of objective alpha stable model basedIQSD prediction method.



### TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration
- **Arxiv ID**: http://arxiv.org/abs/2203.00927v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00927v2)
- **Published**: 2022-03-02 08:14:06+00:00
- **Updated**: 2022-07-28 09:37:33+00:00
- **Authors**: Kunyu Peng, Alina Roitberg, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen
- **Comment**: Accepted to IROS 2022. Code is publicly available at
  https://github.com/KPeng9510/TransDARC
- **Journal**: None
- **Summary**: Traditional video-based human activity recognition has experienced remarkable progress linked to the rise of deep learning, but this effect was slower as it comes to the downstream task of driver behavior understanding. Understanding the situation inside the vehicle cabin is essential for Advanced Driving Assistant System (ADAS) as it enables identifying distraction, predicting driver's intent and leads to more convenient human-vehicle interaction. At the same time, driver observation systems face substantial obstacles as they need to capture different granularities of driver states, while the complexity of such secondary activities grows with the rising automation and increased driver freedom. Furthermore, a model is rarely deployed under conditions identical to the ones in the training set, as sensor placements and types vary from vehicle to vehicle, constituting a substantial obstacle for real-life deployment of data-driven models. In this work, we present a novel vision-based framework for recognizing secondary driver behaviours based on visual transformers and an additional augmented feature distribution calibration module. This module operates in the latent feature-space enriching and diversifying the training set at feature-level in order to improve generalization to novel data appearances, (e.g., sensor changes) and general feature quality. Our framework consistently leads to better recognition rates, surpassing previous state-of-the-art results of the public Drive&Act benchmark on all granularity levels. Our code is publicly available at https://github.com/KPeng9510/TransDARC.



### Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01324v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01324v3)
- **Published**: 2022-03-02 08:38:09+00:00
- **Updated**: 2022-06-28 13:17:33+00:00
- **Authors**: Yicheng Wu, Zhonghua Wu, Qianyi Wu, Zongyuan Ge, Jianfei Cai
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Semi-supervised segmentation remains challenging in medical imaging since the amount of annotated medical data is often scarce and there are many blurred pixels near the adhesive edges or in the low-contrast regions. To address the issues, we advocate to firstly constrain the consistency of pixels with and without strong perturbations to apply a sufficient smoothness constraint and further encourage the class-level separation to exploit the low-entropy regularization for the model training. Particularly, in this paper, we propose the SS-Net for semi-supervised medical image segmentation tasks, via exploring the pixel-level smoothness and inter-class separation at the same time. The pixel-level smoothness forces the model to generate invariant results under adversarial perturbations. Meanwhile, the inter-class separation encourages individual class features should approach their corresponding high-quality prototypes, in order to make each class distribution compact and separate different classes. We evaluated our SS-Net against five recent methods on the public LA and ACDC datasets. Extensive experimental results under two semi-supervised settings demonstrate the superiority of our proposed SS-Net model, achieving new state-of-the-art (SOTA) performance on both datasets. The code is available at https://github.com/ycwu1997/SS-Net.



### ParaPose: Parameter and Domain Randomization Optimization for Pose Estimation using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2203.00945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00945v2)
- **Published**: 2022-03-02 08:52:00+00:00
- **Updated**: 2022-08-02 06:17:08+00:00
- **Authors**: Frederik Hagelskjaer, Anders Glent Buch
- **Comment**: 8 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Pose estimation is the task of determining the 6D position of an object in a scene. Pose estimation aid the abilities and flexibility of robotic set-ups. However, the system must be configured towards the use case to perform adequately. This configuration is time-consuming and limits the usability of pose estimation and, thereby, robotic systems. Deep learning is a method to overcome this configuration procedure by learning parameters directly from the dataset. However, obtaining this training data can also be very time-consuming. The use of synthetic training data avoids this data collection problem, but a configuration of the training procedure is necessary to overcome the domain gap problem. Additionally, the pose estimation parameters also need to be configured. This configuration is jokingly known as grad student descent as parameters are manually adjusted until satisfactory results are obtained. This paper presents a method for automatic configuration using only synthetic data. This is accomplished by learning the domain randomization during network training, and then using the domain randomization to optimize the pose estimation parameters. The developed approach shows state-of-the-art performance of 82.0 % recall on the challenging OCCLUSION dataset, outperforming all previous methods with a large margin. These results prove the validity of automatic set-up of pose estimation using purely synthetic data.



### CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors
- **Arxiv ID**: http://arxiv.org/abs/2203.00948v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00948v3)
- **Published**: 2022-03-02 08:58:06+00:00
- **Updated**: 2023-05-14 17:20:01+00:00
- **Authors**: Jin-Ju Wang, Nicolas Dobigeon, Marie Chabert, Ding-Cheng Wang, Ting-Zhu Huang, Jie Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In the context of Earth observation, the detection of changes is performed from multitemporal images acquired by sensors with possibly different spatial and/or spectral resolutions or even different modalities (e.g. optical, radar). Even limiting to the optical modality, this task has proved to be challenging as soon as the sensors have different spatial and/or spectral resolutions. This paper proposes a novel unsupervised change detection method dedicated to images acquired with such so-called heterogeneous optical sensors. This method capitalizes on recent advances which frame the change detection problem into a robust fusion framework. More precisely, we show that a deep adversarial network designed and trained beforehand to fuse a pair of multiband optical images can be easily complemented by a network with the same architecture to perform change detection. The resulting overall architecture itself follows an adversarial strategy where the fusion network and the additional network are interpreted as essential building blocks of a generator. A comparison with state-of-the-art change detection methods demonstrates the versatility and the effectiveness of the proposed approach.



### Sketched RT3D: How to reconstruct billions of photons per second
- **Arxiv ID**: http://arxiv.org/abs/2203.00952v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2203.00952v1)
- **Published**: 2022-03-02 09:02:52+00:00
- **Updated**: 2022-03-02 09:02:52+00:00
- **Authors**: Julin Tachella, Michael P. Sheehan, Mike E. Davies
- **Comment**: Accepted at ICASSP 2022
- **Journal**: None
- **Summary**: Single-photon light detection and ranging (lidar) captures depth and intensity information of a 3D scene. Reconstructing a scene from observed photons is a challenging task due to spurious detections associated with background illumination sources. To tackle this problem, there is a plethora of 3D reconstruction algorithms which exploit spatial regularity of natural scenes to provide stable reconstructions. However, most existing algorithms have computational and memory complexity proportional to the number of recorded photons. This complexity hinders their real-time deployment on modern lidar arrays which acquire billions of photons per second. Leveraging a recent lidar sketching framework, we show that it is possible to modify existing reconstruction algorithms such that they only require a small sketch of the photon information. In particular, we propose a sketched version of a recent state-of-the-art algorithm which uses point cloud denoisers to provide spatially regularized reconstructions. A series of experiments performed on real lidar datasets demonstrates a significant reduction of execution time and memory requirements, while achieving the same reconstruction performance than in the full data case.



### GRASP EARTH: Intuitive Software for Discovering Changes on the Planet
- **Arxiv ID**: http://arxiv.org/abs/2203.00955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00955v1)
- **Published**: 2022-03-02 09:08:42+00:00
- **Updated**: 2022-03-02 09:08:42+00:00
- **Authors**: Waku Hatakeyama, Shirou Kawakita, Ryohei Izawa, Masanari Kimura
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting changes on the Earth, such as urban development, deforestation, or natural disaster, is one of the research fields that is attracting a great deal of attention. One promising tool to solve these problems is satellite imagery. However, satellite images require huge amount of storage, therefore users are required to set Area of Interests first, which was not suitable for detecting potential areas for disaster or development. To tackle with this problem, we develop the novel tool, namely GRASP EARTH, which is the simple change detection application based on Google Earth Engine. GRASP EARTH allows us to handle satellite imagery easily and it has used for disaster monitoring and urban development monitoring.



### Learning Moving-Object Tracking with FMCW LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2203.00959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00959v1)
- **Published**: 2022-03-02 09:11:36+00:00
- **Updated**: 2022-03-02 09:11:36+00:00
- **Authors**: Yi Gu, Hongzhi Cheng, Kafeng Wang, Dejing Dou, Chengzhong Xu, Hui Kong
- **Comment**: Submitted to IROS 2022
- **Journal**: None
- **Summary**: In this paper, we propose a learning-based moving-object tracking method utilizing our newly developed LiDAR sensor, Frequency Modulated Continuous Wave (FMCW) LiDAR. Compared with most existing commercial LiDAR sensors, our FMCW LiDAR can provide additional Doppler velocity information to each 3D point of the point clouds. Benefiting from this, we can generate instance labels as ground truth in a semi-automatic manner. Given the labels, we propose a contrastive learning framework, which pulls together the features from the same instance in embedding space and pushes apart the features from different instances, to improve the tracking quality. Extensive experiments are conducted on our recorded driving data, and the results show that our method outperforms the baseline methods by a large margin.



### Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2203.00960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00960v1)
- **Published**: 2022-03-02 09:14:28+00:00
- **Updated**: 2022-03-02 09:14:28+00:00
- **Authors**: Rui-Yang Ju, Ting-Yu Lin, Jen-Shiun Chiang, Jia-Hao Jian, Yu-Shian Lin, Liu-Rui-Yi Huang
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: With the achievements of Transformer in the field of natural language processing, the encoder-decoder and the attention mechanism in Transformer have been applied to computer vision. Recently, in multiple tasks of computer vision (image classification, object detection, semantic segmentation, etc.), state-of-the-art convolutional neural networks have introduced some concepts of Transformer. This proves that Transformer has a good prospect in the field of image recognition. After Vision Transformer was proposed, more and more works began to use self-attention to completely replace the convolutional layer. This work is based on Vision Transformer, combined with the pyramid architecture, using Split-transform-merge to propose the group encoder and name the network architecture Aggregated Pyramid Vision Transformer (APVT). We perform image classification tasks on the CIFAR-10 dataset and object detection tasks on the COCO 2017 dataset. Compared with other network architectures that use Transformer as the backbone, APVT has excellent results while reducing the computational cost. We hope this improved strategy can provide a reference for future Transformer research in computer vision.



### Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.00962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00962v1)
- **Published**: 2022-03-02 09:14:58+00:00
- **Updated**: 2022-03-02 09:14:58+00:00
- **Authors**: Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, Qianru Sun
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Extracting class activation maps (CAM) is arguably the most standard step of generating pseudo masks for weakly-supervised semantic segmentation (WSSS). Yet, we find that the crux of the unsatisfactory pseudo masks is the binary cross-entropy loss (BCE) widely used in CAM. Specifically, due to the sum-over-class pooling nature of BCE, each pixel in CAM may be responsive to multiple classes co-occurring in the same receptive field. As a result, given a class, its hot CAM pixels may wrongly invade the area belonging to other classes, or the non-hot ones may be actually a part of the class. To this end, we introduce an embarrassingly simple yet surprisingly effective method: Reactivating the converged CAM with BCE by using softmax cross-entropy loss (SCE), dubbed \textbf{ReCAM}. Given an image, we use CAM to extract the feature pixels of each single class, and use them with the class label to learn another fully-connected layer (after the backbone) with SCE. Once converged, we extract ReCAM in the same way as in CAM. Thanks to the contrastive nature of SCE, the pixel response is disentangled into different classes and hence less mask ambiguity is expected. The evaluation on both PASCAL VOC and MS~COCO shows that ReCAM not only generates high-quality masks, but also supports plug-and-play in any CAM variant with little overhead.



### Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training
- **Arxiv ID**: http://arxiv.org/abs/2203.00972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00972v2)
- **Published**: 2022-03-02 09:29:28+00:00
- **Updated**: 2022-04-07 22:02:24+00:00
- **Authors**: Jacek Komorowski
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a simple and effective learning-based method for computing a discriminative 3D point cloud descriptor for place recognition purposes. Recent state-of-the-art methods have relatively complex architectures such as multi-scale oyramid of point Transformers combined with a pyramid of feature aggregation modules. Our method uses a simple and efficient 3D convolutional feature extraction, based on a sparse voxelized representation, enhanced with channel attention blocks. We employ recent advances in image retrieval and propose a modified version of a loss function based on a differentiable average precision approximation. Such loss function requires training with very large batches for the best results. This is enabled by using multistaged backpropagation. Experimental evaluation on the popular benchmarks proves the effectiveness of our approach, with a consistent improvement over the state of the art



### What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors
- **Arxiv ID**: http://arxiv.org/abs/2203.01825v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01825v2)
- **Published**: 2022-03-02 10:13:11+00:00
- **Updated**: 2022-06-09 13:05:06+00:00
- **Authors**: Christos Matsoukas, Johan Fredin Haslum, Moein Sorkhei, Magnus Sderberg, Kevin Smith
- **Comment**: Originally published at CVPR 2022
- **Journal**: None
- **Summary**: Transfer learning is a standard technique to transfer knowledge from one domain to another. For applications in medical imaging, transfer from ImageNet has become the de-facto approach, despite differences in the tasks and image characteristics between the domains. However, it is unclear what factors determine whether - and to what extent - transfer learning to the medical domain is useful. The long-standing assumption that features from the source domain get reused has recently been called into question. Through a series of experiments on several medical image benchmark datasets, we explore the relationship between transfer learning, data size, the capacity and inductive bias of the model, as well as the distance between the source and target domain. Our findings suggest that transfer learning is beneficial in most cases, and we characterize the important role feature reuse plays in its success.



### TableFormer: Table Structure Understanding with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2203.01017v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01017v2)
- **Published**: 2022-03-02 10:46:24+00:00
- **Updated**: 2022-03-11 14:03:47+00:00
- **Authors**: Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar
- **Comment**: None
- **Journal**: None
- **Summary**: Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.



### Asynchronous Optimisation for Event-based Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2203.01037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01037v1)
- **Published**: 2022-03-02 11:28:47+00:00
- **Updated**: 2022-03-02 11:28:47+00:00
- **Authors**: Daqi Liu, Alvaro Parra, Yasir Latif, Bo Chen, Tat-Jun Chin, Ian Reid
- **Comment**: 7 pages abd 5 figures, accepted to ICRA
- **Journal**: None
- **Summary**: Event cameras open up new possibilities for robotic perception due to their low latency and high dynamic range. On the other hand, developing effective event-based vision algorithms that fully exploit the beneficial properties of event cameras remains work in progress. In this paper, we focus on event-based visual odometry (VO). While existing event-driven VO pipelines have adopted continuous-time representations to asynchronously process event data, they either assume a known map, restrict the camera to planar trajectories, or integrate other sensors into the system. Towards map-free event-only monocular VO in SE(3), we propose an asynchronous structure-from-motion optimisation back-end. Our formulation is underpinned by a principled joint optimisation problem involving non-parametric Gaussian Process motion modelling and incremental maximum a posteriori inference. A high-performance incremental computation engine is employed to reason about the camera trajectory with every incoming event. We demonstrate the robustness of our asynchronous back-end in comparison to frame-based methods which depend on accurate temporal accumulation of measurements.



### Image-based material analysis of ancient historical documents
- **Arxiv ID**: http://arxiv.org/abs/2203.01042v1
- **DOI**: 10.5220/0011743700003411
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01042v1)
- **Published**: 2022-03-02 11:39:22+00:00
- **Updated**: 2022-03-02 11:39:22+00:00
- **Authors**: Thomas Reynolds, Maruf A. Dhali, Lambert Schomaker
- **Comment**: 8 pages, 11 figures including supplementary documents; Submitted to
  ICPR 2022
- **Journal**: Proceedings of the 12th International Conference on Pattern
  Recognition Applications and Methods - ICPRAM, 697-706, 2023
- **Summary**: Researchers continually perform corroborative tests to classify ancient historical documents based on the physical materials of their writing surfaces. However, these tests, often performed on-site, requires actual access to the manuscript objects. The procedures involve a considerable amount of time and cost, and can damage the manuscripts. Developing a technique to classify such documents using only digital images can be very useful and efficient. In order to tackle this problem, this study uses images of a famous historical collection, the Dead Sea Scrolls, to propose a novel method to classify the materials of the manuscripts. The proposed classifier uses the two-dimensional Fourier Transform to identify patterns within the manuscript surfaces. Combining a binary classification system employing the transform with a majority voting process is shown to be effective for this classification task. This pilot study shows a successful classification percentage of up to 97% for a confined amount of manuscripts produced from either parchment or papyrus material. Feature vectors based on Fourier-space grid representation outperformed a concentric Fourier-space format.



### 3D object reconstruction and 6D-pose estimation from 2D shape for robotic grasping of objects
- **Arxiv ID**: http://arxiv.org/abs/2203.01051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01051v1)
- **Published**: 2022-03-02 11:58:35+00:00
- **Updated**: 2022-03-02 11:58:35+00:00
- **Authors**: Marcell Wolnitza, Osman Kaya, Tomas Kulvicius, Florentin Wrgtter, Babette Dellen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for 3D object reconstruction and 6D-pose estimation from 2D images that uses knowledge about object shape as the primary key. In the proposed pipeline, recognition and labeling of objects in 2D images deliver 2D segment silhouettes that are compared with the 2D silhouettes of projections obtained from various views of a 3D model representing the recognized object class. By computing transformation parameters directly from the 2D images, the number of free parameters required during the registration process is reduced, making the approach feasible. Furthermore, 3D transformations and projective geometry are employed to arrive at a full 3D reconstruction of the object in camera space using a calibrated set up. Inclusion of a second camera allows resolving remaining ambiguities. The method is quantitatively evaluated using synthetic data and tested with real data, and additional results for the well-known Linemod data set are shown. In robot experiments, successful grasping of objects demonstrates its usability in real-world environments, and, where possible, a comparison with other methods is provided. The method is applicable to scenarios where 3D object models, e.g., CAD-models or point clouds, are available and precise pixel-wise segmentation maps of 2D images can be obtained. Different from other methods, the method does not use 3D depth for training, widening the domain of application.



### Unsupervised Anomaly Detection from Time-of-Flight Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2203.01052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01052v2)
- **Published**: 2022-03-02 11:59:03+00:00
- **Updated**: 2022-04-12 07:11:15+00:00
- **Authors**: Pascal Schneider, Jason Rambach, Bruno Mirbach, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection (VAD) addresses the problem of automatically finding anomalous events in video data. The primary data modalities on which current VAD systems work on are monochrome or RGB images. Using depth data in this context instead is still hardly explored in spite of depth images being a popular choice in many other computer vision research areas and the increasing availability of inexpensive depth camera hardware. We evaluate the application of existing autoencoder-based methods on depth video and propose how the advantages of using depth data can be leveraged by integration into the loss function. Training is done unsupervised using normal sequences without need for any additional annotations. We show that depth allows easy extraction of auxiliary information for scene analysis in the form of a foreground mask and demonstrate its beneficial effect on the anomaly detection performance through evaluation on a large public dataset, for which we are also the first ones to present results on.



### Colar: Effective and Efficient Online Action Detection by Consulting Exemplars
- **Arxiv ID**: http://arxiv.org/abs/2203.01057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01057v2)
- **Published**: 2022-03-02 12:13:08+00:00
- **Updated**: 2022-03-22 13:31:53+00:00
- **Authors**: Le Yang, Junwei Han, Dingwen Zhang
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Online action detection has attracted increasing research interests in recent years. Current works model historical dependencies and anticipate the future to perceive the action evolution within a video segment and improve the detection accuracy. However, the existing paradigm ignores category-level modeling and does not pay sufficient attention to efficiency. Considering a category, its representative frames exhibit various characteristics. Thus, the category-level modeling can provide complimentary guidance to the temporal dependencies modeling. This paper develops an effective exemplar-consultation mechanism that first measures the similarity between a frame and exemplary frames, and then aggregates exemplary features based on the similarity weights. This is also an efficient mechanism, as both similarity measurement and feature aggregation require limited computations. Based on the exemplar-consultation mechanism, the long-term dependencies can be captured by regarding historical frames as exemplars, while the category-level modeling can be achieved by regarding representative frames from a category as exemplars. Due to the complementarity from the category-level modeling, our method employs a lightweight architecture but achieves new high performance on three benchmarks. In addition, using a spatio-temporal network to tackle video frames, our method makes a good trade-off between effectiveness and efficiency. Code is available at https://github.com/VividLe/Online-Action-Detection.



### OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.01072v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01072v3)
- **Published**: 2022-03-02 12:51:33+00:00
- **Updated**: 2022-04-07 18:35:18+00:00
- **Authors**: Dingding Cai, Janne Heikkil, Esa Rahtu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: This paper proposes a universal framework, called OVE6D, for model-based 6D object pose estimation from a single depth image and a target object mask. Our model is trained using purely synthetic data rendered from ShapeNet, and, unlike most of the existing methods, it generalizes well on new real-world objects without any fine-tuning. We achieve this by decomposing the 6D pose into viewpoint, in-plane rotation around the camera optical axis and translation, and introducing novel lightweight modules for estimating each component in a cascaded manner. The resulting network contains less than 4M parameters while demonstrating excellent performance on the challenging T-LESS and Occluded LINEMOD datasets without any dataset-specific training. We show that OVE6D outperforms some contemporary deep learning-based pose estimation methods specifically trained for individual objects or datasets with real-world training data.   The implementation and the pre-trained model will be made publicly available.



### Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01074v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01074v2)
- **Published**: 2022-03-02 12:55:10+00:00
- **Updated**: 2022-07-08 21:27:58+00:00
- **Authors**: Marvin Klingner, Mouadh Ayache, Tim Fingscheidt
- **Comment**: Accepted to IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Environment perception in autonomous driving vehicles often heavily relies on deep neural networks (DNNs), which are subject to domain shifts, leading to a significantly decreased performance during DNN deployment. Usually, this problem is addressed by unsupervised domain adaptation (UDA) approaches trained either simultaneously on source and target domain datasets or even source-free only on target data in an offline fashion. In this work, we further expand a source-free UDA approach to a continual and therefore online-capable UDA on a single-image basis for semantic segmentation. Accordingly, our method only requires the pre-trained model from the supplier (trained in the source domain) and the current (unlabeled target domain) camera image. Our method Continual BatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch normalization layers, using target domain images in an unsupervised fashion, which yields consistent performance improvements during inference. Thereby, in contrast to existing works, our approach can be applied to improve a DNN continuously on a single-image basis during deployment without access to source data, without algorithmic delay, and nearly without computational overhead. We show the consistent effectiveness of our method across a wide variety of source/target domain settings for semantic segmentation. Code is available at https://github.com/ifnspaml/CBNA.



### Vision-based Large-scale 3D Semantic Mapping for Autonomous Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/2203.01087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.5; I.4.6; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2203.01087v1)
- **Published**: 2022-03-02 13:18:38+00:00
- **Updated**: 2022-03-02 13:18:38+00:00
- **Authors**: Qing Cheng, Niclas Zeller, Daniel Cremers
- **Comment**: ICRA 2022 Contributed paper
- **Journal**: None
- **Summary**: In this paper, we present a complete pipeline for 3D semantic mapping solely based on a stereo camera system. The pipeline comprises a direct sparse visual odometry front-end as well as a back-end for global optimization including GNSS integration, and semantic 3D point cloud labeling. We propose a simple but effective temporal voting scheme which improves the quality and consistency of the 3D point labels. Qualitative and quantitative evaluations of our pipeline are performed on the KITTI-360 dataset. The results show the effectiveness of our proposed voting scheme and the capability of our pipeline for efficient large-scale 3D semantic mapping. The large-scale mapping capabilities of our pipeline is furthermore demonstrated by presenting a very large-scale semantic map covering 8000 km of roads generated from data collected by a fleet of vehicles.



### Shape constrained CNN for segmentation guided prediction of myocardial shape and pose parameters in cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2203.01089v1
- **DOI**: 10.1016/j.media.2022.102533
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01089v1)
- **Published**: 2022-03-02 13:20:30+00:00
- **Updated**: 2022-03-02 13:20:30+00:00
- **Authors**: Sofie Tilborghs, Jan Bogaert, Frederik Maes
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation using convolutional neural networks (CNNs) is the state-of-the-art for many medical image segmentation tasks including myocardial segmentation in cardiac MR images. However, the predicted segmentation maps obtained from such standard CNN do not allow direct quantification of regional shape properties such as regional wall thickness. Furthermore, the CNNs lack explicit shape constraints, occasionally resulting in unrealistic segmentations. In this paper, we use a CNN to predict shape parameters of an underlying statistical shape model of the myocardium learned from a training set of images. Additionally, the cardiac pose is predicted, which allows to reconstruct the myocardial contours. The integrated shape model regularizes the predicted contours and guarantees realistic shapes. We enforce robustness of shape and pose prediction by simultaneously performing pixel-wise semantic segmentation during training and define two loss functions to impose consistency between the two predicted representations: one distance-based loss and one overlap-based loss. We evaluated the proposed method in a 5-fold cross validation on an in-house clinical dataset with 75 subjects and on the ACDC and LVQuan19 public datasets. We show the benefits of simultaneous semantic segmentation and the two newly defined loss functions for the prediction of shape parameters. Our method achieved a correlation of 99% for left ventricular (LV) area on the three datasets, between 91% and 97% for myocardial area, 98-99% for LV dimensions and between 80% and 92% for regional wall thickness.



### A Generalized Approach for Cancellable Template and Its Realization for Minutia Cylinder-Code
- **Arxiv ID**: http://arxiv.org/abs/2203.01095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01095v1)
- **Published**: 2022-03-02 13:30:46+00:00
- **Updated**: 2022-03-02 13:30:46+00:00
- **Authors**: Xingbo Dong, Zhe Jin, KokSheik Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing technology gains much attention in protecting the biometric template lately. For instance, Index-of-Max (IoM), a recent reported hashing technique, is a ranking-based locality sensitive hashing technique, which illustrates the feasibility to protect the ordered and fixed-length biometric template. However, biometric templates are not always in the form of ordered and fixed-length, rather it may be an unordered and variable size point set e.g. fingerprint minutiae, which restricts the usage of the traditional hashing technology. In this paper, we proposed a generalized version of IoM hashing namely gIoM, and therefore the unordered and variable size biometric template can be used. We demonstrate a realization using a well-known variable size feature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms MCC into index domain to form indexing-based feature representation. Consequently, the inversion of MCC from the transformed representation is computational infeasible, thus to achieve non-invertibility while the performance is preserved. Public fingerprint databases FVC2002 and FVC2004 are employed for experiment as benchmark to demonstrate a fair comparison with other methods. Moreover, the security and privacy analysis suggest that gIoM meets the criteria of template protection: non-invertibility, revocability, and non-linkability.



### Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations
- **Arxiv ID**: http://arxiv.org/abs/2203.01325v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01325v2)
- **Published**: 2022-03-02 13:30:56+00:00
- **Updated**: 2022-07-20 14:49:54+00:00
- **Authors**: Zhilu Zhang, Ruohao Wang, Hongzhi Zhang, Yunjin Chen, Wangmeng Zuo
- **Comment**: ECCV 2022 camera ready
- **Journal**: None
- **Summary**: In this paper, we consider two challenging issues in reference-based super-resolution (RefSR), (i) how to choose a proper reference image, and (ii) how to learn real-world RefSR in a self-supervised manner. Particularly, we present a novel self-supervised learning approach for real-world image SR from observations at dual camera zooms (SelfDZSR). Considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the SR of the lesser zoomed (short-focus) image. Furthermore, SelfDZSR learns a deep network to obtain the SR result of short-focus image to have the same resolution as the telephoto image. For this purpose, we take the telephoto image instead of an additional high-resolution image as the supervision information and select a center patch from it as the reference to super-resolve the corresponding short-focus image patch. To mitigate the effect of the misalignment between short-focus low-resolution (LR) image and telephoto ground-truth (GT) image, we design an auxiliary-LR generator and map the GT to an auxiliary-LR while keeping the spatial position unchanged. Then the auxiliary-LR can be utilized to deform the LR features by the proposed adaptive spatial transformer networks (AdaSTN), and match the Ref features to GT. During testing, SelfDZSR can be directly deployed to super-solve the whole short-focus image with the reference of telephoto image. Experiments show that our method achieves better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR.



### Self-Supervised Scene Flow Estimation with 4-D Automotive Radar
- **Arxiv ID**: http://arxiv.org/abs/2203.01137v4
- **DOI**: 10.1109/LRA.2022.3187248
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01137v4)
- **Published**: 2022-03-02 14:28:12+00:00
- **Updated**: 2022-07-02 14:55:39+00:00
- **Authors**: Fangqiang Ding, Zhijun Pan, Yimin Deng, Jianning Deng, Chris Xiaoxuan Lu
- **Comment**: Copyright (c) 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 2022
- **Summary**: Scene flow allows autonomous vehicles to reason about the arbitrary motion of multiple independent objects which is the key to long-term mobile autonomy. While estimating the scene flow from LiDAR has progressed recently, it remains largely unknown how to estimate the scene flow from a 4-D radar - an increasingly popular automotive sensor for its robustness against adverse weather and lighting conditions. Compared with the LiDAR point clouds, radar data are drastically sparser, noisier and in much lower resolution. Annotated datasets for radar scene flow are also in absence and costly to acquire in the real world. These factors jointly pose the radar scene flow estimation as a challenging problem. This work aims to address the above challenges and estimate scene flow from 4-D radar point clouds by leveraging self-supervised learning. A robust scene flow estimation architecture and three novel losses are bespoken designed to cope with intractable radar data. Real-world experimental results validate that our method is able to robustly estimate the radar scene flow in the wild and effectively supports the downstream task of motion segmentation.



### Improving Lidar-Based Semantic Segmentation of Top-View Grid Maps by Learning Features in Complementary Representations
- **Arxiv ID**: http://arxiv.org/abs/2203.01151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01151v1)
- **Published**: 2022-03-02 14:49:51+00:00
- **Updated**: 2022-03-02 14:49:51+00:00
- **Authors**: Frank Bieder, Maximilian Link, Simon Romanski, Haohao Hu, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a novel way to predict semantic information from sparse, single-shot LiDAR measurements in the context of autonomous driving. In particular, we fuse learned features from complementary representations. The approach is aimed specifically at improving the semantic segmentation of top-view grid maps. Towards this goal the 3D LiDAR point cloud is projected onto two orthogonal 2D representations. For each representation a tailored deep learning architecture is developed to effectively extract semantic information which are fused by a superordinate deep neural network. The contribution of this work is threefold: (1) We examine different stages within the segmentation network for fusion. (2) We quantify the impact of embedding different features. (3) We use the findings of this survey to design a tailored deep neural network architecture leveraging respective advantages of different representations. Our method is evaluated using the SemanticKITTI dataset which provides a point-wise semantic annotation of more than 23.000 LiDAR measurements.



### DisARM: Displacement Aware Relation Module for 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.01152v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.01152v1)
- **Published**: 2022-03-02 14:49:55+00:00
- **Updated**: 2022-03-02 14:49:55+00:00
- **Authors**: Yao Duan, Chenyang Zhu, Yuqing Lan, Renjiao Yi, Xinwang Liu, Kai Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Displacement Aware Relation Module (DisARM), a novel neural network module for enhancing the performance of 3D object detection in point cloud scenes. The core idea of our method is that contextual information is critical to tell the difference when the instance geometry is incomplete or featureless. We find that relations between proposals provide a good representation to describe the context. However, adopting relations between all the object or patch proposals for detection is inefficient, and an imbalanced combination of local and global relations brings extra noise that could mislead the training. Rather than working with all relations, we found that training with relations only between the most representative ones, or anchors, can significantly boost the detection performance. A good anchor should be semantic-aware with no ambiguity and independent with other anchors as well. To find the anchors, we first perform a preliminary relation anchor module with an objectness-aware sampling approach and then devise a displacement-based module for weighing the relation importance for better utilization of contextual information. This lightweight relation module leads to significantly higher accuracy of object instance detection when being plugged into the state-of-the-art detectors. Evaluations on the public benchmarks of real-world scenes show that our method achieves state-of-the-art performance on both SUN RGB-D and ScanNet V2.



### Detecting Adversarial Perturbations in Multi-Task Perception
- **Arxiv ID**: http://arxiv.org/abs/2203.01177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01177v2)
- **Published**: 2022-03-02 15:25:17+00:00
- **Updated**: 2022-09-11 19:48:54+00:00
- **Authors**: Marvin Klingner, Varun Ravi Kumar, Senthil Yogamani, Andreas Br, Tim Fingscheidt
- **Comment**: Accepted at IROS 2022
- **Journal**: None
- **Summary**: While deep neural networks (DNNs) achieve impressive performance on environment perception tasks, their sensitivity to adversarial perturbations limits their use in practical applications. In this paper, we (i) propose a novel adversarial perturbation detection scheme based on multi-task perception of complex vision tasks (i.e., depth estimation and semantic segmentation). Specifically, adversarial perturbations are detected by inconsistencies between extracted edges of the input image, the depth output, and the segmentation output. To further improve this technique, we (ii) develop a novel edge consistency loss between all three modalities, thereby improving their initial consistency which in turn supports our detection scheme. We verify our detection scheme's effectiveness by employing various known attacks and image noises. In addition, we (iii) develop a multi-task adversarial attack, aiming at fooling both tasks as well as our detection scheme. Experimental evaluation on the Cityscapes and KITTI datasets shows that under an assumption of a 5% false positive rate up to 100% of images are correctly detected as adversarially perturbed, depending on the strength of the perturbation. Code is available at https://github.com/ifnspaml/AdvAttackDet. A short video at https://youtu.be/KKa6gOyWmH4 provides qualitative results.



### Fast and Robust Ground Surface Estimation from LIDAR Measurements using Uniform B-Splines
- **Arxiv ID**: http://arxiv.org/abs/2203.01180v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01180v1)
- **Published**: 2022-03-02 15:26:51+00:00
- **Updated**: 2022-03-02 15:26:51+00:00
- **Authors**: Sascha Wirges, Kevin Rsch, Frank Bieder, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a fast and robust method to estimate the ground surface from LIDAR measurements on an automated vehicle. The ground surface is modeled as a UBS which is robust towards varying measurement densities and with a single parameter controlling the smoothness prior. We model the estimation process as a robust LS optimization problem which can be reformulated as a linear problem and thus solved efficiently. Using the SemanticKITTI data set, we conduct a quantitative evaluation by classifying the point-wise semantic annotations into ground and non-ground points. Finally, we validate the approach on our research vehicle in real-world scenarios.



### Visual Feature Encoding for GNNs on Road Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.01187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01187v1)
- **Published**: 2022-03-02 15:37:50+00:00
- **Updated**: 2022-03-02 15:37:50+00:00
- **Authors**: Oliver Stromann, Alireza Razavi, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel approach to learning an encoding of visual features into graph neural networks with the application on road network data. We propose an architecture that combines state-of-the-art vision backbone networks with graph neural networks. More specifically, we perform a road type classification task on an Open Street Map road network through encoding of satellite imagery using various ResNet architectures. Our architecture further enables fine-tuning and a transfer-learning approach is evaluated by pretraining on the NWPU-RESISC45 image classification dataset for remote sensing and comparing them to purely ImageNet-pretrained ResNet models as visual feature encoders. The results show not only that the visual feature encoders are superior to low-level visual features, but also that the fine-tuning of the visual feature encoder to a general remote sensing dataset such as NWPU-RESISC45 can further improve the performance of a GNN on a machine learning task like road type classification.



### Improving Generalization of Deep Networks for Estimating Physical Properties of Containers and Fillings
- **Arxiv ID**: http://arxiv.org/abs/2203.01192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01192v1)
- **Published**: 2022-03-02 15:47:27+00:00
- **Updated**: 2022-03-02 15:47:27+00:00
- **Authors**: Hengyi Wang, Chaoran Zhu, Ziyin Ma, Changjae Oh
- **Comment**: None
- **Journal**: None
- **Summary**: We present methods to estimate the physical properties of household containers and their fillings manipulated by humans. We use a lightweight, pre-trained convolutional neural network with coordinate attention as a backbone model of the pipelines to accurately locate the object of interest and estimate the physical properties in the CORSMAL Containers Manipulation (CCM) dataset. We address the filling type classification with audio data and then combine this information from audio with video modalities to address the filling level classification. For the container capacity, dimension, and mass estimation, we present a data augmentation and consistency measurement to alleviate the over-fitting issue in the CCM dataset caused by the limited number of containers. We augment the training data using an object-of-interest-based re-scaling that increases the variety of physical values of the containers. We then perform the consistency measurement to choose a model with low prediction variance in the same containers under different scenes, which ensures the generalization ability of the model. Our method improves the generalization ability of the models to estimate the property of the containers that were not previously seen in the training.



### VAE-iForest: Auto-encoding Reconstruction and Isolation-based Anomalies Detecting Fallen Objects on Road Surface
- **Arxiv ID**: http://arxiv.org/abs/2203.01193v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2203.01193v1)
- **Published**: 2022-03-02 15:47:36+00:00
- **Updated**: 2022-03-02 15:47:36+00:00
- **Authors**: Takato Yasuno, Junichiro Fujii, Riku Ogata, Masahiro Okano
- **Comment**: 5 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: In road monitoring, it is an important issue to detect changes in the road surface at an early stage to prevent damage to third parties. The target of the falling object may be a fallen tree due to the external force of a flood or an earthquake, and falling rocks from a slope. Generative deep learning is possible to flexibly detect anomalies of the falling objects on the road surface. We prototype a method that combines auto-encoding reconstruction and isolation-based anomaly detector in application for road surface monitoring. Actually, we apply our method to a set of test images that fallen objects is located on the raw inputs added with fallen stone and plywood, and that snow is covered on the winter road. Finally we mention the future works for practical purpose application.



### Container Localisation and Mass Estimation with an RGB-D Camera
- **Arxiv ID**: http://arxiv.org/abs/2203.01207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01207v1)
- **Published**: 2022-03-02 16:03:04+00:00
- **Updated**: 2022-03-02 16:03:04+00:00
- **Authors**: Tommaso Apicella, Giulia Slavic, Edoardo Ragusa, Paolo Gastaldo, Lucio Marcenaro
- **Comment**: Paper accepted to ICASSP 2022 5 pages, 4 figures, 1 table Code:
  https://github.com/CORSMAL/Visual Info:
  https://corsmal.eecs.qmul.ac.uk/challenge.html
- **Journal**: None
- **Summary**: In the research area of human-robot interactions, the automatic estimation of the mass of a container manipulated by a person leveraging only visual information is a challenging task. The main challenges consist of occlusions, different filling materials and lighting conditions. The mass of an object constitutes key information for the robot to correctly regulate the force required to grasp the container. We propose a single RGB-D camera-based method to locate a manipulated container and estimate its empty mass i.e., independently of the presence of the content. The method first automatically selects a number of candidate containers based on the distance with the fixed frontal view, then averages the mass predictions of a lightweight model to provide the final estimation. Results on the CORSMAL Containers Manipulation dataset show that the proposed method estimates empty container mass obtaining a score of 71.08% under different lighting or filling conditions.



### A Simple and Universal Rotation Equivariant Point-cloud Network
- **Arxiv ID**: http://arxiv.org/abs/2203.01216v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01216v3)
- **Published**: 2022-03-02 16:19:01+00:00
- **Updated**: 2022-05-27 10:17:31+00:00
- **Authors**: Ben Finkelshtein, Chaim Baskin, Haggai Maron, Nadav Dym
- **Comment**: None
- **Journal**: None
- **Summary**: Equivariance to permutations and rigid motions is an important inductive bias for various 3D learning problems. Recently it has been shown that the equivariant Tensor Field Network architecture is universal -- it can approximate any equivariant function. In this paper we suggest a much simpler architecture, prove that it enjoys the same universality guarantees and evaluate its performance on Modelnet40. The code to reproduce our experiments is available at \url{https://github.com/simpleinvariance/UniversalNetwork}



### Hybrid Tracker with Pixel and Instance for Video Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01217v1)
- **Published**: 2022-03-02 16:21:55+00:00
- **Updated**: 2022-03-02 16:21:55+00:00
- **Authors**: Weicai Ye, Xinyue Lan, Ge Su, Hujun Bao, Zhaopeng Cui, Guofeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video Panoptic Segmentation (VPS) requires generating consistent panoptic segmentation and tracking identities to all pixels across video frames. Existing methods are mainly based on the trained instance embedding to maintain consistent panoptic segmentation. However, they inevitably struggle to cope with the challenges of small objects, similar appearance but inconsistent identities, occlusion, and strong instance contour deformations. To address these problems, we present HybridTracker, a lightweight and joint tracking model attempting to eliminate the limitations of the single tracker. HybridTracker performs pixel tracker and instance tracker in parallel to obtain the association matrices, which are fused into a matching matrix. In the instance tracker, we design a differentiable matching layer, ensuring the stability of inter-frame matching. In the pixel tracker, we compute the dice coefficient of the same instance of different frames given the estimated optical flow, forming the Intersection Over Union (IoU) matrix. We additionally propose mutual check and temporal consistency constraints during inference to settle the occlusion and contour deformation challenges. Extensive experiments demonstrate that HybridTracker outperforms state-of-the-art methods on Cityscapes-VPS and VIPER datasets.



### Video Question Answering: Datasets, Algorithms and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2203.01225v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01225v2)
- **Published**: 2022-03-02 16:34:09+00:00
- **Updated**: 2022-11-02 05:25:06+00:00
- **Authors**: Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Weihong Deng, Tat-Seng Chua
- **Comment**: Accepted by EMNLP 2022
- **Journal**: None
- **Summary**: Video Question Answering (VideoQA) aims to answer natural language questions according to the given videos. It has earned increasing attention with recent research trends in joint vision and language understanding. Yet, compared with ImageQA, VideoQA is largely underexplored and progresses slowly. Although different algorithms have continually been proposed and shown success on different VideoQA datasets, we find that there lacks a meaningful survey to categorize them, which seriously impedes its advancements. This paper thus provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on the datasets, algorithms, and unique challenges. We then point out the research trend of studying beyond factoid QA to inference QA towards the cognition of video contents, Finally, we conclude some promising directions for future exploration.



### Differentiable IFS Fractals
- **Arxiv ID**: http://arxiv.org/abs/2203.01231v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01231v1)
- **Published**: 2022-03-02 16:51:00+00:00
- **Updated**: 2022-03-02 16:51:00+00:00
- **Authors**: Cory Braker Scott
- **Comment**: None
- **Journal**: None
- **Summary**: I present my explorations in rendering Iterated Function System (IFS) fractals using a differentiable rendering pipeline. Differentiable rendering is a recent innovation at the intersection of graphics and machine learning. This opens up many possibilities for generating fractals that meet particular criteria. In this paper I show how my method can be used to generate an IFS fractal that resembles a target image.



### H4D: Human 4D Modeling by Learning Neural Compositional Representation
- **Arxiv ID**: http://arxiv.org/abs/2203.01247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01247v2)
- **Published**: 2022-03-02 17:10:49+00:00
- **Updated**: 2022-04-19 12:34:30+00:00
- **Authors**: Boyan Jiang, Yinda Zhang, Xingkui Wei, Xiangyang Xue, Yanwei Fu
- **Comment**: Accepted by CVPR 2022. Project page:
  https://boyanjiang.github.io/H4D/
- **Journal**: None
- **Summary**: Despite the impressive results achieved by deep learning based 3D reconstruction, the techniques of directly learning to model 4D human captures with detailed geometry have been less studied. This work presents a novel framework that can effectively learn a compact and compositional representation for dynamic human by exploiting the human body prior from the widely used SMPL parametric model. Particularly, our representation, named H4D, represents a dynamic 3D human over a temporal span with the SMPL parameters of shape and initial pose, and latent codes encoding motion and auxiliary information. A simple yet effective linear motion model is proposed to provide a rough and regularized motion estimation, followed by per-frame compensation for pose and geometry details with the residual encoded in the auxiliary code. Technically, we introduce novel GRU-based architectures to facilitate learning and improve the representation capability. Extensive experiments demonstrate our method is not only efficacy in recovering dynamic human with accurate motion and detailed geometry, but also amenable to various 4D human related tasks, including motion retargeting, motion completion and future prediction. Please check out the project page for video and code: https://boyanjiang.github.io/H4D/.



### A Unified Query-based Paradigm for Point Cloud Understanding
- **Arxiv ID**: http://arxiv.org/abs/2203.01252v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01252v3)
- **Published**: 2022-03-02 17:17:03+00:00
- **Updated**: 2022-05-09 06:55:32+00:00
- **Authors**: Zetong Yang, Li Jiang, Yanan Sun, Bernt Schiele, Jiaya Jia
- **Comment**: Accepted by CVPR2022; Codes are available at
  https://github.com/dvlab-research/DeepVision3D
- **Journal**: None
- **Summary**: 3D point cloud understanding is an important component in autonomous driving and robotics. In this paper, we present a novel Embedding-Querying paradigm (EQ- Paradigm) for 3D understanding tasks including detection, segmentation, and classification. EQ-Paradigm is a unified paradigm that enables the combination of any existing 3D backbone architectures with different task heads. Under the EQ-Paradigm, the input is firstly encoded in the embedding stage with an arbitrary feature extraction architecture, which is independent of tasks and heads. Then, the querying stage enables the encoded features to be applicable for diverse task heads. This is achieved by introducing an intermediate representation, i.e., Q-representation, in the querying stage to serve as a bridge between the embedding stage and task heads. We design a novel Q- Net as the querying stage network. Extensive experimental results on various 3D tasks, including object detection, semantic segmentation and shape classification, show that EQ-Paradigm in tandem with Q-Net is a general and effective pipeline, which enables a flexible collaboration of backbones and heads, and further boosts the performance of the state-of-the-art methods. Codes and models are available at https://github.com/dvlab-research/DeepVision3D.



### Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2203.01327v3
- **DOI**: 10.1016/j.physletb.2022.137258
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01327v3)
- **Published**: 2022-03-02 17:38:44+00:00
- **Updated**: 2022-11-08 01:53:21+00:00
- **Authors**: Kiran Mantripragada, Faisal Z. Qureshi
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral pixel intensities result from a mixing of reflectances from several materials. This paper develops a method of hyperspectral pixel unmixing that aims to recover the "pure" spectral signal of each material (hereafter referred to as endmembers) together with the mixing ratios (abundances) given the spectrum of a single pixel. The unmixing problem is particularly relevant in the case of low-resolution hyperspectral images captured in a remote sensing setting, where individual pixels can cover large regions of the scene. Under the assumptions that (1) a multivariate Normal distribution can represent the spectra of an endmember and (2) a Dirichlet distribution can encode abundances of different endmembers, we develop a Latent Dirichlet Variational Autoencoder for hyperspectral pixel unmixing. Our approach achieves state-of-the-art results on standard benchmarks and on synthetic data generated using United States Geological Survey spectral library.



### Self-supervised Transformer for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.01265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01265v1)
- **Published**: 2022-03-02 17:44:40+00:00
- **Updated**: 2022-03-02 17:44:40+00:00
- **Authors**: Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Weiming Zhang, Nenghai Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The fast evolution and widespread of deepfake techniques in real-world scenarios require stronger generalization abilities of face forgery detectors. Some works capture the features that are unrelated to method-specific artifacts, such as clues of blending boundary, accumulated up-sampling, to strengthen the generalization ability. However, the effectiveness of these methods can be easily corrupted by post-processing operations such as compression. Inspired by transfer learning, neural networks pre-trained on other large-scale face-related tasks may provide useful features for deepfake detection. For example, lip movement has been proved to be a kind of robust and good-transferring highlevel semantic feature, which can be learned from the lipreading task. However, the existing method pre-trains the lip feature extraction model in a supervised manner, which requires plenty of human resources in data annotation and increases the difficulty of obtaining training data. In this paper, we propose a self-supervised transformer based audio-visual contrastive learning method. The proposed method learns mouth motion representations by encouraging the paired video and audio representations to be close while unpaired ones to be diverse. After pre-training with our method, the model will then be partially fine-tuned for deepfake detection task. Extensive experiments show that our self-supervised method performs comparably or even better than the supervised pre-training counterpart.



### ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.01289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01289v1)
- **Published**: 2022-03-02 18:16:57+00:00
- **Updated**: 2022-03-02 18:16:57+00:00
- **Authors**: Mohammad Mahdi Dehshibi, Mona Ashtari-Majlan, Gereziher Adhane, David Masip
- **Comment**: None
- **Journal**: None
- **Summary**: To equip Convolutional Neural Networks (CNNs) with explainability, it is essential to interpret how opaque models take specific decisions, understand what causes the errors, improve the architecture design, and identify unethical biases in the classifiers. This paper introduces ADVISE, a new explainability method that quantifies and leverages the relevance of each unit of the feature map to provide better visual explanations. To this end, we propose using adaptive bandwidth kernel density estimation to assign a relevance score to each unit of the feature map with respect to the predicted class. We also propose an evaluation protocol to quantitatively assess the visual explainability of CNN models. We extensively evaluate our idea in the image classification task using AlexNet, VGG16, ResNet50, and Xception pretrained on ImageNet. We compare ADVISE with the state-of-the-art visual explainable methods and show that the proposed method outperforms competing approaches in quantifying feature-relevance and visual explainability while maintaining competitive time complexity. Our experiments further show that ADVISE fulfils the sensitivity and implementation independence axioms while passing the sanity checks. The implementation is accessible for reproducibility purposes on https://github.com/dehshibi/ADVISE.



### Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2203.01296v1
- **DOI**: 10.1109/ICIP46576.2022.9897503
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.01296v1)
- **Published**: 2022-03-02 18:25:36+00:00
- **Updated**: 2022-03-02 18:25:36+00:00
- **Authors**: Chi-Mao Fan, Tsung-Jung Liu, Kuan-Hsien Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Low-Light Image Enhancement is a computer vision task which intensifies the dark images to appropriate brightness. It can also be seen as an ill-posed problem in image restoration domain. With the success of deep neural networks, the convolutional neural networks surpass the traditional algorithm-based methods and become the mainstream in the computer vision area. To advance the performance of enhancement algorithms, we propose an image enhancement network (HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use a half wavelet attention block on M-Net+ to enrich the features from wavelet domain. Furthermore, our HWMNet has competitive performance results on two image enhancement datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/FanChiMao/HWMNet.



### DN-DETR: Accelerate DETR Training by Introducing Query DeNoising
- **Arxiv ID**: http://arxiv.org/abs/2203.01305v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.01305v3)
- **Published**: 2022-03-02 18:50:23+00:00
- **Updated**: 2022-12-08 16:46:49+00:00
- **Authors**: Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, Lei Zhang
- **Comment**: Extended version from CVPR 2022
- **Journal**: None
- **Summary**: We present in this paper a novel denoising training method to speedup DETR (DEtection TRansformer) training and offer a deepened understanding of the slow convergence issue of DETR-like methods. We show that the slow convergence results from the instability of bipartite graph matching which causes inconsistent optimization goals in early training stages. To address this issue, except for the Hungarian loss, our method additionally feeds ground-truth bounding boxes with noises into Transformer decoder and trains the model to reconstruct the original boxes, which effectively reduces the bipartite graph matching difficulty and leads to a faster convergence. Our method is universal and can be easily plugged into any DETR-like methods by adding dozens of lines of code to achieve a remarkable improvement. As a result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and $50$ epochs of training respectively) among DETR-like methods with ResNet-$50$ backbone. Compared with the baseline under the same setting, DN-DETR achieves comparable performance with $50\%$ training epochs. Code is available at \url{https://github.com/FengLi-ust/DN-DETR}.



### High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.01311v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.01311v4)
- **Published**: 2022-03-02 18:56:20+00:00
- **Updated**: 2023-06-28 17:58:11+00:00
- **Authors**: Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov
- **Comment**: TMLR 2023, Code available at https://github.com/pliang279/HighMMT
- **Journal**: None
- **Summary**: Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalities {X1,X2} are by measuring how much information can be transferred from X1 to X2, while (2) interaction heterogeneity studies how similarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much information can be transferred from fusing {X1,X2} to {X3,X4}. We show the importance of these 2 proposed metrics as a way to automatically prioritize the fusion of modalities that contain unique information or interactions. The result is a single model, HighMMT, that scales up to 10 modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and 15 tasks from 5 research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning.



### Protecting Celebrities from DeepFake with Identity Consistency Transformer
- **Arxiv ID**: http://arxiv.org/abs/2203.01318v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01318v3)
- **Published**: 2022-03-02 18:59:58+00:00
- **Updated**: 2022-04-05 05:16:37+00:00
- **Authors**: Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Ting Zhang, Weiming Zhang, Nenghai Yu, Dong Chen, Fang Wen, Baining Guo
- **Comment**: To Appear at CVPR 2022, code is available at
  https://github.com/LightDXY/ICT_DeepFake
- **Journal**: None
- **Summary**: In this work we propose Identity Consistency Transformer, a novel face forgery detection method that focuses on high-level semantics, specifically identity information, and detecting a suspect face by finding identity inconsistency in inner and outer face regions. The Identity Consistency Transformer incorporates a consistency loss for identity consistency determination. We show that Identity Consistency Transformer exhibits superior generalization ability not only across different datasets but also across various types of image degradation forms found in real-world applications including deepfake videos. The Identity Consistency Transformer can be easily enhanced with additional identity information when such information is available, and for this reason it is especially well-suited for detecting face forgeries involving celebrities. Code will be released at \url{https://github.com/LightDXY/ICT_DeepFake}



### Conditional Reconstruction for Open-set Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01368v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01368v1)
- **Published**: 2022-03-02 19:32:29+00:00
- **Updated**: 2022-03-02 19:32:29+00:00
- **Authors**: Ian Nunes, Matheus B. Pereira, Hugo Oliveira, Jefersson A. dos Santos, Marcus Poggi
- **Comment**: None
- **Journal**: None
- **Summary**: Open set segmentation is a relatively new and unexploredtask, with just a handful of methods proposed to model suchtasks.We propose a novel method called CoReSeg thattackles the issue using class conditional reconstruction ofthe input images according to their pixelwise mask. Ourmethod conditions each input pixel to all known classes,expecting higher errors for pixels of unknown classes. Itwas observed that the proposed method produces better se-mantic consistency in its predictions, resulting in cleanersegmentation maps that better fit object boundaries. CoRe-Seg outperforms state-of-the-art methods on the Vaihin-gen and Potsdam ISPRS datasets, while also being com-petitive on the Houston 2018 IEEE GRSS Data Fusiondataset. Official implementation for CoReSeg is availableat:https://github.com/iannunes/CoReSeg.



### Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.01386v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.01386v2)
- **Published**: 2022-03-02 20:05:00+00:00
- **Updated**: 2022-07-19 08:41:22+00:00
- **Authors**: Kai Yi, Xiaoqian Shen, Yunhao Gou, Mohamed Elhoseiny
- **Comment**: ECCV 2022, camera-ready version
- **Journal**: None
- **Summary**: The main question we address in this paper is how to scale up visual recognition of unseen classes, also known as zero-shot learning, to tens of thousands of categories as in the ImageNet-21K benchmark. At this scale, especially with many fine-grained categories included in ImageNet-21K, it is critical to learn quality visual semantic representations that are discriminative enough to recognize unseen classes and distinguish them from seen ones. We propose a \emph{H}ierarchical \emph{G}raphical knowledge \emph{R}epresentation framework for the confidence-based classification method, dubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp class inheritance relations by utilizing hierarchical conceptual knowledge. Our method significantly outperformed all existing techniques, boosting the performance by 7\% compared to the runner-up approach on the ImageNet-21K benchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We also analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and 3-hops, demonstrating its generalization ability. Our benchmark and code are available at https://kaiyi.me/p/hgrnet.html.



### DDL-MVS: Depth Discontinuity Learning for MVS Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.01391v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01391v3)
- **Published**: 2022-03-02 20:25:31+00:00
- **Updated**: 2023-06-12 12:05:42+00:00
- **Authors**: Nail Ibrahimli, Hugo Ledoux, Julian Kooij, Liangliang Nan
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional MVS methods have good accuracy but struggle with completeness, while recently developed learning-based multi-view stereo (MVS) techniques have improved completeness except accuracy being compromised. We propose depth discontinuity learning for MVS methods, which further improves accuracy while retaining the completeness of the reconstruction. Our idea is to jointly estimate the depth and boundary maps where the boundary maps are explicitly used for further refinement of the depth maps. We validate our idea and demonstrate that our strategies can be easily integrated into the existing learning-based MVS pipeline where the reconstruction depends on high-quality depth map estimation. Extensive experiments on various datasets show that our method improves reconstruction quality compared to baseline. Experiments also demonstrate that the presented model and strategies have good generalization capabilities. The source code will be available soon.



### Contextual Attention Network: Transformer Meets U-Net
- **Arxiv ID**: http://arxiv.org/abs/2203.01932v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01932v2)
- **Published**: 2022-03-02 21:10:24+00:00
- **Updated**: 2022-03-31 04:54:57+00:00
- **Authors**: Reza Azad, Moein Heidari, Yuli Wu, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, convolutional neural networks (CNN) (e.g., U-Net) have become the de facto standard and attained immense success in medical image segmentation. However, as a downside, CNN based methods are a double-edged sword as they fail to build long-range dependencies and global context connections due to the limited receptive field that stems from the intrinsic characteristics of the convolution operation. Hence, recent articles have exploited Transformer variants for medical image segmentation tasks which open up great opportunities due to their innate capability of capturing long-range correlations through the attention mechanism. Although being feasibly designed, most of the cohort studies incur prohibitive performance in capturing local information, thereby resulting in less lucidness of boundary areas. In this paper, we propose a contextual attention network to tackle the aforementioned limitations. The proposed method uses the strength of the Transformer module to model the long-range contextual dependency. Simultaneously, it utilizes the CNN encoder to capture local semantic information. In addition, an object-level representation is included to model the regional interaction map. The extracted hierarchical features are then fed to the contextual attention module to adaptively recalibrate the representation space using the local information. Then, they emphasize the informative regions while taking into account the long-range contextual dependency derived by the Transformer module. We validate our method on several large-scale public medical image segmentation datasets and achieve state-of-the-art performance. We have provided the implementation code in https://github.com/rezazad68/TMUnet.



### Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations
- **Arxiv ID**: http://arxiv.org/abs/2203.01933v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01933v2)
- **Published**: 2022-03-02 22:11:07+00:00
- **Updated**: 2022-03-31 01:58:14+00:00
- **Authors**: Aishik Konwer, Xuan Xu, Joseph Bae, Chao Chen, Prateek Prasanna
- **Comment**: Accepted in CVPR 2022 (ORAL)
- **Journal**: None
- **Summary**: Clinical outcome or severity prediction from medical images has largely focused on learning representations from single-timepoint or snapshot scans. It has been shown that disease progression can be better characterized by temporal imaging. We therefore hypothesized that outcome predictions can be improved by utilizing the disease progression information from sequential images. We present a deep learning approach that leverages temporal progression information to improve clinical outcome predictions from single-timepoint images. In our method, a self-attention based Temporal Convolutional Network (TCN) is used to learn a representation that is most reflective of the disease trajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised fashion to extract features from single-timepoint images. The key contribution is to design a recalibration module that employs maximum mean discrepancy loss (MMD) to align distributions of the above two contextual representations. We train our system to predict clinical outcomes and severity grades from single-timepoint images. Experiments on chest and osteoarthritis radiography datasets demonstrate that our approach outperforms other state-of-the-art techniques.



### MUAD: Multiple Uncertainties for Autonomous Driving, a benchmark for multiple uncertainty types and tasks
- **Arxiv ID**: http://arxiv.org/abs/2203.01437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01437v2)
- **Published**: 2022-03-02 22:14:12+00:00
- **Updated**: 2022-10-07 16:25:51+00:00
- **Authors**: Gianni Franchi, Xuanlong Yu, Andrei Bursuc, Angel Tena, Rmi Kazmierczak, Sverine Dubuisson, Emanuel Aldea, David Filliat
- **Comment**: Accepted at BMVC 2022
- **Journal**: None
- **Summary**: Predictive uncertainty estimation is essential for safe deployment of Deep Neural Networks in real-world autonomous systems. However, disentangling the different types and sources of uncertainty is non trivial for most datasets, especially since there is no ground truth for uncertainty. In addition, while adverse weather conditions of varying intensities can disrupt neural network predictions, they are usually under-represented in both training and test sets in public datasets.We attempt to mitigate these setbacks and introduce the MUAD dataset (Multiple Uncertainties for Autonomous Driving), consisting of 10,413 realistic synthetic images with diverse adverse weather conditions (night, fog, rain, snow), out-of-distribution objects, and annotations for semantic segmentation, depth estimation, object, and instance detection. MUAD allows to better assess the impact of different sources of uncertainty on model performance. We conduct a thorough experimental study of this impact on several baseline Deep Neural Networks across multiple tasks, and release our dataset to allow researchers to benchmark their algorithm methodically in adverse conditions. More visualizations and the download link for MUAD are available at https://muad-dataset.github.io/.



### Enhancing Adversarial Robustness for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.01439v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01439v1)
- **Published**: 2022-03-02 22:27:44+00:00
- **Updated**: 2022-03-02 22:27:44+00:00
- **Authors**: Mo Zhou, Vishal M. Patel
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Owing to security implications of adversarial vulnerability, adversarial robustness of deep metric learning models has to be improved. In order to avoid model collapse due to excessively hard examples, the existing defenses dismiss the min-max adversarial training, but instead learn from a weak adversary inefficiently. Conversely, we propose Hardness Manipulation to efficiently perturb the training triplet till a specified level of hardness for adversarial training, according to a harder benign triplet or a pseudo-hardness function. It is flexible since regular training and min-max adversarial training are its boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness functions is proposed to gradually increase the specified hardness level during training for a better balance between performance and robustness. Additionally, an Intra-Class Structure loss term among benign and adversarial examples further improves model robustness and efficiency. Comprehensive experimental results suggest that the proposed method, although simple in its form, overwhelmingly outperforms the state-of-the-art defenses in terms of robustness, training efficiency, as well as performance on benign examples.



### 3D Common Corruptions and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01441v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01441v3)
- **Published**: 2022-03-02 22:31:16+00:00
- **Updated**: 2022-04-29 13:08:19+00:00
- **Authors**: Ouzhan Fatih Kar, Teresa Yeo, Andrei Atanov, Amir Zamir
- **Comment**: CVPR 2022 (Oral). Project website at
  https://3dcommoncorruptions.epfl.ch/
- **Journal**: None
- **Summary**: We introduce a set of image transformations that can be used as corruptions to evaluate the robustness of models as well as data augmentation mechanisms for training neural networks. The primary distinction of the proposed transformations is that, unlike existing approaches such as Common Corruptions, the geometry of the scene is incorporated in the transformations -- thus leading to corruptions that are more likely to occur in the real world. We also introduce a set of semantic corruptions (e.g. natural object occlusions). We show these transformations are `efficient' (can be computed on-the-fly), `extendable' (can be applied on most image datasets), expose vulnerability of existing models, and can effectively make models more robust when employed as `3D data augmentation' mechanisms. The evaluations on several tasks and datasets suggest incorporating 3D information into benchmarking and training opens up a promising direction for robustness research.



### LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives
- **Arxiv ID**: http://arxiv.org/abs/2203.01445v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01445v2)
- **Published**: 2022-03-02 22:42:20+00:00
- **Updated**: 2022-03-04 06:08:09+00:00
- **Authors**: Danial Maleki, H. R Tizhoosh
- **Comment**: None
- **Journal**: None
- **Summary**: The volume of available data has grown dramatically in recent years in many applications. Furthermore, the age of networks that used multiple modalities separately has practically ended. Therefore, enabling bidirectional cross-modality data retrieval capable of processing has become a requirement for many domains and disciplines of research. This is especially true in the medical field, as data comes in a multitude of types, including various types of images and reports as well as molecular data. Most contemporary works apply cross attention to highlight the essential elements of an image or text in relation to the other modalities and try to match them together. However, regardless of their importance in their own modality, these approaches usually consider features of each modality equally. In this study, self-attention as an additional loss term will be proposed to enrich the internal representation provided into the cross attention module. This work suggests a novel architecture with a new loss term to help represent images and texts in the joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO and ARCH, show the effectiveness of the proposed method.



### Object Pose Estimation using Mid-level Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2203.01449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.01449v1)
- **Published**: 2022-03-02 22:49:17+00:00
- **Updated**: 2022-03-02 22:49:17+00:00
- **Authors**: Negar Nejatishahidin, Pooya Fayyazsanavi, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a novel pose estimation model for object categories that can be effectively transferred to previously unseen environments. The deep convolutional network models (CNN) for pose estimation are typically trained and evaluated on datasets specifically curated for object detection, pose estimation, or 3D reconstruction, which requires large amounts of training data. In this work, we propose a model for pose estimation that can be trained with small amount of data and is built on the top of generic mid-level representations \cite{taskonomy2018} (e.g. surface normal estimation and re-shading). These representations are trained on a large dataset without requiring pose and object annotations. Later on, the predictions are refined with a small CNN neural network that exploits object masks and silhouette retrieval. The presented approach achieves superior performance on the Pix3D dataset \cite{pix3d} and shows nearly 35\% improvement over the existing models when only 25\% of the training data is available. We show that the approach is favorable when it comes to generalization and transfer to novel environments. Towards this end, we introduce a new pose estimation benchmark for commonly encountered furniture categories on challenging Active Vision Dataset \cite{Ammirato2017ADF} and evaluated the models trained on the Pix3D dataset.



### Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.01452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.01452v2)
- **Published**: 2022-03-02 23:00:32+00:00
- **Updated**: 2022-03-17 23:17:38+00:00
- **Authors**: Jiaming Zhang, Kailun Yang, Chaoxiang Ma, Simon Rei, Kunyu Peng, Rainer Stiefelhagen
- **Comment**: Accepted to CVPR2022. Code will be made publicly available at
  https://github.com/jamycheung/Trans4PASS
- **Journal**: None
- **Summary**: Panoramic images with their 360-degree directional view encompass exhaustive information about the surrounding space, providing a rich foundation for scene understanding. To unfold this potential in the form of robust panoramic segmentation models, large quantities of expensive, pixel-wise annotations are crucial for success. Such annotations are available, but predominantly for narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal resources for training panoramic models. Distortions and the distinct image-feature distribution in 360-degree panoramas impede the transfer from the annotation-rich pinhole domain and therefore come with a big dent in performance. To get around this domain difference and bring together semantic annotations from pinhole- and 360-degree surround-visuals, we propose to learn object deformations and panoramic image distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components which blend into our Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we tie together shared semantics in pinhole- and panoramic feature embeddings by generating multi-scale prototype features and aligning them in our Mutual Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by 14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available at https://github.com/jamycheung/Trans4PASS.




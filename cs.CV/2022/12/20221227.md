# Arxiv Papers in cs.CV on 2022-12-27
### Behavioral Cloning via Search in Video PreTraining Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2212.13326v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13326v2)
- **Published**: 2022-12-27 00:20:37+00:00
- **Updated**: 2023-04-17 05:38:15+00:00
- **Authors**: Federico Malato, Florian Leopold, Amogh Raut, Ville Hautamäki, Andrew Melnik
- **Comment**: None
- **Journal**: None
- **Summary**: Our aim is to build autonomous agents that can solve tasks in environments like Minecraft. To do so, we used an imitation learning-based approach. We formulate our control problem as a search problem over a dataset of experts' demonstrations, where the agent copies actions from a similar demonstration trajectory of image-action pairs. We perform a proximity search over the BASALT MineRL-dataset in the latent representation of a Video PreTraining model. The agent copies the actions from the expert trajectory as long as the distance between the state representations of the agent and the selected expert trajectory from the dataset do not diverge. Then the proximity search is repeated. Our approach can effectively recover meaningful demonstration trajectories and show human-like behavior of an agent in the Minecraft environment.



### Recovering Surveillance Video Using RF Cues
- **Arxiv ID**: http://arxiv.org/abs/2212.13340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13340v1)
- **Published**: 2022-12-27 01:57:03+00:00
- **Updated**: 2022-12-27 01:57:03+00:00
- **Authors**: Xiang Li, Rabih Younes
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Video capture is the most extensively utilized human perception source due to its intuitively understandable nature. A desired video capture often requires multiple environmental conditions such as ample ambient-light, unobstructed space, and proper camera angle. In contrast, wireless measurements are more ubiquitous and have fewer environmental constraints. In this paper, we propose CSI2Video, a novel cross-modal method that leverages only WiFi signals from commercial devices and a source of human identity information to recover fine-grained surveillance video in a real-time manner. Specifically, two tailored deep neural networks are designed to conduct cross-modal mapping and video generation tasks respectively. We make use of an auto-encoder-based structure to extract pose features from WiFi frames. Afterward, both extracted pose features and identity information are merged to generate synthetic surveillance video. Our solution generates realistic surveillance videos without any expensive wireless equipment and has ubiquitous, cheap, and real-time characteristics.



### DiffFace: Diffusion-based Face Swapping with Facial Guidance
- **Arxiv ID**: http://arxiv.org/abs/2212.13344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13344v1)
- **Published**: 2022-12-27 02:51:46+00:00
- **Updated**: 2022-12-27 02:51:46+00:00
- **Authors**: Kihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, Jisu Nam, Kychul Lee, Seungryong Kim, KwangHee Lee
- **Comment**: Project Page: https://hxngiee.github.io/DiffFace
- **Journal**: None
- **Summary**: In this paper, we propose a diffusion-based face swapping framework for the first time, called DiffFace, composed of training ID conditional DDPM, sampling with facial guidance, and a target-preserving blending. In specific, in the training process, the ID conditional DDPM is trained to generate face images with the desired identity. In the sampling process, we use the off-the-shelf facial expert models to make the model transfer source identity while preserving target attributes faithfully. During this process, to preserve the background of the target image and obtain the desired face swapping result, we additionally propose a target-preserving blending strategy. It helps our model to keep the attributes of the target face from noise while transferring the source facial identity. In addition, without any re-training, our model can flexibly apply additional facial guidance and adaptively control the ID-attributes trade-off to achieve the desired results. To the best of our knowledge, this is the first approach that applies the diffusion model in face swapping task. Compared with previous GAN-based approaches, by taking advantage of the diffusion model for the face swapping task, DiffFace achieves better benefits such as training stability, high fidelity, diversity of the samples, and controllability. Extensive experiments show that our DiffFace is comparable or superior to the state-of-the-art methods on several standard face swapping benchmarks.



### A Generalization of ViT/MLP-Mixer to Graphs
- **Arxiv ID**: http://arxiv.org/abs/2212.13350v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13350v2)
- **Published**: 2022-12-27 03:27:46+00:00
- **Updated**: 2023-05-31 03:19:44+00:00
- **Authors**: Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, Xavier Bresson
- **Comment**: In Proceedings of ICML 2023
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the ViT/MLP-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph ViT/MLP-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them. The source code is available for reproducibility at: \url{https://github.com/XiaoxinHe/Graph-ViT-MLPMixer}.



### Attribute-Guided Multi-Level Attention Network for Fine-Grained Fashion Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2301.13014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13014v1)
- **Published**: 2022-12-27 05:28:38+00:00
- **Updated**: 2022-12-27 05:28:38+00:00
- **Authors**: Ling Xiao, Toshihiko Yamasaki
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2002.02814,
  arXiv:2104.02429 by other authors
- **Journal**: None
- **Summary**: This paper proposes an attribute-guided multi-level attention network (AG-MLAN) to learn fine-grained fashion similarity. AG-MLAN is able to make a more accurate attribute positioning and capture more discriminative features under the guidance of the specified attribute. Specifically, the AG-MLAN contains two branches, branch 1 aims to force the model to recognize different attributes, while branch 2 aims to learn multiple attribute-specific embedding spaces for measuring the fine-grained similarity. We first improve the Convolutional Neural Network (CNN) backbone to extract hierarchical feature representations, then the extracted feature representations are passed into branch 1 for attribute classification and branch 2 for multi-level feature extraction. In branch 2, we first propose a multi-level attention module to extract a more discriminative representation under the guidance of a specific attribute. Then, we adopt a masked embedding module to learn attribute-aware embedding. Finally, the AG-MLAN is trained with a weighted loss of the classification loss in branch 1 and the triplet loss of the masked embedding features in branch 2 to further improve the accuracy in attribute location. Extensive experiments on the DeepFashion, FashionAI, and Zappos50k datasets show the effectiveness of AG-MLAN for fine-grained fashion similarity learning and its potential for attribute-guided retrieval tasks. The proposed AG-MLAN outperforms the state-of-the-art methods in the fine-grained fashion similarity retrieval task.



### Semi-supervised Fashion Compatibility Prediction by Color Distortion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2212.14680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14680v1)
- **Published**: 2022-12-27 05:33:23+00:00
- **Updated**: 2022-12-27 05:33:23+00:00
- **Authors**: Ling Xiao, Toshihiko Yamasaki
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning methods have been suffering from the fact that a large-scale labeled dataset is mandatory, which is difficult to obtain. This has been a more significant issue for fashion compatibility prediction because compatibility aims to capture people's perception of aesthetics, which are sparse and changing. Thus, the labeled dataset may become outdated quickly due to fast fashion. Moreover, labeling the dataset always needs some expert knowledge; at least they should have a good sense of aesthetics. However, there are limited self/semi-supervised learning techniques in this field. In this paper, we propose a general color distortion prediction task forcing the baseline to recognize low-level image information to learn more discriminative representation for fashion compatibility prediction. Specifically, we first propose to distort the image by adjusting the image color balance, contrast, sharpness, and brightness. Then, we propose adding Gaussian noise to the distorted image before passing them to the convolutional neural network (CNN) backbone to learn a probability distribution over all possible distortions. The proposed pretext task is adopted in the state-of-the-art methods in fashion compatibility and shows its effectiveness in improving these methods' ability in extracting better feature representations. Applying the proposed pretext task to the baseline can consistently outperform the original baseline.



### MixupE: Understanding and Improving Mixup from Directional Derivative Perspective
- **Arxiv ID**: http://arxiv.org/abs/2212.13381v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13381v4)
- **Published**: 2022-12-27 07:03:52+00:00
- **Updated**: 2023-07-27 06:11:24+00:00
- **Authors**: Yingtian Zou, Vikas Verma, Sarthak Mittal, Wai Hoh Tang, Hieu Pham, Juho Kannala, Yoshua Bengio, Arno Solin, Kenji Kawaguchi
- **Comment**: 16 pages, UAI 2023 oral presentation
- **Journal**: None
- **Summary**: Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.



### Exploring Transformer Backbones for Image Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2212.14678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14678v1)
- **Published**: 2022-12-27 07:05:14+00:00
- **Updated**: 2022-12-27 07:05:14+00:00
- **Authors**: Princy Chahal
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end Transformer based Latent Diffusion model for image synthesis. On the ImageNet class conditioned generation task we show that a Transformer based Latent Diffusion model achieves a 14.1FID which is comparable to the 13.1FID score of a UNet based architecture. In addition to showing the application of Transformer models for Diffusion based image synthesis this simplification in architecture allows easy fusion and modeling of text and image data. The multi-head attention mechanism of Transformers enables simplified interaction between the image and text features which removes the requirement for crossattention mechanism in UNet based Diffusion models.



### DeepCuts: Single-Shot Interpretability based Pruning for BERT
- **Arxiv ID**: http://arxiv.org/abs/2212.13392v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13392v1)
- **Published**: 2022-12-27 07:21:41+00:00
- **Updated**: 2022-12-27 07:21:41+00:00
- **Authors**: Jasdeep Singh Grover, Bhavesh Gawri, Ruskin Raj Manku
- **Comment**: 13 pages, 12 figures, 10 equations, initial preprint
- **Journal**: None
- **Summary**: As language models have grown in parameters and layers, it has become much harder to train and infer with them on single GPUs. This is severely restricting the availability of large language models such as GPT-3, BERT-Large, and many others. A common technique to solve this problem is pruning the network architecture by removing transformer heads, fully-connected weights, and other modules. The main challenge is to discern the important parameters from the less important ones. Our goal is to find strong metrics for identifying such parameters. We thus propose two strategies: Cam-Cut based on the GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for calculating the importance scores. Through this work, we show that our scoring functions are able to assign more relevant task-based scores to the network parameters, and thus both our pruning approaches significantly outperform the standard weight and gradient-based strategies, especially at higher compression ratios in BERT-based models. We also analyze our pruning masks and find them to be significantly different from the ones obtained using standard metrics.



### A Novel Dataset and a Deep Learning Method for Mitosis Nuclei Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2212.13401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13401v1)
- **Published**: 2022-12-27 08:12:42+00:00
- **Updated**: 2022-12-27 08:12:42+00:00
- **Authors**: Huadeng Wang, Zhipeng Liu, Rushi Lan, Zhenbing Liu, Xiaonan Luo, Xipeng Pan, Bingbing Li
- **Comment**: 19 pages,11 figures, 4 tables
- **Journal**: None
- **Summary**: Mitosis nuclei count is one of the important indicators for the pathological diagnosis of breast cancer. The manual annotation needs experienced pathologists, which is very time-consuming and inefficient. With the development of deep learning methods, some models with good performance have emerged, but the generalization ability should be further strengthened. In this paper, we propose a two-stage mitosis segmentation and classification method, named SCMitosis. Firstly, the segmentation performance with a high recall rate is achieved by the proposed depthwise separable convolution residual block and channel-spatial attention gate. Then, a classification network is cascaded to further improve the detection performance of mitosis nuclei. The proposed model is verified on the ICPR 2012 dataset, and the highest F-score value of 0.8687 is obtained compared with the current state-of-the-art algorithms. In addition, the model also achieves good performance on GZMH dataset, which is prepared by our group and will be firstly released with the publication of this paper. The code will be available at: https://github.com/antifen/mitosis-nuclei-segmentation.



### Spacecraft Pose Estimation Based on Unsupervised Domain Adaptation and on a 3D-Guided Loss Combination
- **Arxiv ID**: http://arxiv.org/abs/2212.13415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13415v1)
- **Published**: 2022-12-27 08:57:46+00:00
- **Updated**: 2022-12-27 08:57:46+00:00
- **Authors**: Juan Ignacio Bravo Pérez-Villar, Álvaro García-Martín, Jesús Bescós
- **Comment**: Accepted at ECCV 2022 AI4SPACE Workshop
  (https://aiforspace.github.io/2022/)
- **Journal**: None
- **Summary**: Spacecraft pose estimation is a key task to enable space missions in which two spacecrafts must navigate around each other. Current state-of-the-art algorithms for pose estimation employ data-driven techniques. However, there is an absence of real training data for spacecraft imaged in space conditions due to the costs and difficulties associated with the space environment. This has motivated the introduction of 3D data simulators, solving the issue of data availability but introducing a large gap between the training (source) and test (target) domains. We explore a method that incorporates 3D structure into the spacecraft pose estimation pipeline to provide robustness to intensity domain shift and we present an algorithm for unsupervised domain adaptation with robust pseudo-labelling. Our solution has ranked second in the two categories of the 2021 Pose Estimation Challenge organised by the European Space Agency and the Stanford University, achieving the lowest average error over the two categories.



### Position-Aware Contrastive Alignment for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.13419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13419v1)
- **Published**: 2022-12-27 09:13:19+00:00
- **Updated**: 2022-12-27 09:13:19+00:00
- **Authors**: Bo Chen, Zhiwei Hu, Zhilong Ji, Jinfeng Bai, Wangmeng Zuo
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Referring image segmentation aims to segment the target object described by a given natural language expression. Typically, referring expressions contain complex relationships between the target and its surrounding objects. The main challenge of this task is to understand the visual and linguistic content simultaneously and to find the referred object accurately among all instances in the image. Currently, the most effective way to solve the above problem is to obtain aligned multi-modal features by computing the correlation between visual and linguistic feature modalities under the supervision of the ground-truth mask. However, existing paradigms have difficulty in thoroughly understanding visual and linguistic content due to the inability to perceive information directly about surrounding objects that refer to the target. This prevents them from learning aligned multi-modal features, which leads to inaccurate segmentation. To address this issue, we present a position-aware contrastive alignment network (PCAN) to enhance the alignment of multi-modal features by guiding the interaction between vision and language through prior position information. Our PCAN consists of two modules: 1) Position Aware Module (PAM), which provides position information of all objects related to natural language descriptions, and 2) Contrastive Language Understanding Module (CLUM), which enhances multi-modal alignment by comparing the features of the referred object with those of related objects. Extensive experiments on three benchmarks demonstrate our PCAN performs favorably against the state-of-the-art methods. Our code will be made publicly available.



### 1st Place Solution for YouTubeVOS Challenge 2022: Referring Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.14679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14679v1)
- **Published**: 2022-12-27 09:22:45+00:00
- **Updated**: 2022-12-27 09:22:45+00:00
- **Authors**: Zhiwei Hu, Bo Chen, Yuan Gao, Zhilong Ji, Jinfeng Bai
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: The task of referring video object segmentation aims to segment the object in the frames of a given video to which the referring expressions refer. Previous methods adopt multi-stage approach and design complex pipelines to obtain promising results. Recently, the end-to-end method based on Transformer has proved its superiority. In this work, we draw on the advantages of the above methods to provide a simple and effective pipeline for RVOS. Firstly, We improve the state-of-the-art one-stage method ReferFormer to obtain mask sequences that are strongly correlated with language descriptions. Secondly, based on a reliable and high-quality keyframe, we leverage the superior performance of video object segmentation model to further enhance the quality and temporal consistency of the mask results. Our single model reaches 70.3 J &F on the Referring Youtube-VOS validation set and 63.0 on the test set. After ensemble, we achieve 64.1 on the final leaderboard, ranking 1st place on CVPR2022 Referring Youtube-VOS challenge. Code will be available at https://github.com/Zhiweihhh/cvpr2022-rvos-challenge.git.



### GEDI: GEnerative and DIscriminative Training for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.13425v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13425v4)
- **Published**: 2022-12-27 09:33:50+00:00
- **Updated**: 2023-02-07 04:19:54+00:00
- **Authors**: Emanuele Sansone, Robin Manhaeve
- **Comment**: Fixed typos/cleaned the experimental section
- **Journal**: None
- **Summary**: Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives and propose a unified formulation based on likelihood learning. Our analysis suggests a simple method for integrating self-supervised learning with generative models, allowing for the joint training of these two seemingly distinct approaches. We refer to this combined framework as GEDI, which stands for GEnerative and DIscriminative training. Additionally, we demonstrate an instantiation of the GEDI framework by integrating an energy-based model with a cluster-based self-supervised learning model. Through experiments on synthetic and real-world data, including SVHN, CIFAR10, and CIFAR100, we show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a wide margin. We also demonstrate that GEDI can be integrated into a neural-symbolic framework to address tasks in the small data regime, where it can use logical constraints to further improve clustering and classification performance.



### Robust Cross-vendor Mammographic Texture Models Using Augmentation-based Domain Adaptation for Long-term Breast Cancer Risk
- **Arxiv ID**: http://arxiv.org/abs/2212.13439v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2212.13439v2)
- **Published**: 2022-12-27 10:37:02+00:00
- **Updated**: 2023-01-10 15:06:38+00:00
- **Authors**: Andreas D. Lauritzen, My Catarina von Euler-Chelpin, Elsebeth Lynge, Ilse Vejborg, Mads Nielsen, Nico Karssemeijer, Martin Lillholm
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Risk-stratified breast cancer screening might improve early detection and efficiency without comprising quality. However, modern mammography-based risk models do not ensure adaptation across vendor-domains and rely on cancer precursors, associated with short-term risk, which might limit long-term risk assessment. We report a cross-vendor mammographic texture model for long-term risk. Approach: The texture model was robustly trained using two systematically designed case-control datasets. Textural features, indicative of future breast cancer, were learned by excluding samples with diagnosed/potential malignancies from training. An augmentation-based domain adaption technique, based on flavorization of mammographic views, ensured generalization across vendor-domains. The model was validated in 66,607 consecutively screened Danish women with flavorized Siemens views and 25,706 Dutch women with Hologic-processed views. Performances were evaluated for interval cancers (IC) within two years from screening and long-term cancers (LTC) from two years after screening. The texture model was combined with established risk factors to flag 10% of women with the highest risk. Results: In Danish women, the texture model achieved an area under the receiver operating characteristic (AUC) of 0.71 and 0.65 for ICs and LTCs, respectively. In Dutch women with Hologic-processed views, the AUCs were not different from AUCs in Danish women with flavorized views. The AUC for texture combined with established risk factors increased to 0.68 for LTCs. The 10% of women flagged as high-risk accounted for 25.5% of ICs and 24.8% of LTCs. Conclusions: The texture model robustly estimated long-term breast cancer risk while adapting to an unseen processed vendor-domain and identified a clinically relevant high-risk subgroup.



### The most general manner to injectively align true and predicted segments
- **Arxiv ID**: http://arxiv.org/abs/2212.13445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13445v1)
- **Published**: 2022-12-27 11:00:22+00:00
- **Updated**: 2022-12-27 11:00:22+00:00
- **Authors**: Maarten Marx
- **Comment**: None
- **Journal**: None
- **Summary**: Kirilov et al (2019) develop a metric, called Panoptic Quality (PQ), to evaluate image segmentation methods. The metric is based on a confusion table, and compares a predicted to a ground truth segmentation. The only non straightforward part in this comparison is to align the segments in the two segmentations. A metric only works well if that alignment is a partial bijection. Kirilov et al (2019) list 3 desirable properties for a definition of alignment: it should be simple, interpretable and effectively computable. There are many definitions guaranteeing a partial bijection and these 3 properties. We present the weakest: one that is both sufficient and necessary to guarantee that the alignment is a partial bijection. This new condition is effectively computable and natural. It simply says that the number of correctly predicted elements (in image segmentation, the pixels) should be larger than the number of missed, and larger than the number of spurious elements. This is strictly weaker than the proposal in Kirilov et al (2019). In formulas, instead of |TP|> |FN\textbar| + |FP|, the weaker condition requires that |TP|> |FN| and |TP| > |FP|. We evaluate the new alignment condition theoretically and empirically.



### Scaling Painting Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2212.13459v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13459v1)
- **Published**: 2022-12-27 12:03:38+00:00
- **Updated**: 2022-12-27 12:03:38+00:00
- **Authors**: Bruno Galerne, Lara Raad, José Lezama, Jean-Michel Morel
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Neural style transfer is a deep learning technique that produces an unprecedentedly rich style transfer from a style image to a content image and is particularly impressive when it comes to transferring style from a painting to an image. It was originally achieved by solving an optimization problem to match the global style statistics of the style image while preserving the local geometric features of the content image. The two main drawbacks of this original approach is that it is computationally expensive and that the resolution of the output images is limited by high GPU memory requirements. Many solutions have been proposed to both accelerate neural style transfer and increase its resolution, but they all compromise the quality of the produced images. Indeed, transferring the style of a painting is a complex task involving features at different scales, from the color palette and compositional style to the fine brushstrokes and texture of the canvas. This paper provides a solution to solve the original global optimization for ultra-high resolution images, enabling multiscale style transfer at unprecedented image sizes. This is achieved by spatially localizing the computation of each forward and backward passes through the VGG network. Extensive qualitative and quantitative comparisons show that our method produces a style transfer of unmatched quality for such high resolution painting styles.



### MVTN: Learning Multi-View Transformations for 3D Understanding
- **Arxiv ID**: http://arxiv.org/abs/2212.13462v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.13462v1)
- **Published**: 2022-12-27 12:09:16+00:00
- **Updated**: 2022-12-27 12:09:16+00:00
- **Authors**: Abdullah Hamdi, Faisal AlZahrani, Silvio Giancola, Bernard Ghanem
- **Comment**: under review journal extension for the ICCV 2021 paper
  arXiv:2011.13244
- **Journal**: None
- **Summary**: Multi-view projection techniques have shown themselves to be highly effective in achieving top-performing results in the recognition of 3D shapes. These methods involve learning how to combine information from multiple view-points. However, the camera view-points from which these views are obtained are often fixed for all shapes. To overcome the static nature of current multi-view techniques, we propose learning these view-points. Specifically, we introduce the Multi-View Transformation Network (MVTN), which uses differentiable rendering to determine optimal view-points for 3D shape recognition. As a result, MVTN can be trained end-to-end with any multi-view network for 3D shape classification. We integrate MVTN into a novel adaptive multi-view pipeline that is capable of rendering both 3D meshes and point clouds. Our approach demonstrates state-of-the-art performance in 3D classification and shape retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55). Further analysis indicates that our approach exhibits improved robustness to occlusion compared to other methods. We also investigate additional aspects of MVTN, such as 2D pretraining and its use for segmentation. To support further research in this area, we have released MVTorch, a PyTorch library for 3D understanding and generation using multi-view projections.



### General GAN-generated image detection by data augmentation in fingerprint domain
- **Arxiv ID**: http://arxiv.org/abs/2212.13466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13466v2)
- **Published**: 2022-12-27 12:37:11+00:00
- **Updated**: 2023-04-09 13:11:26+00:00
- **Authors**: Huaming Wang, Jianwei Fei, Yunshu Dai, Lingyun Leng, Zhihua Xia
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we investigate improving the generalizability of GAN-generated image detectors by performing data augmentation in the fingerprint domain. Specifically, we first separate the fingerprints and contents of the GAN-generated images using an autoencoder based GAN fingerprint extractor, followed by random perturbations of the fingerprints. Then the original fingerprints are substituted with the perturbed fingerprints and added to the original contents, to produce images that are visually invariant but with distinct fingerprints. The perturbed images can successfully imitate images generated by different GANs to improve the generalization of the detectors, which is demonstrated by the spectra visualization. To our knowledge, we are the first to conduct data augmentation in the fingerprint domain. Our work explores a novel prospect that is distinct from previous works on spatial and frequency domain augmentation. Extensive cross-GAN experiments demonstrate the effectiveness of our method compared to the state-of-the-art methods in detecting fake images generated by unknown GANs.



### Semi-Supervised Semantic Segmentation Methods for UW-OCTA Diabetic Retinopathy Grade Assessment
- **Arxiv ID**: http://arxiv.org/abs/2212.13486v2
- **DOI**: 10.1007/978-3-031-33658-4_10
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.13486v2)
- **Published**: 2022-12-27 13:40:44+00:00
- **Updated**: 2023-01-13 16:55:51+00:00
- **Authors**: Zhuoyi Tan, Hizmawati Madzin, Zeyu Ding
- **Comment**: None
- **Journal**: 978-3-031-33658-4 Due: 04 July 2023
- **Summary**: People with diabetes are more likely to develop diabetic retinopathy (DR) than healthy people. However, DR is the leading cause of blindness. At present, the diagnosis of diabetic retinopathy mainly relies on the experienced clinician to recognize the fine features in color fundus images. This is a time-consuming task. Therefore, in this paper, to promote the development of UW-OCTA DR automatic detection, we propose a novel semi-supervised semantic segmentation method for UW-OCTA DR image grade assessment. This method, first, uses the MAE algorithm to perform semi-supervised pre-training on the UW-OCTA DR grade assessment dataset to mine the supervised information in the UW-OCTA images, thereby alleviating the need for labeled data. Secondly, to more fully mine the lesion features of each region in the UW-OCTA image, this paper constructs a cross-algorithm ensemble DR tissue segmentation algorithm by deploying three algorithms with different visual feature processing strategies. The algorithm contains three sub-algorithms, namely pre-trained MAE, ConvNeXt, and SegFormer. Based on the initials of these three sub-algorithms, the algorithm can be named MCS-DRNet. Finally, we use the MCS-DRNet algorithm as an inspector to check and revise the results of the preliminary evaluation of the DR grade evaluation algorithm. The experimental results show that the mean dice similarity coefficient of MCS-DRNet v1 and v2 are 0.5161 and 0.5544, respectively. The quadratic weighted kappa of the DR grading evaluation is 0.7559. Our code will be released soon.



### Flattening Surface Based On Using Contour Estimating Subdivision Surface
- **Arxiv ID**: http://arxiv.org/abs/2212.13489v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13489v1)
- **Published**: 2022-12-27 13:48:53+00:00
- **Updated**: 2022-12-27 13:48:53+00:00
- **Authors**: Yuhan Xu, Renqing Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In the process of projecting the surface of a three-dimensional object onto a two-dimensional surface, due to the perspective distortion, the image on the surface of the object will have different degrees of distortion according to the level of the surface curvature. This paper presents an imprecise method for flattening this type of distortion on the surface of a regularly curved body. The main idea of this method is to roughly estimate the gridded surface subdivision that can be used to describe the surface of the three-dimensional object through the contour curve of the two-dimensional image of the object. Then, take each grid block with different sizes and shapes inversely transformed into a rectangle with exactly the same shape and size. Finally, each of the same rectangles is splicing and recombining in turn to obtain a roughly flat rectangle. This paper will introduce and show the specific process and results of using this method to solve the problem of bending page flattening, then demonstrate the feasibility and limitations of this method.



### Truncate-Split-Contrast: A Framework for Learning from Mislabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2212.13495v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13495v2)
- **Published**: 2022-12-27 14:09:14+00:00
- **Updated**: 2022-12-29 02:14:27+00:00
- **Authors**: Zixiao Wang, Junwu Weng, Chun Yuan, Jue Wang
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: Learning with noisy label (LNL) is a classic problem that has been extensively studied for image tasks, but much less for video in the literature. A straightforward migration from images to videos without considering the properties of videos, such as computational cost and redundant information, is not a sound choice. In this paper, we propose two new strategies for video analysis with noisy labels: 1) A lightweight channel selection method dubbed as Channel Truncation for feature-based label noise detection. This method selects the most discriminative channels to split clean and noisy instances in each category; 2) A novel contrastive strategy dubbed as Noise Contrastive Learning, which constructs the relationship between clean and noisy instances to regularize model training. Experiments on three well-known benchmark datasets for video classification show that our proposed tru{\bf N}cat{\bf E}-split-contr{\bf A}s{\bf T} (NEAT) significantly outperforms the existing baselines. By reducing the dimension to 10\% of it, our method achieves over 0.4 noise detection F1-score and 5\% classification accuracy improvement on Mini-Kinetics dataset under severe noise (symmetric-80\%). Thanks to Noise Contrastive Learning, the average classification accuracy improvement on Mini-Kinetics and Sth-Sth-V1 is over 1.6\%.



### DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.13504v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13504v3)
- **Published**: 2022-12-27 14:39:39+00:00
- **Updated**: 2023-07-26 18:32:36+00:00
- **Authors**: Reza Azad, René Arimond, Ehsan Khodapanah Aghdam, Amirhossein Kazerouni, Dorit Merhof
- **Comment**: MICCAI 2023 PRIME workshop
- **Journal**: None
- **Summary**: Transformers have recently gained attention in the computer vision domain due to their ability to model long-range dependencies. However, the self-attention mechanism, which is the core part of the Transformer model, usually suffers from quadratic computational complexity with respect to the number of tokens. Many architectures attempt to reduce model complexity by limiting the self-attention mechanism to local regions or by redesigning the tokenization process. In this paper, we propose DAE-Former, a novel method that seeks to provide an alternative perspective by efficiently designing the self-attention mechanism. More specifically, we reformulate the self-attention mechanism to capture both spatial and channel relations across the whole feature dimension while staying computationally efficient. Furthermore, we redesign the skip connection path by including the cross-attention module to ensure the feature reusability and enhance the localization power. Our method outperforms state-of-the-art methods on multi-organ cardiac and skin lesion segmentation datasets without requiring pre-training weights. The code is publicly available at https://github.com/mindflow-institue/DAEFormer.



### Infusing Definiteness into Randomness: Rethinking Composition Styles for Deep Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2212.13517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13517v1)
- **Published**: 2022-12-27 15:24:49+00:00
- **Updated**: 2022-12-27 15:24:49+00:00
- **Authors**: Zixuan Ye, Yutong Dai, Chaoyi Hong, Zhiguo Cao, Hao Lu
- **Comment**: Accepted to AAAI 2023; 11 pages, 9 figures; Code is available at
  https://github.com/coconuthust/composition_styles
- **Journal**: None
- **Summary**: We study the composition style in deep image matting, a notion that characterizes a data generation flow on how to exploit limited foregrounds and random backgrounds to form a training dataset. Prior art executes this flow in a completely random manner by simply going through the foreground pool or by optionally combining two foregrounds before foreground-background composition. In this work, we first show that naive foreground combination can be problematic and therefore derive an alternative formulation to reasonably combine foregrounds. Our second contribution is an observation that matting performance can benefit from a certain occurrence frequency of combined foregrounds and their associated source foregrounds during training. Inspired by this, we introduce a novel composition style that binds the source and combined foregrounds in a definite triplet. In addition, we also find that different orders of foreground combination lead to different foreground patterns, which further inspires a quadruplet-based composition style. Results under controlled experiments on four matting baselines show that our composition styles outperform existing ones and invite consistent performance improvement on both composited and real-world datasets. Code is available at: https://github.com/coconuthust/composition_styles



### Cross-Resolution Flow Propagation for Foveated Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2212.13525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13525v1)
- **Published**: 2022-12-27 15:38:38+00:00
- **Updated**: 2022-12-27 15:38:38+00:00
- **Authors**: Eugene Lee, Lien-Feng Hsu, Evan Chen, Chen-Yi Lee
- **Comment**: 12 pages, 8 figures, to appear in IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV) 2023
- **Journal**: None
- **Summary**: The demand of high-resolution video contents has grown over the years. However, the delivery of high-resolution video is constrained by either computational resources required for rendering or network bandwidth for remote transmission. To remedy this limitation, we leverage the eye trackers found alongside existing augmented and virtual reality headsets. We propose the application of video super-resolution (VSR) technique to fuse low-resolution context with regional high-resolution context for resource-constrained consumption of high-resolution content without perceivable drop in quality. Eye trackers provide us the gaze direction of a user, aiding us in the extraction of the regional high-resolution context. As only pixels that falls within the gaze region can be resolved by the human eye, a large amount of the delivered content is redundant as we can't perceive the difference in quality of the region beyond the observed region. To generate a visually pleasing frame from the fusion of high-resolution region and low-resolution region, we study the capability of a deep neural network of transferring the context of the observed region to other regions (low-resolution) of the current and future frames. We label this task a Foveated Video Super-Resolution (FVSR), as we need to super-resolve the low-resolution regions of current and future frames through the fusion of pixels from the gaze region. We propose Cross-Resolution Flow Propagation (CRFP) for FVSR. We train and evaluate CRFP on REDS dataset on the task of 8x FVSR, i.e. a combination of 8x VSR and the fusion of foveated region. Departing from the conventional evaluation of per frame quality using SSIM or PSNR, we propose the evaluation of past foveated region, measuring the capability of a model to leverage the noise present in eye trackers during FVSR. Code is made available at https://github.com/eugenelet/CRFP.



### From Single-Visit to Multi-Visit Image-Based Models: Single-Visit Models are Enough to Predict Obstructive Hydronephrosis
- **Arxiv ID**: http://arxiv.org/abs/2212.13535v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13535v1)
- **Published**: 2022-12-27 16:14:49+00:00
- **Updated**: 2022-12-27 16:14:49+00:00
- **Authors**: Stanley Bryan Z. Hua, Mandy Rickard, John Weaver, Alice Xiang, Daniel Alvarez, Kyla N. Velear, Kunj Sheth, Gregory E. Tasian, Armando J. Lorenzo, Anna Goldenberg, Lauren Erdman
- **Comment**: Paper accepted to SIPAIM 2022 (in Valparaiso, Chile)
- **Journal**: None
- **Summary**: Previous work has shown the potential of deep learning to predict renal obstruction using kidney ultrasound images. However, these image-based classifiers have been trained with the goal of single-visit inference in mind. We compare methods from video action recognition (i.e. convolutional pooling, LSTM, TSM) to adapt single-visit convolutional models to handle multiple visit inference. We demonstrate that incorporating images from a patient's past hospital visits provides only a small benefit for the prediction of obstructive hydronephrosis. Therefore, inclusion of prior ultrasounds is beneficial, but prediction based on the latest ultrasound is sufficient for patient risk stratification.



### Learning Spatiotemporal Frequency-Transformer for Low-Quality Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2212.14046v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.14046v1)
- **Published**: 2022-12-27 16:26:15+00:00
- **Updated**: 2022-12-27 16:26:15+00:00
- **Authors**: Zhongwei Qiu, Huan Yang, Jianlong Fu, Daochang Liu, Chang Xu, Dongmei Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Video Super-Resolution (VSR) aims to restore high-resolution (HR) videos from low-resolution (LR) videos. Existing VSR techniques usually recover HR frames by extracting pertinent textures from nearby frames with known degradation processes. Despite significant progress, grand challenges are remained to effectively extract and transmit high-quality textures from high-degraded low-quality sequences, such as blur, additive noises, and compression artifacts. In this work, a novel Frequency-Transformer (FTVSR) is proposed for handling low-quality videos that carry out self-attention in a combined space-time-frequency domain. First, video frames are split into patches and each patch is transformed into spectral maps in which each channel represents a frequency band. It permits a fine-grained self-attention on each frequency band, so that real visual texture can be distinguished from artifacts. Second, a novel dual frequency attention (DFA) mechanism is proposed to capture the global frequency relations and local frequency relations, which can handle different complicated degradation processes in real-world scenarios. Third, we explore different self-attention schemes for video processing in the frequency domain and discover that a ``divided attention'' which conducts a joint space-frequency attention before applying temporal-frequency attention, leads to the best video enhancement quality. Extensive experiments on three widely-used VSR datasets show that FTVSR outperforms state-of-the-art methods on different low-quality videos with clear visual margins. Code and pre-trained models are available at https://github.com/researchmm/FTVSR.



### Interactive Segmentation of Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2212.13545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13545v2)
- **Published**: 2022-12-27 16:33:19+00:00
- **Updated**: 2023-03-25 16:05:36+00:00
- **Authors**: Rahul Goel, Dhawal Sirikonda, Saurabh Saini, PJ Narayanan
- **Comment**: Accepted to CVPR 2023. Project Page:
  https://rahul-goel.github.io/isrf/
- **Journal**: None
- **Summary**: Radiance Fields (RF) are popular to represent casually-captured scenes for new view synthesis and several applications beyond it. Mixed reality on personal spaces needs understanding and manipulating scenes represented as RFs, with semantic segmentation of objects as an important step. Prior segmentation efforts show promise but don't scale to complex objects with diverse appearance. We present the ISRF method to interactively segment objects with fine structure and appearance. Nearest neighbor feature matching using distilled semantic features identifies high-confidence seed regions. Bilateral search in a joint spatio-semantic space grows the region to recover accurate segmentation. We show state-of-the-art results of segmenting objects from RFs and compositing them to another scene, changing appearance, etc., and an interactive segmentation tool that others can use.   Project Page: https://rahul-goel.github.io/isrf/



### NeRN -- Learning Neural Representations for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2212.13554v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13554v2)
- **Published**: 2022-12-27 17:14:44+00:00
- **Updated**: 2023-04-21 15:25:39+00:00
- **Authors**: Maor Ashkenazi, Zohar Rimon, Ron Vainshtein, Shir Levi, Elad Richardson, Pinchas Mintz, Eran Treister
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process. We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet. Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations.



### Noise-aware Learning from Web-crawled Image-Text Data for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2212.13563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13563v1)
- **Published**: 2022-12-27 17:33:40+00:00
- **Updated**: 2022-12-27 17:33:40+00:00
- **Authors**: Wooyoung Kang, Jonghwan Mun, Sungjun Lee, Byungseok Roh
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is one of the straightforward tasks that can take advantage of large-scale web-crawled data which provides rich knowledge about the visual world for a captioning model. However, since web-crawled data contains image-text pairs that are aligned at different levels, the inherent noises (e.g., misaligned pairs) make it difficult to learn a precise captioning model. While the filtering strategy can effectively remove noisy data, however, it leads to a decrease in learnable knowledge and sometimes brings about a new problem of data deficiency. To take the best of both worlds, we propose a noise-aware learning framework, which learns rich knowledge from the whole web-crawled data while being less affected by the noises. This is achieved by the proposed quality controllable model, which is learned using alignment levels of the image-text pairs as an additional control signal during training. The alignment-conditioned training allows the model to generate high-quality captions of well-aligned by simply setting the control signal to desired alignment level at inference time. Through in-depth analysis, we show that our controllable captioning model is effective in handling noise. In addition, with two tasks of zero-shot captioning and text-to-image retrieval using generated captions (i.e., self-retrieval), we also demonstrate our model can produce high-quality captions in terms of descriptiveness and distinctiveness. Code is available at \url{https://github.com/kakaobrain/noc}.



### Sparse Mixture Once-for-all Adversarial Training for Efficient In-Situ Trade-Off Between Accuracy and Robustness of DNNs
- **Arxiv ID**: http://arxiv.org/abs/2302.03523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2302.03523v1)
- **Published**: 2022-12-27 18:14:44+00:00
- **Updated**: 2022-12-27 18:14:44+00:00
- **Authors**: Souvik Kundu, Sairam Sundaresan, Sharath Nittur Sridhar, Shunlin Lu, Han Tang, Peter A. Beerel
- **Comment**: 5 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Existing deep neural networks (DNNs) that achieve state-of-the-art (SOTA) performance on both clean and adversarially-perturbed images rely on either activation or weight conditioned convolution operations. However, such conditional learning costs additional multiply-accumulate (MAC) or addition operations, increasing inference memory and compute costs. To that end, we present a sparse mixture once for all adversarial training (SMART), that allows a model to train once and then in-situ trade-off between accuracy and robustness, that too at a reduced compute and parameter overhead. In particular, SMART develops two expert paths, for clean and adversarial images, respectively, that are then conditionally trained via respective dedicated sets of binary sparsity masks. Extensive evaluations on multiple image classification datasets across different models show SMART to have up to 2.72x fewer non-zero parameters costing proportional reduction in compute overhead, while yielding SOTA accuracy-robustness trade-off. Additionally, we present insightful observations in designing sparse masks to successfully condition on both clean and perturbed images.



### EuclidNet: Deep Visual Reasoning for Constructible Problems in Geometry
- **Arxiv ID**: http://arxiv.org/abs/2301.13007v1
- **DOI**: 10.54364/aaiml.2023.1152
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13007v1)
- **Published**: 2022-12-27 18:32:40+00:00
- **Updated**: 2022-12-27 18:32:40+00:00
- **Authors**: Man Fai Wong, Xintong Qi, Chee Wei Tan
- **Comment**: Accepted by 2nd MATH-AI Workshop at NeurIPS'22
- **Journal**: Adv. Artif. Intell. Mach. Learn.(2023), 3(1):839-852
- **Summary**: In this paper, we present a deep learning-based framework for solving geometric construction problems through visual reasoning, which is useful for automated geometry theorem proving. Constructible problems in geometry often ask for the sequence of straightedge-and-compass constructions to construct a given goal given some initial setup. Our EuclidNet framework leverages the neural network architecture Mask R-CNN to extract the visual features from the initial setup and goal configuration with extra points of intersection, and then generate possible construction steps as intermediary data models that are used as feedback in the training process for further refinement of the construction step sequence. This process is repeated recursively until either a solution is found, in which case we backtrack the path for a step-by-step construction guide, or the problem is identified as unsolvable. Our EuclidNet framework is validated on complex Japanese Sangaku geometry problems, demonstrating its capacity to leverage backtracking for deep visual reasoning of challenging problems.



### Co-supervised learning paradigm with conditional generative adversarial networks for sample-efficient classification
- **Arxiv ID**: http://arxiv.org/abs/2212.13589v1
- **DOI**: 10.3934/aci.2023002
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13589v1)
- **Published**: 2022-12-27 19:24:31+00:00
- **Updated**: 2022-12-27 19:24:31+00:00
- **Authors**: Hao Zhen, Yucheng Shi, Jidong J. Yang, Javad Mohammadpour Vehni
- **Comment**: 14 pages, 5 figures
- **Journal**: Applied Computing and Intelligence, 2023, Volume 3, Issue 1: 13-26
- **Summary**: Classification using supervised learning requires annotating a large amount of classes-balanced data for model training and testing. This has practically limited the scope of applications with supervised learning, in particular deep learning. To address the issues associated with limited and imbalanced data, this paper introduces a sample-efficient co-supervised learning paradigm (SEC-CGAN), in which a conditional generative adversarial network (CGAN) is trained alongside the classifier and supplements semantics-conditioned, confidence-aware synthesized examples to the annotated data during the training process. In this setting, the CGAN not only serves as a co-supervisor but also provides complementary quality examples to aid the classifier training in an end-to-end fashion. Experiments demonstrate that the proposed SEC-CGAN outperforms the external classifier GAN (EC-GAN) and a baseline ResNet-18 classifier. For the comparison, all classifiers in above methods adopt the ResNet-18 architecture as the backbone. Particularly, for the Street View House Numbers dataset, using the 5% of training data, a test accuracy of 90.26% is achieved by SEC-CGAN as opposed to 88.59% by EC-GAN and 87.17% by the baseline classifier; for the highway image dataset, using the 10% of training data, a test accuracy of 98.27% is achieved by SEC-CGAN, compared to 97.84% by EC-GAN and 95.52% by the baseline classifier.



### Brain Cancer Segmentation Using YOLOv5 Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2212.13599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13599v1)
- **Published**: 2022-12-27 20:15:54+00:00
- **Updated**: 2022-12-27 20:15:54+00:00
- **Authors**: Sudipto Paul, Dr. Md Taimur Ahad, Md. Mahedi Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: An expansion of aberrant brain cells is referred to as a brain tumor. The brain's architecture is extremely intricate, with several regions controlling various nervous system processes. Any portion of the brain or skull can develop a brain tumor, including the brain's protective coating, the base of the skull, the brainstem, the sinuses, the nasal cavity, and many other places. Over the past ten years, numerous developments in the field of computer-aided brain tumor diagnosis have been made. Recently, instance segmentation has attracted a lot of interest in numerous computer vision applications. It seeks to assign various IDs to various scene objects, even if they are members of the same class. Typically, a two-stage pipeline is used to perform instance segmentation. This study shows brain cancer segmentation using YOLOv5. Yolo takes dataset as picture format and corresponding text file. You Only Look Once (YOLO) is a viral and widely used algorithm. YOLO is famous for its object recognition properties. You Only Look Once (YOLO) is a popular algorithm that has gone viral. YOLO is well known for its ability to identify objects. YOLO V2, V3, V4, and V5 are some of the YOLO latest versions that experts have published in recent years. Early brain tumor detection is one of the most important jobs that neurologists and radiologists have. However, it can be difficult and error-prone to manually identify and segment brain tumors from Magnetic Resonance Imaging (MRI) data. For making an early diagnosis of the condition, an automated brain tumor detection system is necessary. The model of the research paper has three classes. They are respectively Meningioma, Pituitary, Glioma. The results show that, our model achieves competitive accuracy, in terms of runtime usage of M2 10 core GPU.



### Deep Learning Models for River Classification at Sub-Meter Resolutions from Multispectral and Panchromatic Commercial Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2212.13613v1
- **DOI**: 10.1016/j.rse.2022.113279
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2212.13613v1)
- **Published**: 2022-12-27 20:56:34+00:00
- **Updated**: 2022-12-27 20:56:34+00:00
- **Authors**: Joachim Moortgat, Ziwei Li, Michael Durand, Ian Howat, Bidhyananda Yadav, Chunli Dai
- **Comment**: 21 pages, 10 figures, 3 tables
- **Journal**: Remote Sensing of Environment, Volume 282, 1 December 2022, page
  113279
- **Summary**: Remote sensing of the Earth's surface water is critical in a wide range of environmental studies, from evaluating the societal impacts of seasonal droughts and floods to the large-scale implications of climate change. Consequently, a large literature exists on the classification of water from satellite imagery. Yet, previous methods have been limited by 1) the spatial resolution of public satellite imagery, 2) classification schemes that operate at the pixel level, and 3) the need for multiple spectral bands. We advance the state-of-the-art by 1) using commercial imagery with panchromatic and multispectral resolutions of 30 cm and 1.2 m, respectively, 2) developing multiple fully convolutional neural networks (FCN) that can learn the morphological features of water bodies in addition to their spectral properties, and 3) FCN that can classify water even from panchromatic imagery. This study focuses on rivers in the Arctic, using images from the Quickbird, WorldView, and GeoEye satellites. Because no training data are available at such high resolutions, we construct those manually. First, we use the RGB, and NIR bands of the 8-band multispectral sensors. Those trained models all achieve excellent precision and recall over 90% on validation data, aided by on-the-fly preprocessing of the training data specific to satellite imagery. In a novel approach, we then use results from the multispectral model to generate training data for FCN that only require panchromatic imagery, of which considerably more is available. Despite the smaller feature space, these models still achieve a precision and recall of over 85%. We provide our open-source codes and trained model parameters to the remote sensing community, which paves the way to a wide range of environmental hydrology applications at vastly superior accuracies and 2 orders of magnitude higher spatial resolution than previously possible.



### MindBigData 2022 A Large Dataset of Brain Signals
- **Arxiv ID**: http://arxiv.org/abs/2212.14746v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, q-bio.NC, 68T01 (primary), 68T45 (Secondary), H.2.8; I.2.0; I.2.1; J.3; J.7
- **Links**: [PDF](http://arxiv.org/pdf/2212.14746v1)
- **Published**: 2022-12-27 21:15:13+00:00
- **Updated**: 2022-12-27 21:15:13+00:00
- **Authors**: David Vivancos, Felix Cuesta
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: Understanding our brain is one of the most daunting tasks, one we cannot expect to complete without the use of technology. MindBigData aims to provide a comprehensive and updated dataset of brain signals related to a diverse set of human activities so it can inspire the use of machine learning algorithms as a benchmark of 'decoding' performance from raw brain activities into its corresponding (labels) mental (or physical) tasks. Using commercial of the self, EEG devices or custom ones built by us to explore the limits of the technology. We describe the data collection procedures for each of the sub datasets and with every headset used to capture them. Also, we report possible applications in the field of Brain Computer Interfaces or BCI that could impact the life of billions, in almost every sector like healthcare game changing use cases, industry or entertainment to name a few, at the end why not directly using our brains to 'disintermediate' senses, as the final HCI (Human-Computer Interaction) device? simply what we call the journey from Type to Touch to Talk to Think.



### Annealing Double-Head: An Architecture for Online Calibration of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2212.13621v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13621v2)
- **Published**: 2022-12-27 21:21:58+00:00
- **Updated**: 2023-01-16 04:36:27+00:00
- **Authors**: Erdong Guo, David Draper, Maria De Iorio
- **Comment**: Revised Preprint. 19 pages, 10 figures, 4 tables. Typos fixed, and
  references added
- **Journal**: None
- **Summary**: Model calibration, which is concerned with how frequently the model predicts correctly, not only plays a vital part in statistical model design, but also has substantial practical applications, such as optimal decision-making in the real world. However, it has been discovered that modern deep neural networks are generally poorly calibrated due to the overestimation (or underestimation) of predictive confidence, which is closely related to overfitting. In this paper, we propose Annealing Double-Head, a simple-to-implement but highly effective architecture for calibrating the DNN during training. To be precise, we construct an additional calibration head-a shallow neural network that typically has one latent layer-on top of the last latent layer in the normal model to map the logits to the aligned confidence. Furthermore, a simple Annealing technique that dynamically scales the logits by calibration head in training procedure is developed to improve its performance. Under both the in-distribution and distributional shift circumstances, we exhaustively evaluate our Annealing Double-Head architecture on multiple pairs of contemporary DNN architectures and vision and speech datasets. We demonstrate that our method achieves state-of-the-art model calibration performance without post-processing while simultaneously providing comparable predictive accuracy in comparison to other recently proposed calibration methods on a range of learning tasks.




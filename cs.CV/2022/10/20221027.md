# Arxiv Papers in cs.CV on 2022-10-27
### Robot to Human Object Handover using Vision and Joint Torque Sensor Modalities
- **Arxiv ID**: http://arxiv.org/abs/2210.15085v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15085v1)
- **Published**: 2022-10-27 00:11:34+00:00
- **Updated**: 2022-10-27 00:11:34+00:00
- **Authors**: Mohammadhadi Mohandes, Behnam Moradi, Kamal Gupta, Mehran Mehrandezh
- **Comment**: Note: This paper is submitted to RITA 2022 conference and waiting for
  results
- **Journal**: None
- **Summary**: We present a robot-to-human object handover algorithm and implement it on a 7-DOF arm equipped with a 3-finger mechanical hand. The system performs a fully autonomous and robust object handover to a human receiver in real-time. Our algorithm relies on two complementary sensor modalities: joint torque sensors on the arm and an eye-in-hand RGB-D camera for sensor feedback. Our approach is entirely implicit, i.e., there is no explicit communication between the robot and the human receiver. Information obtained via the aforementioned sensor modalities is used as inputs to their related deep neural networks. While the torque sensor network detects the human receiver's "intention" such as: pull, hold, or bump, the vision sensor network detects if the receiver's fingers have wrapped around the object. Networks' outputs are then fused, based on which a decision is made to either release the object or not. Despite substantive challenges in sensor feedback synchronization, object, and human hand detection, our system achieves robust robot-to-human handover with 98\% accuracy in our preliminary real experiments using human receivers.



### Segmentation of Multiple Sclerosis Lesions across Hospitals: Learn Continually or Train from Scratch?
- **Arxiv ID**: http://arxiv.org/abs/2210.15091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15091v1)
- **Published**: 2022-10-27 00:32:13+00:00
- **Updated**: 2022-10-27 00:32:13+00:00
- **Authors**: Enamundram Naga Karthik, Anne Kerbrat, Pierre Labauge, Tobias Granberg, Jason Talbott, Daniel S. Reich, Massimo Filippi, Rohit Bakshi, Virginie Callot, Sarath Chandar, Julien Cohen-Adad
- **Comment**: Accepted at the Medical Imaging Meets NeurIPS (MedNeurIPS) Workshop
  2022
- **Journal**: None
- **Summary**: Segmentation of Multiple Sclerosis (MS) lesions is a challenging problem. Several deep-learning-based methods have been proposed in recent years. However, most methods tend to be static, that is, a single model trained on a large, specialized dataset, which does not generalize well. Instead, the model should learn across datasets arriving sequentially from different hospitals by building upon the characteristics of lesions in a continual manner. In this regard, we explore experience replay, a well-known continual learning method, in the context of MS lesion segmentation across multi-contrast data from 8 different hospitals. Our experiments show that replay is able to achieve positive backward transfer and reduce catastrophic forgetting compared to sequential fine-tuning. Furthermore, replay outperforms the multi-domain training, thereby emerging as a promising solution for the segmentation of MS lesions. The code is available at this link: https://github.com/naga-karthik/continual-learning-ms



### Predicting Visual Attention and Distraction During Visual Search Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.15093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, I.2.10; I.4.9; I.4.6; I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/2210.15093v1)
- **Published**: 2022-10-27 00:39:43+00:00
- **Updated**: 2022-10-27 00:39:43+00:00
- **Authors**: Manoosh Samiei, James J. Clark
- **Comment**: 33 pages, 24 figures, 12 tables, this is a pre-print manuscript
  currently under review in Journal of Vision
- **Journal**: None
- **Summary**: Most studies in computational modeling of visual attention encompass task-free observation of images. Free-viewing saliency considers limited scenarios of daily life. Most visual activities are goal-oriented and demand a great amount of top-down attention control. Visual search task demands more top-down control of attention, compared to free-viewing. In this paper, we present two approaches to model visual attention and distraction of observers during visual search. Our first approach adapts a light-weight free-viewing saliency model to predict eye fixation density maps of human observers over pixels of search images, using a two-stream convolutional encoder-decoder network, trained and evaluated on COCO-Search18 dataset. This method predicts which locations are more distracting when searching for a particular target. Our network achieves good results on standard saliency metrics (AUC-Judd=0.95, AUC-Borji=0.85, sAUC=0.84, NSS=4.64, KLD=0.93, CC=0.72, SIM=0.54, and IG=2.59). Our second approach is object-based and predicts the distractor and target objects during visual search. Distractors are all objects except the target that observers fixate on during search. This method uses a Mask-RCNN segmentation network pre-trained on MS-COCO and fine-tuned on COCO-Search18 dataset. We release our segmentation annotations of targets and distractors in COCO-Search18 for three target categories: bottle, bowl, and car. The average scores over the three categories are: F1-score=0.64, MAP(iou:0.5)=0.57, MAR(iou:0.5)=0.73. Our implementation code in Tensorflow is publicly available at https://github.com/ManooshSamiei/Distraction-Visual-Search .



### Collaborative Multi-Teacher Knowledge Distillation for Learning Low Bit-width Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.16103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16103v1)
- **Published**: 2022-10-27 01:03:39+00:00
- **Updated**: 2022-10-27 01:03:39+00:00
- **Authors**: Cuong Pham, Tuan Hoang, Thanh-Toan Do
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Knowledge distillation which learns a lightweight student model by distilling knowledge from a cumbersome teacher model is an attractive approach for learning compact deep neural networks (DNNs). Recent works further improve student network performance by leveraging multiple teacher networks. However, most of the existing knowledge distillation-based multi-teacher methods use separately pretrained teachers. This limits the collaborative learning between teachers and the mutual learning between teachers and student. Network quantization is another attractive approach for learning compact DNNs. However, most existing network quantization methods are developed and evaluated without considering multi-teacher support to enhance the performance of quantized student model. In this paper, we propose a novel framework that leverages both multi-teacher knowledge distillation and network quantization for learning low bit-width DNNs. The proposed method encourages both collaborative learning between quantized teachers and mutual learning between quantized teachers and quantized student. During learning process, at corresponding layers, knowledge from teachers will form an importance-aware shared knowledge which will be used as input for teachers at subsequent layers and also be used to guide student. Our experimental results on CIFAR100 and ImageNet datasets show that the compact quantized student models trained with our method achieve competitive results compared to other state-of-the-art methods, and in some cases, indeed surpass the full precision models.



### Boosting Point Clouds Rendering via Radiance Mapping
- **Arxiv ID**: http://arxiv.org/abs/2210.15107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15107v2)
- **Published**: 2022-10-27 01:25:57+00:00
- **Updated**: 2022-12-08 01:52:34+00:00
- **Authors**: Xiaoyang Huang, Yi Zhang, Bingbing Ni, Teng Li, Kai Chen, Wenjun Zhang
- **Comment**: Accepted by Thirty-Seventh AAAI Conference on Artificial Intelligence
  (AAAI 2023)
- **Journal**: None
- **Summary**: Recent years we have witnessed rapid development in NeRF-based image rendering due to its high quality. However, point clouds rendering is somehow less explored. Compared to NeRF-based rendering which suffers from dense spatial sampling, point clouds rendering is naturally less computation intensive, which enables its deployment in mobile computing device. In this work, we focus on boosting the image quality of point clouds rendering with a compact model design. We first analyze the adaption of the volume rendering formulation on point clouds. Based on the analysis, we simplify the NeRF representation to a spatial mapping function which only requires single evaluation per pixel. Further, motivated by ray marching, we rectify the the noisy raw point clouds to the estimated intersection between rays and surfaces as queried coordinates, which could avoid \textit{spatial frequency collapse} and neighbor point disturbance. Composed of rasterization, spatial mapping and the refinement stages, our method achieves the state-of-the-art performance on point clouds rendering, outperforming prior works by notable margins, with a smaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 on ScanNet and 30.81 on DTU. Code and data are publicly available at https://github.com/seanywang0408/RadianceMapping.



### Masked Vision-Language Transformer in Fashion
- **Arxiv ID**: http://arxiv.org/abs/2210.15110v1
- **DOI**: 10.1007/s11633-022-1394-4
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.15110v1)
- **Published**: 2022-10-27 01:44:08+00:00
- **Updated**: 2022-10-27 01:44:08+00:00
- **Authors**: Ge-Peng Ji, Mingcheng Zhuge, Dehong Gao, Deng-Ping Fan, Christos Sakaridis, Luc Van Gool
- **Comment**: Accepted by Machine Intelligence Research (2023)
- **Journal**: Machine Intelligence Research. 20, 421-434 (2023)
- **Summary**: We present a masked vision-language transformer (MVLT) for fashion-specific multi-modal representation. Technically, we simply utilize vision transformer architecture for replacing the BERT in the pre-training model, making MVLT the first end-to-end framework for the fashion domain. Besides, we designed masked image reconstruction (MIR) for a fine-grained understanding of fashion. MVLT is an extensible and convenient architecture that admits raw multi-modal inputs without extra pre-processing models (e.g., ResNet), implicitly modeling the vision-language alignments. More importantly, MVLT can easily generalize to various matching and generative tasks. Experimental results show obvious improvements in retrieval (rank@5: 17%) and recognition (accuracy: 3%) tasks over the Fashion-Gen 2018 winner Kaleido-BERT. Code is made available at https://github.com/GewelsJI/MVLT.



### Bootstrapping Human Optical Flow and Pose
- **Arxiv ID**: http://arxiv.org/abs/2210.15121v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15121v2)
- **Published**: 2022-10-27 02:14:00+00:00
- **Updated**: 2022-10-28 13:58:36+00:00
- **Authors**: Aritro Roy Arko, James J. Little, Kwang Moo Yi
- **Comment**: Accepted at BMVC 2022. Supplementary qualitative results -
  https://aritro30.github.io/results/. Code at
  https://github.com/ubc-vision/bootstrapping-human-optical-flow-and-pose
- **Journal**: None
- **Summary**: We propose a bootstrapping framework to enhance human optical flow and pose. We show that, for videos involving humans in scenes, we can improve both the optical flow and the pose estimation quality of humans by considering the two tasks at the same time. We enhance optical flow estimates by fine-tuning them to fit the human pose estimates and vice versa. In more detail, we optimize the pose and optical flow networks to, at inference time, agree with each other. We show that this results in state-of-the-art results on the Human 3.6M and 3D Poses in the Wild datasets, as well as a human-related subset of the Sintel dataset, both in terms of pose estimation accuracy and the optical flow accuracy at human joint locations. Code available at https://github.com/ubc-vision/bootstrapping-human-optical-flow-and-pose



### HYDRA-HGR: A Hybrid Transformer-based Architecture for Fusion of Macroscopic and Microscopic Neural Drive Information
- **Arxiv ID**: http://arxiv.org/abs/2211.02619v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02619v1)
- **Published**: 2022-10-27 02:23:27+00:00
- **Updated**: 2022-10-27 02:23:27+00:00
- **Authors**: Mansooreh Montazerin, Elahe Rahimian, Farnoosh Naderkhani, S. Farokh Atashzar, Hamid Alinejad-Rokny, Arash Mohammadi
- **Comment**: None
- **Journal**: None
- **Summary**: Development of advance surface Electromyogram (sEMG)-based Human-Machine Interface (HMI) systems is of paramount importance to pave the way towards emergence of futuristic Cyber-Physical-Human (CPH) worlds. In this context, the main focus of recent literature was on development of different Deep Neural Network (DNN)-based architectures that perform Hand Gesture Recognition (HGR) at a macroscopic level (i.e., directly from sEMG signals). At the same time, advancements in acquisition of High-Density sEMG signals (HD-sEMG) have resulted in a surge of significant interest on sEMG decomposition techniques to extract microscopic neural drive information. However, due to complexities of sEMG decomposition and added computational overhead, HGR at microscopic level is less explored than its aforementioned DNN-based counterparts. In this regard, we propose the HYDRA-HGR framework, which is a hybrid model that simultaneously extracts a set of temporal and spatial features through its two independent Vision Transformer (ViT)-based parallel architectures (the so called Macro and Micro paths). The Macro Path is trained directly on the pre-processed HD-sEMG signals, while the Micro path is fed with the p-to-p values of the extracted Motor Unit Action Potentials (MUAPs) of each source. Extracted features at macroscopic and microscopic levels are then coupled via a Fully Connected (FC) fusion layer. We evaluate the proposed hybrid HYDRA-HGR framework through a recently released HD-sEMG dataset, and show that it significantly outperforms its stand-alone counterparts. The proposed HYDRA-HGR framework achieves average accuracy of 94.86% for the 250 ms window size, which is 5.52% and 8.22% higher than that of the Macro and Micro paths, respectively.



### Rethinking the Reverse-engineering of Trojan Triggers
- **Arxiv ID**: http://arxiv.org/abs/2210.15127v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15127v1)
- **Published**: 2022-10-27 02:25:18+00:00
- **Updated**: 2022-10-27 02:25:18+00:00
- **Authors**: Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, Shiqing Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks are vulnerable to Trojan (or backdoor) attacks. Reverse-engineering methods can reconstruct the trigger and thus identify affected models. Existing reverse-engineering methods only consider input space constraints, e.g., trigger size in the input space. Expressly, they assume the triggers are static patterns in the input space and fail to detect models with feature space triggers such as image style transformations. We observe that both input-space and feature-space Trojans are associated with feature space hyperplanes. Based on this observation, we design a novel reverse-engineering method that exploits the feature space constraint to reverse-engineer Trojan triggers. Results on four datasets and seven different attacks demonstrate that our solution effectively defends both input-space and feature-space Trojans. It outperforms state-of-the-art reverse-engineering methods and other types of defenses in both Trojaned model detection and mitigation tasks. On average, the detection accuracy of our method is 93\%. For Trojan mitigation, our method can reduce the ASR (attack success rate) to only 0.26\% with the BA (benign accuracy) remaining nearly unchanged. Our code can be found at https://github.com/RU-System-Software-and-Security/FeatureRE.



### MMFL-Net: Multi-scale and Multi-granularity Feature Learning for Cross-domain Fashion Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.15128v1
- **DOI**: 10.1007/s11042-022-13648-8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15128v1)
- **Published**: 2022-10-27 02:25:52+00:00
- **Updated**: 2022-10-27 02:25:52+00:00
- **Authors**: Chen Bao, Xudong Zhang, Jiazhou Chen, Yongwei Miao
- **Comment**: 27 pages, 12 figures, Published by <Multimedia Tools and
  Applications>
- **Journal**: Multimedia Tools and Applications(2022)1-27
- **Summary**: Instance-level image retrieval in fashion is a challenging issue owing to its increasing importance in real-scenario visual fashion search. Cross-domain fashion retrieval aims to match the unconstrained customer images as queries for photographs provided by retailers; however, it is a difficult task due to a wide range of consumer-to-shop (C2S) domain discrepancies and also considering that clothing image is vulnerable to various non-rigid deformations. To this end, we propose a novel multi-scale and multi-granularity feature learning network (MMFL-Net), which can jointly learn global-local aggregation feature representations of clothing images in a unified framework, aiming to train a cross-domain model for C2S fashion visual similarity. First, a new semantic-spatial feature fusion part is designed to bridge the semantic-spatial gap by applying top-down and bottom-up bidirectional multi-scale feature fusion. Next, a multi-branch deep network architecture is introduced to capture global salient, part-informed, and local detailed information, and extracting robust and discrimination feature embedding by integrating the similarity learning of coarse-to-fine embedding with the multiple granularities. Finally, the improved trihard loss, center loss, and multi-task classification loss are adopted for our MMFL-Net, which can jointly optimize intra-class and inter-class distance and thus explicitly improve intra-class compactness and inter-class discriminability between its visual representations for feature learning. Furthermore, our proposed model also combines the multi-task attribute recognition and classification module with multi-label semantic attributes and product ID labels. Experimental results demonstrate that our proposed MMFL-Net achieves significant improvement over the state-of-the-art methods on the two datasets, DeepFashion-C2S and Street2Shop.



### Learning Variational Motion Prior for Video-based Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/2210.15134v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2210.15134v2)
- **Published**: 2022-10-27 02:45:48+00:00
- **Updated**: 2022-10-28 02:32:05+00:00
- **Authors**: Xin Chen, Zhuo Su, Lingbo Yang, Pei Cheng, Lan Xu, Bin Fu, Gang Yu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Motion capture from a monocular video is fundamental and crucial for us humans to naturally experience and interact with each other in Virtual Reality (VR) and Augmented Reality (AR). However, existing methods still struggle with challenging cases involving self-occlusion and complex poses due to the lack of effective motion prior modeling. In this paper, we present a novel variational motion prior (VMP) learning approach for video-based motion capture to resolve the above issue. Instead of directly building the correspondence between the video and motion domain, We propose to learn a generic latent space for capturing the prior distribution of all natural motions, which serve as the basis for subsequent video-based motion capture tasks. To improve the generalization capacity of prior space, we propose a transformer-based variational autoencoder pretrained over marker-based 3D mocap data, with a novel style-mapping block to boost the generation quality. Afterward, a separate video encoder is attached to the pretrained motion generator for end-to-end fine-tuning over task-specific video datasets. Compared to existing motion prior models, our VMP model serves as a motion rectifier that can effectively reduce temporal jittering and failure modes in frame-wise pose estimation, leading to temporally stable and visually realistic motion capture results. Furthermore, our VMP-based framework models motion at sequence level and can directly generate motion clips in the forward pass, achieving real-time motion capture during inference. Extensive experiments over both public datasets and in-the-wild videos have demonstrated the efficacy and generalization capability of our framework.



### 3D Shape Knowledge Graph for Cross-domain and Cross-modal 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.15136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15136v1)
- **Published**: 2022-10-27 02:51:24+00:00
- **Updated**: 2022-10-27 02:51:24+00:00
- **Authors**: Weizhi Nie, Rihao Chang, Tong Hao, Anan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of 3D modeling and fabrication, 3D shape retrieval has become a hot topic. In recent years, several strategies have been put forth to address this retrieval issue. However, it is difficult for them to handle cross-modal 3D shape retrieval because of the natural differences between modalities. In this paper, we propose an innovative concept, namely, geometric words, which is regarded as the basic element to represent any 3D or 2D entity by combination, and assisted by which, we can simultaneously handle cross-domain or cross-modal retrieval problems. First, to construct the knowledge graph, we utilize the geometric word as the node, and then use the category of the 3D shape as well as the attribute of the geometry to bridge the nodes. Second, based on the knowledge graph, we provide a unique way for learning each entity's embedding. Finally, we propose an effective similarity measure to handle the cross-domain and cross-modal 3D shape retrieval. Specifically, every 3D or 2D entity could locate its geometric terms in the 3D knowledge graph, which serve as a link between cross-domain and cross-modal data. Thus, our approach can achieve the cross-domain and cross-modal 3D shape retrieval at the same time. We evaluated our proposed method on the ModelNet40 dataset and ShapeNetCore55 dataset for both the 3D shape retrieval task and cross-domain 3D shape retrieval task. The classic cross-modal dataset (MI3DOR) is utilized to evaluate cross-modal 3D shape retrieval. Experimental results and comparisons with state-of-the-art methods illustrate the superiority of our approach.



### ScoreMix: A Scalable Augmentation Strategy for Training GANs with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2210.15137v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15137v3)
- **Published**: 2022-10-27 02:55:15+00:00
- **Updated**: 2022-11-04 01:39:18+00:00
- **Authors**: Jie Cao, Mandi Luo, Junchi Yu, Ming-Hsuan Yang, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) typically suffer from overfitting when limited training data is available. To facilitate GAN training, current methods propose to use data-specific augmentation techniques. Despite the effectiveness, it is difficult for these methods to scale to practical applications. In this work, we present ScoreMix, a novel and scalable data augmentation approach for various image synthesis tasks. We first produce augmented samples using the convex combinations of the real samples. Then, we optimize the augmented samples by minimizing the norms of the data scores, i.e., the gradients of the log-density functions. This procedure enforces the augmented samples close to the data manifold. To estimate the scores, we train a deep estimation network with multi-scale score matching. For different image synthesis tasks, we train the score estimation network using different data. We do not require the tuning of the hyperparameters or modifications to the network architecture. The ScoreMix method effectively increases the diversity of data and reduces the overfitting problem. Moreover, it can be easily incorporated into existing GAN models with minor modifications. Experimental results on numerous tasks demonstrate that GAN models equipped with the ScoreMix method achieve significant improvements.



### Open-vocabulary Semantic Segmentation with Frozen Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2210.15138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15138v1)
- **Published**: 2022-10-27 02:57:26+00:00
- **Updated**: 2022-10-27 02:57:26+00:00
- **Authors**: Chaofan Ma, Yuhuan Yang, Yanfeng Wang, Ya Zhang, Weidi Xie
- **Comment**: BMVC 2022 Oral
- **Journal**: None
- **Summary**: When trained at a sufficient scale, self-supervised learning has exhibited a notable ability to solve a wide range of visual or language understanding tasks. In this paper, we investigate simple, yet effective approaches for adapting the pre-trained foundation models to the downstream task of interest, namely, open-vocabulary semantic segmentation. To this end, we make the following contributions: (i) we introduce Fusioner, with a lightweight, transformer-based fusion module, that pairs the frozen visual representation with language concept through a handful of image segmentation data. As a consequence, the model gains the capability of zero-shot transfer to segment novel categories; (ii) without loss of generality, we experiment on a broad range of self-supervised models that have been pre-trained with different schemes, e.g. visual-only models (MoCo v3, DINO), language-only models (BERT), visual-language model (CLIP), and show that, the proposed fusion approach is effective to any pair of visual and language models, even those pre-trained on a corpus of uni-modal data; (iii) we conduct thorough ablation studies to analyze the critical components in our proposed Fusioner, while evaluating on standard benchmarks, e.g. PASCAL-5i and COCO-20i , it surpasses existing state-of-the-art models by a large margin, despite only being trained on frozen visual and language features; (iv) to measure the model's robustness on learning visual-language correspondence, we further evaluate on synthetic dataset, named Mosaic-4, where images are constructed by mosaicking the samples from FSS-1000. Fusioner demonstrates superior performance over previous models.



### Towards Practicality of Sketch-Based Visual Understanding
- **Arxiv ID**: http://arxiv.org/abs/2210.15146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15146v1)
- **Published**: 2022-10-27 03:12:57+00:00
- **Updated**: 2022-10-27 03:12:57+00:00
- **Authors**: Ayan Kumar Bhunia
- **Comment**: PhD thesis successfully defended by Ayan Kumar Bhunia, Supervisor:
  Prof. Yi-Zhe Song, Thesis Examiners: Prof Stella Yu and Prof Adrian Hilton
- **Journal**: None
- **Summary**: Sketches have been used to conceptualise and depict visual objects from pre-historic times. Sketch research has flourished in the past decade, particularly with the proliferation of touchscreen devices. Much of the utilisation of sketch has been anchored around the fact that it can be used to delineate visual concepts universally irrespective of age, race, language, or demography. The fine-grained interactive nature of sketches facilitates the application of sketches to various visual understanding tasks, like image retrieval, image-generation or editing, segmentation, 3D-shape modelling etc. However, sketches are highly abstract and subjective based on the perception of individuals. Although most agree that sketches provide fine-grained control to the user to depict a visual object, many consider sketching a tedious process due to their limited sketching skills compared to other query/support modalities like text/tags. Furthermore, collecting fine-grained sketch-photo association is a significant bottleneck to commercialising sketch applications. Therefore, this thesis aims to progress sketch-based visual understanding towards more practicality.



### Fully Automated Deep Learning-enabled Detection for Hepatic Steatosis on Computed Tomography: A Multicenter International Validation Study
- **Arxiv ID**: http://arxiv.org/abs/2210.15149v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15149v3)
- **Published**: 2022-10-27 03:24:52+00:00
- **Updated**: 2022-11-06 08:28:39+00:00
- **Authors**: Zhongyi Zhang, Guixia Li, Ziqiang Wang, Feng Xia, Ning Zhao, Huibin Nie, Zezhong Ye, Joshua Lin, Yiyi Hui, Xiangchun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Despite high global prevalence of hepatic steatosis, no automated diagnostics demonstrated generalizability in detecting steatosis on multiple international datasets. Traditionally, hepatic steatosis detection relies on clinicians selecting the region of interest (ROI) on computed tomography (CT) to measure liver attenuation. ROI selection demands time and expertise, and therefore is not routinely performed in populations. To automate the process, we validated an existing artificial intelligence (AI) system for 3D liver segmentation and used it to purpose a novel method: AI-ROI, which could automatically select the ROI for attenuation measurements. AI segmentation and AI-ROI method were evaluated on 1,014 non-contrast enhanced chest CT images from eight international datasets: LIDC-IDRI, NSCLC-Lung1, RIDER, VESSEL12, RICORD-1A, RICORD-1B, COVID-19-Italy, and COVID-19-China. AI segmentation achieved a mean dice coefficient of 0.957. Attenuations measured by AI-ROI showed no significant differences (p = 0.545) and a reduction of 71% time compared to expert measurements. The area under the curve (AUC) of the steatosis classification of AI-ROI is 0.921 (95% CI: 0.883 - 0.959). If performed as a routine screening method, our AI protocol could potentially allow early non-invasive, non-pharmacological preventative interventions for hepatic steatosis. 1,014 expert-annotated liver segmentations of patients with hepatic steatosis annotations can be downloaded here: https://drive.google.com/drive/folders/1-g_zJeAaZXYXGqL1OeF6pUjr6KB0igJX.



### Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.17408v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.17408v1)
- **Published**: 2022-10-27 03:28:34+00:00
- **Updated**: 2022-10-27 03:28:34+00:00
- **Authors**: Xutao Guo, Yanwu Yang, Chenfei Ye, Shang Lu, Yang Xiang, Ting Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Based on the Denoising Diffusion Probabilistic Model (DDPM), medical image segmentation can be described as a conditional image generation task, which allows to compute pixel-wise uncertainty maps of the segmentation and allows an implicit ensemble of segmentations to boost the segmentation performance. However, DDPM requires many iterative denoising steps to generate segmentations from Gaussian noise, resulting in extremely inefficient inference. To mitigate the issue, we propose a principled acceleration strategy, called pre-segmentation diffusion sampling DDPM (PD-DDPM), which is specially used for medical image segmentation. The key idea is to obtain pre-segmentation results based on a separately trained segmentation network, and construct noise predictions (non-Gaussian distribution) according to the forward diffusion rule. We can then start with noisy predictions and use fewer reverse steps to generate segmentation results. Experiments show that PD-DDPM yields better segmentation results over representative baseline methods even if the number of reverse steps is significantly reduced. Moreover, PD-DDPM is orthogonal to existing advanced segmentation models, which can be combined to further improve the segmentation performance.



### Towards Complex Backgrounds: A Unified Difference-Aware Decoder for Binary Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.15156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15156v1)
- **Published**: 2022-10-27 03:45:29+00:00
- **Updated**: 2022-10-27 03:45:29+00:00
- **Authors**: Jiepan Li, Wei He, Hongyan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Binary segmentation is used to distinguish objects of interest from background, and is an active area of convolutional encoder-decoder network research. The current decoders are designed for specific objects based on the common backbones as the encoders, but cannot deal with complex backgrounds. Inspired by the way human eyes detect objects of interest, a new unified dual-branch decoder paradigm named the difference-aware decoder is proposed in this paper to explore the difference between the foreground and the background and separate the objects of interest in optical images. The difference-aware decoder imitates the human eye in three stages using the multi-level features output by the encoder. In Stage A, the first branch decoder of the difference-aware decoder is used to obtain a guide map. The highest-level features are enhanced with a novel field expansion module and a dual residual attention module, and are combined with the lowest-level features to obtain the guide map. In Stage B, the other branch decoder adopts a middle feature fusion module to make trade-offs between textural details and semantic information and generate background-aware features. In Stage C, the proposed difference-aware extractor, consisting of a difference guidance model and a difference enhancement module, fuses the guide map from Stage A and the background-aware features from Stage B, to enlarge the differences between the foreground and the background and output a final detection result. The results demonstrate that the difference-aware decoder can achieve a higher accuracy than the other state-of-the-art binary segmentation methods for these tasks.



### Global-to-local Expression-aware Embeddings for Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.15160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15160v2)
- **Published**: 2022-10-27 04:00:04+00:00
- **Updated**: 2022-10-28 02:42:34+00:00
- **Authors**: Rudong An, Wei Zhang, Hao Zeng, Wei Chen, Zhigang Deng, Yu Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Expressions and facial action units (AUs) are two levels of facial behavior descriptors. Expression auxiliary information has been widely used to improve the AU detection performance. However, most existing expression representations can only describe pre-determined discrete categories (e.g., Angry, Disgust, Happy, Sad, etc.) and cannot capture subtle expression transformations like AUs. In this paper, we propose a novel fine-grained \textsl{Global Expression representation Encoder} to capture subtle and continuous facial movements, to promote AU detection. To obtain such a global expression representation, we propose to train an expression embedding model on a large-scale expression dataset according to global expression similarity. Moreover, considering the local definition of AUs, it is essential to extract local AU features. Therefore, we design a \textsl{Local AU Features Module} to generate local facial features for each AU. Specifically, it consists of an AU feature map extractor and a corresponding AU mask extractor. First, the two extractors transform the global expression representation into AU feature maps and masks, respectively. Then, AU feature maps and their corresponding AU masks are multiplied to generate AU masked features focusing on local facial region. Finally, the AU masked features are fed into an AU classifier for judging the AU occurrence. Extensive experiment results demonstrate the superiority of our proposed method. Our method validly outperforms previous works and achieves state-of-the-art performances on widely-used face datasets, including BP4D, DISFA, and BP4D+.



### FAS-UNet: A Novel FAS-driven Unet to Learn Variational Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.15164v2
- **DOI**: 10.3390/math10214055
- **Categories**: **cs.CV**, cs.AI, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2210.15164v2)
- **Published**: 2022-10-27 04:15:16+00:00
- **Updated**: 2022-11-06 07:14:56+00:00
- **Authors**: Hui Zhu, Shi Shu, Jianping Zhang
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Solving variational image segmentation problems with hidden physics is often expensive and requires different algorithms and manually tunes model parameter. The deep learning methods based on the U-Net structure have obtained outstanding performances in many different medical image segmentation tasks, but designing such networks requires a lot of parameters and training data, not always available for practical problems. In this paper, inspired by traditional multi-phase convexity Mumford-Shah variational model and full approximation scheme (FAS) solving the nonlinear systems, we propose a novel variational-model-informed network (denoted as FAS-Unet) that exploits the model and algorithm priors to extract the multi-scale features. The proposed model-informed network integrates image data and mathematical models, and implements them through learning a few convolution kernels. Based on the variational theory and FAS algorithm, we first design a feature extraction sub-network (FAS-Solution module) to solve the model-driven nonlinear systems, where a skip-connection is employed to fuse the multi-scale features. Secondly, we further design a convolution block to fuse the extracted features from the previous stage, resulting in the final segmentation possibility. Experimental results on three different medical image segmentation tasks show that the proposed FAS-Unet is very competitive with other state-of-the-art methods in qualitative, quantitative and model complexity evaluations. Moreover, it may also be possible to train specialized network architectures that automatically satisfy some of the mathematical and physical laws in other image problems for better accuracy, faster training and improved generalization.The code is available at \url{https://github.com/zhuhui100/FASUNet}.



### Improved Projection Learning for Lower Dimensional Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2210.15170v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T99, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2210.15170v1)
- **Published**: 2022-10-27 04:44:53+00:00
- **Updated**: 2022-10-27 04:44:53+00:00
- **Authors**: Ilan Price, Jared Tanner
- **Comment**: None
- **Journal**: None
- **Summary**: The requirement to repeatedly move large feature maps off- and on-chip during inference with convolutional neural networks (CNNs) imposes high costs in terms of both energy and time. In this work we explore an improved method for compressing all feature maps of pre-trained CNNs to below a specified limit. This is done by means of learned projections trained via end-to-end finetuning, which can then be folded and fused into the pre-trained network. We also introduce a new `ceiling compression' framework in which evaluate such techniques in view of the future goal of performing inference fully on-chip.



### Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather
- **Arxiv ID**: http://arxiv.org/abs/2210.15176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15176v1)
- **Published**: 2022-10-27 05:09:10+00:00
- **Updated**: 2022-10-27 05:09:10+00:00
- **Authors**: Jinlong Li, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, Hongkai Yu
- **Comment**: Accepted by WACV2023. Code is available at
  https://github.com/jinlong17/DA-Detect
- **Journal**: None
- **Summary**: Most object detection methods for autonomous driving usually assume a consistent feature distribution between training and testing data, which is not always the case when weathers differ significantly. The object detection model trained under clear weather might not be effective enough in foggy weather because of the domain gap. This paper proposes a novel domain adaptive object detection framework for autonomous driving under foggy weather. Our method leverages both image-level and object-level adaptation to diminish the domain discrepancy in image style and object appearance. To further enhance the model's capabilities under challenging samples, we also come up with a new adversarial gradient reversal layer to perform adversarial mining for the hard examples together with domain adaptation. Moreover, we propose to generate an auxiliary domain by data augmentation to enforce a new domain-level metric regularization. Experimental results on public benchmarks show the effectiveness and accuracy of the proposed method. The code is available at https://github.com/jinlong17/DA-Detect.



### Text2Model: Model Induction for Zero-shot Generalization Using Task Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2210.15182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15182v1)
- **Published**: 2022-10-27 05:19:55+00:00
- **Updated**: 2022-10-27 05:19:55+00:00
- **Authors**: Ohad Amosy, Tomer Volk, Eyal Ben-David, Roi Reichart, Gal Chechik
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of generating a training-free task-dependent visual classifier from text descriptions without visual samples. This \textit{Text-to-Model} (T2M) problem is closely related to zero-shot learning, but unlike previous work, a T2M model infers a model tailored to a task, taking into account all classes in the task. We analyze the symmetries of T2M, and characterize the equivariance and invariance properties of corresponding models. In light of these properties, we design an architecture based on hypernetworks that given a set of new class descriptions predicts the weights for an object recognition model which classifies images from those zero-shot classes. We demonstrate the benefits of our approach compared to zero-shot learning from text descriptions in image and point-cloud classification using various types of text descriptions: From single words to rich text descriptions.



### SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering
- **Arxiv ID**: http://arxiv.org/abs/2210.15185v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15185v3)
- **Published**: 2022-10-27 05:30:43+00:00
- **Updated**: 2023-05-23 06:56:30+00:00
- **Authors**: Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao, Cewu Lu
- **Comment**: Accepted to Robotics: Science and Systems (RSS) 2023
- **Journal**: None
- **Summary**: Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage at https://sites.google.com/view/rss-sam-rl.



### Learning Joint Representation of Human Motion and Language
- **Arxiv ID**: http://arxiv.org/abs/2210.15187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15187v1)
- **Published**: 2022-10-27 05:32:20+00:00
- **Updated**: 2022-10-27 05:32:20+00:00
- **Authors**: Jihoon Kim, Youngjae Yu, Seungyoun Shin, Taehyun Byun, Sungjoon Choi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present MoLang (a Motion-Language connecting model) for learning joint representation of human motion and language, leveraging both unpaired and paired datasets of motion and language modalities. To this end, we propose a motion-language model with contrastive learning, empowering our model to learn better generalizable representations of the human motion domain. Empirical results show that our model learns strong representations of human motion data through navigating language modality. Our proposed method is able to perform both action recognition and motion retrieval tasks with a single model where it outperforms state-of-the-art approaches on a number of action recognition benchmarks.



### Few-shot Image Generation via Masked Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2210.15194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15194v2)
- **Published**: 2022-10-27 06:02:22+00:00
- **Updated**: 2023-03-07 06:03:38+00:00
- **Authors**: Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image generation aims to generate images of high quality and great diversity with limited data. However, it is difficult for modern GANs to avoid overfitting when trained on only a few images. The discriminator can easily remember all the training samples and guide the generator to replicate them, leading to severe diversity degradation. Several methods have been proposed to relieve overfitting by adapting GANs pre-trained on large source domains to target domains using limited real samples. This work presents a novel approach to realize few-shot GAN adaptation via masked discrimination. Random masks are applied to features extracted by the discriminator from input images. We aim to encourage the discriminator to judge various images which share partially common features with training samples as realistic. Correspondingly, the generator is guided to generate diverse images instead of replicating training samples. In addition, we employ a cross-domain consistency loss for the discriminator to keep relative distances between generated samples in its feature space. It strengthens global image discrimination and guides adapted GANs to preserve more information learned from source domains for higher image quality. The effectiveness of our approach is demonstrated both qualitatively and quantitatively with higher quality and greater diversity on a series of few-shot image generation tasks than prior methods.



### Deep-MDS Framework for Recovering the 3D Shape of 2D Landmarks from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2210.15200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15200v1)
- **Published**: 2022-10-27 06:20:10+00:00
- **Updated**: 2022-10-27 06:20:10+00:00
- **Authors**: Shima Kamyab, Zohreh Azimifar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a low parameter deep learning framework utilizing the Non-metric Multi-Dimensional scaling (NMDS) method, is proposed to recover the 3D shape of 2D landmarks on a human face, in a single input image. Hence, NMDS approach is used for the first time to establish a mapping from a 2D landmark space to the corresponding 3D shape space. A deep neural network learns the pairwise dissimilarity among 2D landmarks, used by NMDS approach, whose objective is to learn the pairwise 3D Euclidean distance of the corresponding 2D landmarks on the input image. This scheme results in a symmetric dissimilarity matrix, with the rank larger than 2, leading the NMDS approach toward appropriately recovering the 3D shape of corresponding 2D landmarks. In the case of posed images and complex image formation processes like perspective projection which causes occlusion in the input image, we consider an autoencoder component in the proposed framework, as an occlusion removal part, which turns different input views of the human face into a profile view. The results of a performance evaluation using different synthetic and real-world human face datasets, including Besel Face Model (BFM), CelebA, CoMA - FLAME, and CASIA-3D, indicates the comparable performance of the proposed framework, despite its small number of training parameters, with the related state-of-the-art and powerful 3D reconstruction methods from the literature, in terms of efficiency and accuracy.



### Multi-view Contrastive Learning with Additive Margin for Adaptive Nasopharyngeal Carcinoma Radiotherapy Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.15201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15201v1)
- **Published**: 2022-10-27 06:21:24+00:00
- **Updated**: 2022-10-27 06:21:24+00:00
- **Authors**: Jiabao Sheng, Yuanpeng Zhang, Jing Cai, Sai-Kit Lam, Zhe Li, Jiang Zhang, Xinzhi Teng
- **Comment**: submitted to ICASSP 2023, 5 pages
- **Journal**: None
- **Summary**: The prediction of adaptive radiation therapy (ART) prior to radiation therapy (RT) for nasopharyngeal carcinoma (NPC) patients is important to reduce toxicity and prolong the survival of patients. Currently, due to the complex tumor micro-environment, a single type of high-resolution image can provide only limited information. Meanwhile, the traditional softmax-based loss is insufficient for quantifying the discriminative power of a model. To overcome these challenges, we propose a supervised multi-view contrastive learning method with an additive margin (MMCon). For each patient, four medical images are considered to form multi-view positive pairs, which can provide additional information and enhance the representation of medical images. In addition, the embedding space is learned by means of contrastive learning. NPC samples from the same patient or with similar labels will remain close in the embedding space, while NPC samples with different labels will be far apart. To improve the discriminative ability of the loss function, we incorporate a margin into the contrastive learning. Experimental result show this new learning objective can be used to find an embedding space that exhibits superior discrimination ability for NPC images.



### SSD: Towards Better Text-Image Consistency Metric in Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.15235v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15235v3)
- **Published**: 2022-10-27 07:47:47+00:00
- **Updated**: 2022-12-03 05:15:10+00:00
- **Authors**: Zhaorui Tan, Xi Yang, Zihan Ye, Qiufeng Wang, Yuyao Yan, Anh Nguyen, Kaizhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Generating consistent and high-quality images from given texts is essential for visual-language understanding. Although impressive results have been achieved in generating high-quality images, text-image consistency is still a major concern in existing GAN-based methods. Particularly, the most popular metric $R$-precision may not accurately reflect the text-image consistency, often resulting in very misleading semantics in the generated images. Albeit its significance, how to design a better text-image consistency metric surprisingly remains under-explored in the community. In this paper, we make a further step forward to develop a novel CLIP-based metric termed as Semantic Similarity Distance ($SSD$), which is both theoretically founded from a distributional viewpoint and empirically verified on benchmark datasets. Benefiting from the proposed metric, we further design the Parallel Deep Fusion Generative Adversarial Networks (PDF-GAN) that aims at improving text-image consistency by fusing semantic information at different granularities and capturing accurate semantics. Equipped with two novel plug-and-play components: Hard-Negative Sentence Constructor and Semantic Projection, the proposed PDF-GAN can mitigate inconsistent semantics and bridge the text-image semantic gap. A series of experiments show that, as opposed to current state-of-the-art methods, our PDF-GAN can lead to significantly better text-image consistency while maintaining decent image quality on the CUB and COCO datasets.



### ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts
- **Arxiv ID**: http://arxiv.org/abs/2210.15257v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15257v2)
- **Published**: 2022-10-27 08:21:35+00:00
- **Updated**: 2023-03-28 03:31:00+00:00
- **Authors**: Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, Yu Sun, Li Chen, Hao Tian, Hua Wu, Haifeng Wang
- **Comment**: Accepted to CVPR 2023, highlight. Project page:
  https://wenxin.baidu.com/ernie-vilg
- **Journal**: None
- **Summary**: Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by: (1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO with zero-shot FID score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.



### Improved Feature Distillation via Projector Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2210.15274v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15274v2)
- **Published**: 2022-10-27 09:08:40+00:00
- **Updated**: 2023-03-01 02:06:06+00:00
- **Authors**: Yudong Chen, Sen Wang, Jiajun Liu, Xuwei Xu, Frank de Hoog, Zi Huang
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: In knowledge distillation, previous feature distillation methods mainly focus on the design of loss functions and the selection of the distilled layers, while the effect of the feature projector between the student and the teacher remains under-explored. In this paper, we first discuss a plausible mechanism of the projector with empirical evidence and then propose a new feature distillation method based on a projector ensemble for further performance improvement. We observe that the student network benefits from a projector even if the feature dimensions of the student and the teacher are the same. Training a student backbone without a projector can be considered as a multi-task learning process, namely achieving discriminative feature extraction for classification and feature matching between the student and the teacher for distillation at the same time. We hypothesize and empirically verify that without a projector, the student network tends to overfit the teacher's feature distributions despite having different architecture and weights initialization. This leads to degradation on the quality of the student's deep features that are eventually used in classification. Adding a projector, on the other hand, disentangles the two learning tasks and helps the student network to focus better on the main feature extraction task while still being able to utilize teacher features as a guidance through the projector. Motivated by the positive effect of the projector in feature distillation, we propose an ensemble of projectors to further improve the quality of student features. Experimental results on different datasets with a series of teacher-student pairs illustrate the effectiveness of the proposed method.



### The 1st-place Solution for ECCV 2022 Multiple People Tracking in Group Dance Challenge
- **Arxiv ID**: http://arxiv.org/abs/2210.15281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15281v1)
- **Published**: 2022-10-27 09:28:44+00:00
- **Updated**: 2022-10-27 09:28:44+00:00
- **Authors**: Yuang Zhang, Tiancai Wang, Weiyao Lin, Xiangyu Zhang
- **Comment**: https://motcomplex.github.io/
- **Journal**: None
- **Summary**: We present our 1st place solution to the Group Dance Multiple People Tracking Challenge. Based on MOTR: End-to-End Multiple-Object Tracking with Transformer, we explore: 1) detect queries as anchors, 2) tracking as query denoising, 3) joint training on pseudo video clips generated from CrowdHuman dataset, and 4) using the YOLOX detection proposals for the anchor initialization of detect queries. Our method achieves 73.4% HOTA on the DanceTrack test set, surpassing the second-place solution by +6.8% HOTA.



### Efficient Similarity-based Passive Filter Pruning for Compressing CNNs
- **Arxiv ID**: http://arxiv.org/abs/2210.17416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.17416v1)
- **Published**: 2022-10-27 09:57:47+00:00
- **Updated**: 2022-10-27 09:57:47+00:00
- **Authors**: Arshdeep Singh, Mark D. Plumbley
- **Comment**: Submitted to ICASSP 2023
- **Journal**: None
- **Summary**: Convolution neural networks (CNNs) have shown great success in various applications. However, the computational complexity and memory storage of CNNs is a bottleneck for their deployment on resource-constrained devices. Recent efforts towards reducing the computation cost and the memory overhead of CNNs involve similarity-based passive filter pruning methods. Similarity-based passive filter pruning methods compute a pairwise similarity matrix for the filters and eliminate a few similar filters to obtain a small pruned CNN. However, the computational complexity of computing the pairwise similarity matrix is high, particularly when a convolutional layer has many filters. To reduce the computational complexity in obtaining the pairwise similarity matrix, we propose to use an efficient method where the complete pairwise similarity matrix is approximated from only a few of its columns by using a Nystr\"om approximation method. The proposed efficient similarity-based passive filter pruning method is 3 times faster and gives same accuracy at the same reduction in computations for CNNs compared to that of the similarity-based pruning method that computes a complete pairwise similarity matrix. Apart from this, the proposed efficient similarity-based pruning method performs similarly or better than the existing norm-based pruning methods. The efficacy of the proposed pruning method is evaluated on CNNs such as DCASE 2021 Task 1A baseline network and a VGGish network designed for acoustic scene classification.



### Isometric 3D Adversarial Examples in the Physical World
- **Arxiv ID**: http://arxiv.org/abs/2210.15291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.15291v1)
- **Published**: 2022-10-27 09:58:15+00:00
- **Updated**: 2022-10-27 09:58:15+00:00
- **Authors**: Yibo Miao, Yinpeng Dong, Jun Zhu, Xiao-Shan Gao
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: 3D deep learning models are shown to be as vulnerable to adversarial examples as 2D models. However, existing attack methods are still far from stealthy and suffer from severe performance degradation in the physical world. Although 3D data is highly structured, it is difficult to bound the perturbations with simple metrics in the Euclidean space. In this paper, we propose a novel $\epsilon$-isometric ($\epsilon$-ISO) attack to generate natural and robust 3D adversarial examples in the physical world by considering the geometric properties of 3D objects and the invariance to physical transformations. For naturalness, we constrain the adversarial example to be $\epsilon$-isometric to the original one by adopting the Gaussian curvature as a surrogate metric guaranteed by a theoretical analysis. For invariance to physical transformations, we propose a maxima over transformation (MaxOT) method that actively searches for the most harmful transformations rather than random ones to make the generated adversarial example more robust in the physical world. Experiments on typical point cloud recognition models validate that our approach can significantly improve the attack success rate and naturalness of the generated 3D adversarial examples than the state-of-the-art attack methods.



### Spatio-Temporal Hybrid Fusion of CAE and SWIn Transformers for Lung Cancer Malignancy Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.15297v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15297v1)
- **Published**: 2022-10-27 10:07:00+00:00
- **Updated**: 2022-10-27 10:07:00+00:00
- **Authors**: Sadaf Khademi, Shahin Heidarian, Parnian Afshar, Farnoosh Naderkhani, Anastasia Oikonomou, Konstantinos Plataniotis, Arash Mohammadi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2110.08721
- **Journal**: None
- **Summary**: The paper proposes a novel hybrid discovery Radiomics framework that simultaneously integrates temporal and spatial features extracted from non-thin chest Computed Tomography (CT) slices to predict Lung Adenocarcinoma (LUAC) malignancy with minimum expert involvement. Lung cancer is the leading cause of mortality from cancer worldwide and has various histologic types, among which LUAC has recently been the most prevalent. LUACs are classified as pre-invasive, minimally invasive, and invasive adenocarcinomas. Timely and accurate knowledge of the lung nodules malignancy leads to a proper treatment plan and reduces the risk of unnecessary or late surgeries. Currently, chest CT scan is the primary imaging modality to assess and predict the invasiveness of LUACs. However, the radiologists' analysis based on CT images is subjective and suffers from a low accuracy compared to the ground truth pathological reviews provided after surgical resections. The proposed hybrid framework, referred to as the CAET-SWin, consists of two parallel paths: (i) The Convolutional Auto-Encoder (CAE) Transformer path that extracts and captures informative features related to inter-slice relations via a modified Transformer architecture, and; (ii) The Shifted Window (SWin) Transformer path, which is a hierarchical vision transformer that extracts nodules' related spatial features from a volumetric CT scan. Extracted temporal (from the CAET-path) and spatial (from the Swin path) are then fused through a fusion path to classify LUACs. Experimental results on our in-house dataset of 114 pathologically proven Sub-Solid Nodules (SSNs) demonstrate that the CAET-SWin significantly improves reliability of the invasiveness prediction task while achieving an accuracy of 82.65%, sensitivity of 83.66%, and specificity of 81.66% using 10-fold cross-validation.



### Leveraging Computer Vision Application in Visual Arts: A Case Study on the Use of Residual Neural Network to Classify and Analyze Baroque Paintings
- **Arxiv ID**: http://arxiv.org/abs/2210.15300v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15300v1)
- **Published**: 2022-10-27 10:15:36+00:00
- **Updated**: 2022-10-27 10:15:36+00:00
- **Authors**: Daniel Kvak
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing availability of large digitized fine art collections, automated analysis and classification of paintings is becoming an interesting area of research. However, due to domain specificity, implicit subjectivity, and pervasive nuances that vaguely separate art movements, analyzing art using machine learning techniques poses significant challenges. Residual networks, or variants thereof, are one the most popular tools for image classification tasks, which can extract relevant features for well-defined classes. In this case study, we focus on the classification of a selected painting 'Portrait of the Painter Charles Bruni' by Johann Kupetzky and the analysis of the performance of the proposed classifier. We show that the features extracted during residual network training can be useful for image retrieval within search systems in online art collections.



### MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2210.15316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.15316v1)
- **Published**: 2022-10-27 10:55:15+00:00
- **Updated**: 2022-10-27 10:55:15+00:00
- **Authors**: Gopi Krishna Erabati, Helder Araujo
- **Comment**: Accepted at the ICPR 2022 Workshop DLVDR2022
- **Journal**: None
- **Summary**: 3D object detection is a significant task for autonomous driving. Recently with the progress of vision transformers, the 2D object detection problem is being treated with the set-to-set loss. Inspired by these approaches on 2D object detection and an approach for multi-view 3D object detection DETR3D, we propose MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer architecture to fuse image and LiDAR features to improve the detection accuracy. Our end-to-end single-stage, anchor-free and NMS-free network takes in multi-view images and LiDAR point clouds and predicts 3D bounding boxes. Firstly, we link the object queries learnt from data to the image and LiDAR features using a novel MSF3DDETR cross-attention block. Secondly, the object queries interacts with each other in multi-head self-attention block. Finally, MSF3DDETR block is repeated for $L$ number of times to refine the object queries. The MSF3DDETR network is trained end-to-end on the nuScenes dataset using Hungarian algorithm based bipartite matching and set-to-set loss inspired by DETR. We present both quantitative and qualitative results which are competitive to the state-of-the-art approaches.



### Efficient and Effective Augmentation Strategy for Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2210.15318v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15318v1)
- **Published**: 2022-10-27 10:59:55+00:00
- **Updated**: 2022-10-27 10:59:55+00:00
- **Authors**: Sravanti Addepalli, Samyak Jain, R. Venkatesh Babu
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Adversarial training of Deep Neural Networks is known to be significantly more data-hungry when compared to standard training. Furthermore, complex data augmentations such as AutoAugment, which have led to substantial gains in standard training of image classifiers, have not been successful with Adversarial Training. We first explain this contrasting behavior by viewing augmentation during training as a problem of domain generalization, and further propose Diverse Augmentation-based Joint Adversarial Training (DAJAT) to use data augmentations effectively in adversarial training. We aim to handle the conflicting goals of enhancing the diversity of the training dataset and training with data that is close to the test distribution by using a combination of simple and complex augmentations with separate batch normalization layers during training. We further utilize the popular Jensen-Shannon divergence loss to encourage the joint learning of the diverse augmentations, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT), that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking. The proposed method DAJAT achieves substantially better robustness-accuracy trade-off when compared to existing methods on the RobustBench Leaderboard on ResNet-18 and WideResNet-34-10. The code for implementing DAJAT is available here: https://github.com/val-iisc/DAJAT.



### Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities
- **Arxiv ID**: http://arxiv.org/abs/2210.15359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15359v1)
- **Published**: 2022-10-27 12:16:25+00:00
- **Updated**: 2022-10-27 12:16:25+00:00
- **Authors**: Haolin Zuo, Rui Liu, Jinming Zhao, Guanglai Gao, Haizhou Li
- **Comment**: 5 pages, 3 figures, 1 table. Submitted to ICASSP 2023. We release the
  code at: https://github.com/ZhuoYulang/IF-MMIN
- **Journal**: None
- **Summary**: Multimodal emotion recognition leverages complementary information across modalities to gain performance. However, we cannot guarantee that the data of all modalities are always present in practice. In the studies to predict the missing data across modalities, the inherent difference between heterogeneous modalities, namely the modality gap, presents a challenge. To address this, we propose to use invariant features for a missing modality imagination network (IF-MMIN) which includes two novel mechanisms: 1) an invariant feature learning strategy that is based on the central moment discrepancy (CMD) distance under the full-modality scenario; 2) an invariant feature based imagination module (IF-IM) to alleviate the modality gap during the missing modalities prediction, thus improving the robustness of multimodal joint representation. Comprehensive experiments on the benchmark dataset IEMOCAP demonstrate that the proposed model outperforms all baselines and invariantly improves the overall emotion recognition performance under uncertain missing-modality conditions. We release the code at: https://github.com/ZhuoYulang/IF-MMIN.



### A Novel Approach for Neuromorphic Vision Data Compression based on Deep Belief Network
- **Arxiv ID**: http://arxiv.org/abs/2210.15362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15362v1)
- **Published**: 2022-10-27 12:21:14+00:00
- **Updated**: 2022-10-27 12:21:14+00:00
- **Authors**: Sally Khaidem, Mansi Sharma, Abhipraay Nevatia
- **Comment**: None
- **Journal**: None
- **Summary**: A neuromorphic camera is an image sensor that emulates the human eyes capturing only changes in local brightness levels. They are widely known as event cameras, silicon retinas or dynamic vision sensors (DVS). DVS records asynchronous per-pixel brightness changes, resulting in a stream of events that encode the brightness change's time, location, and polarity. DVS consumes little power and can capture a wider dynamic range with no motion blur and higher temporal resolution than conventional frame-based cameras. Although this method of event capture results in a lower bit rate than traditional video capture, it is further compressible. This paper proposes a novel deep learning-based compression scheme for event data. Using a deep belief network (DBN), the high dimensional event data is reduced into a latent representation and later encoded using an entropy-based coding technique. The proposed scheme is among the first to incorporate deep learning for event compression. It achieves a high compression ratio while maintaining good reconstruction quality outperforming state-of-the-art event data coders and other lossless benchmark techniques.



### Li3DeTr: A LiDAR based 3D Detection Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.15365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15365v1)
- **Published**: 2022-10-27 12:23:54+00:00
- **Updated**: 2022-10-27 12:23:54+00:00
- **Authors**: Gopi Krishna Erabati, Helder Araujo
- **Comment**: Accepted at the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) 2023
- **Journal**: None
- **Summary**: Inspired by recent advances in vision transformers for object detection, we propose Li3DeTr, an end-to-end LiDAR based 3D Detection Transformer for autonomous driving, that inputs LiDAR point clouds and regresses 3D bounding boxes. The LiDAR local and global features are encoded using sparse convolution and multi-scale deformable attention respectively. In the decoder head, firstly, in the novel Li3DeTr cross-attention block, we link the LiDAR global features to 3D predictions leveraging the sparse set of object queries learnt from the data. Secondly, the object query interactions are formulated using multi-head self-attention. Finally, the decoder layer is repeated $L_{dec}$ number of times to refine the object queries. Inspired by DETR, we employ set-to-set loss to train the Li3DeTr network. Without bells and whistles, the Li3DeTr network achieves 61.3% mAP and 67.6% NDS surpassing the state-of-the-art methods with non-maximum suppression (NMS) on the nuScenes dataset and it also achieves competitive performance on the KITTI dataset. We also employ knowledge distillation (KD) using a teacher and student model that slightly improves the performance of our network.



### Meta-Learning Initializations for Interactive Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2210.15371v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15371v1)
- **Published**: 2022-10-27 12:30:53+00:00
- **Updated**: 2022-10-27 12:30:53+00:00
- **Authors**: Zachary M. C. Baum, Yipeng Hu, Dean Barratt
- **Comment**: 11 pages, 10 figures. Paper accepted to IEEE Transactions on Medical
  Imaging (October 26 2022)
- **Journal**: None
- **Summary**: We present a meta-learning framework for interactive medical image registration. Our proposed framework comprises three components: a learning-based medical image registration algorithm, a form of user interaction that refines registration at inference, and a meta-learning protocol that learns a rapidly adaptable network initialization. This paper describes a specific algorithm that implements the registration, interaction and meta-learning protocol for our exemplar clinical application: registration of magnetic resonance (MR) imaging to interactively acquired, sparsely-sampled transrectal ultrasound (TRUS) images. Our approach obtains comparable registration error (4.26 mm) to the best-performing non-interactive learning-based 3D-to-3D method (3.97 mm) while requiring only a fraction of the data, and occurring in real-time during acquisition. Applying sparsely sampled data to non-interactive methods yields higher registration errors (6.26 mm), demonstrating the effectiveness of interactive MR-TRUS registration, which may be applied intraoperatively given the real-time nature of the adaptation process.



### 2T-UNET: A Two-Tower UNet with Depth Clues for Robust Stereo Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.15374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15374v1)
- **Published**: 2022-10-27 12:34:41+00:00
- **Updated**: 2022-10-27 12:34:41+00:00
- **Authors**: Rohit Choudhary, Mansi Sharma, Rithvik Anil
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo correspondence matching is an essential part of the multi-step stereo depth estimation process. This paper revisits the depth estimation problem, avoiding the explicit stereo matching step using a simple two-tower convolutional neural network. The proposed algorithm is entitled as 2T-UNet. The idea behind 2T-UNet is to replace cost volume construction with twin convolution towers. These towers have an allowance for different weights between them. Additionally, the input for twin encoders in 2T-UNet are different compared to the existing stereo methods. Generally, a stereo network takes a right and left image pair as input to determine the scene geometry. However, in the 2T-UNet model, the right stereo image is taken as one input and the left stereo image along with its monocular depth clue information, is taken as the other input. Depth clues provide complementary suggestions that help enhance the quality of predicted scene geometry. The 2T-UNet surpasses state-of-the-art monocular and stereo depth estimation methods on the challenging Scene flow dataset, both quantitatively and qualitatively. The architecture performs incredibly well on complex natural scenes, highlighting its usefulness for various real-time applications. Pretrained weights and code will be made readily available.



### Retrieving Users' Opinions on Social Media with Multimodal Aspect-Based Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2210.15377v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15377v2)
- **Published**: 2022-10-27 12:38:10+00:00
- **Updated**: 2023-01-09 07:40:32+00:00
- **Authors**: Miriam Anschtz, Tobias Eder, Georg Groh
- **Comment**: 8 pages, 5 figures, published at 2023 IEEE 17th International
  Conference on Semantic Computing (ICSC)
- **Journal**: None
- **Summary**: People post their opinions and experiences on social media, yielding rich databases of end-users' sentiments. This paper shows to what extent machine learning can analyze and structure these databases. An automated data analysis pipeline is deployed to provide insights into user-generated content for researchers in other domains. First, the domain expert can select an image and a term of interest. Then, the pipeline uses image retrieval to find all images showing similar content and applies aspect-based sentiment analysis to outline users' opinions about the selected term. As part of an interdisciplinary project between architecture and computer science researchers, an empirical study of Hamburg's Elbphilharmonie was conveyed. Therefore, we selected 300 thousand posts with the hashtag \enquote{\texttt{hamburg}} from the platform Flickr. Image retrieval methods generated a subset of slightly more than 1.5 thousand images displaying the Elbphilharmonie. We found that these posts mainly convey a neutral or positive sentiment towards it. With this pipeline, we suggest a new semantic computing method that offers novel insights into end-users opinions, e.g., for architecture domain experts.



### LeNo: Adversarial Robust Salient Object Detection Networks with Learnable Noise
- **Arxiv ID**: http://arxiv.org/abs/2210.15392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15392v2)
- **Published**: 2022-10-27 12:52:55+00:00
- **Updated**: 2022-12-07 13:07:47+00:00
- **Authors**: He Wang, Lin Wan, He Tang
- **Comment**: 8 pages, 6 figures, accepted by AAAI 2023
- **Journal**: None
- **Summary**: Pixel-wise prediction with deep neural network has become an effective paradigm for salient object detection (SOD) and achieved remarkable performance. However, very few SOD models are robust against adversarial attacks which are visually imperceptible for human visual attention. The previous work robust saliency (ROSA) shuffles the pre-segmented superpixels and then refines the coarse saliency map by the densely connected conditional random field (CRF). Different from ROSA that relies on various pre- and post-processings, this paper proposes a light-weight Learnable Noise (LeNo) to defend adversarial attacks for SOD models. LeNo preserves accuracy of SOD models on both adversarial and clean images, as well as inference speed. In general, LeNo consists of a simple shallow noise and noise estimation that embedded in the encoder and decoder of arbitrary SOD networks respectively. Inspired by the center prior of human visual attention mechanism, we initialize the shallow noise with a cross-shaped gaussian distribution for better defense against adversarial attacks. Instead of adding additional network components for post-processing, the proposed noise estimation modifies only one channel of the decoder. With the deeply-supervised noise-decoupled training on state-of-the-art RGB and RGB-D SOD networks, LeNo outperforms previous works not only on adversarial images but also on clean images, which contributes stronger robustness for SOD. Our code is available at https://github.com/ssecv/LeNo.



### Facial Video-based Remote Physiological Measurement via Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.15401v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15401v3)
- **Published**: 2022-10-27 13:03:23+00:00
- **Updated**: 2023-07-22 07:21:11+00:00
- **Authors**: Zijie Yue, Miaojing Shi, Shuai Ding
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Facial video-based remote physiological measurement aims to estimate remote photoplethysmography (rPPG) signals from human face videos and then measure multiple vital signs (e.g. heart rate, respiration frequency) from rPPG signals. Recent approaches achieve it by training deep neural networks, which normally require abundant facial videos and synchronously recorded photoplethysmography (PPG) signals for supervision. However, the collection of these annotated corpora is not easy in practice. In this paper, we introduce a novel frequency-inspired self-supervised framework that learns to estimate rPPG signals from facial videos without the need of ground truth PPG signals. Given a video sample, we first augment it into multiple positive/negative samples which contain similar/dissimilar signal frequencies to the original one. Specifically, positive samples are generated using spatial augmentation. Negative samples are generated via a learnable frequency augmentation module, which performs non-linear signal frequency transformation on the input without excessively changing its visual appearance. Next, we introduce a local rPPG expert aggregation module to estimate rPPG signals from augmented samples. It encodes complementary pulsation information from different face regions and aggregate them into one rPPG prediction. Finally, we propose a series of frequency-inspired losses, i.e. frequency contrastive loss, frequency ratio consistency loss, and cross-video frequency agreement loss, for the optimization of estimated rPPG signals from multiple augmented video samples and across temporally neighboring video samples. We conduct rPPG-based heart rate, heart rate variability and respiration frequency estimation on four standard benchmarks. The experimental results demonstrate that our method improves the state of the art by a large margin.



### Layer-wise Shared Attention Network on Dynamical System Perspective
- **Arxiv ID**: http://arxiv.org/abs/2210.16101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.16101v1)
- **Published**: 2022-10-27 13:24:08+00:00
- **Updated**: 2022-10-27 13:24:08+00:00
- **Authors**: Zhongzhan Huang, Senwei Liang, Mingfu Liang, Weiling He, Liang Lin
- **Comment**: Work in progress. arXiv admin note: text overlap with
  arXiv:1905.10671
- **Journal**: None
- **Summary**: Attention networks have successfully boosted accuracy in various vision problems. Previous works lay emphasis on designing a new self-attention module and follow the traditional paradigm that individually plugs the modules into each layer of a network. However, such a paradigm inevitably increases the extra parameter cost with the growth of the number of layers. From the dynamical system perspective of the residual neural network, we find that the feature maps from the layers of the same stage are homogenous, which inspires us to propose a novel-and-simple framework, called the dense and implicit attention (DIA) unit, that shares a single attention module throughout different network layers. With our framework, the parameter cost is independent of the number of layers and we further improve the accuracy of existing popular self-attention modules with significant parameter reduction without any elaborated model crafting. Extensive experiments on benchmark datasets show that the DIA is capable of emphasizing layer-wise feature interrelation and thus leads to significant improvement in various vision tasks, including image classification, object detection, and medical application. Furthermore, the effectiveness of the DIA unit is demonstrated by novel experiments where we destabilize the model training by (1) removing the skip connection of the residual neural network, (2) removing the batch normalization of the model, and (3) removing all data augmentation during training. In these cases, we verify that DIA has a strong regularization ability to stabilize the training, i.e., the dense and implicit connections formed by our method can effectively recover and enhance the information communication across layers and the value of the gradient thus alleviate the training instability.



### Supervised classification methods applied to airborne hyperspectral images: Comparative study using mutual information
- **Arxiv ID**: http://arxiv.org/abs/2210.15422v1
- **DOI**: 10.1016/j.procs.2019.01.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15422v1)
- **Published**: 2022-10-27 13:39:08+00:00
- **Updated**: 2022-10-27 13:39:08+00:00
- **Authors**: Hasna Nhaila, Asma Elmaizi, Elkebir Sarhrouni, Ahmed Hammouch
- **Comment**: None
- **Journal**: Procedia Computer Science, 2019, 148, pp. 97-106 -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-85048829863&partnerID=MN8TOARS
- **Summary**: Nowadays, the hyperspectral remote sensing imagery HSI becomes an important tool to observe the Earth's surface, detect the climatic changes and many other applications. The classification of HSI is one of the most challenging tasks due to the large amount of spectral information and the presence of redundant and irrelevant bands. Although great progresses have been made on classification techniques, few studies have been done to provide practical guidelines to determine the appropriate classifier for HSI. In this paper, we investigate the performance of four supervised learning algorithms, namely, Support Vector Machines SVM, Random Forest RF, K-Nearest Neighbors KNN and Linear Discriminant Analysis LDA with different kernels in terms of classification accuracies. The experiments have been performed on three real hyperspectral datasets taken from the NASA's Airborne Visible/Infrared Imaging Spectrometer Sensor AVIRIS and the Reflective Optics System Imaging Spectrometer ROSIS sensors. The mutual information had been used to reduce the dimensionality of the used datasets for better classification efficiency. The extensive experiments demonstrate that the SVM classifier with RBF kernel and RF produced statistically better results and seems to be respectively the more suitable as supervised classifiers for the hyperspectral remote sensing images.   Keywords: hyperspectral images, mutual information, dimension reduction, Support Vector Machines, K-Nearest Neighbors, Random Forest, Linear Discriminant Analysis.



### One-Class Risk Estimation for One-Class Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.15457v2
- **DOI**: 10.1109/TGRS.2023.3292929
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15457v2)
- **Published**: 2022-10-27 14:15:13+00:00
- **Updated**: 2023-08-25 11:45:02+00:00
- **Authors**: Hengwei Zhao, Yanfei Zhong, Xinyu Wang, Hong Shu
- **Comment**: Accepted by TGRS
- **Journal**: None
- **Summary**: Hyperspectral imagery (HSI) one-class classification is aimed at identifying a single target class from the HSI by using only knowing positive data, which can significantly reduce the requirements for annotation. However, when one-class classification meets HSI, it is difficult for classifiers to find a balance between the overfitting and underfitting of positive data due to the problems of distribution overlap and distribution imbalance. Although deep learning-based methods are currently the mainstream to overcome distribution overlap in HSI multiclassification, few studies focus on deep learning-based HSI one-class classification. In this article, a weakly supervised deep HSI one-class classifier, namely, HOneCls, is proposed, where a risk estimator,the one-class risk estimator, is particularly introduced to make the fully convolutional neural network (FCN) with the ability of one class classification in the case of distribution imbalance. Extensive experiments (20 tasks in total) were conducted to demonstrate the superiority of the proposed classifier.



### A Novel Filter Approach for Band Selection and Classification of Hyperspectral Remotely Sensed Images Using Normalized Mutual Information and Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/2210.15477v1
- **DOI**: 10.1007/978-3-030-03577-8_57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15477v1)
- **Published**: 2022-10-27 14:23:06+00:00
- **Updated**: 2022-10-27 14:23:06+00:00
- **Authors**: Hasna Nhaila, Asma Elmaizi, Elkebir Sarhrouni, Ahmed Hammouch
- **Comment**: http://www.scopus.com/inward/record.url?eid=2-s2.0-85056469155&partnerID=MN8TOARS
- **Journal**: International Conference Europe Middle East & North Africa
  Information Systems and Technologies to Support Learning. Springer, Cham,
  2018. p. 521-530
- **Summary**: Band selection is a great challenging task in the classification of hyperspectral remotely sensed images HSI. This is resulting from its high spectral resolution, the many class outputs and the limited number of training samples. For this purpose, this paper introduces a new filter approach for dimension reduction and classification of hyperspectral images using information theoretic (normalized mutual information) and support vector machines SVM. This method consists to select a minimal subset of the most informative and relevant bands from the input datasets for better classification efficiency. We applied our proposed algorithm on two well-known benchmark datasets gathered by the NASA's AVIRIS sensor over Indiana and Salinas valley in USA. The experimental results were assessed based on different evaluation metrics widely used in this area. The comparison with the state of the art methods proves that our method could produce good performance with reduced number of selected bands in a good timing.   Keywords: Dimension reduction, Hyperspectral images, Band selection, Normalized mutual information, Classification, Support vector machines



### GaitMixer: Skeleton-based Gait Representation Learning via Wide-spectrum Multi-axial Mixer
- **Arxiv ID**: http://arxiv.org/abs/2210.15491v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15491v2)
- **Published**: 2022-10-27 14:30:52+00:00
- **Updated**: 2022-10-29 02:38:31+00:00
- **Authors**: Ekkasit Pinyoanuntapong, Ayman Ali, Pu Wang, Minwoo Lee, Chen Chen
- **Comment**: Submitted to ICASSP 2023
- **Journal**: None
- **Summary**: Most existing gait recognition methods are appearance-based, which rely on the silhouettes extracted from the video data of human walking activities. The less-investigated skeleton-based gait recognition methods directly learn the gait dynamics from 2D/3D human skeleton sequences, which are theoretically more robust solutions in the presence of appearance changes caused by clothes, hairstyles, and carrying objects. However, the performance of skeleton-based solutions is still largely behind the appearance-based ones. This paper aims to close such performance gap by proposing a novel network model, GaitMixer, to learn more discriminative gait representation from skeleton sequence data. In particular, GaitMixer follows a heterogeneous multi-axial mixer architecture, which exploits the spatial self-attention mixer followed by the temporal large-kernel convolution mixer to learn rich multi-frequency signals in the gait feature maps. Experiments on the widely used gait database, CASIA-B, demonstrate that GaitMixer outperforms the previous SOTA skeleton-based methods by a large margin while achieving a competitive performance compared with the representative appearance-based solutions. Code will be available at https://github.com/exitudio/gaitmixer



### Reconstruction of compressed spectral imaging based on global structure and spectral correlation
- **Arxiv ID**: http://arxiv.org/abs/2210.15492v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15492v2)
- **Published**: 2022-10-27 14:31:02+00:00
- **Updated**: 2023-01-09 13:46:18+00:00
- **Authors**: Pan Wang, Jie Li, Jieru Chen, Lin Wang, Chun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a convolutional sparse coding method based on global structure characteristics and spectral correlation is proposed for the reconstruction of compressive spectral images. The spectral data is regarded as the convolution sum of the convolution kernel and the corresponding coefficients, using the convolution kernel operates the global image information, preserving the structure information of the spectral image in the spatial dimension. To take full exploration of the constraints between spectra, the coefficients corresponding to the convolution kernel are constrained by the L_(2,1)norm to improve spectral accuracy. And, to solve the problem that convolutional sparse coding is insensitive to low frequency, the global total-variation (TV) constraint is added to estimate the low-frequency components. It not only ensures the effective estimation of the low-frequency but also transforms the convolutional sparse coding into a de-noising process, which makes the reconstructing process simpler. Simulations show that compared with the current mainstream optimization methods, the proposed method can improve the reconstruction quality by up to 4 dB in PSNR and 10% in SSIM, and has a great improvement in the details of the reconstructed image.



### Fusion-based Few-Shot Morphing Attack Detection and Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/2210.15510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15510v1)
- **Published**: 2022-10-27 14:46:53+00:00
- **Updated**: 2022-10-27 14:46:53+00:00
- **Authors**: Na Zhang, Shan Jia, Siwei Lyu, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: The vulnerability of face recognition systems to morphing attacks has posed a serious security threat due to the wide adoption of face biometrics in the real world. Most existing morphing attack detection (MAD) methods require a large amount of training data and have only been tested on a few predefined attack models. The lack of good generalization properties, especially in view of the growing interest in developing novel morphing attacks, is a critical limitation with existing MAD research. To address this issue, we propose to extend MAD from supervised learning to few-shot learning and from binary detection to multiclass fingerprinting in this paper. Our technical contributions include: 1) We propose a fusion-based few-shot learning (FSL) method to learn discriminative features that can generalize to unseen morphing attack types from predefined presentation attacks; 2) The proposed FSL based on the fusion of the PRNU model and Noiseprint network is extended from binary MAD to multiclass morphing attack fingerprinting (MAF). 3) We have collected a large-scale database, which contains five face datasets and eight different morphing algorithms, to benchmark the proposed few-shot MAF (FS-MAF) method. Extensive experimental results show the outstanding performance of our fusion-based FS-MAF. The code and data will be publicly available at https://github.com/nz0001na/mad maf.



### ProContEXT: Exploring Progressive Context Transformer for Tracking
- **Arxiv ID**: http://arxiv.org/abs/2210.15511v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.15511v4)
- **Published**: 2022-10-27 14:47:19+00:00
- **Updated**: 2023-03-30 06:12:26+00:00
- **Authors**: Jin-Peng Lan, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Xu Bao, Wangmeng Xiang, Yifeng Geng, Xuansong Xie
- **Comment**: Accepted at ICASSP 2023, source code is at
  https://github.com/zhiqic/ProContEXT
- **Journal**: None
- **Summary**: Existing Visual Object Tracking (VOT) only takes the target area in the first frame as a template. This causes tracking to inevitably fail in fast-changing and crowded scenes, as it cannot account for changes in object appearance between frames. To this end, we revamped the tracking framework with Progressive Context Encoding Transformer Tracker (ProContEXT), which coherently exploits spatial and temporal contexts to predict object motion trajectories. Specifically, ProContEXT leverages a context-aware self-attention module to encode the spatial and temporal context, refining and updating the multi-scale static and dynamic templates to progressively perform accurately tracking. It explores the complementary between spatial and temporal context, raising a new pathway to multi-context modeling for transformer-based trackers. In addition, ProContEXT revised the token pruning technique to reduce computational complexity. Extensive experiments on popular benchmark datasets such as GOT-10k and TrackingNet demonstrate that the proposed ProContEXT achieves state-of-the-art performance.



### Point-Voxel Adaptive Feature Abstraction for Robust Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.15514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15514v2)
- **Published**: 2022-10-27 14:49:53+00:00
- **Updated**: 2022-10-30 03:43:05+00:00
- **Authors**: Lifa Zhu, Changwei Lin, Chen Zheng, Ninghua Yang
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Great progress has been made in point cloud classification with learning-based methods. However, complex scene and sensor inaccuracy in real-world application make point cloud data suffer from corruptions, such as occlusion, noise and outliers. In this work, we propose Point-Voxel based Adaptive (PV-Ada) feature abstraction for robust point cloud classification under various corruptions. Specifically, the proposed framework iteratively voxelize the point cloud and extract point-voxel feature with shared local encoding and Transformer. Then, adaptive max-pooling is proposed to robustly aggregate the point cloud feature for classification. Experiments on ModelNet-C dataset demonstrate that PV-Ada outperforms the state-of-the-art methods. In particular, we rank the $2^{nd}$ place in ModelNet-C classification track of PointCloud-C Challenge 2022, with Overall Accuracy (OA) being 0.865. Code will be available at https://github.com/zhulf0804/PV-Ada.



### LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception
- **Arxiv ID**: http://arxiv.org/abs/2210.15518v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.15518v4)
- **Published**: 2022-10-27 14:57:14+00:00
- **Updated**: 2023-03-30 04:02:18+00:00
- **Authors**: Chenyang Li, Zhi-Qi Cheng, Jun-Yan He, Pengyu Li, Bin Luo, Hanyuan Chen, Yifeng Geng, Jin-Peng Lan, Xuansong Xie
- **Comment**: Accepted at ICASSP 2023, source code is at
  https://github.com/zhiqic/LongShortNet
- **Journal**: None
- **Summary**: Streaming perception is a critical task in autonomous driving that requires balancing the latency and accuracy of the autopilot system. However, current methods for streaming perception are limited as they only rely on the current and adjacent two frames to learn movement patterns. This restricts their ability to model complex scenes, often resulting in poor detection results. To address this limitation, we propose LongShortNet, a novel dual-path network that captures long-term temporal motion and integrates it with short-term spatial semantics for real-time perception. LongShortNet is notable as it is the first work to extend long-term temporal modeling to streaming perception, enabling spatiotemporal feature fusion. We evaluate LongShortNet on the challenging Argoverse-HD dataset and demonstrate that it outperforms existing state-of-the-art methods with almost no additional computational cost.



### Masked Transformer for image Anomaly Localization
- **Arxiv ID**: http://arxiv.org/abs/2210.15540v1
- **DOI**: 10.1142/S0129065722500307
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15540v1)
- **Published**: 2022-10-27 15:30:48+00:00
- **Updated**: 2022-10-27 15:30:48+00:00
- **Authors**: Axel De Nardin, Pankaj Mishra, Gian Luca Foresti, Claudio Piciarelli
- **Comment**: None
- **Journal**: Int J Neural Syst. 2022;32(7):2250030
- **Summary**: Image anomaly detection consists in detecting images or image portions that are visually different from the majority of the samples in a dataset. The task is of practical importance for various real-life applications like biomedical image analysis, visual inspection in industrial production, banking, traffic management, etc. Most of the current deep learning approaches rely on image reconstruction: the input image is projected in some latent space and then reconstructed, assuming that the network (mostly trained on normal data) will not be able to reconstruct the anomalous portions. However, this assumption does not always hold. We thus propose a new model based on the Vision Transformer architecture with patch masking: the input image is split in several patches, and each patch is reconstructed only from the surrounding data, thus ignoring the potentially anomalous information contained in the patch itself. We then show that multi-resolution patches and their collective embeddings provide a large improvement in the model's performance compared to the exclusive use of the traditional square patches. The proposed model has been tested on popular anomaly detection datasets such as MVTec and head CT and achieved good results when compared to other state-of-the-art approaches.



### Hyperspectral Images Classification and Dimensionality Reduction using spectral interaction and SVM classifier
- **Arxiv ID**: http://arxiv.org/abs/2210.15546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15546v1)
- **Published**: 2022-10-27 15:37:57+00:00
- **Updated**: 2022-10-27 15:37:57+00:00
- **Authors**: Asma Elmaizi, Elkebir Sarhrouni, Ahmed Hammouch, Nacir Chafik
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decades, the hyperspectral remote sensing technology development has attracted growing interest among scientists in various domains. The rich and detailed spectral information provided by the hyperspectral sensors has improved the monitoring and detection capabilities of the earth surface substances. However, the high dimensionality of the hyperspectral images (HSI) is one of the main challenges for the analysis of the collected data. The existence of noisy, redundant and irrelevant bands increases the computational complexity, induce the Hughes phenomenon and decrease the target's classification accuracy. Hence, the dimensionality reduction is an essential step to face the dimensionality challenges. In this paper, we propose a novel filter approach based on the maximization of the spectral interaction measure and the support vector machines for dimensionality reduction and classification of the HSI. The proposed Max Relevance Max Synergy (MRMS) algorithm evaluates the relevance of every band through the combination of spectral synergy, redundancy and relevance measures. Our objective is to select the optimal subset of synergistic bands providing accurate classification of the supervised scene materials. Experimental results have been performed using three different hyperspectral datasets: "Indiana Pine", "Pavia University" and "Salinas" provided by the "NASA-AVIRIS" and the "ROSIS" spectrometers. Furthermore, a comparison with the state of the art band selection methods has been carried out in order to demonstrate the robustness and efficiency of the proposed approach.   Keywords: Hyperspectral images, remote sensing, dimensionality reduction, classification, synergic, correlation, spectral interaction information, mutual inform



### Robust Monocular Localization of Drones by Adapting Domain Maps to Depth Prediction Inaccuracies
- **Arxiv ID**: http://arxiv.org/abs/2210.15559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15559v1)
- **Published**: 2022-10-27 15:48:53+00:00
- **Updated**: 2022-10-27 15:48:53+00:00
- **Authors**: Priyesh Shukla, Sureshkumar S., Alex C. Stutts, Sathya Ravi, Theja Tulabandhula, Amit R. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel monocular localization framework by jointly training deep learning-based depth prediction and Bayesian filtering-based pose reasoning. The proposed cross-modal framework significantly outperforms deep learning-only predictions with respect to model scalability and tolerance to environmental variations. Specifically, we show little-to-no degradation of pose accuracy even with extremely poor depth estimates from a lightweight depth predictor. Our framework also maintains high pose accuracy in extreme lighting variations compared to standard deep learning, even without explicit domain adaptation. By openly representing the map and intermediate feature maps (such as depth estimates), our framework also allows for faster updates and reusing intermediate predictions for other tasks, such as obstacle avoidance, resulting in much higher resource efficiency.



### Multimodal Transformer Distillation for Audio-Visual Synchronization
- **Arxiv ID**: http://arxiv.org/abs/2210.15563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.15563v1)
- **Published**: 2022-10-27 15:53:38+00:00
- **Updated**: 2022-10-27 15:53:38+00:00
- **Authors**: Xuanjun Chen, Haibin Wu, Chung-Che Wang, Hung-yi Lee, Jyh-Shing Roger Jang
- **Comment**: Submitted to ICASSP 2023
- **Journal**: None
- **Summary**: Audio-visual synchronization aims to determine whether the mouth movements and speech in the video are synchronized. VocaLiST reaches state-of-the-art performance by incorporating multimodal Transformers to model audio-visual interact information. However, it requires high computing resources, making it impractical for real-world applications. This paper proposed an MTDVocaLiST model, which is trained by our proposed multimodal Transformer distillation (MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the cross-attention distribution and value-relation in the Transformer of VocaLiST. Our proposed method is effective in two aspects: From the distillation method perspective, MTD loss outperforms other strong distillation baselines. From the distilled model's performance perspective: 1) MTDVocaLiST outperforms similar-size SOTA models, SyncNet, and PM models by 15.69% and 3.39%; 2) MTDVocaLiST reduces the model size of VocaLiST by 83.52%, yet still maintaining similar performance.



### Bridging the visual gap in VLN via semantically richer instructions
- **Arxiv ID**: http://arxiv.org/abs/2210.15565v1
- **DOI**: 10.1007/978-3-031-19836-6_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15565v1)
- **Published**: 2022-10-27 15:58:07+00:00
- **Updated**: 2022-10-27 15:58:07+00:00
- **Authors**: Joaquin Ossandn, Benjamin Earle, lvaro Soto
- **Comment**: Accepted in ECCV 2022. Research completed on November 21, 2021
- **Journal**: None
- **Summary**: The Visual-and-Language Navigation (VLN) task requires understanding a textual instruction to navigate a natural indoor environment using only visual information. While this is a trivial task for most humans, it is still an open problem for AI models. In this work, we hypothesize that poor use of the visual information available is at the core of the low performance of current models. To support this hypothesis, we provide experimental evidence showing that state-of-the-art models are not severely affected when they receive just limited or even no visual data, indicating a strong overfitting to the textual instructions. To encourage a more suitable use of the visual information, we propose a new data augmentation method that fosters the inclusion of more explicit visual information in the generation of textual navigational instructions. Our main intuition is that current VLN datasets include textual instructions that are intended to inform an expert navigator, such as a human, but not a beginner visual navigational agent, such as a randomly initialized DL model. Specifically, to bridge the visual semantic gap of current VLN datasets, we take advantage of metadata available for the Matterport3D dataset that, among others, includes information about object labels that are present in the scenes. Training a state-of-the-art model with the new set of instructions increase its performance by 8% in terms of success rate on unseen environments, demonstrating the advantages of the proposed data augmentation method.



### UNet-2022: Exploring Dynamics in Non-isomorphic Architecture
- **Arxiv ID**: http://arxiv.org/abs/2210.15566v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15566v1)
- **Published**: 2022-10-27 16:00:04+00:00
- **Updated**: 2022-10-27 16:00:04+00:00
- **Authors**: Jiansen Guo, Hong-Yu Zhou, Liansheng Wang, Yizhou Yu
- **Comment**: Code is available at https://bit.ly/3ggyD5G
- **Journal**: None
- **Summary**: Recent medical image segmentation models are mostly hybrid, which integrate self-attention and convolution layers into the non-isomorphic architecture. However, one potential drawback of these approaches is that they failed to provide an intuitive explanation of why this hybrid combination manner is beneficial, making it difficult for subsequent work to make improvements on top of them. To address this issue, we first analyze the differences between the weight allocation mechanisms of the self-attention and convolution. Based on this analysis, we propose to construct a parallel non-isomorphic block that takes the advantages of self-attention and convolution with simple parallelization. We name the resulting U-shape segmentation model as UNet-2022. In experiments, UNet-2022 obviously outperforms its counterparts in a range segmentation tasks, including abdominal multi-organ segmentation, automatic cardiac diagnosis, neural structures segmentation, and skin lesion segmentation, sometimes surpassing the best performing baseline by 4%. Specifically, UNet-2022 surpasses nnUNet, the most recognized segmentation model at present, by large margins. These phenomena indicate the potential of UNet-2022 to become the model of choice for medical image segmentation.



### Efficient few-shot learning for pixel-precise handwritten document layout analysis
- **Arxiv ID**: http://arxiv.org/abs/2210.15570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15570v1)
- **Published**: 2022-10-27 16:03:52+00:00
- **Updated**: 2022-10-27 16:03:52+00:00
- **Authors**: Axel De Nardin, Silvia Zottin, Matteo Paier, Gian Luca Foresti, Emanuela Colombi, Claudio Piciarelli
- **Comment**: Accepted for publication at IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV), 2023
- **Journal**: None
- **Summary**: Layout analysis is a task of uttermost importance in ancient handwritten document analysis and represents a fundamental step toward the simplification of subsequent tasks such as optical character recognition and automatic transcription. However, many of the approaches adopted to solve this problem rely on a fully supervised learning paradigm. While these systems achieve very good performance on this task, the drawback is that pixel-precise text labeling of the entire training set is a very time-consuming process, which makes this type of information rarely available in a real-world scenario. In the present paper, we address this problem by proposing an efficient few-shot learning framework that achieves performances comparable to current state-of-the-art fully supervised methods on the publicly available DIVA-HisDB dataset.



### Full-scale Deeply Supervised Attention Network for Segmenting COVID-19 Lesions
- **Arxiv ID**: http://arxiv.org/abs/2210.15571v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15571v1)
- **Published**: 2022-10-27 16:05:47+00:00
- **Updated**: 2022-10-27 16:05:47+00:00
- **Authors**: Pallabi Dutta, Sushmita Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Automated delineation of COVID-19 lesions from lung CT scans aids the diagnosis and prognosis for patients. The asymmetric shapes and positioning of the infected regions make the task extremely difficult. Capturing information at multiple scales will assist in deciphering features, at global and local levels, to encompass lesions of variable size and texture. We introduce the Full-scale Deeply Supervised Attention Network (FuDSA-Net), for efficient segmentation of corona-infected lung areas in CT images. The model considers activation responses from all levels of the encoding path, encompassing multi-scalar features acquired at different levels of the network. This helps segment target regions (lesions) of varying shape, size and contrast. Incorporation of the entire gamut of multi-scalar characteristics into the novel attention mechanism helps prioritize the selection of activation responses and locations containing useful information. Determining robust and discriminatory features along the decoder path is facilitated with deep supervision. Connections in the decoder arm are remodeled to handle the issue of vanishing gradient. As observed from the experimental results, FuDSA-Net surpasses other state-of-the-art architectures; especially, when it comes to characterizing complicated geometries of the lesions.



### Joint Multi-Person Body Detection and Orientation Estimation via One Unified Embedding
- **Arxiv ID**: http://arxiv.org/abs/2210.15586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15586v2)
- **Published**: 2022-10-27 16:22:50+00:00
- **Updated**: 2023-03-16 08:25:00+00:00
- **Authors**: Huayi Zhou, Fei Jiang, Jiaxin Si, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Human body orientation estimation (HBOE) is widely applied into various applications, including robotics, surveillance, pedestrian analysis and autonomous driving. Although many approaches have been addressing the HBOE problem from specific under-controlled scenes to challenging in-the-wild environments, they assume human instances are already detected and take a well cropped sub-image as the input. This setting is less efficient and prone to errors in real application, such as crowds of people. In the paper, we propose a single-stage end-to-end trainable framework for tackling the HBOE problem with multi-persons. By integrating the prediction of bounding boxes and direction angles in one embedding, our method can jointly estimate the location and orientation of all bodies in one image directly. Our key idea is to integrate the HBOE task into the multi-scale anchor channel predictions of persons for concurrently benefiting from engaged intermediate features. Therefore, our approach can naturally adapt to difficult instances involving low resolution and occlusion as in object detection. We validated the efficiency and effectiveness of our method in the recently presented benchmark MEBOW with extensive experiments. Besides, we completed ambiguous instances ignored by the MEBOW dataset, and provided corresponding weak body-orientation labels to keep the integrity and consistency of it for supporting studies toward multi-persons. Our work is available at https://github.com/hnuzhy/JointBDOE.



### Class Based Thresholding in Early Exit Semantic Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.15621v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15621v1)
- **Published**: 2022-10-27 17:10:16+00:00
- **Updated**: 2022-10-27 17:10:16+00:00
- **Authors**: Alperen Grmez, Erdem Koyuncu
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: We propose Class Based Thresholding (CBT) to reduce the computational cost of early exit semantic segmentation models while preserving the mean intersection over union (mIoU) performance. A key idea of CBT is to exploit the naturally-occurring neural collapse phenomenon. Specifically, by calculating the mean prediction probabilities of each class in the training set, CBT assigns different masking threshold values to each class, so that the computation can be terminated sooner for pixels belonging to easy-to-predict classes. We show the effectiveness of CBT on Cityscapes and ADE20K datasets. CBT can reduce the computational cost by $23\%$ compared to the previous state-of-the-art early exit models.



### Deep Generative Models on 3D Representations: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2210.15663v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15663v3)
- **Published**: 2022-10-27 17:59:50+00:00
- **Updated**: 2023-08-28 03:02:16+00:00
- **Authors**: Zifan Shi, Sida Peng, Yinghao Xu, Andreas Geiger, Yiyi Liao, Yujun Shen
- **Comment**: Github: https://github.com/justimyhxu/awesome-3D-generation
- **Journal**: None
- **Summary**: Generative models aim to learn the distribution of observed data by generating new instances. With the advent of neural networks, deep generative models, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models (DMs), have progressed remarkably in synthesizing 2D images. Recently, researchers started to shift focus from 2D to 3D space, considering that 3D data is more closely aligned with our physical world and holds immense practical potential. However, unlike 2D images, which possess an inherent and efficient representation (\textit{i.e.}, a pixel grid), representing 3D data poses significantly greater challenges. Ideally, a robust 3D representation should be capable of accurately modeling complex shapes and appearances while being highly efficient in handling high-resolution data with high processing speeds and low memory requirements. Regrettably, existing 3D representations, such as point clouds, meshes, and neural fields, often fail to satisfy all of these requirements simultaneously. In this survey, we thoroughly review the ongoing developments of 3D generative models, including methods that employ 2D and 3D supervision. Our analysis centers on generative models, with a particular focus on the representations utilized in this context. We believe our survey will help the community to track the field's evolution and to spark innovative ideas to propel progress towards solving this challenging task.



### State of the Art in Dense Monocular Non-Rigid 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2210.15664v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.15664v2)
- **Published**: 2022-10-27 17:59:53+00:00
- **Updated**: 2023-03-24 18:45:56+00:00
- **Authors**: Edith Tretschk, Navami Kairanda, Mallikarjun B R, Rishabh Dabral, Adam Kortylewski, Bernhard Egger, Marc Habermann, Pascal Fua, Christian Theobalt, Vladislav Golyanik
- **Comment**: 36 pages, 18 figures, 3 tables; State-of-the-Art Report at
  EUROGRAPHICS 2023
- **Journal**: Computer Graphics Forum, 2023
- **Summary**: 3D reconstruction of deformable (or non-rigid) scenes from a set of monocular 2D image observations is a long-standing and actively researched area of computer vision and graphics. It is an ill-posed inverse problem, since -- without additional prior assumptions -- it permits infinitely many solutions leading to accurate projection to the input 2D images. Non-rigid reconstruction is a foundational building block for downstream applications like robotics, AR/VR, or visual content creation. The key advantage of using monocular cameras is their omnipresence and availability to the end users as well as their ease of use compared to more sophisticated camera set-ups such as stereo or multi-view systems. This survey focuses on state-of-the-art methods for dense non-rigid 3D reconstruction of various deformable objects and composite scenes from monocular videos or sets of monocular views. It reviews the fundamentals of 3D reconstruction and deformation modeling from 2D image observations. We then start from general methods -- that handle arbitrary scenes and make only a few prior assumptions -- and proceed towards techniques making stronger assumptions about the observed objects and types of deformations (e.g. human faces, bodies, hands, and animals). A significant part of this STAR is also devoted to classification and a high-level comparison of the methods, as well as an overview of the datasets for training and evaluation of the discussed techniques. We conclude by discussing open challenges in the field and the social aspects associated with the usage of the reviewed methods.



### Do Pre-trained Models Benefit Equally in Continual Learning?
- **Arxiv ID**: http://arxiv.org/abs/2210.15701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15701v1)
- **Published**: 2022-10-27 18:03:37+00:00
- **Updated**: 2022-10-27 18:03:37+00:00
- **Authors**: Kuan-Ying Lee, Yuanyi Zhong, Yu-Xiong Wang
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Existing work on continual learning (CL) is primarily devoted to developing algorithms for models trained from scratch. Despite their encouraging performance on contrived benchmarks, these algorithms show dramatic performance drops in real-world scenarios. Therefore, this paper advocates the systematic introduction of pre-training to CL, which is a general recipe for transferring knowledge to downstream tasks but is substantially missing in the CL community. Our investigation reveals the multifaceted complexity of exploiting pre-trained models for CL, along three different axes, pre-trained models, CL algorithms, and CL scenarios. Perhaps most intriguingly, improvements in CL algorithms from pre-training are very inconsistent an underperforming algorithm could become competitive and even state-of-the-art when all algorithms start from a pre-trained model. This indicates that the current paradigm, where all CL methods are compared in from-scratch training, is not well reflective of the true CL objective and desired progress. In addition, we make several other important observations, including that CL algorithms that exert less regularization benefit more from a pre-trained model; and that a stronger pre-trained model such as CLIP does not guarantee a better improvement. Based on these findings, we introduce a simple yet effective baseline that employs minimum regularization and leverages the more beneficial pre-trained model, coupled with a two-stage training pipeline. We recommend including this strong baseline in the future development of CL algorithms, due to its demonstrated state-of-the-art performance.



### PatchRot: A Self-Supervised Technique for Training Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.15722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15722v1)
- **Published**: 2022-10-27 18:55:12+00:00
- **Updated**: 2022-10-27 18:55:12+00:00
- **Authors**: Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara, Baoxin Li
- **Comment**: NeurIPS Workshop on Vision Transformers: Theory and Applications
  (VTTA)
- **Journal**: None
- **Summary**: Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.



### Spatio-temporal predictive tasks for abnormal event detection in videos
- **Arxiv ID**: http://arxiv.org/abs/2210.15741v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15741v2)
- **Published**: 2022-10-27 19:45:12+00:00
- **Updated**: 2023-04-23 16:35:35+00:00
- **Authors**: Yassine Naji, Aleksandr Setkov, Anglique Loesch, Michle Gouiffs, Romaric Audigier
- **Comment**: Accepted at the 18th IEEE International Conference on Advanced Video
  and Signal Based Surveillance (AVSS), 2022
- **Journal**: None
- **Summary**: Abnormal event detection in videos is a challenging problem, partly due to the multiplicity of abnormal patterns and the lack of their corresponding annotations. In this paper, we propose new constrained pretext tasks to learn object level normality patterns. Our approach consists in learning a mapping between down-scaled visual queries and their corresponding normal appearance and motion characteristics at the original resolution. The proposed tasks are more challenging than reconstruction and future frame prediction tasks which are widely used in the literature, since our model learns to jointly predict spatial and temporal features rather than reconstructing them. We believe that more constrained pretext tasks induce a better learning of normality patterns. Experiments on several benchmark datasets demonstrate the effectiveness of our approach to localize and track anomalies as it outperforms or reaches the current state-of-the-art on spatio-temporal evaluation metrics.



### Towards Improving Workers' Safety and Progress Monitoring of Construction Sites Through Construction Site Understanding
- **Arxiv ID**: http://arxiv.org/abs/2210.15760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15760v1)
- **Published**: 2022-10-27 20:33:46+00:00
- **Updated**: 2022-10-27 20:33:46+00:00
- **Authors**: Mahdi Bonyani, Maryam Soleymani
- **Comment**: None
- **Journal**: None
- **Summary**: An important component of computer vision research is object detection. In recent years, there has been tremendous progress in the study of construction site images. However, there are obvious problems in construction object detection, including complex backgrounds, varying-sized objects, and poor imaging quality. In the state-of-the-art approaches, elaborate attention mechanisms are developed to handle space-time features, but rarely address the importance of channel-wise feature adjustments. We propose a lightweight Optimized Positioning (OP) module to improve channel relation based on global feature affinity association, which can be used to determine the Optimized weights adaptively for each channel. OP first computes the intermediate optimized position by comparing each channel with the remaining channels for a given set of feature maps. A weighted aggregation of all the channels will then be used to represent each channel. The OP-Net module is a general deep neural network module that can be plugged into any deep neural network. Algorithms that utilize deep learning have demonstrated their ability to identify a wide range of objects from images nearly in real time. Machine intelligence can potentially benefit the construction industry by automatically analyzing productivity and monitoring safety using algorithms that are linked to construction images. The benefits of on-site automatic monitoring are immense when it comes to hazard prevention. Construction monitoring tasks can also be automated once construction objects have been correctly recognized. Object detection task in construction site images is experimented with extensively to demonstrate its efficacy and effectiveness. A benchmark test using SODA demonstrated that our OP-Net was capable of achieving new state-of-the-art performance in accuracy while maintaining a reasonable computational overhead.



### Fully-attentive and interpretable: vision and video vision transformers for pain detection
- **Arxiv ID**: http://arxiv.org/abs/2210.15769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15769v1)
- **Published**: 2022-10-27 21:01:40+00:00
- **Updated**: 2022-10-27 21:01:40+00:00
- **Authors**: Giacomo Fiorentini, Itir Onal Ertugrul, Albert Ali Salah
- **Comment**: 9 pages (12 with references), 10 figures, VTTA2022
- **Journal**: None
- **Summary**: Pain is a serious and costly issue globally, but to be treated, it must first be detected. Vision transformers are a top-performing architecture in computer vision, with little research on their use for pain detection. In this paper, we propose the first fully-attentive automated pain detection pipeline that achieves state-of-the-art performance on binary pain detection from facial expressions. The model is trained on the UNBC-McMaster dataset, after faces are 3D-registered and rotated to the canonical frontal view. In our experiments we identify important areas of the hyperparameter space and their interaction with vision and video vision transformers, obtaining 3 noteworthy models. We analyse the attention maps of one of our models, finding reasonable interpretations for its predictions. We also evaluate Mixup, an augmentation technique, and Sharpness-Aware Minimization, an optimizer, with no success. Our presented models, ViT-1 (F1 score 0.55 +- 0.15), ViViT-1 (F1 score 0.55 +- 0.13), and ViViT-2 (F1 score 0.49 +- 0.04), all outperform earlier works, showing the potential of vision transformers for pain detection. Code is available at https://github.com/IPDTFE/ViT-McMaster



### BI AVAN: Brain inspired Adversarial Visual Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2210.15790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15790v1)
- **Published**: 2022-10-27 22:20:36+00:00
- **Updated**: 2022-10-27 22:20:36+00:00
- **Authors**: Heng Huang, Lin Zhao, Xintao Hu, Haixing Dai, Lu Zhang, Dajiang Zhu, Tianming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention is a fundamental mechanism in the human brain, and it inspires the design of attention mechanisms in deep neural networks. However, most of the visual attention studies adopted eye-tracking data rather than the direct measurement of brain activity to characterize human visual attention. In addition, the adversarial relationship between the attention-related objects and attention-neglected background in the human visual system was not fully exploited. To bridge these gaps, we propose a novel brain-inspired adversarial visual attention network (BI-AVAN) to characterize human visual attention directly from functional brain activity. Our BI-AVAN model imitates the biased competition process between attention-related/neglected objects to identify and locate the visual objects in a movie frame the human brain focuses on in an unsupervised manner. We use independent eye-tracking data as ground truth for validation and experimental results show that our model achieves robust and promising results when inferring meaningful human visual attention and mapping the relationship between brain activities and visual stimuli. Our BI-AVAN model contributes to the emerging field of leveraging the brain's functional architecture to inspire and guide the model design in artificial intelligence (AI), e.g., deep neural networks.



### ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images
- **Arxiv ID**: http://arxiv.org/abs/2210.17415v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 62F15 (Primary) 68T45 (Secondary), G.3; I.5.1; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2210.17415v1)
- **Published**: 2022-10-27 22:35:24+00:00
- **Updated**: 2022-10-27 22:35:24+00:00
- **Authors**: Matthew D. Hoffman, Tuan Anh Le, Pavel Sountsov, Christopher Suter, Ben Lee, Vikash K. Mansinghka, Rif A. Saurous
- **Comment**: 18 pages, 18 figures, 1 table; submitted to the 26th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2023)
- **Journal**: None
- **Summary**: The problem of inferring object shape from a single 2D image is underconstrained. Prior knowledge about what objects are plausible can help, but even given such prior knowledge there may still be uncertainty about the shapes of occluded parts of objects. Recently, conditional neural radiance field (NeRF) models have been developed that can learn to infer good point estimates of 3D models from single 2D images. The problem of inferring uncertainty estimates for these models has received less attention. In this work, we propose probabilistic NeRF (ProbNeRF), a model and inference strategy for learning probabilistic generative models of 3D objects' shapes and appearances, and for doing posterior inference to recover those properties from 2D images. ProbNeRF is trained as a variational autoencoder, but at test time we use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2D images of an object (which may be partially occluded), ProbNeRF is able not only to accurately model the parts it sees, but also to propose realistic and diverse hypotheses about the parts it does not see. We show that key to the success of ProbNeRF are (i) a deterministic rendering scheme, (ii) an annealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and (iv) doing inference over a full set of NeRF weights, rather than just a low-dimensional code.



### Layout Aware Inpainting for Automated Furniture Removal in Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2210.15796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.15796v1)
- **Published**: 2022-10-27 23:26:21+00:00
- **Updated**: 2022-10-27 23:26:21+00:00
- **Authors**: Prakhar Kulshreshtha, Konstantinos-Nektarios Lianos, Brian Pugh, Salma Jiddi
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: We address the problem of detecting and erasing furniture from a wide angle photograph of a room. Inpainting large regions of an indoor scene often results in geometric inconsistencies of background elements within the inpaint mask. To address this problem, we utilize perceptual information (e.g. instance segmentation, and room layout) to produce a geometrically consistent empty version of a room. We share important details to make this system viable, such as per-plane inpainting, automatic rectification, and texture refinement. We provide detailed ablation along with qualitative examples, justifying our design choices. We show an application of our system by removing real furniture from a room and redecorating it with virtual furniture.



### Towards Reliable Zero Shot Classification in Self-Supervised Models with Conformal Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.15805v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15805v1)
- **Published**: 2022-10-27 23:52:14+00:00
- **Updated**: 2022-10-27 23:52:14+00:00
- **Authors**: Bhawesh Kumar, Anil Palepu, Rudraksh Tuwani, Andrew Beam
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Self-supervised models trained with a contrastive loss such as CLIP have shown to be very powerful in zero-shot classification settings. However, to be used as a zero-shot classifier these models require the user to provide new captions over a fixed set of labels at test time. In many settings, it is hard or impossible to know if a new query caption is compatible with the source captions used to train the model. We address these limitations by framing the zero-shot classification task as an outlier detection problem and develop a conformal prediction procedure to assess when a given test caption may be reliably used. On a real-world medical example, we show that our proposed conformal procedure improves the reliability of CLIP-style models in the zero-shot classification setting, and we provide an empirical analysis of the factors that may affect its performance.




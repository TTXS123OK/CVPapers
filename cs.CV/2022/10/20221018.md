# Arxiv Papers in cs.CV on 2022-10-18
### FPGA Hardware Acceleration for Feature-Based Relative Navigation Applications
- **Arxiv ID**: http://arxiv.org/abs/2210.09481v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AR, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09481v1)
- **Published**: 2022-10-18 00:01:57+00:00
- **Updated**: 2022-10-18 00:01:57+00:00
- **Authors**: Ramchander Rao Bhaskara, Manoranjan Majji
- **Comment**: Proceedings of 2022 Astrodynamics Specialist Conference, Charlotte,
  USA
- **Journal**: None
- **Summary**: Estimation of rigid transformation between two point clouds is a computationally challenging problem in vision-based relative navigation. Targeting a real-time navigation solution utilizing point-cloud and image registration algorithms, this paper develops high-performance avionics for power and resource constrained pose estimation framework. A Field-Programmable Gate Array (FPGA) based embedded architecture is developed to accelerate estimation of relative pose between the point-clouds, aided by image features that correspond to the individual point sets. At algorithmic level, the pose estimation method is an adaptation of Optimal Linear Attitude and Translation Estimator (OLTAE) for relative attitude and translation estimation. At the architecture level, the proposed embedded solution is a hardware/software co-design that evaluates the OLTAE computations on the bare-metal hardware for high-speed state estimation. The finite precision FPGA evaluation of the OLTAE algorithm is compared with a double-precision evaluation on MATLAB for performance analysis and error quantification. Implementation results of the proposed finite-precision OLTAE accelerator demonstrate the high-performance compute capabilities of the FPGA-based pose estimation while offering relative numerical errors below 7%.



### Semi-Supervised Domain Adaptation with Auto-Encoder via Simultaneous Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.09486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09486v1)
- **Published**: 2022-10-18 00:10:11+00:00
- **Updated**: 2022-10-18 00:10:11+00:00
- **Authors**: Md Mahmudur Rahman, Rameswar Panda, Mohammad Arif Ul Alam
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new semi-supervised domain adaptation framework that combines a novel auto-encoder-based domain adaptation model with a simultaneous learning scheme providing stable improvements over state-of-the-art domain adaptation models. Our framework holds strong distribution matching property by training both source and target auto-encoders using a novel simultaneous learning scheme on a single graph with an optimally modified MMD loss objective function. Additionally, we design a semi-supervised classification approach by transferring the aligned domain invariant feature spaces from source domain to the target domain. We evaluate on three datasets and show proof that our framework can effectively solve both fragile convergence (adversarial) and weak distribution matching problems between source and target feature space (discrepancy) with a high `speed' of adaptation requiring a very low number of iterations.



### 5th Place Solution to Kaggle Google Universal Image Embedding Competition
- **Arxiv ID**: http://arxiv.org/abs/2210.09495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09495v1)
- **Published**: 2022-10-18 00:34:09+00:00
- **Updated**: 2022-10-18 00:34:09+00:00
- **Authors**: Noriaki Ota, Shingo Yokoi, Shinsuke Yamaoka
- **Comment**: 3 pages, 1 figures
- **Journal**: None
- **Summary**: In this paper, we present our solution, which placed 5th in the kaggle Google Universal Image Embedding Competition in 2022. We use the ViT-H visual encoder of CLIP from the openclip repository as a backbone and train a head model composed of BatchNormalization and Linear layers using ArcFace. The dataset used was a subset of products10K, GLDv2, GPR1200, and Food101. And applying TTA for part of images also improves the score. With this method, we achieve a score of 0.684 on the public and 0.688 on the private leaderboard. Our code is available. https://github.com/riron1206/kaggle-Google-Universal-Image-Embedding-Competition-5th-Place-Solution



### RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction
- **Arxiv ID**: http://arxiv.org/abs/2210.09309v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09309v4)
- **Published**: 2022-10-18 00:55:37+00:00
- **Updated**: 2023-08-01 17:20:28+00:00
- **Authors**: Liang Jin, Shixuan Gu, Donglai Wei, Jason Ken Adhinarta, Kaiming Kuang, Yongjie Jessica Zhang, Hanspeter Pfister, Bingbing Ni, Jiancheng Yang, Ming Li
- **Comment**: 10 pages, 6 figures, journal
- **Journal**: None
- **Summary**: Automatic rib labeling and anatomical centerline extraction are common prerequisites for various clinical applications. Prior studies either use in-house datasets that are inaccessible to communities, or focus on rib segmentation that neglects the clinical significance of rib labeling. To address these issues, we extend our prior dataset (RibSeg) on the binary rib segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT scans (15,466 individual ribs in total) and annotations manually inspected by experts for rib labeling and anatomical centerline extraction. Based on the RibSeg v2, we develop a pipeline including deep learning-based methods for rib labeling, and a skeletonization-based method for centerline extraction. To improve computational efficiency, we propose a sparse point cloud representation of CT scans and compare it with standard dense voxel grids. Moreover, we design and analyze evaluation metrics to address the key challenges of each task. Our dataset, code, and model are available online to facilitate open research at https://github.com/M3DV/RibSeg



### Deep Data Augmentation for Weed Recognition Enhancement: A Diffusion Probabilistic Model and Transfer Learning Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2210.09509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09509v1)
- **Published**: 2022-10-18 01:00:25+00:00
- **Updated**: 2022-10-18 01:00:25+00:00
- **Authors**: Dong Chen, Xinda Qi, Yu Zheng, Yuzhen Lu, Zhaojian Li
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: Weed management plays an important role in many modern agricultural applications. Conventional weed control methods mainly rely on chemical herbicides or hand weeding, which are often cost-ineffective, environmentally unfriendly, or even posing a threat to food safety and human health. Recently, automated/robotic weeding using machine vision systems has seen increased research attention with its potential for precise and individualized weed treatment. However, dedicated, large-scale, and labeled weed image datasets are required to develop robust and effective weed identification systems but they are often difficult and expensive to obtain. To address this issue, data augmentation approaches, such as generative adversarial networks (GANs), have been explored to generate highly realistic images for agricultural applications. Yet, despite some progress, those approaches are often complicated to train or have difficulties preserving fine details in images. In this paper, we present the first work of applying diffusion probabilistic models (also known as diffusion models) to generate high-quality synthetic weed images based on transfer learning. Comprehensive experimental results show that the developed approach consistently outperforms several state-of-the-art GAN models, representing the best trade-off between sample fidelity and diversity and highest FID score on a common weed dataset, CottonWeedID15. In addition, the expanding dataset with synthetic weed images can apparently boost model performance on four deep learning (DL) models for the weed classification tasks. Furthermore, the DL models trained on CottonWeedID15 dataset with only 10% of real images and 90% of synthetic weed images achieve a testing accuracy of over 94%, showing high-quality of the generated weed samples. The codes of this study are made publicly available at https://github.com/DongChen06/DMWeeds.



### Using Language to Extend to Unseen Domains
- **Arxiv ID**: http://arxiv.org/abs/2210.09520v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09520v6)
- **Published**: 2022-10-18 01:14:02+00:00
- **Updated**: 2023-04-29 18:00:13+00:00
- **Authors**: Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E. Gonzalez, Aditi Raghunathan, Anja Rohrbach
- **Comment**: None
- **Journal**: None
- **Summary**: It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply verbalizing the training domain (e.g. "photos of birds") as well as domains we want to extend to but do not have data for (e.g. "paintings of birds") can improve robustness. Using a multimodal model with a joint image and language embedding space, our method LADS learns a transformation of the image embeddings from the training domain to each unseen test domain, while preserving task relevant information. Without using any images from the unseen test domain, we show that over the extended domain containing both training and unseen test domains, LADS outperforms standard fine-tuning and ensemble approaches over a suite of four benchmarks targeting domain adaptation and dataset bias.



### Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.09549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 94A08, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2210.09549v1)
- **Published**: 2022-10-18 02:50:34+00:00
- **Updated**: 2022-10-18 02:50:34+00:00
- **Authors**: Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, Quan Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Google's Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e., MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.



### A novel statistical methodology for quantifying the spatial arrangements of axons in peripheral nerves
- **Arxiv ID**: http://arxiv.org/abs/2210.09554v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2210.09554v1)
- **Published**: 2022-10-18 03:04:11+00:00
- **Updated**: 2022-10-18 03:04:11+00:00
- **Authors**: Abida Sanjana Shemonti, Emanuele Plebani, Natalia P. Biscola, Deborah M. Jaffey, Leif A. Havton, Janet R. Keast, Alex Pothen, M. Murat Dundar, Terry L. Powley, Bartek Rajwa
- **Comment**: 10 figures
- **Journal**: None
- **Summary**: A thorough understanding of the neuroanatomy of peripheral nerves is required for a better insight into their function and the development of neuromodulation tools and strategies. In biophysical modeling, it is commonly assumed that the complex spatial arrangement of myelinated and unmyelinated axons in peripheral nerves is random, however, in reality the axonal organization is inhomogeneous and anisotropic. Present quantitative neuroanatomy methods analyze peripheral nerves in terms of the number of axons and the morphometric characteristics of the axons, such as area and diameter. In this study, we employed spatial statistics and point process models to describe the spatial arrangement of axons and Sinkhorn distances to compute the similarities between these arrangements (in terms of first- and second-order statistics) in various vagus and pelvic nerve cross-sections. We utilized high-resolution TEM images that have been segmented using a custom-built high-throughput deep learning system based on a highly modified U-Net architecture. Our findings show a novel and innovative approach to quantifying similarities between spatial point patterns using metrics derived from the solution to the optimal transport problem. We also present a generalizable pipeline for quantitative analysis of peripheral nerve architecture. Our data demonstrate differences between male- and female-originating samples and similarities between the pelvic and abdominal vagus nerves.



### Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity
- **Arxiv ID**: http://arxiv.org/abs/2210.09558v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09558v1)
- **Published**: 2022-10-18 03:25:00+00:00
- **Updated**: 2022-10-18 03:25:00+00:00
- **Authors**: Gitaek Kwon, Eunjin Kim, Sunho Kim, Seongwon Bak, Minsung Kim, Jaeyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical coherence tomography angiography (UW-OCTA) has been used in clinical practices to detect signs of early DR. However, developing a deep learning-based DR analysis system using UW-OCTA images is not trivial due to the difficulty of data collection and the absence of public datasets. By realistic constraints, a model trained on small datasets may obtain sub-par performance. Therefore, to help ophthalmologists be less confused about models' incorrect decisions, the models should be robust even in data scarcity settings. To address the above practical challenging, we present a comprehensive empirical study for DR analysis tasks, including lesion segmentation, image quality assessment, and DR grading. For each task, we introduce a robust training scheme by leveraging ensemble learning, data augmentation, and semi-supervised learning. Furthermore, we propose reliable pseudo labeling that excludes uncertain pseudo-labels based on the model's confidence scores to reduce the negative effect of noisy pseudo-labels. By exploiting the proposed approaches, we achieved 1st place in the Diabetic Retinopathy Analysis Challenge.



### FedForgery: Generalized Face Forgery Detection with Residual Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.09563v2
- **DOI**: 10.1109/TIFS.2023.3293951
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09563v2)
- **Published**: 2022-10-18 03:32:18+00:00
- **Updated**: 2023-07-21 10:01:25+00:00
- **Authors**: Decheng Liu, Zhan Dang, Chunlei Peng, Yu Zheng, Shuang Li, Nannan Wang, Xinbo Gao
- **Comment**: The code is available at https://github.com/GANG370/FedForgery. The
  paper has been accepted in the IEEE Transactions on Information Forensics &
  Security
- **Journal**: None
- **Summary**: With the continuous development of deep learning in the field of image generation models, a large number of vivid forged faces have been generated and spread on the Internet. These high-authenticity artifacts could grow into a threat to society security. Existing face forgery detection methods directly utilize the obtained public shared or centralized data for training but ignore the personal privacy and security issues when personal data couldn't be centralizedly shared in real-world scenarios. Additionally, different distributions caused by diverse artifact types would further bring adverse influences on the forgery detection task. To solve the mentioned problems, the paper proposes a novel generalized residual Federated learning for face Forgery detection (FedForgery). The designed variational autoencoder aims to learn robust discriminative residual feature maps to detect forgery faces (with diverse or even unknown artifact types). Furthermore, the general federated learning strategy is introduced to construct distributed detection model trained collaboratively with multiple local decentralized devices, which could further boost the representation generalization. Experiments conducted on publicly available face forgery detection datasets prove the superior performance of the proposed FedForgery. The designed novel generalized face forgery detection protocols and source code would be publicly available.



### Spatio-Temporal-based Context Fusion for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.09572v1
- **DOI**: 10.1016/S0550-3213(01)00405-9
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09572v1)
- **Published**: 2022-10-18 04:07:10+00:00
- **Updated**: 2022-10-18 04:07:10+00:00
- **Authors**: Chao Hu, Weibin Qiu, Weijie Wu, Liqiang Zhu
- **Comment**: None
- **Journal**: Machine Intelligence Research 2022
- **Summary**: Video anomaly detection aims to discover abnormal events in videos, and the principal objects are target objects such as people and vehicles. Each target in the video data has rich spatio-temporal context information. Most existing methods only focus on the temporal context, ignoring the role of the spatial context in anomaly detection. The spatial context information represents the relationship between the detection target and surrounding targets. Anomaly detection makes a lot of sense. To this end, a video anomaly detection algorithm based on target spatio-temporal context fusion is proposed. Firstly, the target in the video frame is extracted through the target detection network to reduce background interference. Then the optical flow map of two adjacent frames is calculated. Motion features are used multiple targets in the video frame to construct spatial context simultaneously, re-encoding the target appearance and motion features, and finally reconstructing the above features through the spatio-temporal dual-stream network, and using the reconstruction error to represent the abnormal score. The algorithm achieves frame-level AUCs of 98.5% and 86.3% on the UCSDped2 and Avenue datasets, respectively. On the UCSDped2 dataset, the spatio-temporal dual-stream network improves frames by 5.1% and 0.3%, respectively, compared to the temporal and spatial stream networks. After using spatial context encoding, the frame-level AUC is enhanced by 1%, which verifies the method's effectiveness.



### ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design
- **Arxiv ID**: http://arxiv.org/abs/2210.09573v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09573v2)
- **Published**: 2022-10-18 04:07:23+00:00
- **Updated**: 2022-12-11 00:23:07+00:00
- **Authors**: Haoran You, Zhanyi Sun, Huihong Shi, Zhongzhi Yu, Yang Zhao, Yongan Zhang, Chaojian Li, Baopu Li, Yingyan Lin
- **Comment**: Accepted to HPCA 2023
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have achieved state-of-the-art performance on various vision tasks. However, ViTs' self-attention module is still arguably a major bottleneck, limiting their achievable hardware efficiency. Meanwhile, existing accelerators dedicated to NLP Transformers are not optimal for ViTs. This is because there is a large difference between ViTs and NLP Transformers: ViTs have a relatively fixed number of input tokens, whose attention maps can be pruned by up to 90% even with fixed sparse patterns; while NLP Transformers need to handle input sequences of varying numbers of tokens and rely on on-the-fly predictions of dynamic sparse attention patterns for each input to achieve a decent sparsity (e.g., >=50%). To this end, we propose a dedicated algorithm and accelerator co-design framework dubbed ViTCoD for accelerating ViTs. Specifically, on the algorithm level, ViTCoD prunes and polarizes the attention maps to have either denser or sparser fixed patterns for regularizing two levels of workloads without hurting the accuracy, largely reducing the attention computations while leaving room for alleviating the remaining dominant data movements; on top of that, we further integrate a lightweight and learnable auto-encoder module to enable trading the dominant high-cost data movements for lower-cost computations. On the hardware level, we develop a dedicated accelerator to simultaneously coordinate the enforced denser/sparser workloads and encoder/decoder engines for boosted hardware utilization. Extensive experiments and ablation studies validate that ViTCoD largely reduces the dominant data movement costs, achieving speedups of up to 235.3x, 142.9x, 86.0x, 10.1x, and 6.8x over general computing platforms CPUs, EdgeGPUs, GPUs, and prior-art Transformer accelerators SpAtten and Sanger under an attention sparsity of 90%, respectively.



### Depth Contrast: Self-Supervised Pretraining on 3DPM Images for Mining Material Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.10633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10633v1)
- **Published**: 2022-10-18 04:17:16+00:00
- **Updated**: 2022-10-18 04:17:16+00:00
- **Authors**: Prakash Chandra Chhipa, Richa Upadhyay, Rajkumar Saini, Lars Lindqvist, Richard Nordenskjold, Seiichi Uchida, Marcus Liwicki
- **Comment**: Accepted to CVF European Conference on Computer Vision Workshop(ECCVW
  2022)
- **Journal**: None
- **Summary**: This work presents a novel self-supervised representation learning method to learn efficient representations without labels on images from a 3DPM sensor (3-Dimensional Particle Measurement; estimates the particle size distribution of material) utilizing RGB images and depth maps of mining material on the conveyor belt. Human annotations for material categories on sensor-generated data are scarce and cost-intensive. Currently, representation learning without human annotations remains unexplored for mining materials and does not leverage on utilization of sensor-generated data. The proposed method, Depth Contrast, enables self-supervised learning of representations without labels on the 3DPM dataset by exploiting depth maps and inductive transfer. The proposed method outperforms material classification over ImageNet transfer learning performance in fully supervised learning settings and achieves an F1 score of 0.73. Further, The proposed method yields an F1 score of 0.65 with an 11% improvement over ImageNet transfer learning performance in a semi-supervised setting when only 20% of labels are used in fine-tuning. Finally, the Proposed method showcases improved performance generalization on linear evaluation. The implementation of proposed method is available on GitHub.



### Perceptual Multi-Exposure Fusion
- **Arxiv ID**: http://arxiv.org/abs/2210.09604v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09604v2)
- **Published**: 2022-10-18 05:34:58+00:00
- **Updated**: 2022-10-19 06:58:48+00:00
- **Authors**: Xiaoning Liu
- **Comment**: The current version is our previous work rejected by IEEE TMM. I'm
  very sorry and I want to withdraw this submitted version. I will resubmit it
  when I improve it in the future. The version involves some ideas we are doing
- **Journal**: None
- **Summary**: As an ever-increasing demand for high dynamic range (HDR) scene shooting, multi-exposure image fusion (MEF) technology has abounded. In recent years, multi-scale exposure fusion approaches based on detail-enhancement have led the way for improvement in highlight and shadow details. Most of such methods, however, are too computationally expensive to be deployed on mobile devices. This paper presents a perceptual multi-exposure fusion method that not just ensures fine shadow/highlight details but with lower complexity than detailenhanced methods. We analyze the potential defects of three classical exposure measures in lieu of using detail-enhancement component and improve two of them, namely adaptive Wellexposedness (AWE) and the gradient of color images (3-D gradient). AWE designed in YCbCr color space considers the difference between varying exposure images. 3-D gradient is employed to extract fine details. We build a large-scale multiexposure benchmark dataset suitable for static scenes, which contains 167 image sequences all told. Experiments on the constructed dataset demonstrate that the proposed method exceeds existing eight state-of-the-art approaches in terms of visually and MEF-SSIM value. Moreover, our approach can achieve a better improvement for current image enhancement techniques, ensuring fine detail in bright light.



### Degradation-invariant Enhancement of Fundus Images via Pyramid Constraint Network
- **Arxiv ID**: http://arxiv.org/abs/2210.09606v1
- **DOI**: 10.1007/978-3-031-16434-7_49
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09606v1)
- **Published**: 2022-10-18 05:45:13+00:00
- **Updated**: 2022-10-18 05:45:13+00:00
- **Authors**: Haofeng Liu, Heng Li, Huazhu Fu, Ruoxiu Xiao, Yunshu Gao, Yan Hu, Jiang Liu
- **Comment**: None
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention. (2022) 507-516
- **Summary**: As an economical and efficient fundus imaging modality, retinal fundus images have been widely adopted in clinical fundus examination. Unfortunately, fundus images often suffer from quality degradation caused by imaging interferences, leading to misdiagnosis. Despite impressive enhancement performances that state-of-the-art methods have achieved, challenges remain in clinical scenarios. For boosting the clinical deployment of fundus image enhancement, this paper proposes the pyramid constraint to develop a degradation-invariant enhancement network (PCE-Net), which mitigates the demand for clinical data and stably enhances unknown data. Firstly, high-quality images are randomly degraded to form sequences of low-quality ones sharing the same content (SeqLCs). Then individual low-quality images are decomposed to Laplacian pyramid features (LPF) as the multi-level input for the enhancement. Subsequently, a feature pyramid constraint (FPC) for the sequence is introduced to enforce the PCE-Net to learn a degradation-invariant model. Extensive experiments have been conducted under the evaluation metrics of enhancement and segmentation. The effectiveness of the PCE-Net was demonstrated in comparison with state-of-the-art methods and the ablation study. The source code of this study is publicly available at https://github.com/HeverLaw/PCENet-Image-Enhancement.



### Homogeneous Multi-modal Feature Fusion and Interaction for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.09615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09615v1)
- **Published**: 2022-10-18 06:15:56+00:00
- **Updated**: 2022-10-18 06:15:56+00:00
- **Authors**: Xin Li, Botian Shi, Yuenan Hou, Xingjiao Wu, Tianlong Ma, Yikang Li, Liang He
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Multi-modal 3D object detection has been an active research topic in autonomous driving. Nevertheless, it is non-trivial to explore the cross-modal feature fusion between sparse 3D points and dense 2D pixels. Recent approaches either fuse the image features with the point cloud features that are projected onto the 2D image plane or combine the sparse point cloud with dense image pixels. These fusion approaches often suffer from severe information loss, thus causing sub-optimal performance. To address these problems, we construct the homogeneous structure between the point cloud and images to avoid projective information loss by transforming the camera features into the LiDAR 3D space. In this paper, we propose a homogeneous multi-modal feature fusion and interaction method (HMFI) for 3D object detection. Specifically, we first design an image voxel lifter module (IVLM) to lift 2D image features into the 3D space and generate homogeneous image voxel features. Then, we fuse the voxelized point cloud features with the image features from different regions by introducing the self-attention based query fusion mechanism (QFM). Next, we propose a voxel feature interaction module (VFIM) to enforce the consistency of semantic information from identical objects in the homogeneous point cloud and image voxel representations, which can provide object-level alignment guidance for cross-modal feature fusion and strengthen the discriminative ability in complex backgrounds. We conduct extensive experiments on the KITTI and Waymo Open Dataset, and the proposed HMFI achieves better performance compared with the state-of-the-art multi-modal methods. Particularly, for the 3D detection of cyclist on the KITTI benchmark, HMFI surpasses all the published algorithms by a large margin.



### Object Recognition in Different Lighting Conditions at Various Angles by Deep Learning Method
- **Arxiv ID**: http://arxiv.org/abs/2210.09618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09618v1)
- **Published**: 2022-10-18 06:23:26+00:00
- **Updated**: 2022-10-18 06:23:26+00:00
- **Authors**: Imran Khan Mirani, Chen Tianhua, Malak Abid Ali Khan, Syed Muhammad Aamir, Waseef Menhaj
- **Comment**: None
- **Journal**: None
- **Summary**: Existing computer vision and object detection methods strongly rely on neural networks and deep learning. This active research area is used for applications such as autonomous driving, aerial photography, protection, and monitoring. Futuristic object detection methods rely on rectangular, boundary boxes drawn over an object to accurately locate its location. The modern object recognition algorithms, however, are vulnerable to multiple factors, such as illumination, occlusion, viewing angle, or camera rotation as well as cost. Therefore, deep learning-based object recognition will significantly increase the recognition speed and compatible external interference. In this study, we use convolutional neural networks (CNN) to recognize items, the neural networks have the advantages of end-to-end, sparse relation, and sharing weights. This article aims to classify the name of the various object based on the position of an object's detected box. Instead, under different distances, we can get recognition results with different confidence. Through this study, we find that this model's accuracy through recognition is mainly influenced by the proportion of objects and the number of samples. When we have a small proportion of an object on camera, then we get higher recognition accuracy; if we have a much small number of samples, we can get greater accuracy in recognition. The epidemic has a great impact on the world economy where designing a cheaper object recognition system is the need of time.



### Analysis of Master Vein Attacks on Finger Vein Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2210.10667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10667v1)
- **Published**: 2022-10-18 06:36:59+00:00
- **Updated**: 2022-10-18 06:36:59+00:00
- **Authors**: Huy H. Nguyen, Trung-Nghia Le, Junichi Yamagishi, Isao Echizen
- **Comment**: Accepted to be Published in Proceedings of the IEEE/CVF Winter
  Conference on Applications of Computer Vision (WACV) 2023
- **Journal**: None
- **Summary**: Finger vein recognition (FVR) systems have been commercially used, especially in ATMs, for customer verification. Thus, it is essential to measure their robustness against various attack methods, especially when a hand-crafted FVR system is used without any countermeasure methods. In this paper, we are the first in the literature to introduce master vein attacks in which we craft a vein-looking image so that it can falsely match with as many identities as possible by the FVR systems. We present two methods for generating master veins for use in attacking these systems. The first uses an adaptation of the latent variable evolution algorithm with a proposed generative model (a multi-stage combination of beta-VAE and WGAN-GP models). The second uses an adversarial machine learning attack method to attack a strong surrogate CNN-based recognition system. The two methods can be easily combined to boost their attack ability. Experimental results demonstrated that the proposed methods alone and together achieved false acceptance rates up to 73.29% and 88.79%, respectively, against Miura's hand-crafted FVR system. We also point out that Miura's system is easily compromised by non-vein-looking samples generated by a WGAN-GP model with false acceptance rates up to 94.21%. The results raise the alarm about the robustness of such systems and suggest that master vein attacks should be considered an important security measure.



### 1st Place Solutions for the UVO Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2210.09629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09629v1)
- **Published**: 2022-10-18 06:54:37+00:00
- **Updated**: 2022-10-18 06:54:37+00:00
- **Authors**: Jiajun Zhang, Boyu Chen, Zhilong Ji, Jinfeng Bai, Zonghai Hu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the approach we have taken in the challenge. We still adopted the two-stage scheme same as the last champion, that is, detection first and segmentation followed. We trained more powerful detector and segmentor separately. Besides, we also perform pseudo-label training on the test set, based on student-teacher framework and end-to-end transformer based object detection. The method ranks first on the 2nd Unidentified Video Objects (UVO) challenge, achieving AR@100 of 46.8, 64.7 and 32.2 in the limited data frame track, unlimited data frame track and video track respectively.



### Improving GANs with a Feature Cycling Generator
- **Arxiv ID**: http://arxiv.org/abs/2210.09638v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.09638v2)
- **Published**: 2022-10-18 07:14:07+00:00
- **Updated**: 2023-02-17 15:05:25+00:00
- **Authors**: Seung Park, Yong-Goo Shin
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs), built with a generator and discriminator, significantly have advanced image generation. Typically, existing papers build their generators by stacking up multiple residual blocks since it makes ease the training of generators. However, some recent papers commented on the limitation of the residual block and proposed a new architectural unit that improves the GANs performance. Following this trend, this paper presents a novel unit, called feature cycling block (FCB), which achieves impressive results in the image generation task. Specifically, the FCB has two branches: one is a memory branch and the other is an image branch. The memory branch keeps meaningful information at each stage of the generator, whereas the image branch takes some useful features from the memory branch to produce a high-quality image. To show the capability of the proposed method, we conducted extensive experiments using various datasets including CIFAR-10, CIFAR-100, FFHQ, AFHQ, and subsets of LSUN. Experimental results demonstrate the substantial superiority of our approach over the baseline without incurring any objective functions or training skills. For instance, the proposed method improves Frechet inception distance (FID) of StyleGAN2 from 4.89 to 3.72 on the FFHQ dataset and from 6.64 to 5.57 on the LSUN Bed dataset. We believe that the pioneering attempt presented in this paper could inspire the community with better-designed generator architecture and with training objectives or skills compatible with the proposed method.



### Improving Adversarial Robustness by Contrastive Guided Diffusion Process
- **Arxiv ID**: http://arxiv.org/abs/2210.09643v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09643v2)
- **Published**: 2022-10-18 07:20:53+00:00
- **Updated**: 2023-07-04 12:08:26+00:00
- **Authors**: Yidong Ouyang, Liyan Xie, Guang Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic data generation has become an emerging tool to help improve the adversarial robustness in classification tasks since robust learning requires a significantly larger amount of training samples compared with standard classification tasks. Among various deep generative models, the diffusion model has been shown to produce high-quality synthetic images and has achieved good performance in improving the adversarial robustness. However, diffusion-type methods are typically slow in data generation as compared with other generative models. Although different acceleration techniques have been proposed recently, it is also of great importance to study how to improve the sample efficiency of generated data for the downstream task. In this paper, we first analyze the optimality condition of synthetic distribution for achieving non-trivial robust accuracy. We show that enhancing the distinguishability among the generated data is critical for improving adversarial robustness. Thus, we propose the Contrastive-Guided Diffusion Process (Contrastive-DP), which adopts the contrastive loss to guide the diffusion model in data generation. We verify our theoretical results using simulations and demonstrate the good performance of Contrastive-DP on image datasets.



### WaGI : Wavelet-based GAN Inversion for Preserving High-frequency Image Details
- **Arxiv ID**: http://arxiv.org/abs/2210.09655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09655v1)
- **Published**: 2022-10-18 07:48:59+00:00
- **Updated**: 2022-10-18 07:48:59+00:00
- **Authors**: Seung-Jun Moon, Chaewon Kim, Gyeong-Moon Park
- **Comment**: None
- **Journal**: None
- **Summary**: Recent GAN inversion models focus on preserving image-specific details through various methods, e.g., generator tuning or feature mixing. While those are helpful for preserving details compared to a naiive low-rate latent inversion, they still fail to maintain high-frequency features precisely. In this paper, we point out that the existing GAN inversion models have inherent limitations in both structural and training aspects, which preclude the delicate reconstruction of high-frequency features. Especially, we prove that the widely-used loss term in GAN inversion, i.e., L2, is biased to reconstruct low-frequency features mainly. To overcome this problem, we propose a novel GAN inversion model, coined WaGI, which enables to handle high-frequency features explicitly, by using a novel wavelet-based loss term and a newly proposed wavelet fusion scheme. To the best of our knowledge, WaGI is the first attempt to interpret GAN inversion in the frequency domain. We demonstrate that WaGI shows outstanding results on both inversion and editing, compared to the existing state-of-the-art GAN inversion models. Especially, WaGI robustly preserves high-frequency features of images even in the editing scenario. We will release our code with the pre-trained model after the review.



### Hierarchical Normalization for Robust Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.09670v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09670v1)
- **Published**: 2022-10-18 08:18:29+00:00
- **Updated**: 2022-10-18 08:18:29+00:00
- **Authors**: Chi Zhang, Wei Yin, Zhibin Wang, Gang Yu, Bin Fu, Chunhua Shen
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper, we address monocular depth estimation with deep neural networks. To enable training of deep monocular estimation models with various sources of datasets, state-of-the-art methods adopt image-level normalization strategies to generate affine-invariant depth representations. However, learning with image-level normalization mainly emphasizes the relations of pixel representations with the global statistic in the images, such as the structure of the scene, while the fine-grained depth difference may be overlooked. In this paper, we propose a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions. Compared with previous normalization strategies applied only at the holistic image level, the proposed hierarchical normalization can effectively preserve the fine-grained details and improve accuracy. We present two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain, respectively. Our extensive experiments show that the proposed normalization strategy remarkably outperforms previous normalization methods, and we set new state-of-the-art on five zero-shot transfer benchmark datasets.



### Virtual Reality via Object Pose Estimation and Active Learning: Realizing Telepresence Robots with Aerial Manipulation Capabilities
- **Arxiv ID**: http://arxiv.org/abs/2210.09678v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09678v2)
- **Published**: 2022-10-18 08:42:30+00:00
- **Updated**: 2023-02-10 11:04:02+00:00
- **Authors**: Jongseok Lee, Ribin Balachandran, Konstantin Kondak, Andre Coelho, Marco De Stefano, Matthias Humt, Jianxiang Feng, Tamim Asfour, Rudolph Triebel
- **Comment**: Accepted to Field Robotics
- **Journal**: None
- **Summary**: This article presents a novel telepresence system for advancing aerial manipulation in dynamic and unstructured environments. The proposed system not only features a haptic device, but also a virtual reality (VR) interface that provides real-time 3D displays of the robot's workspace as well as a haptic guidance to its remotely located operator. To realize this, multiple sensors namely a LiDAR, cameras and IMUs are utilized. For processing of the acquired sensory data, pose estimation pipelines are devised for industrial objects of both known and unknown geometries. We further propose an active learning pipeline in order to increase the sample efficiency of a pipeline component that relies on Deep Neural Networks (DNNs) based object detection. All these algorithms jointly address various challenges encountered during the execution of perception tasks in industrial scenarios. In the experiments, exhaustive ablation studies are provided to validate the proposed pipelines. Methodologically, these results commonly suggest how an awareness of the algorithms' own failures and uncertainty (`introspection') can be used tackle the encountered problems. Moreover, outdoor experiments are conducted to evaluate the effectiveness of the overall system in enhancing aerial manipulation capabilities. In particular, with flight campaigns over days and nights, from spring to winter, and with different users and locations, we demonstrate over 70 robust executions of pick-and-place, force application and peg-in-hole tasks with the DLR cable-Suspended Aerial Manipulator (SAM). As a result, we show the viability of the proposed system in future industrial applications.



### Weakly Supervised Learning with Automated Labels from Radiology Reports for Glioma Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.09698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09698v2)
- **Published**: 2022-10-18 09:15:27+00:00
- **Updated**: 2023-01-26 16:07:48+00:00
- **Authors**: Tommaso Di Noto, Meritxell Bach Cuadra, Chirine Atat, Eduardo Gamito Teiga, Monika Hegi, Andreas Hottinger, Patric Hagmann, Jonas Richiardi
- **Comment**: This work has been submitted as Original Paper to a Journal
- **Journal**: None
- **Summary**: Gliomas are the most frequent primary brain tumors in adults. Glioma change detection aims at finding the relevant parts of the image that change over time. Although Deep Learning (DL) shows promising performances in similar change detection tasks, the creation of large annotated datasets represents a major bottleneck for supervised DL applications in radiology. To overcome this, we propose a combined use of weak labels (imprecise, but fast-to-create annotations) and Transfer Learning (TL). Specifically, we explore inductive TL, where source and target domains are identical, but tasks are different due to a label shift: our target labels are created manually by three radiologists, whereas our source weak labels are generated automatically from radiology reports via NLP. We frame knowledge transfer as hyperparameter optimization, thus avoiding heuristic choices that are frequent in related works. We investigate the relationship between model size and TL, comparing a low-capacity VGG with a higher-capacity ResNeXt model. We evaluate our models on 1693 T2-weighted magnetic resonance imaging difference maps created from 183 patients, by classifying them into stable or unstable according to tumor evolution. The weak labels extracted from radiology reports allowed us to increase dataset size more than 3-fold, and improve VGG classification results from 75% to 82% AUC. Mixed training from scratch led to higher performance than fine-tuning or feature extraction. To assess generalizability, we ran inference on an open dataset (BraTS-2015: 15 patients, 51 difference maps), reaching up to 76% AUC. Overall, results suggest that medical imaging problems may benefit from smaller models and different TL strategies with respect to computer vision datasets, and that report-generated weak labels are effective in improving model performances. Code, in-house dataset and BraTS labels are released.



### ATCON: Attention Consistency for Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2210.09705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09705v1)
- **Published**: 2022-10-18 09:30:20+00:00
- **Updated**: 2022-10-18 09:30:20+00:00
- **Authors**: Ali Mirzazadeh, Florian Dubost, Maxwell Pike, Krish Maniar, Max Zuo, Christopher Lee-Messer, Daniel Rubin
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Attention--or attribution--maps methods are methods designed to highlight regions of the model's input that were discriminative for its predictions. However, different attention maps methods can highlight different regions of the input, with sometimes contradictory explanations for a prediction. This effect is exacerbated when the training set is small. This indicates that either the model learned incorrect representations or that the attention maps methods did not accurately estimate the model's representations. We propose an unsupervised fine-tuning method that optimizes the consistency of attention maps and show that it improves both classification performance and the quality of attention maps. We propose an implementation for two state-of-the-art attention computation methods, Grad-CAM and Guided Backpropagation, which relies on an input masking technique. We also show results on Grad-CAM and Integrated Gradients in an ablation study. We evaluate this method on our own dataset of event detection in continuous video recordings of hospital patients aggregated and curated for this work. As a sanity check, we also evaluate the proposed method on PASCAL VOC and SVHN. With the proposed method, with small training sets, we achieve a 6.6 points lift of F1 score over the baselines on our video dataset, a 2.9 point lift of F1 score on PASCAL, and a 1.8 points lift of mean Intersection over Union over Grad-CAM for weakly supervised detection on PASCAL. Those improved attention maps may help clinicians better understand vision model predictions and ease the deployment of machine learning systems into clinical care. We share part of the code for this article at the following repository: https://github.com/alimirzazadeh/SemisupervisedAttention.



### Fine-tune your Classifier: Finding Correlations With Temperature
- **Arxiv ID**: http://arxiv.org/abs/2210.09715v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09715v1)
- **Published**: 2022-10-18 09:48:46+00:00
- **Updated**: 2022-10-18 09:48:46+00:00
- **Authors**: Benjamin Chamand, Olivier Risser-Maroix, Camille Kurtz, Philippe Joly, Nicolas Lom√©nie
- **Comment**: None
- **Journal**: None
- **Summary**: Temperature is a widely used hyperparameter in various tasks involving neural networks, such as classification or metric learning, whose choice can have a direct impact on the model performance. Most of existing works select its value using hyperparameter optimization methods requiring several runs to find the optimal value. We propose to analyze the impact of temperature on classification tasks by describing a dataset as a set of statistics computed on representations on which we can build a heuristic giving us a default value of temperature. We study the correlation between these extracted statistics and the observed optimal temperatures. This preliminary study on more than a hundred combinations of different datasets and features extractors highlights promising results towards the construction of a general heuristic for temperature.



### 3D Scalable Quantum Convolutional Neural Networks for Point Cloud Data Processing in Classification Applications
- **Arxiv ID**: http://arxiv.org/abs/2210.09728v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.09728v1)
- **Published**: 2022-10-18 10:14:03+00:00
- **Updated**: 2022-10-18 10:14:03+00:00
- **Authors**: Hankyul Baek, Won Joon Yun, Joongheon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: With the beginning of the noisy intermediate-scale quantum (NISQ) era, a quantum neural network (QNN) has recently emerged as a solution for several specific problems that classical neural networks cannot solve. Moreover, a quantum convolutional neural network (QCNN) is the quantum-version of CNN because it can process high-dimensional vector inputs in contrast to QNN. However, due to the nature of quantum computing, it is difficult to scale up the QCNN to extract a sufficient number of features due to barren plateaus. Motivated by this, a novel 3D scalable QCNN (sQCNN-3D) is proposed for point cloud data processing in classification applications. Furthermore, reverse fidelity training (RF-Train) is additionally considered on top of sQCNN-3D for diversifying features with a limited number of qubits using the fidelity of quantum computing. Our data-intensive performance evaluation verifies that the proposed algorithm achieves desired performance.



### HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2210.09729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.09729v1)
- **Published**: 2022-10-18 10:14:11+00:00
- **Updated**: 2022-10-18 10:14:11+00:00
- **Authors**: Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Learning to generate diverse scene-aware and goal-oriented human motions in 3D scenes remains challenging due to the mediocre characteristics of the existing datasets on Human-Scene Interaction (HSI); they only have limited scale/quality and lack semantics. To fill in the gap, we propose a large-scale and semantic-rich synthetic HSI dataset, denoted as HUMANISE, by aligning the captured human motion sequences with various 3D indoor scenes. We automatically annotate the aligned motions with language descriptions that depict the action and the unique interacting objects in the scene; e.g., sit on the armchair near the desk. HUMANISE thus enables a new generation task, language-conditioned human motion generation in 3D scenes. The proposed task is challenging as it requires joint modeling of the 3D scene, human motion, and natural language. To tackle this task, we present a novel scene-and-language conditioned generative model that can produce 3D human motions of the desirable action interacting with the specified objects. Our experiments demonstrate that our model generates diverse and semantically consistent human motions in 3D scenes.



### Real-Time Multi-Modal Semantic Fusion on Unmanned Aerial Vehicles with Label Propagation for Cross-Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.09739v1
- **DOI**: 10.1016/j.robot.2022.104286
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.09739v1)
- **Published**: 2022-10-18 10:32:11+00:00
- **Updated**: 2022-10-18 10:32:11+00:00
- **Authors**: Simon Bultmann, Jan Quenzel, Sven Behnke
- **Comment**: 35 pages, 18 figures, Accepted for Robotics and Autonomous Systems,
  Elsevier. arXiv admin note: substantial text overlap with arXiv:2108.06608
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors have tremendous potential for fast autonomous or remote-controlled semantic scene analysis, e.g., for disaster examination. Here, we propose a UAV system for real-time semantic inference and fusion of multiple sensor modalities. Semantic segmentation of LiDAR scans and RGB images, as well as object detection on RGB and thermal images, run online onboard the UAV computer using lightweight CNN architectures and embedded inference accelerators. We follow a late fusion approach where semantic information from multiple sensor modalities augments 3D point clouds and image segmentation masks while also generating an allocentric semantic map. Label propagation on the semantic map allows for sensor-specific adaptation with cross-modality and cross-domain supervision. Our system provides augmented semantic images and point clouds with $\approx$ 9 Hz. We evaluate the integrated system in real-world experiments in an urban environment and at a disaster test site.



### A Dashboard to Analysis and Synthesis of Dimensionality Reduction Methods in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2210.09743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.09743v1)
- **Published**: 2022-10-18 10:42:14+00:00
- **Updated**: 2022-10-18 10:42:14+00:00
- **Authors**: Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine
- **Comment**: Journal Paper On Concepts Of Selection
- **Journal**: IJET Vol 5 No 3 Jun-Jul 2013 -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84880952006&partnerID=MN8TOARS
- **Summary**: Hyperspectral images (HSI) classification is a high technical remote sensing software. The purpose is to reproduce a thematic map . The HSI contains more than a hundred hyperspectral measures, as bands (or simply images), of the concerned region. They are taken at neighbors frequencies. Unfortunately, some bands are redundant features, others are noisily measured, and the high dimensionality of features made classification accuracy poor. The problematic is how to find the good bands to classify the regions items. Some methods use Mutual Information (MI) and thresholding, to select relevant images, without processing redundancy. Others control and avoid redundancy. But they process the dimensionality reduction, some times as selection, other times as wrapper methods without any relationship . Here , we introduce a survey on all scheme used, and after critics and improvement, we synthesize a dashboard, that helps user to analyze an hypothesize features selection and extraction softwares.



### A Real-Time Fusion Framework for Long-term Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2210.09757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.09757v1)
- **Published**: 2022-10-18 11:13:20+00:00
- **Updated**: 2022-10-18 11:13:20+00:00
- **Authors**: Yuchen Yang, Xudong Zhang, Shuang Gao, Jixiang Wan, Yishan Ping, Yuyue Liu, Jijunnan Li, Yandong Guo
- **Comment**: Submitted to ICRA 2023
- **Journal**: None
- **Summary**: Visual localization is a fundamental task that regresses the 6 Degree Of Freedom (6DoF) poses with image features in order to serve the high precision localization requests in many robotics applications. Degenerate conditions like motion blur, illumination changes and environment variations place great challenges in this task. Fusion with additional information, such as sequential information and Inertial Measurement Unit (IMU) inputs, would greatly assist such problems. In this paper, we present an efficient client-server visual localization architecture that fuses global and local pose estimations to realize promising precision and efficiency. We include additional geometry hints in mapping and global pose regressing modules to improve the measurement quality. A loosely coupled fusion policy is adopted to leverage the computation complexity and accuracy. We conduct the evaluations on two typical open-source benchmarks, 4Seasons and OpenLORIS. Quantitative results prove that our framework has competitive performance with respect to other state-of-the-art visual localization solutions.



### Very Low-Resolution Iris Recognition Via Eigen-Patch Super-Resolution and Matcher Fusion
- **Arxiv ID**: http://arxiv.org/abs/2210.09765v1
- **DOI**: 10.1109/BTAS.2016.7791208
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09765v1)
- **Published**: 2022-10-18 11:25:19+00:00
- **Updated**: 2022-10-18 11:25:19+00:00
- **Authors**: Fernando Alonso-Fernandez, Reuben A. Farrugia, Josef Bigun
- **Comment**: Published at Intl Conf on Biometrics: Theory, Apps and Systems, BTAS
  2016
- **Journal**: None
- **Summary**: Current research in iris recognition is moving towards enabling more relaxed acquisition conditions. This has effects on the quality of acquired images, with low resolution being a predominant issue. Here, we evaluate a super-resolution algorithm used to reconstruct iris images based on Eigen-transformation of local image patches. Each patch is reconstructed separately, allowing better quality of enhanced images by preserving local information. Contrast enhancement is used to improve the reconstruction quality, while matcher fusion has been adopted to improve iris recognition performance. We validate the system using a database of 1,872 near-infrared iris images. The presented approach is superior to bilinear or bicubic interpolation, especially at lower resolutions, and the fusion of the two systems pushes the EER to below 5% for down-sampling factors up to a image size of only 13x13.



### Compact multi-scale periocular recognition using SAFE features
- **Arxiv ID**: http://arxiv.org/abs/2210.09778v1
- **DOI**: 10.1109/ICPR.2016.7899842
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09778v1)
- **Published**: 2022-10-18 11:46:38+00:00
- **Updated**: 2022-10-18 11:46:38+00:00
- **Authors**: Fernando Alonso-Fernandez, Anna Mikaelyan, Josef Bigun
- **Comment**: Published at IEEE/IAPR Intl Conf on Pattern Recognition, ICPR 2016
- **Journal**: None
- **Summary**: In this paper, we present a new approach for periocular recognition based on the Symmetry Assessment by Feature Expansion (SAFE) descriptor, which encodes the presence of various symmetric curve families around image key points. We use the sclera center as single key point for feature extraction, highlighting the object-like identity properties that concentrates to this unique point of the eye. As it is demonstrated, such discriminative properties can be encoded with a reduced set of symmetric curves. Experiments are done with a database of periocular images captured with a digital camera. We test our system against reference periocular features, achieving top performance with a considerably smaller feature vector (given by the use of a single key point). All the systems tested also show a nearly steady correlation between acquisition distance and performance, and they are also able to cope well when enrolment and test images are not captured at the same distance. Fusion experiments among the available systems are also provided.



### Decoupling Features in Hierarchical Propagation for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.09782v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09782v3)
- **Published**: 2022-10-18 11:51:30+00:00
- **Updated**: 2022-11-28 14:21:14+00:00
- **Authors**: Zongxin Yang, Yi Yang
- **Comment**: Accepted by NeurIPS 2022 (Spotlight)
- **Journal**: None
- **Summary**: This paper focuses on developing a more effective method of hierarchical propagation for semi-supervised Video Object Segmentation (VOS). Based on vision transformers, the recently-developed Associating Objects with Transformers (AOT) approach introduces hierarchical propagation into VOS and has shown promising results. The hierarchical propagation can gradually propagate information from past frames to the current frame and transfer the current frame feature from object-agnostic to object-specific. However, the increase of object-specific information will inevitably lead to the loss of object-agnostic visual information in deep propagation layers. To solve such a problem and further facilitate the learning of visual embeddings, this paper proposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach. Firstly, DeAOT decouples the hierarchical propagation of object-agnostic and object-specific embeddings by handling them in two independent branches. Secondly, to compensate for the additional computation from dual-branch propagation, we propose an efficient module for constructing hierarchical propagation, i.e., Gated Propagation Module, which is carefully designed with single-head attention. Extensive experiments show that DeAOT significantly outperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can achieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations, we achieve new state-of-the-art performance on four benchmarks, i.e., YouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020 (0.622). Project page: https://github.com/z-x-yang/AOT.



### Inception-Based Crowd Counting -- Being Fast while Remaining Accurate
- **Arxiv ID**: http://arxiv.org/abs/2210.09796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09796v1)
- **Published**: 2022-10-18 12:12:13+00:00
- **Updated**: 2022-10-18 12:12:13+00:00
- **Authors**: Yiming Ma
- **Comment**: This is the paper that summarises my MSc's research project. Not plan
  to publish it. Share it on arXiv as a beginner-friendly paper for new
  researchers into crowd counting
- **Journal**: None
- **Summary**: Recent sophisticated CNN-based algorithms have demonstrated their extraordinary ability to automate counting crowds from images, thanks to their structures which are designed to address the issue of various head scales. However, these complicated architectures also increase computational complexity enormously, making real-time estimation implausible. Thus, in this paper, a new method, based on Inception-V3, is proposed to reduce the amount of computation. This proposed approach (ICC), exploits the first five inception blocks and the contextual module designed in CAN to extract features at different receptive fields, thereby being context-aware. The employment of these two different strategies can also increase the model's robustness. Experiments show that ICC can at best reduce 85.3 percent calculations with 24.4 percent performance loss. This high efficiency contributes significantly to the deployment of crowd counting models in surveillance systems to guard the public safety. The code will be available at https://github.com/YIMINGMA/CrowdCounting-ICC,and its pre-trained weights on the Crowd Counting dataset, which comprises a large variety of scenes from surveillance perspectives, will also open-sourced.



### HistoStarGAN: A Unified Approach to Stain Normalisation, Stain Transfer and Stain Invariant Segmentation in Renal Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2210.09798v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.09798v1)
- **Published**: 2022-10-18 12:22:26+00:00
- **Updated**: 2022-10-18 12:22:26+00:00
- **Authors**: Jelica Vasiljeviƒá, Friedrich Feuerhake, C√©dric Wemmert, Thomas Lampert
- **Comment**: None
- **Journal**: None
- **Summary**: Virtual stain transfer is a promising area of research in Computational Pathology, which has a great potential to alleviate important limitations when applying deeplearningbased solutions such as lack of annotations and sensitivity to a domain shift. However, in the literature, the majority of virtual staining approaches are trained for a specific staining or stain combination, and their extension to unseen stainings requires the acquisition of additional data and training. In this paper, we propose HistoStarGAN, a unified framework that performs stain transfer between multiple stainings, stain normalisation and stain invariant segmentation, all in one inference of the model. We demonstrate the generalisation abilities of the proposed solution to perform diverse stain transfer and accurate stain invariant segmentation over numerous unseen stainings, which is the first such demonstration in the field. Moreover, the pre-trained HistoStar-GAN model can serve as a synthetic data generator, which paves the way for the use of fully annotated synthetic image data to improve the training of deep learning-based algorithms. To illustrate the capabilities of our approach, as well as the potential risks in the microscopy domain, inspired by applications in natural images, we generated KidneyArtPathology, a fully annotated artificial image dataset for renal pathology.



### Scrape, Cut, Paste and Learn: Automated Dataset Generation Applied to Parcel Logistics
- **Arxiv ID**: http://arxiv.org/abs/2210.09814v1
- **DOI**: 10.1109/ICMLA55696.2022.00171
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09814v1)
- **Published**: 2022-10-18 12:49:04+00:00
- **Updated**: 2022-10-18 12:49:04+00:00
- **Authors**: Alexander Naumann, Felix Hertlein, Benchun Zhou, Laura D√∂rr, Kai Furmans
- **Comment**: Accepted at ICMLA 2022
- **Journal**: None
- **Summary**: State-of-the-art approaches in computer vision heavily rely on sufficiently large training datasets. For real-world applications, obtaining such a dataset is usually a tedious task. In this paper, we present a fully automated pipeline to generate a synthetic dataset for instance segmentation in four steps. In contrast to existing work, our pipeline covers every step from data acquisition to the final dataset. We first scrape images for the objects of interest from popular image search engines and since we rely only on text-based queries the resulting data comprises a wide variety of images. Hence, image selection is necessary as a second step. This approach of image scraping and selection relaxes the need for a real-world domain-specific dataset that must be either publicly available or created for this purpose. We employ an object-agnostic background removal model and compare three different methods for image selection: Object-agnostic pre-processing, manual image selection and CNN-based image selection. In the third step, we generate random arrangements of the object of interest and distractors on arbitrary backgrounds. Finally, the composition of the images is done by pasting the objects using four different blending methods. We present a case study for our dataset generation approach by considering parcel segmentation. For the evaluation we created a dataset of parcel photos that were annotated automatically. We find that (1) our dataset generation pipeline allows a successful transfer to real test images (Mask AP 86.2), (2) a very accurate image selection process - in contrast to human intuition - is not crucial and a broader category definition can help to bridge the domain gap, (3) the usage of blending methods is beneficial compared to simple copy-and-paste. We made our full code for scraping, image composition and training publicly available at https://a-nau.github.io/parcel2d.



### On-the-go Reflectance Transformation Imaging with Ordinary Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2210.09821v1
- **DOI**: 10.1007/978-3-031-25056-9_17
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.09821v1)
- **Published**: 2022-10-18 13:00:22+00:00
- **Updated**: 2022-10-18 13:00:22+00:00
- **Authors**: Mara Pistellato, Filippo Bergamasco
- **Comment**: VISART VI - Workshop at the European Conference of Computer Vision
  (ECCV)
- **Journal**: ECCV 2022 Workshops. ECCV 2022. Lecture Notes in Computer Science,
  vol 13801
- **Summary**: Reflectance Transformation Imaging (RTI) is a popular technique that allows the recovery of per-pixel reflectance information by capturing an object under different light conditions. This can be later used to reveal surface details and interactively relight the subject. Such process, however, typically requires dedicated hardware setups to recover the light direction from multiple locations, making the process tedious when performed outside the lab.   We propose a novel RTI method that can be carried out by recording videos with two ordinary smartphones. The flash led-light of one device is used to illuminate the subject while the other captures the reflectance. Since the led is mounted close to the camera lenses, we can infer the light direction for thousands of images by freely moving the illuminating device while observing a fiducial marker surrounding the subject. To deal with such amount of data, we propose a neural relighting model that reconstructs object appearance for arbitrary light directions from extremely compact reflectance distribution data compressed via Principal Components Analysis (PCA). Experiments shows that the proposed technique can be easily performed on the field with a resulting RTI model that can outperform state-of-the-art approaches involving dedicated hardware setups.



### BIOWISH: Biometric Recognition using Wearable Inertial Sensors detecting Heart Activity
- **Arxiv ID**: http://arxiv.org/abs/2210.09843v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2210.09843v1)
- **Published**: 2022-10-18 13:26:49+00:00
- **Updated**: 2022-10-18 13:26:49+00:00
- **Authors**: Emanuele Maiorana, Chiara Romano, Emiliano Schena, Carlo Massaroni
- **Comment**: None
- **Journal**: None
- **Summary**: Wearable devices are increasingly used, thanks to the wide set of applications that can be deployed exploiting their ability to monitor physical activity and health-related parameters. Their usage has been recently proposed to perform biometric recognition, leveraging on the uniqueness of the recorded traits to generate discriminative identifiers. Most of the studies conducted on this topic have considered signals derived from cardiac activity, detecting it mainly using electrical measurements thorugh electrocardiography, or optical recordings employing photoplethysmography. In this paper we instead propose a BIOmetric recognition approach using Wearable Inertial Sensors detecting Heart activity (BIOWISH). In more detail, we investigate the feasibility of exploiting mechanical measurements obtained through seismocardiography and gyrocardiography to recognize a person. Several feature extractors and classifiers, including deep learning techniques relying on transfer learning and siamese training, are employed to derive distinctive characteristics from the considered signals, and differentiate between legitimate and impostor subjects. An multi-session database, comprising acquisitions taken from subjects performing different activities, is employed to perform experimental tests simulating a verification system. The obtained results testify that identifiers derived from measurements of chest vibrations, collected by wearable inertial sensors, could be employed to guarantee high recognition performance, even when considering short-time recordings.



### Multimodal Image Fusion based on Hybrid CNN-Transformer and Non-local Cross-modal Attention
- **Arxiv ID**: http://arxiv.org/abs/2210.09847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09847v1)
- **Published**: 2022-10-18 13:30:52+00:00
- **Updated**: 2022-10-18 13:30:52+00:00
- **Authors**: Yu Yuan, Jiaqi Wu, Zhongliang Jing, Henry Leung, Han Pan
- **Comment**: None
- **Journal**: None
- **Summary**: The fusion of images taken by heterogeneous sensors helps to enrich the information and improve the quality of imaging. In this article, we present a hybrid model consisting of a convolutional encoder and a Transformer-based decoder to fuse multimodal images. In the encoder, a non-local cross-modal attention block is proposed to capture both local and global dependencies of multiple source images. A branch fusion module is designed to adaptively fuse the features of the two branches. We embed a Transformer module with linear complexity in the decoder to enhance the reconstruction capability of the proposed network. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method by comparing it with existing state-of-the-art fusion models. The source code of our work is available at https://github.com/pandayuanyu/HCFusion.



### Scaling Adversarial Training to Large Perturbation Bounds
- **Arxiv ID**: http://arxiv.org/abs/2210.09852v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.09852v1)
- **Published**: 2022-10-18 13:34:16+00:00
- **Updated**: 2022-10-18 13:34:16+00:00
- **Authors**: Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, R. Venkatesh Babu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most Adversarial Training algorithms aim at defending attacks constrained within low magnitude Lp norm bounds, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness within larger bounds, against perturbations that may be perceptible, but do not change human (or Oracle) prediction. The presence of images that flip Oracle predictions and those that do not makes this a challenging setting for adversarial robustness. We discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds (such as an L-inf bound of 16/255 on CIFAR-10) while outperforming existing defenses (AWP, TRADES, PGD-AT) at standard bounds (8/255) as well.



### Towards Efficient and Effective Self-Supervised Learning of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.09866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09866v1)
- **Published**: 2022-10-18 13:55:25+00:00
- **Updated**: 2022-10-18 13:55:25+00:00
- **Authors**: Sravanti Addepalli, Kaushal Bhogale, Priyam Dey, R. Venkatesh Babu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Self-supervision has emerged as a propitious method for visual representation learning after the recent paradigm shift from handcrafted pretext tasks to instance-similarity based approaches. Most state-of-the-art methods enforce similarity between various augmentations of a given image, while some methods additionally use contrastive approaches to explicitly ensure diverse representations. While these approaches have indeed shown promising direction, they require a significantly larger number of training iterations when compared to the supervised counterparts. In this work, we explore reasons for the slow convergence of these methods, and further propose to strengthen them using well-posed auxiliary tasks that converge significantly faster, and are also useful for representation learning. The proposed method utilizes the task of rotation prediction to improve the efficiency of existing state-of-the-art methods. We demonstrate significant gains in performance using the proposed method on multiple datasets, specifically for lower training epochs.



### Sequence and Circle: Exploring the Relationship Between Patches
- **Arxiv ID**: http://arxiv.org/abs/2210.09871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09871v2)
- **Published**: 2022-10-18 14:04:43+00:00
- **Updated**: 2022-10-19 15:29:22+00:00
- **Authors**: Zhengyang Yu, Jochen Triesch
- **Comment**: 7 pages, 1 figure
- **Journal**: None
- **Summary**: The vision transformer (ViT) has achieved state-of-the-art results in various vision tasks. It utilizes a learnable position embedding (PE) mechanism to encode the location of each image patch. However, it is presently unclear if this learnable PE is really necessary and what its benefits are. This paper explores two alternative ways of encoding the location of individual patches that exploit prior knowledge about their spatial arrangement. One is called the sequence relationship embedding (SRE), and the other is called the circle relationship embedding (CRE). Among them, the SRE considers all patches to be in order, and adjacent patches have the same interval distance. The CRE considers the central patch as the center of the circle and measures the distance of the remaining patches from the center based on the four neighborhoods principle. Multiple concentric circles with different radii combine different patches. Finally, we implemented these two relations on three classic ViTs and tested them on four popular datasets. Experiments show that SRE and CRE can replace PE to reduce the random learnable parameters while achieving the same performance. Combining SRE or CRE with PE gets better performance than only using PE.



### Unsupervised visualization of image datasets using contrastive learning
- **Arxiv ID**: http://arxiv.org/abs/2210.09879v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2210.09879v3)
- **Published**: 2022-10-18 14:13:20+00:00
- **Updated**: 2023-02-28 18:35:23+00:00
- **Authors**: Jan Niklas B√∂hm, Philipp Berens, Dmitry Kobak
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Visualization methods based on the nearest neighbor graph, such as t-SNE or UMAP, are widely used for visualizing high-dimensional data. Yet, these approaches only produce meaningful results if the nearest neighbors themselves are meaningful. For images represented in pixel space this is not the case, as distances in pixel space are often not capturing our sense of similarity and therefore neighbors are not semantically close. This problem can be circumvented by self-supervised approaches based on contrastive learning, such as SimCLR, relying on data augmentation to generate implicit neighbors, but these methods do not produce two-dimensional embeddings suitable for visualization. Here, we present a new method, called t-SimCNE, for unsupervised visualization of image data. T-SimCNE combines ideas from contrastive learning and neighbor embeddings, and trains a parametric mapping from the high-dimensional pixel space into two dimensions. We show that the resulting 2D embeddings achieve classification accuracy comparable to the state-of-the-art high-dimensional SimCLR representations, thus faithfully capturing semantic relationships. Using t-SimCNE, we obtain informative visualizations of the CIFAR-10 and CIFAR-100 datasets, showing rich cluster structure and highlighting artifacts and outliers.



### MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.09887v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09887v5)
- **Published**: 2022-10-18 14:23:05+00:00
- **Updated**: 2023-08-14 20:24:24+00:00
- **Authors**: Mathias Parger, Chengcheng Tang, Thomas Neff, Christopher D. Twigg, Cem Keskin, Robert Wang, Markus Steinberger
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network inference on video input is computationally expensive and requires high memory bandwidth. Recently, DeltaCNN managed to reduce the cost by only processing pixels with significant updates over the previous frame. However, DeltaCNN relies on static camera input. Moving cameras add new challenges in how to fuse newly unveiled image regions with already processed regions efficiently to minimize the update rate - without increasing memory overhead and without knowing the camera extrinsics of future frames. In this work, we propose MotionDeltaCNN, a sparse CNN inference framework that supports moving cameras. We introduce spherical buffers and padded convolutions to enable seamless fusion of newly unveiled regions and previously processed regions -- without increasing memory footprint. Our evaluation shows that we outperform DeltaCNN by up to 90% for moving camera videos.



### SA-DNet: A on-demand semantic object registration network adapting to non-rigid deformation
- **Arxiv ID**: http://arxiv.org/abs/2210.09900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09900v2)
- **Published**: 2022-10-18 14:41:28+00:00
- **Updated**: 2022-10-26 02:13:42+00:00
- **Authors**: Housheng Xie, Junhui Qiu, Yuan Dai, Yang Yang, Changcheng Xiang, Yukuan Zhang
- **Comment**: 15 pages, 12 figures
- **Journal**: None
- **Summary**: As an essential processing step before the fusing of infrared and visible images, the performance of image registration determines whether the two images can be fused at correct spatial position. In the actual scenario, the varied imaging devices may lead to a change in perspective or time gap between shots, making significant non-rigid spatial relationship in infrared and visible images. Even if a large number of feature points are matched, the registration accuracy may still be inadequate, affecting the result of image fusion and other vision tasks. To alleviate this problem, we propose a Semantic-Aware on-Demand registration network (SA-DNet), which mainly purpose is to confine the feature matching process to the semantic region of interest (sROI) by designing semantic-aware module (SAM) and HOL-Deep hybrid matching module (HDM). After utilizing TPS to transform infrared and visible images based on the corresponding feature points in sROI, the registered images are fused using image fusion module (IFM) to achieve a fully functional registration and fusion network. Moreover, we point out that for different demands, this type of approach allows us to select semantic objects for feature matching as needed and accomplishes task-specific registration based on specific requirements. To demonstrate the robustness of SA-DNet for non-rigid distortions, we conduct extensive experiments by comparing SA-DNet with five state-of-the-art infrared and visible image feature matching methods, and the experimental results show that our method adapts better to the presence of non-rigid distortions in the images and provides semantically well-registered images.



### Uncertainty estimation for out-of-distribution detection in computational histopathology
- **Arxiv ID**: http://arxiv.org/abs/2210.09909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09909v1)
- **Published**: 2022-10-18 14:49:44+00:00
- **Updated**: 2022-10-18 14:49:44+00:00
- **Authors**: Lea Goetz
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: In computational histopathology algorithms now outperform humans on a range of tasks, but to date none are employed for automated diagnoses in the clinic. Before algorithms can be involved in such high-stakes decisions they need to "know when they don't know", i.e., they need to estimate their predictive uncertainty. This allows them to defer potentially erroneous predictions to a human pathologist, thus increasing their safety. Here, we evaluate the predictive performance and calibration of several uncertainty estimation methods on clinical histopathology data. We show that a distance-aware uncertainty estimation method outperforms commonly used approaches, such as Monte Carlo dropout and deep ensembles. However, we observe a drop in predictive performance and calibration on novel samples across all uncertainty estimation methods tested. We also investigate the use of uncertainty thresholding to reject out-of-distribution samples for selective prediction. We demonstrate the limitations of this approach and suggest areas for future research.



### Dense FixMatch: a simple semi-supervised learning method for pixel-wise prediction tasks
- **Arxiv ID**: http://arxiv.org/abs/2210.09919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09919v1)
- **Published**: 2022-10-18 15:02:51+00:00
- **Updated**: 2022-10-18 15:02:51+00:00
- **Authors**: Miquel Mart√≠ i Rabad√°n, Alessandro Pieropan, Hossein Azizpour, Atsuto Maki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Dense FixMatch, a simple method for online semi-supervised learning of dense and structured prediction tasks combining pseudo-labeling and consistency regularization via strong data augmentation. We enable the application of FixMatch in semi-supervised learning problems beyond image classification by adding a matching operation on the pseudo-labels. This allows us to still use the full strength of data augmentation pipelines, including geometric transformations. We evaluate it on semi-supervised semantic segmentation on Cityscapes and Pascal VOC with different percentages of labeled data and ablate design choices and hyper-parameters. Dense FixMatch significantly improves results compared to supervised learning using only labeled data, approaching its performance with 1/4 of the labeled samples.



### Bridging Language and Geometric Primitives for Zero-shot Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.09923v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09923v2)
- **Published**: 2022-10-18 15:06:54+00:00
- **Updated**: 2023-08-04 05:57:05+00:00
- **Authors**: Runnan Chen, Xinge Zhu, Nenglun Chen, Wei Li, Yuexin Ma, Ruigang Yang, Wenping Wang
- **Comment**: ACM MM 2023
- **Journal**: None
- **Summary**: We investigate transductive zero-shot point cloud semantic segmentation, where the network is trained on seen objects and able to segment unseen objects. The 3D geometric elements are essential cues to imply a novel 3D object type. However, previous methods neglect the fine-grained relationship between the language and the 3D geometric elements. To this end, we propose a novel framework to learn the geometric primitives shared in seen and unseen categories' objects and employ a fine-grained alignment between language and the learned geometric primitives. Therefore, guided by language, the network recognizes the novel objects represented with geometric primitives. Specifically, we formulate a novel point visual representation, the similarity vector of the point's feature to the learnable prototypes, where the prototypes automatically encode geometric primitives via back-propagation. Besides, we propose a novel Unknown-aware InfoNCE Loss to fine-grained align the visual representation with language. Extensive experiments show that our method significantly outperforms other state-of-the-art methods in the harmonic mean-intersection-over-union (hIoU), with the improvement of 17.8\%, 30.4\%, 9.2\% and 7.9\% on S3DIS, ScanNet, SemanticKITTI and nuScenes datasets, respectively. Codes are available (https://github.com/runnanchen/Zero-Shot-Point-Cloud-Segmentation)



### Soil moisture estimation from Sentinel-1 interferometric observations over arid regions
- **Arxiv ID**: http://arxiv.org/abs/2210.10665v1
- **DOI**: 10.1016/j.cageo.2023.105410
- **Categories**: **cs.CV**, eess.IV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2210.10665v1)
- **Published**: 2022-10-18 15:32:13+00:00
- **Updated**: 2022-10-18 15:32:13+00:00
- **Authors**: Kleanthis Karamvasis, Vassilia Karathanassi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a methodology based on interferometric synthetic aperture radar (InSAR) time series analysis that can provide surface (top 5 cm) soil moisture (SSM) estimations. The InSAR time series analysis consists of five processing steps. A co-registered Single Look Complex (SLC) SAR stack as well as meteorological information are required as input of the proposed workflow. In the first step, ice/snow-free and zero-precipitation SAR images are identified using meteorological data. In the second step, construction and phase extraction of distributed scatterers (DSs) (over bare land) is performed. In the third step, for each DS the ordering of surface soil moisture (SSM) levels of SAR acquisitions based on interferometric coherence is calculated. In the fourth step, for each DS the coherence due to SSM variations is calculated. In the fifth step, SSM is estimated by a constrained inversion of an analytical interferometric model using coherence and phase closure information. The implementation of the proposed approach is provided as an open-source software toolbox (INSAR4SM) available at www.github.com/kleok/INSAR4SM.   A case study over an arid region in California/Arizona is presented. The proposed workflow was applied in Sentinel- 1 (C-band) VV-polarized InSAR observations. The estimated SSM results were assessed with independent SSM observations from a station of the International Soil Moisture Network (ISMN) (RMSE: 0.027 $m^3/m^3$ R: 0.88) and ERA5-Land reanalysis model data (RMSE: 0.035 $m^3/m^3$ R: 0.71). The proposed methodology was able to provide accurate SSM estimations at high spatial resolution (~250 m). A discussion of the benefits and the limitations of the proposed methodology highlighted the potential of interferometric observables for SSM estimation over arid regions.



### On the Importance of Architectures and Hyperparameters for Fairness in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.09943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09943v1)
- **Published**: 2022-10-18 15:46:05+00:00
- **Updated**: 2022-10-18 15:46:05+00:00
- **Authors**: Rhea Sukthanker, Samuel Dooley, John P. Dickerson, Colin White, Frank Hutter, Micah Goldblum
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition systems are deployed across the world by government agencies and contractors for sensitive and impactful tasks, such as surveillance and database matching. Despite their widespread use, these systems are known to exhibit bias across a range of sociodemographic dimensions, such as gender and race. Nonetheless, an array of works proposing pre-processing, training, and post-processing methods have failed to close these gaps. Here, we take a very different approach to this problem, identifying that both architectures and hyperparameters of neural networks are instrumental in reducing bias. We first run a large-scale analysis of the impact of architectures and training hyperparameters on several common fairness metrics and show that the implicit convention of choosing high-accuracy architectures may be suboptimal for fairness. Motivated by our findings, we run the first neural architecture search for fairness, jointly with a search for hyperparameters. We output a suite of models which Pareto-dominate all other competitive architectures in terms of accuracy and fairness. Furthermore, we show that these models transfer well to other face recognition datasets with similar and distinct protected attributes. We release our code and raw result files so that researchers and practitioners can replace our fairness metrics with a bias measure of their choice.



### Number-Adaptive Prototype Learning for 3D Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.09948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09948v1)
- **Published**: 2022-10-18 15:57:20+00:00
- **Updated**: 2022-10-18 15:57:20+00:00
- **Authors**: Yangheng Zhao, Jun Wang, Xiaolong Li, Yue Hu, Ce Zhang, Yanfeng Wang, Siheng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point cloud semantic segmentation is one of the fundamental tasks for 3D scene understanding and has been widely used in the metaverse applications. Many recent 3D semantic segmentation methods learn a single prototype (classifier weights) for each semantic class, and classify 3D points according to their nearest prototype. However, learning only one prototype for each class limits the model's ability to describe the high variance patterns within a class. Instead of learning a single prototype for each class, in this paper, we propose to use an adaptive number of prototypes to dynamically describe the different point patterns within a semantic class. With the powerful capability of vision transformer, we design a Number-Adaptive Prototype Learning (NAPL) model for point cloud semantic segmentation. To train our NAPL model, we propose a simple yet effective prototype dropout training strategy, which enables our model to adaptively produce prototypes for each class. The experimental results on SemanticKITTI dataset demonstrate that our method achieves 2.3% mIoU improvement over the baseline model based on the point-wise classification paradigm.



### Nighttime Dehaze-Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2210.09962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.09962v1)
- **Published**: 2022-10-18 16:19:25+00:00
- **Updated**: 2022-10-18 16:19:25+00:00
- **Authors**: Harshan Baskar, Anirudh S Chakravarthy, Prateek Garg, Divyam Goel, Abhijith S Raj, Kshitij Kumar, Lakshya, Ravichandra Parvatham, V Sushant, Bijay Kumar Rout
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new computer vision task called nighttime dehaze-enhancement. This task aims to jointly perform dehazing and lightness enhancement. Our task fundamentally differs from nighttime dehazing -- our goal is to jointly dehaze and enhance scenes, while nighttime dehazing aims to dehaze scenes under a nighttime setting. In order to facilitate further research on this task, we release a new benchmark dataset called Reside-$\beta$ Night dataset, consisting of 4122 nighttime hazed images from 2061 scenes and 2061 ground truth images. Moreover, we also propose a new network called NDENet (Nighttime Dehaze-Enhancement Network), which jointly performs dehazing and low-light enhancement in an end-to-end manner. We evaluate our method on the proposed benchmark and achieve SSIM of 0.8962 and PSNR of 26.25. We also compare our network with other baseline networks on our benchmark to demonstrate the effectiveness of our approach. We believe that nighttime dehaze-enhancement is an essential task particularly for autonomous navigation applications, and hope that our work will open up new frontiers in research. Our dataset and code will be made publicly available upon acceptance of our paper.



### Transfer-learning for video classification: Video Swin Transformer on multiple domains
- **Arxiv ID**: http://arxiv.org/abs/2210.09969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, I.2.7; I.2.10; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2210.09969v1)
- **Published**: 2022-10-18 16:24:55+00:00
- **Updated**: 2022-10-18 16:24:55+00:00
- **Authors**: Daniel Oliveira, David Martins de Matos
- **Comment**: 7 pages, 11 figures
- **Journal**: None
- **Summary**: The computer vision community has seen a shift from convolutional-based to pure transformer architectures for both image and video tasks. Training a transformer from zero for these tasks usually requires a lot of data and computational resources. Video Swin Transformer (VST) is a pure-transformer model developed for video classification which achieves state-of-the-art results in accuracy and efficiency on several datasets. In this paper, we aim to understand if VST generalizes well enough to be used in an out-of-domain setting. We study the performance of VST on two large-scale datasets, namely FCVID and Something-Something using a transfer learning approach from Kinetics-400, which requires around 4x less memory than training from scratch. We then break down the results to understand where VST fails the most and in which scenarios the transfer-learning approach is viable. Our experiments show an 85\% top-1 accuracy on FCVID without retraining the whole model which is equal to the state-of-the-art for the dataset and a 21\% accuracy on Something-Something. The experiments also suggest that the performance of the VST decreases on average when the video duration increases which seems to be a consequence of a design choice of the model. From the results, we conclude that VST generalizes well enough to classify out-of-domain videos without retraining when the target classes are from the same type as the classes used to train the model. We observed this effect when we performed transfer-learning from Kinetics-400 to FCVID, where most datasets target mostly objects. On the other hand, if the classes are not from the same type, then the accuracy after the transfer-learning approach is expected to be poor. We observed this effect when we performed transfer-learning from Kinetics-400, where the classes represent mostly objects, to Something-Something, where the classes represent mostly actions.



### Kurdish Handwritten Character Recognition using Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2210.13734v1
- **DOI**: 10.1016/j.gep.2022.119278
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2210.13734v1)
- **Published**: 2022-10-18 16:48:28+00:00
- **Updated**: 2022-10-18 16:48:28+00:00
- **Authors**: Rebin M. Ahmed, Tarik A. Rashid, Polla Fattah, Abeer Alsadoon, Nebojsa Bacanin, Seyedali Mirjalili, S. Vimal, Amit Chhabra
- **Comment**: 12 pages
- **Journal**: Gene Expression Patterns, 2022
- **Summary**: Handwriting recognition is one of the active and challenging areas of research in the field of image processing and pattern recognition. It has many applications that include: a reading aid for visual impairment, automated reading and processing for bank checks, making any handwritten document searchable, and converting them into structural text form, etc. Moreover, high accuracy rates have been recorded by handwriting recognition systems for English, Chinese Arabic, Persian, and many other languages. Yet there is no such system available for offline Kurdish handwriting recognition. In this paper, an attempt is made to design and develop a model that can recognize handwritten characters for Kurdish alphabets using deep learning techniques. Kurdish (Sorani) contains 34 characters and mainly employs an Arabic\Persian based script with modified alphabets. In this work, a Deep Convolutional Neural Network model is employed that has shown exemplary performance in handwriting recognition systems. Then, a comprehensive dataset was created for handwritten Kurdish characters, which contains more than 40 thousand images. The created dataset has been used for training the Deep Convolutional Neural Network model for classification and recognition tasks. In the proposed system, the experimental results show an acceptable recognition level. The testing results reported a 96% accuracy rate, and training accuracy reported a 97% accuracy rate. From the experimental results, it is clear that the proposed deep learning model is performing well and is comparable to the similar model of other languages' handwriting recognition systems.



### Perceptual Grouping in Contrastive Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2210.09996v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.09996v3)
- **Published**: 2022-10-18 17:01:35+00:00
- **Updated**: 2023-08-22 01:40:44+00:00
- **Authors**: Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, Jonathon Shlens
- **Comment**: Accepted and presented at ICCV 2023
- **Journal**: None
- **Summary**: Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robustness analyses. We find that the resulting model achieves state-of-the-art results in terms of unsupervised segmentation, and demonstrate that the learned representations are uniquely robust to spurious correlations in datasets designed to probe the causal behavior of vision models.



### Otsu based Differential Evolution Method for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.10005v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2210.10005v1)
- **Published**: 2022-10-18 17:21:24+00:00
- **Updated**: 2022-10-18 17:21:24+00:00
- **Authors**: Afreen Shaikh, Sharmila Botcha, Murali Krishna
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an OTSU based differential evolution method for satellite image segmentation and compares it with four other methods such as Modified Artificial Bee Colony Optimizer (MABC), Artificial Bee Colony (ABC), Genetic Algorithm (GA), and Particle Swarm Optimization (PSO) using the objective function proposed by Otsu for optimal multilevel thresholding. The experiments conducted and their results illustrate that our proposed DE and OTSU algorithm segmentation can effectively and precisely segment the input image, close to results obtained by the other methods. In the proposed DE and OTSU algorithm, instead of passing the fitness function variables, the entire image is passed as an input to the DE algorithm after obtaining the threshold values for the input number of levels in the OTSU algorithm. The image segmentation results are obtained after learning about the image instead of learning about the fitness variables. In comparison to other segmentation methods examined, the proposed DE and OTSU algorithm yields promising results with minimized computational time compared to some algorithms.



### ULN: Towards Underspecified Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2210.10020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.10020v1)
- **Published**: 2022-10-18 17:45:06+00:00
- **Updated**: 2022-10-18 17:45:06+00:00
- **Authors**: Weixi Feng, Tsu-Jui Fu, Yujie Lu, William Yang Wang
- **Comment**: EMNLP 2022
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a task to guide an embodied agent moving to a target position using language instructions. Despite the significant performance improvement, the wide use of fine-grained instructions fails to characterize more practical linguistic variations in reality. To fill in this gap, we introduce a new setting, namely Underspecified vision-and-Language Navigation (ULN), and associated evaluation datasets. ULN evaluates agents using multi-level underspecified instructions instead of purely fine-grained or coarse-grained, which is a more realistic and general setting. As a primary step toward ULN, we propose a VLN framework that consists of a classification module, a navigation agent, and an Exploitation-to-Exploration (E2E) module. Specifically, we propose to learn Granularity Specific Sub-networks (GSS) for the agent to ground multi-level instructions with minimal additional parameters. Then, our E2E module estimates grounding uncertainty and conducts multi-step lookahead exploration to improve the success rate further. Experimental results show that existing VLN models are still brittle to multi-level language underspecification. Our framework is more robust and outperforms the baselines on ULN by ~10% relative success rate across all levels.



### ARAH: Animatable Volume Rendering of Articulated Human SDFs
- **Arxiv ID**: http://arxiv.org/abs/2210.10036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10036v1)
- **Published**: 2022-10-18 17:56:59+00:00
- **Updated**: 2022-10-18 17:56:59+00:00
- **Authors**: Shaofei Wang, Katja Schwarz, Andreas Geiger, Siyu Tang
- **Comment**: Accepted to ECCV 2022. Project page:
  https://neuralbodies.github.io/arah/
- **Journal**: None
- **Summary**: Combining human body models with differentiable rendering has recently enabled animatable avatars of clothed humans from sparse sets of multi-view RGB videos. While state-of-the-art approaches achieve realistic appearance with neural radiance fields (NeRF), the inferred geometry often lacks detail due to missing geometric constraints. Further, animating avatars in out-of-distribution poses is not yet possible because the mapping from observation space to canonical space does not generalize faithfully to unseen poses. In this work, we address these shortcomings and propose a model to create animatable clothed human avatars with detailed geometry that generalize well to out-of-distribution poses. To achieve detailed geometry, we combine an articulated implicit surface representation with volume rendering. For generalization, we propose a novel joint root-finding algorithm for simultaneous ray-surface intersection search and correspondence search. Our algorithm enables efficient point sampling and accurate point canonicalization while generalizing well to unseen poses. We demonstrate that our proposed pipeline can generate clothed avatars with high-quality pose-dependent geometry and appearance from a sparse set of multi-view RGB videos. Our method achieves state-of-the-art performance on geometry and appearance reconstruction while creating animatable avatars that generalize well to out-of-distribution poses beyond the small number of training poses.



### How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2210.10039v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10039v1)
- **Published**: 2022-10-18 17:58:25+00:00
- **Updated**: 2022-10-18 17:58:25+00:00
- **Authors**: Mantas Mazeika, Eric Tang, Andy Zou, Steven Basart, Jun Shern Chan, Dawn Song, David Forsyth, Jacob Steinhardt, Dan Hendrycks
- **Comment**: NeurIPS 2022; datasets available at
  https://github.com/hendrycks/emodiversity/
- **Journal**: None
- **Summary**: In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.



### Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion
- **Arxiv ID**: http://arxiv.org/abs/2210.10044v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2210.10044v1)
- **Published**: 2022-10-18 17:59:30+00:00
- **Updated**: 2022-10-18 17:59:30+00:00
- **Authors**: Zipeng Fu, Xuxin Cheng, Deepak Pathak
- **Comment**: CoRL 2022 (Oral). Project website at https://maniploco.github.io
- **Journal**: None
- **Summary**: An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard hierarchical control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective. It requires immense engineering to support coordination between the arm and legs, and error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible given evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups. Videos are at https://maniploco.github.io



### A Tri-Layer Plugin to Improve Occluded Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.10046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10046v1)
- **Published**: 2022-10-18 17:59:51+00:00
- **Updated**: 2022-10-18 17:59:51+00:00
- **Authors**: Guanqi Zhan, Weidi Xie, Andrew Zisserman
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Detecting occluded objects still remains a challenge for state-of-the-art object detectors. The objective of this work is to improve the detection for such objects, and thereby improve the overall performance of a modern object detector.   To this end we make the following four contributions: (1) We propose a simple 'plugin' module for the detection head of two-stage object detectors to improve the recall of partially occluded objects. The module predicts a tri-layer of segmentation masks for the target object, the occluder and the occludee, and by doing so is able to better predict the mask of the target object. (2) We propose a scalable pipeline for generating training data for the module by using amodal completion of existing object detection and instance segmentation training datasets to establish occlusion relationships. (3) We also establish a COCO evaluation dataset to measure the recall performance of partially occluded and separated objects. (4) We show that the plugin module inserted into a two-stage detector can boost the performance significantly, by only fine-tuning the detection head, and with additional improvements if the entire architecture is fine-tuned. COCO results are reported for Mask R-CNN with Swin-T or Swin-S backbones, and Cascade Mask R-CNN with a Swin-B backbone.



### From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data
- **Arxiv ID**: http://arxiv.org/abs/2210.10047v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10047v3)
- **Published**: 2022-10-18 17:59:55+00:00
- **Updated**: 2022-12-15 23:47:39+00:00
- **Authors**: Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
- **Comment**: Code and data available at: https://play-to-policy.github.io; (fixed
  metadata author name format)
- **Journal**: None
- **Summary**: While large-scale sequence modeling from offline data has led to impressive performance gains in natural language and image generation, directly translating such ideas to robotics has been challenging. One critical reason for this is that uncurated robot demonstration data, i.e. play data, collected from non-expert human demonstrators are often noisy, diverse, and distributionally multi-modal. This makes extracting useful, task-centric behaviors from such data a difficult generative modeling problem. In this work, we present Conditional Behavior Transformers (C-BeT), a method that combines the multi-modal generation ability of Behavior Transformer with future-conditioned goal specification. On a suite of simulated benchmark tasks, we find that C-BeT improves upon prior state-of-the-art work in learning from play data by an average of 45.7%. Further, we demonstrate for the first time that useful task-centric behaviors can be learned on a real-world robot purely from play data without any task labels or reward information. Robot videos are best viewed on our project website: https://play-to-policy.github.io



### How to Boost Face Recognition with StyleGAN?
- **Arxiv ID**: http://arxiv.org/abs/2210.10090v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10090v2)
- **Published**: 2022-10-18 18:41:56+00:00
- **Updated**: 2023-07-28 18:18:39+00:00
- **Authors**: Artem Sevastopolsky, Yury Malkov, Nikita Durasov, Luisa Verdoliva, Matthias Nie√üner
- **Comment**: 16 pages, 9 figures, 11 tables; accepted to ICCV 2023
- **Journal**: None
- **Summary**: State-of-the-art face recognition systems require vast amounts of labeled training data. Given the priority of privacy in face recognition applications, the data is limited to celebrity web crawls, which have issues such as limited numbers of identities. On the other hand, self-supervised revolution in the industry motivates research on the adaptation of related techniques to facial recognition. One of the most popular practical tricks is to augment the dataset by the samples drawn from generative models while preserving the identity. We show that a simple approach based on fine-tuning pSp encoder for StyleGAN allows us to improve upon the state-of-the-art facial recognition and performs better compared to training on synthetic face identities. We also collect large-scale unlabeled datasets with controllable ethnic constitution -- AfricanFaceSet-5M (5 million images of different people) and AsianFaceSet-3M (3 million images of different people) -- and we show that pretraining on each of them improves recognition of the respective ethnicities (as well as others), while combining all unlabeled datasets results in the biggest performance increase. Our self-supervised strategy is the most useful with limited amounts of labeled training data, which can be beneficial for more tailored face recognition tasks and when facing privacy concerns. Evaluation is based on a standard RFW dataset and a new large-scale RB-WebFace benchmark. The code and data are made publicly available at https://github.com/seva100/stylegan-for-facerec.



### Parallel Inversion of Neural Radiance Fields for Robust Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.10108v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.10108v2)
- **Published**: 2022-10-18 19:09:58+00:00
- **Updated**: 2023-03-10 06:27:33+00:00
- **Authors**: Yunzhi Lin, Thomas M√ºller, Jonathan Tremblay, Bowen Wen, Stephen Tyree, Alex Evans, Patricio A. Vela, Stan Birchfield
- **Comment**: ICRA 2023. Project page at https://pnerfp.github.io/
- **Journal**: None
- **Summary**: We present a parallelized optimization method based on fast Neural Radiance Fields (NeRF) for estimating 6-DoF pose of a camera with respect to an object or scene. Given a single observed RGB image of the target, we can predict the translation and rotation of the camera by minimizing the residual between pixels rendered from a fast NeRF model and pixels in the observed image. We integrate a momentum-based camera extrinsic optimization procedure into Instant Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By introducing parallel Monte Carlo sampling into the pose estimation task, our method overcomes local minima and improves efficiency in a more extensive search space. We also show the importance of adopting a more robust pixel-based loss function to reduce error. Experiments demonstrate that our method can achieve improved generalization and robustness on both synthetic and real-world benchmarks.



### Trixi the Librarian
- **Arxiv ID**: http://arxiv.org/abs/2210.10110v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10110v2)
- **Published**: 2022-10-18 19:16:34+00:00
- **Updated**: 2022-10-20 11:33:59+00:00
- **Authors**: Fabian Wieczorek, Shang-Ching Liu, Bj√∂rn Sygo, Mykhailo Koshil
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a three-part system that automatically sorts books on a shelf using the PR- 2 platform. The paper describes a methodology to sufficiently detect and recognize books using a multistep vision pipeline based on deep learning models as well as conventional computer vision. Furthermore, the difficulties of relocating books using a bi-manual robot along with solutions based on MoveIt and BioIK are being addressed. Experiments show that the performance is overall good enough to repeatedly sort three books on a shelf. Nevertheless, further improvements are being discussed, potentially leading to a more robust book recognition and more versatile manipulation techniques.



### Transferable Unlearnable Examples
- **Arxiv ID**: http://arxiv.org/abs/2210.10114v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10114v1)
- **Published**: 2022-10-18 19:23:52+00:00
- **Updated**: 2022-10-18 19:23:52+00:00
- **Authors**: Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, Jiliang Tang
- **Comment**: None
- **Journal**: None
- **Summary**: With more people publishing their personal data online, unauthorized data usage has become a serious concern. The unlearnable strategies have been introduced to prevent third parties from training on the data without permission. They add perturbations to the users' data before publishing, which aims to make the models trained on the perturbed published dataset invalidated. These perturbations have been generated for a specific training setting and a target dataset. However, their unlearnable effects significantly decrease when used in other training settings and datasets. To tackle this issue, we propose a novel unlearnable strategy based on Classwise Separability Discriminant (CSD), which aims to better transfer the unlearnable effects to other training settings and datasets by enhancing the linear separability. Extensive experiments demonstrate the transferability of the proposed unlearnable examples across training settings and datasets.



### Initial Orbit Determination from Only Heading Measurements
- **Arxiv ID**: http://arxiv.org/abs/2210.10120v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10120v2)
- **Published**: 2022-10-18 19:31:00+00:00
- **Updated**: 2023-01-18 15:32:12+00:00
- **Authors**: John A. Christian
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces the problem of initial orbit determination (IOD) from only heading measurements. Such a problem occurs in practice when estimating the orbit of a spacecraft using visual odometry measurements from an optical camera. After reviewing the problem geometry, a simple solution is developed in the form of an iterative scheme on the parameters describing the orbital hodograph. Numerical results are presented for an example spacecraft in low lunar orbit. The principal intent of this brief study is to communicate the existence of a new class of IOD problem to the community and to encourage the broader study of hodographs and heading-only IOD.



### Interpolated SelectionConv for Spherical Images and Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2210.10123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10123v1)
- **Published**: 2022-10-18 19:49:07+00:00
- **Updated**: 2022-10-18 19:49:07+00:00
- **Authors**: David Hart, Michael Whitney, Bryan Morse
- **Comment**: To be presented at WACV 2023
- **Journal**: None
- **Summary**: We present a new and general framework for convolutional neural network operations on spherical (or omnidirectional) images. Our approach represents the surface as a graph of connected points that doesn't rely on a particular sampling strategy. Additionally, by using an interpolated version of SelectionConv, we can operate on the sphere while using existing 2D CNNs and their weights. Since our method leverages existing graph implementations, it is also fast and can be fine-tuned efficiently. Our method is also general enough to be applied to any surface type, even those that are topologically non-simple. We demonstrate the effectiveness of our technique on the tasks of style transfer and segmentation for spheres as well as stylization for 3D meshes. We provide a thorough ablation study of the performance of various spherical sampling strategies.



### PERI: Part Aware Emotion Recognition In The Wild
- **Arxiv ID**: http://arxiv.org/abs/2210.10130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10130v1)
- **Published**: 2022-10-18 20:01:40+00:00
- **Updated**: 2022-10-18 20:01:40+00:00
- **Authors**: Akshita Mittel, Shashank Tripathi
- **Comment**: Accepted at ECCVW 2022
- **Journal**: None
- **Summary**: Emotion recognition aims to interpret the emotional states of a person based on various inputs including audio, visual, and textual cues. This paper focuses on emotion recognition using visual features. To leverage the correlation between facial expression and the emotional state of a person, pioneering methods rely primarily on facial features. However, facial features are often unreliable in natural unconstrained scenarios, such as in crowded scenes, as the face lacks pixel resolution and contains artifacts due to occlusion and blur. To address this, in the wild emotion recognition exploits full-body person crops as well as the surrounding scene context. In a bid to use body pose for emotion recognition, such methods fail to realize the potential that facial expressions, when available, offer. Thus, the aim of this paper is two-fold. First, we demonstrate our method, PERI, to leverage both body pose and facial landmarks. We create part aware spatial (PAS) images by extracting key regions from the input image using a mask generated from both body pose and facial landmarks. This allows us to exploit body pose in addition to facial context whenever available. Second, to reason from the PAS images, we introduce context infusion (Cont-In) blocks. These blocks attend to part-specific information, and pass them onto the intermediate features of an emotion recognition network. Our approach is conceptually simple and can be applied to any existing emotion recognition method. We provide our results on the publicly available in the wild EMOTIC dataset. Compared to existing methods, PERI achieves superior performance and leads to significant improvements in the mAP of emotion categories, while decreasing Valence, Arousal and Dominance errors. Importantly, we observe that our method improves performance in both images with fully visible faces as well as in images with occluded or blurred faces.



### Class-Level Confidence Based 3D Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.10138v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10138v2)
- **Published**: 2022-10-18 20:13:28+00:00
- **Updated**: 2022-10-21 20:50:06+00:00
- **Authors**: Zhimin Chen, Longlong Jing, Liang Yang, Yingwei Li, Bing Li
- **Comment**: None
- **Journal**: WACV 2023 accepeted
- **Summary**: Recent state-of-the-art method FlexMatch firstly demonstrated that correctly estimating learning status is crucial for semi-supervised learning (SSL). However, the estimation method proposed by FlexMatch does not take into account imbalanced data, which is the common case for 3D semi-supervised learning. To address this problem, we practically demonstrate that unlabeled data class-level confidence can represent the learning status in the 3D imbalanced dataset. Based on this finding, we present a novel class-level confidence based 3D SSL method. Firstly, a dynamic thresholding strategy is proposed to utilize more unlabeled data, especially for low learning status classes. Then, a re-sampling strategy is designed to avoid biasing toward high learning status classes, which dynamically changes the sampling probability of each class. To show the effectiveness of our method in 3D SSL tasks, we conduct extensive experiments on 3D SSL classification and detection tasks. Our method significantly outperforms state-of-the-art counterparts for both 3D SSL classification and detection tasks in all datasets.



### Computational pathology in renal disease: a comprehensive perspective
- **Arxiv ID**: http://arxiv.org/abs/2210.10162v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, J.3; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2210.10162v1)
- **Published**: 2022-10-18 21:05:40+00:00
- **Updated**: 2022-10-18 21:05:40+00:00
- **Authors**: Manuel Cossio
- **Comment**: 10 pages, 7 figures and 3 tables
- **Journal**: None
- **Summary**: Computational pathology is a field that has complemented various subspecialties of diagnostic pathology over the last few years. In this article a brief analyzis the different applications in nephrology is developed. To begin, an overview of the different forms of image production is provided. To continue, the most frequent applications of computer vision models, the salient features of the different clinical applications, and the data protection considerations encountered are described. To finish the development, I delve into the interpretability of these applications, expanding in depth on the three dimensions of this area.



### MedCLIP: Contrastive Learning from Unpaired Medical Images and Text
- **Arxiv ID**: http://arxiv.org/abs/2210.10163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.10163v1)
- **Published**: 2022-10-18 21:06:29+00:00
- **Updated**: 2022-10-18 21:06:29+00:00
- **Authors**: Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun
- **Comment**: EMNLP 2022
- **Journal**: None
- **Summary**: Existing vision-text contrastive learning like CLIP aims to match the paired image and caption embeddings while pushing others apart, which improves representation transferability and supports zero-shot prediction. However, medical image-text datasets are orders of magnitude below the general images and captions from the internet. Moreover, previous methods encounter many false negatives, i.e., images and reports from separate patients probably carry the same semantics but are wrongly treated as negatives. In this paper, we decouple images and texts for multimodal contrastive learning thus scaling the usable training data in a combinatorial magnitude with low cost. We also propose to replace the InfoNCE loss with semantic matching loss based on medical knowledge to eliminate false negatives in contrastive learning. We prove that MedCLIP is a simple yet effective framework: it outperforms state-of-the-art methods on zero-shot prediction, supervised classification, and image-text retrieval. Surprisingly, we observe that with only 20K pre-training data, MedCLIP wins over the state-of-the-art method (using around 200K data). Our code is available at https://github.com/RyanWangZf/MedCLIP.



### Intra-Source Style Augmentation for Improved Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2210.10175v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10175v2)
- **Published**: 2022-10-18 21:33:25+00:00
- **Updated**: 2023-05-29 07:19:55+00:00
- **Authors**: Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva
- **Comment**: Accepted at WACV 2023. Code is available at
  https://github.com/boschresearch/ISSA
- **Journal**: None
- **Summary**: The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by $3\%$ mIoU in Cityscapes to Dark Z\"urich.



### Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2210.10176v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10176v2)
- **Published**: 2022-10-18 21:39:24+00:00
- **Updated**: 2022-10-20 20:48:49+00:00
- **Authors**: Jialin Wu, Raymond J. Mooney
- **Comment**: EMNLP 2022
- **Journal**: None
- **Summary**: Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a two-stage framework that first retrieves external knowledge given the visual question and then predicts the answer based on the retrieved content. However, the retrieved knowledge is often inadequate. Retrievals are frequently too general and fail to cover specific knowledge needed to answer the question. Also, the naturally available supervision (whether the passage contains the correct answer) is weak and does not guarantee question relevancy. To address these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that provides stronger supervision during training and recognizes question-relevant entities to help retrieve more specific knowledge. Experiments show that our EnFoRe model achieves superior retrieval performance on OK-VQA, the currently largest outside-knowledge VQA dataset. We also combine the retrieved knowledge with state-of-the-art VQA models, and achieve a new state-of-the-art performance on OK-VQA.



### Domain Adaptation in 3D Object Detection with Gradual Batch Alternation Training
- **Arxiv ID**: http://arxiv.org/abs/2210.10180v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10180v2)
- **Published**: 2022-10-18 22:03:37+00:00
- **Updated**: 2023-08-05 01:29:51+00:00
- **Authors**: Mrigank Rochan, Xingxin Chen, Alaap Grandhi, Eduardo R. Corral-Soto, Bingbing Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of domain adaptation in LiDAR-based 3D object detection. Towards this, we propose a simple yet effective training strategy called Gradual Batch Alternation that can adapt from a large labeled source domain to an insufficiently labeled target domain. The idea is to initiate the training with the batch of samples from the source and target domain data in an alternate fashion, but then gradually reduce the amount of the source domain data over time as the training progresses. This way the model slowly shifts towards the target domain and eventually better adapt to it. The domain adaptation experiments for 3D object detection on four benchmark autonomous driving datasets, namely ONCE, PandaSet, Waymo, and nuScenes, demonstrate significant performance gains over prior arts and strong baselines.



### Landmark Enforcement and Style Manipulation for Generative Morphing
- **Arxiv ID**: http://arxiv.org/abs/2210.10182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10182v1)
- **Published**: 2022-10-18 22:10:25+00:00
- **Updated**: 2022-10-18 22:10:25+00:00
- **Authors**: Samuel Price, Sobhan Soleymani, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Morph images threaten Facial Recognition Systems (FRS) by presenting as multiple individuals, allowing an adversary to swap identities with another subject. Morph generation using generative adversarial networks (GANs) results in high-quality morphs unaffected by the spatial artifacts caused by landmark-based methods, but there is an apparent loss in identity with standard GAN-based morphing methods. In this paper, we propose a novel StyleGAN morph generation technique by introducing a landmark enforcement method to resolve this issue. Considering this method, we aim to enforce the landmarks of the morph image to represent the spatial average of the landmarks of the bona fide faces and subsequently the morph images to inherit the geometric identity of both bona fide faces. Exploration of the latent space of our model is conducted using Principal Component Analysis (PCA) to accentuate the effect of both the bona fide faces on the morphed latent representation and address the identity loss issue with latent domain averaging. Additionally, to improve high frequency reconstruction in the morphs, we study the train-ability of the noise input for the StyleGAN2 model.



### Aligning MAGMA by Few-Shot Learning and Finetuning
- **Arxiv ID**: http://arxiv.org/abs/2210.14161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.14161v1)
- **Published**: 2022-10-18 22:20:47+00:00
- **Updated**: 2022-10-18 22:20:47+00:00
- **Authors**: Jean-Charles Layoun, Alexis Roger, Irina Rish
- **Comment**: Accepted by the Montreal AI Symposium conference in 2022
- **Journal**: None
- **Summary**: The goal of vision-language modeling is to allow models to tie language understanding with visual inputs. The aim of this paper is to evaluate and align the Visual Language Model (VLM) called Multimodal Augmentation of Generative Models through Adapter-based finetuning (MAGMA) with human values. MAGMA is a VLM that is capable of image captioning and visual question-answering. We will evaluate its alignment in three different scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the checkpoint provided by Hugging Face. Then, we measure if few-shot learning manages to improve the results. Finally, we finetune the model on aligned examples and evaluate its behavior.



### Rethinking Prototypical Contrastive Learning through Alignment, Uniformity and Correlation
- **Arxiv ID**: http://arxiv.org/abs/2210.10194v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10194v1)
- **Published**: 2022-10-18 22:33:12+00:00
- **Updated**: 2022-10-18 22:33:12+00:00
- **Authors**: Shentong Mo, Zhun Sun, Chao Li
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Contrastive self-supervised learning (CSL) with a prototypical regularization has been introduced in learning meaningful representations for downstream tasks that require strong semantic information. However, to optimize CSL with a loss that performs the prototypical regularization aggressively, e.g., the ProtoNCE loss, might cause the "coagulation" of examples in the embedding space. That is, the intra-prototype diversity of samples collapses to trivial solutions for their prototype being well-separated from others. Motivated by previous works, we propose to mitigate this phenomenon by learning Prototypical representation through Alignment, Uniformity and Correlation (PAUC). Specifically, the ordinary ProtoNCE loss is revised with: (1) an alignment loss that pulls embeddings from positive prototypes together; (2) a uniformity loss that distributes the prototypical level features uniformly; (3) a correlation loss that increases the diversity and discriminability between prototypical level features. We conduct extensive experiments on various benchmarks where the results demonstrate the effectiveness of our method in improving the quality of prototypical contrastive representations. Particularly, in the classification down-stream tasks with linear probes, our proposed method outperforms the state-of-the-art instance-wise and prototypical contrastive learning methods on the ImageNet-100 dataset by 2.96% and the ImageNet-1K dataset by 2.46% under the same settings of batch size and epochs.



### BirdSoundsDenoising: Deep Visual Audio Denoising for Bird Sounds
- **Arxiv ID**: http://arxiv.org/abs/2210.10196v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.10196v1)
- **Published**: 2022-10-18 22:37:25+00:00
- **Updated**: 2022-10-18 22:37:25+00:00
- **Authors**: Youshan Zhang, Jialu Li
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Audio denoising has been explored for decades using both traditional and deep learning-based methods. However, these methods are still limited to either manually added artificial noise or lower denoised audio quality. To overcome these challenges, we collect a large-scale natural noise bird sound dataset. We are the first to transfer the audio denoising problem into an image segmentation problem and propose a deep visual audio denoising (DVAD) model. With a total of 14,120 audio images, we develop an audio ImageMask tool and propose to use a few-shot generalization strategy to label these images. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance. We also show that our method can be easily generalized to speech denoising, audio separation, audio enhancement, and noise estimation.



### Optimizing Hierarchical Image VAEs for Sample Quality
- **Arxiv ID**: http://arxiv.org/abs/2210.10205v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10205v1)
- **Published**: 2022-10-18 23:10:58+00:00
- **Updated**: 2022-10-18 23:10:58+00:00
- **Authors**: Eric Luhman, Troy Luhman
- **Comment**: 21 pages, 12 figures
- **Journal**: None
- **Summary**: While hierarchical variational autoencoders (VAEs) have achieved great density estimation on image modeling tasks, samples from their prior tend to look less convincing than models with similar log-likelihood. We attribute this to learned representations that over-emphasize compressing imperceptible details of the image. To address this, we introduce a KL-reweighting strategy to control the amount of infor mation in each latent group, and employ a Gaussian output layer to reduce sharpness in the learning objective. To trade off image diversity for fidelity, we additionally introduce a classifier-free guidance strategy for hierarchical VAEs. We demonstrate the effectiveness of these techniques in our experiments. Code is available at https://github.com/tcl9876/visual-vae.



### Exclusive Supermask Subnetwork Training for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.10209v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10209v2)
- **Published**: 2022-10-18 23:27:07+00:00
- **Updated**: 2023-07-05 16:57:43+00:00
- **Authors**: Prateek Yadav, Mohit Bansal
- **Comment**: ACL Findings 2023 (17 pages, 7 figures)
- **Journal**: None
- **Summary**: Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT) module that utilizes previously acquired knowledge to learn new tasks better and faster. We demonstrate that ExSSNeT outperforms strong previous methods on both NLP and Vision domains while preventing forgetting. Moreover, ExSSNeT is particularly advantageous for sparse masks that activate 2-10% of the model parameters, resulting in an average improvement of 8.3% over SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code is available at https://github.com/prateeky2806/exessnet.



### Multi-Source Transformer Architectures for Audiovisual Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.10212v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10212v1)
- **Published**: 2022-10-18 23:42:42+00:00
- **Updated**: 2022-10-18 23:42:42+00:00
- **Authors**: Wim Boes, Hugo Van hamme
- **Comment**: Technical report of submission to DCASE 2021 Challenge Task 1B
- **Journal**: None
- **Summary**: In this technical report, the systems we submitted for subtask 1B of the DCASE 2021 challenge, regarding audiovisual scene classification, are described in detail. They are essentially multi-source transformers employing a combination of auditory and visual features to make predictions. These models are evaluated utilizing the macro-averaged multi-class cross-entropy and accuracy metrics.   In terms of the macro-averaged multi-class cross-entropy, our best model achieved a score of 0.620 on the validation data. This is slightly better than the performance of the baseline system (0.658).   With regard to the accuracy measure, our best model achieved a score of 77.1\% on the validation data, which is about the same as the performance obtained by the baseline system (77.0\%).




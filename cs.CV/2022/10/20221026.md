# Arxiv Papers in cs.CV on 2022-10-26
### Can Transformer Attention Spread Give Insights Into Uncertainty of Detected and Tracked Objects?
- **Arxiv ID**: http://arxiv.org/abs/2210.14391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14391v1)
- **Published**: 2022-10-26 00:05:16+00:00
- **Updated**: 2022-10-26 00:05:16+00:00
- **Authors**: Felicia Ruppel, Florian Faion, Claudius Gläser, Klaus Dietmayer
- **Comment**: Accepted for publication at the 2022 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) PNARUDE workshop, Oct 27,
  2022, in Kyoto, Japan
- **Journal**: None
- **Summary**: Transformers have recently been utilized to perform object detection and tracking in the context of autonomous driving. One unique characteristic of these models is that attention weights are computed in each forward pass, giving insights into the model's interior, in particular, which part of the input data it deemed interesting for the given task. Such an attention matrix with the input grid is available for each detected (or tracked) object in every transformer decoder layer. In this work, we investigate the distribution of these attention weights: How do they change through the decoder layers and through the lifetime of a track? Can they be used to infer additional information about an object, such as a detection uncertainty? Especially in unstructured environments, or environments that were not common during training, a reliable measure of detection uncertainty is crucial to decide whether the system can still be trusted or not.



### Zero-Shot Learning of a Conditional Generative Adversarial Network for Data-Free Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/2210.14392v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2210.14392v1)
- **Published**: 2022-10-26 00:05:57+00:00
- **Updated**: 2022-10-26 00:05:57+00:00
- **Authors**: Yoojin Choi, Mostafa El-Khamy, Jungwon Lee
- **Comment**: IEEE ICIP 2021
- **Journal**: None
- **Summary**: We propose a novel method for training a conditional generative adversarial network (CGAN) without the use of training data, called zero-shot learning of a CGAN (ZS-CGAN). Zero-shot learning of a conditional generator only needs a pre-trained discriminative (classification) model and does not need any training data. In particular, the conditional generator is trained to produce labeled synthetic samples whose characteristics mimic the original training data by using the statistics stored in the batch normalization layers of the pre-trained model. We show the usefulness of ZS-CGAN in data-free quantization of deep neural networks. We achieved the state-of-the-art data-free network quantization of the ResNet and MobileNet classification models trained on the ImageNet dataset. Data-free quantization using ZS-CGAN showed a minimal loss in accuracy compared to that obtained by conventional data-dependent quantization.



### IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text
- **Arxiv ID**: http://arxiv.org/abs/2210.14395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14395v1)
- **Published**: 2022-10-26 00:22:41+00:00
- **Updated**: 2022-10-26 00:22:41+00:00
- **Authors**: Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Alireza Dirafzoon, Aparajita Saraf, Amy Bearman, Babak Damavandi
- **Comment**: None
- **Journal**: None
- **Summary**: We present IMU2CLIP, a novel pre-training approach to align Inertial Measurement Unit (IMU) motion sensor recordings with video and text, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to translate human motions (as measured by IMU sensors) into their corresponding textual descriptions and videos -- while preserving the transitivity across these modalities.   We explore several new IMU-based applications that IMU2CLIP enables, such as motion-based media retrieval and natural language reasoning tasks with motion data. In addition, we show that IMU2CLIP can significantly improve the downstream performance when fine-tuned for each application (e.g. activity recognition), demonstrating the universal usage of IMU2CLIP as a new pre-trained resource. Our code will be made publicly available.



### Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.04325v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2211.04325v1)
- **Published**: 2022-10-26 00:28:40+00:00
- **Updated**: 2022-10-26 00:28:40+00:00
- **Authors**: Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, Anson Ho
- **Comment**: None
- **Journal**: None
- **Summary**: We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.



### Adaptive Test-Time Defense with the Manifold Hypothesis
- **Arxiv ID**: http://arxiv.org/abs/2210.14404v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14404v3)
- **Published**: 2022-10-26 01:00:57+00:00
- **Updated**: 2022-11-17 18:50:11+00:00
- **Authors**: Zhaoyuan Yang, Zhiwei Xu, Jing Zhang, Richard Hartley, Peter Tu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we formulate a novel framework of adversarial robustness using the manifold hypothesis. Our framework provides sufficient conditions for defending against adversarial examples. We develop a test-time defense method with variational inference and our formulation. The developed approach combines manifold learning with variational inference to provide adversarial robustness without the need for adversarial training. We show that our approach can provide adversarial robustness even if attackers are aware of the existence of test-time defense. In addition, our approach can also serve as a test-time defense mechanism for variational autoencoders.



### Adversarially Robust Medical Classification via Attentive Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.14405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14405v1)
- **Published**: 2022-10-26 01:04:31+00:00
- **Updated**: 2022-10-26 01:04:31+00:00
- **Authors**: Isaac Wasserman
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network-based medical image classifiers have been shown to be especially susceptible to adversarial examples. Such instabilities are likely to be unacceptable in the future of automated diagnoses. Though statistical adversarial example detection methods have proven to be effective defense mechanisms, additional research is necessary that investigates the fundamental vulnerabilities of deep-learning-based systems and how best to build models that jointly maximize traditional and robust accuracy. This paper presents the inclusion of attention mechanisms in CNN-based medical image classifiers as a reliable and effective strategy for increasing robust accuracy without sacrifice. This method is able to increase robust accuracy by up to 16% in typical adversarial scenarios and up to 2700% in extreme cases.



### Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes
- **Arxiv ID**: http://arxiv.org/abs/2210.14410v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14410v2)
- **Published**: 2022-10-26 01:23:33+00:00
- **Updated**: 2023-05-10 22:33:51+00:00
- **Authors**: Sina Baharlouei, Fatemeh Sheikholeslami, Meisam Razaviyayn, Zico Kolter
- **Comment**: 20 pages, 6 figures
- **Journal**: International Conference on Artificial Intelligence and
  Statistics, PMLR 2023
- **Summary**: This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the "abstain" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to "model degeneracy", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.



### RBP-DIP: High-Quality CT Reconstruction Using an Untrained Neural Network with Residual Back Projection and Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2210.14416v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14416v1)
- **Published**: 2022-10-26 01:58:09+00:00
- **Updated**: 2022-10-26 01:58:09+00:00
- **Authors**: Ziyu Shu, Alireza Entezari
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network related methods, due to their unprecedented success in image processing, have emerged as a new set of tools in CT reconstruction with the potential to change the field. However, the lack of high-quality training data and theoretical guarantees, together with increasingly complicated network structures, make its implementation impractical. In this paper, we present a new framework (RBP-DIP) based on Deep Image Prior (DIP) and a special residual back projection (RBP) connection to tackle these challenges. Comparing to other pre-trained neural network related algorithms, the proposed framework is closer to an iterative reconstruction (IR) algorithm as it requires no training data or training process. In that case, the proposed framework can be altered (e.g, different hyperparameters and constraints) on demand, adapting to different conditions (e.g, different imaged objects, imaging instruments, and noise levels) without retraining. Experiments show that the proposed framework has significant improvements over other state-of-the-art conventional methods, as well as pre-trained and untrained models with similar network structures, especially under sparse-view, limited-angle, and low-dose conditions.



### Discovering Design Concepts for CAD Sketches
- **Arxiv ID**: http://arxiv.org/abs/2210.14451v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14451v1)
- **Published**: 2022-10-26 03:53:33+00:00
- **Updated**: 2022-10-26 03:53:33+00:00
- **Authors**: Yuezhi Yang, Hao Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Sketch design concepts are recurring patterns found in parametric CAD sketches. Though rarely explicitly formalized by the CAD designers, these concepts are implicitly used in design for modularity and regularity. In this paper, we propose a learning based approach that discovers the modular concepts by induction over raw sketches. We propose the dual implicit-explicit representation of concept structures that allows implicit detection and explicit generation, and the separation of structure generation and parameter instantiation for parameterized concept generation, to learn modular concepts by end-to-end training. We demonstrate the design concept learning on a large scale CAD sketch dataset and show its applications for design intent interpretation and auto-completion.



### Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization
- **Arxiv ID**: http://arxiv.org/abs/2210.14457v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14457v2)
- **Published**: 2022-10-26 04:02:29+00:00
- **Updated**: 2023-03-10 02:45:34+00:00
- **Authors**: Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, Haoqiang Fan, Zheng Ge
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: In this paper, we analyse the generalization ability of binary classifiers for the task of deepfake detection. We find that the stumbling block to their generalization is caused by the unexpected learned identity representation on images. Termed as the Implicit Identity Leakage, this phenomenon has been qualitatively and quantitatively verified among various DNNs. Furthermore, based on such understanding, we propose a simple yet effective method named the ID-unaware Deepfake Detection Model to reduce the influence of this phenomenon. Extensive experimental results demonstrate that our method outperforms the state-of-the-art in both in-dataset and cross-dataset evaluation. The code is available at https://github.com/megvii-research/CADDM.



### PredNAS: A Universal and Sample Efficient Neural Architecture Search Framework
- **Arxiv ID**: http://arxiv.org/abs/2210.14460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14460v1)
- **Published**: 2022-10-26 04:15:58+00:00
- **Updated**: 2022-10-26 04:15:58+00:00
- **Authors**: Liuchun Yuan, Zehao Huang, Naiyan Wang
- **Comment**: 11 Pages,4 figures
- **Journal**: None
- **Summary**: In this paper, we present a general and effective framework for Neural Architecture Search (NAS), named PredNAS. The motivation is that given a differentiable performance estimation function, we can directly optimize the architecture towards higher performance by simple gradient ascent. Specifically, we adopt a neural predictor as the performance predictor. Surprisingly, PredNAS can achieve state-of-the-art performances on NAS benchmarks with only a few training samples (less than 100). To validate the universality of our method, we also apply our method on large-scale tasks and compare our method with RegNet on ImageNet and YOLOX on MSCOCO. The results demonstrate that our PredNAS can explore novel architectures with competitive performances under specific computational complexity constraints.



### TPFNet: A Novel Text In-painting Transformer for Text Removal
- **Arxiv ID**: http://arxiv.org/abs/2210.14461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.14461v2)
- **Published**: 2022-10-26 04:16:50+00:00
- **Updated**: 2022-10-27 14:14:55+00:00
- **Authors**: Onkar Susladkar, Dhruv Makwana, Gayatri Deshmukh, Sparsh Mittal, Sai Chandra Teja R, Rekha Singhal
- **Comment**: 10 pages, 5 figures, 5 tables, Neurips Proceedings
- **Journal**: None
- **Summary**: Text erasure from an image is helpful for various tasks such as image editing and privacy preservation. In this paper, we present TPFNet, a novel one-stage (end-toend) network for text removal from images. Our network has two parts: feature synthesis and image generation. Since noise can be more effectively removed from low-resolution images, part 1 operates on low-resolution images. The output of part 1 is a low-resolution text-free image. Part 2 uses the features learned in part 1 to predict a high-resolution text-free image. In part 1, we use "pyramidal vision transformer" (PVT) as the encoder. Further, we use a novel multi-headed decoder that generates a high-pass filtered image and a segmentation map, in addition to a text-free image. The segmentation branch helps locate the text precisely, and the high-pass branch helps in learning the image structure. To precisely locate the text, TPFNet employs an adversarial loss that is conditional on the segmentation map rather than the input image. On Oxford, SCUT, and SCUT-EnsText datasets, our network outperforms recently proposed networks on nearly all the metrics. For example, on SCUT-EnsText dataset, TPFNet has a PSNR (higher is better) of 39.0 and text-detection precision (lower is better) of 21.1, compared to the best previous technique, which has a PSNR of 32.3 and precision of 53.2. The source code can be obtained from https://github.com/CandleLabAI/TPFNet



### Reconstruction from edge image combined with color and gradient difference for industrial surface anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2210.14485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14485v1)
- **Published**: 2022-10-26 05:21:43+00:00
- **Updated**: 2022-10-26 05:21:43+00:00
- **Authors**: Tongkun Liu, Bing Li, Zhuo Zhao, Xiao Du, Bingke Jiang, Leqi Geng
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Reconstruction-based methods are widely explored in industrial visual anomaly detection. Such methods commonly require the model to well reconstruct the normal patterns but fail in the anomalies, and thus the anomalies can be detected by evaluating the reconstruction errors. However, in practice, it's usually difficult to control the generalization boundary of the model. The model with an overly strong generalization capability can even well reconstruct the abnormal regions, making them less distinguishable, while the model with a poor generalization capability can not reconstruct those changeable high-frequency components in the normal regions, which ultimately leads to false positives. To tackle the above issue, we propose a new reconstruction network where we reconstruct the original RGB image from its gray value edges (EdgRec). Specifically, this is achieved by an UNet-type denoising autoencoder with skip connections. The input edge and skip connections can well preserve the high-frequency information in the original image. Meanwhile, the proposed restoration task can force the network to memorize the normal low-frequency and color information. Besides, the denoising design can prevent the model from directly copying the original high-frequent components. To evaluate the anomalies, we further propose a new interpretable hand-crafted evaluation function that considers both the color and gradient differences. Our method achieves competitive results on the challenging benchmark MVTec AD (97.8\% for detection and 97.7\% for localization, AUROC). In addition, we conduct experiments on the MVTec 3D-AD dataset and show convincing results using RGB images only. Our code will be available at https://github.com/liutongkun/EdgRec.



### SimpleDG: Simple Domain Generalization Baseline without Bells and Whistles
- **Arxiv ID**: http://arxiv.org/abs/2210.14507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14507v1)
- **Published**: 2022-10-26 06:35:31+00:00
- **Updated**: 2022-10-26 06:35:31+00:00
- **Authors**: Zhi Lv, Bo Lin, Siyuan Liang, Lihua Wang, Mochen Yu, Yao Tang, Jiajun Liang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple domain generalization baseline, which wins second place in both the common context generalization track and the hybrid context generalization track respectively in NICO CHALLENGE 2022. We verify the founding in recent literature, domainbed, that ERM is a strong baseline compared to recent state-of-the-art domain generalization methods and propose SimpleDG which includes several simple yet effective designs that further boost generalization performance. Code is available at https://github.com/megvii-research/SimpleDG



### End-to-End Multimodal Representation Learning for Video Dialog
- **Arxiv ID**: http://arxiv.org/abs/2210.14512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14512v1)
- **Published**: 2022-10-26 06:50:07+00:00
- **Updated**: 2022-10-26 06:50:07+00:00
- **Authors**: Huda Alamri, Anthony Bilic, Michael Hu, Apoorva Beedu, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based dialog task is a challenging multimodal learning task that has received increasing attention over the past few years with state-of-the-art obtaining new performance records. This progress is largely powered by the adaptation of the more powerful transformer-based language encoders. Despite this progress, existing approaches do not effectively utilize visual features to help solve tasks. Recent studies show that state-of-the-art models are biased toward textual information rather than visual cues. In order to better leverage the available visual information, this study proposes a new framework that combines 3D-CNN network and transformer-based networks into a single visual encoder to extract more robust semantic representations from videos. The visual encoder is jointly trained end-to-end with other input modalities such as text and audio. Experiments on the AVSD task show significant improvement over baselines in both generative and retrieval tasks.



### RGB-T Semantic Segmentation with Location, Activation, and Sharpening
- **Arxiv ID**: http://arxiv.org/abs/2210.14530v1
- **DOI**: 10.1109/TCSVT.2022.3208833
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14530v1)
- **Published**: 2022-10-26 07:42:34+00:00
- **Updated**: 2022-10-26 07:42:34+00:00
- **Authors**: Gongyang Li, Yike Wang, Zhi Liu, Xinpeng Zhang, Dan Zeng
- **Comment**: 12 pages, 7 figures, Accepted by IEEE Transactions on Circuits and
  Systems for Video Technology 2022
- **Journal**: None
- **Summary**: Semantic segmentation is important for scene understanding. To address the scenes of adverse illumination conditions of natural images, thermal infrared (TIR) images are introduced. Most existing RGB-T semantic segmentation methods follow three cross-modal fusion paradigms, i.e. encoder fusion, decoder fusion, and feature fusion. Some methods, unfortunately, ignore the properties of RGB and TIR features or the properties of features at different levels. In this paper, we propose a novel feature fusion-based network for RGB-T semantic segmentation, named \emph{LASNet}, which follows three steps of location, activation, and sharpening. The highlight of LASNet is that we fully consider the characteristics of cross-modal features at different levels, and accordingly propose three specific modules for better segmentation. Concretely, we propose a Collaborative Location Module (CLM) for high-level semantic features, aiming to locate all potential objects. We propose a Complementary Activation Module for middle-level features, aiming to activate exact regions of different objects. We propose an Edge Sharpening Module (ESM) for low-level texture features, aiming to sharpen the edges of objects. Furthermore, in the training phase, we attach a location supervision and an edge supervision after CLM and ESM, respectively, and impose two semantic supervisions in the decoder part to facilitate network convergence. Experimental results on two public datasets demonstrate that the superiority of our LASNet over relevant state-of-the-art methods. The code and results of our method are available at https://github.com/MathLee/LASNet.



### Compressing And Debiasing Vision-Language Pre-Trained Models for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2210.14558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14558v1)
- **Published**: 2022-10-26 08:25:03+00:00
- **Updated**: 2022-10-26 08:25:03+00:00
- **Authors**: Qingyi Si, Yuanxin Liu, Zheng Lin, Peng Fu, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the excellent performance of large-scale vision-language pre-trained models (VLPs) on conventional visual question answering task, they still suffer from two problems: First, VLPs tend to rely on language biases in datasets and fail to generalize to out-of-distribution (OOD) data. Second, they are inefficient in terms of memory footprint and computation. Although promising progress has been made in both problems, most existing works tackle them independently. To facilitate the application of VLP to VQA tasks, it is imperative to jointly study VLP compression and OOD robustness, which, however, has not yet been explored. In this paper, we investigate whether a VLP can be compressed and debiased simultaneously by searching sparse and robust subnetworks. To this end, we conduct extensive experiments with LXMERT, a representative VLP, on the OOD dataset VQA-CP v2. We systematically study the design of a training and compression pipeline to search the subnetworks, as well as the assignment of sparsity to different modality-specific modules. Our results show that there indeed exist sparse and robust LXMERT subnetworks, which significantly outperform the full model (without debiasing) with much fewer parameters. These subnetworks also exceed the current SoTA debiasing models with comparable or fewer parameters. We will release the codes on publication.



### FairCLIP: Social Bias Elimination based on Attribute Prototype Learning and Representation Neutralization
- **Arxiv ID**: http://arxiv.org/abs/2210.14562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14562v1)
- **Published**: 2022-10-26 08:46:24+00:00
- **Updated**: 2022-10-26 08:46:24+00:00
- **Authors**: Junyang Wang, Yi Zhang, Jitao Sang
- **Comment**: None
- **Journal**: None
- **Summary**: The Vision-Language Pre-training (VLP) models like CLIP have gained popularity in recent years. However, many works found that the social biases hidden in CLIP easily manifest in downstream tasks, especially in image retrieval, which can have harmful effects on human society. In this work, we propose FairCLIP to eliminate the social bias in CLIP-based image retrieval without damaging the retrieval performance achieving the compatibility between the debiasing effect and the retrieval performance. FairCLIP is divided into two steps: Attribute Prototype Learning (APL) and Representation Neutralization (RN). In the first step, we extract the concepts needed for debiasing in CLIP. We use the query with learnable word vector prefixes as the extraction structure. In the second step, we first divide the attributes into target and bias attributes. By analysis, we find that both attributes have an impact on the bias. Therefore, we try to eliminate the bias by using Re-Representation Matrix (RRM) to achieve the neutralization of the representation. We compare the debiasing effect and retrieval performance with other methods, and experiments demonstrate that FairCLIP can achieve the best compatibility. Although FairCLIP is used to eliminate bias in image retrieval, it achieves the neutralization of the representation which is common to all CLIP downstream tasks. This means that FairCLIP can be applied as a general debiasing method for other fairness issues related to CLIP.



### Towards the Detection of Diffusion Model Deepfakes
- **Arxiv ID**: http://arxiv.org/abs/2210.14571v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14571v3)
- **Published**: 2022-10-26 09:01:19+00:00
- **Updated**: 2023-05-15 10:16:58+00:00
- **Authors**: Jonas Ricker, Simon Damm, Thorsten Holz, Asja Fischer
- **Comment**: 27 pages, 25 figures
- **Journal**: None
- **Summary**: Diffusion models (DMs) have recently emerged as a promising method in image synthesis. However, to date, only little attention has been paid to the detection of DM-generated images, which is critical to prevent adverse impacts on our society. In this work, we address this pressing challenge from two different angles: First, we evaluate the performance of state-of-the-art detectors, which are very effective against images generated by generative adversarial networks (GANs), on a variety of DMs. Second, we analyze DM-generated images in the frequency domain and study different factors that influence the spectral properties of these images. Most importantly, we demonstrate that GANs and DMs produce images with different characteristics, which requires adaptation of existing classifiers to ensure reliable detection. We are convinced that this work provides the foundation and starting point for further research on effective detection of DM-generated images.



### Compressed Sensing MRI Reconstruction Regularized by VAEs with Structured Image Covariance
- **Arxiv ID**: http://arxiv.org/abs/2210.14586v2
- **DOI**: 10.1088/1361-6560/ace49a
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.14586v2)
- **Published**: 2022-10-26 09:51:49+00:00
- **Updated**: 2023-06-16 12:10:44+00:00
- **Authors**: Margaret Duff, Ivor J. A. Simpson, Matthias J. Ehrhardt, Neill D. F. Campbell
- **Comment**: None
- **Journal**: Phys. Med. Biol. 68 16500 (2023)
- **Summary**: Objective: This paper investigates how generative models, trained on ground-truth images, can be used \changes{as} priors for inverse problems, penalizing reconstructions far from images the generator can produce. The aim is that learned regularization will provide complex data-driven priors to inverse problems while still retaining the control and insight of a variational regularization method. Moreover, unsupervised learning, without paired training data, allows the learned regularizer to remain flexible to changes in the forward problem such as noise level, sampling pattern or coil sensitivities in MRI.   Approach: We utilize variational autoencoders (VAEs) that generate not only an image but also a covariance uncertainty matrix for each image. The covariance can model changing uncertainty dependencies caused by structure in the image, such as edges or objects, and provides a new distance metric from the manifold of learned images.   Main results: We evaluate these novel generative regularizers on retrospectively sub-sampled real-valued MRI measurements from the fastMRI dataset. We compare our proposed learned regularization against other unlearned regularization approaches and unsupervised and supervised deep learning methods.   Significance: Our results show that the proposed method is competitive with other state-of-the-art methods and behaves consistently with changing sampling patterns and noise levels.



### A Stronger Baseline For Automatic Pfirrmann Grading Of Lumbar Spine MRI Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.14597v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14597v1)
- **Published**: 2022-10-26 10:12:21+00:00
- **Updated**: 2022-10-26 10:12:21+00:00
- **Authors**: Narasimharao Kowlagi, Huy Hoang Nguyen, Terence McSweeney, Simo Saarakkala, Juhani määttä, Jaro Karppinen, Aleksei Tiulpin
- **Comment**: 5 pages, under review
- **Journal**: None
- **Summary**: This paper addresses the challenge of grading visual features in lumbar spine MRI using Deep Learning. Such a method is essential for the automatic quantification of structural changes in the spine, which is valuable for understanding low back pain. Multiple recent studies investigated different architecture designs, and the most recent success has been attributed to the use of transformer architectures. In this work, we argue that with a well-tuned three-stage pipeline comprising semantic segmentation, localization, and classification, convolutional networks outperform the state-of-the-art approaches. We conducted an ablation study of the existing methods in a population cohort, and report performance generalization across various subgroups. Our code is publicly available to advance research on disc degeneration and low back pain.



### End-to-end Tracking with a Multi-query Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.14601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14601v1)
- **Published**: 2022-10-26 10:19:37+00:00
- **Updated**: 2022-10-26 10:19:37+00:00
- **Authors**: Bruno Korbar, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple-object tracking (MOT) is a challenging task that requires simultaneous reasoning about location, appearance, and identity of the objects in the scene over time. Our aim in this paper is to move beyond tracking-by-detection approaches, that perform well on datasets where the object classes are known, to class-agnostic tracking that performs well also for unknown object classes.To this end, we make the following three contributions: first, we introduce {\em semantic detector queries} that enable an object to be localized by specifying its approximate position, or its appearance, or both; second, we use these queries within an auto-regressive framework for tracking, and propose a multi-query tracking transformer (\textit{MQT}) model for simultaneous tracking and appearance-based re-identification (reID) based on the transformer architecture with deformable attention. This formulation allows the tracker to operate in a class-agnostic manner, and the model can be trained end-to-end; finally, we demonstrate that \textit{MQT} performs competitively on standard MOT benchmarks, outperforms all baselines on generalised-MOT, and generalises well to a much harder tracking problems such as tracking any object on the TAO dataset.



### A novel filter based on three variables mutual information for dimensionality reduction and classification of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2210.14609v1
- **DOI**: 10.1109/EITech.2016.7519622
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14609v1)
- **Published**: 2022-10-26 10:29:00+00:00
- **Updated**: 2022-10-26 10:29:00+00:00
- **Authors**: Asma Elmaizi, Elkebir Sarhrouni, Ahmed hammouch, Chafik Nacir
- **Comment**: None
- **Journal**: 2016 International Conference on Electrical and Information
  Technologies (ICEIT) -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84992221810&partnerID=MN8TOARS
- **Summary**: The high dimensionality of hyperspectral images (HSI) that contains more than hundred bands (images) for the same region called Ground Truth Map, often imposes a heavy computational burden for image processing and complicates the learning process. In fact, the removal of irrelevant, noisy and redundant bands helps increase the classification accuracy. Band selection filter based on "Mutual Information" is a common technique for dimensionality reduction. In this paper, a categorization of dimensionality reduction methods according to the evaluation process is presented. Moreover, a new filter approach based on three variables mutual information is developed in order to measure band correlation for classification, it considers not only bands relevance but also bands interaction. The proposed approach is compared to a reproduced filter algorithm based on mutual information. Experimental results on HSI AVIRIS 92AV3C have shown that the proposed approach is very competitive, effective and outperforms the reproduced filter strategy performance.   Keywords - Hyperspectral images, Classification, band Selection, Three variables Mutual Information, information gain.



### Automatic Diagnosis of Myocarditis Disease in Cardiac MRI Modality using Deep Transformers and Explainable Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2210.14611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14611v1)
- **Published**: 2022-10-26 10:34:20+00:00
- **Updated**: 2022-10-26 10:34:20+00:00
- **Authors**: Mahboobeh Jafari, Afshin Shoeibi, Navid Ghassemi, Jonathan Heras, Abbas Khosravi, Sai Ho Ling, Roohallah Alizadehsani, Amin Beheshti, Yu-Dong Zhang, Shui-Hua Wang, Juan M. Gorriz, U. Rajendra Acharya, Hamid Alinejad Rokny
- **Comment**: None
- **Journal**: None
- **Summary**: Myocarditis is among the most important cardiovascular diseases (CVDs), endangering the health of many individuals by damaging the myocardium. Microbes and viruses, such as HIV, play a vital role in myocarditis disease (MCD) incidence. Lack of MCD diagnosis in the early stages is associated with irreversible complications. Cardiac magnetic resonance imaging (CMRI) is highly popular among cardiologists to diagnose CVDs. In this paper, a deep learning (DL) based computer-aided diagnosis system (CADS) is presented for the diagnosis of MCD using CMRI images. The proposed CADS includes dataset, preprocessing, feature extraction, classification, and post-processing steps. First, the Z-Alizadeh dataset was selected for the experiments. The preprocessing step included noise removal, image resizing, and data augmentation (DA). In this step, CutMix, and MixUp techniques were used for the DA. Then, the most recent pre-trained and transformers models were used for feature extraction and classification using CMRI images. Our results show high performance for the detection of MCD using transformer models compared with the pre-trained architectures. Among the DL architectures, Turbulence Neural Transformer (TNT) architecture achieved an accuracy of 99.73% with 10-fold cross-validation strategy. Explainable-based Grad Cam method is used to visualize the MCD suspected areas in CMRI images.



### Analyzing Deep Learning Representations of Point Clouds for Real-Time In-Vehicle LiDAR Perception
- **Arxiv ID**: http://arxiv.org/abs/2210.14612v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.RO, 68T07, I.2.10; I.2.9; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2210.14612v3)
- **Published**: 2022-10-26 10:39:59+00:00
- **Updated**: 2023-05-15 08:03:26+00:00
- **Authors**: Marc Uecker, Tobias Fleck, Marcel Pflugfelder, J. Marius Zöllner
- **Comment**: Accepted at the NeurIPS 2022 Workshop on Machine Learning for
  Autonomous Driving (ML4AD). Changed in v3: corrected erroneously cited mIoU
  score for PolarNet
- **Journal**: None
- **Summary**: LiDAR sensors are an integral part of modern autonomous vehicles as they provide an accurate, high-resolution 3D representation of the vehicle's surroundings. However, it is computationally difficult to make use of the ever-increasing amounts of data from multiple high-resolution LiDAR sensors. As frame-rates, point cloud sizes and sensor resolutions increase, real-time processing of these point clouds must still extract semantics from this increasingly precise picture of the vehicle's environment. One deciding factor of the run-time performance and accuracy of deep neural networks operating on these point clouds is the underlying data representation and the way it is computed. In this work, we examine the relationship between the computational representations used in neural networks and their performance characteristics. To this end, we propose a novel computational taxonomy of LiDAR point cloud representations used in modern deep neural networks for 3D point cloud processing. Using this taxonomy, we perform a structured analysis of different families of approaches. Thereby, we uncover common advantages and limitations in terms of computational efficiency, memory requirements, and representational capacity as measured by semantic segmentation performance. Finally, we provide some insights and guidance for future developments in neural point cloud processing methods.



### SemFormer: Semantic Guided Activation Transformer for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.14618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14618v1)
- **Published**: 2022-10-26 10:51:20+00:00
- **Updated**: 2022-10-26 10:51:20+00:00
- **Authors**: Junliang Chen, Xiaodong Zhao, Cheng Luo, Linlin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent mainstream weakly supervised semantic segmentation (WSSS) approaches are mainly based on Class Activation Map (CAM) generated by a CNN (Convolutional Neural Network) based image classifier. In this paper, we propose a novel transformer-based framework, named Semantic Guided Activation Transformer (SemFormer), for WSSS. We design a transformer-based Class-Aware AutoEncoder (CAAE) to extract the class embeddings for the input image and learn class semantics for all classes of the dataset. The class embeddings and learned class semantics are then used to guide the generation of activation maps with four losses, i.e., class-foreground, class-background, activation suppression, and activation complementation loss. Experimental results show that our SemFormer achieves \textbf{74.3}\% mIoU and surpasses many recent mainstream WSSS approaches by a large margin on PASCAL VOC 2012 dataset. Code will be available at \url{https://github.com/JLChen-C/SemFormer}.



### A new band selection approach based on information theory and support vector machine for hyperspectral images reduction and classification
- **Arxiv ID**: http://arxiv.org/abs/2210.14621v1
- **DOI**: 10.1109/ISNCC.2017.8072002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14621v1)
- **Published**: 2022-10-26 10:54:23+00:00
- **Updated**: 2022-10-26 10:54:23+00:00
- **Authors**: A. Elmaizi, E. Sarhrouni, A. Hammouch, C. Nacir
- **Comment**: None
- **Journal**: 2017 International Symposium on Networks, Computers and
  Communications (ISNCC) -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-85039974269&partnerID=MN8TOARS
- **Summary**: The high dimensionality of hyperspectral images consisting of several bands often imposes a big computational challenge for image processing. Therefore, spectral band selection is an essential step for removing the irrelevant, noisy and redundant bands. Consequently increasing the classification accuracy. However, identification of useful bands from hundreds or even thousands of related bands is a nontrivial task. This paper aims at identifying a small set of highly discriminative bands, for improving computational speed and prediction accuracy. Hence, we proposed a new strategy based on joint mutual information to measure the statistical dependence and correlation between the selected bands and evaluate the relative utility of each one to classification. The proposed filter approach is compared to an effective reproduced filters based on mutual information. Simulations results on the hyperpectral image HSI AVIRIS 92AV3C using the SVM classifier have shown that the effective proposed algorithm outperforms the reproduced filters strategy performance.   Keywords-Hyperspectral images, Classification, band Selection, Joint Mutual Information, dimensionality reduction ,correlation, SVM.



### RapidAI4EO: Mono- and Multi-temporal Deep Learning models for Updating the CORINE Land Cover Product
- **Arxiv ID**: http://arxiv.org/abs/2210.14624v1
- **DOI**: 10.1109/IGARSS46834.2022.9883198
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14624v1)
- **Published**: 2022-10-26 11:08:13+00:00
- **Updated**: 2022-10-26 11:08:13+00:00
- **Authors**: Priyash Bhugra, Benjamin Bischke, Christoph Werner, Robert Syrnicki, Carolin Packbier, Patrick Helber, Caglar Senaras, Akhil Singh Rana, Tim Davis, Wanda De Keersmaecker, Daniele Zanaga, Annett Wania, Ruben Van De Kerchove, Giovanni Marchisio
- **Comment**: Published in IGARSS 2022 - 2022 IEEE International Geoscience and
  Remote Sensing Symposium
- **Journal**: None
- **Summary**: In the remote sensing community, Land Use Land Cover (LULC) classification with satellite imagery is a main focus of current research activities. Accurate and appropriate LULC classification, however, continues to be a challenging task. In this paper, we evaluate the performance of multi-temporal (monthly time series) compared to mono-temporal (single time step) satellite images for multi-label classification using supervised learning on the RapidAI4EO dataset. As a first step, we trained our CNN model on images at a single time step for multi-label classification, i.e. mono-temporal. We incorporated time-series images using a LSTM model to assess whether or not multi-temporal signals from satellites improves CLC classification. The results demonstrate an improvement of approximately 0.89% in classifying satellite imagery on 15 classes using a multi-temporal approach on monthly time series images compared to the mono-temporal approach. Using features from multi-temporal or mono-temporal images, this work is a step towards an efficient change detection and land monitoring approach.



### Super-Resolution Based Patch-Free 3D Image Segmentation with High-Frequency Guidance
- **Arxiv ID**: http://arxiv.org/abs/2210.14645v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14645v2)
- **Published**: 2022-10-26 11:46:08+00:00
- **Updated**: 2023-07-10 07:53:28+00:00
- **Authors**: Hongyi Wang, Lanfen Lin, Hongjie Hu, Qingqing Chen, Yinhao Li, Yutaro Iwamoto, Xian-Hua Han, Yen-Wei Chen, Ruofeng Tong
- **Comment**: Version #2 uploaded in Jul 10, 2023
- **Journal**: None
- **Summary**: High resolution (HR) 3D images are widely used nowadays, such as medical images like Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). However, segmentation of these 3D images remains a challenge due to their high spatial resolution and dimensionality in contrast to currently limited GPU memory. Therefore, most existing 3D image segmentation methods use patch-based models, which have low inference efficiency and ignore global contextual information. To address these problems, we propose a super-resolution (SR) based patch-free 3D image segmentation framework that can realize HR segmentation from a global-wise low-resolution (LR) input. The framework contains two sub-tasks, of which semantic segmentation is the main task and super resolution is an auxiliary task aiding in rebuilding the high frequency information from the LR input. To furthermore balance the information loss with the LR input, we propose a High-Frequency Guidance Module (HGM), and design an efficient selective cropping algorithm to crop an HR patch from the original image as restoration guidance for it. In addition, we also propose a Task-Fusion Module (TFM) to exploit the inter connections between segmentation and SR task, realizing joint optimization of the two tasks. When predicting, only the main segmentation task is needed, while other modules can be removed for acceleration. The experimental results on two different datasets show that our framework has a four times higher inference speed compared to traditional patch-based methods, while its performance also surpasses other patch-based and patch-free models.



### Masked Modeling Duo: Learning Representations by Encouraging Both Networks to Model the Input
- **Arxiv ID**: http://arxiv.org/abs/2210.14648v3
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2210.14648v3)
- **Published**: 2022-10-26 11:49:30+00:00
- **Updated**: 2023-03-02 09:42:58+00:00
- **Authors**: Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino
- **Comment**: 6 pages, 3 figures, and 6 tables. To appear at ICASSP2023
- **Journal**: None
- **Summary**: Masked Autoencoders is a simple yet powerful self-supervised learning method. However, it learns representations indirectly by reconstructing masked input patches. Several methods learn representations directly by predicting representations of masked patches; however, we think using all patches to encode training signal representations is suboptimal. We propose a new method, Masked Modeling Duo (M2D), that learns representations directly while obtaining training signals using only masked patches. In the M2D, the online network encodes visible patches and predicts masked patch representations, and the target network, a momentum encoder, encodes masked patches. To better predict target representations, the online network should model the input well, while the target network should also model it well to agree with online predictions. Then the learned representations should better model the input. We validated the M2D by learning general-purpose audio representations, and M2D set new state-of-the-art performance on tasks such as UrbanSound8K, VoxCeleb1, AudioSet20K, GTZAN, and SpeechCommandsV2. We additionally validate the effectiveness of M2D for images using ImageNet-1K in the appendix.



### Boosting Semi-Supervised Semantic Segmentation with Probabilistic Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.14670v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14670v3)
- **Published**: 2022-10-26 12:47:29+00:00
- **Updated**: 2022-12-16 01:23:13+00:00
- **Authors**: Haoyu Xie, Changqi Wang, Mingkai Zheng, Minjing Dong, Shan You, Chong Fu, Chang Xu
- **Comment**: Accepted to AAAI 2023
- **Journal**: None
- **Summary**: Recent breakthroughs in semi-supervised semantic segmentation have been developed through contrastive learning. In prevalent pixel-wise contrastive learning solutions, the model maps pixels to deterministic representations and regularizes them in the latent space. However, there exist inaccurate pseudo-labels which map the ambiguous representations of pixels to the wrong classes due to the limited cognitive ability of the model. In this paper, we define pixel-wise representations from a new perspective of probability theory and propose a Probabilistic Representation Contrastive Learning (PRCL) framework that improves representation quality by taking its probability into consideration. Through modelling the mapping from pixels to representations as the probability via multivariate Gaussian distributions, we can tune the contribution of the ambiguous representations to tolerate the risk of inaccurate pseudo-labels. Furthermore, we define prototypes in the form of distributions, which indicates the confidence of a class, while the point prototype cannot. Moreover, we propose to regularize the distribution variance to enhance the reliability of representations. Taking advantage of these benefits, high-quality feature representations can be derived in the latent space, thereby the performance of semantic segmentation can be further improved. We conduct sufficient experiment to evaluate PRCL on Pascal VOC and CityScapes to demonstrate its superiority. The code is available at https://github.com/Haoyu-Xie/PRCL.



### How precise are performance estimates for typical medical image segmentation tasks?
- **Arxiv ID**: http://arxiv.org/abs/2210.14677v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2210.14677v3)
- **Published**: 2022-10-26 12:53:15+00:00
- **Updated**: 2023-05-24 12:32:40+00:00
- **Authors**: Rosana El Jurdi, Olivier Colliot
- **Comment**: None
- **Journal**: None
- **Summary**: An important issue in medical image processing is to be able to estimate not only the performances of algorithms but also the precision of the estimation of these performances. Reporting precision typically amounts to reporting standard-error of the mean (SEM) or equivalently confidence intervals. However, this is rarely done in medical image segmentation studies. In this paper, we aim to estimate what is the typical confidence that can be expected in such studies. To that end, we first perform experiments for Dice metric estimation using a standard deep learning model (U-net) and a classical task from the Medical Segmentation Decathlon. We extensively study precision estimation using both Gaussian assumption and bootstrapping (which does not require any assumption on the distribution). We then perform simulations for other test set sizes and performance spreads. Overall, our work shows that small test sets lead to wide confidence intervals (e.g. $\sim$8 points of Dice for 20 samples with $\sigma \simeq 10$).



### TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early Intent Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.14714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.14714v1)
- **Published**: 2022-10-26 13:47:23+00:00
- **Updated**: 2022-10-26 13:47:23+00:00
- **Authors**: Nada Osman, Guglielmo Camporese, Lamberto Ballan
- **Comment**: None
- **Journal**: None
- **Summary**: Human intention prediction is a growing area of research where an activity in a video has to be anticipated by a vision-based system. To this end, the model creates a representation of the past, and subsequently, it produces future hypotheses about upcoming scenarios. In this work, we focus on pedestrians' early intention prediction in which, from a current observation of an urban scene, the model predicts the future activity of pedestrians that approach the street. Our method is based on a multi-modal transformer that encodes past observations and produces multiple predictions at different anticipation times. Moreover, we propose to learn the attention masks of our transformer-based model (Temporal Adaptive Mask Transformer) in order to weigh differently present and past temporal dependencies. We investigate our method on several public benchmarks for early intention prediction, improving the prediction performances at different anticipation times compared to the previous works.



### The eyes and hearts of UAV pilots: observations of physiological responses in real-life scenarios
- **Arxiv ID**: http://arxiv.org/abs/2210.14910v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.14910v1)
- **Published**: 2022-10-26 14:16:56+00:00
- **Updated**: 2022-10-26 14:16:56+00:00
- **Authors**: Alexandre Duval, Anita Paas, Abdalwhab Abdalwhab, David St-Onge
- **Comment**: None
- **Journal**: None
- **Summary**: The drone industry is diversifying and the number of pilots increases rapidly. In this context, flight schools need adapted tools to train pilots, most importantly with regard to their own awareness of their physiological and cognitive limits. In civil and military aviation, pilots can train themselves on realistic simulators to tune their reaction and reflexes, but also to gather data on their piloting behavior and physiological states. It helps them to improve their performances. Opposed to cockpit scenarios, drone teleoperation is conducted outdoor in the field, thus with only limited potential from desktop simulation training. This work aims to provide a solution to gather pilots behavior out in the field and help them increase their performance. We combined advance object detection from a frontal camera to gaze and heart-rate variability measurements. We observed pilots and analyze their behavior over three flight challenges. We believe this tool can support pilots both in their training and in their regular flight tasks. A demonstration video is available on https://www.youtube.com/watch?v=eePhjd2qNiI



### Long-tailed Food Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.14748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14748v1)
- **Published**: 2022-10-26 14:29:30+00:00
- **Updated**: 2022-10-26 14:29:30+00:00
- **Authors**: Jiangpeng He, Luotao Lin, Heather Eicher-Miller, Fengqing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Food classification serves as the basic step of image-based dietary assessment to predict the types of foods in each input image. However, food image predictions in a real world scenario are usually long-tail distributed among different food classes, which cause heavy class-imbalance problems and a restricted performance. In addition, none of the existing long-tailed classification methods focus on food data, which can be more challenging due to the lower inter-class and higher intra-class similarity among foods. In this work, we first introduce two new benchmark datasets for long-tailed food classification including Food101-LT and VFN-LT where the number of samples in VFN-LT exhibits the real world long-tailed food distribution. Then we propose a novel 2-Phase framework to address the problem of class-imbalance by (1) undersampling the head classes to remove redundant samples along with maintaining the learned information through knowledge distillation, and (2) oversampling the tail classes by performing visual-aware data augmentation. We show the effectiveness of our method by comparing with existing state-of-the-art long-tailed classification methods and show improved performance on both Food101-LT and VFN-LT benchmarks. The results demonstrate the potential to apply our method to related real life applications.



### Rapid and robust endoscopic content area estimation: A lean GPU-based pipeline and curated benchmark dataset
- **Arxiv ID**: http://arxiv.org/abs/2210.14771v1
- **DOI**: 10.1080/21681163.2022.2156393
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14771v1)
- **Published**: 2022-10-26 15:10:44+00:00
- **Updated**: 2022-10-26 15:10:44+00:00
- **Authors**: Charlie Budd, Luis C. Garcia-Peraza-Herrera, Martin Huber, Sebastien Ourselin, Tom Vercauteren
- **Comment**: Presented at AE-CAI MICCAI workshop
- **Journal**: None
- **Summary**: Endoscopic content area refers to the informative area enclosed by the dark, non-informative, border regions present in most endoscopic footage. The estimation of the content area is a common task in endoscopic image processing and computer vision pipelines. Despite the apparent simplicity of the problem, several factors make reliable real-time estimation surprisingly challenging. The lack of rigorous investigation into the topic combined with the lack of a common benchmark dataset for this task has been a long-lasting issue in the field. In this paper, we propose two variants of a lean GPU-based computational pipeline combining edge detection and circle fitting. The two variants differ by relying on handcrafted features, and learned features respectively to extract content area edge point candidates. We also present a first-of-its-kind dataset of manually annotated and pseudo-labelled content areas across a range of surgical indications. To encourage further developments, the curated dataset, and an implementation of both algorithms, has been made public (https://doi.org/10.7303/syn32148000, https://github.com/charliebudd/torch-content-area). We compare our proposed algorithm with a state-of-the-art U-Net-based approach and demonstrate significant improvement in terms of both accuracy (Hausdorff distance: 6.3 px versus 118.1 px) and computational time (Average runtime per frame: 0.13 ms versus 11.2 ms).



### Evaluation of Synthetically Generated CT for use in Transcranial Focused Ultrasound Procedures
- **Arxiv ID**: http://arxiv.org/abs/2210.14775v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14775v1)
- **Published**: 2022-10-26 15:15:24+00:00
- **Updated**: 2022-10-26 15:15:24+00:00
- **Authors**: Han Liu, Michelle K. Sigona, Thomas J. Manuel, Li Min Chen, Benoit M. Dawant, Charles F. Caskey
- **Comment**: arXiv admin note: text overlap with arXiv:2202.10136
- **Journal**: None
- **Summary**: Transcranial focused ultrasound (tFUS) is a therapeutic ultrasound method that focuses sound through the skull to a small region noninvasively and often under MRI guidance. CT imaging is used to estimate the acoustic properties that vary between individual skulls to enable effective focusing during tFUS procedures, exposing patients to potentially harmful radiation. A method to estimate acoustic parameters in the skull without the need for CT would be desirable. Here, we synthesized CT images from routinely acquired T1-weighted MRI by using a 3D patch-based conditional generative adversarial network (cGAN) and evaluated the performance of synthesized CT (sCT) images for treatment planning with tFUS. We compared the performance of sCT to real CT (rCT) images for tFUS planning using Kranion and simulations using the acoustic toolbox, k-Wave. Simulations were performed for 3 tFUS scenarios: 1) no aberration correction, 2) correction with phases calculated from Kranion, and 3) phase shifts calculated from time-reversal. From Kranion, skull density ratio, skull thickness, and number of active elements between rCT and sCT had Pearson's Correlation Coefficients of 0.94, 0.92, and 0.98, respectively. Among 20 targets, differences in simulated peak pressure between rCT and sCT were largest without phase correction (12.4$\pm$8.1%) and smallest with Kranion phases (7.3$\pm$6.0%). The distance between peak focal locations between rCT and sCT was less than 1.3 mm for all simulation cases. Real and synthetically generated skulls had comparable image similarity, skull measurements, and acoustic simulation metrics. Our work demonstrates the feasibility of replacing real CTs with the MR-synthesized CT for tFUS planning. Source code and a docker image with the trained model are available at https://github.com/han-liu/SynCT_TcMRgFUS



### Decoupled Mixup for Generalized Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.14783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14783v1)
- **Published**: 2022-10-26 15:21:39+00:00
- **Updated**: 2022-10-26 15:21:39+00:00
- **Authors**: Haozhe Liu, Wentian Zhang, Jinheng Xie, Haoqian Wu, Bing Li, Ziqi Zhang, Yuexiang Li, Yawen Huang, Bernard Ghanem, Yefeng Zheng
- **Comment**: Accepted by ECCV'2022 Workshop: Causality in Vision
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have demonstrated remarkable performance when the training and testing data are from the same distribution. However, such trained CNN models often largely degrade on testing data which is unseen and Out-Of-the-Distribution (OOD). To address this issue, we propose a novel "Decoupled-Mixup" method to train CNN models for OOD visual recognition. Different from previous work combining pairs of images homogeneously, our method decouples each image into discriminative and noise-prone regions, and then heterogeneously combines these regions of image pairs to train CNN models. Since the observation is that noise-prone regions such as textural and clutter backgrounds are adverse to the generalization ability of CNN models during training, we enhance features from discriminative regions and suppress noise-prone ones when combining an image pair. To further improve the generalization ability of trained models, we propose to disentangle discriminative and noise-prone regions in frequency-based and context-based fashions. Experiment results show the high generalization performance of our method on testing data that are composed of unseen contexts, where our method achieves 85.76\% top-1 accuracy in Track-1 and 79.92\% in Track-2 in the NICO Challenge. The source code is available at https://github.com/HaozheLiu-ST/NICOChallenge-OOD-Classification.



### ViNL: Visual Navigation and Locomotion Over Obstacles
- **Arxiv ID**: http://arxiv.org/abs/2210.14791v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14791v2)
- **Published**: 2022-10-26 15:38:28+00:00
- **Updated**: 2023-01-14 20:19:59+00:00
- **Authors**: Simar Kareer, Naoki Yokoyama, Dhruv Batra, Sehoon Ha, Joanne Truong
- **Comment**: None
- **Journal**: None
- **Summary**: We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely "model-free", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely "zero-shot" (without any co-training). While prior works have developed learning methods for visual navigation or visual locomotion, to the best of our knowledge, this is the first fully learned approach that leverages vision to accomplish both (1) intelligent navigation in new environments, and (2) intelligent visual locomotion that aims to traverse cluttered environments without disrupting obstacles. On the task of navigation to distant goals in unknown environments, ViNL using just egocentric vision significantly outperforms prior work on robust locomotion using privileged terrain maps (+32.8% success and -4.42 collisions per meter). Additionally, we ablate our locomotion policy to show that each aspect of our approach helps reduce obstacle collisions. Videos and code at http://www.joannetruong.com/projects/vinl.html



### M$^3$ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design
- **Arxiv ID**: http://arxiv.org/abs/2210.14793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14793v1)
- **Published**: 2022-10-26 15:40:24+00:00
- **Updated**: 2022-10-26 15:40:24+00:00
- **Authors**: Hanxue Liang, Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning (MTL) encapsulates multiple learned tasks in a single model and often lets those tasks learn better jointly. However, when deploying MTL onto those real-world systems that are often resource-constrained or latency-sensitive, two prominent challenges arise: (i) during training, simultaneously optimizing all tasks is often difficult due to gradient conflicts across tasks; (ii) at inference, current MTL regimes have to activate nearly the entire model even to just execute a single task. Yet most real systems demand only one or two tasks at each moment, and switch between tasks as needed: therefore such all tasks activated inference is also highly inefficient and non-scalable. In this paper, we present a model-accelerator co-design framework to enable efficient on-device MTL. Our framework, dubbed M$^3$ViT, customizes mixture-of-experts (MoE) layers into a vision transformer (ViT) backbone for MTL, and sparsely activates task-specific experts during training. Then at inference with any task of interest, the same design allows for activating only the task-corresponding sparse expert pathway, instead of the full model. Our new model design is further enhanced by hardware-level innovations, in particular, a novel computation reordering scheme tailored for memory-constrained MTL that achieves zero-overhead switching between tasks and can scale to any number of experts. When executing single-task inference, M$^{3}$ViT achieves higher accuracies than encoder-focused MTL methods, while significantly reducing 88% inference FLOPs. When implemented on a hardware platform of one Xilinx ZCU104 FPGA, our co-design framework reduces the memory requirement by 2.4 times, while achieving energy efficiency up to 9.23 times higher than a comparable FPGA baseline. Code is available at: https://github.com/VITA-Group/M3ViT.



### Is Multi-Task Learning an Upper Bound for Continual Learning?
- **Arxiv ID**: http://arxiv.org/abs/2210.14797v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14797v1)
- **Published**: 2022-10-26 15:45:11+00:00
- **Updated**: 2022-10-26 15:45:11+00:00
- **Authors**: Zihao Wu, Huy Tran, Hamed Pirsiavash, Soheil Kolouri
- **Comment**: None
- **Journal**: None
- **Summary**: Continual and multi-task learning are common machine learning approaches to learning from multiple tasks. The existing works in the literature often assume multi-task learning as a sensible performance upper bound for various continual learning algorithms. While this assumption is empirically verified for different continual learning benchmarks, it is not rigorously justified. Moreover, it is imaginable that when learning from multiple tasks, a small subset of these tasks could behave as adversarial tasks reducing the overall learning performance in a multi-task setting. In contrast, continual learning approaches can avoid the performance drop caused by such adversarial tasks to preserve their performance on the rest of the tasks, leading to better performance than a multi-task learner. This paper proposes a novel continual self-supervised learning setting, where each task corresponds to learning an invariant representation for a specific class of data augmentations. In this setting, we show that continual learning often beats multi-task learning on various benchmark datasets, including MNIST, CIFAR-10, and CIFAR-100.



### Segmentation of Bruch's Membrane in retinal OCT with AMD using anatomical priors and uncertainty quantification
- **Arxiv ID**: http://arxiv.org/abs/2210.14799v2
- **DOI**: 10.1109/JBHI.2022.3217962
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14799v2)
- **Published**: 2022-10-26 15:49:07+00:00
- **Updated**: 2022-10-30 08:32:44+00:00
- **Authors**: Botond Fazekas, Dmitrii Lachinov, Guilherme Aresta, Julia Mai, Ursula Schmidt-Erfurth, Hrvoje Bogunovic
- **Comment**: None
- **Journal**: None
- **Summary**: Bruch's membrane (BM) segmentation on optical coherence tomography (OCT) is a pivotal step for the diagnosis and follow-up of age-related macular degeneration (AMD), one of the leading causes of blindness in the developed world. Automated BM segmentation methods exist, but they usually do not account for the anatomical coherence of the results, neither provide feedback on the confidence of the prediction. These factors limit the applicability of these systems in real-world scenarios. With this in mind, we propose an end-to-end deep learning method for automated BM segmentation in AMD patients. An Attention U-Net is trained to output a probability density function of the BM position, while taking into account the natural curvature of the surface. Besides the surface position, the method also estimates an A-scan wise uncertainty measure of the segmentation output. Subsequently, the A-scans with high uncertainty are interpolated using thin plate splines (TPS). We tested our method with ablation studies on an internal dataset with 138 patients covering all three AMD stages, and achieved a mean absolute localization error of 4.10 um. In addition, the proposed segmentation method was compared against the state-of-the-art methods and showed a superior performance on an external publicly available dataset from a different patient cohort and OCT device, demonstrating strong generalization ability.



### Visual Answer Localization with Cross-modal Mutual Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2210.14823v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.14823v3)
- **Published**: 2022-10-26 16:11:09+00:00
- **Updated**: 2022-10-28 08:42:01+00:00
- **Authors**: Yixuan Weng, Bin Li
- **Comment**: 4 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: The goal of visual answering localization (VAL) in the video is to obtain a relevant and concise time clip from a video as the answer to the given natural language question. Early methods are based on the interaction modelling between video and text to predict the visual answer by the visual predictor. Later, using the textual predictor with subtitles for the VAL proves to be more precise. However, these existing methods still have cross-modal knowledge deviations from visual frames or textual subtitles. In this paper, we propose a cross-modal mutual knowledge transfer span localization (MutualSL) method to reduce the knowledge deviation. MutualSL has both visual predictor and textual predictor, where we expect the prediction results of these both to be consistent, so as to promote semantic knowledge understanding between cross-modalities. On this basis, we design a one-way dynamic loss function to dynamically adjust the proportion of knowledge transfer. We have conducted extensive experiments on three public datasets for evaluation. The experimental results show that our method outperforms other competitive state-of-the-art (SOTA) methods, demonstrating its effectiveness.



### Streaming Radiance Fields for 3D Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.14831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14831v1)
- **Published**: 2022-10-26 16:23:02+00:00
- **Updated**: 2022-10-26 16:23:02+00:00
- **Authors**: Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, Ping Tan
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: We present an explicit-grid based method for efficiently reconstructing streaming radiance fields for novel view synthesis of real world dynamic scenes. Instead of training a single model that combines all the frames, we formulate the dynamic modeling problem with an incremental learning paradigm in which per-frame model difference is trained to complement the adaption of a base model on the current frame. By exploiting the simple yet effective tuning strategy with narrow bands, the proposed method realizes a feasible framework for handling video sequences on-the-fly with high training efficiency. The storage overhead induced by using explicit grid representations can be significantly reduced through the use of model difference based compression. We also introduce an efficient strategy to further accelerate model optimization for each frame. Experiments on challenging video sequences demonstrate that our approach is capable of achieving a training speed of 15 seconds per-frame with competitive rendering quality, which attains $1000 \times$ speedup over the state-of-the-art implicit methods. Code is available at https://github.com/AlgoHunt/StreamRF.



### Synthetic Tumors Make AI Segment Tumors Better
- **Arxiv ID**: http://arxiv.org/abs/2210.14845v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14845v1)
- **Published**: 2022-10-26 16:45:19+00:00
- **Updated**: 2022-10-26 16:45:19+00:00
- **Authors**: Qixin Hu, Junfei Xiao, Yixiong Chen, Shuwen Sun, Jie-Neng Chen, Alan Yuille, Zongwei Zhou
- **Comment**: NeurIPS Workshop on Medical Imaging Meets NeurIPS, 2022
- **Journal**: None
- **Summary**: We develop a novel strategy to generate synthetic tumors. Unlike existing works, the tumors generated by our strategy have two intriguing advantages: (1) realistic in shape and texture, which even medical professionals can confuse with real tumors; (2) effective for AI model training, which can perform liver tumor segmentation similarly to a model trained on real tumors - this result is unprecedented because no existing work, using synthetic tumors only, has thus far reached a similar or even close performance to the model trained on real tumors. This result also implies that manual efforts for developing per-voxel annotation of tumors (which took years to create) can be considerably reduced for training AI models in the future. Moreover, our synthetic tumors have the potential to improve the success rate of small tumor detection by automatically generating enormous examples of small (or tiny) synthetic tumors.



### A deep scalable neural architecture for soil properties estimation from spectral information
- **Arxiv ID**: http://arxiv.org/abs/2210.17314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.17314v1)
- **Published**: 2022-10-26 16:50:06+00:00
- **Updated**: 2022-10-26 16:50:06+00:00
- **Authors**: Flavio Piccoli, Micol Rossini, Roberto Colombo, Raimondo Schettini, Paolo Napoletano
- **Comment**: 14 pages + 13 of appendix. Journal paper
- **Journal**: None
- **Summary**: In this paper we propose an adaptive deep neural architecture for the prediction of multiple soil characteristics from the analysis of hyperspectral signatures. The proposed method overcomes the limitations of previous methods in the state of art: (i) it allows to predict multiple soil variables at once; (ii) it permits to backtrace the spectral bands that most contribute to the estimation of a given variable; (iii) it is based on a flexible neural architecture capable of automatically adapting to the spectral library under analysis. The proposed architecture is experimented on LUCAS, a large laboratory dataset and on a dataset achieved by simulating PRISMA hyperspectral sensor. 'Results, compared with other state-of-the-art methods confirm the effectiveness of the proposed solution.



### Visual Semantic Parsing: From Images to Abstract Meaning Representation
- **Arxiv ID**: http://arxiv.org/abs/2210.14862v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14862v2)
- **Published**: 2022-10-26 17:06:42+00:00
- **Updated**: 2022-10-27 15:54:50+00:00
- **Authors**: Mohamed Ashraf Abdelsalam, Zhan Shi, Federico Fancellu, Kalliopi Basioti, Dhaivat J. Bhatt, Vladimir Pavlovic, Afsaneh Fazly
- **Comment**: published in CoNLL 2022
- **Journal**: None
- **Summary**: The success of scene graphs for visual scene understanding has brought attention to the benefits of abstracting a visual input (e.g., image) into a structured representation, where entities (people and objects) are nodes connected by edges specifying their relations. Building these representations, however, requires expensive manual annotation in the form of images paired with their scene graphs or frames. These formalisms remain limited in the nature of entities and relations they can capture. In this paper, we propose to leverage a widely-used meaning representation in the field of natural language processing, the Abstract Meaning Representation (AMR), to address these shortcomings. Compared to scene graphs, which largely emphasize spatial relationships, our visual AMR graphs are more linguistically informed, with a focus on higher-level semantic concepts extrapolated from visual input. Moreover, they allow us to generate meta-AMR graphs to unify information contained in multiple image descriptions under one representation. Through extensive experimentation and analysis, we demonstrate that we can re-purpose an existing text-to-AMR parser to parse images into AMRs. Our findings point to important future research directions for improved scene understanding.



### Anisotropic multiresolution analyses for deepfake detection
- **Arxiv ID**: http://arxiv.org/abs/2210.14874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14874v2)
- **Published**: 2022-10-26 17:26:09+00:00
- **Updated**: 2022-11-04 20:00:11+00:00
- **Authors**: Wei Huang, Michelangelo Valsecchi, Michael Multerer
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have paved the path towards entirely new media generation capabilities at the forefront of image, video, and audio synthesis. However, they can also be misused and abused to fabricate elaborate lies, capable of stirring up the public debate. The threat posed by GANs has sparked the need to discern between genuine content and fabricated one. Previous studies have tackled this task by using classical machine learning techniques, such as k-nearest neighbours and eigenfaces, which unfortunately did not prove very effective. Subsequent methods have focused on leveraging on frequency decompositions, i.e., discrete cosine transform, wavelets, and wavelet packets, to preprocess the input features for classifiers. However, existing approaches only rely on isotropic transformations. We argue that, since GANs primarily utilize isotropic convolutions to generate their output, they leave clear traces, their fingerprint, in the coefficient distribution on sub-bands extracted by anisotropic transformations. We employ the fully separable wavelet transform and multiwavelets to obtain the anisotropic features to feed to standard CNN classifiers. Lastly, we find the fully separable transform capable of improving the state-of-the-art.



### A Sign That Spells: DALL-E 2, Invisual Images and The Racial Politics of Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2211.06323v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.06323v1)
- **Published**: 2022-10-26 17:49:17+00:00
- **Updated**: 2022-10-26 17:49:17+00:00
- **Authors**: Fabian Offert, Thao Phan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we examine how generative machine learning systems produce a new politics of visual culture. We focus on DALL-E 2 and related models as an emergent approach to image-making that operates through the cultural techniques of feature extraction and semantic compression. These techniques, we argue, are inhuman, invisual, and opaque, yet are still caught in a paradox that is ironically all too human: the consistent reproduction of whiteness as a latent feature of dominant visual culture. We use Open AI's failed efforts to 'debias' their system as a critical opening to interrogate how systems like DALL-E 2 dissolve and reconstitute politically salient human concepts like race. This example vividly illustrates the stakes of this moment of transformation, when so-called foundation models reconfigure the boundaries of visual culture and when 'doing' anti-racism means deploying quick technical fixes to mitigate personal discomfort, or more importantly, potential commercial loss.



### DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2210.14896v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14896v4)
- **Published**: 2022-10-26 17:54:20+00:00
- **Updated**: 2023-07-06 11:53:19+00:00
- **Authors**: Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau
- **Comment**: Accepted to ACL 2023 (nominated for best paper, top 1.6% of
  submissions, oral presentation). 17 pages, 11 figures. The dataset is
  available at https://huggingface.co/datasets/poloclub/diffusiondb. The code
  is at https://github.com/poloclub/diffusiondb. The interactive visualization
  demo is at https://poloclub.github.io/diffusiondb/explorer/
- **Journal**: None
- **Summary**: With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb.



### Searching Dense Point Correspondences via Permutation Matrix Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.14897v1
- **DOI**: 10.1109/LSP.2022.3172844
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14897v1)
- **Published**: 2022-10-26 17:56:09+00:00
- **Updated**: 2022-10-26 17:56:09+00:00
- **Authors**: Zhiyuan Zhang, Jiadai Sun, Yuchao Dai, Bin Fan, Qi Liu
- **Comment**: Accepted to IEEE Signal Processing Letters (SPL) 2022
- **Journal**: None
- **Summary**: Although 3D point cloud data has received widespread attentions as a general form of 3D signal expression, applying point clouds to the task of dense correspondence estimation between 3D shapes has not been investigated widely. Furthermore, even in the few existing 3D point cloud-based methods, an important and widely acknowledged principle, i.e . one-to-one matching, is usually ignored. In response, this paper presents a novel end-to-end learning-based method to estimate the dense correspondence of 3D point clouds, in which the problem of point matching is formulated as a zero-one assignment problem to achieve a permutation matching matrix to implement the one-to-one principle fundamentally. Note that the classical solutions of this assignment problem are always non-differentiable, which is fatal for deep learning frameworks. Thus we design a special matching module, which solves a doubly stochastic matrix at first and then projects this obtained approximate solution to the desired permutation matrix. Moreover, to guarantee end-to-end learning and the accuracy of the calculated loss, we calculate the loss from the learned permutation matrix but propagate the gradient to the doubly stochastic matrix directly which bypasses the permutation matrix during the backward propagation. Our method can be applied to both non-rigid and rigid 3D point cloud data and extensive experiments show that our method achieves state-of-the-art performance for dense correspondence learning.



### CU-Net: LiDAR Depth-Only Completion With Coupled U-Net
- **Arxiv ID**: http://arxiv.org/abs/2210.14898v1
- **DOI**: 10.1109/LRA.2022.3201193
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14898v1)
- **Published**: 2022-10-26 17:57:13+00:00
- **Updated**: 2022-10-26 17:57:13+00:00
- **Authors**: Yufei Wang, Yuchao Dai, Qi Liu, Peng Yang, Jiadai Sun, Bo Li
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RA-L), Code:
  https://github.com/YufeiWang777/CU-Net
- **Journal**: None
- **Summary**: LiDAR depth-only completion is a challenging task to estimate dense depth maps only from sparse measurement points obtained by LiDAR. Even though the depth-only methods have been widely developed, there is still a significant performance gap with the RGB-guided methods that utilize extra color images. We find that existing depth-only methods can obtain satisfactory results in the areas where the measurement points are almost accurate and evenly distributed (denoted as normal areas), while the performance is limited in the areas where the foreground and background points are overlapped due to occlusion (denoted as overlap areas) and the areas where there are no measurement points around (denoted as blank areas) since the methods have no reliable input information in these areas. Building upon these observations, we propose an effective Coupled U-Net (CU-Net) architecture for depth-only completion. Instead of directly using a large network for regression, we employ the local U-Net to estimate accurate values in the normal areas and provide the global U-Net with reliable initial values in the overlap and blank areas. The depth maps predicted by the two coupled U-Nets are fused by learned confidence maps to obtain final results. In addition, we propose a confidence-based outlier removal module, which removes outliers using simple judgment conditions. Our proposed method boosts the final results with fewer parameters and achieves state-of-the-art results on the KITTI benchmark. Moreover, it owns a powerful generalization ability under various depth densities, varying lighting, and weather conditions.



### Learning a Task-specific Descriptor for Robust Matching of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2210.14899v1
- **DOI**: 10.1109/TCSVT.2022.3195944
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14899v1)
- **Published**: 2022-10-26 17:57:23+00:00
- **Updated**: 2022-10-26 17:57:23+00:00
- **Authors**: Zhiyuan Zhang, Yuchao Dai, Bin Fan, Jiadai Sun, Mingyi He
- **Comment**: Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT) 2022
- **Journal**: None
- **Summary**: Existing learning-based point feature descriptors are usually task-agnostic, which pursue describing the individual 3D point clouds as accurate as possible. However, the matching task aims at describing the corresponding points consistently across different 3D point clouds. Therefore these too accurate features may play a counterproductive role due to the inconsistent point feature representations of correspondences caused by the unpredictable noise, partiality, deformation, \etc, in the local geometry. In this paper, we propose to learn a robust task-specific feature descriptor to consistently describe the correct point correspondence under interference. Born with an Encoder and a Dynamic Fusion module, our method EDFNet develops from two aspects. First, we augment the matchability of correspondences by utilizing their repetitive local structure. To this end, a special encoder is designed to exploit two input point clouds jointly for each point descriptor. It not only captures the local geometry of each point in the current point cloud by convolution, but also exploits the repetitive structure from paired point cloud by Transformer. Second, we propose a dynamical fusion module to jointly use different scale features. There is an inevitable struggle between robustness and discriminativeness of the single scale feature. Specifically, the small scale feature is robust since little interference exists in this small receptive field. But it is not sufficiently discriminative as there are many repetitive local structures within a point cloud. Thus the resultant descriptors will lead to many incorrect matches. In contrast, the large scale feature is more discriminative by integrating more neighborhood information. ...



### What's Different between Visual Question Answering for Machine "Understanding" Versus for Accessibility?
- **Arxiv ID**: http://arxiv.org/abs/2210.14966v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14966v1)
- **Published**: 2022-10-26 18:23:53+00:00
- **Updated**: 2022-10-26 18:23:53+00:00
- **Authors**: Yang Trista Cao, Kyle Seelman, Kyungjun Lee, Hal Daumé III
- **Comment**: None
- **Journal**: AACL-IJCNLP 2022 The 2nd Conference of the Asia-Pacific Chapter of
  the Association for Computational Linguistics and the 12th International
  Joint Conference on Natural Language Processing
- **Summary**: In visual question answering (VQA), a machine must answer a question given an associated image. Recently, accessibility researchers have explored whether VQA can be deployed in a real-world setting where users with visual impairments learn about their environment by capturing their visual surroundings and asking questions. However, most of the existing benchmarking datasets for VQA focus on machine "understanding" and it remains unclear how progress on those datasets corresponds to improvements in this real-world use case. We aim to answer this question by evaluating discrepancies between machine "understanding" datasets (VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA models. Based on our findings, we discuss opportunities and challenges in VQA for accessibility and suggest directions for future work.



### SINCO: A Novel structural regularizer for image compression using implicit neural representations
- **Arxiv ID**: http://arxiv.org/abs/2210.14974v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14974v1)
- **Published**: 2022-10-26 18:35:54+00:00
- **Updated**: 2022-10-26 18:35:54+00:00
- **Authors**: Harry Gao, Weijie Gan, Zhixin Sun, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit neural representations (INR) have been recently proposed as deep learning (DL) based solutions for image compression. An image can be compressed by training an INR model with fewer weights than the number of image pixels to map the coordinates of the image to corresponding pixel values. While traditional training approaches for INRs are based on enforcing pixel-wise image consistency, we propose to further improve image quality by using a new structural regularizer. We present structural regularization for INR compression (SINCO) as a novel INR method for image compression. SINCO imposes structural consistency of the compressed images to the groundtruth by using a segmentation network to penalize the discrepancy of segmentation masks predicted from compressed images. We validate SINCO on brain MRI images by showing that it can achieve better performance than some recent INR methods.



### Fast and Efficient Scene Categorization for Autonomous Driving using VAEs
- **Arxiv ID**: http://arxiv.org/abs/2210.14981v1
- **DOI**: 10.56541/SUHE3553
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14981v1)
- **Published**: 2022-10-26 18:50:15+00:00
- **Updated**: 2022-10-26 18:50:15+00:00
- **Authors**: Saravanabalagi Ramachandran, Jonathan Horgan, Ganesh Sistu, John McDonald
- **Comment**: Published in the 24th Irish Machine Vision and Image Processing
  Conference (IMVIP 2022)
- **Journal**: The 24th Irish Machine Vision and Image Processing Conference
  (IMVIP), 2022, 9-16
- **Summary**: Scene categorization is a useful precursor task that provides prior knowledge for many advanced computer vision tasks with a broad range of applications in content-based image indexing and retrieval systems. Despite the success of data driven approaches in the field of computer vision such as object detection, semantic segmentation, etc., their application in learning high-level features for scene recognition has not achieved the same level of success. We propose to generate a fast and efficient intermediate interpretable generalized global descriptor that captures coarse features from the image and use a classification head to map the descriptors to 3 scene categories: Rural, Urban and Suburban. We train a Variational Autoencoder in an unsupervised manner and map images to a constrained multi-dimensional latent space and use the latent vectors as compact embeddings that serve as global descriptors for images. The experimental results evidence that the VAE latent vectors capture coarse information from the image, supporting their usage as global descriptors. The proposed global descriptor is very compact with an embedding length of 128, significantly faster to compute, and is robust to seasonal and illuminational changes, while capturing sufficient scene information required for scene categorization.



### LiDAR-guided object search and detection in Subterranean Environments
- **Arxiv ID**: http://arxiv.org/abs/2210.14997v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14997v1)
- **Published**: 2022-10-26 19:38:19+00:00
- **Updated**: 2022-10-26 19:38:19+00:00
- **Authors**: Manthan Patel, Gabriel Waibel, Shehryar Khattak, Marco Hutter
- **Comment**: 6 pages, 5 Figures, 2 Tables, conference: IEEE International
  Symposium on Safety, Security and Rescue Robotics (SSRR-2022), Seville, Spain
- **Journal**: None
- **Summary**: Detecting objects of interest, such as human survivors, safety equipment, and structure access points, is critical to any search-and-rescue operation. Robots deployed for such time-sensitive efforts rely on their onboard sensors to perform their designated tasks. However, as disaster response operations are predominantly conducted under perceptually degraded conditions, commonly utilized sensors such as visual cameras and LiDARs suffer in terms of performance degradation. In response, this work presents a method that utilizes the complementary nature of vision and depth sensors to leverage multi-modal information to aid object detection at longer distances. In particular, depth and intensity values from sparse LiDAR returns are used to generate proposals for objects present in the environment. These proposals are then utilized by a Pan-Tilt-Zoom (PTZ) camera system to perform a directed search by adjusting its pose and zoom level for performing object detection and classification in difficult environments. The proposed work has been thoroughly verified using an ANYmal quadruped robot in underground settings and on datasets collected during the DARPA Subterranean Challenge finals.



### Trade-off between reconstruction loss and feature alignment for domain generalization
- **Arxiv ID**: http://arxiv.org/abs/2210.15000v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15000v1)
- **Published**: 2022-10-26 19:40:25+00:00
- **Updated**: 2022-10-26 19:40:25+00:00
- **Authors**: Thuan Nguyen, Boyang Lyu, Prakash Ishwar, Matthias Scheutz, Shuchin Aeron
- **Comment**: 13 pages, 2 tables
- **Journal**: International Conference on Machine Learning and Applications
  (ICMLA-2022)
- **Summary**: Domain generalization (DG) is a branch of transfer learning that aims to train the learning models on several seen domains and subsequently apply these pre-trained models to other unseen (unknown but related) domains. To deal with challenging settings in DG where both data and label of the unseen domain are not available at training time, the most common approach is to design the classifiers based on the domain-invariant representation features, i.e., the latent representations that are unchanged and transferable between domains. Contrary to popular belief, we show that designing classifiers based on invariant representation features alone is necessary but insufficient in DG. Our analysis indicates the necessity of imposing a constraint on the reconstruction loss induced by representation functions to preserve most of the relevant information about the label in the latent space. More importantly, we point out the trade-off between minimizing the reconstruction loss and achieving domain alignment in DG. Our theoretical results motivate a new DG framework that jointly optimizes the reconstruction loss and the domain discrepancy. Both theoretical and numerical results are provided to justify our approach.



### Automatic Assessment of Infant Face and Upper-Body Symmetry as Early Signs of Torticollis
- **Arxiv ID**: http://arxiv.org/abs/2210.15022v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15022v2)
- **Published**: 2022-10-26 20:39:14+00:00
- **Updated**: 2022-11-07 17:04:16+00:00
- **Authors**: Michael Wan, Xiaofei Huang, Bethany Tunik, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: We apply computer vision pose estimation techniques developed expressly for the data-scarce infant domain to the study of torticollis, a common condition in infants for which early identification and treatment is critical. Specifically, we use a combination of facial landmark and body joint estimation techniques designed for infants to estimate a range of geometric measures pertaining to face and upper body symmetry, drawn from an array of sources in the physical therapy and ophthalmology research literature in torticollis. We gauge performance with a range of metrics and show that the estimates of most these geometric measures are successful, yielding strong to very strong Spearman's $\rho$ correlation with ground truth values. Furthermore, we show that these estimates, derived from pose estimation neural networks designed for the infant domain, cleanly outperform estimates derived from more widely known networks designed for the adult domain



### Addressing Heterogeneity in Federated Learning via Distributional Transformation
- **Arxiv ID**: http://arxiv.org/abs/2210.15025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15025v1)
- **Published**: 2022-10-26 20:42:01+00:00
- **Updated**: 2022-10-26 20:42:01+00:00
- **Authors**: Haolin Yuan, Bo Hui, Yuchen Yang, Philippe Burlina, Neil Zhenqiang Gong, Yinzhi Cao
- **Comment**: In the Proceedings of European Conference on Computer Vision (ECCV),
  2022
- **Journal**: None
- **Summary**: Federated learning (FL) allows multiple clients to collaboratively train a deep learning model. One major challenge of FL is when data distribution is heterogeneous, i.e., differs from one client to another. Existing personalized FL algorithms are only applicable to narrow cases, e.g., one or two data classes per client, and therefore they do not satisfactorily address FL under varying levels of data heterogeneity. In this paper, we propose a novel framework, called DisTrans, to improve FL performance (i.e., model accuracy) via train and test-time distributional transformations along with a double-input-channel model structure. DisTrans works by optimizing distributional offsets and models for each FL client to shift their data distribution, and aggregates these offsets at the FL server to further improve performance in case of distributional heterogeneity. Our evaluation on multiple benchmark datasets shows that DisTrans outperforms state-of-the-art FL methods and data augmentation methods under various settings and different degrees of client distributional heterogeneity.



### A novel information gain-based approach for classification and dimensionality reduction of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2210.15027v1
- **DOI**: 10.1016/j.procs.2019.01.016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15027v1)
- **Published**: 2022-10-26 20:59:57+00:00
- **Updated**: 2022-10-26 20:59:57+00:00
- **Authors**: Asma Elmaizi, Hasna Nhaila, Elkebir Sarhrouni, Ahmed Hammouch, Chafik Nacir
- **Comment**: None
- **Journal**: Procedia Computer Science, 2019, 148, pp. 126-134. DOI:
  10.1016/j.procs.2019.01.016 -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-85062681587&partnerID=MN8TOARS
- **Summary**: Recently, the hyperspectral sensors have improved our ability to monitor the earth surface with high spectral resolution. However, the high dimensionality of spectral data brings challenges for the image processing. Consequently, the dimensionality reduction is a necessary step in order to reduce the computational complexity and increase the classification accuracy. In this paper, we propose a new filter approach based on information gain for dimensionality reduction and classification of hyperspectral images. A special strategy based on hyperspectral bands selection is adopted to pick the most informative bands and discard the irrelevant and noisy ones. The algorithm evaluates the relevancy of the bands based on the information gain function with the support vector machine classifier. The proposed method is compared using two benchmark hyperspectral datasets (Indiana, Pavia) with three competing methods. The comparison results showed that the information gain filter approach outperforms the other methods on the tested datasets and could significantly reduce the computation cost while improving the classification accuracy. Keywords: Hyperspectral images; dimensionality reduction; information gain; classification accuracy.   Keywords: Hyperspectral images; dimensionality reduction; information gain; classification accuracy.



### FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning
- **Arxiv ID**: http://arxiv.org/abs/2210.15028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15028v1)
- **Published**: 2022-10-26 21:01:19+00:00
- **Updated**: 2022-10-26 21:01:19+00:00
- **Authors**: Suvir Mirchandani, Licheng Yu, Mengjiao Wang, Animesh Sinha, Wenwen Jiang, Tao Xiang, Ning Zhang
- **Comment**: 14 pages, 4 figures. To appear at Conference on Empirical Methods in
  Natural Language Processing (EMNLP) 2022
- **Journal**: None
- **Summary**: Multimodal tasks in the fashion domain have significant potential for e-commerce, but involve challenging vision-and-language learning problems - e.g., retrieving a fashion item given a reference image plus text feedback from a user. Prior works on multimodal fashion tasks have either been limited by the data in individual benchmarks, or have leveraged generic vision-and-language pre-training but have not taken advantage of the characteristics of fashion data. Additionally, these works have mainly been restricted to multimodal understanding tasks. To address these gaps, we make two key contributions. First, we propose a novel fashion-specific pre-training framework based on weakly-supervised triplets constructed from fashion image-text pairs. We show the triplet-based tasks are an effective addition to standard multimodal pre-training tasks. Second, we propose a flexible decoder-based model architecture capable of both fashion retrieval and captioning tasks. Together, our model design and pre-training approach are competitive on a diverse set of fashion tasks, including cross-modal retrieval, image retrieval with text feedback, image captioning, relative image captioning, and multimodal categorization.



### Multi-Scale Structural-aware Exposure Correction for Endoscopic Imaging
- **Arxiv ID**: http://arxiv.org/abs/2210.15033v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15033v1)
- **Published**: 2022-10-26 21:04:54+00:00
- **Updated**: 2022-10-26 21:04:54+00:00
- **Authors**: Axel Garcia-Vega, Ricardo Espinosa, Luis Ramirez-Guzman, Thomas Bazin, Luis Falcon-Morales, Gilberto Ochoa-Ruiz, Dominique Lamarque, Christian Daul
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Endoscopy is the most widely used imaging technique for the diagnosis of cancerous lesions in hollow organs. However, endoscopic images are often affected by illumination artefacts: image parts may be over- or underexposed according to the light source pose and the tissue orientation. These artifacts have a strong negative impact on the performance of computer vision or AI-based diagnosis tools. Although endoscopic image enhancement methods are greatly required, little effort has been devoted to over- and under-exposition enhancement in real-time. This contribution presents an extension to the objective function of LMSPEC, a method originally introduced to enhance images from natural scenes. It is used here for the exposure correction in endoscopic imaging and the preservation of structural information. To the best of our knowledge, this contribution is the first one that addresses the enhancement of endoscopic images using deep learning (DL) methods. Tested on the Endo4IE dataset, the proposed implementation has yielded a significant improvement over LMSPEC reaching a SSIM increase of 4.40% and 4.21% for over- and underexposed images, respectively.



### Generalization Differences between End-to-End and Neuro-Symbolic Vision-Language Reasoning Systems
- **Arxiv ID**: http://arxiv.org/abs/2210.15037v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15037v1)
- **Published**: 2022-10-26 21:11:47+00:00
- **Updated**: 2022-10-26 21:11:47+00:00
- **Authors**: Wang Zhu, Jesse Thomason, Robin Jia
- **Comment**: Accepted by the Findings of EMNLP 2022
- **Journal**: None
- **Summary**: For vision-and-language reasoning tasks, both fully connectionist, end-to-end methods and hybrid, neuro-symbolic methods have achieved high in-distribution performance. In which out-of-distribution settings does each paradigm excel? We investigate this question on both single-image and multi-image visual question-answering through four types of generalization tests: a novel segment-combine test for multi-image queries, contrast set, compositional generalization, and cross-benchmark transfer. Vision-and-language end-to-end trained systems exhibit sizeable performance drops across all these tests. Neuro-symbolic methods suffer even more on cross-benchmark transfer from GQA to VQA, but they show smaller accuracy drops on the other generalization tests and their performance quickly improves by few-shot training. Overall, our results demonstrate the complementary benefits of these two paradigms, and emphasize the importance of using a diverse suite of generalization tests to fully characterize model robustness to distribution shift.



### PERGAMO: Personalized 3D Garments from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2210.15040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15040v1)
- **Published**: 2022-10-26 21:15:54+00:00
- **Updated**: 2022-10-26 21:15:54+00:00
- **Authors**: Andrés Casado-Elvira, Marc Comino Trinidad, Dan Casas
- **Comment**: Published at Computer Graphics Forum (Proc. of ACM/SIGGRAPH SCA),
  2022. Project website http://mslab.es/projects/PERGAMO/
- **Journal**: None
- **Summary**: Clothing plays a fundamental role in digital humans. Current approaches to animate 3D garments are mostly based on realistic physics simulation, however, they typically suffer from two main issues: high computational run-time cost, which hinders their development; and simulation-to-real gap, which impedes the synthesis of specific real-world cloth samples. To circumvent both issues we propose PERGAMO, a data-driven approach to learn a deformable model for 3D garments from monocular images. To this end, we first introduce a novel method to reconstruct the 3D geometry of garments from a single image, and use it to build a dataset of clothing from monocular videos. We use these 3D reconstructions to train a regression model that accurately predicts how the garment deforms as a function of the underlying body pose. We show that our method is capable of producing garment animations that match the real-world behaviour, and generalizes to unseen body motions extracted from motion capture dataset.



### Generative modeling of the enteric nervous system employing point pattern analysis and graph construction
- **Arxiv ID**: http://arxiv.org/abs/2210.15044v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, q-bio.QM, stat.OT, 92E99, 05C10, 62H11, I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2210.15044v1)
- **Published**: 2022-10-26 21:22:41+00:00
- **Updated**: 2022-10-26 21:22:41+00:00
- **Authors**: Abida Sanjana Shemonti, Joshua D. Eisenberg, Robert O. Heuckeroth, Marthe J. Howard, Alex Pothen, Bartek Rajwa
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: We describe a generative network model of the architecture of the enteric nervous system (ENS) in the colon employing data from images of human and mouse tissue samples obtained through confocal microscopy. Our models combine spatial point pattern analysis with graph generation to characterize the spatial and topological properties of the ganglia (clusters of neurons and glial cells), the inter-ganglionic connections, and the neuronal organization within the ganglia. We employ a hybrid hardcore-Strauss process for spatial patterns and a planar random graph generation for constructing the spatially embedded network. We show that our generative model may be helpful in both basic and translational studies, and it is sufficiently expressive to model the ENS architecture of individuals who vary in age and health status. Increased understanding of the ENS connectome will enable the use of neuromodulation strategies in treatment and clarify anatomic diagnostic criteria for people with bowel motility disorders.



### SeaDroneSim: Simulation of Aerial Images for Detection of Objects Above Water
- **Arxiv ID**: http://arxiv.org/abs/2210.16107v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.16107v3)
- **Published**: 2022-10-26 21:50:50+00:00
- **Updated**: 2022-11-18 22:54:06+00:00
- **Authors**: Xiaomin Lin, Cheng Liu, Allen Pattillo, Miao Yu, Yiannis Aloimonous
- **Comment**: None
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) are known for their fast and versatile applicability. With UAVs' growth in availability and applications, they are now of vital importance in serving as technological support in search-and-rescue(SAR) operations in marine environments. High-resolution cameras and GPUs can be equipped on the UAVs to provide effective and efficient aid to emergency rescue operations. With modern computer vision algorithms, we can detect objects for aiming such rescue missions. However, these modern computer vision algorithms are dependent on numerous amounts of training data from UAVs, which is time-consuming and labor-intensive for maritime environments. To this end, we present a new benchmark suite, SeaDroneSim, that can be used to create photo-realistic aerial image datasets with the ground truth for segmentation masks of any given object. Utilizing only the synthetic data generated from SeaDroneSim, we obtain 71 mAP on real aerial images for detecting BlueROV as a feasibility study. This result from the new simulation suit also serves as a baseline for the detection of BlueROV.



### Automated Reconstruction of 3D Open Surfaces from Sparse Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2210.15059v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.15059v2)
- **Published**: 2022-10-26 22:02:45+00:00
- **Updated**: 2023-01-18 20:28:42+00:00
- **Authors**: Mohammad Samiul Arshad, William J. Beksi
- **Comment**: To be presented at the 2022 IEEE International Symposium on Mixed and
  Augmented Reality (ISMAR) Workshop on Photorealistic Image and Environment
  Synthesis for Mixed Reality (PIES-MR)
- **Journal**: None
- **Summary**: Real-world 3D data may contain intricate details defined by salient surface gaps. Automated reconstruction of these open surfaces (e.g., non-watertight meshes) is a challenging problem for environment synthesis in mixed reality applications. Current learning-based implicit techniques can achieve high fidelity on closed-surface reconstruction. However, their dependence on the distinction between the inside and outside of a surface makes them incapable of reconstructing open surfaces. Recently, a new class of implicit functions have shown promise in reconstructing open surfaces by regressing an unsigned distance field. Yet, these methods rely on a discretized representation of the raw data, which loses important surface details and can lead to outliers in the reconstruction. We propose IPVNet, a learning-based implicit model that predicts the unsigned distance between a surface and a query point in 3D space by leveraging both raw point cloud data and its discretized voxel counterpart. Experiments on synthetic and real-world public datasets demonstrates that IPVNet outperforms the state of the art while producing far fewer outliers in the reconstruction.



### Improving Adversarial Robustness with Self-Paced Hard-Class Pair Reweighting
- **Arxiv ID**: http://arxiv.org/abs/2210.15068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15068v2)
- **Published**: 2022-10-26 22:51:36+00:00
- **Updated**: 2022-11-29 20:09:41+00:00
- **Authors**: Pengyue Hou, Jie Han, Xingyu Li
- **Comment**: AAAI-23
- **Journal**: None
- **Summary**: Deep Neural Networks are vulnerable to adversarial attacks. Among many defense strategies, adversarial training with untargeted attacks is one of the most effective methods. Theoretically, adversarial perturbation in untargeted attacks can be added along arbitrary directions and the predicted labels of untargeted attacks should be unpredictable. However, we find that the naturally imbalanced inter-class semantic similarity makes those hard-class pairs become virtual targets of each other. This study investigates the impact of such closely-coupled classes on adversarial attacks and develops a self-paced reweighting strategy in adversarial training accordingly. Specifically, we propose to upweight hard-class pair losses in model optimization, which prompts learning discriminative features from hard classes. We further incorporate a term to quantify hard-class pair consistency in adversarial training, which greatly boosts model robustness. Extensive experiments show that the proposed adversarial training method achieves superior robustness performance over state-of-the-art defenses against a wide range of adversarial attacks.



### IDEAL: Improved DEnse locAL Contrastive Learning for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.15075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15075v2)
- **Published**: 2022-10-26 23:11:02+00:00
- **Updated**: 2023-03-02 08:25:37+00:00
- **Authors**: Hritam Basak, Soumitri Chattopadhyay, Rohit Kundu, Sayan Nag, Rammohan Mallipeddi
- **Comment**: Paper accepted for publication at IEEE ICASSP 2023
- **Journal**: None
- **Summary**: Due to the scarcity of labeled data, Contrastive Self-Supervised Learning (SSL) frameworks have lately shown great potential in several medical image analysis tasks. However, the existing contrastive mechanisms are sub-optimal for dense pixel-level segmentation tasks due to their inability to mine local features. To this end, we extend the concept of metric learning to the segmentation task, using a dense (dis)similarity learning for pre-training a deep encoder network, and employing a semi-supervised paradigm to fine-tune for the downstream task. Specifically, we propose a simple convolutional projection head for obtaining dense pixel-level features, and a new contrastive loss to utilize these dense projections thereby improving the local representations. A bidirectional consistency regularization mechanism involving two-stream model training is devised for the downstream task. Upon comparison, our IDEAL method outperforms the SoTA methods by fair margins on cardiac MRI segmentation. Code available: https://github.com/hritam-98/IDEAL-ICASSP23




# Arxiv Papers in cs.CV on 2022-10-19
### Non-iterative optimization of pseudo-labeling thresholds for training object detection models from multiple datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.10221v1
- **DOI**: 10.1109/ICIP46576.2022.9898014
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10221v1)
- **Published**: 2022-10-19 00:31:34+00:00
- **Updated**: 2022-10-19 00:31:34+00:00
- **Authors**: Yuki Tanaka, Shuhei M. Yoshida, Makoto Terao
- **Comment**: ICIP2022
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP),
  2022, pp. 1676-1680
- **Summary**: We propose a non-iterative method to optimize pseudo-labeling thresholds for learning object detection from a collection of low-cost datasets, each of which is annotated for only a subset of all the object classes. A popular approach to this problem is first to train teacher models and then to use their confident predictions as pseudo ground-truth labels when training a student model. To obtain the best result, however, thresholds for prediction confidence must be adjusted. This process typically involves iterative search and repeated training of student models and is time-consuming. Therefore, we develop a method to optimize the thresholds without iterative optimization by maximizing the $F_\beta$-score on a validation dataset, which measures the quality of pseudo labels and can be measured without training a student model. We experimentally demonstrate that our proposed method achieves an mAP comparable to that of grid search on the COCO and VOC datasets.



### A Real-Time Wrong-Way Vehicle Detection Based on YOLO and Centroid Tracking
- **Arxiv ID**: http://arxiv.org/abs/2210.10226v1
- **DOI**: 10.1109/TENSYMP50017.2020.9230463
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10226v1)
- **Published**: 2022-10-19 00:53:28+00:00
- **Updated**: 2022-10-19 00:53:28+00:00
- **Authors**: Zillur Rahman, Amit Mazumder Ami, Muhammad Ahsan Ullah
- **Comment**: 5 pages
- **Journal**: 2020 IEEE Region 10 Symposium (TENSYMP), page:916-920
- **Summary**: Wrong-way driving is one of the main causes of road accidents and traffic jam all over the world. By detecting wrong-way vehicles, the number of accidents can be minimized and traffic jam can be reduced. With the increasing popularity of real-time traffic management systems and due to the availability of cheaper cameras, the surveillance video has become a big source of data. In this paper, we propose an automatic wrong-way vehicle detection system from on-road surveillance camera footage. Our system works in three stages: the detection of vehicles from the video frame by using the You Only Look Once (YOLO) algorithm, track each vehicle in a specified region of interest using centroid tracking algorithm and detect the wrong-way driving vehicles. YOLO is very accurate in object detection and the centroid tracking algorithm can track any moving object efficiently. Experiment with some traffic videos shows that our proposed system can detect and identify any wrong-way vehicle in different light and weather conditions. The system is very simple and easy to implement.



### Vision-Based Robust Lane Detection and Tracking under Different Challenging Environmental Conditions
- **Arxiv ID**: http://arxiv.org/abs/2210.10233v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10233v3)
- **Published**: 2022-10-19 01:25:21+00:00
- **Updated**: 2023-06-15 03:35:28+00:00
- **Authors**: Samia Sultana, Boshir Ahmed, Manoranjan Paul, Muhammad Rafiqul Islam, Shamim Ahmad
- **Comment**: 19 pages, 11 figures, submitted to IEEE Access
- **Journal**: None
- **Summary**: Lane marking detection is fundamental for both advanced driving assistance systems. However, detecting lane is highly challenging when the visibility of a road lane marking is low due to real-life challenging environment and adverse weather. Most of the lane detection methods suffer from four types of challenges: (i) light effects i.e., shadow, glare of light, reflection etc.; (ii) Obscured visibility of eroded, blurred, colored and cracked lane caused by natural disasters and adverse weather; (iii) lane marking occlusion by different objects from surroundings (wiper, vehicles etc.); and (iv) presence of confusing lane like lines inside the lane view e.g., guardrails, pavement marking, road divider etc. Here, we propose a robust lane detection and tracking method with three key technologies. First, we introduce a comprehensive intensity threshold range (CITR) to improve the performance of the canny operator in detecting low intensity lane edges. Second, we propose a two-step lane verification technique, the angle based geometric constraint (AGC) and length-based geometric constraint (LGC) followed by Hough Transform, to verify the characteristics of lane marking and to prevent incorrect lane detection. Finally, we propose a novel lane tracking technique, by defining a range of horizontal lane position (RHLP) along the x axis which will be updating with respect to the lane position of previous frame. It can keep track of the lane position when either left or right or both lane markings are partially and fully invisible. To evaluate the performance of the proposed method we used the DSDLDE [1] and SLD [2] dataset with 1080x1920 and 480x720 resolutions at 24 and 25 frames/sec respectively. Experimental results show that the average detection rate is 97.55%, and the average processing time is 22.33 msec/frame, which outperform the state of-the-art method.



### GSV-Cities: Toward Appropriate Supervised Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.10239v1
- **DOI**: 10.1016/j.neucom.2022.09.127
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10239v1)
- **Published**: 2022-10-19 01:39:29+00:00
- **Updated**: 2022-10-19 01:39:29+00:00
- **Authors**: Amar Ali-bey, Brahim Chaib-draa, Philippe Giguère
- **Comment**: Neurocomputing 2022
- **Journal**: None
- **Summary**: This paper aims to investigate representation learning for large scale visual place recognition, which consists of determining the location depicted in a query image by referring to a database of reference images. This is a challenging task due to the large-scale environmental changes that can occur over time (i.e., weather, illumination, season, traffic, occlusion). Progress is currently challenged by the lack of large databases with accurate ground truth. To address this challenge, we introduce GSV-Cities, a new image dataset providing the widest geographic coverage to date with highly accurate ground truth, covering more than 40 cities across all continents over a 14-year period. We subsequently explore the full potential of recent advances in deep metric learning to train networks specifically for place recognition, and evaluate how different loss functions influence performance. In addition, we show that performance of existing methods substantially improves when trained on GSV-Cities. Finally, we introduce a new fully convolutional aggregation layer that outperforms existing techniques, including GeM, NetVLAD and CosPlace, and establish a new state-of-the-art on large-scale benchmarks, such as Pittsburgh, Mapillary-SLS, SPED and Nordland. The dataset and code are available for research purposes at https://github.com/amaralibey/gsv-cities.



### Performance of different machine learning methods on activity recognition and pose estimation datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.10247v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/2210.10247v1)
- **Published**: 2022-10-19 02:07:43+00:00
- **Updated**: 2022-10-19 02:07:43+00:00
- **Authors**: Love Trivedi, Raviit Vij
- **Comment**: 14
- **Journal**: None
- **Summary**: With advancements in computer vision taking place day by day, recently a lot of light is being shed on activity recognition. With the range for real-world applications utilizing this field of study increasing across a multitude of industries such as security and healthcare, it becomes crucial for businesses to distinguish which machine learning methods perform better than others in the area. This paper strives to aid in this predicament i.e. building upon previous related work, it employs both classical and ensemble approaches on rich pose estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to evaluate the performance for each model, the results show that overall, random forest yields the highest accuracy in classifying ADLs. Relatively all the models have excellent performance across both datasets, except for logistic regression and AdaBoost perform poorly in the HAR one. With the limitations of this paper also discussed in the end, the scope for further research is vast, which can use this paper as a base in aims of producing better results.



### Discovering Limitations of Image Quality Assessments with Noised Deep Learning Image Sets
- **Arxiv ID**: http://arxiv.org/abs/2210.10249v2
- **DOI**: 10.1109/BigData55660.2022.10020507
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2210.10249v2)
- **Published**: 2022-10-19 02:15:09+00:00
- **Updated**: 2023-01-29 19:44:06+00:00
- **Authors**: Wei Dai, Daniel Berleant
- **Comment**: 10 pages, 11 figures, 10 tables
- **Journal**: 2022 IEEE International Conference on Big Data (Big Data), Osaka,
  Japan, 2022, pp. 3735-3744
- **Summary**: Image quality is important, and can affect overall performance in image processing and computer vision as well as for numerous other reasons. Image quality assessment (IQA) is consequently a vital task in different applications from aerial photography interpretation to object detection to medical image analysis. In previous research, the BRISQUE algorithm and the PSNR algorithm were evaluated with high resolution (atleast 512x384 pixels), but relatively small image sets (no more than 4,744 images). However, scientists have not evaluated IQA algorithms on low resolution (no more than 32x32 pixels), multi-perturbation, big image sets (for example, tleast 60,000 different images not counting their perturbations). This study explores these two IQA algorithms through experimental investigation. We first chose two deep learning image sets, CIFAR-10 and MNIST. Then, we added 68 perturbations that add noise to the images in specific sequences and noise intensities. In addition, we tracked the performance outputs of the two IQA algorithms with singly and multiply noised images. After quantitatively analyzing experimental results, we report the limitations of the two IQAs with these noised CIFAR-10 and MNIST image sets. We also explain three potential root causes for performance degradation. These findings point out weaknesses of the two IQA algorithms. The research results provide guidance to scientists and engineers developing accurate, robust IQA algorithms. All source codes, related image sets, and figures are shared on the website (https://github.com/caperock/imagequality) to support future scientific and industrial projects.



### On the Adversarial Robustness of Mixture of Experts
- **Arxiv ID**: http://arxiv.org/abs/2210.10253v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10253v1)
- **Published**: 2022-10-19 02:24:57+00:00
- **Updated**: 2022-10-19 02:24:57+00:00
- **Authors**: Joan Puigcerver, Rodolphe Jenatton, Carlos Riquelme, Pranjal Awasthi, Srinadh Bhojanapalli
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Adversarial robustness is a key desirable property of neural networks. It has been empirically shown to be affected by their sizes, with larger networks being typically more robust. Recently, Bubeck and Sellke proved a lower bound on the Lipschitz constant of functions that fit the training data in terms of their number of parameters. This raises an interesting open question, do -- and can -- functions with more parameters, but not necessarily more computational cost, have better robustness? We study this question for sparse Mixture of Expert models (MoEs), that make it possible to scale up the model size for a roughly constant computational cost. We theoretically show that under certain conditions on the routing and the structure of the data, MoEs can have significantly smaller Lipschitz constants than their dense counterparts. The robustness of MoEs can suffer when the highest weighted experts for an input implement sufficiently different functions. We next empirically evaluate the robustness of MoEs on ImageNet using adversarial attacks and show they are indeed more robust than dense models with the same computational cost. We make key observations showing the robustness of MoEs to the choice of experts, highlighting the redundancy of experts in models trained in practice.



### Time and Cost-Efficient Bathymetric Mapping System using Sparse Point Cloud Generation and Automatic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.10263v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10263v1)
- **Published**: 2022-10-19 02:58:08+00:00
- **Updated**: 2022-10-19 02:58:08+00:00
- **Authors**: Andres Pulido, Ruoyao Qin, Antonio Diaz, Andrew Ortega, Peter Ifju, Jaejeong Shin
- **Comment**: Submitted to OCEANS 2022
- **Journal**: None
- **Summary**: Generating 3D point cloud (PC) data from noisy sonar measurements is a problem that has potential applications for bathymetry mapping, artificial object inspection, mapping of aquatic plants and fauna as well as underwater navigation and localization of vehicles such as submarines. Side-scan sonar sensors are available in inexpensive cost ranges, especially in fish-finders, where the transducers are usually mounted to the bottom of a boat and can approach shallower depths than the ones attached to an Uncrewed Underwater Vehicle (UUV) can. However, extracting 3D information from side-scan sonar imagery is a difficult task because of its low signal-to-noise ratio and missing angle and depth information in the imagery. Since most algorithms that generate a 3D point cloud from side-scan sonar imagery use Shape from Shading (SFS) techniques, extracting 3D information is especially difficult when the seafloor is smooth, is slowly changing in depth, or does not have identifiable objects that make acoustic shadows. This paper introduces an efficient algorithm that generates a sparse 3D point cloud from side-scan sonar images. This computation is done in a computationally efficient manner by leveraging the geometry of the first sonar return combined with known positions provided by GPS and down-scan sonar depth measurement at each data point. Additionally, this paper implements another algorithm that uses a Convolutional Neural Network (CNN) using transfer learning to perform object detection on side-scan sonar images collected in real life and generated with a simulation. The algorithm was tested on both real and synthetic images to show reasonably accurate anomaly detection and classification.



### Synthetic Sonar Image Simulation with Various Seabed Conditions for Automatic Target Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.10267v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10267v1)
- **Published**: 2022-10-19 03:08:02+00:00
- **Updated**: 2022-10-19 03:08:02+00:00
- **Authors**: Jaejeong Shin, Shi Chang, Matthew Bays, Joshua Weaver, Tom Wettergren, Silvia Ferrari
- **Comment**: Submitted to OCEANS 2022
- **Journal**: None
- **Summary**: We propose a novel method to generate underwater object imagery that is acoustically compliant with that generated by side-scan sonar using the Unreal Engine. We describe the process to develop, tune, and generate imagery to provide representative images for use in training automated target recognition (ATR) and machine learning algorithms. The methods provide visual approximations for acoustic effects such as back-scatter noise and acoustic shadow, while allowing fast rendering with C++ actor in UE for maximizing the size of potential ATR training datasets. Additionally, we provide analysis of its utility as a replacement for actual sonar imagery or physics-based sonar data.



### Commonsense Knowledge from Scene Graphs for Textual Environments
- **Arxiv ID**: http://arxiv.org/abs/2210.14162v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.14162v1)
- **Published**: 2022-10-19 03:09:17+00:00
- **Updated**: 2022-10-19 03:09:17+00:00
- **Authors**: Tsunehiko Tanaka, Daiki Kimura, Michiaki Tatsubori
- **Comment**: AAAI-22 Workshop on Reinforcement Learning in Games
- **Journal**: None
- **Summary**: Text-based games are becoming commonly used in reinforcement learning as real-world simulation environments. They are usually imperfect information games, and their interactions are only in the textual modality. To challenge these games, it is effective to complement the missing information by providing knowledge outside the game, such as human common sense. However, such knowledge has only been available from textual information in previous works. In this paper, we investigate the advantage of employing commonsense reasoning obtained from visual datasets such as scene graph datasets. In general, images convey more comprehensive information compared with text for humans. This property enables to extract commonsense relationship knowledge more useful for acting effectively in a game. We compare the statistics of spatial relationships available in Visual Genome (a scene graph dataset) and ConceptNet (a text-based knowledge) to analyze the effectiveness of introducing scene graph datasets. We also conducted experiments on a text-based game task that requires commonsense reasoning. Our experimental results demonstrated that our proposed methods have higher and competitive performance than existing state-of-the-art methods.



### Training set cleansing of backdoor poisoning by self-supervised representation learning
- **Arxiv ID**: http://arxiv.org/abs/2210.10272v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10272v2)
- **Published**: 2022-10-19 03:29:58+00:00
- **Updated**: 2023-03-14 17:02:34+00:00
- **Authors**: H. Wang, S. Karami, O. Dia, H. Ritter, E. Emamjomeh-Zadeh, J. Chen, Z. Xiang, D. J. Miller, G. Kesidis
- **Comment**: None
- **Journal**: None
- **Summary**: A backdoor or Trojan attack is an important type of data poisoning attack against deep neural network (DNN) classifiers, wherein the training dataset is poisoned with a small number of samples that each possess the backdoor pattern (usually a pattern that is either imperceptible or innocuous) and which are mislabeled to the attacker's target class. When trained on a backdoor-poisoned dataset, a DNN behaves normally on most benign test samples but makes incorrect predictions to the target class when the test sample has the backdoor pattern incorporated (i.e., contains a backdoor trigger). Here we focus on image classification tasks and show that supervised training may build stronger association between the backdoor pattern and the associated target class than that between normal features and the true class of origin. By contrast, self-supervised representation learning ignores the labels of samples and learns a feature embedding based on images' semantic content. %We thus propose to use unsupervised representation learning to avoid emphasising backdoor-poisoned training samples and learn a similar feature embedding for samples of the same class. Using a feature embedding found by self-supervised representation learning, a data cleansing method, which combines sample filtering and re-labeling, is developed. Experiments on CIFAR-10 benchmark datasets show that our method achieves state-of-the-art performance in mitigating backdoor attacks.



### Towards Explaining Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2210.10275v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10275v2)
- **Published**: 2022-10-19 03:38:57+00:00
- **Updated**: 2023-06-20 04:30:42+00:00
- **Authors**: Sean Kulinski, David I. Inouye
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work focuses on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of optimal transport, where the candidate mappings are restricted to a set of interpretable mappings. We then inspect multiple quintessential use-cases of distribution shift in real-world tabular, text, and image datasets to showcase how our explanatory mappings provide a better balance between detail and interpretability than baseline explanations by both visual inspection and our PercentExplained metric.



### CLIP-Driven Fine-grained Text-Image Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2210.10276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10276v1)
- **Published**: 2022-10-19 03:43:12+00:00
- **Updated**: 2022-10-19 03:43:12+00:00
- **Authors**: Shuanglin Yan, Neng Dong, Liyan Zhang, Jinhui Tang
- **Comment**: None
- **Journal**: None
- **Summary**: TIReID aims to retrieve the image corresponding to the given text query from a pool of candidate images. Existing methods employ prior knowledge from single-modality pre-training to facilitate learning, but lack multi-modal correspondences. Besides, due to the substantial gap between modalities, existing methods embed the original modal features into the same latent space for cross-modal alignment. However, feature embedding may lead to intra-modal information distortion. Recently, CLIP has attracted extensive attention from researchers due to its powerful semantic concept learning capacity and rich multi-modal knowledge, which can help us solve the above problems. Accordingly, in the paper, we propose a CLIP-driven Fine-grained information excavation framework (CFine) to fully utilize the powerful knowledge of CLIP for TIReID. To transfer the multi-modal knowledge effectively, we perform fine-grained information excavation to mine intra-modal discriminative clues and inter-modal correspondences. Specifically, we first design a multi-grained global feature learning module to fully mine intra-modal discriminative local information, which can emphasize identity-related discriminative clues by enhancing the interactions between global image (text) and informative local patches (words). Secondly, cross-grained feature refinement (CFR) and fine-grained correspondence discovery (FCD) modules are proposed to establish the cross-grained and fine-grained interactions between modalities, which can filter out non-modality-shared image patches/words and mine cross-modal correspondences from coarse to fine. CFR and FCD are removed during inference to save computational costs. Note that the above process is performed in the original modality space without further feature embedding. Extensive experiments on multiple benchmarks demonstrate the superior performance of our method on TIReID.



### Dense but Efficient VideoQA for Intricate Compositional Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2210.10300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10300v1)
- **Published**: 2022-10-19 05:01:20+00:00
- **Updated**: 2022-10-19 05:01:20+00:00
- **Authors**: Jihyeon Lee, Wooyoung Kang, Eun-Sol Kim
- **Comment**: Accepted to WACV'23
- **Journal**: None
- **Summary**: It is well known that most of the conventional video question answering (VideoQA) datasets consist of easy questions requiring simple reasoning processes. However, long videos inevitably contain complex and compositional semantic structures along with the spatio-temporal axis, which requires a model to understand the compositional structures inherent in the videos. In this paper, we suggest a new compositional VideoQA method based on transformer architecture with a deformable attention mechanism to address the complex VideoQA tasks. The deformable attentions are introduced to sample a subset of informative visual features from the dense visual feature map to cover a temporally long range of frames efficiently. Furthermore, the dependency structure within the complex question sentences is also combined with the language embeddings to readily understand the relations among question words. Extensive experiments and ablation studies show that the suggested dense but efficient model outperforms other baselines.



### LAVA: Label-efficient Visual Learning and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.10317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10317v1)
- **Published**: 2022-10-19 06:19:14+00:00
- **Updated**: 2022-10-19 06:19:14+00:00
- **Authors**: Islam Nassar, Munawar Hayat, Ehsan Abbasnejad, Hamid Rezatofighi, Mehrtash Harandi, Gholamreza Haffari
- **Comment**: Accepted in WACV2023
- **Journal**: None
- **Summary**: We present LAVA, a simple yet effective method for multi-domain visual transfer learning with limited data. LAVA builds on a few recent innovations to enable adapting to partially labelled datasets with class and domain shifts. First, LAVA learns self-supervised visual representations on the source dataset and ground them using class label semantics to overcome transfer collapse problems associated with supervised pretraining. Secondly, LAVA maximises the gains from unlabelled target data via a novel method which uses multi-crop augmentations to obtain highly robust pseudo-labels. By combining these ingredients, LAVA achieves a new state-of-the-art on ImageNet semi-supervised protocol, as well as on 7 out of 10 datasets in multi-domain few-shot learning on the Meta-dataset. Code and models are made available.



### WebtoonMe: A Data-Centric Approach for Full-Body Portrait Stylization
- **Arxiv ID**: http://arxiv.org/abs/2210.10335v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10335v2)
- **Published**: 2022-10-19 07:09:03+00:00
- **Updated**: 2022-10-27 05:01:19+00:00
- **Authors**: Jihye Back, Seungkwon Kim, Namhyuk Ahn
- **Comment**: SIGGRAPH Asia 2022 Technical Communications
- **Journal**: None
- **Summary**: Full-body portrait stylization, which aims to translate portrait photography into a cartoon style, has drawn attention recently. However, most methods have focused only on converting face regions, restraining the feasibility of use in real-world applications. A recently proposed two-stage method expands the rendering area to full bodies, but the outputs are less plausible and fail to achieve quality robustness of non-face regions. Furthermore, they cannot reflect diverse skin tones. In this study, we propose a data-centric solution to build a production-level full-body portrait stylization system. Based on the two-stage scheme, we construct a novel and advanced dataset preparation paradigm that can effectively resolve the aforementioned problems. Experiments reveal that with our pipeline, high-quality portrait stylization can be achieved without additional losses or architectural changes.



### Using deep convolutional neural networks to classify poisonous and edible mushrooms found in China
- **Arxiv ID**: http://arxiv.org/abs/2210.10351v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10351v1)
- **Published**: 2022-10-19 07:36:26+00:00
- **Updated**: 2022-10-19 07:36:26+00:00
- **Authors**: Baiming Zhang, Ying Zhao, Zhixiang Li
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Because of their abundance of amino acids, polysaccharides, and many other nutrients that benefit human beings, mushrooms are deservedly popular as dietary cuisine both worldwide and in China. However, if people eat poisonous fungi by mistake, they may suffer from nausea, vomiting, mental disorder, acute anemia, or even death. Each year in China, there are around 8000 people became sick, and 70 died as a result of eating toxic mushrooms by mistake. It is counted that there are thousands of kinds of mushrooms among which only around 900 types are edible, thus without specialized knowledge, the probability of eating toxic mushrooms by mistake is very high. Most people deem that the only characteristic of poisonous mushrooms is a bright colour, however, some kinds of them do not correspond to this trait. In order to prevent people from eating these poisonous mushrooms, we propose to use deep learning methods to indicate whether a mushroom is toxic through analyzing hundreds of edible and toxic mushrooms smartphone pictures. We crowdsource a mushroom image dataset that contains 250 images of poisonous mushrooms and 200 images of edible mushrooms. The Convolutional Neural Network (CNN) is a specialized type of artificial neural networks that use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers, which can generate a relatively precise result by analyzing a huge amount of images, and thus is very suitable for our research. The experimental results demonstrate that the proposed model has high credibility and can provide a decision-making basis for the selection of edible fungi, so as to reduce the morbidity and mortality caused by eating poisonous mushrooms. We also open source our hand collected mushroom image dataset so that peer researchers can also deploy their own model to advance poisonous mushroom identification.



### Temporal Action Segmentation: An Analysis of Modern Techniques
- **Arxiv ID**: http://arxiv.org/abs/2210.10352v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10352v4)
- **Published**: 2022-10-19 07:40:47+00:00
- **Updated**: 2023-08-12 13:07:18+00:00
- **Authors**: Guodong Ding, Fadime Sener, Angela Yao
- **Comment**: 19 pages, 9 figures, 8 tables
- **Journal**: None
- **Summary**: Temporal action segmentation (TAS) in videos aims at densely identifying video frames in minutes-long videos with multiple action classes. As a long-range video understanding task, researchers have developed an extended collection of methods and examined their performance using various benchmarks. Despite the rapid growth of TAS techniques in recent years, no systematic survey has been conducted in these sectors. This survey analyzes and summarizes the most significant contributions and trends. In particular, we first examine the task definition, common benchmarks, types of supervision, and prevalent evaluation measures. In addition, we systematically investigate two essential techniques of this topic, i.e., frame representation and temporal modeling, which have been studied extensively in the literature. We then conduct a thorough review of existing TAS works categorized by their levels of supervision and conclude our survey by identifying and emphasizing several research gaps. In addition, we have curated a list of TAS resources, which is available at https://github.com/nus-cvml/awesome-temporal-action-segmentation.



### Stating Comparison Score Uncertainty and Verification Decision Confidence Towards Transparent Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.10354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10354v1)
- **Published**: 2022-10-19 07:43:48+00:00
- **Updated**: 2022-10-19 07:43:48+00:00
- **Authors**: Marco Huber, Philipp Terhörst, Florian Kirchbuchner, Naser Damer, Arjan Kuijper
- **Comment**: Accepted at British Machine Vision Conference (BMVC) 2022
- **Journal**: None
- **Summary**: Face Recognition (FR) is increasingly used in critical verification decisions and thus, there is a need for assessing the trustworthiness of such decisions. The confidence of a decision is often based on the overall performance of the model or on the image quality. We propose to propagate model uncertainties to scores and decisions in an effort to increase the transparency of verification decisions. This work presents two contributions. First, we propose an approach to estimate the uncertainty of face comparison scores. Second, we introduce a confidence measure of the system's decision to provide insights into the verification decision. The suitability of the comparison scores uncertainties and the verification decision confidences have been experimentally proven on three face recognition models on two datasets.



### CPL: Counterfactual Prompt Learning for Vision and Language Models
- **Arxiv ID**: http://arxiv.org/abs/2210.10362v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.10362v3)
- **Published**: 2022-10-19 08:06:39+00:00
- **Updated**: 2022-11-05 03:51:49+00:00
- **Authors**: Xuehai He, Diji Yang, Weixi Feng, Tsu-Jui Fu, Arjun Akula, Varun Jampani, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Eric Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Prompt tuning is a new few-shot transfer learning technique that only tunes the learnable prompt for pre-trained vision and language models such as CLIP. However, existing prompt tuning methods tend to learn spurious or entangled representations, which leads to poor generalization to unseen concepts. Towards non-spurious and efficient prompt learning from limited examples, this paper presents a novel \underline{\textbf{C}}ounterfactual \underline{\textbf{P}}rompt \underline{\textbf{L}}earning (CPL) method for vision and language models, which simultaneously employs counterfactual generation and contrastive learning in a joint optimization framework. Particularly, CPL constructs counterfactual by identifying minimal non-spurious feature change between semantically-similar positive and negative samples that causes concept change, and learns more generalizable prompt representation from both factual and counterfactual examples via contrastive learning. Extensive experiments demonstrate that CPL can obtain superior few-shot performance on different vision and language tasks than previous prompt tuning methods on CLIP. On image classification, we achieve 3.55\% average relative improvement on unseen classes across seven datasets; on image-text retrieval and visual question answering, we gain up to 4.09\% and 25.08\% relative improvements across three few-shot scenarios on unseen test sets respectively.



### Variational Model Perturbation for Source-Free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.10378v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10378v1)
- **Published**: 2022-10-19 08:41:19+00:00
- **Updated**: 2022-10-19 08:41:19+00:00
- **Authors**: Mengmeng Jing, Xiantong Zhen, Jingjing Li, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: We aim for source-free domain adaptation, where the task is to deploy a model pre-trained on source domains to target domains. The challenges stem from the distribution shift from the source to the target domain, coupled with the unavailability of any source data and labeled target data for optimization. Rather than fine-tuning the model by updating the parameters, we propose to perturb the source model to achieve adaptation to target domains. We introduce perturbations into the model parameters by variational Bayesian inference in a probabilistic framework. By doing so, we can effectively adapt the model to the target domain while largely preserving the discriminative ability. Importantly, we demonstrate the theoretical connection to learning Bayesian neural networks, which proves the generalizability of the perturbed model to target domains. To enable more efficient optimization, we further employ a parameter sharing strategy, which substantially reduces the learnable parameters compared to a fully Bayesian neural network. Our model perturbation provides a new probabilistic way for domain adaptation which enables efficient adaptation to target domains while maximally preserving knowledge in source models. Experiments on several source-free benchmarks under three different evaluation settings verify the effectiveness of the proposed variational model perturbation for source-free domain adaptation.



### Spatio-channel Attention Blocks for Cross-modal Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2210.10392v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10392v4)
- **Published**: 2022-10-19 09:05:00+00:00
- **Updated**: 2022-11-14 11:40:00+00:00
- **Authors**: Youjia Zhang, Soyun Choi, Sungeun Hong
- **Comment**: Accepted to ACCV 2022 (Oral). Code is available at
  https://github.com/vcllab/csca
- **Journal**: None
- **Summary**: Crowd counting research has made significant advancements in real-world applications, but it remains a formidable challenge in cross-modal settings. Most existing methods rely solely on the optical features of RGB images, ignoring the feasibility of other modalities such as thermal and depth images. The inherently significant differences between the different modalities and the diversity of design choices for model architectures make cross-modal crowd counting more challenging. In this paper, we propose Cross-modal Spatio-Channel Attention (CSCA) blocks, which can be easily integrated into any modality-specific architecture. The CSCA blocks first spatially capture global functional correlations among multi-modality with less overhead through spatial-wise cross-modal attention. Cross-modal features with spatial attention are subsequently refined through adaptive channel-wise feature aggregation. In our experiments, the proposed block consistently shows significant performance improvement across various backbone networks, resulting in state-of-the-art results in RGB-T and RGB-D crowd counting.



### Segmentation-free Direct Iris Localization Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.10403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10403v1)
- **Published**: 2022-10-19 09:13:39+00:00
- **Updated**: 2022-10-19 09:13:39+00:00
- **Authors**: Takahiro Toizumi, Koichi Takahashi, Masato Tsukada
- **Comment**: Accepted by WACV 2023
- **Journal**: None
- **Summary**: This paper proposes an efficient iris localization method without using iris segmentation and circle fitting. Conventional iris localization methods first extract iris regions by using semantic segmentation methods such as U-Net. Afterward, the inner and outer iris circles are localized using the traditional circle fitting algorithm. However, this approach requires high-resolution encoder-decoder networks for iris segmentation, so it causes computational costs to be high. In addition, traditional circle fitting tends to be sensitive to noise in input images and fitting parameters, causing the iris recognition performance to be poor. To solve these problems, we propose an iris localization network (ILN), that can directly localize pupil and iris circles with eyelid points from a low-resolution iris image. We also introduce a pupil refinement network (PRN) to improve the accuracy of pupil localization. Experimental results show that the combination of ILN and PRN works in 34.5 ms for one iris image on a CPU, and its localization performance outperforms conventional iris segmentation methods. In addition, generalized evaluation results show that the proposed method has higher robustness for datasets in different domain than other segmentation methods. Furthermore, we also confirm that the proposed ILN and PRN improve the iris recognition accuracy.



### Domain generalization Person Re-identification on Attention-aware multi-operation strategery
- **Arxiv ID**: http://arxiv.org/abs/2210.10409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10409v1)
- **Published**: 2022-10-19 09:18:46+00:00
- **Updated**: 2022-10-19 09:18:46+00:00
- **Authors**: Yingchun Guo, Huan He, Ye Zhu, Yang Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization person re-identification (DG Re-ID) aims to directly deploy a model trained on the source domain to the unseen target domain with good generalization, which is a challenging problem and has practical value in a real-world deployment. In the existing DG Re-ID methods, invariant operations are effective in extracting domain generalization features, and Instance Normalization (IN) or Batch Normalization (BN) is used to alleviate the bias to unseen domains. Due to domain-specific information being used to capture discriminability of the individual source domain, the generalized ability for unseen domains is unsatisfactory. To address this problem, an Attention-aware Multi-operation Strategery (AMS) for DG Re-ID is proposed to extract more generalized features. We investigate invariant operations and construct a multi-operation module based on IN and group whitening (GW) to extract domain-invariant feature representations. Furthermore, we analyze different domain-invariant characteristics, and apply spatial attention to the IN operation and channel attention to the GW operation to enhance the domain-invariant features. The proposed AMS module can be used as a plug-and-play module to incorporate into existing network architectures. Extensive experimental results show that AMS can effectively enhance the model's generalization ability to unseen domains and significantly improves the recognition performance in DG Re-ID on three protocols with ten datasets.



### Real Image Super-Resolution using GAN through modeling of LR and HR process
- **Arxiv ID**: http://arxiv.org/abs/2210.10413v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10413v1)
- **Published**: 2022-10-19 09:23:37+00:00
- **Updated**: 2022-10-19 09:23:37+00:00
- **Authors**: Rao Muhammad Umer, Christian Micheloni
- **Comment**: Accepted in 18th IEEE International Conference on Advanced Video and
  Signal Based Surveillance (AVSS), 2022. arXiv admin note: text overlap with
  arXiv:2009.03693, arXiv:2005.00953
- **Journal**: None
- **Summary**: The current existing deep image super-resolution methods usually assume that a Low Resolution (LR) image is bicubicly downscaled of a High Resolution (HR) image. However, such an ideal bicubic downsampling process is different from the real LR degradations, which usually come from complicated combinations of different degradation processes, such as camera blur, sensor noise, sharpening artifacts, JPEG compression, and further image editing, and several times image transmission over the internet and unpredictable noises. It leads to the highly ill-posed nature of the inverse upscaling problem. To address these issues, we propose a GAN-based SR approach with learnable adaptive sinusoidal nonlinearities incorporated in LR and SR models by directly learn degradation distributions and then synthesize paired LR/HR training data to train the generalized SR model to real image degradations. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments.



### High-Resolution Depth Estimation for 360-degree Panoramas through Perspective and Panoramic Depth Images Registration
- **Arxiv ID**: http://arxiv.org/abs/2210.10414v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10414v3)
- **Published**: 2022-10-19 09:25:12+00:00
- **Updated**: 2022-10-26 05:55:54+00:00
- **Authors**: Chi-Han Peng, Jiayao Zhang
- **Comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
  2023, to appear
- **Journal**: None
- **Summary**: We propose a novel approach to compute high-resolution (2048x1024 and higher) depths for panoramas that is significantly faster and qualitatively and qualitatively more accurate than the current state-of-the-art method (360MonoDepth). As traditional neural network-based methods have limitations in the output image sizes (up to 1024x512) due to GPU memory constraints, both 360MonoDepth and our method rely on stitching multiple perspective disparity or depth images to come out a unified panoramic depth map. However, to achieve globally consistent stitching, 360MonoDepth relied on solving extensive disparity map alignment and Poisson-based blending problems, leading to high computation time. Instead, we propose to use an existing panoramic depth map (computed in real-time by any panorama-based method) as the common target for the individual perspective depth maps to register to. This key idea made producing globally consistent stitching results from a straightforward task. Our experiments show that our method generates qualitatively better results than existing panorama-based methods, and further outperforms them quantitatively on datasets unseen by these methods.



### p$^3$VAE: a physics-integrated generative model. Application to the semantic segmentation of optical remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/2210.10418v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, 68T45, I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2210.10418v3)
- **Published**: 2022-10-19 09:32:15+00:00
- **Updated**: 2023-04-18 08:45:29+00:00
- **Authors**: Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet
- **Comment**: 21 pages, 11 figures, submitted to the International Journal of
  Computer Vision
- **Journal**: None
- **Summary**: The combination of machine learning models with physical models is a recent research path to learn robust data representations. In this paper, we introduce p$^3$VAE, a generative model that integrates a perfect physical model which partially explains the true underlying factors of variation in the data. To fully leverage our hybrid design, we propose a semi-supervised optimization procedure and an inference scheme that comes along meaningful uncertainty estimates. We apply p$^3$VAE to the semantic segmentation of high-resolution hyperspectral remote sensing images. Our experiments on a simulated data set demonstrated the benefits of our hybrid model against conventional machine learning models in terms of extrapolation capabilities and interpretability. In particular, we show that p$^3$VAE naturally has high disentanglement capabilities. Our code and data have been made publicly available at https://github.com/Romain3Ch216/p3VAE.



### Multi-view Gait Recognition based on Siamese Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.10421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10421v1)
- **Published**: 2022-10-19 09:38:54+00:00
- **Updated**: 2022-10-19 09:38:54+00:00
- **Authors**: Yanchen Yang, Lijun Yun, Ruoyu Li, Feiyan Cheng
- **Comment**: 13 pages,9 figures,1 table
- **Journal**: None
- **Summary**: While the Vision Transformer has been used in gait recognition, its application in multi-view gait recognition is still limited. Different views significantly affect the extraction and identification accuracy of the characteristics of gait contour. To address this, this paper proposes a Siamese Mobile Vision Transformer (SMViT). This model not only focuses on the local characteristics of the human gait space but also considers the characteristics of long-distance attention associations, which can extract multi-dimensional step status characteristics. In addition, it describes how different perspectives affect gait characteristics and generate reliable perspective feature relationship factors. The average recognition rate of SMViT on the CASIA B data set reached 96.4%. The experimental results show that SMViT can attain state-of-the-art performance compared to advanced step recognition models such as GaitGAN, Multi_view GAN, Posegait and other gait recognition models.



### Pseudo-Label Noise Suppression Techniques for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.10426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10426v1)
- **Published**: 2022-10-19 09:46:27+00:00
- **Updated**: 2022-10-19 09:46:27+00:00
- **Authors**: Sebastian Scherer, Robin Schön, Rainer Lienhart
- **Comment**: Accepted to BMVC 2022
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) can reduce the need for large labelled datasets by incorporating unlabelled data into the training. This is particularly interesting for semantic segmentation, where labelling data is very costly and time-consuming. Current SSL approaches use an initially supervised trained model to generate predictions for unlabelled images, called pseudo-labels, which are subsequently used for training a new model from scratch. Since the predictions usually do not come from an error-free neural network, they are naturally full of errors. However, training with partially incorrect labels often reduce the final model performance. Thus, it is crucial to manage errors/noise of pseudo-labels wisely. In this work, we use three mechanisms to control pseudo-label noise and errors: (1) We construct a solid base framework by mixing images with cow-patterns on unlabelled images to reduce the negative impact of wrong pseudo-labels. Nevertheless, wrong pseudo-labels still have a negative impact on the performance. Therefore, (2) we propose a simple and effective loss weighting scheme for pseudo-labels defined by the feedback of the model trained on these pseudo-labels. This allows us to soft-weight the pseudo-label training examples based on their determined confidence score during training. (3) We also study the common practice to ignore pseudo-labels with low confidence and empirically analyse the influence and effect of pseudo-labels with different confidence ranges on SSL and the contribution of pseudo-label filtering to the achievable performance gains. We show that our method performs superior to state of-the-art alternatives on various datasets. Furthermore, we show that our findings also transfer to other tasks such as human pose estimation. Our code is available at https://github.com/ChristmasFan/SSL_Denoising_Segmentation.



### MC-hands-1M: A glove-wearing hand dataset for pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.10428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10428v1)
- **Published**: 2022-10-19 09:50:04+00:00
- **Updated**: 2022-10-19 09:50:04+00:00
- **Authors**: Prodromos Boutis, Zisis Batzos, Konstantinos Konstantoudakis, Anastasios Dimou, Petros Daras
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, the need for large amounts of carefully and complexly annotated data for the training of computer vision modules continues to grow. Furthermore, although the research community presents state of the art solutions to many problems, there exist special cases, such as the pose estimation and tracking of a glove-wearing hand, where the general approaches tend to be unable to provide an accurate solution or fail completely. In this work, we are presenting a synthetic dataset1 for 3D pose estimation of glove-wearing hands, in order to depict the value of data synthesis in computer vision. The dataset is used to fine-tune a public hand joint detection model, achieving significant performance in both synthetic and real images of glove-wearing hands.



### Hierarchical Reinforcement Learning for Furniture Layout in Virtual Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2210.10431v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10431v1)
- **Published**: 2022-10-19 09:58:10+00:00
- **Updated**: 2022-10-19 09:58:10+00:00
- **Authors**: Xinhan Di, Pengqian Yu
- **Comment**: Accepted by Reinforcement Learning for Real Life Workshop @ NeurIPS
  2022
- **Journal**: None
- **Summary**: In real life, the decoration of 3D indoor scenes through designing furniture layout provides a rich experience for people. In this paper, we explore the furniture layout task as a Markov decision process (MDP) in virtual reality, which is solved by hierarchical reinforcement learning (HRL). The goal is to produce a proper two-furniture layout in the virtual reality of the indoor scenes. In particular, we first design a simulation environment and introduce the HRL formulation for a two-furniture layout. We then apply a hierarchical actor-critic algorithm with curriculum learning to solve the MDP. We conduct our experiments on a large-scale real-world interior layout dataset that contains industrial designs from professional designers. Our numerical results demonstrate that the proposed model yields higher-quality layouts as compared with the state-of-art models.



### A scan-specific unsupervised method for parallel MRI reconstruction via implicit neural representation
- **Arxiv ID**: http://arxiv.org/abs/2210.10439v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10439v1)
- **Published**: 2022-10-19 10:16:03+00:00
- **Updated**: 2022-10-19 10:16:03+00:00
- **Authors**: Ruimin Feng, Qing Wu, Yuyao Zhang, Hongjiang Wei
- **Comment**: conference
- **Journal**: None
- **Summary**: Parallel imaging is a widely-used technique to accelerate magnetic resonance imaging (MRI). However, current methods still perform poorly in reconstructing artifact-free MRI images from highly undersampled k-space data. Recently, implicit neural representation (INR) has emerged as a new deep learning paradigm for learning the internal continuity of an object. In this study, we adopted INR to parallel MRI reconstruction. The MRI image was modeled as a continuous function of spatial coordinates. This function was parameterized by a neural network and learned directly from the measured k-space itself without additional fully sampled high-quality training data. Benefitting from the powerful continuous representations provided by INR, the proposed method outperforms existing methods by suppressing the aliasing artifacts and noise, especially at higher acceleration rates and smaller sizes of the auto-calibration signals. The high-quality results and scanning specificity make the proposed method hold the potential for further accelerating the data acquisition of parallel MRI.



### Estimating the coverage in 3d reconstructions of the colon from colonoscopy videos
- **Arxiv ID**: http://arxiv.org/abs/2210.10459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10459v1)
- **Published**: 2022-10-19 10:53:34+00:00
- **Updated**: 2022-10-19 10:53:34+00:00
- **Authors**: Emmanuelle Muhlethaler, Erez Posner, Moshe Bouhnik
- **Comment**: Accepted at Imaging Systems for GI Endoscopy workshop, the 25th
  International Conference on Medical Image Computing and Computer Assisted
  Intervention - MICCAI 2022 ISGIE
- **Journal**: None
- **Summary**: Colonoscopy is the most common procedure for early detection and removal of polyps, a critical component of colorectal cancer prevention. Insufficient visual coverage of the colon surface during the procedure often results in missed polyps. To mitigate this issue, reconstructing the 3D surfaces of the colon in order to visualize the missing regions has been proposed. However, robustly estimating the local and global coverage from such a reconstruction has not been thoroughly investigated until now. In this work, we present a new method to estimate the coverage from a reconstructed colon pointcloud. Our method splits a reconstructed colon into segments and estimates the coverage of each segment by estimating the area of the missing surfaces. We achieve a mean absolute coverage error of 3-6\% on colon segments generated from synthetic colonoscopy data and real colonography CT scans. In addition, we show good qualitative results on colon segments reconstructed from real colonoscopy videos.



### FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2210.10473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10473v2)
- **Published**: 2022-10-19 11:31:38+00:00
- **Updated**: 2022-10-21 09:46:19+00:00
- **Authors**: Felix Rosberg, Eren Erdal Aksoy, Fernando Alonso-Fernandez, Cristofer Englund
- **Comment**: Fixed the supplementary material layout in the end (past references).
  Added link to video results, which is mentioned in Results but was missing in
  the supplementary material
- **Journal**: None
- **Summary**: In this work, we present a new single-stage method for subject agnostic face swapping and identity transfer, named FaceDancer. We have two major contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature Similarity Regularization (IFSR). The AFFA module is embedded in the decoder and adaptively learns to fuse attribute features and features conditioned on identity information without requiring any additional facial segmentation process. In IFSR, we leverage the intermediate features in an identity encoder to preserve important attributes such as head pose, facial expression, lighting, and occlusion in the target face, while still transferring the identity of the source face with high fidelity. We conduct extensive quantitative and qualitative experiments on various datasets and show that the proposed FaceDancer outperforms other state-of-the-art networks in terms of identityn transfer, while having significantly better pose preservation than most of the previous methods.



### Video super-resolution for single-photon LIDAR
- **Arxiv ID**: http://arxiv.org/abs/2210.10474v1
- **DOI**: 10.1364/OE.478308
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2210.10474v1)
- **Published**: 2022-10-19 11:33:29+00:00
- **Updated**: 2022-10-19 11:33:29+00:00
- **Authors**: Germán Mora Martín, Stirling Scholes, Alice Ruget, Robert K. Henderson, Jonathan Leach, Istvan Gyongy
- **Comment**: 18 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: 3D Time-of-Flight (ToF) image sensors are used widely in applications such as self-driving cars, Augmented Reality (AR) and robotics. When implemented with Single-Photon Avalanche Diodes (SPADs), compact, array format sensors can be made that offer accurate depth maps over long distances, without the need for mechanical scanning. However, array sizes tend to be small, leading to low lateral resolution, which combined with low Signal-to-Noise Ratio (SNR) levels under high ambient illumination, may lead to difficulties in scene interpretation. In this paper, we use synthetic depth sequences to train a 3D Convolutional Neural Network (CNN) for denoising and upscaling (x4) depth data. Experimental results, based on synthetic as well as real ToF data, are used to demonstrate the effectiveness of the scheme. With GPU acceleration, frames are processed at >30 frames per second, making the approach suitable for low-latency imaging, as required for obstacle avoidance.



### RLM-Tracking: Online Multi-Pedestrian Tracking Supported by Relative Location Mapping
- **Arxiv ID**: http://arxiv.org/abs/2210.10477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10477v1)
- **Published**: 2022-10-19 11:37:14+00:00
- **Updated**: 2022-10-19 11:37:14+00:00
- **Authors**: Kai Ren, Chuanping Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of multi-object tracking is a fundamental computer vision research focus, widely used in public safety, transport, autonomous vehicles, robotics, and other regions involving artificial intelligence. Because of the complexity of natural scenes, object occlusion and semi-occlusion usually occur in fundamental tracking tasks. These can easily lead to ID switching, object loss, detect errors, and misaligned limitation boxes. These conditions have a significant impact on the precision of multi-object tracking. In this paper, we design a new multi-object tracker for the above issues that contains an object \textbf{Relative Location Mapping} (RLM) model and \textbf{Target Region Density} (TRD) model. The new tracker is more sensitive to the differences in position relationships between objects. It can introduce low-score detection frames into different regions in real-time according to the density of object regions in the video. This improves the accuracy of object tracking without consuming extensive arithmetic resources. Our study shows that the proposed model has considerably enhanced the HOTA and DF1 measurements on the MOT17 and MOT20 data sets when applied to the advanced MOT method.



### Cross-Modal Fusion Distillation for Fine-Grained Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.10486v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10486v1)
- **Published**: 2022-10-19 11:50:14+00:00
- **Updated**: 2022-10-19 11:50:14+00:00
- **Authors**: Abhra Chaudhuri, Massimiliano Mancini, Yanbei Chen, Zeynep Akata, Anjan Dutta
- **Comment**: British Machine Vision Conference (BMVC) 2022
- **Journal**: None
- **Summary**: Representation learning for sketch-based image retrieval has mostly been tackled by learning embeddings that discard modality-specific information. As instances from different modalities can often provide complementary information describing the underlying concept, we propose a cross-attention framework for Vision Transformers (XModalViT) that fuses modality-specific information instead of discarding them. Our framework first maps paired datapoints from the individual photo and sketch modalities to fused representations that unify information from both modalities. We then decouple the input space of the aforementioned modality fusion network into independent encoders of the individual modalities via contrastive and relational cross-modal knowledge distillation. Such encoders can then be applied to downstream tasks like cross-modal retrieval. We demonstrate the expressive capacity of the learned representations by performing a wide range of experiments and achieving state-of-the-art results on three fine-grained sketch-based image retrieval benchmarks: Shoe-V2, Chair-V2 and Sketchy. Implementation is available at https://github.com/abhrac/xmodal-vit.



### A Robust Pedestrian Detection Approach for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2210.10489v1
- **DOI**: 10.1109/ICSPIS56952.2022.10043934
- **Categories**: **cs.CV**, cs.AI, I.4.8; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2210.10489v1)
- **Published**: 2022-10-19 11:53:14+00:00
- **Updated**: 2022-10-19 11:53:14+00:00
- **Authors**: Bahareh Ghari, Ali Tourani, Asadollah Shahbahrami
- **Comment**: 6 pages, 5 figures, one table
- **Journal**: None
- **Summary**: Nowadays, utilizing Advanced Driver-Assistance Systems (ADAS) has absorbed a huge interest as a potential solution for reducing road traffic issues. Despite recent technological advances in such systems, there are still many inquiries that need to be overcome. For instance, ADAS requires accurate and real-time detection of pedestrians in various driving scenarios. To solve the mentioned problem, this paper aims to fine-tune the YOLOv5s framework for handling pedestrian detection challenges on the real-world instances of Caltech pedestrian dataset. We also introduce a developed toolbox for preparing training and test data and annotations of Caltech pedestrian dataset into the format recognizable by YOLOv5. Experimental results of utilizing our approach show that the mean Average Precision (mAP) of our fine-tuned model for pedestrian detection task is more than 91 percent when performing at the highest rate of 70 FPS. Moreover, the experiments on the Caltech pedestrian dataset samples have verified that our proposed approach is an effective and accurate method for pedestrian detection and can outperform other existing methodologies.



### Visual SLAM: What are the Current Trends and What to Expect?
- **Arxiv ID**: http://arxiv.org/abs/2210.10491v2
- **DOI**: 10.3390/s22239297
- **Categories**: **cs.CV**, cs.RO, I.2.9; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2210.10491v2)
- **Published**: 2022-10-19 11:56:32+00:00
- **Updated**: 2022-10-22 11:01:22+00:00
- **Authors**: Ali Tourani, Hriday Bavle, Jose Luis Sanchez-Lopez, Holger Voos
- **Comment**: 18 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of forty-five impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.



### ADPS: Asymmetric Distillation Post-Segmentation for Image Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.10495v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10495v3)
- **Published**: 2022-10-19 12:04:47+00:00
- **Updated**: 2023-07-24 07:43:31+00:00
- **Authors**: Peng Xing, Hao Tang, Jinhui Tang, Zechao Li
- **Comment**: 11pages,9 figures
- **Journal**: None
- **Summary**: Knowledge Distillation-based Anomaly Detection (KDAD) methods rely on the teacher-student paradigm to detect and segment anomalous regions by contrasting the unique features extracted by both networks. However, existing KDAD methods suffer from two main limitations: 1) the student network can effortlessly replicate the teacher network's representations, and 2) the features of the teacher network serve solely as a ``reference standard" and are not fully leveraged. Toward this end, we depart from the established paradigm and instead propose an innovative approach called Asymmetric Distillation Post-Segmentation (ADPS). Our ADPS employs an asymmetric distillation paradigm that takes distinct forms of the same image as the input of the teacher-student networks, driving the student network to learn discriminating representations for anomalous regions.   Meanwhile, a customized Weight Mask Block (WMB) is proposed to generate a coarse anomaly localization mask that transfers the distilled knowledge acquired from the asymmetric paradigm to the teacher network. Equipped with WMB, the proposed Post-Segmentation Module (PSM) is able to effectively detect and segment abnormal regions with fine structures and clear boundaries. Experimental results demonstrate that the proposed ADPS outperforms the state-of-the-art methods in detecting and segmenting anomalies. Surprisingly, ADPS significantly improves Average Precision (AP) metric by 9% and 20% on the MVTec AD and KolektorSDD2 datasets, respectively.



### Deep-based quality assessment of medical images through domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.10533v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10533v1)
- **Published**: 2022-10-19 13:13:06+00:00
- **Updated**: 2022-10-19 13:13:06+00:00
- **Authors**: Marouane Tliba, Aymen Sekhri, Mohamed Amine Kerkouri, Aladine Chetouani
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: Predicting the quality of multimedia content is often needed in different fields. In some applications, quality metrics are crucial with a high impact, and can affect decision making such as diagnosis from medical multimedia. In this paper, we focus on such applications by proposing an efficient and shallow model for predicting the quality of medical images without reference from a small amount of annotated data. Our model is based on convolution self-attention that aims to model complex representation from relevant local characteristics of images, which itself slide over the image to interpolate the global quality score. We also apply domain adaptation learning in unsupervised and semi-supervised manner. The proposed model is evaluated through a dataset composed of several images and their corresponding subjective scores. The obtained results showed the efficiency of the proposed method, but also, the relevance of the applying domain adaptation to generalize over different multimedia domains regarding the downstream task of perceptual quality prediction. \footnote{Funded by the TIC-ART project, Regional fund (Region Centre-Val de Loire)}



### Online LiDAR-Camera Extrinsic Parameters Self-checking
- **Arxiv ID**: http://arxiv.org/abs/2210.10537v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10537v1)
- **Published**: 2022-10-19 13:17:48+00:00
- **Updated**: 2022-10-19 13:17:48+00:00
- **Authors**: Pengjin Wei, Guohang Yan, Yikang Li, Kun Fang, Jie Yang, Wei Liu
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: With the development of neural networks and the increasing popularity of automatic driving, the calibration of the LiDAR and the camera has attracted more and more attention. This calibration task is multi-modal, where the rich color and texture information captured by the camera and the accurate three-dimensional spatial information from the LiDAR is incredibly significant for downstream tasks. Current research interests mainly focus on obtaining accurate calibration results through information fusion. However, they seldom analyze whether the calibrated results are correct or not, which could be of significant importance in real-world applications. For example, in large-scale production, the LiDARs and the cameras of each smart car have to get well-calibrated as the car leaves the production line, while in the rest of the car life period, the poses of the LiDARs and cameras should also get continually supervised to ensure the security. To this end, this paper proposes a self-checking algorithm to judge whether the extrinsic parameters are well-calibrated by introducing a binary classification network based on the fused information from the camera and the LiDAR. Moreover, since there is no such dataset for the task in this work, we further generate a new dataset branch from the KITTI dataset tailored for the task. Our experiments on the proposed dataset branch demonstrate the performance of our method. To the best of our knowledge, this is the first work to address the significance of continually checking the calibrated extrinsic parameters for autonomous driving. The code is open-sourced on the Github website at https://github.com/OpenCalib/LiDAR2camera_self-check.



### PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2210.10542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10542v1)
- **Published**: 2022-10-19 13:30:39+00:00
- **Updated**: 2022-10-19 13:30:39+00:00
- **Authors**: Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, Grégory Rogez
- **Comment**: ECCV'22 Conference paper
- **Journal**: None
- **Summary**: We address the problem of action-conditioned generation of human motion sequences. Existing work falls into two categories: forecast models conditioned on observed past motions, or generative models conditioned on action labels and duration only. In contrast, we generate motion conditioned on observations of arbitrary length, including none. To solve this generalized problem, we propose PoseGPT, an auto-regressive transformer-based approach which internally compresses human motion into quantized latent sequences. An auto-encoder first maps human motion to latent index sequences in a discrete space, and vice-versa. Inspired by the Generative Pretrained Transformer (GPT), we propose to train a GPT-like model for next-index prediction in that space; this allows PoseGPT to output distributions on possible futures, with or without conditioning on past motion. The discrete and compressed nature of the latent space allows the GPT-like model to focus on long-range signal, as it removes low-level redundancy in the input signal. Predicting discrete indices also alleviates the common pitfall of predicting averaged poses, a typical failure case when regressing continuous values, as the average of discrete targets is not a target itself. Our experimental results show that our proposed approach achieves state-of-the-art results on HumanAct12, a standard but small scale dataset, as well as on BABEL, a recent large scale MoCap dataset, and on GRAB, a human-object interactions dataset.



### Improved lung segmentation based on U-Net architecture and morphological operations
- **Arxiv ID**: http://arxiv.org/abs/2210.10545v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10545v1)
- **Published**: 2022-10-19 13:32:00+00:00
- **Updated**: 2022-10-19 13:32:00+00:00
- **Authors**: S Ali John Naqvi, Abdullah Tauqeer, Rohaib Bhatti, S Bazil Ali
- **Comment**: 8 pages, 5 figures, conference
- **Journal**: None
- **Summary**: An essential stage in computer aided diagnosis of chest X rays is automated lung segmentation. Due to rib cages and the unique modalities of each persons lungs, it is essential to construct an effective automated lung segmentation model. This paper presents a reliable model for the segmentation of lungs in chest radiographs. Our model overcomes the challenges by learning to ignore unimportant areas in the source Chest Radiograph and emphasize important features for lung segmentation. We evaluate our model on public datasets, Montgomery and Shenzhen. The proposed model has a DICE coefficient of 98.1 percent which demonstrates the reliability of our model.



### Active Learning for Imbalanced Civil Infrastructure Data
- **Arxiv ID**: http://arxiv.org/abs/2210.10586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10586v1)
- **Published**: 2022-10-19 14:28:04+00:00
- **Updated**: 2022-10-19 14:28:04+00:00
- **Authors**: Thomas Frick, Diego Antognini, Mattia Rigotti, Ioana Giurgiu, Benjamin Grewe, Cristiano Malossi
- **Comment**: None
- **Journal**: None
- **Summary**: Aging civil infrastructures are closely monitored by engineers for damage and critical defects. As the manual inspection of such large structures is costly and time-consuming, we are working towards fully automating the visual inspections to support the prioritization of maintenance activities. To that end we combine recent advances in drone technology and deep learning. Unfortunately, annotation costs are incredibly high as our proprietary civil engineering dataset must be annotated by highly trained engineers. Active learning is, therefore, a valuable tool to optimize the trade-off between model performance and annotation costs. Our use-case differs from the classical active learning setting as our dataset suffers from heavy class imbalance and consists of a much larger already labeled data pool than other active learning research. We present a novel method capable of operating in this challenging setting by replacing the traditional active learning acquisition function with an auxiliary binary discriminator. We experimentally show that our novel method outperforms the best-performing traditional active learning method (BALD) by 5% and 38% accuracy on CIFAR-10 and our proprietary dataset respectively.



### RT-MOT: Confidence-Aware Real-Time Scheduling Framework for Multi-Object Tracking Tasks
- **Arxiv ID**: http://arxiv.org/abs/2210.11946v1
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2210.11946v1)
- **Published**: 2022-10-19 14:31:19+00:00
- **Updated**: 2022-10-19 14:31:19+00:00
- **Authors**: Donghwa Kang, Seunghoon Lee, Hoon Sung Chwa, Seung-Hwan Bae, Chang Mook Kang, Jinkyu Lee, Hyeongboo Baek
- **Comment**: Accepted to 2022 Real-Time Systems Symposium (RTSS)
- **Journal**: None
- **Summary**: Different from existing MOT (Multi-Object Tracking) techniques that usually aim at improving tracking accuracy and average FPS, real-time systems such as autonomous vehicles necessitate new requirements of MOT under limited computing resources: (R1) guarantee of timely execution and (R2) high tracking accuracy. In this paper, we propose RT-MOT, a novel system design for multiple MOT tasks, which addresses R1 and R2. Focusing on multiple choices of a workload pair of detection and association, which are two main components of the tracking-by-detection approach for MOT, we tailor a measure of object confidence for RT-MOT and develop how to estimate the measure for the next frame of each MOT task. By utilizing the estimation, we make it possible to predict tracking accuracy variation according to different workload pairs to be applied to the next frame of an MOT task. Next, we develop a novel confidence-aware real-time scheduling framework, which offers an offline timing guarantee for a set of MOT tasks based on non-preemptive fixed-priority scheduling with the smallest workload pair. At run-time, the framework checks the feasibility of a priority-inversion associated with a larger workload pair, which does not compromise the timing guarantee of every task, and then chooses a feasible scenario that yields the largest tracking accuracy improvement based on the proposed prediction. Our experiment results demonstrate that RT-MOT significantly improves overall tracking accuracy by up to 1.5x, compared to existing popular tracking-by-detection approaches, while guaranteeing timely execution of all MOT tasks.



### Motion correction in MRI using deep learning and a novel hybrid loss function
- **Arxiv ID**: http://arxiv.org/abs/2210.14156v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14156v1)
- **Published**: 2022-10-19 14:40:41+00:00
- **Updated**: 2022-10-19 14:40:41+00:00
- **Authors**: Lei Zhang, Xiaoke Wang, Michael Rawson, Radu Balan, Edward H. Herskovits, Elias Melhem, Linda Chang, Ze Wang, Thomas Ernst
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose To develop and evaluate a deep learning-based method (MC-Net) to suppress motion artifacts in brain magnetic resonance imaging (MRI). Methods MC-Net was derived from a UNet combined with a two-stage multi-loss function. T1-weighted axial brain images contaminated with synthetic motions were used to train the network. Evaluation used simulated T1 and T2-weighted axial, coronal, and sagittal images unseen during training, as well as T1-weighted images with motion artifacts from real scans. Performance indices included the peak signal to noise ratio (PSNR), structural similarity index measure (SSIM), and visual reading scores. Two clinical readers scored the images. Results The MC-Net outperformed other methods implemented in terms of PSNR and SSIM on the T1 axial test set. The MC-Net significantly improved the quality of all T1-weighted images (for all directions and for simulated as well as real motion artifacts), both on quantitative measures and visual scores. However, the MC-Net performed poorly on images of untrained contrast (T2-weighted). Conclusion The proposed two-stage multi-loss MC-Net can effectively suppress motion artifacts in brain MRI without compromising image context. Given the efficiency of the MC-Net (single image processing time ~40ms), it can potentially be used in real clinical settings. To facilitate further research, the code and trained model are available at https://github.com/MRIMoCo/DL_Motion_Correction.



### Provably Convergent Plug & Play Linearized ADMM, applied to Deblurring Spatially Varying Kernels
- **Arxiv ID**: http://arxiv.org/abs/2210.10605v3
- **DOI**: 10.1109/ICASSP49357.2023.10096037
- **Categories**: **cs.CV**, math.OC, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2210.10605v3)
- **Published**: 2022-10-19 14:51:44+00:00
- **Updated**: 2023-01-23 19:56:55+00:00
- **Authors**: Charles Laroche, Andrés Almansa, Eva Coupeté, Matias Tassano
- **Comment**: None
- **Journal**: None
- **Summary**: Plug & Play methods combine proximal algorithms with denoiser priors to solve inverse problems. These methods rely on the computability of the proximal operator of the data fidelity term. In this paper, we propose a Plug & Play framework based on linearized ADMM that allows us to bypass the computation of intractable proximal operators. We demonstrate the convergence of the algorithm and provide results on restoration tasks such as super-resolution and deblurring with non-uniform blur.



### A Unified View of Masked Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2210.10615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10615v1)
- **Published**: 2022-10-19 14:59:18+00:00
- **Updated**: 2022-10-19 14:59:18+00:00
- **Authors**: Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Masked image modeling has demonstrated great potential to eliminate the label-hungry problem of training large-scale vision Transformers, achieving impressive performance on various downstream tasks. In this work, we propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images. Experimental results on image classification and semantic segmentation show that MaskDistill achieves comparable or superior performance than state-of-the-art methods. When using the huge vision Transformer and pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on ImageNet-1k (224 size) and 58.8% semantic segmentation mIoU metric on ADE20k (512 size). The code and pretrained models will be available at https://aka.ms/unimim.



### HAVANA: Hard negAtiVe sAmples aware self-supervised coNtrastive leArning for Airborne laser scanning point clouds semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.10626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10626v1)
- **Published**: 2022-10-19 15:05:17+00:00
- **Updated**: 2022-10-19 15:05:17+00:00
- **Authors**: Yunsheng Zhang, Jianguo Yao, Ruixiang Zhang, Siyang Chen, Haifeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) based point cloud semantic segmentation has presented significant achievements on large-scale labeled aerial laser point cloud datasets. However, annotating such large-scaled point clouds is time-consuming. Due to density variations and spatial heterogeneity of the Airborne Laser Scanning (ALS) point clouds, DNNs lack generalization capability and thus lead to unpromising semantic segmentation, as the DNN trained in one region underperform when directly utilized in other regions. However, Self-Supervised Learning (SSL) is a promising way to solve this problem by pre-training a DNN model utilizing unlabeled samples followed by a fine-tuned downstream task involving very limited labels. Hence, this work proposes a hard-negative sample aware self-supervised contrastive learning method to pre-train the model for semantic segmentation. The traditional contrastive learning for point clouds selects the hardest negative samples by solely relying on the distance between the embedded features derived from the learning process, potentially evolving some negative samples from the same classes to reduce the contrastive learning effectiveness. Therefore, we design an AbsPAN (Absolute Positive And Negative samples) strategy based on k-means clustering to filter the possible false-negative samples. Experiments on two typical ALS benchmark datasets demonstrate that the proposed method is more appealing than supervised training schemes without pre-training. Especially when the labels are severely inadequate (10% of the ISPRS training set), the results obtained by the proposed HAVANA method still exceed 94% of the supervised paradigm performance with full training set.



### Multi-Granularity Cross-Modality Representation Learning for Named Entity Recognition on Social Media
- **Arxiv ID**: http://arxiv.org/abs/2210.14163v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.14163v2)
- **Published**: 2022-10-19 15:14:55+00:00
- **Updated**: 2022-11-20 02:02:24+00:00
- **Authors**: Peipei Liu, Gaosheng Wang, Hong Li, Jie Liu, Yimo Ren, Hongsong Zhu, Limin Sun
- **Comment**: We have reconducted experiments of the paper, but found that there
  were fatal errors in our datasets leading to the wrong results and analyses.
  Therefore, we have to withdraw the paper to ensure the authenticity of
  science. We are very sorry
- **Journal**: None
- **Summary**: Named Entity Recognition (NER) on social media refers to discovering and classifying entities from unstructured free-form content, and it plays an important role for various applications such as intention understanding and user recommendation. With social media posts tending to be multimodal, Multimodal Named Entity Recognition (MNER) for the text with its accompanying image is attracting more and more attention since some textual components can only be understood in combination with visual information. However, there are two drawbacks in existing approaches: 1) Meanings of the text and its accompanying image do not match always, so the text information still plays a major role. However, social media posts are usually shorter and more informal compared with other normal contents, which easily causes incomplete semantic description and the data sparsity problem. 2) Although the visual representations of whole images or objects are already used, existing methods ignore either fine-grained semantic correspondence between objects in images and words in text or the objective fact that there are misleading objects or no objects in some images. In this work, we solve the above two problems by introducing the multi-granularity cross-modality representation learning. To resolve the first problem, we enhance the representation by semantic augmentation for each word in text. As for the second issue, we perform the cross-modality semantic interaction between text and vision at the different vision granularity to get the most effective multimodal guidance representation for every word. Experiments show that our proposed approach can achieve the SOTA or approximate SOTA performance on two benchmark datasets of tweets. The code, data and the best performing models are available at https://github.com/LiuPeiP-CS/IIE4MNER



### Comparative analysis of deep learning approaches for AgNOR-stained cytology samples interpretation
- **Arxiv ID**: http://arxiv.org/abs/2210.10641v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, 68T45, 68T07,, I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2210.10641v1)
- **Published**: 2022-10-19 15:15:32+00:00
- **Updated**: 2022-10-19 15:15:32+00:00
- **Authors**: João Gustavo Atkinson Amorim, André Victória Matias, Allan Cerentini, Luiz Antonio Buschetto Macarini, Alexandre Sherlley Onofre, Fabiana Botelho Onofre, Aldo von Wangenheim
- **Comment**: None
- **Journal**: None
- **Summary**: Cervical cancer is a public health problem, where the treatment has a better chance of success if detected early. The analysis is a manual process which is subject to a human error, so this paper provides a way to analyze argyrophilic nucleolar organizer regions (AgNOR) stained slide using deep learning approaches. Also, this paper compares models for instance and semantic detection approaches. Our results show that the semantic segmentation using U-Net with ResNet-18 or ResNet-34 as the backbone have similar results, and the best model shows an IoU for nucleus, cluster, and satellites of 0.83, 0.92, and 0.99 respectively. For instance segmentation, the Mask R-CNN using ResNet-50 performs better in the visual inspection and has a 0.61 of the IoU metric. We conclude that the instance segmentation and semantic segmentation models can be used in combination to make a cascade model able to select a nucleus and subsequently segment the nucleus and its respective nucleolar organizer regions (NORs).



### Attaining Class-level Forgetting in Pretrained Model using Few Samples
- **Arxiv ID**: http://arxiv.org/abs/2210.10670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10670v1)
- **Published**: 2022-10-19 15:36:01+00:00
- **Updated**: 2022-10-19 15:36:01+00:00
- **Authors**: Pravendra Singh, Pratik Mazumder, Mohammed Asad Karim
- **Comment**: Accepted in ECCV 2022
- **Journal**: None
- **Summary**: In order to address real-world problems, deep learning models are jointly trained on many classes. However, in the future, some classes may become restricted due to privacy/ethical concerns, and the restricted class knowledge has to be removed from the models that have been trained on them. The available data may also be limited due to privacy/ethical concerns, and re-training the model will not be possible. We propose a novel approach to address this problem without affecting the model's prediction power for the remaining classes. Our approach identifies the model parameters that are highly relevant to the restricted classes and removes the knowledge regarding the restricted classes from them using the limited available training data. Our approach is significantly faster and performs similar to the model re-trained on the complete data of the remaining classes.



### Image Semantic Relation Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.11253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.11253v1)
- **Published**: 2022-10-19 16:15:19+00:00
- **Updated**: 2022-10-19 16:15:19+00:00
- **Authors**: Mingzhe Du
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graphs provide structured semantic understanding beyond images. For downstream tasks, such as image retrieval, visual question answering, visual relationship detection, and even autonomous vehicle technology, scene graphs can not only distil complex image information but also correct the bias of visual models using semantic-level relations, which has broad application prospects. However, the heavy labour cost of constructing graph annotations may hinder the application of PSG in practical scenarios. Inspired by the observation that people usually identify the subject and object first and then determine the relationship between them, we proposed to decouple the scene graphs generation task into two sub-tasks: 1) an image segmentation task to pick up the qualified objects. 2) a restricted auto-regressive text generation task to generate the relation between given objects. Therefore, in this work, we introduce image semantic relation generation (ISRG), a simple but effective image-to-text model, which achieved 31 points on the OpenPSG dataset and outperforms strong baselines respectively by 16 points (ResNet-50) and 5 points (CLIP).



### OCR-VQGAN: Taming Text-within-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.11248v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11248v2)
- **Published**: 2022-10-19 16:37:48+00:00
- **Updated**: 2022-10-21 18:32:27+00:00
- **Authors**: Juan A. Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, Pau Rodriguez
- **Comment**: Paper accepted at WACV 2023
- **Journal**: None
- **Summary**: Synthetic image generation has recently experienced significant improvements in domains such as natural image or art generation. However, the problem of figure and diagram generation remains unexplored. A challenging aspect of generating figures and diagrams is effectively rendering readable texts within the images. To alleviate this problem, we present OCR-VQGAN, an image encoder, and decoder that leverages OCR pre-trained features to optimize a text perceptual loss, encouraging the architecture to preserve high-fidelity text and diagram structure. To explore our approach, we introduce the Paper2Fig100k dataset, with over 100k images of figures and texts from research papers. The figures show architecture diagrams and methodologies of articles available at arXiv.org from fields like artificial intelligence and computer vision. Figures usually include text and discrete objects, e.g., boxes in a diagram, with lines and arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by conducting several experiments on the task of figure reconstruction. Additionally, we explore the qualitative and quantitative impact of weighting different perceptual metrics in the overall loss function. We release code, models, and dataset at https://github.com/joanrod/ocr-vqgan.



### CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion
- **Arxiv ID**: http://arxiv.org/abs/2210.10716v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10716v2)
- **Published**: 2022-10-19 16:50:36+00:00
- **Updated**: 2023-01-12 20:29:45+00:00
- **Authors**: Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Brégier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, Jérôme Revaud
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Masked Image Modeling (MIM) has recently been established as a potent pre-training paradigm. A pretext task is constructed by masking patches in an input image, and this masked content is then predicted by a neural network using visible patches as sole input. This pre-training leads to state-of-the-art performance when finetuned for high-level semantic tasks, e.g. image classification and object detection. In this paper we instead seek to learn representations that transfer well to a wide variety of 3D vision and lower-level geometric downstream tasks, such as depth prediction or optical flow estimation. Inspired by MIM, we propose an unsupervised representation learning task trained from pairs of images showing the same scene from different viewpoints. More precisely, we propose the pretext task of cross-view completion where the first input image is partially masked, and this masked content has to be reconstructed from the visible content and the second image. In single-view MIM, the masked content often cannot be inferred precisely from the visible portion only, so the model learns to act as a prior influenced by high-level semantics. In contrast, this ambiguity can be resolved with cross-view completion from the second unmasked image, on the condition that the model is able to understand the spatial relationship between the two images. Our experiments show that our pretext task leads to significantly improved performance for monocular 3D vision downstream tasks such as depth estimation. In addition, our model can be directly applied to binocular downstream tasks like optical flow or relative camera pose estimation, for which we obtain competitive results without bells and whistles, i.e., using a generic architecture without any task-specific design.



### OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping
- **Arxiv ID**: http://arxiv.org/abs/2210.10732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10732v1)
- **Published**: 2022-10-19 17:20:16+00:00
- **Updated**: 2022-10-19 17:20:16+00:00
- **Authors**: Junshi Xia, Naoto Yokoya, Bruno Adriano, Clifford Broni-Bediako
- **Comment**: Accepted by WACV 2023
- **Journal**: None
- **Summary**: We introduce OpenEarthMap, a benchmark dataset, for global high-resolution land cover mapping. OpenEarthMap consists of 2.2 million segments of 5000 aerial and satellite images covering 97 regions from 44 countries across 6 continents, with manually annotated 8-class land cover labels at a 0.25--0.5m ground sampling distance. Semantic segmentation models trained on the OpenEarthMap generalize worldwide and can be used as off-the-shelf models in a variety of applications. We evaluate the performance of state-of-the-art methods for unsupervised domain adaptation and present challenging problem settings suitable for further technical development. We also investigate lightweight models using automated neural architecture search for limited computational resources and fast mapping. The dataset is available at https://open-earth-map.org.



### Two-level Data Augmentation for Calibrated Multi-view Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.10756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10756v1)
- **Published**: 2022-10-19 17:55:13+00:00
- **Updated**: 2022-10-19 17:55:13+00:00
- **Authors**: Martin Engilberge, Haixin Shi, Zhiye Wang, Pascal Fua
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: Data augmentation has proven its usefulness to improve model generalization and performance. While it is commonly applied in computer vision application when it comes to multi-view systems, it is rarely used. Indeed geometric data augmentation can break the alignment among views. This is problematic since multi-view data tend to be scarce and it is expensive to annotate. In this work we propose to solve this issue by introducing a new multi-view data augmentation pipeline that preserves alignment among views. Additionally to traditional augmentation of the input image we also propose a second level of augmentation applied directly at the scene level. When combined with our simple multi-view detection model, our two-level augmentation pipeline outperforms all existing baselines by a significant margin on the two main multi-view multi-person detection datasets WILDTRACK and MultiviewX.



### GraphCSPN: Geometry-Aware Depth Completion via Dynamic GCNs
- **Arxiv ID**: http://arxiv.org/abs/2210.10758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10758v1)
- **Published**: 2022-10-19 17:56:03+00:00
- **Updated**: 2022-10-19 17:56:03+00:00
- **Authors**: Xin Liu, Xiaofei Shao, Bo Wang, Yali Li, Shengjin Wang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Image guided depth completion aims to recover per-pixel dense depth maps from sparse depth measurements with the help of aligned color images, which has a wide range of applications from robotics to autonomous driving. However, the 3D nature of sparse-to-dense depth completion has not been fully explored by previous methods. In this work, we propose a Graph Convolution based Spatial Propagation Network (GraphCSPN) as a general approach for depth completion. First, unlike previous methods, we leverage convolution neural networks as well as graph neural networks in a complementary way for geometric representation learning. In addition, the proposed networks explicitly incorporate learnable geometric constraints to regularize the propagation process performed in three-dimensional space rather than in two-dimensional plane. Furthermore, we construct the graph utilizing sequences of feature patches, and update it dynamically with an edge attention module during propagation, so as to better capture both the local neighboring features and global relationships over long distance. Extensive experiments on both indoor NYU-Depth-v2 and outdoor KITTI datasets demonstrate that our method achieves the state-of-the-art performance, especially when compared in the case of using only a few propagation steps. Code and models are available at the project page.



### On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.10763v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.10763v2)
- **Published**: 2022-10-19 17:57:06+00:00
- **Updated**: 2023-06-15 17:57:18+00:00
- **Authors**: Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, Zhuowen Tu
- **Comment**: Project page with code: https://nicklashansen.github.io/xtra
- **Journal**: None
- **Summary**: Reinforcement Learning (RL) algorithms can solve challenging control problems directly from image observations, but they often require millions of environment interactions to do so. Recently, model-based RL algorithms have greatly improved sample-efficiency by concurrently learning an internal model of the world, and supplementing real environment interactions with imagined rollouts for policy improvement. However, learning an effective model of the world from scratch is challenging, and in stark contrast to humans that rely heavily on world understanding and visual cues for learning new skills. In this work, we investigate whether internal models learned by modern model-based RL algorithms can be leveraged to solve new, distinctly different tasks faster. We propose Model-Based Cross-Task Transfer (XTRA), a framework for sample-efficient online RL with scalable pretraining and finetuning of learned world models. By offline multi-task pretraining and online cross-task finetuning, we achieve substantial improvements over a baseline trained from scratch; we improve mean performance of model-based algorithm EfficientZero by 23%, and by as much as 71% in some instances.



### LaMAR: Benchmarking Localization and Mapping for Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2210.10770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10770v1)
- **Published**: 2022-10-19 17:58:17+00:00
- **Updated**: 2022-10-19 17:58:17+00:00
- **Authors**: Paul-Edouard Sarlin, Mihai Dusmanu, Johannes L. Schönberger, Pablo Speciale, Lukas Gruber, Viktor Larsson, Ondrej Miksik, Marc Pollefeys
- **Comment**: Accepted at ECCV 2022, website at https://lamar.ethz.ch/
- **Journal**: None
- **Summary**: Localization and mapping is the foundational technology for augmented reality (AR) that enables sharing and persistence of digital content in the real world. While significant progress has been made, researchers are still mostly driven by unrealistic benchmarks not representative of real-world AR scenarios. These benchmarks are often based on small-scale datasets with low scene diversity, captured from stationary cameras, and lack other sensor inputs like inertial, radio, or depth data. Furthermore, their ground-truth (GT) accuracy is mostly insufficient to satisfy AR requirements. To close this gap, we introduce LaMAR, a new benchmark with a comprehensive capture and GT pipeline that co-registers realistic trajectories and sensor streams captured by heterogeneous AR devices in large, unconstrained scenes. To establish an accurate GT, our pipeline robustly aligns the trajectories against laser scans in a fully automated manner. As a result, we publish a benchmark dataset of diverse and large-scale scenes recorded with head-mounted and hand-held AR devices. We extend several state-of-the-art methods to take advantage of the AR-specific setup and evaluate them on our benchmark. The results offer new insights on current research and reveal promising avenues for future work in the field of localization and mapping for AR.



### Multi-view Tracking Using Weakly Supervised Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.10771v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10771v1)
- **Published**: 2022-10-19 17:58:23+00:00
- **Updated**: 2022-10-19 17:58:23+00:00
- **Authors**: Martin Engilberge, Weizhe Liu, Pascal Fua
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: Multi-view approaches to people-tracking have the potential to better handle occlusions than single-view ones in crowded scenes. They often rely on the tracking-by-detection paradigm, which involves detecting people first and then connecting the detections. In this paper, we argue that an even more effective approach is to predict people motion over time and infer people's presence in individual frames from these. This enables to enforce consistency both over time and across views of a single temporal frame. We validate our approach on the PETS2009 and WILDTRACK datasets and demonstrate that it outperforms state-of-the-art methods.



### Anomaly Detection Requires Better Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.10773v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10773v1)
- **Published**: 2022-10-19 17:59:32+00:00
- **Updated**: 2022-10-19 17:59:32+00:00
- **Authors**: Tal Reiss, Niv Cohen, Eliahu Horwitz, Ron Abutbul, Yedid Hoshen
- **Comment**: Accepted to ECCV SSLWIN Workshop (2022)
- **Journal**: None
- **Summary**: Anomaly detection seeks to identify unusual phenomena, a central task in science and industry. The task is inherently unsupervised as anomalies are unexpected and unknown during training. Recent advances in self-supervised representation learning have directly driven improvements in anomaly detection. In this position paper, we first explain how self-supervised representations can be easily used to achieve state-of-the-art performance in commonly reported anomaly detection benchmarks. We then argue that tackling the next generation of anomaly detection tasks requires new technical and conceptual improvements in representation learning.



### Learning to Discover and Detect Objects
- **Arxiv ID**: http://arxiv.org/abs/2210.10774v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10774v2)
- **Published**: 2022-10-19 17:59:55+00:00
- **Updated**: 2022-11-30 07:27:01+00:00
- **Authors**: Vladimir Fomenko, Ismail Elezi, Deva Ramanan, Laura Leal-Taixé, Aljoša Ošep
- **Comment**: Accepted to NeurIPS 2022, Homepage: https://vlfom.github.io/RNCDL/
- **Journal**: None
- **Summary**: We tackle the problem of novel class discovery and localization (NCDL). In this setting, we assume a source dataset with supervision for only some object classes. Instances of other classes need to be discovered, classified, and localized automatically based on visual similarity without any human supervision. To tackle NCDL, we propose a two-stage object detection network Region-based NCDL (RNCDL) that uses a region proposal network to localize regions of interest (RoIs). We then train our network to learn to classify each RoI, either as one of the known classes, seen in the source dataset, or one of the novel classes, with a long-tail distribution constraint on the class assignments, reflecting the natural frequency of classes in the real world. By training our detection network with this objective in an end-to-end manner, it learns to classify all region proposals for a large variety of classes, including those not part of the labeled object class vocabulary. Our experiments conducted using COCO and LVIS datasets reveal that our method is significantly more effective than multi-stage pipelines that rely on traditional clustering algorithms. Furthermore, we demonstrate the generality of our approach by applying our method to a large-scale Visual Genome dataset, where our network successfully learns to detect various semantic classes without direct supervision.



### TOIST: Task Oriented Instance Segmentation Transformer with Noun-Pronoun Distillation
- **Arxiv ID**: http://arxiv.org/abs/2210.10775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10775v1)
- **Published**: 2022-10-19 17:59:56+00:00
- **Updated**: 2022-10-19 17:59:56+00:00
- **Authors**: Pengfei Li, Beiwen Tian, Yongliang Shi, Xiaoxue Chen, Hao Zhao, Guyue Zhou, Ya-Qin Zhang
- **Comment**: Accepted by NeurIPS 2022. Codes are available at
  https://github.com/AIR-DISCOVER/TOIST
- **Journal**: None
- **Summary**: Current referring expression comprehension algorithms can effectively detect or segment objects indicated by nouns, but how to understand verb reference is still under-explored. As such, we study the challenging problem of task oriented detection, which aims to find objects that best afford an action indicated by verbs like sit comfortably on. Towards a finer localization that better serves downstream applications like robot interaction, we extend the problem into task oriented instance segmentation. A unique requirement of this task is to select preferred candidates among possible alternatives. Thus we resort to the transformer architecture which naturally models pair-wise query relationships with attention, leading to the TOIST method. In order to leverage pre-trained noun referring expression comprehension models and the fact that we can access privileged noun ground truth during training, a novel noun-pronoun distillation framework is proposed. Noun prototypes are generated in an unsupervised manner and contextual pronoun features are trained to select prototypes. As such, the network remains noun-agnostic during inference. We evaluate TOIST on the large-scale task oriented dataset COCO-Tasks and achieve +10.9% higher $\rm{mAP^{box}}$ than the best-reported results. The proposed noun-pronoun distillation can boost $\rm{mAP^{box}}$ and $\rm{mAP^{mask}}$ by +2.8% and +3.8%. Codes and models are publicly available at https://github.com/AIR-DISCOVER/TOIST.



### Self-Supervised Representation Learning for CAD
- **Arxiv ID**: http://arxiv.org/abs/2210.10807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.10807v1)
- **Published**: 2022-10-19 18:00:18+00:00
- **Updated**: 2022-10-19 18:00:18+00:00
- **Authors**: Benjamin T. Jones, Michael Hu, Vladimir G. Kim, Adriana Schulz
- **Comment**: None
- **Journal**: None
- **Summary**: The design of man-made objects is dominated by computer aided design (CAD) tools. Assisting design with data-driven machine learning methods is hampered by lack of labeled data in CAD's native format; the parametric boundary representation (B-Rep). Several data sets of mechanical parts in B-Rep format have recently been released for machine learning research. However, large scale databases are largely unlabeled, and labeled datasets are small. Additionally, task specific label sets are rare, and costly to annotate. This work proposes to leverage unlabeled CAD geometry on supervised learning tasks. We learn a novel, hybrid implicit/explicit surface representation for B-Rep geometry, and show that this pre-training significantly improves few-shot learning performance and also achieves state-of-the-art performance on several existing B-Rep benchmarks.



### VTC: Improving Video-Text Retrieval with User Comments
- **Arxiv ID**: http://arxiv.org/abs/2210.10820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10820v1)
- **Published**: 2022-10-19 18:11:39+00:00
- **Updated**: 2022-10-19 18:11:39+00:00
- **Authors**: Laura Hanu, James Thewlis, Yuki M. Asano, Christian Rupprecht
- **Comment**: Accepted paper at the European Conference on Computer Vision (ECCV)
  2022
- **Journal**: None
- **Summary**: Multi-modal retrieval is an important problem for many applications, such as recommendation and search. Current benchmarks and even datasets are often manually constructed and consist of mostly clean samples where all modalities are well-correlated with the content. Thus, current video-text retrieval literature largely focuses on video titles or audio transcripts, while ignoring user comments, since users often tend to discuss topics only vaguely related to the video. Despite the ubiquity of user comments online, there is currently no multi-modal representation learning datasets that includes comments. In this paper, we a) introduce a new dataset of videos, titles and comments; b) present an attention-based mechanism that allows the model to learn from sometimes irrelevant data such as comments; c) show that by using comments, our method is able to learn better, more contextualised, representations for image, video and audio representations. Project page: https://unitaryai.github.io/vtc-paper.



### Grounded Video Situation Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.10828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10828v1)
- **Published**: 2022-10-19 18:38:10+00:00
- **Updated**: 2022-10-19 18:38:10+00:00
- **Authors**: Zeeshan Khan, C. V. Jawahar, Makarand Tapaswi
- **Comment**: Accepted to NeurIPS 2022. Project Page:
  https://zeeshank95.github.io/grvidsitu
- **Journal**: None
- **Summary**: Dense video understanding requires answering several questions such as who is doing what to whom, with what, how, why, and where. Recently, Video Situation Recognition (VidSitu) is framed as a task for structured prediction of multiple events, their relationships, and actions and various verb-role pairs attached to descriptive entities. This task poses several challenges in identifying, disambiguating, and co-referencing entities across multiple verb-role pairs, but also faces some challenges of evaluation. In this work, we propose the addition of spatio-temporal grounding as an essential component of the structured prediction task in a weakly supervised setting, and present a novel three stage Transformer model, VideoWhisperer, that is empowered to make joint predictions. In stage one, we learn contextualised embeddings for video features in parallel with key objects that appear in the video clips to enable fine-grained spatio-temporal reasoning. The second stage sees verb-role queries attend and pool information from object embeddings, localising answers to questions posed about the action. The final stage generates these answers as captions to describe each verb-role pair present in the video. Our model operates on a group of events (clips) simultaneously and predicts verbs, verb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on a grounding-augmented version of the VidSitu dataset, we observe a large improvement in entity captioning accuracy, as well as the ability to localize verb-roles without grounding annotations at training time.



### Scene Text Recognition with Semantics
- **Arxiv ID**: http://arxiv.org/abs/2210.10836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10836v1)
- **Published**: 2022-10-19 18:58:15+00:00
- **Updated**: 2022-10-19 18:58:15+00:00
- **Authors**: Joshua Cesare Placidi, Yishu Miao, Zixu Wang, Lucia Specia
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Scene Text Recognition (STR) models have achieved high performance in recent years on benchmark datasets where text images are presented with minimal noise. Traditional STR recognition pipelines take a cropped image as sole input and attempt to identify the characters present. This infrastructure can fail in instances where the input image is noisy or the text is partially obscured. This paper proposes using semantic information from the greater scene to contextualise predictions. We generate semantic vectors using object tags and fuse this information into a transformer-based architecture. The results demonstrate that our multimodal approach yields higher performance than traditional benchmark models, particularly on noisy instances.



### Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2210.10841v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10841v1)
- **Published**: 2022-10-19 19:13:07+00:00
- **Updated**: 2022-10-19 19:13:07+00:00
- **Authors**: Yue Zhang, Hongliang Fei, Dingcheng Li, Tan Yu, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Prompt learning is a new learning paradigm which reformulates downstream tasks as similar pretraining tasks on pretrained models by leveraging textual prompts. Recent works have demonstrated that prompt learning is particularly useful for few-shot learning, where there is limited training data. Depending on the granularity of prompts, those methods can be roughly divided into task-level prompting and instance-level prompting. Task-level prompting methods learn one universal prompt for all input samples, which is efficient but ineffective to capture subtle differences among different classes. Instance-level prompting methods learn a specific prompt for each input, though effective but inefficient. In this work, we develop a novel prototype-based prompt learning method to overcome the above limitations. In particular, we focus on few-shot image recognition tasks on pretrained vision-language models (PVLMs) and develop a method of prompting through prototype (PTP), where we define $K$ image prototypes and $K$ prompt prototypes. In PTP, the image prototype represents a centroid of a certain image cluster in the latent space and a prompt prototype is defined as a soft prompt in the continuous space. The similarity between a query image and an image prototype determines how much this prediction relies on the corresponding prompt prototype. Hence, in PTP, similar images will utilize similar prompting ways. Through extensive experiments on seven real-world benchmarks, we show that PTP is an effective method to leverage the latent knowledge and adaptive to various PVLMs. Moreover, through detailed analysis, we discuss pros and cons for prompt learning and parameter-efficient fine-tuning under the context of few-shot learning.



### MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy
- **Arxiv ID**: http://arxiv.org/abs/2210.10842v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.10842v3)
- **Published**: 2022-10-19 19:15:07+00:00
- **Updated**: 2023-05-07 16:04:40+00:00
- **Authors**: Yuhao Chen, Hayden Gunraj, E. Zhixuan Zeng, Robbie Meyer, Maximilian Gilles, Alexander Wong
- **Comment**: Accepted to CVPR TCV Workshop
- **Journal**: None
- **Summary**: Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to address sensor failure issues during deployment. In particular, we realize the multimodal redundancy framework with a gate fusion module and dynamic ensemble learning. Finally, we present a new label-free multi-modal consistency (MC) score that utilizes the output from all modalities to measure the overall system output reliability and uncertainty. Through experiments, we demonstrate that in an event of missing modality, our system provides a much more reliable performance compared to baseline models. We also demonstrate that our MC score is a more reliability indicator for outputs during inference time compared to the model generated confidence scores that are often over-confident.



### Computer-Aided Cancer Diagnosis via Machine Learning and Deep Learning: A comparative review
- **Arxiv ID**: http://arxiv.org/abs/2210.11943v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11943v1)
- **Published**: 2022-10-19 19:30:56+00:00
- **Updated**: 2022-10-19 19:30:56+00:00
- **Authors**: Solene Bechelli
- **Comment**: None
- **Journal**: None
- **Summary**: The past years have seen a considerable increase in cancer cases. However, a cancer diagnosis is often complex and depends on the types of images provided for analysis. It requires highly skilled practitioners but is often time-consuming and error-prone. If Machine Learning and deep learning algorithms have been widely used, a comprehensive review of the techniques used from the pre-processing steps to the final prediction is lacking. With this review, we aim to provide a comprehensive overview of the current steps required in building efficient and accurate machine learning algorithm for cancer prediction, detection and classification. To do so, we compile the results of cancer related study using AI over the past years. We include various cancers that encompass different types of images, and therefore different related techniques. We show that tremendous improvements have been made in the early detection of cancerous tumors and tissues. The techniques used are various and often problem-tailored and our findings is confirmed through the study of a large number of research. Moreover, we investigate the approaches best suited for different types of images such as histology, dermoscopic, MRI, etc. With this work, we summarize the main finding over the past years in cancer detection using deep learning techniques. We discuss the challenges of cancer research related to the large discrepancies in the images, and we provide some notable results in the field for lung, breast, and skin cancers.



### Cluster and Aggregate: Face Recognition with Large Probe Set
- **Arxiv ID**: http://arxiv.org/abs/2210.10864v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10864v3)
- **Published**: 2022-10-19 20:01:15+00:00
- **Updated**: 2023-02-16 04:40:16+00:00
- **Authors**: Minchul Kim, Feng Liu, Anil Jain, Xiaoming Liu
- **Comment**: To appear in NeurIPS 2022
- **Journal**: None
- **Summary**: Feature fusion plays a crucial role in unconstrained face recognition where inputs (probes) comprise of a set of $N$ low quality images whose individual qualities vary. Advances in attention and recurrent modules have led to feature fusion that can model the relationship among the images in the input set. However, attention mechanisms cannot scale to large $N$ due to their quadratic complexity and recurrent modules suffer from input order sensitivity. We propose a two-stage feature fusion paradigm, Cluster and Aggregate, that can both scale to large $N$ and maintain the ability to perform sequential inference with order invariance. Specifically, Cluster stage is a linear assignment of $N$ inputs to $M$ global cluster centers, and Aggregation stage is a fusion over $M$ clustered features. The clustered features play an integral role when the inputs are sequential as they can serve as a summarization of past features. By leveraging the order-invariance of incremental averaging operation, we design an update rule that achieves batch-order invariance, which guarantees that the contributions of early image in the sequence do not diminish as time steps increase. Experiments on IJB-B and IJB-S benchmark datasets show the superiority of the proposed two-stage paradigm in unconstrained face recognition. Code and pretrained models are available in https://github.com/mk-minchul/caface



### Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.10886v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10886v3)
- **Published**: 2022-10-19 21:03:34+00:00
- **Updated**: 2023-07-16 02:01:43+00:00
- **Authors**: Ruinan Jin, Xiaoxiao Li
- **Comment**: 25 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2207.00762
- **Journal**: None
- **Summary**: Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poisoned data and corrupting the local GAN equilibrium, which then further contaminates other clients when averaging the generator's parameters and yields high generator loss. Therefore, we proposed FedDetect, an efficient and effective way of defending against the backdoor attack in the FL setting, which allows the server to detect the client's adversarial behavior based on their losses and block the malicious clients. Our extensive experiments on two medical datasets with different modalities demonstrate the backdoor attack on FedGANs can result in synthetic images with low fidelity. After detecting and suppressing the detected malicious clients using the proposed defense strategy, we show that FedGANs can synthesize high-quality medical datasets (with labels) for data augmentation to improve classification models' performance.



### Window-Based Distribution Shift Detection for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.10897v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10897v3)
- **Published**: 2022-10-19 21:27:25+00:00
- **Updated**: 2023-06-08 14:47:19+00:00
- **Authors**: Guy Bar-Shalom, Yonatan Geifman, Ran El-Yaniv
- **Comment**: None
- **Journal**: None
- **Summary**: To deploy and operate deep neural models in production, the quality of their predictions, which might be contaminated benignly or manipulated maliciously by input distributional deviations, must be monitored and assessed. Specifically, we study the case of monitoring the healthy operation of a deep neural network (DNN) receiving a stream of data, with the aim of detecting input distributional deviations over which the quality of the network's predictions is potentially damaged. Using selective prediction principles, we propose a distribution deviation detection method for DNNs. The proposed method is derived from a tight coverage generalization bound computed over a sample of instances drawn from the true underlying distribution. Based on this bound, our detector continuously monitors the operation of the network out-of-sample over a test window and fires off an alarm whenever a deviation is detected. Our novel detection method performs on-par or better than the state-of-the-art, while consuming substantially lower computation time (five orders of magnitude reduction) and space complexities. Unlike previous methods, which require at least linear dependence on the size of the source distribution for each detection, rendering them inapplicable to ``Google-Scale'' datasets, our approach eliminates this dependence, making it suitable for real-world applications.



### Model-Free Prediction of Adversarial Drop Points in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2210.14164v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14164v2)
- **Published**: 2022-10-19 21:52:01+00:00
- **Updated**: 2022-10-26 20:01:49+00:00
- **Authors**: Hanieh Naderi, Chinthaka Dinesh, Ivan V. Bajic, Shohreh Kasaei
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in the network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the deep model itself in order to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, in which adversarial points can be predicted independently of the model. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for model-free adversarial point prediction, and which combination of features is best suited for this purpose. Experiments show that a suitable combination of features is able to predict adversarial points of three different networks -- PointNet, PointNet++, and DGCNN -- significantly better than a random guess. The results also provide further insight into DNNs for point cloud analysis, by showing which features play key roles in their decision-making process.



### Prophet Attention: Predicting Attention with Future Attention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2210.10914v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.10914v2)
- **Published**: 2022-10-19 22:29:31+00:00
- **Updated**: 2023-04-11 06:12:01+00:00
- **Authors**: Fenglin Liu, Xuancheng Ren, Xian Wu, Wei Fan, Yuexian Zou, Xu Sun
- **Comment**: Accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words. However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions. Under this setting, these attention models have a "deviated focus" problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the "ideal" attention weights towards image regions. These calculated "ideal" weights are further used to regularize the "deviated" attention. In this manner, image regions are grounded with the correct words. The proposed Prophet Attention can be easily incorporated into existing image captioning models to improve their performance of both grounding and captioning. The experiments on the Flickr30k Entities and the MSCOCO datasets show that the proposed Prophet Attention consistently outperforms baselines in both automatic metrics and human evaluations. It is worth noticing that we set new state-of-the-arts on the two benchmark datasets and achieve the 1st place on the leaderboard of the online MSCOCO benchmark in terms of the default ranking score, i.e., CIDEr-c40.



### Hierarchical classification at multiple operating points
- **Arxiv ID**: http://arxiv.org/abs/2210.10929v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.10929v2)
- **Published**: 2022-10-19 23:36:16+00:00
- **Updated**: 2023-02-10 12:15:31+00:00
- **Authors**: Jack Valmadre
- **Comment**: To appear at NeurIPS 2022
- **Journal**: None
- **Summary**: Many classification problems consider classes that form a hierarchy. Classifiers that are aware of this hierarchy may be able to make confident predictions at a coarse level despite being uncertain at the fine-grained level. While it is generally possible to vary the granularity of predictions using a threshold at inference time, most contemporary work considers only leaf-node prediction, and almost no prior work has compared methods at multiple operating points. We present an efficient algorithm to produce operating characteristic curves for any method that assigns a score to every class in the hierarchy. Applying this technique to evaluate existing methods reveals that top-down classifiers are dominated by a naive flat softmax classifier across the entire operating range. We further propose two novel loss functions and show that a soft variant of the structured hinge loss is able to significantly outperform the flat baseline. Finally, we investigate the poor accuracy of top-down classifiers and demonstrate that they perform relatively well on unseen classes. Code is available online at https://github.com/jvlmdr/hiercls.




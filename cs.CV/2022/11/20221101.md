# Arxiv Papers in cs.CV on 2022-11-01
### Frequency Cam: Imaging Periodic Signals in Real-Time
- **Arxiv ID**: http://arxiv.org/abs/2211.00198v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2211.00198v1)
- **Published**: 2022-11-01 00:08:35+00:00
- **Updated**: 2022-11-01 00:08:35+00:00
- **Authors**: Bernd Pfrommer
- **Comment**: 13 pages, 16 figures, one table
- **Journal**: None
- **Summary**: Due to their high temporal resolution and large dynamic range event cameras are uniquely suited for the analysis of time-periodic signals in an image. In this work we present an efficient and fully asynchronous event camera algorithm for detecting the fundamental frequency at which image pixels flicker. The algorithm employs a second-order digital infinite impulse response (IIR) filter to perform an approximate per-pixel brightness reconstruction and is more robust to high-frequency noise than the baseline method we compare to. We further demonstrate that using the falling edge of the signal leads to more accurate period estimates than the rising edge, and that for certain signals interpolating the zero-level crossings can further increase accuracy. Our experiments find that the outstanding capabilities of the camera in detecting frequencies up to 64kHz for a single pixel do not carry over to full sensor imaging as readout bandwidth limitations become a serious obstacle. This suggests that a hardware implementation closer to the sensor will allow for greatly improved frequency imaging. We discuss the important design parameters for fullsensor frequency imaging and present Frequency Cam, an open-source implementation as a ROS node that can run on a single core of a laptop CPU at more than 50 million events per second. It produces results that are qualitatively very similar to those obtained from the closed source vibration analysis module in Prophesee's Metavision Toolkit. The code for Frequency Cam and a demonstration video can be found at https://github.com/berndpfrommer/frequency_cam



### Learning Melanocytic Cell Masks from Adjacent Stained Tissue
- **Arxiv ID**: http://arxiv.org/abs/2211.00646v3
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00646v3)
- **Published**: 2022-11-01 00:40:09+00:00
- **Updated**: 2023-08-30 19:25:14+00:00
- **Authors**: Mikio Tada, Ursula E. Lang, Iwei Yeh, Maria L. Wei, Michael J. Keiser
- **Comment**: Accepted at Medical Image Learning with Limited & Noisy Data
  Workshop, Medical Image Computing and Computer Assisted Interventions
  (MICCAI) 2022
- **Journal**: None
- **Summary**: Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&E) stained sections and paired immunohistochemistry (IHC) of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.



### GMF: General Multimodal Fusion Framework for Correspondence Outlier Rejection
- **Arxiv ID**: http://arxiv.org/abs/2211.00207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00207v1)
- **Published**: 2022-11-01 01:18:46+00:00
- **Updated**: 2022-11-01 01:18:46+00:00
- **Authors**: Xiaoshui Huang, Wentao Qu, Yifan Zuo, Yuming Fang, Xiaowei Zhao
- **Comment**: Accepted by IEEE RAL
- **Journal**: None
- **Summary**: Rejecting correspondence outliers enables to boost the correspondence quality, which is a critical step in achieving high point cloud registration accuracy. The current state-of-the-art correspondence outlier rejection methods only utilize the structure features of the correspondences. However, texture information is critical to reject the correspondence outliers in our human vision system. In this paper, we propose General Multimodal Fusion (GMF) to learn to reject the correspondence outliers by leveraging both the structure and texture information. Specifically, two cross-attention-based fusion layers are proposed to fuse the texture information from paired images and structure information from point correspondences. Moreover, we propose a convolutional position encoding layer to enhance the difference between Tokens and enable the encoding feature pay attention to neighbor information. Our position encoding layer will make the cross-attention operation integrate both local and global information. Experiments on multiple datasets(3DMatch, 3DLoMatch, KITTI) and recent state-of-the-art models (3DRegNet, DGR, PointDSC) prove that our GMF achieves wide generalization ability and consistently improves the point cloud registration accuracy. Furthermore, several ablation studies demonstrate the robustness of the proposed GMF on different loss functions, lighting conditions and noises.The code is available at https://github.com/XiaoshuiHuang/GMF.



### Pixel-Wise Contrastive Distillation
- **Arxiv ID**: http://arxiv.org/abs/2211.00218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00218v2)
- **Published**: 2022-11-01 02:00:32+00:00
- **Updated**: 2023-08-27 02:54:22+00:00
- **Authors**: Junqiang Huang, Zichao Guo
- **Comment**: ICCV 2023 camera-ready
- **Journal**: None
- **Summary**: We present a simple but effective pixel-level self-supervised distillation framework friendly to dense prediction tasks. Our method, called Pixel-Wise Contrastive Distillation (PCD), distills knowledge by attracting the corresponding pixels from student's and teacher's output feature maps. PCD includes a novel design called SpatialAdaptor which ``reshapes'' a part of the teacher network while preserving the distribution of its output features. Our ablation experiments suggest that this reshaping behavior enables more informative pixel-to-pixel distillation. Moreover, we utilize a plug-in multi-head self-attention module that explicitly relates the pixels of student's feature maps to enhance the effective receptive field, leading to a more competitive student. PCD \textbf{outperforms} previous self-supervised distillation methods on various dense prediction tasks. A backbone of \mbox{ResNet-18-FPN} distilled by PCD achieves $37.4$ AP$^\text{bbox}$ and $34.0$ AP$^\text{mask}$ on COCO dataset using the detector of \mbox{Mask R-CNN}. We hope our study will inspire future research on how to pre-train a small model friendly to dense prediction tasks in a self-supervised fashion.



### Detection of (Hidden) Emotions from Videos using Muscles Movements and Face Manifold Embedding
- **Arxiv ID**: http://arxiv.org/abs/2211.00233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.3; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2211.00233v1)
- **Published**: 2022-11-01 02:48:35+00:00
- **Updated**: 2022-11-01 02:48:35+00:00
- **Authors**: Juni Kim, Zhikang Dong, Eric Guan, Judah Rosenthal, Shi Fu, Miriam Rafailovich, Pawel Polak
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a new non-invasive, easy-to-scale for large amounts of subjects and a remotely accessible method for (hidden) emotion detection from videos of human faces. Our approach combines face manifold detection for accurate location of the face in the video with local face manifold embedding to create a common domain for the measurements of muscle micro-movements that is invariant to the movement of the subject in the video. In the next step, we employ the Digital Image Speckle Correlation (DISC) and the optical flow algorithm to compute the pattern of micro-movements in the face. The corresponding vector field is mapped back to the original space and superimposed on the original frames of the videos. Hence, the resulting videos include additional information about the direction of the movement of the muscles in the face. We take the publicly available CK++ dataset of visible emotions and add to it videos of the same format but with hidden emotions. We process all the videos using our micro-movement detection and use the results to train a state-of-the-art network for emotions classification from videos -- Frame Attention Network (FAN). Although the original FAN model achieves very high out-of-sample performance on the original CK++ videos, it does not perform so well on hidden emotions videos. The performance improves significantly when the model is trained and tested on videos with the vector fields of muscle movements. Intuitively, the corresponding arrows serve as edges in the image that are easily captured by the convolutions filters in the FAN network.



### Angular upsampling in diffusion MRI using contextual HemiHex sub-sampling in q-space
- **Arxiv ID**: http://arxiv.org/abs/2211.00240v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00240v1)
- **Published**: 2022-11-01 03:13:07+00:00
- **Updated**: 2022-11-01 03:13:07+00:00
- **Authors**: Abrar Faiyaz, Md Nasir Uddin, Giovanni Schifitto
- **Comment**: 4 pages, 6 figures, Submitted methodology to MICCAI Challenge 2022
  (QuaD22) at https://www.lpi.tel.uva.es/quad22/index.html
- **Journal**: None
- **Summary**: Artificial Intelligence (Deep Learning(DL)/ Machine Learning(ML)) techniques are widely being used to address and overcome all kinds of ill-posed problems in medical imaging which was or in fact is seemingly impossible. Reducing gradient directions but harnessing high angular resolution(HAR) diffusion data in MR that retains clinical features is an important and challenging problem in the field. While the DL/ML approaches are promising, it is important to incorporate relevant context for the data to ensure that maximum prior information is provided for the AI model to infer the posterior. In this paper, we introduce HemiHex (HH) subsampling to suggestively address training data sampling on q-space geometry, followed by a nearest neighbor regression training on the HH-samples to finally upsample the dMRI data. Earlier studies has tried to use regression for up-sampling dMRI data but yields performance issues as it fails to provide structured geometrical measures for inference. Our proposed approach is a geometrically optimized regression technique which infers the unknown q-space thus addressing the limitations in the earlier studies.



### Training Vision-Language Models with Less Bimodal Supervision
- **Arxiv ID**: http://arxiv.org/abs/2211.00262v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00262v1)
- **Published**: 2022-11-01 04:07:11+00:00
- **Updated**: 2022-11-01 04:07:11+00:00
- **Authors**: Elad Segal, Ben Bogin, Jonathan Berant
- **Comment**: AKBC 2022
- **Journal**: None
- **Summary**: Standard practice in pretraining multimodal models, such as vision-language models, is to rely on pairs of aligned inputs from both modalities, for example, aligned image-text pairs. However, such pairs can be difficult to obtain in low-resource settings and for some modality pairs (e.g., structured tables and images). In this work, we investigate the extent to which we can reduce the reliance on such parallel data, which we term \emph{bimodal supervision}, and use models that are pretrained on each modality independently. We experiment with a high-performing vision-language model, and analyze the effect of bimodal supervision on three vision-language tasks. We find that on simpler tasks, such as VQAv2 and GQA, one can eliminate bimodal supervision completely, suffering only a minor loss in performance. Conversely, for NLVR2, which requires more complex reasoning, training without bimodal supervision leads to random performance. Nevertheless, using only 5\% of the bimodal data (142K images along with their captions), or leveraging weak supervision in the form of a list of machine-generated labels for each image, leads to only a moderate degradation compared to using 3M image-text pairs: 74\%$\rightarrow$$\sim$70\%. Our code is available at https://github.com/eladsegal/less-bimodal-sup.



### Generating Clear Images From Images With Distortions Caused by Adverse Weather Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.05234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05234v1)
- **Published**: 2022-11-01 05:02:44+00:00
- **Updated**: 2022-11-01 05:02:44+00:00
- **Authors**: Nuriel Shalom Mor
- **Comment**: 14 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2112.11245
- **Journal**: None
- **Summary**: We presented a method for improving computer vision tasks on images affected by adverse weather conditions, including distortions caused by adherent raindrops. Overcoming the challenge of applying computer vision to images affected by adverse weather conditions is essential for autonomous vehicles utilizing RGB cameras. For this purpose, we trained an appropriate generative adversarial network and showed that it was effective at removing the effect of the distortions, in the context of image reconstruction and computer vision tasks. We showed that object recognition, a vital task for autonomous driving vehicles, is completely impaired by the distortions and occlusions caused by adherent raindrops and that performance can be restored by our de-raining model. The approach described in this paper could be applied to all adverse weather conditions.



### Strategies for Optimizing End-to-End Artificial Intelligence Pipelines on Intel Xeon Processors
- **Arxiv ID**: http://arxiv.org/abs/2211.00286v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2211.00286v1)
- **Published**: 2022-11-01 05:45:04+00:00
- **Updated**: 2022-11-01 05:45:04+00:00
- **Authors**: Meena Arunachalam, Vrushabh Sanghavi, Yi A Yao, Yi A Zhou, Lifeng A Wang, Zongru Wen, Niroop Ammbashankar, Ning W Wang, Fahim Mohammad
- **Comment**: 10 pages, 11 figures, 3 tables
- **Journal**: The Parallel Universe Magazine, Issue 48, 2022
- **Summary**: End-to-end (E2E) artificial intelligence (AI) pipelines are composed of several stages including data preprocessing, data ingestion, defining and training the model, hyperparameter optimization, deployment, inference, postprocessing, followed by downstream analyses. To obtain efficient E2E workflow, it is required to optimize almost all the stages of pipeline. Intel Xeon processors come with large memory capacities, bundled with AI acceleration (e.g., Intel Deep Learning Boost), well suited to run multiple instances of training and inference pipelines in parallel and has low total cost of ownership (TCO). To showcase the performance on Xeon processors, we applied comprehensive optimization strategies coupled with software and hardware acceleration on variety of E2E pipelines in the areas of Computer Vision, NLP, Recommendation systems, etc. We were able to achieve a performance improvement, ranging from 1.8x to 81.7x across different E2E pipelines. In this paper, we will be highlighting the optimization strategies adopted by us to achieve this performance on Intel Xeon processors with a set of eight different E2E pipelines.



### Self-supervised Character-to-Character Distillation for Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.00288v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00288v4)
- **Published**: 2022-11-01 05:48:18+00:00
- **Updated**: 2023-08-18 14:34:03+00:00
- **Authors**: Tongkun Guan, Wei Shen, Xue Yang, Qi Feng, Zekun Jiang, Xiaokang Yang
- **Comment**: Accepted by ICCV2023
- **Journal**: None
- **Summary**: When handling complicated text images (e.g., irregular structures, low resolution, heavy occlusion, and uneven illumination), existing supervised text recognition methods are data-hungry. Although these methods employ large-scale synthetic text images to reduce the dependence on annotated real images, the domain gap still limits the recognition performance. Therefore, exploring the robust text feature representations on unlabeled real images by self-supervised learning is a good solution. However, existing self-supervised text recognition methods conduct sequence-to-sequence representation learning by roughly splitting the visual features along the horizontal axis, which limits the flexibility of the augmentations, as large geometric-based augmentations may lead to sequence-to-sequence feature inconsistency. Motivated by this, we propose a novel self-supervised Character-to-Character Distillation method, CCD, which enables versatile augmentations to facilitate general text representation learning. Specifically, we delineate the character structures of unlabeled real images by designing a self-supervised character segmentation module. Following this, CCD easily enriches the diversity of local characters while keeping their pairwise alignment under flexible augmentations, using the transformation matrix between two augmented views from images. Experiments demonstrate that CCD achieves state-of-the-art results, with average performance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24 dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code is available at https://github.com/TongkunGuan/CCD.



### Exploring Structure-Wise Uncertainty for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.00303v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00303v1)
- **Published**: 2022-11-01 06:53:00+00:00
- **Updated**: 2022-11-01 06:53:00+00:00
- **Authors**: Anton Vasiliuk, Daria Frolova, Mikhail Belyaev, Boris Shirokikh
- **Comment**: None
- **Journal**: None
- **Summary**: When applying a Deep Learning model to medical images, it is crucial to estimate the model uncertainty. Voxel-wise uncertainty is a useful visual marker for human experts and could be used to improve the model's voxel-wise output, such as segmentation. Moreover, uncertainty provides a solid foundation for out-of-distribution (OOD) detection, improving the model performance on the image-wise level. However, one of the frequent tasks in medical imaging is the segmentation of distinct, local structures such as tumors or lesions. Here, the structure-wise uncertainty allows more precise operations than image-wise and more semantic-aware than voxel-wise. The way to produce uncertainty for individual structures remains poorly explored. We propose a framework to measure the structure-wise uncertainty and evaluate the impact of OOD data on the model performance. Thus, we identify the best UE method to improve the segmentation quality. The proposed framework is tested on three datasets with the tumor segmentation task: LIDC-IDRI, LiTS, and a private one with multiple brain metastases cases.



### HDNet: Hierarchical Dynamic Network for Gait Recognition using Millimeter-Wave Radar
- **Arxiv ID**: http://arxiv.org/abs/2211.00312v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00312v1)
- **Published**: 2022-11-01 07:34:22+00:00
- **Updated**: 2022-11-01 07:34:22+00:00
- **Authors**: Yanyan Huang, Yong Wang, Kun Shi, Chaojie Gu, Yu Fu, Cheng Zhuo, Zhiguo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition is widely used in diversified practical applications. Currently, the most prevalent approach is to recognize human gait from RGB images, owing to the progress of computer vision technologies. Nevertheless, the perception capability of RGB cameras deteriorates in rough circumstances, and visual surveillance may cause privacy invasion. Due to the robustness and non-invasive feature of millimeter wave (mmWave) radar, radar-based gait recognition has attracted increasing attention in recent years. In this research, we propose a Hierarchical Dynamic Network (HDNet) for gait recognition using mmWave radar. In order to explore more dynamic information, we propose point flow as a novel point clouds descriptor. We also devise a dynamic frame sampling module to promote the efficiency of computation without deteriorating performance noticeably. To prove the superiority of our methods, we perform extensive experiments on two public mmWave radar-based gait recognition datasets, and the results demonstrate that our model is superior to existing state-of-the-art methods.



### RGMIM: Region-Guided Masked Image Modeling for Learning Meaningful Representation from X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2211.00313v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00313v4)
- **Published**: 2022-11-01 07:41:03+00:00
- **Updated**: 2023-05-21 14:36:59+00:00
- **Authors**: Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Self-supervised learning has been gaining attention in the medical field for its potential to improve computer-aided diagnosis. One popular method of self-supervised learning is masked image modeling (MIM), which involves masking a subset of input pixels and predicting the masked pixels. However, traditional MIM methods typically use a random masking strategy, which may not be ideal for medical images that often have a small region of interest for disease detection. To address this issue, this work aims to improve MIM for medical images and evaluate its effectiveness in an open X-ray image dataset. Methods: In this paper, we present a novel method called region-guided masked image modeling (RGMIM) for learning meaningful representation from X-ray images. Our method adopts a new masking strategy that utilizes organ mask information to identify valid regions for learning more meaningful representations. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We conduct quantitative evaluations on an open lung X-ray image dataset as well as masking ratio hyperparameter studies. Results: When using the entire training set, RGMIM outperformed other comparable methods, achieving a 0.962 lung disease detection accuracy. Specifically, RGMIM significantly improved performance in small data volumes, such as 5% and 10% of the training set (846 and 1,693 images) compared to other methods, and achieved a 0.957 detection accuracy even when only 50% of the training set was used. Conclusions: RGMIM can mask more valid regions, facilitating the learning of discriminative representations and the subsequent high-accuracy lung disease detection. RGMIM outperforms other state-of-the-art self-supervised learning methods in experiments, particularly when limited training data is used.



### PIPPI2021: An Approach to Automated Diagnosis and Texture Analysis of the Fetal Liver & Placenta in Fetal Growth Restriction
- **Arxiv ID**: http://arxiv.org/abs/2211.02639v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02639v1)
- **Published**: 2022-11-01 08:41:30+00:00
- **Updated**: 2022-11-01 08:41:30+00:00
- **Authors**: Aya Mutaz Zeidan, Paula Ramirez Gilliland, Ashay Patel, Zhanchong Ou, Dimitra Flouri, Nada Mufti, Kasia Maksym, Rosalind Aughwane, Sebastien Ourselin, Anna David, Andrew Melbourne
- **Comment**: None
- **Journal**: None
- **Summary**: Fetal growth restriction (FGR) is a prevalent pregnancy condition characterised by failure of the fetus to reach its genetically predetermined growth potential. We explore the application of model fitting techniques, linear regression machine learning models, deep learning regression, and Haralick textured features from multi-contrast MRI for multi-fetal organ analysis of FGR. We employed T2 relaxometry and diffusion-weighted MRI datasets (using a combined T2-diffusion scan) for 12 normally grown and 12 FGR gestational age (GA) matched pregnancies. We applied the Intravoxel Incoherent Motion Model and novel multi-compartment models for MRI fetal analysis, which exhibit potential to provide a multi-organ FGR assessment, overcoming the limitations of empirical indicators - such as abnormal artery Doppler findings - to evaluate placental dysfunction. The placenta and fetal liver presented key differentiators between FGR and normal controls (decreased perfusion, abnormal fetal blood motion and reduced fetal blood oxygenation. This may be associated with the preferential shunting of the fetal blood towards the fetal brain. These features were further explored to determine their role in assessing FGR severity, by employing simple machine learning models to predict FGR diagnosis (100\% accuracy in test data, n=5), GA at delivery, time from MRI scan to delivery, and baby weight. Moreover, we explored the use of deep learning to regress the latter three variables. Image texture analysis of the fetal organs demonstrated prominent textural variations in the placental perfusion fractions maps between the groups (p$<$0.0009), and spatial differences in the incoherent fetal capillary blood motion in the liver (p$<$0.009). This research serves as a proof-of-concept, investigating the effect of FGR on fetal organs.



### Siamese Transition Masked Autoencoders as Uniform Unsupervised Visual Anomaly Detector
- **Arxiv ID**: http://arxiv.org/abs/2211.00349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00349v1)
- **Published**: 2022-11-01 09:45:49+00:00
- **Updated**: 2022-11-01 09:45:49+00:00
- **Authors**: Haiming Yao, Xue Wang, Wenyong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised visual anomaly detection conveys practical significance in many scenarios and is a challenging task due to the unbounded definition of anomalies. Moreover, most previous methods are application-specific, and establishing a unified model for anomalies across application scenarios remains unsolved. This paper proposes a novel hybrid framework termed Siamese Transition Masked Autoencoders(ST-MAE) to handle various visual anomaly detection tasks uniformly via deep feature transition. Concretely, the proposed method first extracts hierarchical semantics features from a pre-trained deep convolutional neural network and then develops a feature decoupling strategy to split the deep features into two disjoint feature patch subsets. Leveraging the decoupled features, the ST-MAE is developed with the Siamese encoders that operate on each subset of feature patches and perform the latent representations transition of two subsets, along with a lightweight decoder that reconstructs the original feature from the transitioned latent representation. Finally, the anomalous attributes can be detected using the semantic deep feature residual. Our deep feature transition scheme yields a nontrivial and semantic self-supervisory task to extract prototypical normal patterns, which allows for learning uniform models that generalize well for different visual anomaly detection tasks. The extensive experiments conducted demonstrate that the proposed ST-MAE method can advance state-of-the-art performance on multiple benchmarks across application scenarios with a superior inference efficiency, which exhibits great potential to be the uniform model for unsupervised visual anomaly detection.



### Universal Perturbation Attack on Differentiable No-Reference Image- and Video-Quality Metrics
- **Arxiv ID**: http://arxiv.org/abs/2211.00366v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2211.00366v1)
- **Published**: 2022-11-01 10:28:13+00:00
- **Updated**: 2022-11-01 10:28:13+00:00
- **Authors**: Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Universal adversarial perturbation attacks are widely used to analyze image classifiers that employ convolutional neural networks. Nowadays, some attacks can deceive image- and video-quality metrics. So sustainability analysis of these metrics is important. Indeed, if an attack can confuse the metric, an attacker can easily increase quality scores. When developers of image- and video-algorithms can boost their scores through detached processing, algorithm comparisons are no longer fair. Inspired by the idea of universal adversarial perturbation for classifiers, we suggest a new method to attack differentiable no-reference quality metrics through universal perturbation. We applied this method to seven no-reference image- and video-quality metrics (PaQ-2-PiQ, Linearity, VSFA, MDTVSFA, KonCept512, Nima and SPAQ). For each one, we trained a universal perturbation that increases the respective scores. We also propose a method for assessing metric stability and identify the metrics that are the most vulnerable and the most resistant to our attack. The existence of successful universal perturbations appears to diminish the metric's ability to provide reliable scores. We therefore recommend our proposed method as an additional verification of metric reliability to complement traditional subjective tests and benchmarks.



### combined digital drone camera and optical channel parameters for air surveillance
- **Arxiv ID**: http://arxiv.org/abs/2211.00377v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2211.00377v1)
- **Published**: 2022-11-01 10:51:28+00:00
- **Updated**: 2022-11-01 10:51:28+00:00
- **Authors**: Wamidh Jalil Mazher, hadeel Tariq Ibrahim
- **Comment**: None
- **Journal**: None
- **Summary**: Digital drone cameras with free-space optical (FSO) communication networks have been proposed to be promising for air surveillance. In the FSO channel, atmospheric turbulence (AT) degrades the signal. In this study, we combined the parameters of the digital drone camera and the optical channel to mitigate the AT effect. The digital drone camera parameters are indicated by the field of view and camera object distance to support this proposal. Meanwhile, the optical channel parameters, rather than the altitude, are denoted by the most critical parameter, which is the refractive index structure parameter used to characterize the effects of AT. Consequently, two lemmas are proposed and combined to present the optimum relationship between the digital drone camera and optical channel parameters. Therefore, the quality of the entire air surveillance system with a digital drone camera FSO is significantly improved. Furthermore, the analysis and optimization for practical cases were applied to support our findings. Finally, our results demonstrated that an impressive performance improvement of an air surveillance system of 17 dB is possible compared without optimization by combining digital drone camera and FSO parameters at a target outage transceiver probability of $10^-6$.



### Seg&Struct: The Interplay Between Part Segmentation and Structure Inference for 3D Shape Parsing
- **Arxiv ID**: http://arxiv.org/abs/2211.00382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00382v1)
- **Published**: 2022-11-01 10:59:15+00:00
- **Updated**: 2022-11-01 10:59:15+00:00
- **Authors**: Jeonghyun Kim, Kaichun Mo, Minhyuk Sung, Woontack Woo
- **Comment**: WACV 2023 (Algorithm Track)
- **Journal**: None
- **Summary**: We propose Seg&Struct, a supervised learning framework leveraging the interplay between part segmentation and structure inference and demonstrating their synergy in an integrated framework. Both part segmentation and structure inference have been extensively studied in the recent deep learning literature, while the supervisions used for each task have not been fully exploited to assist the other task. Namely, structure inference has been typically conducted with an autoencoder that does not leverage the point-to-part associations. Also, segmentation has been mostly performed without structural priors that tell the plausibility of the output segments. We present how these two tasks can be best combined while fully utilizing supervision to improve performance. Our framework first decomposes a raw input shape into part segments using an off-the-shelf algorithm, whose outputs are then mapped to nodes in a part hierarchy, establishing point-to-part associations. Following this, ours predicts the structural information, e.g., part bounding boxes and part relationships. Lastly, the segmentation is rectified by examining the confusion of part boundaries using the structure-based part features. Our experimental results based on the StructureNet and PartNet demonstrate that the interplay between the two tasks results in remarkable improvements in both tasks: 27.91% in structure inference and 0.5% in segmentation.



### Behavioral Intention Prediction in Driving Scenes: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2211.00385v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.00385v2)
- **Published**: 2022-11-01 11:07:37+00:00
- **Updated**: 2022-11-02 03:02:58+00:00
- **Authors**: Jianwu Fang, Fan Wang, Peining Shen, Zhedong Zheng, Jianru Xue, Tat-seng Chua
- **Comment**: Submitted to IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: In the driving scene, the road participants usually show frequent interaction and intention understanding with the surrounding. Ego-agent (each road participant itself) conducts the prediction of what behavior will be done by other road users all the time and expects a shared and consistent understanding. For instance, we need to predict the next movement of other road users and expect a consistent joint action to avoid unexpected accident. Behavioral Intention Prediction (BIP) is to simulate such a human consideration process and fulfill the beginning time prediction of specific behaviors. It provides an earlier signal promptly than the specific behaviors for whether the surrounding road participants will present specific behavior (crossing, overtaking, and turning, etc.) in near future or not. More and more works in BIP are based on deep learning models to take advantage of big data, and focus on developing effective inference approaches (e.g., explainable inference, cross-modality fusion, and simulation augmentation). Therefore, in this work, we focus on BIP-conditioned prediction tasks, including trajectory prediction, behavior prediction, and accident prediction and explore the differences among various works in this field. Based on this investigation and the findings, we discuss the open problems in behavioral intention prediction and propose future research directions.



### Expansion of Visual Hints for Improved Generalization in Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2211.00392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00392v1)
- **Published**: 2022-11-01 11:30:26+00:00
- **Updated**: 2022-11-01 11:30:26+00:00
- **Authors**: Andrea Pilzer, Yuxin Hou, Niki Loppi, Arno Solin, Juho Kannala
- **Comment**: 2023 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: We introduce visual hints expansion for guiding stereo matching to improve generalization. Our work is motivated by the robustness of Visual Inertial Odometry (VIO) in computer vision and robotics, where a sparse and unevenly distributed set of feature points characterizes a scene. To improve stereo matching, we propose to elevate 2D hints to 3D points. These sparse and unevenly distributed 3D visual hints are expanded using a 3D random geometric graph, which enhances the learning and inference process. We evaluate our proposal on multiple widely adopted benchmarks and show improved performance without access to additional sensors other than the image sequence. To highlight practical applicability and symbiosis with visual odometry, we demonstrate how our methods run on embedded hardware.



### Galaxy classification: a deep learning approach for classifying Sloan Digital Sky Survey images
- **Arxiv ID**: http://arxiv.org/abs/2211.00397v1
- **DOI**: 10.1093/mnras/stac457
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00397v1)
- **Published**: 2022-11-01 11:43:21+00:00
- **Updated**: 2022-11-01 11:43:21+00:00
- **Authors**: Sarvesh Gharat, Yogesh Dandawate
- **Comment**: Published in MNRAS
- **Journal**: MNRAS, 511, 2022, 5120-5124
- **Summary**: In recent decades, large-scale sky surveys such as Sloan Digital Sky Survey (SDSS) have resulted in generation of tremendous amount of data. The classification of this enormous amount of data by astronomers is time consuming. To simplify this process, in 2007 a volunteer-based citizen science project called Galaxy Zoo was introduced, which has reduced the time for classification by a good extent. However, in this modern era of deep learning, automating this classification task is highly beneficial as it reduces the time for classification. For the last few years, many algorithms have been proposed which happen to do a phenomenal job in classifying galaxies into multiple classes. But all these algorithms tend to classify galaxies into less than six classes. However, after considering the minute information which we know about galaxies, it is necessary to classify galaxies into more than eight classes. In this study, a neural network model is proposed so as to classify SDSS data into 10 classes from an extended Hubble Tuning Fork. Great care is given to disc edge and disc face galaxies, distinguishing between a variety of substructures and minute features which are associated with each class. The proposed model consists of convolution layers to extract features making this method fully automatic. The achieved test accuracy is 84.73 per cent which happens to be promising after considering such minute details in classes. Along with convolution layers, the proposed model has three more layers responsible for classification, which makes the algorithm consume less time.



### Oracle-guided Contrastive Clustering
- **Arxiv ID**: http://arxiv.org/abs/2211.00409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00409v1)
- **Published**: 2022-11-01 12:05:12+00:00
- **Updated**: 2022-11-01 12:05:12+00:00
- **Authors**: Mengdie Wang, Liyuan Shang, Suyun Zhao, Yiming Wang, Hong Chen, Cuiping Li, Xizhao Wang
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Deep clustering aims to learn a clustering representation through deep architectures. Most of the existing methods usually conduct clustering with the unique goal of maximizing clustering performance, that ignores the personalized demand of clustering tasks.% and results in unguided clustering solutions. However, in real scenarios, oracles may tend to cluster unlabeled data by exploiting distinct criteria, such as distinct semantics (background, color, object, etc.), and then put forward personalized clustering tasks. To achieve task-aware clustering results, in this study, Oracle-guided Contrastive Clustering(OCC) is then proposed to cluster by interactively making pairwise ``same-cluster" queries to oracles with distinctive demands. Specifically, inspired by active learning, some informative instance pairs are queried, and evaluated by oracles whether the pairs are in the same cluster according to their desired orientation. And then these queried same-cluster pairs extend the set of positive instance pairs for contrastive learning, guiding OCC to extract orientation-aware feature representation. Accordingly, the query results, guided by oracles with distinctive demands, may drive the OCC's clustering results in a desired orientation. Theoretically, the clustering risk in an active learning manner is given with a tighter upper bound, that guarantees active queries to oracles do mitigate the clustering risk. Experimentally, extensive results verify that OCC can cluster accurately along the specific orientation and it substantially outperforms the SOTA clustering methods as well. To the best of our knowledge, it is the first deep framework to perform personalized clustering.



### A new filter for dimensionality reduction and classification of hyperspectral images using GLCM features and mutual information
- **Arxiv ID**: http://arxiv.org/abs/2211.00446v1
- **DOI**: 10.1504/IJSISE.2018.093824
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00446v1)
- **Published**: 2022-11-01 13:19:08+00:00
- **Updated**: 2022-11-01 13:19:08+00:00
- **Authors**: Hasna Nhaila, Elkebir Sarhrouni, Ahmed Hammouch
- **Comment**: None
- **Journal**: International Journal of Signal and Imaging Systems Engineering,
  2018, 11(4), pp. 193-205. -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-85051431092&partnerID=MN8TOARS
- **Summary**: Dimensionality reduction is an important preprocessing step of the hyperspectral images classification (HSI), it is inevitable task. Some methods use feature selection or extraction algorithms based on spectral and spatial information. In this paper, we introduce a new methodology for dimensionality reduction and classification of HSI taking into account both spectral and spatial information based on mutual information. We characterise the spatial information by the texture features extracted from the grey level cooccurrence matrix (GLCM); we use Homogeneity, Contrast, Correlation and Energy. For classification, we use support vector machine (SVM). The experiments are performed on three well-known hyperspectral benchmark datasets. The proposed algorithm is compared with the state of the art methods. The obtained results of this fusion show that our method outperforms the other approaches by increasing the classification accuracy in a good timing. This method may be improved for more performance   Keywords: hyperspectral images; classification; spectral and spatial features; grey level cooccurrence matrix; GLCM; mutual information; support vector machine; SVM.



### Signing Outside the Studio: Benchmarking Background Robustness for Continuous Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.00448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00448v1)
- **Published**: 2022-11-01 13:27:44+00:00
- **Updated**: 2022-11-01 13:27:44+00:00
- **Authors**: Youngjoon Jang, Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, Joon Son Chung, In So Kweon
- **Comment**: Our dataset is available at
  https://github.com/art-jang/Signing-Outside-the-Studio
- **Journal**: None
- **Summary**: The goal of this work is background-robust continuous sign language recognition. Most existing Continuous Sign Language Recognition (CSLR) benchmarks have fixed backgrounds and are filmed in studios with a static monochromatic background. However, signing is not limited only to studios in the real world. In order to analyze the robustness of CSLR models under background shifts, we first evaluate existing state-of-the-art CSLR models on diverse backgrounds. To synthesize the sign videos with a variety of backgrounds, we propose a pipeline to automatically generate a benchmark dataset utilizing existing CSLR benchmarks. Our newly constructed benchmark dataset consists of diverse scenes to simulate a real-world environment. We observe even the most recent CSLR method cannot recognize glosses well on our new dataset with changed backgrounds. In this regard, we also propose a simple yet effective training scheme including (1) background randomization and (2) feature disentanglement for CSLR models. The experimental results on our dataset demonstrate that our method generalizes well to other unseen background data with minimal additional training images.



### Recognition of Defective Mineral Wool Using Pruned ResNet Models
- **Arxiv ID**: http://arxiv.org/abs/2211.00466v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00466v1)
- **Published**: 2022-11-01 13:58:02+00:00
- **Updated**: 2022-11-01 13:58:02+00:00
- **Authors**: Mehdi Rafiei, Dat Thanh Tran, Alexandros Iosifidis
- **Comment**: 6 pages, 5 figures, 3 tables Submitted on IEEE Transactions on
  Industrial Informatics
- **Journal**: None
- **Summary**: Mineral wool production is a non-linear process that makes it hard to control the final quality. Therefore, having a non-destructive method to analyze the product quality and recognize defective products is critical. For this purpose, we developed a visual quality control system for mineral wool. X-ray images of wool specimens were collected to create a training set of defective and non-defective samples. Afterward, we developed several recognition models based on the ResNet architecture to find the most efficient model. In order to have a light-weight and fast inference model for real-life applicability, two structural pruning methods are applied to the classifiers. Considering the low quantity of the dataset, cross-validation and augmentation methods are used during the training. As a result, we obtained a model with more than 98% accuracy, which in comparison to the current procedure used at the company, it can recognize 20% more defective products.



### Understanding the Unforeseen via the Intentional Stance
- **Arxiv ID**: http://arxiv.org/abs/2211.00478v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2211.00478v1)
- **Published**: 2022-11-01 14:14:14+00:00
- **Updated**: 2022-11-01 14:14:14+00:00
- **Authors**: Stephanie Stacy, Alfredo Gabaldon, John Karigiannis, James Kubrich, Peter Tu
- **Comment**: None
- **Journal**: None
- **Summary**: We present an architecture and system for understanding novel behaviors of an observed agent. The two main features of our approach are the adoption of Dennett's intentional stance and analogical reasoning as one of the main computational mechanisms for understanding unforeseen experiences. Our approach uses analogy with past experiences to construct hypothetical rationales that explain the behavior of an observed agent. Moreover, we view analogies as partial; thus multiple past experiences can be blended to analogically explain an unforeseen event, leading to greater inferential flexibility. We argue that this approach results in more meaningful explanations of observed behavior than approaches based on surface-level comparisons. A key advantage of behavior explanation over classification is the ability to i) take appropriate responses based on reasoning and ii) make non-trivial predictions that allow for the verification of the hypothesized explanation. We provide a simple use case to demonstrate novel experience understanding through analogy in a gas station environment.



### Self-Supervised Intensity-Event Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2211.00509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00509v1)
- **Published**: 2022-11-01 14:52:25+00:00
- **Updated**: 2022-11-01 14:52:25+00:00
- **Authors**: Jinjin Gu, Jinan Zhou, Ringo Sai Wo Chu, Yan Chen, Jiawei Zhang, Xuanye Cheng, Song Zhang, Jimmy S. Ren
- **Comment**: This paper has been accepted by the Journal of Imaging Science &
  Technology
- **Journal**: None
- **Summary**: Event cameras are novel bio-inspired vision sensors that output pixel-level intensity changes in microsecond accuracy with a high dynamic range and low power consumption. Despite these advantages, event cameras cannot be directly applied to computational imaging tasks due to the inability to obtain high-quality intensity and events simultaneously. This paper aims to connect a standalone event camera and a modern intensity camera so that the applications can take advantage of both two sensors. We establish this connection through a multi-modal stereo matching task. We first convert events to a reconstructed image and extend the existing stereo networks to this multi-modality condition. We propose a self-supervised method to train the multi-modal stereo network without using ground truth disparity data. The structure loss calculated on image gradients is used to enable self-supervised learning on such multi-modal data. Exploiting the internal stereo constraint between views with different modalities, we introduce general stereo loss functions, including disparity cross-consistency loss and internal disparity loss, leading to improved performance and robustness compared to existing approaches. The experiments demonstrate the effectiveness of the proposed method, especially the proposed general stereo loss functions, on both synthetic and real datasets. At last, we shed light on employing the aligned events and intensity images in downstream tasks, e.g., video interpolation application.



### Infinite-Dimensional Adaptive Boundary Observer for Inner-Domain Temperature Estimation of 3D Electrosurgical Processes using Surface Thermography Sensing
- **Arxiv ID**: http://arxiv.org/abs/2211.00515v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY, 92-10, 80-10, 93C40, 68T05, I.4.8; I.6.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2211.00515v1)
- **Published**: 2022-11-01 15:03:01+00:00
- **Updated**: 2022-11-01 15:03:01+00:00
- **Authors**: Hamza El-Kebir, Junren Ran, Martin Ostoja-Starzewski, Richard Berlin, Joseph Bentsman, Leonardo P. Chamorro
- **Comment**: Paper accepted to the 2022 IEEE Conference on Decision and Control
  (CDC 2022)
- **Journal**: None
- **Summary**: We present a novel 3D adaptive observer framework for use in the determination of subsurface organic tissue temperatures in electrosurgery. The observer structure leverages pointwise 2D surface temperature readings obtained from a real-time infrared thermographer for both parameter estimation and temperature field observation. We introduce a novel approach to decoupled parameter adaptation and estimation, wherein the parameter estimation can run in real-time, while the observer loop runs on a slower time scale. To achieve this, we introduce a novel parameter estimation method known as attention-based noise-robust averaging, in which surface thermography time series are used to directly estimate the tissue's diffusivity. Our observer contains a real-time parameter adaptation component based on this diffusivity adaptation law, as well as a Luenberger-type corrector based on the sensed surface temperature. In this work, we also present a novel model structure adapted to the setting of robotic surgery, wherein we model the electrosurgical heat distribution as a compactly supported magnitude- and velocity-controlled heat source involving a new nonlinear input mapping. We demonstrate satisfactory performance of the adaptive observer in simulation, using real-life experimental ex vivo porcine tissue data.



### Learning Neural Implicit Representations with Surface Signal Parameterizations
- **Arxiv ID**: http://arxiv.org/abs/2211.00519v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00519v2)
- **Published**: 2022-11-01 15:10:58+00:00
- **Updated**: 2023-06-26 00:32:56+00:00
- **Authors**: Yanran Guan, Andrei Chubarau, Ruby Rao, Derek Nowrouzezahrai
- **Comment**: None
- **Journal**: None
- **Summary**: Neural implicit surface representations have recently emerged as popular alternative to explicit 3D object encodings, such as polygonal meshes, tabulated points, or voxels. While significant work has improved the geometric fidelity of these representations, much less attention is given to their final appearance. Traditional explicit object representations commonly couple the 3D shape data with auxiliary surface-mapped image data, such as diffuse color textures and fine-scale geometric details in normal maps that typically require a mapping of the 3D surface onto a plane, i.e., a surface parameterization; implicit representations, on the other hand, cannot be easily textured due to lack of configurable surface parameterization. Inspired by this digital content authoring methodology, we design a neural network architecture that implicitly encodes the underlying surface parameterization suitable for appearance data. As such, our model remains compatible with existing mesh-based digital content with appearance data. Motivated by recent work that overfits compact networks to individual 3D objects, we present a new weight-encoded neural implicit representation that extends the capability of neural implicit surfaces to enable various common and important applications of texture mapping. Our method outperforms reasonable baselines and state-of-the-art alternatives.



### The Enemy of My Enemy is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2211.00525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00525v1)
- **Published**: 2022-11-01 15:24:26+00:00
- **Updated**: 2022-11-01 15:24:26+00:00
- **Authors**: Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, Xiaohua Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Although current deep learning techniques have yielded superior performance on various computer vision tasks, yet they are still vulnerable to adversarial examples. Adversarial training and its variants have been shown to be the most effective approaches to defend against adversarial examples. These methods usually regularize the difference between output probabilities for an adversarial and its corresponding natural example. However, it may have a negative impact if the model misclassifies a natural example. To circumvent this issue, we propose a novel adversarial training scheme that encourages the model to produce similar outputs for an adversarial example and its ``inverse adversarial'' counterpart. These samples are generated to maximize the likelihood in the neighborhood of natural examples. Extensive experiments on various vision datasets and architectures demonstrate that our training method achieves state-of-the-art robustness as well as natural accuracy. Furthermore, using a universal version of inverse adversarial examples, we improve the performance of single-step adversarial training techniques at a low computational cost.



### Self-Supervised Learning with Limited Labeled Data for Prostate Cancer Detection in High Frequency Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2211.00527v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00527v1)
- **Published**: 2022-11-01 15:28:15+00:00
- **Updated**: 2022-11-01 15:28:15+00:00
- **Authors**: Paul F. R. Wilson, Mahdi Gilany, Amoon Jamzad, Fahimeh Fooladgar, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based analysis of high-frequency, high-resolution micro-ultrasound data shows great promise for prostate cancer detection. Previous approaches to analysis of ultrasound data largely follow a supervised learning paradigm. Ground truth labels for ultrasound images used for training deep networks often include coarse annotations generated from the histopathological analysis of tissue samples obtained via biopsy. This creates inherent limitations on the availability and quality of labeled data, posing major challenges to the success of supervised learning methods. On the other hand, unlabeled prostate ultrasound data are more abundant. In this work, we successfully apply self-supervised representation learning to micro-ultrasound data. Using ultrasound data from 1028 biopsy cores of 391 subjects obtained in two clinical centres, we demonstrate that feature representations learnt with this method can be used to classify cancer from non-cancer tissue, obtaining an AUROC score of 91% on an independent test set. To the best of our knowledge, this is the first successful end-to-end self-supervised learning approach for prostate cancer detection using ultrasound data. Our method outperforms baseline supervised learning approaches, generalizes well between different data centers, and scale well in performance as more unlabeled data are added, making it a promising approach for future research using large volumes of unlabeled data.



### DOLPH: Diffusion Models for Phase Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2211.00529v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00529v2)
- **Published**: 2022-11-01 15:31:34+00:00
- **Updated**: 2022-11-02 03:25:20+00:00
- **Authors**: Shirin Shoushtari, Jiaming Liu, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Phase retrieval refers to the problem of recovering an image from the magnitudes of its complex-valued linear measurements. Since the problem is ill-posed, the recovery requires prior knowledge on the unknown image. We present DOLPH as a new deep model-based architecture for phase retrieval that integrates an image prior specified using a diffusion model with a nonconvex data-fidelity term for phase retrieval. Diffusion models are a recent class of deep generative models that are relatively easy to train due to their implementation as image denoisers. DOLPH reconstructs high-quality solutions by alternating data-consistency updates with the sampling step of a diffusion model. Our numerical results show the robustness of DOLPH to noise and its ability to generate several candidate solutions given a set of measurements.



### Robustness of Deep Equilibrium Architectures to Changes in the Measurement Model
- **Arxiv ID**: http://arxiv.org/abs/2211.00531v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00531v1)
- **Published**: 2022-11-01 15:35:07+00:00
- **Updated**: 2022-11-01 15:35:07+00:00
- **Authors**: Junhao Hu, Shirin Shoushtari, Zihao Zou, Jiaming Liu, Zhixin Sun, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep model-based architectures (DMBAs) are widely used in imaging inverse problems to integrate physical measurement models and learned image priors. Plug-and-play priors (PnP) and deep equilibrium models (DEQ) are two DMBA frameworks that have received significant attention. The key difference between the two is that the image prior in DEQ is trained by using a specific measurement model, while that in PnP is trained as a general image denoiser. This difference is behind a common assumption that PnP is more robust to changes in the measurement models compared to DEQ. This paper investigates the robustness of DEQ priors to changes in the measurement models. Our results on two imaging inverse problems suggest that DEQ priors trained under mismatched measurement models outperform image denoisers.



### Deep Learning for Global Wildfire Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2211.00534v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00534v2)
- **Published**: 2022-11-01 15:39:01+00:00
- **Updated**: 2022-11-06 13:52:22+00:00
- **Authors**: Ioannis Prapas, Akanksha Ahuja, Spyros Kondylatos, Ilektra Karasante, Eleanna Panagiotou, Lazaro Alonso, Charalampos Davalas, Dimitrios Michail, Nuno Carvalhais, Ioannis Papoutsis
- **Comment**: Accepted at the NeurIPS 2022 workshop on Tackling Climate Change with
  Machine Learning
- **Journal**: None
- **Summary**: Climate change is expected to aggravate wildfire activity through the exacerbation of fire weather. Improving our capabilities to anticipate wildfires on a global scale is of uttermost importance for mitigating their negative effects. In this work, we create a global fire dataset and demonstrate a prototype for predicting the presence of global burned areas on a sub-seasonal scale with the use of segmentation deep learning models. Particularly, we present an open-access global analysis-ready datacube, which contains a variety of variables related to the seasonal and sub-seasonal fire drivers (climate, vegetation, oceanic indices, human-related variables), as well as the historical burned areas and wildfire emissions for 2001-2021. We train a deep learning model, which treats global wildfire forecasting as an image segmentation task and skillfully predicts the presence of burned areas 8, 16, 32 and 64 days ahead of time. Our work motivates the use of deep learning for global burned area forecasting and paves the way towards improved anticipation of global wildfire patterns.



### Geo-Information Harvesting from Social Media Data
- **Arxiv ID**: http://arxiv.org/abs/2211.00543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00543v1)
- **Published**: 2022-11-01 15:47:18+00:00
- **Updated**: 2022-11-01 15:47:18+00:00
- **Authors**: Xiao Xiang Zhu, Yuanyuan Wang, Mrinalini Kochupillai, Martin Werner, Matthias Hberle, Eike Jens Hoffmann, Hannes Taubenbck, Devis Tuia, Alex Levering, Nathan Jacobs, Anna Kruspe, Karam Abdulahhad
- **Comment**: Accepted for publication IEEE Geoscience and Remote Sensing Magazine
- **Journal**: None
- **Summary**: As unconventional sources of geo-information, massive imagery and text messages from open platforms and social media form a temporally quasi-seamless, spatially multi-perspective stream, but with unknown and diverse quality. Due to its complementarity to remote sensing data, geo-information from these sources offers promising perspectives, but harvesting is not trivial due to its data characteristics. In this article, we address key aspects in the field, including data availability, analysis-ready data preparation and data management, geo-information extraction from social media text messages and images, and the fusion of social media and remote sensing data. We then showcase some exemplary geographic applications. In addition, we present the first extensive discussion of ethical considerations of social media data in the context of geo-information harvesting and geographic applications. With this effort, we wish to stimulate curiosity and lay the groundwork for researchers who intend to explore social media data for geo-applications. We encourage the community to join forces by sharing their code and data.



### No-audio speaking status detection in crowded settings via visual pose-based filtering and wearable acceleration
- **Arxiv ID**: http://arxiv.org/abs/2211.00549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2211.00549v1)
- **Published**: 2022-11-01 15:55:48+00:00
- **Updated**: 2022-11-01 15:55:48+00:00
- **Authors**: Jose Vargas-Quiros, Laura Cabrera-Quiros, Hayley Hung
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing who is speaking in a crowded scene is a key challenge towards the understanding of the social interactions going on within. Detecting speaking status from body movement alone opens the door for the analysis of social scenes in which personal audio is not obtainable. Video and wearable sensors make it possible recognize speaking in an unobtrusive, privacy-preserving way. When considering the video modality, in action recognition problems, a bounding box is traditionally used to localize and segment out the target subject, to then recognize the action taking place within it. However, cross-contamination, occlusion, and the articulated nature of the human body, make this approach challenging in a crowded scene. Here, we leverage articulated body poses for subject localization and in the subsequent speech detection stage. We show that the selection of local features around pose keypoints has a positive effect on generalization performance while also significantly reducing the number of local features considered, making for a more efficient method. Using two in-the-wild datasets with different viewpoints of subjects, we investigate the role of cross-contamination in this effect. We additionally make use of acceleration measured through wearable sensors for the same task, and present a multimodal approach combining both methods.



### Leveraging commonsense for object localisation in partial scenes
- **Arxiv ID**: http://arxiv.org/abs/2211.00562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00562v1)
- **Published**: 2022-11-01 16:17:07+00:00
- **Updated**: 2022-11-01 16:17:07+00:00
- **Authors**: Francesco Giuliari, Geri Skenderi, Marco Cristani, Alessio Del Bue, Yiming Wang
- **Comment**: arXiv admin note: text overlap with arXiv:2203.05380
- **Journal**: None
- **Summary**: We propose an end-to-end solution to address the problem of object localisation in partial scenes, where we aim to estimate the position of an object in an unknown area given only a partial 3D scan of the scene. We propose a novel scene representation to facilitate the geometric reasoning, Directed Spatial Commonsense Graph (D-SCG), a spatial scene graph that is enriched with additional concept nodes from a commonsense knowledge base. Specifically, the nodes of D-SCG represent the scene objects and the edges are their relative positions. Each object node is then connected via different commonsense relationships to a set of concept nodes. With the proposed graph-based scene representation, we estimate the unknown position of the target object using a Graph Neural Network that implements a novel attentional message passing mechanism. The network first predicts the relative positions between the target object and each visible object by learning a rich representation of the objects via aggregating both the object nodes and the concept nodes in D-SCG. These relative positions then are merged to obtain the final position. We evaluate our method using Partial ScanNet, improving the state-of-the-art by 5.9% in terms of the localisation accuracy at a 8x faster training speed.



### Text-Only Training for Image Captioning using Noise-Injected CLIP
- **Arxiv ID**: http://arxiv.org/abs/2211.00575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00575v1)
- **Published**: 2022-11-01 16:36:01+00:00
- **Updated**: 2022-11-01 16:36:01+00:00
- **Authors**: David Nukrai, Ron Mokady, Amir Globerson
- **Comment**: Will be presented at EMNLP 2022. GitHub:
  https://github.com/DavidHuji/CapDec
- **Journal**: EMNLP 2022
- **Summary**: We consider the task of image-captioning using only the CLIP model and additional text data at training time, and no additional captioned images. Our approach relies on the fact that CLIP is trained to make visual and textual embeddings similar. Therefore, we only need to learn how to translate CLIP textual embeddings back into text, and we can learn how to do this by learning a decoder for the frozen CLIP text encoder using only text. We argue that this intuition is "almost correct" because of a gap between the embedding spaces, and propose to rectify this via noise injection during training. We demonstrate the effectiveness of our approach by showing SOTA zero-shot image captioning across four benchmarks, including style transfer. Code, data, and models are available on GitHub.



### Fine-tuned Generative Adversarial Network-based Model for Medical Images Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2211.00577v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00577v5)
- **Published**: 2022-11-01 16:48:04+00:00
- **Updated**: 2022-11-17 12:43:21+00:00
- **Authors**: Alireza Aghelan, Modjtaba Rouhani
- **Comment**: None
- **Journal**: None
- **Summary**: In medical image analysis, low-resolution images negatively affect the performance of medical image interpretation and may cause misdiagnosis. Single image super-resolution (SISR) methods can improve the resolution and quality of medical images. Currently, Generative Adversarial Networks (GAN) based super-resolution models have shown very good performance. Real-Enhanced Super-Resolution Generative Adversarial Network (Real-ESRGAN) is one of the practical GAN-based models which is widely used in the field of general image super-resolution. One of the challenges in the field of medical image super-resolution is that, unlike natural images, medical images do not have high spatial resolution. To solve this problem, we can use transfer learning technique and fine-tune the model that has been trained on external datasets (often natural datasets). In our proposed approach, the pre-trained generator and discriminator networks of the Real-ESRGAN model are fine-tuned using medical image datasets. In this paper, we worked on chest X-ray and retinal images and used the STARE dataset of retinal images and Tuberculosis Chest X-rays (Shenzhen) dataset for fine-tuning. The proposed model produces more accurate and natural textures, and its outputs have better details and resolution compared to the original Real-ESRGAN outputs.



### MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model
- **Arxiv ID**: http://arxiv.org/abs/2211.00611v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00611v5)
- **Published**: 2022-11-01 17:24:44+00:00
- **Updated**: 2023-01-15 03:50:56+00:00
- **Authors**: Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu, Yanwu Xu
- **Comment**: Code is released at: https://github.com/WuJunde/MedSegDiff
- **Journal**: None
- **Summary**: Diffusion probabilistic model (DPM) recently becomes one of the hottest topic in computer vision. Its image generation application such as Imagen, Latent Diffusion Models and Stable Diffusion have shown impressive generation capabilities, which aroused extensive discussion in the community. Many recent studies also found it is useful in many other vision tasks, like image deblurring, super-resolution and anomaly detection. Inspired by the success of DPM, we propose the first DPM based model toward general medical image segmentation tasks, which we named MedSegDiff. In order to enhance the step-wise regional attention in DPM for the medical image segmentation, we propose dynamic conditional encoding, which establishes the state-adaptive conditions for each sampling step. We further propose Feature Frequency Parser (FF-Parser), to eliminate the negative effect of high-frequency noise component in this process. We verify MedSegDiff on three medical segmentation tasks with different image modalities, which are optic cup segmentation over fundus images, brain tumor segmentation over MRI images and thyroid nodule segmentation over ultrasound images. The experimental results show that MedSegDiff outperforms state-of-the-art (SOTA) methods with considerable performance gap, indicating the generalization and effectiveness of the proposed model. Our code is released at https://github.com/WuJunde/MedSegDiff.



### Semi-Supervised Domain Adaptation for Cross-Survey Galaxy Morphology Classification and Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.00677v3
- **DOI**: None
- **Categories**: **astro-ph.GA**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00677v3)
- **Published**: 2022-11-01 18:07:21+00:00
- **Updated**: 2022-11-11 16:18:37+00:00
- **Authors**: Aleksandra iprijanovi, Ashia Lewis, Kevin Pedro, Sandeep Madireddy, Brian Nord, Gabriel N. Perdue, Stefan M. Wild
- **Comment**: 3 figures, 1 table; accepted to Machine Learning and the Physical
  Sciences - Workshop at the 36th conference on Neural Information Processing
  Systems (NeurIPS)
- **Journal**: None
- **Summary**: In the era of big astronomical surveys, our ability to leverage artificial intelligence algorithms simultaneously for multiple datasets will open new avenues for scientific discovery. Unfortunately, simply training a deep neural network on images from one data domain often leads to very poor performance on any other dataset. Here we develop a Universal Domain Adaptation method DeepAstroUDA, capable of performing semi-supervised domain alignment that can be applied to datasets with different types of class overlap. Extra classes can be present in any of the two datasets, and the method can even be used in the presence of unknown classes. For the first time, we demonstrate the successful use of domain adaptation on two very different observational datasets (from SDSS and DECaLS). We show that our method is capable of bridging the gap between two astronomical surveys, and also performs well for anomaly detection and clustering of unknown data in the unlabeled dataset. We apply our model to two examples of galaxy morphology classification tasks with anomaly detection: 1) classifying spiral and elliptical galaxies with detection of merging galaxies (three classes including one unknown anomaly class); 2) a more granular problem where the classes describe more detailed morphological properties of galaxies, with the detection of gravitational lenses (ten classes including one unknown anomaly class).



### Transfer learning and Local interpretable model agnostic based visual approach in Monkeypox Disease Detection and Classification: A Deep Learning insights
- **Arxiv ID**: http://arxiv.org/abs/2211.05633v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.05633v2)
- **Published**: 2022-11-01 18:07:34+00:00
- **Updated**: 2022-11-14 19:18:16+00:00
- **Authors**: Md Manjurul Ahsan, Tareque Abu Abdullah, Md Shahin Ali, Fatematuj Jahora, Md Khairul Islam, Amin G. Alhashim, Kishor Datta Gupta
- **Comment**: We have removed the picture of the children from Figure 1 as we found
  it not to be appropriate or necessarily important
- **Journal**: None
- **Summary**: The recent development of Monkeypox disease among various nations poses a global pandemic threat when the world is still fighting Coronavirus Disease-2019 (COVID-19). At its dawn, the slow and steady transmission of Monkeypox disease among individuals needs to be addressed seriously. Over the years, Deep learning (DL) based disease prediction has demonstrated true potential by providing early, cheap, and affordable diagnosis facilities. Considering this opportunity, we have conducted two studies where we modified and tested six distinct deep learning models-VGG16, InceptionResNetV2, ResNet50, ResNet101, MobileNetV2, and VGG19-using transfer learning approaches. Our preliminary computational results show that the proposed modified InceptionResNetV2 and MobileNetV2 models perform best by achieving an accuracy ranging from 93% to 99%. Our findings are reinforced by recent academic work that demonstrates improved performance in constructing multiple disease diagnosis models using transfer learning approaches. Lastly, we further explain our model prediction using Local Interpretable Model-Agnostic Explanations (LIME), which play an essential role in identifying important features that characterize the onset of Monkeypox disease.



### On the detection of synthetic images generated by diffusion models
- **Arxiv ID**: http://arxiv.org/abs/2211.00680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00680v1)
- **Published**: 2022-11-01 18:10:55+00:00
- **Updated**: 2022-11-01 18:10:55+00:00
- **Authors**: Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decade, there has been tremendous progress in creating synthetic media, mainly thanks to the development of powerful methods based on generative adversarial networks (GAN). Very recently, methods based on diffusion models (DM) have been gaining the spotlight. In addition to providing an impressive level of photorealism, they enable the creation of text-based visual content, opening up new and exciting opportunities in many different application fields, from arts to video games. On the other hand, this property is an additional asset in the hands of malicious users, who can generate and distribute fake media perfectly adapted to their attacks, posing new challenges to the media forensic community. With this work, we seek to understand how difficult it is to distinguish synthetic images generated by diffusion models from pristine ones and whether current state-of-the-art detectors are suitable for the task. To this end, first we expose the forensics traces left by diffusion models, then study how current detectors, developed for GAN-generated images, perform on these new synthetic images, especially in challenging social-networks scenarios involving image compression and resizing. Datasets and code are available at github.com/grip-unina/DMimageDetection.



### SleepyWheels: An Ensemble Model for Drowsiness Detection leading to Accident Prevention
- **Arxiv ID**: http://arxiv.org/abs/2211.00718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.00718v1)
- **Published**: 2022-11-01 19:36:47+00:00
- **Updated**: 2022-11-01 19:36:47+00:00
- **Authors**: Jomin Jose, Andrew J, Kumudha Raimond, Shweta Vincent
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Around 40 percent of accidents related to driving on highways in India occur due to the driver falling asleep behind the steering wheel. Several types of research are ongoing to detect driver drowsiness but they suffer from the complexity and cost of the models. In this paper, SleepyWheels a revolutionary method that uses a lightweight neural network in conjunction with facial landmark identification is proposed to identify driver fatigue in real time. SleepyWheels is successful in a wide range of test scenarios, including the lack of facial characteristics while covering the eye or mouth, the drivers varying skin tones, camera placements, and observational angles. It can work well when emulated to real time systems. SleepyWheels utilized EfficientNetV2 and a facial landmark detector for identifying drowsiness detection. The model is trained on a specially created dataset on driver sleepiness and it achieves an accuracy of 97 percent. The model is lightweight hence it can be further deployed as a mobile application for various platforms.



### VIINTER: View Interpolation with Implicit Neural Representations of Images
- **Arxiv ID**: http://arxiv.org/abs/2211.00722v1
- **DOI**: 10.1145/3550469.3555417
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00722v1)
- **Published**: 2022-11-01 19:51:30+00:00
- **Updated**: 2022-11-01 19:51:30+00:00
- **Authors**: Brandon Yushan Feng, Susmija Jabbireddy, Amitabh Varshney
- **Comment**: SIGGRAPH Asia 2022
- **Journal**: None
- **Summary**: We present VIINTER, a method for view interpolation by interpolating the implicit neural representation (INR) of the captured images. We leverage the learned code vector associated with each image and interpolate between these codes to achieve viewpoint transitions. We propose several techniques that significantly enhance the interpolation quality. VIINTER signifies a new way to achieve view interpolation without constructing 3D structure, estimating camera poses, or computing pixel correspondence. We validate the effectiveness of VIINTER on several multi-view scenes with different types of camera layout and scene composition. As the development of INR of images (as opposed to surface or volume) has centered around tasks like image fitting and super-resolution, with VIINTER, we show its capability for view interpolation and offer a promising outlook on using INR for image manipulation tasks.



### LARO: Learned Acquisition and Reconstruction Optimization to accelerate Quantitative Susceptibility Mapping
- **Arxiv ID**: http://arxiv.org/abs/2211.00725v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00725v1)
- **Published**: 2022-11-01 20:04:29+00:00
- **Updated**: 2022-11-01 20:04:29+00:00
- **Authors**: Jinwei Zhang, Pascal Spincemaille, Hang Zhang, Thanh D. Nguyen, Chao Li, Jiahao Li, Ilhami Kovanlikaya, Mert R. Sabuncu, Yi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative susceptibility mapping (QSM) involves acquisition and reconstruction of a series of images at multi-echo time points to estimate tissue field, which prolongs scan time and requires specific reconstruction technique. In this paper, we present our new framework, called Learned Acquisition and Reconstruction Optimization (LARO), which aims to accelerate the multi-echo gradient echo (mGRE) pulse sequence for QSM. Our approach involves optimizing a Cartesian multi-echo k-space sampling pattern with a deep reconstruction network. Next, this optimized sampling pattern was implemented in an mGRE sequence using Cartesian fan-beam k-space segmenting and ordering for prospective scans. Furthermore, we propose to insert a recurrent temporal feature fusion module into the reconstruction network to capture signal redundancies along echo time. Our ablation studies show that both the optimized sampling pattern and proposed reconstruction strategy help improve the quality of the multi-echo image reconstructions. Generalization experiments show that LARO is robust on the test data with new pathologies and different sequence parameters. Our code is available at https://github.com/Jinwei1209/LARO.git.



### State-of-the-art Models for Object Detection in Various Fields of Application
- **Arxiv ID**: http://arxiv.org/abs/2211.00733v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.00733v1)
- **Published**: 2022-11-01 20:25:32+00:00
- **Updated**: 2022-11-01 20:25:32+00:00
- **Authors**: Syed Ali John Naqvi, Syed Bazil Ali
- **Comment**: 4 pages, 5 tables
- **Journal**: None
- **Summary**: We present a list of datasets and their best models with the goal of advancing the state-of-the-art in object detection by placing the question of object recognition in the context of the two types of state-of-the-art methods: one-stage methods and two stage-methods. We provided an in-depth statistical analysis of the five top datasets in the light of recent developments in granulated Deep Learning models - COCO minival, COCO test, Pascal VOC 2007, ADE20K, and ImageNet. The datasets are handpicked after closely comparing them with the rest in terms of diversity, quality of data, minimal bias, labeling quality etc. More importantly, our work extends to provide the best combination of these datasets with the emerging models in the last two years. It lists the top models and their optimal use cases for each of the respective datasets. We have provided a comprehensive overview of a variety of both generic and specific object detection models, enlisting comparative results like inference time and average precision of box (AP) fixed at different Intersection Over Union (IoUs) and for different sized objects. The qualitative and quantitative analysis will allow experts to achieve new performance records using the best combination of datasets and models.



### Self-supervised Physics-based Denoising for Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2211.00745v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00745v1)
- **Published**: 2022-11-01 20:58:50+00:00
- **Updated**: 2022-11-01 20:58:50+00:00
- **Authors**: Elvira Zainulina, Alexey Chernyavskiy, Dmitry V. Dylov
- **Comment**: 13 pages, 12 figures. Under review
- **Journal**: None
- **Summary**: Computed Tomography (CT) imposes risk on the patients due to its inherent X-ray radiation, stimulating the development of low-dose CT (LDCT) imaging methods. Lowering the radiation dose reduces the health risks but leads to noisier measurements, which decreases the tissue contrast and causes artifacts in CT images. Ultimately, these issues could affect the perception of medical personnel and could cause misdiagnosis. Modern deep learning noise suppression methods alleviate the challenge but require low-noise-high-noise CT image pairs for training, rarely collected in regular clinical workflows. In this work, we introduce a new self-supervised approach for CT denoising Noise2NoiseTD-ANM that can be trained without the high-dose CT projection ground truth images. Unlike previously proposed self-supervised techniques, the introduced method exploits the connections between the adjacent projections and the actual model of CT noise distribution. Such a combination allows for interpretable no-reference denoising using nothing but the original noisy LDCT projections. Our experiments with LDCT data demonstrate that the proposed method reaches the level of the fully supervised models, sometimes superseding them, easily generalizes to various noise levels, and outperforms state-of-the-art self-supervised denoising algorithms.



### 3DMODT: Attention-Guided Affinities for Joint Detection & Tracking in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2211.00746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.00746v1)
- **Published**: 2022-11-01 20:59:38+00:00
- **Updated**: 2022-11-01 20:59:38+00:00
- **Authors**: Jyoti Kini, Ajmal Mian, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for joint detection and tracking of multiple objects in 3D point clouds, a task conventionally treated as a two-step process comprising object detection followed by data association. Our method embeds both steps into a single end-to-end trainable network eliminating the dependency on external object detectors. Our model exploits temporal information employing multiple frames to detect objects and track them in a single network, thereby making it a utilitarian formulation for real-world scenarios. Computing affinity matrix by employing features similarity across consecutive point cloud scans forms an integral part of visual tracking. We propose an attention-based refinement module to refine the affinity matrix by suppressing erroneous correspondences. The module is designed to capture the global context in affinity matrix by employing self-attention within each affinity matrix and cross-attention across a pair of affinity matrices. Unlike competing approaches, our network does not require complex post-processing algorithms, and processes raw LiDAR frames to directly output tracking results. We demonstrate the effectiveness of our method on the three tracking benchmarks: JRDB, Waymo, and KITTI. Experimental evaluations indicate the ability of our model to generalize well across datasets.



### ViT-DeiT: An Ensemble Model for Breast Cancer Histopathological Images Classification
- **Arxiv ID**: http://arxiv.org/abs/2211.00749v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00749v1)
- **Published**: 2022-11-01 21:10:00+00:00
- **Updated**: 2022-11-01 21:10:00+00:00
- **Authors**: Amira Alotaibi, Tarik Alafif, Faris Alkhilaiwi, Yasser Alatawi, Hassan Althobaiti, Abdulmajeed Alrefaei, Yousef M Hawsawi, Tin Nguyen
- **Comment**: 7 pages, 10 figures, 7 tables
- **Journal**: None
- **Summary**: Breast cancer is the most common cancer in the world and the second most common type of cancer that causes death in women. The timely and accurate diagnosis of breast cancer using histopathological images is crucial for patient care and treatment. Pathologists can make more accurate diagnoses with the help of a novel approach based on image processing. This approach is an ensemble model of two types of pre-trained vision transformer models, namely, Vision Transformer and Data-Efficient Image Transformer. The proposed ensemble model classifies breast cancer histopathology images into eight classes, four of which are categorized as benign, whereas the others are categorized as malignant. A public dataset was used to evaluate the proposed model. The experimental results showed 98.17% accuracy, 98.18% precision, 98.08% recall, and a 98.12% F1 score.



### Automatic Quantitative Analysis of Brain Organoids via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.00750v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2211.00750v1)
- **Published**: 2022-11-01 21:10:28+00:00
- **Updated**: 2022-11-01 21:10:28+00:00
- **Authors**: Jingli Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in brain organoid technology are exciting new ways, which have the potential to change the way how doctors and researchers understand and treat cerebral diseases. Despite the remarkable use of brain organoids derived from human stem cells in new drug testing, disease modeling, and scientific research, it is still heavily time-consuming work to observe and analyze the internal structure, cells, and neural inside the organoid by humans, specifically no standard quantitative analysis method combined growing AI technology for brain organoid. In this paper, an automated computer-assisted analysis method is proposed for brain organoid slice channels tagged with different fluorescent. We applied the method on two channels of two group microscopy images and the experiment result shows an obvious difference between Wild Type and Mutant Type cerebral organoids.



### Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality
- **Arxiv ID**: http://arxiv.org/abs/2211.00768v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00768v4)
- **Published**: 2022-11-01 22:16:58+00:00
- **Updated**: 2022-12-03 08:12:08+00:00
- **Authors**: Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, Kyle Mahowald
- **Comment**: Accepted at EMNLP 2022. We release our annotation and code at
  https://github.com/ajd12342/why-winoground-hard . 15 pages, 3 figures
- **Journal**: None
- **Summary**: Recent visuolinguistic pre-trained models show promising progress on various end tasks such as image retrieval and video captioning. Yet, they fail miserably on the recently proposed Winoground dataset, which challenges models to match paired images and English captions, with items constructed to overlap lexically but differ in meaning (e.g., "there is a mug in some grass" vs. "there is some grass in a mug"). By annotating the dataset using new fine-grained tags, we show that solving the Winoground task requires not just compositional language understanding, but a host of other abilities like commonsense reasoning or locating small, out-of-focus objects in low-resolution images. In this paper, we identify the dataset's main challenges through a suite of experiments on related tasks (probing task, image retrieval task), data augmentation, and manual inspection of the dataset. Our analysis suggests that a main challenge in visuolinguistic models may lie in fusing visual and textual representations, rather than in compositional language understanding. We release our annotation and code at https://github.com/ajd12342/why-winoground-hard .



### Predicting air quality via multimodal AI and satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2211.00780v2
- **DOI**: 10.1016/j.rse.2023.113609
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00780v2)
- **Published**: 2022-11-01 22:56:15+00:00
- **Updated**: 2023-05-05 16:00:41+00:00
- **Authors**: Andrew Rowley, Oktay Karaku
- **Comment**: 14 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Climate change may be classified as the most important environmental problem that the Earth is currently facing, and affects all living species on Earth. Given that air-quality monitoring stations are typically ground-based their abilities to detect pollutant distributions are often restricted to wide areas. Satellites however have the potential for studying the atmosphere at large; the European Space Agency (ESA) Copernicus project satellite, "Sentinel-5P" is a newly launched satellite capable of measuring a variety of pollutant information with publicly available data outputs. This paper seeks to create a multi-modal machine learning model for predicting air-quality metrics where monitoring stations do not exist. The inputs of this model will include a fusion of ground measurements and satellite data with the goal of highlighting pollutant distribution and motivating change in societal and industrial behaviors. A new dataset of European pollution monitoring station measurements is created with features including $\textit{altitude, population, etc.}$ from the ESA Copernicus project. This dataset is used to train a multi-modal ML model, Air Quality Network (AQNet) capable of fusing these various types of data sources to output predictions of various pollutants. These predictions are then aggregated to create an "air-quality index" that could be used to compare air quality over different regions. Three pollutants, NO$_2$, O$_3$, and PM$_{10}$, are predicted successfully by AQNet and the network was found to be useful compared to a model only using satellite imagery. It was also found that the addition of supporting data improves predictions. When testing the developed AQNet on out-of-sample data of the UK and Ireland, we obtain satisfactory estimates though on average pollution metrics were roughly overestimated by around 20\%.




# Arxiv Papers in cs.CV on 2022-11-03
### Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments
- **Arxiv ID**: http://arxiv.org/abs/2211.01527v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.SY, eess.SY, I.2.8; I.2.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2211.01527v2)
- **Published**: 2022-11-03 00:03:14+00:00
- **Updated**: 2023-05-22 19:53:33+00:00
- **Authors**: J. Brian Burns, Aravind Sundaresan, Pedro Sequeira, Vidyasagar Sadhu
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We present an approach for autonomous sensor control for information gathering under partially observable, dynamic and sparsely sampled environments that maximizes information about entities present in that space. We describe our approach for the task of Radio-Frequency (RF) spectrum monitoring, where the goal is to search for and track unknown, dynamic signals in the environment. To this end, we extend the Deep Anticipatory Network (DAN) Reinforcement Learning (RL) framework by (1) improving exploration in sparse, non-stationary environments using a novel information gain reward, and (2) scaling up the control space and enabling the monitoring of complex, dynamic activity patterns using hybrid convolutional-recurrent neural layers. We also extend this problem to situations in which sampling from the intended RF spectrum/field is limited and propose a model-based version of the original RL algorithm that fine-tunes the controller via a model that is iteratively improved from the limited field sampling. Results in simulated RF environments of differing complexity show that our system outperforms the standard DAN architecture and is more flexible and robust than baseline expert-designed agents. We also show that it is adaptable to non-stationary emission environments.



### Haven't I Seen You Before? Assessing Identity Leakage in Synthetic Irises
- **Arxiv ID**: http://arxiv.org/abs/2211.05629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05629v1)
- **Published**: 2022-11-03 00:34:47+00:00
- **Updated**: 2022-11-03 00:34:47+00:00
- **Authors**: Patrick Tinsley, Adam Czajka, Patrick Flynn
- **Comment**: Presented at IJCB 2022 Special Session on Synthetic Data in
  Biometrics
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have proven to be a preferred method of synthesizing fake images of objects, such as faces, animals, and automobiles. It is not surprising these models can also generate ISO-compliant, yet synthetic iris images, which can be used to augment training data for iris matchers and liveness detectors. In this work, we trained one of the most recent GAN models (StyleGAN3) to generate fake iris images with two primary goals: (i) to understand the GAN's ability to produce "never-before-seen" irises, and (ii) to investigate the phenomenon of identity leakage as a function of the GAN's training time. Previous work has shown that personal biometric data can inadvertently flow from training data into synthetic samples, raising a privacy concern for subjects who accidentally appear in the training dataset. This paper presents analysis for three different iris matchers at varying points in the GAN training process to diagnose where and when authentic training samples are in jeopardy of leaking through the generative process. Our results show that while most synthetic samples do not show signs of identity leakage, a handful of generated samples match authentic (training) samples nearly perfectly, with consensus across all matchers. In order to prioritize privacy, security, and trust in the machine learning model development process, the research community must strike a delicate balance between the benefits of using synthetic data and the corresponding threats against privacy from potential identity leakage.



### Ground Plane Matters: Picking Up Ground Plane Prior in Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.01556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01556v1)
- **Published**: 2022-11-03 02:21:35+00:00
- **Updated**: 2022-11-03 02:21:35+00:00
- **Authors**: Fan Yang, Xinhao Xu, Hui Chen, Yuchen Guo, Jungong Han, Kai Ni, Guiguang Ding
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: The ground plane prior is a very informative geometry clue in monocular 3D object detection (M3OD). However, it has been neglected by most mainstream methods. In this paper, we identify two key factors that limit the applicability of ground plane prior: the projection point localization issue and the ground plane tilt issue. To pick up the ground plane prior for M3OD, we propose a Ground Plane Enhanced Network (GPENet) which resolves both issues at one go. For the projection point localization issue, instead of using the bottom vertices or bottom center of the 3D bounding box (BBox), we leverage the object's ground contact points, which are explicit pixels in the image and easy for the neural network to detect. For the ground plane tilt problem, our GPENet estimates the horizon line in the image and derives a novel mathematical expression to accurately estimate the ground plane equation. An unsupervised vertical edge mining algorithm is also proposed to address the occlusion of the horizon line. Furthermore, we design a novel 3D bounding box deduction method based on a dynamic back projection algorithm, which could take advantage of the accurate contact points and the ground plane equation. Additionally, using only M3OD labels, contact point and horizon line pseudo labels can be easily generated with NO extra data collection and label annotation cost. Extensive experiments on the popular KITTI benchmark show that our GPENet can outperform other methods and achieve state-of-the-art performance, well demonstrating the effectiveness and the superiority of the proposed approach. Moreover, our GPENet works better than other methods in cross-dataset evaluation on the nuScenes dataset. Our code and models will be published.



### NaRPA: Navigation and Rendering Pipeline for Astronautics
- **Arxiv ID**: http://arxiv.org/abs/2211.01566v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.01566v1)
- **Published**: 2022-11-03 03:07:21+00:00
- **Updated**: 2022-11-03 03:07:21+00:00
- **Authors**: Roshan Thomas Eapen, Ramchander Rao Bhaskara, Manoranjan Majji
- **Comment**: 49 pages, 22 figures
- **Journal**: None
- **Summary**: This paper presents Navigation and Rendering Pipeline for Astronautics (NaRPA) - a novel ray-tracing-based computer graphics engine to model and simulate light transport for space-borne imaging. NaRPA incorporates lighting models with attention to atmospheric and shading effects for the synthesis of space-to-space and ground-to-space virtual observations. In addition to image rendering, the engine also possesses point cloud, depth, and contour map generation capabilities to simulate passive and active vision-based sensors and to facilitate the designing, testing, or verification of visual navigation algorithms. Physically based rendering capabilities of NaRPA and the efficacy of the proposed rendering algorithm are demonstrated using applications in representative space-based environments. A key demonstration includes NaRPA as a tool for generating stereo imagery and application in 3D coordinate estimation using triangulation. Another prominent application of NaRPA includes a novel differentiable rendering approach for image-based attitude estimation is proposed to highlight the efficacy of the NaRPA engine for simulating vision-based navigation and guidance operations.



### Galaxy Image Deconvolution for Weak Gravitational Lensing with Unrolled Plug-and-Play ADMM
- **Arxiv ID**: http://arxiv.org/abs/2211.01567v3
- **DOI**: 10.1093/mnrasl/slad032
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01567v3)
- **Published**: 2022-11-03 03:12:52+00:00
- **Updated**: 2023-03-13 07:05:21+00:00
- **Authors**: Tianao Li, Emma Alexander
- **Comment**: None
- **Journal**: None
- **Summary**: Removing optical and atmospheric blur from galaxy images significantly improves galaxy shape measurements for weak gravitational lensing and galaxy evolution studies. This ill-posed linear inverse problem is usually solved with deconvolution algorithms enhanced by regularisation priors or deep learning. We introduce a so-called "physics-informed deep learning" approach to the Point Spread Function (PSF) deconvolution problem in galaxy surveys. We apply algorithm unrolling and the Plug-and-Play technique to the Alternating Direction Method of Multipliers (ADMM), in which a neural network learns appropriate hyperparameters and denoising priors from simulated galaxy images. We characterise the time-performance trade-off of several methods for galaxies of differing brightness levels as well as our method's robustness to systematic PSF errors and network ablations. We show an improvement in reduced shear ellipticity error of 38.6% (SNR=20)/45.0% (SNR=200) compared to classic methods and 7.4% (SNR=20)/33.2% (SNR=200) compared to modern methods.



### Data-free Defense of Black Box Models Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2211.01579v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01579v2)
- **Published**: 2022-11-03 04:19:27+00:00
- **Updated**: 2023-06-28 21:38:48+00:00
- **Authors**: Gaurav Kumar Nayak, Inder Khatri, Ruchit Rawal, Anirban Chakraborty
- **Comment**: Neural Networks Journal (Under Review)
- **Journal**: None
- **Summary**: Several companies often safeguard their trained deep models (i.e., details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as black boxes through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose 'wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our 'wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a 'regenerator' network with the objective of retrieving the coefficients such that the reconstructed image yields similar to original predictions on the surrogate model. At test time, WNR combined with trained regenerator network is prepended to the black box network, resulting in a high boost in adversarial accuracy. Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01% on state-of-the-art Auto Attack compared to baseline, even when the attacker uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box architecture (Alexnet) with same model stealing strategy as defender. The code is available at https://github.com/vcl-iisc/data-free-black-box-defense



### PolyBuilding: Polygon Transformer for End-to-End Building Extraction
- **Arxiv ID**: http://arxiv.org/abs/2211.01589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01589v1)
- **Published**: 2022-11-03 04:53:17+00:00
- **Updated**: 2022-11-03 04:53:17+00:00
- **Authors**: Yuan Hu, Zhibin Wang, Zhou Huang, Yu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We present PolyBuilding, a fully end-to-end polygon Transformer for building extraction. PolyBuilding direct predicts vector representation of buildings from remote sensing images. It builds upon an encoder-decoder transformer architecture and simultaneously outputs building bounding boxes and polygons. Given a set of polygon queries, the model learns the relations among them and encodes context information from the image to predict the final set of building polygons with fixed vertex numbers. Corner classification is performed to distinguish the building corners from the sampled points, which can be used to remove redundant vertices along the building walls during inference. A 1-d non-maximum suppression (NMS) is further applied to reduce vertex redundancy near the building corners. With the refinement operations, polygons with regular shapes and low complexity can be effectively obtained. Comprehensive experiments are conducted on the CrowdAI dataset. Quantitative and qualitative results show that our approach outperforms prior polygonal building extraction methods by a large margin. It also achieves a new state-of-the-art in terms of pixel-level coverage, instance-level precision and recall, and geometry-level properties (including contour regularity and polygon complexity).



### Robust Few-shot Learning Without Using any Adversarial Samples
- **Arxiv ID**: http://arxiv.org/abs/2211.01598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.01598v1)
- **Published**: 2022-11-03 05:58:26+00:00
- **Updated**: 2022-11-03 05:58:26+00:00
- **Authors**: Gaurav Kumar Nayak, Ruchit Rawal, Inder Khatri, Anirban Chakraborty
- **Comment**: TNNLS Submission (Under Review)
- **Journal**: None
- **Summary**: The high cost of acquiring and annotating samples has made the `few-shot' learning problem of prime importance. Existing works mainly focus on improving performance on clean data and overlook robustness concerns on the data perturbed with adversarial noise. Recently, a few efforts have been made to combine the few-shot problem with the robustness objective using sophisticated Meta-Learning techniques. These methods rely on the generation of adversarial samples in every episode of training, which further adds a computational burden. To avoid such time-consuming and complicated procedures, we propose a simple but effective alternative that does not require any adversarial samples. Inspired by the cognitive decision-making process in humans, we enforce high-level feature matching between the base class data and their corresponding low-frequency samples in the pretraining stage via self distillation. The model is then fine-tuned on the samples of novel classes where we additionally improve the discriminability of low-frequency query set features via cosine similarity. On a 1-shot setting of the CIFAR-FS dataset, our method yields a massive improvement of $60.55\%$ & $62.05\%$ in adversarial accuracy on the PGD and state-of-the-art Auto Attack, respectively, with a minor drop in clean accuracy compared to the baseline. Moreover, our method only takes $1.69\times$ of the standard training time while being $\approx$ $5\times$ faster than state-of-the-art adversarial meta-learning methods. The code is available at https://github.com/vcl-iisc/robust-few-shot-learning.



### nerf2nerf: Pairwise Registration of Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2211.01600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2211.01600v1)
- **Published**: 2022-11-03 06:04:59+00:00
- **Updated**: 2022-11-03 06:04:59+00:00
- **Authors**: Lily Goli, Daniel Rebain, Sara Sabour, Animesh Garg, Andrea Tagliasacchi
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a technique for pairwise registration of neural fields that extends classical optimization-based local registration (i.e. ICP) to operate on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained from collections of calibrated images. NeRF does not decompose illumination and color, so to make registration invariant to illumination, we introduce the concept of a ''surface field'' -- a field distilled from a pre-trained NeRF model that measures the likelihood of a point being on the surface of an object. We then cast nerf2nerf registration as a robust optimization that iteratively seeks a rigid transformation that aligns the surface fields of the two scenes. We evaluate the effectiveness of our technique by introducing a dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative evaluations and comparisons to classical registration techniques, while our real scenes demonstrate the validity of our technique in real-world scenarios. Additional results available at: https://nerf2nerf.github.io



### Image-based Early Detection System for Wildfires
- **Arxiv ID**: http://arxiv.org/abs/2211.01629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.01629v1)
- **Published**: 2022-11-03 07:38:30+00:00
- **Updated**: 2022-11-03 07:38:30+00:00
- **Authors**: Omkar Ranadive, Jisu Kim, Serin Lee, Youngseo Cha, Heechan Park, Minkook Cho, Young K. Hwang
- **Comment**: Published in Tackling Climate Change with Machine Learning workshop,
  Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS
  2022)
- **Journal**: None
- **Summary**: Wildfires are a disastrous phenomenon which cause damage to land, loss of property, air pollution, and even loss of human life. Due to the warmer and drier conditions created by climate change, more severe and uncontrollable wildfires are expected to occur in the coming years. This could lead to a global wildfire crisis and have dire consequences on our planet. Hence, it has become imperative to use technology to help prevent the spread of wildfires. One way to prevent the spread of wildfires before they become too large is to perform early detection i.e, detecting the smoke before the actual fire starts. In this paper, we present our Wildfire Detection and Alert System which use machine learning to detect wildfire smoke with a high degree of accuracy and can send immediate alerts to users. Our technology is currently being used in the USA to monitor data coming in from hundreds of cameras daily. We show that our system has a high true detection rate and a low false detection rate. Our performance evaluation study also shows that on an average our system detects wildfire smoke faster than an actual person.



### $\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing
- **Arxiv ID**: http://arxiv.org/abs/2211.01631v1
- **DOI**: 10.1109/TPAMI.2022.3225418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01631v1)
- **Published**: 2022-11-03 07:39:10+00:00
- **Updated**: 2022-11-03 07:39:10+00:00
- **Authors**: Xinzhe Luo, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a generic probabilistic framework for estimating the statistical dependency and finding the anatomical correspondences among an arbitrary number of medical images. The method builds on a novel formulation of the $N$-dimensional joint intensity distribution by representing the common anatomy as latent variables and estimating the appearance model with nonparametric estimators. Through connection to maximum likelihood and the expectation-maximization algorithm, an information\hyp{}theoretic metric called $\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg are induced, allowing groupwise registration of the $N$ observed images with computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally extends for a weakly-supervised scenario where anatomical labels of certain images are provided. This leads to a combined\hyp{}computing framework implemented with deep learning, which performs registration and segmentation simultaneously and collaboratively in an end-to-end fashion. Extensive experiments were conducted to demonstrate the versatility and applicability of our model, including multimodal groupwise registration, motion correction for dynamic contrast enhanced magnetic resonance images, and deep combined computing for multimodal medical images. Results show the superiority of our method in various applications in terms of both accuracy and efficiency, highlighting the advantage of the proposed representation of the imaging process.



### P4P: Conflict-Aware Motion Prediction for Planning in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2211.01634v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01634v1)
- **Published**: 2022-11-03 07:51:40+00:00
- **Updated**: 2022-11-03 07:51:40+00:00
- **Authors**: Qiao Sun, Xin Huang, Brian C. Williams, Hang Zhao
- **Comment**: 7 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Motion prediction is crucial in enabling safe motion planning for autonomous vehicles in interactive scenarios. It allows the planner to identify potential conflicts with other traffic agents and generate safe plans. Existing motion predictors often focus on reducing prediction errors, yet it remains an open question on how well they help identify the conflicts for the planner. In this paper, we evaluate state-of-the-art predictors through novel conflict-related metrics, such as the success rate of identifying conflicts. Surprisingly, the predictors suffer from a low success rate and thus lead to a large percentage of collisions when we test the prediction-planning system in an interactive simulator. To fill the gap, we propose a simple but effective alternative that combines a physics-based trajectory generator and a learning-based relation predictor to identify conflicts and infer conflict relations. We demonstrate that our predictor, P4P, achieves superior performance over existing learning-based predictors in realistic interactive driving scenarios from Waymo Open Motion Dataset.



### Unified Multi-View Orthonormal Non-Negative Graph Based Clustering Framework
- **Arxiv ID**: http://arxiv.org/abs/2211.02883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02883v2)
- **Published**: 2022-11-03 08:18:27+00:00
- **Updated**: 2022-12-01 05:05:55+00:00
- **Authors**: Liangchen Liu, Qiuhong Ke, Chaojie Li, Feiping Nie, Yingying Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral clustering is an effective methodology for unsupervised learning. Most traditional spectral clustering algorithms involve a separate two-step procedure and apply the transformed new representations for the final clustering results. Recently, much progress has been made to utilize the non-negative feature property in real-world data and to jointly learn the representation and clustering results. However, to our knowledge, no previous work considers a unified model that incorporates the important multi-view information with those properties, which severely limits the performance of existing methods. In this paper, we formulate a novel clustering model, which exploits the non-negative feature property and, more importantly, incorporates the multi-view information into a unified joint learning framework: the unified multi-view orthonormal non-negative graph based clustering framework (Umv-ONGC). Then, we derive an effective three-stage iterative solution for the proposed model and provide analytic solutions for the three sub-problems from the three stages. We also explore, for the first time, the multi-model non-negative graph-based approach to clustering data based on deep features. Extensive experiments on three benchmark data sets demonstrate the effectiveness of the proposed method.



### Temporal Consistency Learning of inter-frames for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2211.01639v1
- **DOI**: 10.1109/tcsvt.2022.3214538
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01639v1)
- **Published**: 2022-11-03 08:23:57+00:00
- **Updated**: 2022-11-03 08:23:57+00:00
- **Authors**: Meiqin Liu, Shuo Jin, Chao Yao, Chunyu Lin, Yao Zhao
- **Comment**: Accepted by IEEE Trans. Circuits Syst. Video Technol
- **Journal**: None
- **Summary**: Video super-resolution (VSR) is a task that aims to reconstruct high-resolution (HR) frames from the low-resolution (LR) reference frame and multiple neighboring frames. The vital operation is to utilize the relative misaligned frames for the current frame reconstruction and preserve the consistency of the results. Existing methods generally explore information propagation and frame alignment to improve the performance of VSR. However, few studies focus on the temporal consistency of inter-frames. In this paper, we propose a Temporal Consistency learning Network (TCNet) for VSR in an end-to-end manner, to enhance the consistency of the reconstructed videos. A spatio-temporal stability module is designed to learn the self-alignment from inter-frames. Especially, the correlative matching is employed to exploit the spatial dependency from each frame to maintain structural stability. Moreover, a self-attention mechanism is utilized to learn the temporal correspondence to implement an adaptive warping operation for temporal consistency among multi-frames. Besides, a hybrid recurrent architecture is designed to leverage short-term and long-term information. We further present a progressive fusion module to perform a multistage fusion of spatio-temporal features. And the final reconstructed frames are refined by these fused features. Objective and subjective results of various experiments demonstrate that TCNet has superior performance on different benchmark datasets, compared to several state-of-the-art methods.



### StereoPose: Category-Level 6D Transparent Object Pose Estimation from Stereo Images via Back-View NOCS
- **Arxiv ID**: http://arxiv.org/abs/2211.01644v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.01644v1)
- **Published**: 2022-11-03 08:36:09+00:00
- **Updated**: 2022-11-03 08:36:09+00:00
- **Authors**: Kai Chen, Stephen James, Congying Sui, Yun-Hui Liu, Pieter Abbeel, Qi Dou
- **Comment**: 7 pages, 6 figures, Project homepage:
  https://appsrv.cse.cuhk.edu.hk/~kaichen/stereopose.html
- **Journal**: None
- **Summary**: Most existing methods for category-level pose estimation rely on object point clouds. However, when considering transparent objects, depth cameras are usually not able to capture meaningful data, resulting in point clouds with severe artifacts. Without a high-quality point cloud, existing methods are not applicable to challenging transparent objects. To tackle this problem, we present StereoPose, a novel stereo image framework for category-level object pose estimation, ideally suited for transparent objects. For a robust estimation from pure stereo images, we develop a pipeline that decouples category-level pose estimation into object size estimation, initial pose estimation, and pose refinement. StereoPose then estimates object pose based on representation in the normalized object coordinate space~(NOCS). To address the issue of image content aliasing, we further define a back-view NOCS map for the transparent object. The back-view NOCS aims to reduce the network learning ambiguity caused by content aliasing, and leverage informative cues on the back of the transparent object for more accurate pose estimation. To further improve the performance of the stereo framework, StereoPose is equipped with a parallax attention module for stereo feature fusion and an epipolar loss for improving the stereo-view consistency of network predictions. Extensive experiments on the public TOD dataset demonstrate the superiority of the proposed StereoPose framework for category-level 6D transparent object pose estimation.



### PointSee: Image Enhances Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2211.01664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01664v1)
- **Published**: 2022-11-03 09:12:48+00:00
- **Updated**: 2022-11-03 09:12:48+00:00
- **Authors**: Lipeng Gu, Xuefeng Yan, Peng Cui, Lina Gong, Haoran Xie, Fu Lee Wang, Jin Qin, Mingqiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: There is a trend to fuse multi-modal information for 3D object detection (3OD). However, the challenging problems of low lightweightness, poor flexibility of plug-and-play, and inaccurate alignment of features are still not well-solved, when designing multi-modal fusion newtorks. We propose PointSee, a lightweight, flexible and effective multi-modal fusion solution to facilitate various 3OD networks by semantic feature enhancement of LiDAR point clouds assembled with scene images. Beyond the existing wisdom of 3OD, PointSee consists of a hidden module (HM) and a seen module (SM): HM decorates LiDAR point clouds using 2D image information in an offline fusion manner, leading to minimal or even no adaptations of existing 3OD networks; SM further enriches the LiDAR point clouds by acquiring point-wise representative semantic features, leading to enhanced performance of existing 3OD networks. Besides the new architecture of PointSee, we propose a simple yet efficient training strategy, to ease the potential inaccurate regressions of 2D object detection networks. Extensive experiments on the popular outdoor/indoor benchmarks show numerical improvements of our PointSee over twenty-two state-of-the-arts.



### Active CT Reconstruction with a Learned Sampling Policy
- **Arxiv ID**: http://arxiv.org/abs/2211.01670v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01670v1)
- **Published**: 2022-11-03 09:27:33+00:00
- **Updated**: 2022-11-03 09:27:33+00:00
- **Authors**: Ce Wang, Kun Shang, Haimiao Zhang, Shang Zhao, Dong Liang, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) is a widely-used imaging technology that assists clinical decision-making with high-quality human body representations. To reduce the radiation dose posed by CT, sparse-view and limited-angle CT are developed with preserved image quality. However, these methods are still stuck with a fixed or uniform sampling strategy, which inhibits the possibility of acquiring a better image with an even reduced dose. In this paper, we explore this possibility via learning an active sampling policy that optimizes the sampling positions for patient-specific, high-quality reconstruction. To this end, we design an \textit{intelligent agent} for active recommendation of sampling positions based on on-the-fly reconstruction with obtained sinograms in a progressive fashion. With such a design, we achieve better performances on the NIH-AAPM dataset over popular uniform sampling, especially when the number of views is small. Finally, such a design also enables RoI-aware reconstruction with improved reconstruction quality within regions of interest (RoI's) that are clinically important. Experiments on the VerSe dataset demonstrate this ability of our sampling policy, which is difficult to achieve based on uniform sampling.



### Visually Adversarial Attacks and Defenses in the Physical World: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2211.01671v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01671v5)
- **Published**: 2022-11-03 09:28:45+00:00
- **Updated**: 2023-07-13 14:18:09+00:00
- **Authors**: Xingxing Wei, Bangzheng Pu, Jiefan Lu, Baoyuan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Although Deep Neural Networks (DNNs) have been widely applied in various real-world scenarios, they are vulnerable to adversarial examples. The current adversarial attacks in computer vision can be divided into digital attacks and physical attacks according to their different attack forms. Compared with digital attacks, which generate perturbations in the digital pixels, physical attacks are more practical in the real world. Owing to the serious security problem caused by physically adversarial examples, many works have been proposed to evaluate the physically adversarial robustness of DNNs in the past years. In this paper, we summarize a survey versus the current physically adversarial attacks and physically adversarial defenses in computer vision. To establish a taxonomy, we organize the current physical attacks from attack tasks, attack forms, and attack methods, respectively. Thus, readers can have a systematic knowledge of this topic from different aspects. For the physical defenses, we establish the taxonomy from pre-processing, in-processing, and post-processing for the DNN models to achieve full coverage of the adversarial defenses. Based on the above survey, we finally discuss the challenges of this research field and further outlook on the future direction.



### Exploring Explainability Methods for Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.01770v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01770v1)
- **Published**: 2022-11-03 12:50:46+00:00
- **Updated**: 2022-11-03 12:50:46+00:00
- **Authors**: Harsh Patel, Shivam Sahni
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing use of deep learning methods, particularly graph neural networks, which encode intricate interconnectedness information, for a variety of real tasks, there is a necessity for explainability in such settings. In this paper, we demonstrate the applicability of popular explainability approaches on Graph Attention Networks (GAT) for a graph-based super-pixel image classification task. We assess the qualitative and quantitative performance of these techniques on three different datasets and describe our findings. The results shed a fresh light on the notion of explainability in GNNs, particularly GATs.



### Evaluating a Synthetic Image Dataset Generated with Stable Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2211.01777v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.01777v2)
- **Published**: 2022-11-03 13:02:08+00:00
- **Updated**: 2022-11-04 09:28:00+00:00
- **Authors**: Andreas St√∂ckl
- **Comment**: None
- **Journal**: None
- **Summary**: We generate synthetic images with the "Stable Diffusion" image generation model using the Wordnet taxonomy and the definitions of concepts it contains. This synthetic image database can be used as training data for data augmentation in machine learning applications, and it is used to investigate the capabilities of the Stable Diffusion model.   Analyses show that Stable Diffusion can produce correct images for a large number of concepts, but also a large variety of different representations. The results show differences depending on the test concepts considered and problems with very specific concepts. These evaluations were performed using a vision transformer model for image classification.



### Progressive Transformation Learning for Leveraging Virtual Images in Training
- **Arxiv ID**: http://arxiv.org/abs/2211.01778v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01778v2)
- **Published**: 2022-11-03 13:04:15+00:00
- **Updated**: 2023-03-27 19:21:27+00:00
- **Authors**: Yi-Ting Shen, Hyungtae Lee, Heesung Kwon, Shuvra Shikhar Bhattacharyya
- **Comment**: CVPR 2023 (Selected as Highlight)
- **Journal**: None
- **Summary**: To effectively interrogate UAV-based images for detecting objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transformation Learning (PTL), which gradually augments a training dataset by adding transformed virtual images with enhanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists between real and virtual images. To deal with the domain gap, PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of virtual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while removing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demonstrate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian distribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily computed. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime.



### Exploring explicit coarse-grained structure in artificial neural networks
- **Arxiv ID**: http://arxiv.org/abs/2211.01779v2
- **DOI**: 10.1088/0256-307X/40/2/020501
- **Categories**: **cs.LG**, cond-mat.stat-mech, cond-mat.str-el, cs.CV, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2211.01779v2)
- **Published**: 2022-11-03 13:06:37+00:00
- **Updated**: 2022-11-04 07:33:57+00:00
- **Authors**: Xi-Ci Yang, Z. Y. Xie, Xiao-Tao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to employ the hierarchical coarse-grained structure in the artificial neural networks explicitly to improve the interpretability without degrading performance. The idea has been applied in two situations. One is a neural network called TaylorNet, which aims to approximate the general mapping from input data to output result in terms of Taylor series directly, without resorting to any magic nonlinear activations. The other is a new setup for data distillation, which can perform multi-level abstraction of the input dataset and generate new data that possesses the relevant features of the original dataset and can be used as references for classification. In both cases, the coarse-grained structure plays an important role in simplifying the network and improving both the interpretability and efficiency. The validity has been demonstrated on MNIST and CIFAR-10 datasets. Further improvement and some open questions related are also discussed.



### Video Event Extraction via Tracking Visual States of Arguments
- **Arxiv ID**: http://arxiv.org/abs/2211.01781v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2211.01781v2)
- **Published**: 2022-11-03 13:12:49+00:00
- **Updated**: 2022-11-05 15:27:43+00:00
- **Authors**: Guang Yang, Manling Li, Jiajie Zhang, Xudong Lin, Shih-Fu Chang, Heng Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Video event extraction aims to detect salient events from a video and identify the arguments for each event as well as their semantic roles. Existing methods focus on capturing the overall visual scene of each frame, ignoring fine-grained argument-level information. Inspired by the definition of events as changes of states, we propose a novel framework to detect video events by tracking the changes in the visual states of all involved arguments, which are expected to provide the most informative evidence for the extraction of video events. In order to capture the visual state changes of arguments, we decompose them into changes in pixels within objects, displacements of objects, and interactions among multiple arguments. We further propose Object State Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to encode and track these changes respectively. Experiments on various video event extraction tasks demonstrate significant improvements compared to state-of-the-art models. In particular, on verb classification, we achieve 3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation Recognition.



### Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.01783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01783v1)
- **Published**: 2022-11-03 13:17:53+00:00
- **Updated**: 2022-11-03 13:17:53+00:00
- **Authors**: Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2206.02846
- **Journal**: None
- **Summary**: There is limited understanding of the information captured by deep spatiotemporal models in their intermediate representations. For example, while evidence suggests that action recognition algorithms are heavily influenced by visual appearance in single frames, no quantitative methodology exists for evaluating such static bias in the latent representation compared to bias toward dynamics. We tackle this challenge by proposing an approach for quantifying the static and dynamic biases of any spatiotemporal model, and apply our approach to three tasks, action recognition, automatic video object segmentation (AVOS) and video instance segmentation (VIS). Our key findings are: (i) Most examined models are biased toward static information. (ii) Some datasets that are assumed to be biased toward dynamics are actually biased toward static information. (iii) Individual channels in an architecture can be biased toward static, dynamic or a combination of the two. (iv) Most models converge to their culminating biases in the first half of training. We then explore how these biases affect performance on dynamically biased datasets. For action recognition, we propose StaticDropout, a semantically guided dropout that debiases a model from static information toward dynamics. For AVOS, we design a better combination of fusion and cross connection layers compared with previous architectures.



### MALUNet: A Multi-Attention and Light-weight UNet for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.01784v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01784v1)
- **Published**: 2022-11-03 13:19:22+00:00
- **Updated**: 2022-11-03 13:19:22+00:00
- **Authors**: Jiacheng Ruan, Suncheng Xiang, Mingye Xie, Ting Liu, Yuzhuo Fu
- **Comment**: 7 pages, 7 figures, 5 tables; This work has been accepted as a
  regular paper in IEEE BIBM 2022
- **Journal**: None
- **Summary**: Recently, some pioneering works have preferred applying more complex modules to improve segmentation performances. However, it is not friendly for actual clinical environments due to limited computing resources. To address this challenge, we propose a light-weight model to achieve competitive performances for skin lesion segmentation at the lowest cost of parameters and computational complexity so far. Briefly, we propose four modules: (1) DGA consists of dilated convolution and gated attention mechanisms to extract global and local feature information; (2) IEA, which is based on external attention to characterize the overall datasets and enhance the connection between samples; (3) CAB is composed of 1D convolution and fully connected layers to perform a global and local fusion of multi-stage features to generate attention maps at channel axis; (4) SAB, which operates on multi-stage features by a shared 2D convolution to generate attention maps at spatial axis. We combine four modules with our U-shape architecture and obtain a light-weight medical image segmentation model dubbed as MALUNet. Compared with UNet, our model improves the mIoU and DSC metrics by 2.39% and 1.49%, respectively, with a 44x and 166x reduction in the number of parameters and computational complexity. In addition, we conduct comparison experiments on two skin lesion segmentation datasets (ISIC2017 and ISIC2018). Experimental results show that our model achieves state-of-the-art in balancing the number of parameters, computational complexity and segmentation performances. Code is available at https://github.com/JCruan519/MALUNet.



### Rethinking Hierarchies in Pre-trained Plain Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2211.01785v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01785v2)
- **Published**: 2022-11-03 13:19:23+00:00
- **Updated**: 2022-11-08 15:07:29+00:00
- **Authors**: Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao
- **Comment**: Tech report, work in progress
- **Journal**: None
- **Summary**: Self-supervised pre-training vision transformer (ViT) via masked image modeling (MIM) has been proven very effective. However, customized algorithms should be carefully designed for the hierarchical ViTs, e.g., GreenMIM, instead of using the vanilla and simple MAE for the plain ViT. More importantly, since these hierarchical ViTs cannot reuse the off-the-shelf pre-trained weights of the plain ViTs, the requirement of pre-training them leads to a massive amount of computational cost, thereby incurring both algorithmic and computational complexity. In this paper, we address this problem by proposing a novel idea of disentangling the hierarchical architecture design from the self-supervised pre-training. We transform the plain ViT into a hierarchical one with minimal changes. Technically, we change the stride of linear embedding layer from 16 to 4 and add convolution (or simple average) pooling layers between the transformer blocks, thereby reducing the feature size from 1/4 to 1/32 sequentially. Despite its simplicity, it outperforms the plain ViT baseline in classification, detection, and segmentation tasks on ImageNet, MS COCO, Cityscapes, and ADE20K benchmarks, respectively. We hope this preliminary study could draw more attention from the community on developing effective (hierarchical) ViTs while avoiding the pre-training cost by leveraging the off-the-shelf checkpoints. The code and models will be released at https://github.com/ViTAE-Transformer/HPViT.



### Self Similarity Matrix based CNN Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2211.01814v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01814v1)
- **Published**: 2022-11-03 13:47:44+00:00
- **Updated**: 2022-11-03 13:47:44+00:00
- **Authors**: S Rakshith, Jayesh Rajkumar Vachhani, Sourabh Vasant Gothe, Rishabh Khurana
- **Comment**: Paper accepted in the 7th International Conference on Computer Vision
  & Image Processing (2022)
- **Journal**: None
- **Summary**: In recent years, most of the deep learning solutions are targeted to be deployed in mobile devices. This makes the need for development of lightweight models all the more imminent. Another solution is to optimize and prune regular deep learning models. In this paper, we tackle the problem of CNN model pruning with the help of Self-Similarity Matrix (SSM) computed from the 2D CNN filters. We propose two novel algorithms to rank and prune redundant filters which contribute similar activation maps to the output. One of the key features of our method is that there is no need of finetuning after training the model. Both the training and pruning process is completed simultaneously. We benchmark our method on two of the most popular CNN models - ResNet and VGG and record their performance on the CIFAR-10 dataset.



### Fast Noise Removal in Hyperspectral Images via Representative Coefficient Total Variation
- **Arxiv ID**: http://arxiv.org/abs/2211.01825v1
- **DOI**: 10.1109/TGRS.2022.3229012
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01825v1)
- **Published**: 2022-11-03 14:06:37+00:00
- **Updated**: 2022-11-03 14:06:37+00:00
- **Authors**: Jiangjun Peng, Hailin Wang, Xiangyong Cao, Xinlin Liu, Xiangyu Rui, Deyu Meng
- **Comment**: 16 pages, 18 figures, 5 tables, 1 theorem
- **Journal**: None
- **Summary**: Mining structural priors in data is a widely recognized technique for hyperspectral image (HSI) denoising tasks, whose typical ways include model-based methods and data-based methods. The model-based methods have good generalization ability, while the runtime cannot meet the fast processing requirements of the practical situations due to the large size of an HSI data $ \mathbf{X} \in \mathbb{R}^{MN\times B}$. For the data-based methods, they perform very fast on new test data once they have been trained. However, their generalization ability is always insufficient. In this paper, we propose a fast model-based HSI denoising approach. Specifically, we propose a novel regularizer named Representative Coefficient Total Variation (RCTV) to simultaneously characterize the low rank and local smooth properties. The RCTV regularizer is proposed based on the observation that the representative coefficient matrix $\mathbf{U}\in\mathbb{R}^{MN\times R} (R\ll B)$ obtained by orthogonally transforming the original HSI $\mathbf{X}$ can inherit the strong local-smooth prior of $\mathbf{X}$. Since $R/B$ is very small, the HSI denoising model based on the RCTV regularizer has lower time complexity. Additionally, we find that the representative coefficient matrix $\mathbf{U}$ is robust to noise, and thus the RCTV regularizer can somewhat promote the robustness of the HSI denoising model. Extensive experiments on mixed noise removal demonstrate the superiority of the proposed method both in denoising performance and denoising speed compared with other state-of-the-art methods. Remarkably, the denoising speed of our proposed method outperforms all the model-based techniques and is comparable with the deep learning-based approaches.



### Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars
- **Arxiv ID**: http://arxiv.org/abs/2211.01842v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2211.01842v2)
- **Published**: 2022-11-03 14:23:00+00:00
- **Updated**: 2023-06-05 12:22:19+00:00
- **Authors**: Simon Schrodi, Danny Stoll, Binxin Ru, Rhea Sukthanker, Thomas Brox, Frank Hutter
- **Comment**: None
- **Journal**: None
- **Summary**: The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Code is available at https://github.com/automl/hierarchical_nas_construction.



### Seeing the Unseen: Errors and Bias in Visual Datasets
- **Arxiv ID**: http://arxiv.org/abs/2211.01847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2211.01847v1)
- **Published**: 2022-11-03 14:34:28+00:00
- **Updated**: 2022-11-03 14:34:28+00:00
- **Authors**: Hongrui Jin
- **Comment**: 13 pages, 2 figures
- **Journal**: None
- **Summary**: From face recognition in smartphones to automatic routing on self-driving cars, machine vision algorithms lie in the core of these features. These systems solve image based tasks by identifying and understanding objects, subsequently making decisions from these information. However, errors in datasets are usually induced or even magnified in algorithms, at times resulting in issues such as recognising black people as gorillas and misrepresenting ethnicities in search results. This paper tracks the errors in datasets and their impacts, revealing that a flawed dataset could be a result of limited categories, incomprehensive sourcing and poor classification.



### Computed tomography coronary angiogram images, annotations and associated data of normal and diseased arteries
- **Arxiv ID**: http://arxiv.org/abs/2211.01859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01859v1)
- **Published**: 2022-11-03 14:50:43+00:00
- **Updated**: 2022-11-03 14:50:43+00:00
- **Authors**: Ramtin Gharleghi, Dona Adikari, Katy Ellenberger, Mark Webster, Chris Ellis, Arcot Sowmya, Sze-Yuan Ooi, Susann Beier
- **Comment**: 10 pages, 3 figures. Submitted to the journal Scientific Data. For
  associated challenge, see https://asoca.grand-challenge.org/
- **Journal**: None
- **Summary**: Computed Tomography Coronary Angiography (CTCA) is a non-invasive method to evaluate coronary artery anatomy and disease. CTCA is ideal for geometry reconstruction to create virtual models of coronary arteries. To our knowledge there is no public dataset that includes centrelines and segmentation of the full coronary tree.   We provide anonymized CTCA images, voxel-wise annotations and associated data in the form of centrelines, calcification scores and meshes of the coronary lumen in 20 normal and 20 diseased cases. Images were obtained along with patient information with informed, written consent as part of Coronary Atlas (https://www.coronaryatlas.org/). Cases were classified as normal (zero calcium score with no signs of stenosis) or diseased (confirmed coronary artery disease). Manual voxel-wise segmentations by three experts were combined using majority voting to generate the final annotations.   Provided data can be used for a variety of research purposes, such as 3D printing patient-specific models, development and validation of segmentation algorithms, education and training of medical personnel and in-silico analyses such as testing of medical devices.



### ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations
- **Arxiv ID**: http://arxiv.org/abs/2211.01866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.01866v1)
- **Published**: 2022-11-03 14:56:32+00:00
- **Updated**: 2022-11-03 14:56:32+00:00
- **Authors**: Badr Youbi Idrissi, Diane Bouchacourt, Randall Balestriero, Ivan Evtimov, Caner Hazirbas, Nicolas Ballas, Pascal Vincent, Michal Drozdzal, David Lopez-Paz, Mark Ibrahim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning vision systems are widely deployed across applications where reliability is critical. However, even today's best models can fail to recognize an object when its pose, lighting, or background varies. While existing benchmarks surface examples challenging for models, they do not explain why such mistakes arise. To address this need, we introduce ImageNet-X, a set of sixteen human annotations of factors such as pose, background, or lighting the entire ImageNet-1k validation set as well as a random subset of 12k training images. Equipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model's (1) architecture, e.g. transformer vs. convolutional, (2) learning paradigm, e.g. supervised vs. self-supervised, and (3) training procedures, e.g., data augmentation. Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories. We also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors. For example, strong random cropping hurts robustness on smaller objects. Together, these insights suggest to advance the robustness of modern vision models, future research should focus on collecting additional data and understanding data augmentation schemes. Along with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes image recognition systems make.



### A Survey on Evaluation Metrics for Synthetic Material Micro-Structure Images from Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2211.09727v1
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, cs.LG, eess.IV, I.2.m; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2211.09727v1)
- **Published**: 2022-11-03 15:17:42+00:00
- **Updated**: 2022-11-03 15:17:42+00:00
- **Authors**: Devesh Shah, Anirudh Suresh, Alemayehu Admasu, Devesh Upadhyay, Kalyanmoy Deb
- **Comment**: Accepted in Neural Information Processing Systems (NeurIPS) 2022
  Workshop on AI for Accelerated Materials Design (AI4Mat). Selected as
  spotlight paper for workshop
- **Journal**: None
- **Summary**: The evaluation of synthetic micro-structure images is an emerging problem as machine learning and materials science research have evolved together. Typical state of the art methods in evaluating synthetic images from generative models have relied on the Fr\'echet Inception Distance. However, this and other similar methods, are limited in the materials domain due to both the unique features that characterize physically accurate micro-structures and limited dataset sizes. In this study we evaluate a variety of methods on scanning electron microscope (SEM) images of graphene-reinforced polyurethane foams. The primary objective of this paper is to report our findings with regards to the shortcomings of existing methods so as to encourage the machine learning community to consider enhancements in metrics for assessing quality of synthetic images in the material science domain.



### Using U-Net Network for Efficient Brain Tumor Segmentation in MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2211.01885v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2211.01885v1)
- **Published**: 2022-11-03 15:19:58+00:00
- **Updated**: 2022-11-03 15:19:58+00:00
- **Authors**: Jason Walsh, Alice Othmani, Mayank Jain, Soumyabrata Dev
- **Comment**: Published in Healthcare Analytics, 2022
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is the most commonly used non-intrusive technique for medical image acquisition. Brain tumor segmentation is the process of algorithmically identifying tumors in brain MRI scans. While many approaches have been proposed in the literature for brain tumor segmentation, this paper proposes a lightweight implementation of U-Net. Apart from providing real-time segmentation of MRI scans, the proposed architecture does not need large amount of data to train the proposed lightweight U-Net. Moreover, no additional data augmentation step is required. The lightweight U-Net shows very promising results on BITE dataset and it achieves a mean intersection-over-union (IoU) of 89% while outperforming the standard benchmark algorithms. Additionally, this work demonstrates an effective use of the three perspective planes, instead of the original three-dimensional volumetric images, for simplified brain tumor segmentation.



### Analysing the effectiveness of a generative model for semi-supervised medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2211.01886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01886v1)
- **Published**: 2022-11-03 15:19:59+00:00
- **Updated**: 2022-11-03 15:19:59+00:00
- **Authors**: Margherita Rosnati, Fabio De Sousa Ribeiro, Miguel Monteiro, Daniel Coelho de Castro, Ben Glocker
- **Comment**: Accepted at ML4H 2022
- **Journal**: None
- **Summary**: Image segmentation is important in medical imaging, providing valuable, quantitative information for clinical decision-making in diagnosis, therapy, and intervention. The state-of-the-art in automated segmentation remains supervised learning, employing discriminative models such as U-Net. However, training these models requires access to large amounts of manually labelled data which is often difficult to obtain in real medical applications. In such settings, semi-supervised learning (SSL) attempts to leverage the abundance of unlabelled data to obtain more robust and reliable models. Recently, generative models have been proposed for semantic segmentation, as they make an attractive choice for SSL. Their ability to capture the joint distribution over input images and output label maps provides a natural way to incorporate information from unlabelled images. This paper analyses whether deep generative models such as the SemanticGAN are truly viable alternatives to tackle challenging medical image segmentation problems. To that end, we thoroughly evaluate the segmentation performance, robustness, and potential subgroup disparities of discriminative and generative segmentation methods when applied to large-scale, publicly available chest X-ray datasets.



### Deep meta-learning for the selection of accurate ultrasound based breast mass classifier
- **Arxiv ID**: http://arxiv.org/abs/2211.01892v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2211.01892v1)
- **Published**: 2022-11-03 15:26:28+00:00
- **Updated**: 2022-11-03 15:26:28+00:00
- **Authors**: Michal Byra, Piotr Karwat, Ivan Ryzhankow, Piotr Komorowski, Ziemowit Klimonda, Lukasz Fura, Anna Pawlowska, Norbert Zolek, Jerzy Litniewski
- **Comment**: Work presented at the 2022 IEEE International Ultrasonics Symposium,
  submission #2078
- **Journal**: None
- **Summary**: Standard classification methods based on handcrafted morphological and texture features have achieved good performance in breast mass differentiation in ultrasound (US). In comparison to deep neural networks, commonly perceived as "black-box" models, classical techniques are based on features that have well-understood medical and physical interpretation. However, classifiers based on morphological features commonly underperform in the presence of the shadowing artifact and ill-defined mass borders, while texture based classifiers may fail when the US image is too noisy. Therefore, in practice it would be beneficial to select the classification method based on the appearance of the particular US image. In this work, we develop a deep meta-network that can automatically process input breast mass US images and recommend whether to apply the shape or texture based classifier for the breast mass differentiation. Our preliminary results demonstrate that meta-learning techniques can be used to improve the performance of the standard classifiers based on handcrafted features. With the proposed meta-learning based approach, we achieved the area under the receiver operating characteristic curve of 0.95 and accuracy of 0.91.



### Expanding Accurate Person Recognition to New Altitudes and Ranges: The BRIAR Dataset
- **Arxiv ID**: http://arxiv.org/abs/2211.01917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.01917v1)
- **Published**: 2022-11-03 15:51:39+00:00
- **Updated**: 2022-11-03 15:51:39+00:00
- **Authors**: David Cornett III, Joel Brogan, Nell Barber, Deniz Aykac, Seth Baird, Nick Burchfield, Carl Dukes, Andrew Duncan, Regina Ferrell, Jim Goddard, Gavin Jager, Matt Larson, Bart Murphy, Christi Johnson, Ian Shelley, Nisha Srinivas, Brandon Stockwell, Leanne Thompson, Matt Yohe, Robert Zhang, Scott Dolvin, Hector J. Santos-Villalobos, David S. Bolme
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition technology has advanced significantly in recent years due largely to the availability of large and increasingly complex training datasets for use in deep learning models. These datasets, however, typically comprise images scraped from news sites or social media platforms and, therefore, have limited utility in more advanced security, forensics, and military applications. These applications require lower resolution, longer ranges, and elevated viewpoints. To meet these critical needs, we collected and curated the first and second subsets of a large multi-modal biometric dataset designed for use in the research and development (R&D) of biometric recognition technologies under extremely challenging conditions. Thus far, the dataset includes more than 350,000 still images and over 1,300 hours of video footage of approximately 1,000 subjects. To collect this data, we used Nikon DSLR cameras, a variety of commercial surveillance cameras, specialized long-rage R&D cameras, and Group 1 and Group 2 UAV platforms. The goal is to support the development of algorithms capable of accurately recognizing people at ranges up to 1,000 m and from high angles of elevation. These advances will include improvements to the state of the art in face recognition and will support new research in the area of whole-body recognition using methods based on gait and anthropometry. This paper describes methods used to collect and curate the dataset, and the dataset's characteristics at the current stage.



### Photorealistic Facial Wrinkles Removal
- **Arxiv ID**: http://arxiv.org/abs/2211.01930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01930v1)
- **Published**: 2022-11-03 16:09:51+00:00
- **Updated**: 2022-11-03 16:09:51+00:00
- **Authors**: Marcelo Sanchez, Gil Triginer, Coloma Ballester, Lara Raad, Eduard Ramon
- **Comment**: None
- **Journal**: None
- **Summary**: Editing and retouching facial attributes is a complex task that usually requires human artists to obtain photo-realistic results. Its applications are numerous and can be found in several contexts such as cosmetics or digital media retouching, to name a few. Recently, advancements in conditional generative modeling have shown astonishing results at modifying facial attributes in a realistic manner. However, current methods are still prone to artifacts, and focus on modifying global attributes like age and gender, or local mid-sized attributes like glasses or moustaches. In this work, we revisit a two-stage approach for retouching facial wrinkles and obtain results with unprecedented realism. First, a state of the art wrinkle segmentation network is used to detect the wrinkles within the facial region. Then, an inpainting module is used to remove the detected wrinkles, filling them in with a texture that is statistically consistent with the surrounding skin. To achieve this, we introduce a novel loss term that reuses the wrinkle segmentation network to penalize those regions that still contain wrinkles after the inpainting. We evaluate our method qualitatively and quantitatively, showing state of the art results for the task of wrinkle removal. Moreover, we introduce the first high-resolution dataset, named FFHQ-Wrinkles, to evaluate wrinkle detection methods.



### Automatic Crater Shape Retrieval using Unsupervised and Semi-Supervised Systems
- **Arxiv ID**: http://arxiv.org/abs/2211.01933v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01933v1)
- **Published**: 2022-11-03 16:16:29+00:00
- **Updated**: 2022-11-03 16:16:29+00:00
- **Authors**: Atal Tewari, Vikrant Jain, Nitin Khanna
- **Comment**: None
- **Journal**: None
- **Summary**: Impact craters are formed due to continuous impacts on the surface of planetary bodies. Most recent deep learning-based crater detection methods treat craters as circular shapes, and less attention is paid to extracting the exact shapes of craters. Extracting precise shapes of the craters can be helpful for many advanced analyses, such as crater formation. This paper proposes a combination of unsupervised non-deep learning and semi-supervised deep learning approach to accurately extract shapes of the craters and detect missing craters from the existing catalog. In unsupervised non-deep learning, we have proposed an adaptive rim extraction algorithm to extract craters' shapes. In this adaptive rim extraction algorithm, we utilized the elevation profiles of DEMs and applied morphological operation on DEM-derived slopes to extract craters' shapes. The extracted shapes of the craters are used in semi-supervised deep learning to get the locations, size, and refined shapes. Further, the extracted shapes of the craters are utilized to improve the estimate of the craters' diameter, depth, and other morphological factors. The craters' shape, estimated diameter, and depth with other morphological factors will be publicly available.



### Revisiting and Optimising a CNN Colour Constancy Method for Multi-Illuminant Estimation
- **Arxiv ID**: http://arxiv.org/abs/2211.01946v1
- **DOI**: 10.2352/issn.2694-118X.2021.LIM-68
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.01946v1)
- **Published**: 2022-11-03 16:33:56+00:00
- **Updated**: 2022-11-03 16:33:56+00:00
- **Authors**: Ghalia Hemrit, Joseph Meehan
- **Comment**: None
- **Journal**: London Imaging Meeting, 68-72, 2021
- **Summary**: The aim of colour constancy is to discount the effect of the scene illumination from the image colours and restore the colours of the objects as captured under a 'white' illuminant. For the majority of colour constancy methods, the first step is to estimate the scene illuminant colour. Generally, it is assumed that the illumination is uniform in the scene. However, real world scenes have multiple illuminants, like sunlight and spot lights all together in one scene. We present in this paper a simple yet very effective framework using a deep CNN-based method to estimate and use multiple illuminants for colour constancy. Our approach works well in both the multi and single illuminant cases. The output of the CNN method is a region-wise estimate map of the scene which is smoothed and divided out from the image to perform colour constancy. The method that we propose outperforms other recent and state of the art methods and has promising visual results.



### MarginNCE: Robust Sound Localization with a Negative Margin
- **Arxiv ID**: http://arxiv.org/abs/2211.01966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01966v1)
- **Published**: 2022-11-03 16:44:14+00:00
- **Updated**: 2022-11-03 16:44:14+00:00
- **Authors**: Sooyoung Park, Arda Senocak, Joon Son Chung
- **Comment**: Submitted to ICASSP 2023. SOTA performance in Audio-Visual Sound
  Localization. 5 Pages
- **Journal**: None
- **Summary**: The goal of this work is to localize sound sources in visual scenes with a self-supervised approach. Contrastive learning in the context of sound source localization leverages the natural correspondence between audio and visual signals where the audio-visual pairs from the same source are assumed as positive, while randomly selected pairs are negatives. However, this approach brings in noisy correspondences; for example, positive audio and visual pair signals that may be unrelated to each other, or negative pairs that may contain semantically similar samples to the positive one. Our key contribution in this work is to show that using a less strict decision boundary in contrastive learning can alleviate the effect of noisy correspondences in sound source localization. We propose a simple yet effective approach by slightly modifying the contrastive loss with a negative margin. Extensive experimental results show that our approach gives on-par or better performance than the state-of-the-art methods. Furthermore, we demonstrate that the introduction of a negative margin to existing methods results in a consistent improvement in performance.



### Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing
- **Arxiv ID**: http://arxiv.org/abs/2211.01969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.01969v1)
- **Published**: 2022-11-03 16:46:46+00:00
- **Updated**: 2022-11-03 16:46:46+00:00
- **Authors**: Aditay Tripathi, Anand Mishra, Anirban Chakraborty
- **Comment**: None
- **Journal**: WACV 2023
- **Summary**: This paper presents a framework for jointly grounding objects that follow certain semantic relationship constraints given in a scene graph. A typical natural scene contains several objects, often exhibiting visual relationships of varied complexities between them. These inter-object relationships provide strong contextual cues toward improving grounding performance compared to a traditional object query-only-based localization task. A scene graph is an efficient and structured way to represent all the objects and their semantic relationships in the image. In an attempt towards bridging these two modalities representing scenes and utilizing contextual information for improving object localization, we rigorously study the problem of grounding scene graphs on natural images. To this end, we propose a novel graph neural network-based approach referred to as Visio-Lingual Message PAssing Graph Neural Network (VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object proposals as nodes and an edge between a pair of nodes representing a plausible relation between them. Then a three-step inter-graph and intra-graph message passing is performed to learn the context-dependent representation of the proposals and query objects. These object representations are used to score the proposals to generate object localization. The proposed method significantly outperforms the baselines on four public datasets.



### Quantifying Model Uncertainty for Semantic Segmentation using Operators in the RKHS
- **Arxiv ID**: http://arxiv.org/abs/2211.01999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, eess.IV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2211.01999v1)
- **Published**: 2022-11-03 17:10:49+00:00
- **Updated**: 2022-11-03 17:10:49+00:00
- **Authors**: Rishabh Singh, Jose C. Principe
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models for semantic segmentation are prone to poor performance in real-world applications due to the highly challenging nature of the task. Model uncertainty quantification (UQ) is one way to address this issue of lack of model trustworthiness by enabling the practitioner to know how much to trust a segmentation output. Current UQ methods in this application domain are mainly restricted to Bayesian based methods which are computationally expensive and are only able to extract central moments of uncertainty thereby limiting the quality of their uncertainty estimates. We present a simple framework for high-resolution predictive uncertainty quantification of semantic segmentation models that leverages a multi-moment functional definition of uncertainty associated with the model's feature space in the reproducing kernel Hilbert space (RKHS). The multiple uncertainty functionals extracted from this framework are defined by the local density dynamics of the model's feature space and hence automatically align themselves at the tail-regions of the intrinsic probability density function of the feature space (where uncertainty is the highest) in such a way that the successively higher order moments quantify the more uncertain regions. This leads to a significantly more accurate view of model uncertainty than conventional Bayesian methods. Moreover, the extraction of such moments is done in a single-shot computation making it much faster than Bayesian and ensemble approaches (that involve a high number of forward stochastic passes of the model to quantify its uncertainty). We demonstrate these advantages through experimental evaluations of our framework implemented over four different state-of-the-art model architectures that are trained and evaluated on two benchmark road-scene segmentation datasets (Camvid and Cityscapes).



### Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast
- **Arxiv ID**: http://arxiv.org/abs/2211.02556v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02556v1)
- **Published**: 2022-11-03 17:19:43+00:00
- **Updated**: 2022-11-03 17:19:43+00:00
- **Authors**: Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian
- **Comment**: 19 pages, 13 figures: the first ever AI-based method that outperforms
  traditional numerical weather prediction methods
- **Journal**: None
- **Summary**: In this paper, we present Pangu-Weather, a deep learning based system for fast and accurate global weather forecast. For this purpose, we establish a data-driven environment by downloading $43$ years of hourly global weather data from the 5th generation of ECMWF reanalysis (ERA5) data and train a few deep neural networks with about $256$ million parameters in total. The spatial resolution of forecast is $0.25^\circ\times0.25^\circ$, comparable to the ECMWF Integrated Forecast Systems (IFS). More importantly, for the first time, an AI-based method outperforms state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy (latitude-weighted RMSE and ACC) of all factors (e.g., geopotential, specific humidity, wind speed, temperature, etc.) and in all time ranges (from one hour to one week). There are two key strategies to improve the prediction accuracy: (i) designing a 3D Earth Specific Transformer (3DEST) architecture that formulates the height (pressure level) information into cubic data, and (ii) applying a hierarchical temporal aggregation algorithm to alleviate cumulative forecast errors. In deterministic forecast, Pangu-Weather shows great advantages for short to medium-range forecast (i.e., forecast time ranges from one hour to one week). Pangu-Weather supports a wide range of downstream forecast scenarios, including extreme weather forecast (e.g., tropical cyclone tracking) and large-member ensemble forecast in real-time. Pangu-Weather not only ends the debate on whether AI-based methods can surpass conventional NWP methods, but also reveals novel directions for improving deep learning weather forecast systems.



### SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency
- **Arxiv ID**: http://arxiv.org/abs/2211.02006v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02006v2)
- **Published**: 2022-11-03 17:20:55+00:00
- **Updated**: 2023-03-15 18:19:12+00:00
- **Authors**: Yang Liu, Yao Zhang, Yixin Wang, Yang Zhang, Jiang Tian, Zhongchao Shi, Jianping Fan, Zhiqiang He
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Recently, the dominant DETR-based approaches apply central-concept spatial prior to accelerate Transformer detector convergency. These methods gradually refine the reference points to the center of target objects and imbue object queries with the updated central reference information for spatially conditional attention. However, centralizing reference points may severely deteriorate queries' saliency and confuse detectors due to the indiscriminative spatial prior. To bridge the gap between the reference points of salient queries and Transformer detectors, we propose SAlient Point-based DETR (SAP-DETR) by treating object detection as a transformation from salient points to instance objects. In SAP-DETR, we explicitly initialize a query-specific reference point for each object query, gradually aggregate them into an instance object, and then predict the distance from each side of the bounding box to these points. By rapidly attending to query-specific reference region and other conditional extreme regions from the image features, SAP-DETR can effectively bridge the gap between the salient point and the query-based Transformer detector with a significant convergency speed. Our extensive experiments have demonstrated that SAP-DETR achieves 1.4 times convergency speed with competitive performance. Under the standard training scheme, SAP-DETR stably promotes the SOTA approaches by 1.0 AP. Based on ResNet-DC-101, SAP-DETR achieves 46.9 AP.



### Could Giant Pretrained Image Models Extract Universal Representations?
- **Arxiv ID**: http://arxiv.org/abs/2211.02043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02043v1)
- **Published**: 2022-11-03 17:57:10+00:00
- **Updated**: 2022-11-03 17:57:10+00:00
- **Authors**: Yutong Lin, Ze Liu, Zheng Zhang, Han Hu, Nanning Zheng, Stephen Lin, Yue Cao
- **Comment**: To be published in NeurIPS2022
- **Journal**: None
- **Summary**: Frozen pretrained models have become a viable alternative to the pretraining-then-finetuning paradigm for transfer learning. However, with frozen models there are relatively few parameters available for adapting to downstream tasks, which is problematic in computer vision where tasks vary significantly in input/output format and the type of information that is of value. In this paper, we present a study of frozen pretrained models when applied to diverse and representative computer vision tasks, including object detection, semantic segmentation and video action recognition. From this empirical analysis, our work answers the questions of what pretraining task fits best with this frozen setting, how to make the frozen setting more flexible to various downstream tasks, and the effect of larger model sizes. We additionally examine the upper bound of performance using a giant frozen pretrained model with 3 billion parameters (SwinV2-G) and find that it reaches competitive performance on a varied set of major benchmarks with only one shared frozen base network: 60.0 box mAP and 52.2 mask mAP on COCO object detection test-dev, 57.6 val mIoU on ADE20K semantic segmentation, and 81.7 top-1 accuracy on Kinetics-400 action recognition. With this work, we hope to bring greater attention to this promising path of freezing pretrained image models.



### Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2211.02048v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.02048v3)
- **Published**: 2022-11-03 17:59:55+00:00
- **Updated**: 2023-04-11 19:08:09+00:00
- **Authors**: Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, Jun-Yan Zhu
- **Comment**: NeurIPS 2022 Website: https://www.cs.cmu.edu/~sige/ Code:
  https://github.com/lmxyy/sige
- **Journal**: None
- **Summary**: During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users tend to gradually change the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about $1\%$-area edits, our method reduces the computation of DDPM by $7.5\times$, Stable Diffusion by $8.2\times$, and GauGAN by $18\times$ while preserving the visual fidelity. With SIGE, we accelerate the inference time of DDPM by $3.0\times$ on NVIDIA RTX 3090 and $6.6\times$ on Apple M1 Pro CPU, Stable Diffusion by $7.2\times$ on 3090, and GauGAN by $5.6\times$ on 3090 and $14\times$ on M1 Pro CPU.



### Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2211.02077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2211.02077v1)
- **Published**: 2022-11-03 18:12:32+00:00
- **Updated**: 2022-11-03 18:12:32+00:00
- **Authors**: Junru Wu, Yi Liang, Feng Han, Hassan Akbari, Zhangyang Wang, Cong Yu
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Self-supervised pre-training recently demonstrates success on large-scale multimodal data, and state-of-the-art contrastive learning methods often enforce the feature consistency from cross-modality inputs, such as video/audio or video/text pairs. Despite its convenience to formulate and leverage in practice, such cross-modality alignment (CMA) is only a weak and noisy supervision, since two modalities can be semantically misaligned even they are temporally aligned. For example, even in the commonly adopted instructional videos, a speaker can sometimes refer to something that is not visually present in the current frame; and the semantic misalignment would only be more unpredictable for the raw videos from the internet. We conjecture that might cause conflicts and biases among modalities, and may hence prohibit CMA from scaling up to training with larger and more heterogeneous data. This paper first verifies our conjecture by observing that, even in the latest VATT pre-training using only instructional videos, there exist strong gradient conflicts between different CMA losses within the same video, audio, text triplet, indicating them as the noisy source of supervision. We then propose to harmonize such gradients, via two techniques: (i) cross-modality gradient realignment: modifying different CMA loss gradients for each sample triplet, so that their gradient directions are more aligned; and (ii) gradient-based curriculum learning: leveraging the gradient conflict information on an indicator of sample noisiness, to develop a curriculum learning strategy to prioritize training on less noisy sample triplets. Applying those techniques to pre-training VATT on the HowTo100M dataset, we consistently improve its performance on different downstream tasks. Moreover, we are able to scale VATT pre-training to more complicated non-narrative Youtube8M dataset to further improve the state-of-the-arts.



### Sky-image-based solar forecasting using deep learning with multi-location data: training models locally, globally or via transfer learning?
- **Arxiv ID**: http://arxiv.org/abs/2211.02108v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.02108v2)
- **Published**: 2022-11-03 19:25:28+00:00
- **Updated**: 2022-12-05 18:21:08+00:00
- **Authors**: Yuhao Nie, Quentin Paletta, Andea Scott, Luis Martin Pomares, Guillaume Arbod, Sgouris Sgouridis, Joan Lasenby, Adam Brandt
- **Comment**: None
- **Journal**: None
- **Summary**: Solar forecasting from ground-based sky images has shown great promise in reducing the uncertainty in solar power generation. With more and more sky image datasets open sourced in recent years, the development of accurate and reliable deep learning-based solar forecasting methods has seen a huge growth in potential. In this study, we explore three different training strategies for solar forecasting models by leveraging three heterogeneous datasets collected globally with different climate patterns. Specifically, we compare the performance of local models trained individually based on single datasets and global models trained jointly based on the fusion of multiple datasets, and further examine the knowledge transfer from pre-trained solar forecasting models to a new dataset of interest. The results suggest that the local models work well when deployed locally, but significant errors are observed when applied offsite. The global model can adapt well to individual locations at the cost of a potential increase in training efforts. Pre-training models on a large and diversified source dataset and transferring to a target dataset generally achieves superior performance over the other two strategies. With 80% less training data, it can achieve comparable performance as the local baseline trained using the entire dataset.



### Translated Skip Connections -- Expanding the Receptive Fields of Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2211.02111v1
- **DOI**: 10.1109/ICIP46576.2022.9897165
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2211.02111v1)
- **Published**: 2022-11-03 19:30:40+00:00
- **Updated**: 2022-11-03 19:30:40+00:00
- **Authors**: Joshua Bruton, Hairong Wang
- **Comment**: 5 pages, 2 figures, 1 table, published at the 2022 IEEE International
  Conference on Image Processing
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP),
  2022, pp. 631-635
- **Summary**: The effective receptive field of a fully convolutional neural network is an important consideration when designing an architecture, as it defines the portion of the input visible to each convolutional kernel. We propose a neural network module, extending traditional skip connections, called the translated skip connection. Translated skip connections geometrically increase the receptive field of an architecture with negligible impact on both the size of the parameter space and computational complexity. By embedding translated skip connections into a benchmark architecture, we demonstrate that our module matches or outperforms four other approaches to expanding the effective receptive fields of fully convolutional neural networks. We confirm this result across five contemporary image segmentation datasets from disparate domains, including the detection of COVID-19 infection, segmentation of aerial imagery, common object segmentation, and segmentation for self-driving cars.



### Abstract Images Have Different Levels of Retrievability Per Reverse Image Search Engine
- **Arxiv ID**: http://arxiv.org/abs/2211.02115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, H.3.3; H.3.7; H.3.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2211.02115v1)
- **Published**: 2022-11-03 19:40:46+00:00
- **Updated**: 2022-11-03 19:40:46+00:00
- **Authors**: Shawn M. Jones, Diane Oyen
- **Comment**: 20 pages; 7 figures; to be published in the proceedings of the
  Drawings and abstract Imagery: Representation and Analysis (DIRA) Workshop
  from ECCV 2022
- **Journal**: None
- **Summary**: Much computer vision research has focused on natural images, but technical documents typically consist of abstract images, such as charts, drawings, diagrams, and schematics. How well do general web search engines discover abstract images? Recent advancements in computer vision and machine learning have led to the rise of reverse image search engines. Where conventional search engines accept a text query and return a set of document results, including images, a reverse image search accepts an image as a query and returns a set of images as results. This paper evaluates how well common reverse image search engines discover abstract images. We conducted an experiment leveraging images from Wikimedia Commons, a website known to be well indexed by Baidu, Bing, Google, and Yandex. We measure how difficult an image is to find again (retrievability), what percentage of images returned are relevant (precision), and the average number of results a visitor must review before finding the submitted image (mean reciprocal rank). When trying to discover the same image again among similar images, Yandex performs best. When searching for pages containing a specific image, Google and Yandex outperform the others when discovering photographs with precision scores ranging from 0.8191 to 0.8297, respectively. In both of these cases, Google and Yandex perform better with natural images than with abstract ones achieving a difference in retrievability as high as 54\% between images in these categories. These results affect anyone applying common web search engines to search for technical documents that use abstract images.



### Handwritten Arabic Character Recognition for Children Writ-ing Using Convolutional Neural Network and Stroke Identification
- **Arxiv ID**: http://arxiv.org/abs/2211.02119v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.02119v1)
- **Published**: 2022-11-03 19:48:11+00:00
- **Updated**: 2022-11-03 19:48:11+00:00
- **Authors**: Mais Alheraki, Rawan Al-Matham, Hend Al-Khalifa
- **Comment**: 17
- **Journal**: None
- **Summary**: Automatic Arabic handwritten recognition is one of the recently studied problems in the field of Machine Learning. Unlike Latin languages, Arabic is a Semitic language that forms a harder challenge, especially with variability of patterns caused by factors such as writer age. Most of the studies focused on adults, with only one recent study on children. Moreover, much of the recent Machine Learning methods focused on using Convolutional Neural Networks, a powerful class of neural networks that can extract complex features from images. In this paper we propose a convolutional neural network (CNN) model that recognizes children handwriting with an accuracy of 91% on the Hijja dataset, a recent dataset built by collecting images of the Arabic characters written by children, and 97% on Arabic Handwritten Character Dataset. The results showed a good improvement over the proposed model from the Hijja dataset authors, yet it reveals a bigger challenge to solve for children Arabic handwritten character recognition. Moreover, we proposed a new approach using multi models instead of single model based on the number of strokes in a character, and merged Hijja with AHCD which reached an averaged prediction accuracy of 96%.



### Graph-Based Multi-Camera Soccer Player Tracker
- **Arxiv ID**: http://arxiv.org/abs/2211.02125v1
- **DOI**: 10.1109/IJCNN55064.2022.9892562
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02125v1)
- **Published**: 2022-11-03 20:01:48+00:00
- **Updated**: 2022-11-03 20:01:48+00:00
- **Authors**: Jacek Komorowski, Grzegorz Kurzejamski
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a multi-camera tracking method intended for tracking soccer players in long shot video recordings from multiple calibrated cameras installed around the playing field. The large distance to the camera makes it difficult to visually distinguish individual players, which adversely affects the performance of traditional solutions relying on the appearance of tracked objects. Our method focuses on individual player dynamics and interactions between neighborhood players to improve tracking performance. To overcome the difficulty of reliably merging detections from multiple cameras in the presence of calibration errors, we propose the novel tracking approach, where the tracker operates directly on raw detection heat maps from multiple cameras. Our model is trained on a large synthetic dataset generated using Google Research Football Environment and fine-tuned using real-world data to reduce costs involved with ground truth preparation.



### Efficient Information Sharing in ICT Supply Chain Social Network via Table Structure Recognition
- **Arxiv ID**: http://arxiv.org/abs/2211.02128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2211.02128v1)
- **Published**: 2022-11-03 20:03:07+00:00
- **Updated**: 2022-11-03 20:03:07+00:00
- **Authors**: Bin Xiao, Yakup Akkaya, Murat Simsek, Burak Kantarci, Ala Abu Alkheir
- **Comment**: Globecom 2022
- **Journal**: None
- **Summary**: The global Information and Communications Technology (ICT) supply chain is a complex network consisting of all types of participants. It is often formulated as a Social Network to discuss the supply chain network's relations, properties, and development in supply chain management. Information sharing plays a crucial role in improving the efficiency of the supply chain, and datasheets are the most common data format to describe e-component commodities in the ICT supply chain because of human readability. However, with the surging number of electronic documents, it has been far beyond the capacity of human readers, and it is also challenging to process tabular data automatically because of the complex table structures and heterogeneous layouts. Table Structure Recognition (TSR) aims to represent tables with complex structures in a machine-interpretable format so that the tabular data can be processed automatically. In this paper, we formulate TSR as an object detection problem and propose to generate an intuitive representation of a complex table structure to enable structuring of the tabular data related to the commodities. To cope with border-less and small layouts, we propose a cost-sensitive loss function by considering the detection difficulty of each class. Besides, we propose a novel anchor generation method using the character of tables that columns in a table should share an identical height, and rows in a table should share the same width. We implement our proposed method based on Faster-RCNN and achieve 94.79% on mean Average Precision (AP), and consistently improve more than 1.5% AP for different benchmark models.



### Streaming Audio-Visual Speech Recognition with Alignment Regularization
- **Arxiv ID**: http://arxiv.org/abs/2211.02133v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2211.02133v2)
- **Published**: 2022-11-03 20:20:47+00:00
- **Updated**: 2023-07-02 00:33:36+00:00
- **Authors**: Pingchuan Ma, Niko Moritz, Stavros Petridis, Christian Fuegen, Maja Pantic
- **Comment**: Accepted to Interspeech 2023
- **Journal**: None
- **Summary**: In this work, we propose a streaming AV-ASR system based on a hybrid connectionist temporal classification (CTC)/attention neural network architecture. The audio and the visual encoder neural networks are both based on the conformer architecture, which is made streamable using chunk-wise self-attention (CSA) and causal convolution. Streaming recognition with a decoder neural network is realized by using the triggered attention technique, which performs time-synchronous decoding with joint CTC/attention scoring. Additionally, we propose a novel alignment regularization technique that promotes synchronization of the audio and visual encoder, which in turn results in better word error rates (WERs) at all SNR levels for streaming and offline AV-ASR models. The proposed AV-ASR model achieves WERs of 2.0% and 2.6% on the Lip Reading Sentences 3 (LRS3) dataset in an offline and online setup, respectively, which both present state-of-the-art results when no external training data are used.



### Book Cover Synthesis from the Summary
- **Arxiv ID**: http://arxiv.org/abs/2211.02138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02138v1)
- **Published**: 2022-11-03 20:43:40+00:00
- **Updated**: 2022-11-03 20:43:40+00:00
- **Authors**: Emdadul Haque, Md. Faraz Kabir Khan, Mohammad Imrul Jubair, Jarin Anjum, Abrar Zahir Niloy
- **Comment**: Accepted as a full paper in AICCSA2022 (19th ACS/IEEE International
  Conference on Computer Systems and Applications)
- **Journal**: None
- **Summary**: The cover is the face of a book and is a point of attraction for the readers. Designing book covers is an essential task in the publishing industry. One of the main challenges in creating a book cover is representing the theme of the book's content in a single image. In this research, we explore ways to produce a book cover using artificial intelligence based on the fact that there exists a relationship between the summary of the book and its cover. Our key motivation is the application of text-to-image synthesis methods to generate images from given text or captions. We explore several existing text-to-image conversion techniques for this purpose and propose an approach to exploit these frameworks for producing book covers from provided summaries. We construct a dataset of English books that contains a large number of samples of summaries of existing books and their cover images. In this paper, we describe our approach to collecting, organizing, and pre-processing the dataset to use it for training models. We apply different text-to-image synthesis techniques to generate book covers from the summary and exhibit the results in this paper.



### Shapes2Toon: Generating Cartoon Characters from Simple Geometric Shapes
- **Arxiv ID**: http://arxiv.org/abs/2211.02141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02141v1)
- **Published**: 2022-11-03 20:52:19+00:00
- **Updated**: 2022-11-03 20:52:19+00:00
- **Authors**: Simanta Deb Turja, Mohammad Imrul Jubair, Md. Shafiur Rahman, Md. Hasib Al Zadid, Mohtasim Hossain Shovon, Md. Faraz Kabir Khan
- **Comment**: Accepted as a full paper in AICCSA2022 (19th ACS/IEEE International
  Conference on Computer Systems and Applications)
- **Journal**: None
- **Summary**: Cartoons are an important part of our entertainment culture. Though drawing a cartoon is not for everyone, creating it using an arrangement of basic geometric primitives that approximates that character is a fairly frequent technique in art. The key motivation behind this technique is that human bodies - as well as cartoon figures - can be split down into various basic geometric primitives. Numerous tutorials are available that demonstrate how to draw figures using an appropriate arrangement of fundamental shapes, thus assisting us in creating cartoon characters. This technique is very beneficial for children in terms of teaching them how to draw cartoons. In this paper, we develop a tool - shape2toon - that aims to automate this approach by utilizing a generative adversarial network which combines geometric primitives (i.e. circles) and generate a cartoon figure (i.e. Mickey Mouse) depending on the given approximation. For this purpose, we created a dataset of geometrically represented cartoon characters. We apply an image-to-image translation technique on our dataset and report the results in this paper. The experimental results show that our system can generate cartoon characters from input layout of geometric shapes. In addition, we demonstrate a web-based tool as a practical implication of our work.



### Improving Semi-supervised Deep Learning by using Automatic Thresholding to Deal with Out of Distribution Data for COVID-19 Detection using Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2211.02142v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.02142v1)
- **Published**: 2022-11-03 20:56:45+00:00
- **Updated**: 2022-11-03 20:56:45+00:00
- **Authors**: Isaac Benavides-Mata, Saul Calderon-Ramirez
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) leverages both labeled and unlabeled data for training models when the labeled data is limited and the unlabeled data is vast. Frequently, the unlabeled data is more widely available than the labeled data, hence this data is used to improve the level of generalization of a model when the labeled data is scarce. However, in real-world settings unlabeled data might depict a different distribution than the labeled dataset distribution. This is known as distribution mismatch. Such problem generally occurs when the source of unlabeled data is different from the labeled data. For instance, in the medical imaging domain, when training a COVID-19 detector using chest X-ray images, different unlabeled datasets sampled from different hospitals might be used. In this work, we propose an automatic thresholding method to filter out-of-distribution data in the unlabeled dataset. We use the Mahalanobis distance between the labeled and unlabeled datasets using the feature space built by a pre-trained Image-net Feature Extractor (FE) to score each unlabeled observation. We test two simple automatic thresholding methods in the context of training a COVID-19 detector using chest X-ray images. The tested methods provide an automatic manner to define what unlabeled data to preserve when training a semi-supervised deep learning architecture.



### FactorMatte: Redefining Video Matting for Re-Composition Tasks
- **Arxiv ID**: http://arxiv.org/abs/2211.02145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02145v1)
- **Published**: 2022-11-03 21:05:01+00:00
- **Updated**: 2022-11-03 21:05:01+00:00
- **Authors**: Zeqi Gu, Wenqi Xian, Noah Snavely, Abe Davis
- **Comment**: Project webpage: https://factormatte.github.io
- **Journal**: None
- **Summary**: We propose "factor matting", an alternative formulation of the video matting problem in terms of counterfactual video synthesis that is better suited for re-composition tasks. The goal of factor matting is to separate the contents of video into independent components, each visualizing a counterfactual version of the scene where contents of other components have been removed. We show that factor matting maps well to a more general Bayesian framing of the matting problem that accounts for complex conditional interactions between layers. Based on this observation, we present a method for solving the factor matting problem that produces useful decompositions even for video with complex cross-layer interactions like splashes, shadows, and reflections. Our method is trained per-video and requires neither pre-training on external large datasets, nor knowledge about the 3D structure of the scene. We conduct extensive experiments, and show that our method not only can disentangle scenes with complex interactions, but also outperforms top methods on existing tasks such as classical video matting and background subtraction. In addition, we demonstrate the benefits of our approach on a range of downstream tasks. Please refer to our project webpage for more details: https://factormatte.github.io



### 3D Reconstruction of Multiple Objects by mmWave Radar on UAV
- **Arxiv ID**: http://arxiv.org/abs/2211.02150v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2211.02150v1)
- **Published**: 2022-11-03 21:23:36+00:00
- **Updated**: 2022-11-03 21:23:36+00:00
- **Authors**: Yue Sun, Zhuoming Huang, Honggang Zhang, Xiaohui Liang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore the feasibility of utilizing a mmWave radar sensor installed on a UAV to reconstruct the 3D shapes of multiple objects in a space. The UAV hovers at various locations in the space, and its onboard radar senor collects raw radar data via scanning the space with Synthetic Aperture Radar (SAR) operation. The radar data is sent to a deep neural network model, which outputs the point cloud reconstruction of the multiple objects in the space. We evaluate two different models. Model 1 is our recently proposed 3DRIMR/R2P model, and Model 2 is formed by adding a segmentation stage in the processing pipeline of Model 1. Our experiments have demonstrated that both models are promising in solving the multiple object reconstruction problem. We also show that Model 2, despite producing denser and smoother point clouds, can lead to higher reconstruction loss or even loss of objects. In addition, we find that both models are robust to the highly noisy radar data obtained by unstable SAR operation due to the instability or vibration of a small UAV hovering at its intended scanning point. Our exploratory study has shown a promising direction of applying mmWave radar sensing in 3D object reconstruction.



### Large Scale Real-World Multi-Person Tracking
- **Arxiv ID**: http://arxiv.org/abs/2211.02175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.02175v1)
- **Published**: 2022-11-03 23:03:13+00:00
- **Updated**: 2022-11-03 23:03:13+00:00
- **Authors**: Bing Shuai, Alessandro Bergamo, Uta Buechler, Andrew Berneshawi, Alyssa Boden, Joseph Tighe
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: This paper presents a new large scale multi-person tracking dataset -- \texttt{PersonPath22}, which is over an order of magnitude larger than currently available high quality multi-object tracking datasets such as MOT17, HiEve, and MOT20 datasets. The lack of large scale training and test data for this task has limited the community's ability to understand the performance of their tracking systems on a wide range of scenarios and conditions such as variations in person density, actions being performed, weather, and time of day. \texttt{PersonPath22} dataset was specifically sourced to provide a wide variety of these conditions and our annotations include rich meta-data such that the performance of a tracker can be evaluated along these different dimensions. The lack of training data has also limited the ability to perform end-to-end training of tracking systems. As such, the highest performing tracking systems all rely on strong detectors trained on external image datasets. We hope that the release of this dataset will enable new lines of research that take advantage of large scale video based training data.



### Zero-shot Video Moment Retrieval With Off-the-Shelf Models
- **Arxiv ID**: http://arxiv.org/abs/2211.02178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2211.02178v1)
- **Published**: 2022-11-03 23:11:04+00:00
- **Updated**: 2022-11-03 23:11:04+00:00
- **Authors**: Anuj Diwan, Puyuan Peng, Raymond J. Mooney
- **Comment**: Accepted to the NeurIPS 2022 Workshop on Transfer Learning for NLP
  (TL4NLP). 12 pages, 5 figures
- **Journal**: None
- **Summary**: For the majority of the machine learning community, the expensive nature of collecting high-quality human-annotated data and the inability to efficiently finetune very large state-of-the-art pretrained models on limited compute are major bottlenecks for building models for new tasks. We propose a zero-shot simple approach for one such task, Video Moment Retrieval (VMR), that does not perform any additional finetuning and simply repurposes off-the-shelf models trained on other tasks. Our three-step approach consists of moment proposal, moment-query matching and postprocessing, all using only off-the-shelf models. On the QVHighlights benchmark for VMR, we vastly improve performance of previous zero-shot approaches by at least 2.5x on all metrics and reduce the gap between zero-shot and state-of-the-art supervised by over 74%. Further, we also show that our zero-shot approach beats non-pretrained supervised models on the Recall metrics and comes very close on mAP metrics; and that it also performs better than the best pretrained supervised model on shorter moments. Finally, we ablate and analyze our results and propose interesting future directions.



### Deep Learning based Defect classification and detection in SEM images: A Mask R-CNN approach
- **Arxiv ID**: http://arxiv.org/abs/2211.02185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2211.02185v1)
- **Published**: 2022-11-03 23:26:40+00:00
- **Updated**: 2022-11-03 23:26:40+00:00
- **Authors**: Bappaditya Dey, Enrique Dehaerne, Kasem Khalil, Sandip Halder, Philippe Leray, Magdy A. Bayoumi
- **Comment**: arXiv admin note: text overlap with arXiv:2206.13505
- **Journal**: None
- **Summary**: In this research work, we have demonstrated the application of Mask-RCNN (Regional Convolutional Neural Network), a deep-learning algorithm for computer vision and specifically object detection, to semiconductor defect inspection domain. Stochastic defect detection and classification during semiconductor manufacturing has grown to be a challenging task as we continuously shrink circuit pattern dimensions (e.g., for pitches less than 32 nm). Defect inspection and analysis by state-of-the-art optical and e-beam inspection tools is generally driven by some rule-based techniques, which in turn often causes to misclassification and thereby necessitating human expert intervention. In this work, we have revisited and extended our previous deep learning-based defect classification and detection method towards improved defect instance segmentation in SEM images with precise extent of defect as well as generating a mask for each defect category/instance. This also enables to extract and calibrate each segmented mask and quantify the pixels that make up each mask, which in turn enables us to count each categorical defect instances as well as to calculate the surface area in terms of pixels. We are aiming at detecting and segmenting different types of inter-class stochastic defect patterns such as bridge, break, and line collapse as well as to differentiate accurately between intra-class multi-categorical defect bridge scenarios (as thin/single/multi-line/horizontal/non-horizontal) for aggressive pitches as well as thin resists (High NA applications). Our proposed approach demonstrates its effectiveness both quantitatively and qualitatively.




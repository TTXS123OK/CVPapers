# Arxiv Papers in cs.CV on 2022-05-07
### Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising
- **Arxiv ID**: http://arxiv.org/abs/2205.03519v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.03519v2)
- **Published**: 2022-05-07 01:49:31+00:00
- **Updated**: 2022-09-23 02:33:03+00:00
- **Authors**: Peizhou Huang, Chaoyi Zhang, Xiaoliang Zhang, Xiaojuan Li, Liang Dong, Leslie Ying
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable unsupervised learning for MR image reconstruction by combining an unsupervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of unsupervised learning by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality.



### Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction
- **Arxiv ID**: http://arxiv.org/abs/2205.03521v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.03521v1)
- **Published**: 2022-05-07 02:10:55+00:00
- **Updated**: 2022-05-07 02:10:55+00:00
- **Authors**: Xiang Chen, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen
- **Comment**: Accepted by NAACL 2022
- **Journal**: None
- **Summary**: Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts. To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we regard visual representation as pluggable visual prefix to guide the textual representation for error insensitive forecasting decision. We further propose a dynamic gated aggregation strategy to achieve hierarchical multi-scaled visual features as visual prefix for fusion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, and achieve state-of-the-art performance. Code is available in https://github.com/zjunlp/HVPNeT.



### Optimizing Terrain Mapping and Landing Site Detection for Autonomous UAVs
- **Arxiv ID**: http://arxiv.org/abs/2205.03522v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03522v1)
- **Published**: 2022-05-07 02:16:29+00:00
- **Updated**: 2022-05-07 02:16:29+00:00
- **Authors**: Pedro F. Proença, Jeff Delaune, Roland Brockers
- **Comment**: Accepted to ICRA 2022
- **Journal**: None
- **Summary**: The next generation of Mars rotorcrafts requires on-board autonomous hazard avoidance landing. To this end, this work proposes a system that performs continuous multi-resolution height map reconstruction and safe landing spot detection. Structure-from-Motion measurements are aggregated in a pyramid structure using a novel Optimal Mixture of Gaussians formulation that provides a comprehensive uncertainty model. Our multiresolution pyramid is built more efficiently and accurately than past work by decoupling pyramid filling from the measurement updates of different resolutions. To detect the safest landing location, after an optimized hazard segmentation, we use a mean shift algorithm on multiple distance transform peaks to account for terrain roughness and uncertainty. The benefits of our contributions are evaluated on real and synthetic flight data.



### Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.03524v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03524v1)
- **Published**: 2022-05-07 02:55:39+00:00
- **Updated**: 2022-05-07 02:55:39+00:00
- **Authors**: Xiaoqian Xu, Pengxu Wei, Weikai Chen, Mingzhi Mao, Liang Lin, Guanbin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the sophisticated imaging process, an identical scene captured by different cameras could exhibit distinct imaging patterns, introducing distinct proficiency among the super-resolution (SR) models trained on images from different devices. In this paper, we investigate a novel and practical task coded cross-device SR, which strives to adapt a real-world SR model trained on the paired images captured by one camera to low-resolution (LR) images captured by arbitrary target devices. The proposed task is highly challenging due to the absence of paired data from various imaging devices. To address this issue, we propose an unsupervised domain adaptation mechanism for real-world SR, named Dual ADversarial Adaptation (DADA), which only requires LR images in the target domain with available real paired data from a source camera. DADA employs the Domain-Invariant Attention (DIA) module to establish the basis of target model training even without HR supervision. Furthermore, the dual framework of DADA facilitates an Inter-domain Adversarial Adaptation (InterAA) in one branch for two LR input images from two domains, and an Intra-domain Adversarial Adaptation (IntraAA) in two branches for an LR input image. InterAA and IntraAA together improve the model transferability from the source domain to the target. We empirically conduct experiments under six Real to Real adaptation settings among three different cameras, and achieve superior performance compared with existing state-of-the-art approaches. We also evaluate the proposed DADA to address the adaptation to the video camera, which presents a promising research topic to promote the wide applications of real-world super-resolution. Our source code is publicly available at https://github.com/lonelyhope/DADA.git.



### Automatic segmentation of meniscus based on MAE self-supervision and point-line weak supervision paradigm
- **Arxiv ID**: http://arxiv.org/abs/2205.03525v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2205.03525v1)
- **Published**: 2022-05-07 02:57:50+00:00
- **Updated**: 2022-05-07 02:57:50+00:00
- **Authors**: Yuhan Xie, Kexin Jiang, Zhiyong Zhang, Shaolong Chen, Xiaodong Zhang, Changzhen Qiu
- **Comment**: 8 pages,10 figures
- **Journal**: None
- **Summary**: Medical image segmentation based on deep learning is often faced with the problems of insufficient datasets and long time-consuming labeling. In this paper, we introduce the self-supervised method MAE(Masked Autoencoders) into knee joint images to provide a good initial weight for the segmentation model and improve the adaptability of the model to small datasets. Secondly, we propose a weakly supervised paradigm for meniscus segmentation based on the combination of point and line to reduce the time of labeling. Based on the weak label ,we design a region growing algorithm to generate pseudo-label. Finally we train the segmentation network based on pseudo-labels with weight transfer from self-supervision. Sufficient experimental results show that our proposed method combining self-supervision and weak supervision can almost approach the performance of purely fully supervised models while greatly reducing the required labeling time and dataset size.



### Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information
- **Arxiv ID**: http://arxiv.org/abs/2205.03534v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.03534v1)
- **Published**: 2022-05-07 03:33:00+00:00
- **Updated**: 2022-05-07 03:33:00+00:00
- **Authors**: Zhipeng Zhang, Xinglin Hou, Kai Niu, Zhongzhen Huang, Tiezheng Ge, Yuning Jiang, Qi Wu, Peng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, online shopping has gradually become a common way of shopping for people all over the world. Wonderful merchandise advertisements often attract more people to buy. These advertisements properly integrate multimodal multi-structured information of commodities, such as visual spatial information and fine-grained structure information. However, traditional multimodal text generation focuses on the conventional description of what existed and happened, which does not match the requirement of advertisement copywriting in the real world. Because advertisement copywriting has a vivid language style and higher requirements of faithfulness. Unfortunately, there is a lack of reusable evaluation frameworks and a scarcity of datasets. Therefore, we present a dataset, E-MMAD (e-commercial multimodal multi-structured advertisement copywriting), which requires, and supports much more detailed information in text generation. Noticeably, it is one of the largest video captioning datasets in this field. Accordingly, we propose a baseline method and faithfulness evaluation metric on the strength of structured information reasoning to solve the demand in reality on this dataset. It surpasses the previous methods by a large margin on all metrics. The dataset and method are coming soon on \url{https://e-mmad.github.io/e-mmad.net/index.html}.



### BiCo-Net: Regress Globally, Match Locally for Robust 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.03536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03536v1)
- **Published**: 2022-05-07 03:37:33+00:00
- **Updated**: 2022-05-07 03:37:33+00:00
- **Authors**: Zelin Xu, Yichen Zhang, Ke Chen, Kui Jia
- **Comment**: Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: The challenges of learning a robust 6D pose function lie in 1) severe occlusion and 2) systematic noises in depth images. Inspired by the success of point-pair features, the goal of this paper is to recover the 6D pose of an object instance segmented from RGB-D images by locally matching pairs of oriented points between the model and camera space. To this end, we propose a novel Bi-directional Correspondence Mapping Network (BiCo-Net) to first generate point clouds guided by a typical pose regression, which can thus incorporate pose-sensitive information to optimize generation of local coordinates and their normal vectors. As pose predictions via geometric computation only rely on one single pair of local oriented points, our BiCo-Net can achieve robustness against sparse and occluded point clouds. An ensemble of redundant pose predictions from locally matching and direct pose regression further refines final pose output against noisy observations. Experimental results on three popularly benchmarking datasets can verify that our method can achieve state-of-the-art performance, especially for the more challenging severe occluded scenes. Source codes are available at https://github.com/Gorilla-Lab-SCUT/BiCo-Net.



### Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2205.03546v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03546v1)
- **Published**: 2022-05-07 04:17:25+00:00
- **Updated**: 2022-05-07 04:17:25+00:00
- **Authors**: Binghui Wang, Youqi Li, Pan Zhou
- **Comment**: Accepted by CVPR 2022 Oral presentation
- **Journal**: None
- **Summary**: Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph-based tasks such as node classification and graph classification. However, many recent works have demonstrated that an attacker can mislead GNN models by slightly perturbing the graph structure. Existing attacks to GNNs are either under the less practical threat model where the attacker is assumed to access the GNN model parameters, or under the practical black-box threat model but consider perturbing node features that are shown to be not enough effective. In this paper, we aim to bridge this gap and consider black-box attacks to GNNs with structure perturbation as well as with theoretical guarantees. We propose to address this challenge through bandit techniques. Specifically, we formulate our attack as an online optimization with bandit feedback. This original problem is essentially NP-hard due to the fact that perturbing the graph structure is a binary optimization problem. We then propose an online attack based on bandit optimization which is proven to be {sublinear} to the query number $T$, i.e., $\mathcal{O}(\sqrt{N}T^{3/4})$ where $N$ is the number of nodes in the graph. Finally, we evaluate our proposed attack by conducting experiments over multiple datasets and GNN models. The experimental results on various citation graphs and image graphs show that our attack is both effective and efficient. Source code is available at~\url{https://github.com/Metaoblivion/Bandit_GNN_Attack}



### Deep Learning-enabled Detection and Classification of Bacterial Colonies using a Thin Film Transistor (TFT) Image Sensor
- **Arxiv ID**: http://arxiv.org/abs/2205.03549v1
- **DOI**: 10.1021/acsphotonics.2c00572
- **Categories**: **physics.ins-det**, cs.CV, eess.IV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.03549v1)
- **Published**: 2022-05-07 04:45:58+00:00
- **Updated**: 2022-05-07 04:45:58+00:00
- **Authors**: Yuzhu Li, Tairan Liu, Hatice Ceylan Koydemir, Hongda Wang, Keelan O'Riordan, Bijie Bai, Yuta Haga, Junji Kobashi, Hitoshi Tanaka, Takaya Tamaru, Kazunori Yamaguchi, Aydogan Ozcan
- **Comment**: 18 Pages, 6 Figures
- **Journal**: ACS Photonics (2022)
- **Summary**: Early detection and identification of pathogenic bacteria such as Escherichia coli (E. coli) is an essential task for public health. The conventional culture-based methods for bacterial colony detection usually take >24 hours to get the final read-out. Here, we demonstrate a bacterial colony-forming-unit (CFU) detection system exploiting a thin-film-transistor (TFT)-based image sensor array that saves ~12 hours compared to the Environmental Protection Agency (EPA)-approved methods. To demonstrate the efficacy of this CFU detection system, a lensfree imaging modality was built using the TFT image sensor with a sample field-of-view of ~10 cm^2. Time-lapse images of bacterial colonies cultured on chromogenic agar plates were automatically collected at 5-minute intervals. Two deep neural networks were used to detect and count the growing colonies and identify their species. When blindly tested with 265 colonies of E. coli and other coliform bacteria (i.e., Citrobacter and Klebsiella pneumoniae), our system reached an average CFU detection rate of 97.3% at 9 hours of incubation and an average recovery rate of 91.6% at ~12 hours. This TFT-based sensor can be applied to various microbiological detection methods. Due to the large scalability, ultra-large field-of-view, and low cost of the TFT-based image sensors, this platform can be integrated with each agar plate to be tested and disposed of after the automated CFU count. The imaging field-of-view of this platform can be cost-effectively increased to >100 cm^2 to provide a massive throughput for CFU detection using, e.g., roll-to-roll manufacturing of TFTs as used in the flexible display industry.



### From Heavy Rain Removal to Detail Restoration: A Faster and Better Network
- **Arxiv ID**: http://arxiv.org/abs/2205.03553v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03553v1)
- **Published**: 2022-05-07 04:55:05+00:00
- **Updated**: 2022-05-07 04:55:05+00:00
- **Authors**: Tao Gao, Yuanbo Wen, Jing Zhang, Kaihao Zhang, Ting Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The dense rain accumulation in heavy rain can significantly wash out images and thus destroy the background details of images. Although existing deep rain removal models lead to improved performance for heavy rain removal, we find that most of them ignore the detail reconstruction accuracy of rain-free images. In this paper, we propose a dual-stage progressive enhancement network (DPENet) to achieve effective deraining with structure-accurate rain-free images. Two main modules are included in our framework, namely a rain streaks removal network (R$^2$Net) and a detail reconstruction network (DRNet). The former aims to achieve accurate rain removal, and the latter is designed to recover the details of rain-free images. We introduce two main strategies within our networks to achieve trade-off between the effectiveness of deraining and the detail restoration of rain-free images. Firstly, a dilated dense residual block (DDRB) within the rain streaks removal network is presented to aggregate high/low level features of heavy rain. Secondly, an enhanced residual pixel-wise attention block (ERPAB) within the detail reconstruction network is designed for context information aggregation. We also propose a comprehensive loss function to highlight the marginal and regional accuracy of rain-free images. Extensive experiments on benchmark public datasets show both efficiency and effectiveness of the proposed method in achieving structure-preserving rain-free images for heavy rain removal. The source code and pre-trained models can be found at \url{https://github.com/wybchd/DPENet}.



### Multi-Target Active Object Tracking with Monte Carlo Tree Search and Target Motion Modeling
- **Arxiv ID**: http://arxiv.org/abs/2205.03555v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.03555v1)
- **Published**: 2022-05-07 05:08:15+00:00
- **Updated**: 2022-05-07 05:08:15+00:00
- **Authors**: Zheng Chen, Jian Zhao, Mingyu Yang, Wengang Zhou, Houqiang Li
- **Comment**: 7 pages,7 figures
- **Journal**: None
- **Summary**: In this work, we are dedicated to multi-target active object tracking (AOT), where there are multiple targets as well as multiple cameras in the environment. The goal is maximize the overall target coverage of all cameras. Previous work makes a strong assumption that each camera is fixed in a location and only allowed to rotate, which limits its application. In this work, we relax the setting by allowing all cameras to both move along the boundary lines and rotate. In our setting, the action space becomes much larger, which leads to much higher computational complexity to identify the optimal action. To this end, we propose to leverage the action selection from multi-agent reinforcement learning (MARL) network to prune the search tree of Monte Carlo Tree Search (MCTS) method, so as to find the optimal action more efficiently. Besides, we model the motion of the targets to predict the future position of the targets, which makes a better estimation of the future environment state in the MCTS process. We establish a multi-target 2D environment to simulate the sports games, and experimental results demonstrate that our method can effectively improve the target coverage.



### Graph Fusion Network for Multi-Oriented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.03562v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03562v3)
- **Published**: 2022-05-07 05:47:22+00:00
- **Updated**: 2023-06-20 03:50:38+00:00
- **Authors**: Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Xu-Cheng Yin
- **Comment**: Accepted by Applied Intelligence (APIN 2022)
- **Journal**: None
- **Summary**: In object detection, non-maximum suppression (NMS) methods are extensively adopted to remove horizontal duplicates of detected dense boxes for generating final object instances. However, due to the degraded quality of dense detection boxes and not explicit exploration of the context information, existing NMS methods via simple intersection-over-union (IoU) metrics tend to underperform on multi-oriented and long-size objects detection. Distinguishing with general NMS methods via duplicate removal, we propose a novel graph fusion network, named GFNet, for multi-oriented object detection. Our GFNet is extensible and adaptively fuse dense detection boxes to detect more accurate and holistic multi-oriented object instances. Specifically, we first adopt a locality-aware clustering algorithm to group dense detection boxes into different clusters. We will construct an instance sub-graph for the detection boxes belonging to one cluster. Then, we propose a graph-based fusion network via Graph Convolutional Network (GCN) to learn to reason and fuse the detection boxes for generating final instance boxes. Extensive experiments both on public available multi-oriented text datasets (including MSRA-TD500, ICDAR2015, ICDAR2017-MLT) and multi-oriented object datasets (DOTA) verify the effectiveness and robustness of our method against general NMS methods in multi-oriented object detection.



### Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2205.03569v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03569v3)
- **Published**: 2022-05-07 06:26:49+00:00
- **Updated**: 2022-06-16 02:14:17+00:00
- **Authors**: Bing Li, Jiaxin Chen, Dongming Zhang, Xiuguo Bao, Di Huang
- **Comment**: Accepted to IJCAI 2022
- **Journal**: None
- **Summary**: Compressed video action recognition has recently drawn growing attention, since it remarkably reduces the storage and computational cost via replacing raw videos by sparsely sampled RGB frames and compressed motion cues (e.g., motion vectors and residuals). However, this task severely suffers from the coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB and motion modalities. To address the two issues above, this paper proposes a novel framework, namely Attentive Cross-modal Interaction Network with Motion Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for the RGB modality and the other for the motion modality. Particularly, the motion stream employs a multi-scale block embedded with a denoising module to enhance representation learning. The interaction between the two streams is then strengthened by introducing the Selective Motion Complement (SMC) and Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality with spatio-temporally attentive local motion features and CMA further combines the two modalities with selective feature augmentation. Extensive experiments on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the effectiveness and efficiency of MEACI-Net.



### Utility-Oriented Underwater Image Quality Assessment Based on Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.03574v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03574v1)
- **Published**: 2022-05-07 06:52:36+00:00
- **Updated**: 2022-05-07 06:52:36+00:00
- **Authors**: Weiling Chen, Rongfu Lin, Honggang Liao, Tiesong Zhao, Ke Gu, Patrick Le Callet
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread image applications have greatly promoted the vision-based tasks, in which the Image Quality Assessment (IQA) technique has become an increasingly significant issue. For user enjoyment in multimedia systems, the IQA exploits image fidelity and aesthetics to characterize user experience; while for other tasks such as popular object recognition, there exists a low correlation between utilities and perceptions. In such cases, the fidelity-based and aesthetics-based IQA methods cannot be directly applied. To address this issue, this paper proposes a utility-oriented IQA in object recognition. In particular, we initialize our research in the scenario of underwater fish detection, which is a critical task that has not yet been perfectly addressed. Based on this task, we build an Underwater Image Utility Database (UIUD) and a learning-based Underwater Image Utility Measure (UIUM). Inspired by the top-down design of fidelity-based IQA, we exploit the deep models of object recognition and transfer their features to our UIUM. Experiments validate that the proposed transfer-learning-based UIUM achieves promising performance in the recognition task. We envision our research provides insights to bridge the researches of IQA and computer vision.



### Unified Chinese License Plate Detection and Recognition with High Efficiency
- **Arxiv ID**: http://arxiv.org/abs/2205.03582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03582v1)
- **Published**: 2022-05-07 07:35:51+00:00
- **Updated**: 2022-05-07 07:35:51+00:00
- **Authors**: Yanxiang Gong, Linjie Deng, Shuai Tao, Xinchen Lu, Peicheng Wu, Zhiwei Xie, Zheng Ma, Mei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning-based methods have reached an excellent performance on License Plate (LP) detection and recognition tasks. However, it is still challenging to build a robust model for Chinese LPs since there are not enough large and representative datasets. In this work, we propose a new dataset named Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP images as a supplement to the existing public benchmarks. The images are mainly captured with electronic monitoring systems with detailed annotations. To our knowledge, CRPD is the largest public multi-objective Chinese LP dataset with annotations of vertices. With CRPD, a unified detection and recognition network with high efficiency is presented as the baseline. The network is end-to-end trainable with totally real-time inference efficiency (30 fps with 640p). The experiments on several public benchmarks demonstrate that our method has reached competitive performance. The code and dataset will be publicly available at https://github.com/yxgong0/CRPD.



### SPQE: Structure-and-Perception-Based Quality Evaluation for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.03584v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03584v1)
- **Published**: 2022-05-07 07:52:55+00:00
- **Updated**: 2022-05-07 07:52:55+00:00
- **Authors**: Keke Zhang, Tiesong Zhao, Weiling Chen, Yuzhen Niu, Jinsong Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The image Super-Resolution (SR) technique has greatly improved the visual quality of images by enhancing their resolutions. It also calls for an efficient SR Image Quality Assessment (SR-IQA) to evaluate those algorithms or their generated images. In this paper, we focus on the SR-IQA under deep learning and propose a Structure-and-Perception-based Quality Evaluation (SPQE). In emerging deep-learning-based SR, a generated high-quality, visually pleasing image may have different structures from its corresponding low-quality image. In such case, how to balance the quality scores between no-reference perceptual quality and referenced structural similarity is a critical issue. To help ease this problem, we give a theoretical analysis on this tradeoff and further calculate adaptive weights for the two types of quality scores. We also propose two deep-learning-based regressors to model the no-reference and referenced scores. By combining the quality scores and their weights, we propose a unified SPQE metric for SR-IQA. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts in different datasets.



### Efficient VVC Intra Prediction Based on Deep Feature Fusion and Probability Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.03587v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03587v1)
- **Published**: 2022-05-07 08:01:32+00:00
- **Updated**: 2022-05-07 08:01:32+00:00
- **Authors**: Tiesong Zhao, Yuhang Huang, Weize Feng, Yiwen Xu, Sam Kwong
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: The ever-growing multimedia traffic has underscored the importance of effective multimedia codecs. Among them, the up-to-date lossy video coding standard, Versatile Video Coding (VVC), has been attracting attentions of video coding community. However, the gain of VVC is achieved at the cost of significant encoding complexity, which brings the need to realize fast encoder with comparable Rate Distortion (RD) performance. In this paper, we propose to optimize the VVC complexity at intra-frame prediction, with a two-stage framework of deep feature fusion and probability estimation. At the first stage, we employ the deep convolutional network to extract the spatialtemporal neighboring coding features. Then we fuse all reference features obtained by different convolutional kernels to determine an optimal intra coding depth. At the second stage, we employ a probability-based model and the spatial-temporal coherence to select the candidate partition modes within the optimal coding depth. Finally, these selected depths and partitions are executed whilst unnecessary computations are excluded. Experimental results on standard database demonstrate the superiority of proposed method, especially for High Definition (HD) and Ultra-HD (UHD) video sequences.



### GAN-Based Multi-View Video Coding with Spatio-Temporal EPI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.03599v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03599v2)
- **Published**: 2022-05-07 08:52:54+00:00
- **Updated**: 2023-05-05 17:19:31+00:00
- **Authors**: Chengdong Lan, Hao Yan, Cheng Luo, Tiesong Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: The introduction of multiple viewpoints in video scenes inevitably increases the bitrates required for storage and transmission. To reduce bitrates, researchers have developed methods to skip intermediate viewpoints during compression and delivery, and ultimately reconstruct them using Side Information (SI). Typically, depth maps are used to construct SI. However, their methods suffer from inaccuracies in reconstruction and inherently high bitrates. In this paper, we propose a novel multi-view video coding method that leverages the image generation capabilities of Generative Adversarial Network (GAN) to improve the reconstruction accuracy of SI. Additionally, we consider incorporating information from adjacent temporal and spatial viewpoints to further reduce SI redundancy. At the encoder, we construct a spatio-temporal Epipolar Plane Image (EPI) and further utilize a convolutional network to extract the latent code of a GAN as SI. At the decoder side, we combine the SI and adjacent viewpoints to reconstruct intermediate views using the GAN generator. Specifically, we establish a joint encoder constraint for reconstruction cost and SI entropy to achieve an optimal trade-off between reconstruction quality and bitrates overhead. Experiments demonstrate significantly improved Rate-Distortion (RD) performance compared with state-of-the-art methods.



### Automatic Block-wise Pruning with Auxiliary Gating Structures for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.03602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.03602v1)
- **Published**: 2022-05-07 09:03:32+00:00
- **Updated**: 2022-05-07 09:03:32+00:00
- **Authors**: Zhaofeng Si, Honggang Qi, Xiaoyu Song
- **Comment**: 7 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Convolutional neural networks are prevailing in deep learning tasks. However, they suffer from massive cost issues when working on mobile devices. Network pruning is an effective method of model compression to handle such problems. This paper presents a novel structured network pruning method with auxiliary gating structures which assigns importance marks to blocks in backbone network as a criterion when pruning. Block-wise pruning is then realized by proposed voting strategy, which is different from prevailing methods who prune a model in small granularity like channel-wise. We further develop a three-stage training scheduling for the proposed architecture incorporating knowledge distillation for better performance. Our experiments demonstrate that our method can achieve state-of-the-arts compression performance for the classification tasks. In addition, our approach can integrate synergistically with other pruning methods by providing pretrained models, thus achieving a better performance than the unpruned model with over 93\% FLOPs reduced.



### Sparse Regularized Correlation Filter for UAV Object Tracking with adaptive Contextual Learning and Keyfilter Selection
- **Arxiv ID**: http://arxiv.org/abs/2205.03627v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03627v2)
- **Published**: 2022-05-07 10:25:56+00:00
- **Updated**: 2022-10-13 01:27:55+00:00
- **Authors**: Zhangjian Ji, Kai Feng, Yuhua Qian, Jiye Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, correlation filter has been widely applied in unmanned aerial vehicle (UAV) tracking due to its high frame rates, robustness and low calculation resources. However, it is fragile because of two inherent defects, i.e, boundary effect and filter corruption. Some methods by enlarging the search area can mitigate the boundary effect, yet introducing the undesired background distractors. Another approaches can alleviate the temporal degeneration of learned filters by introducing the temporal regularizer, which depends on the assumption that the filers between consecutive frames should be coherent. In fact, sometimes the filers at the ($t-1$)th frame is vulnerable to heavy occlusion from backgrounds, which causes that the assumption does not hold. To handle them, in this work, we propose a novel $\ell_{1}$ regularization correlation filter with adaptive contextual learning and keyfilter selection for UAV tracking. Firstly, we adaptively detect the positions of effective contextual distractors by the aid of the distribution of local maximum values on the response map of current frame which is generated by using the previous correlation filter model. Next, we eliminate inconsistent labels for the tracked target by removing one on each distractor and develop a new score scheme for each distractor. Then, we can select the keyfilter from the filters pool by finding the maximal similarity between the target at the current frame and the target template corresponding to each filter in the filters pool. Finally, quantitative and qualitative experiments on three authoritative UAV datasets show that the proposed method is superior to the state-of-the-art tracking methods based on correlation filter framework.



### Deep Quality Assessment of Compressed Videos: A Subjective and Objective Study
- **Arxiv ID**: http://arxiv.org/abs/2205.03630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03630v1)
- **Published**: 2022-05-07 10:50:06+00:00
- **Updated**: 2022-05-07 10:50:06+00:00
- **Authors**: Liqun Lin, Zheng Wang, Jiachen He, Weiling Chen, Yiwen Xu, Tiesong Zhao
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: In the video coding process, the perceived quality of a compressed video is evaluated by full-reference quality evaluation metrics. However, it is difficult to obtain reference videos with perfect quality. To solve this problem, it is critical to design no-reference compressed video quality assessment algorithms, which assists in measuring the quality of experience on the server side and resource allocation on the network side. Convolutional Neural Network (CNN) has shown its advantage in Video Quality Assessment (VQA) with promising successes in recent years. A large-scale quality database is very important for learning accurate and powerful compressed video quality metrics. In this work, a semi-automatic labeling method is adopted to build a large-scale compressed video quality database, which allows us to label a large number of compressed videos with manageable human workload. The resulting Compressed Video quality database with Semi-Automatic Ratings (CVSAR), so far the largest of compressed video quality database. We train a no-reference compressed video quality assessment model with a 3D CNN for SpatioTemporal Feature Extraction and Evaluation (STFEE). Experimental results demonstrate that the proposed method outperforms state-of-the-art metrics and achieves promising generalization performance in cross-database tests. The CVSAR database and STFEE model will be made publicly available to facilitate reproducible research.



### Comparison Knowledge Translation for Generalizable Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.03633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03633v1)
- **Published**: 2022-05-07 11:05:18+00:00
- **Updated**: 2022-05-07 11:05:18+00:00
- **Authors**: Zunlei Feng, Tian Qiu, Sai Wu, Xiaotuan Jin, Zengliang He, Mingli Song, Huiqiong Wang
- **Comment**: Accepted by IJCAI 2022; Adding Supplementary Materials
- **Journal**: None
- **Summary**: Deep learning has recently achieved remarkable performance in image classification tasks, which depends heavily on massive annotation. However, the classification mechanism of existing deep learning models seems to contrast to humans' recognition mechanism. With only a glance at an image of the object even unknown type, humans can quickly and precisely find other same category objects from massive images, which benefits from daily recognition of various objects. In this paper, we attempt to build a generalizable framework that emulates the humans' recognition mechanism in the image classification task, hoping to improve the classification performance on unseen categories with the support of annotations of other categories. Specifically, we investigate a new task termed Comparison Knowledge Translation (CKT). Given a set of fully labeled categories, CKT aims to translate the comparison knowledge learned from the labeled categories to a set of novel categories. To this end, we put forward a Comparison Classification Translation Network (CCT-Net), which comprises a comparison classifier and a matching discriminator. The comparison classifier is devised to classify whether two images belong to the same category or not, while the matching discriminator works together in an adversarial manner to ensure whether classified results match the truth. Exhaustive experiments show that CCT-Net achieves surprising generalization ability on unseen categories and SOTA performance on target categories.



### Ultrafast Image Categorization in Biology and Neural Models
- **Arxiv ID**: http://arxiv.org/abs/2205.03635v4
- **DOI**: 10.3390/vision7020029
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03635v4)
- **Published**: 2022-05-07 11:19:40+00:00
- **Updated**: 2023-05-31 05:30:51+00:00
- **Authors**: Jean-Nicolas Jérémie, Laurent U Perrinet
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are able to categorize images very efficiently, in particular to detect the presence of an animal very quickly. Recently, deep learning algorithms based on convolutional neural networks (CNNs) have achieved higher than human accuracy for a wide range of visual categorization tasks. However, the tasks on which these artificial networks are typically trained and evaluated tend to be highly specialized and do not generalize well, e.g., accuracy drops after image rotation. In this respect, biological visual systems are more flexible and efficient than artificial systems for more general tasks, such as recognizing an animal. To further the comparison between biological and artificial neural networks, we re-trained the standard VGG 16 CNN on two independent tasks that are ecologically relevant to humans: detecting the presence of an animal or an artifact. We show that re-training the network achieves a human-like level of performance, comparable to that reported in psychophysical tasks. In addition, we show that the categorization is better when the outputs of the models are combined. Indeed, animals (e.g., lions) tend to be less present in photographs that contain artifacts (e.g., buildings). Furthermore, these re-trained models were able to reproduce some unexpected behavioral observations from human psychophysics, such as robustness to rotation (e.g., an upside-down or tilted image) or to a grayscale transformation. Finally, we quantified the number of CNN layers required to achieve such performance and showed that good accuracy for ultrafast image categorization can be achieved with only a few layers, challenging the belief that image recognition requires deep sequential analysis of visual objects.



### DL4DS -- Deep Learning for empirical DownScaling
- **Arxiv ID**: http://arxiv.org/abs/2205.08967v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.08967v1)
- **Published**: 2022-05-07 11:24:43+00:00
- **Updated**: 2022-05-07 11:24:43+00:00
- **Authors**: Carlos Alberto Gomez Gonzalez
- **Comment**: None
- **Journal**: None
- **Summary**: A common task in Earth Sciences is to infer climate information at local and regional scales from global climate models. Dynamical downscaling requires running expensive numerical models at high resolution which can be prohibitive due to long model runtimes. On the other hand, statistical downscaling techniques present an alternative approach for learning links between the large- and local-scale climate in a more efficient way. A large number of deep neural network-based approaches for statistical downscaling have been proposed in recent years, mostly based on convolutional architectures developed for computer vision and super-resolution tasks. This paper presents DL4DS, Deep Learning for empirical DownScaling, a python library that implements a wide variety of state-of-the-art and novel algorithms for downscaling gridded Earth Science data with deep neural networks. DL4DS has been designed with the goal of providing a general framework for training convolutional neural networks with configurable architectures and learning strategies to facilitate the conduction of comparative and ablation studies in a robust way. We showcase the capabilities of DL4DS on air quality CAMS data over the western Mediterranean area. The DL4DS library can be found in this repository: https://github.com/carlos-gg/dl4ds



### Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.03644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03644v1)
- **Published**: 2022-05-07 12:53:06+00:00
- **Updated**: 2022-05-07 12:53:06+00:00
- **Authors**: Yiqun Lin, Huifeng Yao, Zezhong Li, Guoyan Zheng, Xiaomeng Li
- **Comment**: Provisionally accepted to MICCAI 2022; 10 pages, 3 figures
- **Journal**: None
- **Summary**: Segmentation of 3D knee MR images is important for the assessment of osteoarthritis. Like other medical data, the volume-wise labeling of knee MR images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL), particularly barely-supervised learning, is highly desirable for training with insufficient labeled data. We observed that the class imbalance problem is severe in the knee MR images as the cartilages only occupy 6% of foreground volumes, and the situation becomes worse without sufficient labeled data. To address the above problem, we present a novel framework for barely-supervised knee segmentation with noisy and imbalanced labels. Our framework leverages label distribution to encourage the network to put more effort into learning cartilage parts. Specifically, we utilize 1.) label quantity distribution for modifying the objective loss function to a class-aware weighted form and 2.) label position distribution for constructing a cropping probability mask to crop more sub-volumes in cartilage areas from both labeled and unlabeled inputs. In addition, we design dual uncertainty-aware sampling supervision to enhance the supervision of low-confident categories for efficient unsupervised learning. Experiments show that our proposed framework brings significant improvements by incorporating the unlabeled data and alleviating the problem of class imbalance. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting.



### Automatic Velocity Picking Using a Multi-Information Fusion Deep Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2205.03645v1
- **DOI**: 10.1109/TGRS.2022.3188669
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03645v1)
- **Published**: 2022-05-07 12:55:13+00:00
- **Updated**: 2022-05-07 12:55:13+00:00
- **Authors**: H. T. Wang, J. S. Zhang, Z. X. Zhao, C. X. Zhang, L. Li, Z. Y. Yang, W. F. Geng
- **Comment**: None
- **Journal**: None
- **Summary**: Velocity picking, a critical step in seismic data processing, has been studied for decades. Although manual picking can produce accurate normal moveout (NMO) velocities from the velocity spectra of prestack gathers, it is time-consuming and becomes infeasible with the emergence of large amount of seismic data. Numerous automatic velocity picking methods have thus been developed. In recent years, deep learning (DL) methods have produced good results on the seismic data with medium and high signal-to-noise ratios (SNR). Unfortunately, it still lacks a picking method to automatically generate accurate velocities in the situations of low SNR. In this paper, we propose a multi-information fusion network (MIFN) to estimate stacking velocity from the fusion information of velocity spectra and stack gather segments (SGS). In particular, we transform the velocity picking problem into a semantic segmentation problem based on the velocity spectrum images. Meanwhile, the information provided by SGS is used as a prior in the network to assist segmentation. The experimental results on two field datasets show that the picking results of MIFN are stable and accurate for the scenarios with medium and high SNR, and it also performs well in low SNR scenarios.



### Label Adversarial Learning for Skeleton-level to Pixel-level Adjustable Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.03646v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.03646v1)
- **Published**: 2022-05-07 12:56:16+00:00
- **Updated**: 2022-05-07 12:56:16+00:00
- **Authors**: Mingchao Li, Kun Huang, Zetian Zhang, Xiao Ma, Qiang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: You can have your cake and eat it too. Microvessel segmentation in optical coherence tomography angiography (OCTA) images remains challenging. Skeleton-level segmentation shows clear topology but without diameter information, while pixel-level segmentation shows a clear caliber but low topology. To close this gap, we propose a novel label adversarial learning (LAL) for skeleton-level to pixel-level adjustable vessel segmentation. LAL mainly consists of two designs: a label adversarial loss and an embeddable adjustment layer. The label adversarial loss establishes an adversarial relationship between the two label supervisions, while the adjustment layer adjusts the network parameters to match the different adversarial weights. Such a design can efficiently capture the variation between the two supervisions, making the segmentation continuous and tunable. This continuous process allows us to recommend high-quality vessel segmentation with clear caliber and topology. Experimental results show that our results outperform manual annotations of current public datasets and conventional filtering effects. Furthermore, such a continuous process can also be used to generate an uncertainty map representing weak vessel boundaries and noise.



### Automatic Stack Velocity Picking Using an Unsupervised Ensemble Learning Method
- **Arxiv ID**: http://arxiv.org/abs/2205.08372v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2205.08372v2)
- **Published**: 2022-05-07 12:58:04+00:00
- **Updated**: 2023-05-23 07:43:24+00:00
- **Authors**: H. T. Wang, J. S. Zhang, C. X. Zhang, Z. X. Zhao, W. F. Geng
- **Comment**: None
- **Journal**: None
- **Summary**: Seismic velocity picking algorithms that are both accurate and efficient can greatly speed up seismic data processing, with the primary approach being the use of velocity spectra. Despite the development of some supervised deep learning-based approaches to automatically pick the velocity, they often come with costly manual labeling expenses or lack interpretability. In comparison, using physical knowledge to drive unsupervised learning techniques has the potential to solve this problem in an efficient manner. We suggest an Unsupervised Ensemble Learning (UEL) approach to achieving a balance between reliance on labeled data and picking accuracy, with the aim of determining the stack velocity. UEL makes use of the data from nearby velocity spectra and other known sources to help pick efficient and reasonable velocity points, which are acquired through a clustering technique. Testing on both the synthetic and field data sets shows that UEL is more reliable and precise in auto-picking than traditional clustering-based techniques and the widely used Convolutional Neural Network (CNN) method.



### Distilling Inter-Class Distance for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.03650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03650v2)
- **Published**: 2022-05-07 13:13:55+00:00
- **Updated**: 2022-07-15 02:15:30+00:00
- **Authors**: Zhengbo Zhang, Chunluan Zhou, Zhigang Tu
- **Comment**: IJCAI-ECAI2022 Long Oral
- **Journal**: None
- **Summary**: Knowledge distillation is widely adopted in semantic segmentation to reduce the computation cost.The previous knowledge distillation methods for semantic segmentation focus on pixel-wise feature alignment and intra-class feature variation distillation, neglecting to transfer the knowledge of the inter-class distance in the feature space, which is important for semantic segmentation. To address this issue, we propose an Inter-class Distance Distillation (IDD) method to transfer the inter-class distance in the feature space from the teacher network to the student network. Furthermore, semantic segmentation is a position-dependent task,thus we exploit a position information distillation module to help the student network encode more position information. Extensive experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show that our method is helpful to improve the accuracy of semantic segmentation models and achieves the state-of-the-art performance. E.g. it boosts the benchmark model("PSPNet+ResNet18") by 7.50% in accuracy on the Cityscapes dataset.



### Towards Robust 3D Object Recognition with Dense-to-Sparse Deep Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2205.03654v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03654v1)
- **Published**: 2022-05-07 13:42:43+00:00
- **Updated**: 2022-05-07 13:42:43+00:00
- **Authors**: Prajval Kumar Murali, Cong Wang, Ravinder Dahiya, Mohsen Kaboli
- **Comment**: None
- **Journal**: None
- **Summary**: Three-dimensional (3D) object recognition is crucial for intelligent autonomous agents such as autonomous vehicles and robots alike to operate effectively in unstructured environments. Most state-of-art approaches rely on relatively dense point clouds and performance drops significantly for sparse point clouds. Unsupervised domain adaption allows to minimise the discrepancy between dense and sparse point clouds with minimal unlabelled sparse point clouds, thereby saving additional sparse data collection, annotation and retraining costs. In this work, we propose a novel method for point cloud based object recognition with competitive performance with state-of-art methods on dense and sparse point clouds while being trained only with dense point clouds.



### Arrhythmia Classifier using Binarized Convolutional Neural Network for Resource-Constrained Devices
- **Arxiv ID**: http://arxiv.org/abs/2205.03661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03661v2)
- **Published**: 2022-05-07 14:21:32+00:00
- **Updated**: 2022-05-13 05:46:10+00:00
- **Authors**: Ao Wang, Wenxing Xu, Hanshi Sun, Ninghao Pu, Zijin Liu, Hao Liu
- **Comment**: IEEE-CISCE 2022
- **Journal**: None
- **Summary**: Monitoring electrocardiogram signals is of great significance for the diagnosis of arrhythmias. In recent years, deep learning and convolutional neural networks have been widely used in the classification of cardiac arrhythmias. However, the existing neural network applied to ECG signal detection usually requires a lot of computing resources, which is not friendlyF to resource-constrained equipment, and it is difficult to realize real-time monitoring. In this paper, a binarized convolutional neural network suitable for ECG monitoring is proposed, which is hardware-friendly and more suitable for use in resource-constrained wearable devices. Targeting the MIT-BIH arrhythmia database, the classifier based on this network reached an accuracy of 95.67% in the five-class test. Compared with the proposed baseline full-precision network with an accuracy of 96.45%, it is only 0.78% lower. Importantly, it achieves 12.65 times the computing speedup, 24.8 times the storage compression ratio, and only requires a quarter of the memory overhead.



### Playing Tic-Tac-Toe Games with Intelligent Single-pixel Imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.03663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03663v1)
- **Published**: 2022-05-07 14:45:54+00:00
- **Updated**: 2022-05-07 14:45:54+00:00
- **Authors**: Shuming Jiao, Jiaxiang Li, Wei Huang, Zibang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Single-pixel imaging (SPI) is a novel optical imaging technique by replacing a two-dimensional pixelated sensor with a single-pixel detector and pattern illuminations. SPI have been extensively used for various tasks related to image acquisition and processing. In this work, a novel non-image-based task of playing Tic-Tac-Toe games interactively is merged into the framework of SPI. An optoelectronic artificial intelligent (AI) player with minimal digital computation can detect the game states, generate optimal moves and display output results mainly by pattern illumination and single-pixel detection. Simulated and experimental results demonstrate the feasibility of proposed scheme and its unbeatable performance against human players.



### Block Modulating Video Compression: An Ultra Low Complexity Image Compression Encoder for Resource Limited Platforms
- **Arxiv ID**: http://arxiv.org/abs/2205.03677v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03677v1)
- **Published**: 2022-05-07 16:20:09+00:00
- **Updated**: 2022-05-07 16:20:09+00:00
- **Authors**: Yujia Xue, Siming Zheng, Waleed Tahir, Zhengjue Wang, Hao Zhang, Ziyi Meng, Lei Tian, Xin Yuan
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: We consider the image and video compression on resource limited platforms. An ultra low-cost image encoder, named Block Modulating Video Compression (BMVC) with an encoding complexity ${\cal O}(1)$ is proposed to be implemented on mobile platforms with low consumption of power and computation resources. We also develop two types of BMVC decoders, implemented by deep neural networks. The first BMVC decoder is based on the Plug-and-Play (PnP) algorithm, which is flexible to different compression ratios. And the second decoder is a memory efficient end-to-end convolutional neural network, which aims for real-time decoding. Extensive results on the high definition images and videos demonstrate the superior performance of the proposed codec and the robustness against bit quantization.



### GenISP: Neural ISP for Low-Light Machine Cognition
- **Arxiv ID**: http://arxiv.org/abs/2205.03688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03688v1)
- **Published**: 2022-05-07 17:17:24+00:00
- **Updated**: 2022-05-07 17:17:24+00:00
- **Authors**: Igor Morawski, Yu-An Chen, Yu-Sheng Lin, Shusil Dangi, Kai He, Winston H. Hsu
- **Comment**: Accepted to CVPR 2022 Workshop NTIRE: New Trends in Image Restoration
  and Enhancement workshop and Challenges
- **Journal**: None
- **Summary**: Object detection in low-light conditions remains a challenging but important problem with many practical implications. Some recent works show that, in low-light conditions, object detectors using raw image data are more robust than detectors using image data processed by a traditional ISP pipeline. To improve detection performance in low-light conditions, one can fine-tune the detector to use raw image data or use a dedicated low-light neural pipeline trained with paired low- and normal-light data to restore and enhance the image. However, different camera sensors have different spectral sensitivity and learning-based models using raw images process data in the sensor-specific color space. Thus, once trained, they do not guarantee generalization to other camera sensors. We propose to improve generalization to unseen camera sensors by implementing a minimal neural ISP pipeline for machine cognition, named GenISP, that explicitly incorporates Color Space Transformation to a device-independent color space. We also propose a two-stage color processing implemented by two image-to-parameter modules that take down-sized image as input and regress global color correction parameters. Moreover, we propose to train our proposed GenISP under the guidance of a pre-trained object detector and avoid making assumptions about perceptual quality of the image, but rather optimize the image representation for machine cognition. At the inference stage, GenISP can be paired with any object detector. We perform extensive experiments to compare our method to other low-light image restoration and enhancement methods in an extrinsic task-based evaluation and validate that GenISP can generalize to unseen sensors and object detectors. Finally, we contribute a low-light dataset of 7K raw images annotated with 46K bounding boxes for task-based benchmarking of future low-light image restoration and object detection.



### Keratoconus Classifier for Smartphone-based Corneal Topographer
- **Arxiv ID**: http://arxiv.org/abs/2205.03702v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2205.03702v1)
- **Published**: 2022-05-07 18:39:37+00:00
- **Updated**: 2022-05-07 18:39:37+00:00
- **Authors**: Siddhartha Gairola, Pallavi Joshi, Anand Balasubramaniam, Kaushik Murali, Nipun Kwatra, Mohit Jain
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Keratoconus is a severe eye disease that leads to deformation of the cornea. It impacts people aged 10-25 years and is the leading cause of blindness in that demography. Corneal topography is the gold standard for keratoconus diagnosis. It is a non-invasive process performed using expensive and bulky medical devices called corneal topographers. This makes it inaccessible to large populations, especially in the Global South. Low-cost smartphone-based corneal topographers, such as SmartKC, have been proposed to make keratoconus diagnosis accessible. Similar to medical-grade topographers, SmartKC outputs curvature heatmaps and quantitative metrics that need to be evaluated by doctors for keratoconus diagnosis. An automatic scheme for evaluation of these heatmaps and quantitative values can play a crucial role in screening keratoconus in areas where doctors are not available. In this work, we propose a dual-head convolutional neural network (CNN) for classifying keratoconus on the heatmaps generated by SmartKC. Since SmartKC is a new device and only had a small dataset (114 samples), we developed a 2-stage transfer learning strategy -- using historical data collected from a medical-grade topographer and a subset of SmartKC data -- to satisfactorily train our network. This, combined with our domain-specific data augmentations, achieved a sensitivity of 91.3% and a specificity of 94.2%.



### A Review on Viewpoints and Path-planning for UAV-based 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.03716v1
- **DOI**: 10.1109/JSTARS.2023.3276427
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.03716v1)
- **Published**: 2022-05-07 20:29:39+00:00
- **Updated**: 2022-05-07 20:29:39+00:00
- **Authors**: Mehdi Maboudi, MohammadReza Homaei, Soohwan Song, Shirin Malihi, Mohammad Saadatseresht, Markus Gerke
- **Comment**: 33 page- 177 references
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing-2023
- **Summary**: Unmanned aerial vehicles (UAVs) are widely used platforms to carry data capturing sensors for various applications. The reason for this success can be found in many aspects: the high maneuverability of the UAVs, the capability of performing autonomous data acquisition, flying at different heights, and the possibility to reach almost any vantage point. The selection of appropriate viewpoints and planning the optimum trajectories of UAVs is an emerging topic that aims at increasing the automation, efficiency and reliability of the data capturing process to achieve a dataset with desired quality. On the other hand, 3D reconstruction using the data captured by UAVs is also attracting attention in research and industry. This review paper investigates a wide range of model-free and model-based algorithms for viewpoint and path planning for 3D reconstruction of large-scale objects. The analyzed approaches are limited to those that employ a single-UAV as a data capturing platform for outdoor 3D reconstruction purposes. In addition to discussing the evaluation strategies, this paper also highlights the innovations and limitations of the investigated approaches. It concludes with a critical analysis of the existing challenges and future research perspectives.



### Category-Independent Articulated Object Tracking with Factor Graphs
- **Arxiv ID**: http://arxiv.org/abs/2205.03721v2
- **DOI**: 10.1109/IROS47612.2022.9982029
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03721v2)
- **Published**: 2022-05-07 20:59:44+00:00
- **Updated**: 2023-01-18 16:14:07+00:00
- **Authors**: Nick Heppert, Toki Migimatsu, Brent Yi, Claire Chen, Jeannette Bohg
- **Comment**: V2: Camera-ready IROS 2022 version 11 pages, 10 figures, IROS 2022
- **Journal**: None
- **Summary**: Robots deployed in human-centric environments may need to manipulate a diverse range of articulated objects, such as doors, dishwashers, and cabinets. Articulated objects often come with unexpected articulation mechanisms that are inconsistent with categorical priors: for example, a drawer might rotate about a hinge joint instead of sliding open. We propose a category-independent framework for predicting the articulation models of unknown objects from sequences of RGB-D images. The prediction is performed by a two-step process: first, a visual perception module tracks object part poses from raw images, and second, a factor graph takes these poses and infers the articulation model including the current configuration between the parts as a 6D twist. We also propose a manipulation-oriented metric to evaluate predicted joint twists in terms of how well a compliant robot controller would be able to manipulate the articulated object given the predicted twist. We demonstrate that our visual perception and factor graph modules outperform baselines on simulated data and show the applicability of our factor graph on real world data.



### Synthetic Point Cloud Generation for Class Segmentation Applications
- **Arxiv ID**: http://arxiv.org/abs/2205.03738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.03738v1)
- **Published**: 2022-05-07 23:21:58+00:00
- **Updated**: 2022-05-07 23:21:58+00:00
- **Authors**: Maria Gonzalez Stefanelli, Avi Rajesh Jain, Sandeep Kamal Jalui, Eva Agapaki
- **Comment**: None
- **Journal**: None
- **Summary**: Maintenance of industrial facilities is a growing hazard due to the cumbersome process needed to identify infrastructure degradation. Digital Twins have the potential to improve maintenance by monitoring the continuous digital representation of infrastructure. However, the time needed to map the existing geometry makes their use prohibitive. We previously developed class segmentation algorithms to automate digital twinning, however a vast amount of annotated point clouds is needed. Currently, synthetic data generation for automated segmentation is non-existent. We used Helios++ to automatically segment point clouds from 3D models. Our research has the potential to pave the ground for efficient industrial class segmentation.



### Decoupled-and-Coupled Networks: Self-Supervised Hyperspectral Image Super-Resolution with Subpixel Fusion
- **Arxiv ID**: http://arxiv.org/abs/2205.03742v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03742v1)
- **Published**: 2022-05-07 23:40:36+00:00
- **Updated**: 2022-05-07 23:40:36+00:00
- **Authors**: Danfeng Hong, Jing Yao, Deyu Meng, Naoto Yokoya, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: Enormous efforts have been recently made to super-resolve hyperspectral (HS) images with the aid of high spatial resolution multispectral (MS) images. Most prior works usually perform the fusion task by means of multifarious pixel-level priors. Yet the intrinsic effects of a large distribution gap between HS-MS data due to differences in the spatial and spectral resolution are less investigated. The gap might be caused by unknown sensor-specific properties or highly-mixed spectral information within one pixel (due to low spatial resolution). To this end, we propose a subpixel-level HS super-resolution framework by devising a novel decoupled-and-coupled network, called DC-Net, to progressively fuse HS-MS information from the pixel- to subpixel-level, from the image- to feature-level. As the name suggests, DC-Net first decouples the input into common (or cross-sensor) and sensor-specific components to eliminate the gap between HS-MS images before further fusion, and then fully blends them by a model-guided coupled spectral unmixing (CSU) net. More significantly, we append a self-supervised learning module behind the CSU net by guaranteeing the material consistency to enhance the detailed appearances of the restored HS product. Extensive experimental results show the superiority of our method both visually and quantitatively and achieve a significant improvement in comparison with the state-of-the-arts. Furthermore, the codes and datasets will be available at https://sites.google.com/view/danfeng-hong for the sake of reproducibility.




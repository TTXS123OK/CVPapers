# Arxiv Papers in cs.CV on 2022-05-14
### A Saliency-Guided Street View Image Inpainting Framework for Efficient Last-Meters Wayfinding
- **Arxiv ID**: http://arxiv.org/abs/2205.06934v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06934v2)
- **Published**: 2022-05-14 00:16:38+00:00
- **Updated**: 2022-11-17 13:40:37+00:00
- **Authors**: Chuanbo Hu, Shan Jia, Fan Zhang, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Global Positioning Systems (GPS) have played a crucial role in various navigation applications. Nevertheless, localizing the perfect destination within the last few meters remains an important but unresolved problem. Limited by the GPS positioning accuracy, navigation systems always show users a vicinity of a destination, but not its exact location. Street view images (SVI) in maps as an immersive media technology have served as an aid to provide the physical environment for human last-meters wayfinding. However, due to the large diversity of geographic context and acquisition conditions, the captured SVI always contains various distracting objects (e.g., pedestrians and vehicles), which will distract human visual attention from efficiently finding the destination in the last few meters. To address this problem, we highlight the importance of reducing visual distraction in image-based wayfinding by proposing a saliency-guided image inpainting framework. It aims at redirecting human visual attention from distracting objects to destination-related objects for more efficient and accurate wayfinding in the last meters. Specifically, a context-aware distracting object detection method driven by deep salient object detection has been designed to extract distracting objects from three semantic levels in SVI. Then we employ a large-mask inpainting method with fast Fourier convolutions to remove the detected distracting objects. Experimental results with both qualitative and quantitative analysis show that our saliency-guided inpainting method can not only achieve great perceptual quality in street view images but also redirect the human's visual attention to focus more on static location-related objects than distracting ones. The human-based evaluation also justified the effectiveness of our method in improving the efficiency of locating the target destination.



### Dense residual Transformer for image denoising
- **Arxiv ID**: http://arxiv.org/abs/2205.06944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06944v1)
- **Published**: 2022-05-14 01:59:38+00:00
- **Updated**: 2022-05-14 01:59:38+00:00
- **Authors**: Chao Yao, Shuo Jin, Meiqin Liu, Xiaojuan Ban
- **Comment**: Updated on 0514
- **Journal**: None
- **Summary**: Image denoising is an important low-level computer vision task, which aims to reconstruct a noise-free and high-quality image from a noisy image. With the development of deep learning, convolutional neural network (CNN) has been gradually applied and achieved great success in image denoising, image compression, image enhancement, etc. Recently, Transformer has been a hot technique, which is widely used to tackle computer vision tasks. However, few Transformer-based methods have been proposed for low-level vision tasks. In this paper, we proposed an image denoising network structure based on Transformer, which is named DenSformer. DenSformer consists of three modules, including a preprocessing module, a local-global feature extraction module, and a reconstruction module. Specifically, the local-global feature extraction module consists of several Sformer groups, each of which has several ETransformer layers and a convolution layer, together with a residual connection. These Sformer groups are densely skip-connected to fuse the feature of different layers, and they jointly capture the local and global information from the given noisy images. We conduct our model on comprehensive experiments. Experimental results prove that our DenSformer achieves improvement compared to some state-of-the-art methods, both for the synthetic noise data and real noise data, in the objective and subjective evaluations.



### BronchusNet: Region and Structure Prior Embedded Representation Learning for Bronchus Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.06947v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.06947v2)
- **Published**: 2022-05-14 02:32:33+00:00
- **Updated**: 2022-05-24 01:29:56+00:00
- **Authors**: Wenhao Huang, Haifan Gong, Huan Zhang, Yu Wang, Haofeng Li, Guanbin Li, Hong Shen
- **Comment**: None
- **Journal**: None
- **Summary**: CT-based bronchial tree analysis plays an important role in the computer-aided diagnosis for respiratory diseases, as it could provide structured information for clinicians. The basis of airway analysis is bronchial tree reconstruction, which consists of bronchus segmentation and classification. However, there remains a challenge for accurate bronchial analysis due to the individual variations and the severe class imbalance. In this paper, we propose a region and structure prior embedded framework named BronchusNet to achieve accurate segmentation and classification of bronchial regions in CT images. For bronchus segmentation, we propose an adaptive hard region-aware UNet that incorporates multi-level prior guidance of hard pixel-wise samples in the general Unet segmentation network to achieve better hierarchical feature learning. For the classification of bronchial branches, we propose a hybrid point-voxel graph learning module to fully exploit bronchial structure priors and to support simultaneous feature interactions across different branches. To facilitate the study of bronchial analysis, we contribute~\textbf{BRSC}: an open-access benchmark of \textbf{BR}onchus imaging analysis with high-quality pixel-wise \textbf{S}egmentation masks and the \textbf{C}lass of bronchial segments. Experimental results on BRSC show that our proposed method not only achieves the state-of-the-art performance for binary segmentation of bronchial region but also exceeds the best existing method on bronchial branches classification by 6.9\%.



### A Novel Face-Anti Spoofing Neural Network Model For Face Recognition And Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11240v1)
- **Published**: 2022-05-14 05:14:48+00:00
- **Updated**: 2022-05-14 05:14:48+00:00
- **Authors**: Soham S. Sarpotdar
- **Comment**: 9 Pages
- **Journal**: None
- **Summary**: Face Recognition (FR) systems are being used in a variety of applications, including road crossings, banking, and mobile banking. The widespread use of FR systems has raised concerns about the safety of face biometrics against spoofing attacks, which use the use of a photo or video of a legitimate user's face to gain illegal access to the resources or activities. Despite the development of several FAS or liveness detection methods (which determine whether a face is live or spoofed at the time of acquisition), the problem remains unsolved due to the difficulty of identifying discrimination and operationally reasonably priced spoof characteristics but also approaches. Additionally, certain facial portions are frequently repeated or correlate to image clutter, resulting in poor performance overall. This research proposes a face-anti-spoofing neural network model that outperforms existing models and has an efficiency of 0.89 percent.



### RiCS: A 2D Self-Occlusion Map for Harmonizing Volumetric Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.06975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06975v1)
- **Published**: 2022-05-14 05:35:35+00:00
- **Updated**: 2022-05-14 05:35:35+00:00
- **Authors**: Yunseok Jang, Ruben Villegas, Jimei Yang, Duygu Ceylan, Xin Sun, Honglak Lee
- **Comment**: Accepted paper at AI for Content Creation Workshop (AICC) at CVPR
  2022
- **Journal**: None
- **Summary**: There have been remarkable successes in computer vision with deep learning. While such breakthroughs show robust performance, there have still been many challenges in learning in-depth knowledge, like occlusion or predicting physical interactions. Although some recent works show the potential of 3D data in serving such context, it is unclear how we efficiently provide 3D input to the 2D models due to the misalignment in dimensionality between 2D and 3D. To leverage the successes of 2D models in predicting self-occlusions, we design Ray-marching in Camera Space (RiCS), a new method to represent the self-occlusions of foreground objects in 3D into a 2D self-occlusion map. We test the effectiveness of our representation on the human image harmonization task by predicting shading that is coherent with a given background image. Our experiments demonstrate that our representation map not only allows us to enhance the image quality but also to model temporally coherent complex shadow effects compared with the simulation-to-real and harmonization methods, both quantitatively and qualitatively. We further show that we can significantly improve the performance of human parts segmentation networks trained on existing synthetic datasets by enhancing the harmonization quality with our method.



### Efficient Gesture Recognition for the Assistance of Visually Impaired People using Multi-Head Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.06980v1
- **DOI**: 10.1016/j.engappai.2022.105188
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06980v1)
- **Published**: 2022-05-14 06:01:47+00:00
- **Updated**: 2022-05-14 06:01:47+00:00
- **Authors**: Samer Alashhab, Antonio Javier Gallego, Miguel Ángel Lozano
- **Comment**: None
- **Journal**: Engineering Applications of Artificial Intelligence, 2022
- **Summary**: This paper proposes an interactive system for mobile devices controlled by hand gestures aimed at helping people with visual impairments. This system allows the user to interact with the device by making simple static and dynamic hand gestures. Each gesture triggers a different action in the system, such as object recognition, scene description or image scaling (e.g., pointing a finger at an object will show a description of it). The system is based on a multi-head neural network architecture, which initially detects and classifies the gestures, and subsequently, depending on the gesture detected, performs a second stage that carries out the corresponding action. This multi-head architecture optimizes the resources required to perform different tasks simultaneously, and takes advantage of the information obtained from an initial backbone to perform different processes in a second stage. To train and evaluate the system, a dataset with about 40k images was manually compiled and labeled including different types of hand gestures, backgrounds (indoors and outdoors), lighting conditions, etc. This dataset contains synthetic gestures (whose objective is to pre-train the system in order to improve the results) and real images captured using different mobile phones. The results obtained and the comparison made with the state of the art show competitive results as regards the different actions performed by the system, such as the accuracy of classification and localization of gestures, or the generation of descriptions for objects and scenes.



### Voxel-wise Adversarial Semi-supervised Learning for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.06987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06987v1)
- **Published**: 2022-05-14 06:57:19+00:00
- **Updated**: 2022-05-14 06:57:19+00:00
- **Authors**: Chae Eun Lee, Hyelim Park, Yeong-Gil Shin, Minyoung Chung
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning for medical image segmentation is an important area of research for alleviating the huge cost associated with the construction of reliable large-scale annotations in the medical domain. Recent semi-supervised approaches have demonstrated promising results by employing consistency regularization, pseudo-labeling techniques, and adversarial learning. These methods primarily attempt to learn the distribution of labeled and unlabeled data by enforcing consistency in the predictions or embedding context. However, previous approaches have focused only on local discrepancy minimization or context relations across single classes. In this paper, we introduce a novel adversarial learning-based semi-supervised segmentation method that effectively embeds both local and global features from multiple hidden layers and learns context relations between multiple classes. Our voxel-wise adversarial learning method utilizes a voxel-wise feature discriminator, which considers multilayer voxel-wise features (involving both local and global features) as an input by embedding class-specific voxel-wise feature distribution. Furthermore, we improve our previous representation learning method by overcoming information loss and learning stability problems, which enables rich representations of labeled data. Our method outperforms current best-performing state-of-the-art semi-supervised learning approaches on the image segmentation of the left atrium (single class) and multiorgan datasets (multiclass). Moreover, our visual interpretation of the feature space demonstrates that our proposed method enables a well-distributed and separated feature space from both labeled and unlabeled data, which improves the overall prediction results.



### Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap
- **Arxiv ID**: http://arxiv.org/abs/2205.07002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07002v1)
- **Published**: 2022-05-14 08:16:13+00:00
- **Updated**: 2022-05-14 08:16:13+00:00
- **Authors**: Jinke Li, Xiao He, Yang Wen, Yuan Gao, Xiaoqiang Cheng, Dan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As a rising task, panoptic segmentation is faced with challenges in both semantic segmentation and instance segmentation. However, in terms of speed and accuracy, existing LiDAR methods in the field are still limited. In this paper, we propose a fast and high-performance LiDAR-based framework, referred to as Panoptic-PHNet, with three attractive aspects: 1) We introduce a clustering pseudo heatmap as a new paradigm, which, followed by a center grouping module, yields instance centers for efficient clustering without object-level learning tasks. 2) A knn-transformer module is proposed to model the interaction among foreground points for accurate offset regression. 3) For backbone design, we fuse the fine-grained voxel features and the 2D Bird's Eye View (BEV) features with different receptive fields to utilize both detailed and global information. Extensive experiments on both SemanticKITTI dataset and nuScenes dataset show that our Panoptic-PHNet surpasses state-of-the-art methods by remarkable margins with a real-time speed. We achieve the 1st place on the public leaderboard of SemanticKITTI and leading performance on the recently released leaderboard of nuScenes.



### SaiNet: Stereo aware inpainting behind objects with generative networks
- **Arxiv ID**: http://arxiv.org/abs/2205.07014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07014v1)
- **Published**: 2022-05-14 09:07:30+00:00
- **Updated**: 2022-05-14 09:07:30+00:00
- **Authors**: Violeta Menéndez González, Andrew Gilbert, Graeme Phillipson, Stephen Jolly, Simon Hadfield
- **Comment**: Presented at AI4CC workshop at CVPR
- **Journal**: None
- **Summary**: In this work, we present an end-to-end network for stereo-consistent image inpainting with the objective of inpainting large missing regions behind objects. The proposed model consists of an edge-guided UNet-like network using Partial Convolutions. We enforce multi-view stereo consistency by introducing a disparity loss. More importantly, we develop a training scheme where the model is learned from realistic stereo masks representing object occlusions, instead of the more common random masks. The technique is trained in a supervised way. Our evaluation shows competitive results compared to previous state-of-the-art techniques.



### Importance Weighted Structure Learning for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.07017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07017v1)
- **Published**: 2022-05-14 09:25:14+00:00
- **Updated**: 2022-05-14 09:25:14+00:00
- **Authors**: Daqi Liu, Miroslaw Bober, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graph generation is a structured prediction task aiming to explicitly model objects and their relationships via constructing a visually-grounded scene graph for an input image. Currently, the message passing neural network based mean field variational Bayesian methodology is the ubiquitous solution for such a task, in which the variational inference objective is often assumed to be the classical evidence lower bound. However, the variational approximation inferred from such loose objective generally underestimates the underlying posterior, which often leads to inferior generation performance. In this paper, we propose a novel importance weighted structure learning method aiming to approximate the underlying log-partition function with a tighter importance weighted lower bound, which is computed from multiple samples drawn from a reparameterizable Gumbel-Softmax sampler. A generic entropic mirror descent algorithm is applied to solve the resulting constrained variational inference task. The proposed method achieves the state-of-the-art performance on various popular scene graph generation benchmarks.



### Evaluating the Generalization Ability of Super-Resolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.07019v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07019v1)
- **Published**: 2022-05-14 09:33:20+00:00
- **Updated**: 2022-05-14 09:33:20+00:00
- **Authors**: Yihao Liu, Hengyuan Zhao, Jinjin Gu, Yu Qiao, Chao Dong
- **Comment**: First Generalization Assessment Index for SR networks
- **Journal**: None
- **Summary**: Performance and generalization ability are two important aspects to evaluate deep learning models. However, research on the generalization ability of Super-Resolution (SR) networks is currently absent. We make the first attempt to propose a Generalization Assessment Index for SR networks, namely SRGA. SRGA exploits the statistical characteristics of internal features of deep networks, not output images to measure the generalization ability. Specially, it is a non-parametric and non-learning metric. To better validate our method, we collect a patch-based image evaluation set (PIES) that includes both synthetic and real-world images, covering a wide range of degradations. With SRGA and PIES dataset, we benchmark existing SR models on the generalization ability. This work could lay the foundation for future research on model generalization in low-level vision.



### Self-supervised Assisted Active Learning for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.07021v1
- **DOI**: 10.1109/EMBC48229.2022.9871734
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07021v1)
- **Published**: 2022-05-14 09:40:18+00:00
- **Updated**: 2022-05-14 09:40:18+00:00
- **Authors**: Ziyuan Zhao, Wenjing Lu, Zeng Zeng, Kaixin Xu, Bharadwaj Veeravalli, Cuntai Guan
- **Comment**: Accepted by the 44th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society (EMBC 2022)
- **Journal**: 2022 44th Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC)
- **Summary**: Label scarcity has been a long-standing issue for biomedical image segmentation, due to high annotation costs and professional requirements. Recently, active learning (AL) strategies strive to reduce annotation costs by querying a small portion of data for annotation, receiving much traction in the field of medical imaging. However, most of the existing AL methods have to initialize models with some randomly selected samples followed by active selection based on various criteria, such as uncertainty and diversity. Such random-start initialization methods inevitably introduce under-value redundant samples and unnecessary annotation costs. For the purpose of addressing the issue, we propose a novel self-supervised assisted active learning framework in the cold-start setting, in which the segmentation model is first warmed up with self-supervised learning (SSL), and then SSL features are used for sample selection via latent feature clustering without accessing labels. We assess our proposed methodology on skin lesions segmentation task. Extensive experiments demonstrate that our approach is capable of achieving promising performance with substantial improvements over existing baselines.



### Object-Aware Self-supervised Multi-Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07028v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07028v2)
- **Published**: 2022-05-14 10:14:08+00:00
- **Updated**: 2022-07-13 06:49:07+00:00
- **Authors**: Xu Kaixin, Liu Liyang, Zhao Ziyuan, Zeng Zeng, Bharadwaj Veeravalli
- **Comment**: Accepted by IEEE International Conference on Image Processing (ICIP
  2022)
- **Journal**: None
- **Summary**: Multi-label Learning on Image data has been widely exploited with deep learning models. However, supervised training on deep CNN models often cannot discover sufficient discriminative features for classification. As a result, numerous self-supervision methods are proposed to learn more robust image representations. However, most self-supervised approaches focus on single-instance single-label data and fall short on more complex images with multiple objects. Therefore, we propose an Object-Aware Self-Supervision (OASS) method to obtain more fine-grained representations for multi-label learning, dynamically generating auxiliary tasks based on object locations. Secondly, the robust representation learned by OASS can be leveraged to efficiently generate Class-Specific Instances (CSI) in a proposal-free fashion to better guide multi-label supervision signal transfer to instances. Extensive experiments on the VOC2012 dataset for multi-label classification demonstrate the effectiveness of the proposed method against the state-of-the-art counterparts.



### Realistic Defocus Blur for Multiplane Computer-Generated Holography
- **Arxiv ID**: http://arxiv.org/abs/2205.07030v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2205.07030v2)
- **Published**: 2022-05-14 10:17:34+00:00
- **Updated**: 2023-02-06 21:06:15+00:00
- **Authors**: Koray Kavaklı, Yuta Itoh, Hakan Urey, Kaan Akşit
- **Comment**: 16 pages in total, first 9 pages are for the manuscript, remaining
  pages are for supplementary. For more visit:
  https://complightlab.com/publications/realistic_defocus_cgh For our codebase
  visit https://github.com/complight/realistic_defocus
- **Journal**: None
- **Summary**: This paper introduces a new multiplane CGH computation method to reconstruct artefact-free high-quality holograms with natural-looking defocus blur. Our method introduces a new targeting scheme and a new loss function. While the targeting scheme accounts for defocused parts of the scene at each depth plane, the new loss function analyzes focused and defocused parts separately in reconstructed images. Our method support phase-only CGH calculations using various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative (e.g., Double Phase) CGH techniques. We achieve our best image quality using a modified gradient descent-based optimization recipe where we introduce a constraint inspired by the double phase method. We validate our method experimentally using our proof-of-concept holographic display, comparing various algorithms, including multi-depth scenes with sparse and dense contents.



### Transformer Scale Gate for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.07056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07056v1)
- **Published**: 2022-05-14 13:11:39+00:00
- **Updated**: 2022-05-14 13:11:39+00:00
- **Authors**: Hengcan Shi, Munawar Hayat, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Effectively encoding multi-scale contextual information is crucial for accurate semantic segmentation. Existing transformer-based segmentation models combine features across scales without any selection, where features on sub-optimal scales may degrade segmentation outcomes. Leveraging from the inherent properties of Vision Transformers, we propose a simple yet effective module, Transformer Scale Gate (TSG), to optimally combine multi-scale features.TSG exploits cues in self and cross attentions in Vision Transformers for the scale selection. TSG is a highly flexible plug-and-play module, and can easily be incorporated with any encoder-decoder-based hierarchical vision Transformer architecture. Extensive experiments on the Pascal Context and ADE20K datasets demonstrate that our feature selection strategy achieves consistent gains.



### RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2205.07058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07058v2)
- **Published**: 2022-05-14 13:15:32+00:00
- **Updated**: 2022-10-25 01:44:56+00:00
- **Authors**: Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki Takikawa, Stan Birchfield
- **Comment**: ECCV 2022 Workshop on Learning to Generate 3D Shapes and Scenes.
  Project page at http://www.cs.umd.edu/~mmeshry/projects/rtmv
- **Journal**: None
- **Summary**: We present a large-scale synthetic dataset for novel view synthesis consisting of ~300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures. Because our dataset is too large for existing methods to process, we propose Sparse Voxel Light Field (SVLF), an efficient voxel-based light field approach for novel view synthesis that achieves comparable performance to NeRF on synthetic data, while being an order of magnitude faster to train and two orders of magnitude faster to render. SVLF achieves this speed by relying on a sparse voxel octree, careful voxel sampling (requiring only a handful of queries per ray), and reduced network structure; as well as ground truth depth maps at training time. Our dataset is generated by NViSII, a Python-based ray tracing renderer, which is designed to be simple for non-experts to use and share, flexible and powerful through its use of scripting, and able to create high-quality and physically-based rendered images. Experiments with a subset of our dataset allow us to compare standard methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for category-level modeling, pointing toward the need for future improvements in this area.



### An Interpretable MRI Reconstruction Network with Two-grid-cycle Correction and Geometric Prior Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.07062v2
- **DOI**: 10.1016/j.bspc.2023.104821
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07062v2)
- **Published**: 2022-05-14 13:36:27+00:00
- **Updated**: 2023-03-06 03:00:48+00:00
- **Authors**: Xiaohong Fan, Yin Yang, Ke Chen, Jianping Zhang, Ke Dong
- **Comment**: 14 pages, accepted to Biomedical Signal Processing and Control,March,
  2023
- **Journal**: Biomedical Signal Processing and Control, vol 84, 2023
- **Summary**: Although existing deep learning compressed-sensing-based Magnetic Resonance Imaging (CS-MRI) methods have achieved considerably impressive performance, explainability and generalizability continue to be challenging for such methods since the transition from mathematical analysis to network design not always natural enough, often most of them are not flexible enough to handle multi-sampling-ratio reconstruction assignments. {In this work, to tackle explainability and generalizability, we propose a unifying deep unfolding multi-sampling-ratio interpretable CS-MRI framework.} The combined approach offers more generalizability than previous works whereas deep learning gains explainability through a geometric prior module. Inspired by the multigrid algorithm, we first embed the CS-MRI-based optimization algorithm into correction-distillation scheme that consists of three ingredients: pre-relaxation module, correction module and geometric prior distillation module. Furthermore, we employ a condition module to learn adaptively step-length and noise level, which enables the proposed framework to jointly train multi-ratio tasks through a single model. { The proposed model not only compensates for the lost contextual information of reconstructed image which is refined from low frequency error in geometric characteristic k-space}, but also integrates the theoretical guarantee of model-based methods and the superior reconstruction performances of deep learning-based methods. Therefore, it can give us a novel perspective to design biomedical imaging networks. { Numerical experiments show that our framework outperforms state-of-the-art methods in terms of qualitative and quantitative evaluations.} {Our method achieves 3.18 dB improvement at low CS ratio 10\% and average 1.42 dB improvement over other comparison methods on brain dataset using Cartesian sampling mask.



### An Architecture for the detection of GAN-generated Flood Images with Localization Capabilities
- **Arxiv ID**: http://arxiv.org/abs/2205.07073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2205.07073v1)
- **Published**: 2022-05-14 14:23:44+00:00
- **Updated**: 2022-05-14 14:23:44+00:00
- **Authors**: Jun Wang, Omran Alamayreh, Benedetta Tondi, Mauro Barni
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address a new image forensics task, namely the detection of fake flood images generated by ClimateGAN architecture. We do so by proposing a hybrid deep learning architecture including both a detection and a localization branch, the latter being devoted to the identification of the image regions manipulated by ClimateGAN. Even if our goal is the detection of fake flood images, in fact, we found that adding a localization branch helps the network to focus on the most relevant image regions with significant improvements in terms of generalization capabilities and robustness against image processing operations. The good performance of the proposed architecture is validated on two datasets of pristine flood images downloaded from the internet and three datasets of fake flood images generated by ClimateGAN starting from a large set of diverse street images.



### Corrosion Detection for Industrial Objects: From Multi-Sensor System to 5D Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2205.07075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07075v1)
- **Published**: 2022-05-14 14:45:58+00:00
- **Updated**: 2022-05-14 14:45:58+00:00
- **Authors**: Dennis Haitz, Boris Jutzi, Patrick Huebner, Markus Ulrich
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Corrosion is a form of damage that often appears on the surface of metal-made objects used in industrial applications. Those damages can be critical depending on the purpose of the used object. Optical-based testing systems provide a form of non-contact data acquisition, where the acquired data can then be used to analyse the surface of an object. In the field of industrial image processing, this is called surface inspection. We provide a testing setup consisting of a rotary table which rotates the object by 360 degrees, as well as industrial RGB cameras and laser triangulation sensors for the acquisition of 2D and 3D data as our multi-sensor system. These sensors acquire data while the object to be tested takes a full rotation. Further on, data augmentation is applied to prepare new data or enhance already acquired data. In order to evaluate the impact of a laser triangulation sensor for corrosion detection, one challenge is to at first fuse the data of both domains. After the data fusion process, 5 different channels can be utilized to create a 5D feature space. Besides the red, green and blue channels of the image (1-3), additional range data from the laser triangulation sensor is incorporated (4). As a fifth channel, said sensor provides additional intensity data (5). With a multi-channel image classification, a 5D feature space will lead to slightly superior results opposed to a 3D feature space, composed of only the RGB channels of the image.



### Spiking Approximations of the MaxPooling Operation in Deep SNNs
- **Arxiv ID**: http://arxiv.org/abs/2205.07076v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07076v1)
- **Published**: 2022-05-14 14:47:10+00:00
- **Updated**: 2022-05-14 14:47:10+00:00
- **Authors**: Ramashish Gaurav, Bryan Tripp, Apurva Narayan
- **Comment**: Accepted in IJCNN-2022
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) are an emerging domain of biologically inspired neural networks that have shown promise for low-power AI. A number of methods exist for building deep SNNs, with Artificial Neural Network (ANN)-to-SNN conversion being highly successful. MaxPooling layers in Convolutional Neural Networks (CNNs) are an integral component to downsample the intermediate feature maps and introduce translational invariance, but the absence of their hardware-friendly spiking equivalents limits such CNNs' conversion to deep SNNs. In this paper, we present two hardware-friendly methods to implement Max-Pooling in deep SNNs, thus facilitating easy conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware (with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our approach.



### Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.07085v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.07085v3)
- **Published**: 2022-05-14 15:24:06+00:00
- **Updated**: 2023-02-27 02:32:56+00:00
- **Authors**: David Ahmedt-Aristizabal, Chuong Nguyen, Lachlan Tychsen-Smith, Ashley Stacey, Shenghong Li, Joseph Pathikulangara, Lars Petersson, Dadong Wang
- **Comment**: In Computer Methods and Programs in Biomedicine
- **Journal**: None
- **Summary**: Advanced artificial intelligence and machine learning have great potential to redefine how skin lesions are detected, mapped, tracked and documented. Here, We propose a 3D whole-body imaging system known as 3DSkin-mapper to enable automated detection, evaluation and mapping of skin lesions. A modular camera rig arranged in a cylindrical configuration was designed to automatically capture images of the entire skin surface of a subject synchronously from multiple angles. Based on the images, we developed algorithms for 3D model reconstruction, data processing and skin lesion detection and tracking based on deep convolutional neural networks. We also introduced a customised, user-friendly, and adaptable interface that enables individuals to interactively visualise, manipulate, and annotate the images. The proposed system is developed for skin lesion screening, the focus of this paper is to introduce the system instead of clinical study. Using synthetic and real images we demonstrate the effectiveness of the proposed system by providing multiple views of a target skin lesion, enabling further 3D geometry analysis and longitudinal tracking. It takes only a few seconds to capture the entire skin surface, and about half an hour to process and analyse the images. Our experiments show that the proposed system allow fast and easy whole body 3D imaging. It can be used by dermatological clinics to conduct skin screening, detect and track skin lesions over time, identify suspicious lesions, and document pigmented lesions. The system can potentially save clinicians time and effort significantly. The 3D imaging and analysis has the potential to change the paradigm of whole body photography with many applications in skin diseases, including inflammatory and pigmentary disorders.



### Multi-modal curb detection and filtering
- **Arxiv ID**: http://arxiv.org/abs/2205.07096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.07096v1)
- **Published**: 2022-05-14 17:03:41+00:00
- **Updated**: 2022-05-14 17:03:41+00:00
- **Authors**: Sandipan Das, Navid Mahabadi, Saikat Chatterjee, Maurice Fallon
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable knowledge of road boundaries is critical for autonomous vehicle navigation. We propose a robust curb detection and filtering technique based on the fusion of camera semantics and dense lidar point clouds. The lidar point clouds are collected by fusing multiple lidars for robust feature detection. The camera semantics are based on a modified EfficientNet architecture which is trained with labeled data collected from onboard fisheye cameras. The point clouds are associated with the closest curb segment with $L_2$-norm analysis after projecting into the image space with the fisheye model projection. Next, the selected points are clustered using unsupervised density-based spatial clustering to detect different curb regions. As new curb points are detected in consecutive frames they are associated with the existing curb clusters using temporal reachability constraints. If no reachability constraints are found a new curb cluster is formed from these new points. This ensures we can detect multiple curbs present in road segments consisting of multiple lanes if they are in the sensors' field of view. Finally, Delaunay filtering is applied for outlier removal and its performance is compared to traditional RANSAC-based filtering. An objective evaluation of the proposed solution is done using a high-definition map containing ground truth curb points obtained from a commercial map supplier. The proposed system has proven capable of detecting curbs of any orientation in complex urban road scenarios comprising straight roads, curved roads, and intersections with traffic isles.



### Differentiable SAR Renderer and SAR Target Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.07099v1
- **DOI**: 10.1109/TIP.2022.3215069
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07099v1)
- **Published**: 2022-05-14 17:24:32+00:00
- **Updated**: 2022-05-14 17:24:32+00:00
- **Authors**: Shilei Fu, Feng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Forward modeling of wave scattering and radar imaging mechanisms is the key to information extraction from synthetic aperture radar (SAR) images. Like inverse graphics in optical domain, an inherently-integrated forward-inverse approach would be promising for SAR advanced information retrieval and target reconstruction. This paper presents such an attempt to the inverse graphics for SAR imagery. A differentiable SAR renderer (DSR) is developed which reformulates the mapping and projection algorithm of SAR imaging mechanism in the differentiable form of probability maps. First-order gradients of the proposed DSR are then analytically derived which can be back-propagated from rendered image/silhouette to the target geometry and scattering attributes. A 3D inverse target reconstruction algorithm from SAR images is devised. Several simulation and reconstruction experiments are conducted, including targets with and without background, using both synthesized data or real measured inverse SAR (ISAR) data by ground radar. Results demonstrate the efficacy of the proposed DSR and its inverse approach.



### Efficient Deep Learning Methods for Identification of Defective Casting Products
- **Arxiv ID**: http://arxiv.org/abs/2205.07118v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07118v1)
- **Published**: 2022-05-14 19:35:05+00:00
- **Updated**: 2022-05-14 19:35:05+00:00
- **Authors**: Bharath Kumar Bolla, Mohan Kingam, Sabeesh Ethiraj
- **Comment**: Accepted at ICCR 2021: International Conference on Cognition and
  Recognition 2021
- **Journal**: None
- **Summary**: Quality inspection has become crucial in any large-scale manufacturing industry recently. In order to reduce human error, it has become imperative to use efficient and low computational AI algorithms to identify such defective products. In this paper, we have compared and contrasted various pre-trained and custom-built architectures using model size, performance and CPU latency in the detection of defective casting products. Our results show that custom architectures are efficient than pre-trained mobile architectures. Moreover, custom models perform 6 to 9 times faster than lightweight models such as MobileNetV2 and NasNet. The number of training parameters and the model size of the custom architectures is significantly lower (~386 times & ~119 times respectively) than the best performing models such as MobileNetV2 and NasNet. Augmentation experimentations have also been carried out on the custom architectures to make the models more robust and generalizable. Our work sheds light on the efficiency of these custom-built architectures for deployment on Edge and IoT devices and that transfer learning models may not always be ideal. Instead, they should be specific to the kind of dataset and the classification problem at hand.



### Revisiting Facial Key Point Detection: An Efficient Approach Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.07121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07121v1)
- **Published**: 2022-05-14 19:49:03+00:00
- **Updated**: 2022-05-14 19:49:03+00:00
- **Authors**: Prathima Dileep, Bharath Kumar Bolla, Sabeesh Ethiraj
- **Comment**: Accepted at international Conference On Big Data, Machine Learning
  and Applications (BigDML 2021)
- **Journal**: None
- **Summary**: Facial landmark detection is a widely researched field of deep learning as this has a wide range of applications in many fields. These key points are distinguishing characteristic points on the face, such as the eyes center, the eye's inner and outer corners, the mouth center, and the nose tip from which human emotions and intent can be explained. The focus of our work has been evaluating transfer learning models such as MobileNetV2 and NasNetMobile, including custom CNN architectures. The objective of the research has been to develop efficient deep learning models in terms of model size, parameters, and inference time and to study the effect of augmentation imputation and fine-tuning on these models. It was found that while augmentation techniques produced lower RMSE scores than imputation techniques, they did not affect the inference time. MobileNetV2 architecture produced the lowest RMSE and inference time. Moreover, our results indicate that manually optimized CNN architectures performed similarly to Auto Keras tuned architecture. However, manually optimized architectures yielded better inference time and training curves.



### Classification of Astronomical Bodies by Efficient Layer Fine-Tuning of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.07124v1
- **DOI**: 10.1109/CICT53865.2020.9672430
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07124v1)
- **Published**: 2022-05-14 20:08:19+00:00
- **Updated**: 2022-05-14 20:08:19+00:00
- **Authors**: Sabeesh Ethiraj, Bharath Kumar Bolla
- **Comment**: Accepted at 5th Conference on Information and Communication
  Technology (CICT), 2021
- **Journal**: None
- **Summary**: The SDSS-IV dataset contains information about various astronomical bodies such as Galaxies, Stars, and Quasars captured by observatories. Inspired by our work on deep multimodal learning, which utilized transfer learning to classify the SDSS-IV dataset, we further extended our research in the fine tuning of these architectures to study the effect in the classification scenario. Architectures such as Resnet-50, DenseNet-121 VGG-16, Xception, EfficientNetB2, MobileNetV2 and NasnetMobile have been built using layer wise fine tuning at different levels. Our findings suggest that freezing all layers with Imagenet weights and adding a final trainable layer may not be the optimal solution. Further, baseline models and models that have higher number of trainable layers performed similarly in certain architectures. Model need to be fine tuned at different levels and a specific training ratio is required for a model to be termed ideal. Different architectures had different responses to the change in the number of trainable layers w.r.t accuracies. While models such as DenseNet-121, Xception, EfficientNetB2 achieved peak accuracies that were relatively consistent with near perfect training curves, models such as Resnet-50,VGG-16, MobileNetV2 and NasnetMobile had lower, delayed peak accuracies with poorly fitting training curves. It was also found that though mobile neural networks have lesser parameters and model size, they may not always be ideal for deployment on a low computational device as they had consistently lower validation accuracies. Customized evaluation metrics such as Tuning Parameter Ratio and Tuning Layer Ratio are used for model evaluation.



### ETAD: Training Action Detection End to End on a Laptop
- **Arxiv ID**: http://arxiv.org/abs/2205.07134v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07134v2)
- **Published**: 2022-05-14 21:16:21+00:00
- **Updated**: 2022-11-28 11:43:24+00:00
- **Authors**: Shuming Liu, Mengmeng Xu, Chen Zhao, Xu Zhao, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action detection (TAD) with end-to-end training often suffers from the pain of huge demand for computing resources due to long video duration. In this work, we propose an efficient temporal action detector (ETAD) that can train directly from video frames with extremely low GPU memory consumption. Our main idea is to minimize and balance the heavy computation among features and gradients in each training iteration. We propose to sequentially forward the snippet frame through the video encoder, and backward only a small necessary portion of gradients to update the encoder. To further alleviate the computational redundancy in training, we propose to dynamically sample only a small subset of proposals during training. Moreover, various sampling strategies and ratios are studied for both the encoder and detector. ETAD achieves state-of-the-art performance on TAD benchmarks with remarkable efficiency. On ActivityNet-1.3, training ETAD in 18 hours can reach 38.25% average mAP with only 1.3 GB memory consumption per video under end-to-end training. Our code will be publicly released.



### Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training
- **Arxiv ID**: http://arxiv.org/abs/2205.07139v1
- **DOI**: 10.1007/978-3-031-16443-9_66
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07139v1)
- **Published**: 2022-05-14 21:44:05+00:00
- **Updated**: 2022-05-14 21:44:05+00:00
- **Authors**: Constantin Seibold, Simon Reiß, M. Saquib Sarfraz, Rainer Stiefelhagen, Jens Kleesiek
- **Comment**: Provisionally Accepted at MICCAI2022
- **Journal**: None
- **Summary**: When reading images, radiologists generate text reports describing the findings therein. Current state-of-the-art computer-aided diagnosis tools utilize a fixed set of predefined categories automatically extracted from these medical reports for training. This form of supervision limits the potential usage of models as they are unable to pick up on anomalies outside of their predefined set, thus, making it a necessity to retrain the classifier with additional data when faced with novel classes. In contrast, we investigate direct text supervision to break away from this closed set assumption. By doing so, we avoid noisy label extraction via text classifiers and incorporate more contextual information.   We employ a contrastive global-local dual-encoder architecture to learn concepts directly from unstructured medical reports while maintaining its ability to perform free form classification.   We investigate relevant properties of open set recognition for radiological data and propose a method to employ currently weakly annotated data into training.   We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR, CheXpert, and ChestX-Ray14 for disease classification. We show that despite using unstructured medical report supervision, we perform on par with direct label supervision through a sophisticated inference setting.




# Arxiv Papers in cs.CV on 2022-05-15
### Evaluating Uncertainty Calibration for Open-Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.07160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.07160v1)
- **Published**: 2022-05-15 02:08:35+00:00
- **Updated**: 2022-05-15 02:08:35+00:00
- **Authors**: Zongyao Lyu, Nolan B. Gutierrez, William J. Beksi
- **Comment**: To be presented at the 2022 IEEE International Conference on Robotics
  and Automation (ICRA) Workshop on Safe and Reliable Robot Autonomy under
  Uncertainty
- **Journal**: None
- **Summary**: Despite achieving enormous success in predictive accuracy for visual classification problems, deep neural networks (DNNs) suffer from providing overconfident probabilities on out-of-distribution (OOD) data. Yet, accurate uncertainty estimation is crucial for safe and reliable robot autonomy. In this paper, we evaluate popular calibration techniques for open-set conditions in a way that is distinctly different from the conventional evaluation of calibration methods on OOD data. Our results show that closed-set DNN calibration approaches are much less effective for open-set recognition, which highlights the need to develop new DNN calibration methods to address this problem.



### GLaMa: Joint Spatial and Frequency Loss for General Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2205.07162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07162v1)
- **Published**: 2022-05-15 02:18:59+00:00
- **Updated**: 2022-05-15 02:18:59+00:00
- **Authors**: Zeyu Lu, Junjun Jiang, Junqin Huang, Gang Wu, Xianming Liu
- **Comment**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Journal**: None
- **Summary**: The purpose of image inpainting is to recover scratches and damaged areas using context information from remaining parts. In recent years, thanks to the resurgence of convolutional neural networks (CNNs), image inpainting task has made great breakthroughs. However, most of the work consider insufficient types of mask, and their performance will drop dramatically when encountering unseen masks. To combat these challenges, we propose a simple yet general method to solve this problem based on the LaMa image inpainting framework, dubbed GLaMa. Our proposed GLaMa can better capture different types of missing information by using more types of masks. By incorporating more degraded images in the training phase, we can expect to enhance the robustness of the model with respect to various masks. In order to yield more reasonable results, we further introduce a frequency-based loss in addition to the traditional spatial reconstruction loss and adversarial loss. In particular, we introduce an effective reconstruction loss both in the spatial and frequency domain to reduce the chessboard effect and ripples in the reconstructed image. Extensive experiments demonstrate that our method can boost the performance over the original LaMa method for each type of mask on FFHQ, ImageNet, Places2 and WikiArt dataset. The proposed GLaMa was ranked first in terms of PSNR, LPIPS and SSIM in the NTIRE 2022 Image Inpainting Challenge Track 1 Unsupervised.



### Proxyless Neural Architecture Adaptation for Supervised Learning and Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07168v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07168v1)
- **Published**: 2022-05-15 02:49:48+00:00
- **Updated**: 2022-05-15 02:49:48+00:00
- **Authors**: Do-Guk Kim, Heung-Chang Lee
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.08231
- **Journal**: None
- **Summary**: Recently, Neural Architecture Search (NAS) methods have been introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to adapt the given neural architecture to improve performance while maintaining computational costs. However, NAT lacks reproducibility and it requires an additional architecture adaptation process before network weight training. In this paper, we propose proxyless neural architecture adaptation that is reproducible and efficient. Our method can be applied to both supervised learning and self-supervised learning. The proposed method shows stable performance on various architectures. Extensive reproducibility experiments on two datasets, i.e., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and is applicable to other models and datasets.



### Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.07179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07179v1)
- **Published**: 2022-05-15 04:44:44+00:00
- **Updated**: 2022-05-15 04:44:44+00:00
- **Authors**: Wei Ji, Jingjing Li, Qi Bi, Chuan Guo, Jie Liu, Li Cheng
- **Comment**: This paper appeared at ICLR 2022
- **Journal**: None
- **Summary**: Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.



### Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT
- **Arxiv ID**: http://arxiv.org/abs/2205.07180v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2205.07180v2)
- **Published**: 2022-05-15 04:48:41+00:00
- **Updated**: 2022-07-14 23:02:59+00:00
- **Authors**: Bowen Shi, Abdelrahman Mohamed, Wei-Ning Hsu
- **Comment**: Interspeech 2022
- **Journal**: None
- **Summary**: This paper investigates self-supervised pre-training for audio-visual speaker representation learning where a visual stream showing the speaker's mouth area is used alongside speech as inputs. Our study focuses on the Audio-Visual Hidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose audio-visual speech pre-training framework. We conducted extensive experiments probing the effectiveness of pre-training and visual modality. Experimental results suggest that AV-HuBERT generalizes decently to speaker related downstream tasks, improving label efficiency by roughly ten fold for both audio-only and audio-visual speaker verification. We also show that incorporating visual information, even just the lip area, greatly improves the performance and noise robustness, reducing EER by 38% in the clean condition and 75% in noisy conditions.



### Nonconvex ${L_ {1/2}} $-Regularized Nonlocal Self-similarity Denoiser for Compressive Sensing based CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.07185v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07185v1)
- **Published**: 2022-05-15 05:24:48+00:00
- **Updated**: 2022-05-15 05:24:48+00:00
- **Authors**: Yunyi Li, Yiqiu Jiang, Hengmin Zhang, Jianxun Liu, Xiangling Ding, Guan Gui
- **Comment**: Preprint submitted to Journal of The Franklin Institute.
  Corresponding Author: yunyili@hnust.edu.cn, guiguan@njupt.edu.cn
- **Journal**: None
- **Summary**: Compressive sensing (CS) based computed tomography (CT) image reconstruction aims at reducing the radiation risk through sparse-view projection data. It is usually challenging to achieve satisfying image quality from incomplete projections. Recently, the nonconvex ${{L_ {{1/2}}}} $-norm has achieved promising performance in sparse recovery, while the applications on imaging are unsatisfactory due to its nonconvexity. In this paper, we develop a ${{L_ {{1/2}}}} $-regularized nonlocal self-similarity (NSS) denoiser for CT reconstruction problem, which integrates low-rank approximation with group sparse coding (GSC) framework. Concretely, we first split the CT reconstruction problem into two subproblems, and then improve the CT image quality furtherly using our ${{L_ {{1/2}}}} $-regularized NSS denoiser. Instead of optimizing the nonconvex problem under the perspective of GSC, we particularly reconstruct CT image via low-rank minimization based on two simple yet essential schemes, which build the equivalent relationship between GSC based denoiser and low-rank minimization. Furtherly, the weighted singular value thresholding (WSVT) operator is utilized to optimize the resulting nonconvex ${{L_ {{1/2}}}} $ minimization problem. Following this, our proposed denoiser is integrated with the CT reconstruction problem by alternating direction method of multipliers (ADMM) framework. Extensive experimental results on typical clinical CT images have demonstrated that our approach can further achieve better performance than popular approaches.



### Real-centric Consistency Learning for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.07201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07201v1)
- **Published**: 2022-05-15 07:01:28+00:00
- **Updated**: 2022-05-15 07:01:28+00:00
- **Authors**: Ruiqi Zha, Zhichao Lian, Qianmu Li, Siqi Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Most of previous deepfake detection researches bent their efforts to describe and discriminate artifacts in human perceptible ways, which leave a bias in the learned networks of ignoring some critical invariance features intra-class and underperforming the robustness of internet interference. Essentially, the target of deepfake detection problem is to represent natural faces and fake faces at the representation space discriminatively, and it reminds us whether we could optimize the feature extraction procedure at the representation space through constraining intra-class consistence and inter-class inconsistence to bring the intra-class representations close and push the inter-class representations apart? Therefore, inspired by contrastive representation learning, we tackle the deepfake detection problem through learning the invariant representations of both classes and propose a novel real-centric consistency learning method. We constraint the representation from both the sample level and the feature level. At the sample level, we take the procedure of deepfake synthesis into consideration and propose a novel forgery semantical-based pairing strategy to mine latent generation-related features. At the feature level, based on the centers of natural faces at the representation space, we design a hard positive mining and synthesizing method to simulate the potential marginal features. Besides, a hard negative fusion method is designed to improve the discrimination of negative marginal features with the help of supervised contrastive margin loss we developed. The effectiveness and robustness of the proposed method has been demonstrated through extensive experiments.



### Fused Deep Neural Network based Transfer Learning in Occluded Face Classification and Person re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2205.07203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07203v1)
- **Published**: 2022-05-15 07:13:33+00:00
- **Updated**: 2022-05-15 07:13:33+00:00
- **Authors**: Mohamed Mohana, Prasanalakshmi B, Salem Alelyani, Mohammed Saleh Alsaqer
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: Recent period of pandemic has brought person identification even with occluded face image a great importance with increased number of mask usage. This paper aims to recognize the occlusion of one of four types in face images. Various transfer learning methods were tested, and the results show that MobileNet V2 with Gated Recurrent Unit(GRU) performs better than any other Transfer Learning methods, with a perfect accuracy of 99% in classification of images as with or without occlusion and if with occlusion, then the type of occlusion. In parallel, identifying the Region of interest from the device captured image is done. This extracted Region of interest is utilised in face identification. Such a face identification process is done using the ResNet model with its Caffe implementation. To reduce the execution time, after the face occlusion type was recognized the person was searched to confirm their face image in the registered database. The face label of the person obtained from both simultaneous processes was verified for their matching score. If the matching score was above 90, the recognized label of the person was logged into a file with their name, type of mask, date, and time of recognition. MobileNetV2 is a lightweight framework which can also be used in embedded or IoT devices to perform real time detection and identification in suspicious areas of investigations using CCTV footages. When MobileNetV2 was combined with GRU, a reliable accuracy was obtained. The data provided in the paper belong to two categories, being either collected from Google Images for occlusion classification, face recognition, and facial landmarks, or collected in fieldwork. The motive behind this research is to identify and log person details which could serve surveillance activities in society-based e-governance.



### Video Frame Interpolation with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.07230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07230v1)
- **Published**: 2022-05-15 09:30:28+00:00
- **Updated**: 2022-05-15 09:30:28+00:00
- **Authors**: Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, Jiaya Jia
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Video frame interpolation (VFI), which aims to synthesize intermediate frames of a video, has made remarkable progress with development of deep convolutional networks over past years. Existing methods built upon convolutional networks generally face challenges of handling large motion due to the locality of convolution operations. To overcome this limitation, we introduce a novel framework, which takes advantage of Transformer to model long-range pixel correlation among video frames. Further, our network is equipped with a novel cross-scale window-based attention mechanism, where cross-scale windows interact with each other. This design effectively enlarges the receptive field and aggregates multi-scale information. Extensive quantitative and qualitative experiments demonstrate that our method achieves new state-of-the-art results on various benchmarks.



### Combating COVID-19 using Generative Adversarial Networks and Artificial Intelligence for Medical Images: A Scoping Review
- **Arxiv ID**: http://arxiv.org/abs/2205.07236v1
- **DOI**: 10.2196/37365
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07236v1)
- **Published**: 2022-05-15 09:43:54+00:00
- **Updated**: 2022-05-15 09:43:54+00:00
- **Authors**: Hazrat Ali, Zubair Shah
- **Comment**: None
- **Journal**: JMIR Medical Informatics, 2022
- **Summary**: This review presents a comprehensive study on the role of GANs in addressing the challenges related to COVID-19 data scarcity and diagnosis. It is the first review that summarizes the different GANs methods and the lungs images datasets for COVID-19. It attempts to answer the questions related to applications of GANs, popular GAN architectures, frequently used image modalities, and the availability of source code. This review included 57 full-text studies that reported the use of GANs for different applications in COVID-19 lungs images data. Most of the studies (n=42) used GANs for data augmentation to enhance the performance of AI techniques for COVID-19 diagnosis. Other popular applications of GANs were segmentation of lungs and super-resolution of the lungs images. The cycleGAN and the conditional GAN were the most commonly used architectures used in nine studies each. 29 studies used chest X-Ray images while 21 studies used CT images for the training of GANs. For majority of the studies (n=47), the experiments were done and results were reported using publicly available data. A secondary evaluation of the results by radiologists/clinicians was reported by only two studies. Conclusion: Studies have shown that GANs have great potential to address the data scarcity challenge for lungs images of COVID-19. Data synthesized with GANs have been helpful to improve the training of the Convolutional Neural Network (CNN) models trained for the diagnosis of COVID-19. Besides, GANs have also contributed to enhancing the CNNs performance through the super-resolution of the images and segmentation. This review also identified key limitations of the potential transformation of GANs based methods in clinical applications.



### FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07246v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07246v3)
- **Published**: 2022-05-15 10:07:52+00:00
- **Updated**: 2023-01-31 10:04:52+00:00
- **Authors**: Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, Xing Xie
- **Comment**: Accepted by ICLR 2023. Code:
  https://github.com/microsoft/Semi-supervised-learning
- **Journal**: None
- **Summary**: Semi-supervised Learning (SSL) has witnessed great success owing to the impressive performances brought by various methods based on pseudo labeling and consistency regularization. However, we argue that existing methods might fail to utilize the unlabeled data more effectively since they either use a pre-defined / fixed threshold or an ad-hoc threshold adjusting scheme, resulting in inferior performance and slow convergence. We first analyze a motivating example to obtain intuitions on the relationship between the desirable threshold and model's learning status. Based on the analysis, we hence propose FreeMatch to adjust the confidence threshold in a self-adaptive manner according to the model's learning status. We further introduce a self-adaptive class fairness regularization penalty to encourage the model for diverse predictions during the early training stage. Extensive experiments indicate the superiority of FreeMatch especially when the labeled data are extremely rare. FreeMatch achieves 5.78%, 13.59%, and 1.28% error rate reduction over the latest state-of-the-art method FlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels per class, respectively. Moreover, FreeMatch can also boost the performance of imbalanced SSL. The codes can be found at https://github.com/microsoft/Semi-supervised-learning.



### Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.07260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07260v1)
- **Published**: 2022-05-15 11:22:25+00:00
- **Updated**: 2022-05-15 11:22:25+00:00
- **Authors**: Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Dong Gu Lee, Wonseok Jeong, Sang Woo Kim
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: L2 regularization for weights in neural networks is widely used as a standard training trick. However, L2 regularization for gamma, a trainable parameter of batch normalization, remains an undiscussed mystery and is applied in different ways depending on the library and practitioner. In this paper, we study whether L2 regularization for gamma is valid. To explore this issue, we consider two approaches: 1) variance control to make the residual network behave like identity mapping and 2) stable optimization through the improvement of effective learning rate. Through two analyses, we specify the desirable and undesirable gamma to apply L2 regularization and propose four guidelines for managing them. In several experiments, we observed the increase and decrease in performance caused by applying L2 regularization to gamma of four categories, which is consistent with our four guidelines. Our proposed guidelines were validated through various tasks and architectures, including variants of residual networks and transformers.



### Regulating Facial Processing Technologies: Tensions Between Legal and Technical Considerations in the Application of Illinois BIPA
- **Arxiv ID**: http://arxiv.org/abs/2205.07299v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07299v1)
- **Published**: 2022-05-15 14:38:35+00:00
- **Updated**: 2022-05-15 14:38:35+00:00
- **Authors**: Rui-Jie Yew, Alice Xiang
- **Comment**: Forthcoming at FAccT 2022
- **Journal**: None
- **Summary**: Harms resulting from the development and deployment of facial processing technologies (FPT) have been met with increasing controversy. Several states and cities in the U.S. have banned the use of facial recognition by law enforcement and governments, but FPT are still being developed and used in a wide variety of contexts where they primarily are regulated by state biometric information privacy laws. Among these laws, the 2008 Illinois Biometric Information Privacy Act (BIPA) has generated a significant amount of litigation. Yet, with most BIPA lawsuits reaching settlements before there have been meaningful clarifications of relevant technical intricacies and legal definitions, there remains a great degree of uncertainty as to how exactly this law applies to FPT. What we have found through applications of BIPA in FPT litigation so far, however, points to potential disconnects between technical and legal communities. This paper analyzes what we know based on BIPA court proceedings and highlights these points of tension: areas where the technical operationalization of BIPA may create unintended and undesirable incentives for FPT development, as well as areas where BIPA litigation can bring to light the limitations of solely technical methods in achieving legal privacy values. These factors are relevant for (i) reasoning about biometric information privacy laws as a governing mechanism for FPT, (ii) assessing the potential harms of FPT, and (iii) providing incentives for the mitigation of these harms. By illuminating these considerations, we hope to empower courts and lawmakers to take a more nuanced approach to regulating FPT and developers to better understand privacy values in the current U.S. legal landscape.



### Conditional Vector Graphics Generation for Music Cover Images
- **Arxiv ID**: http://arxiv.org/abs/2205.07301v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.07301v1)
- **Published**: 2022-05-15 14:43:03+00:00
- **Updated**: 2022-05-15 14:43:03+00:00
- **Authors**: Valeria Efimova, Ivan Jarsky, Ilya Bizyaev, Andrey Filchenkov
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have motivated a rapid growth of the domain of computer image synthesis. As almost all the existing image synthesis algorithms consider an image as a pixel matrix, the high-resolution image synthesis is complicated.A good alternative can be vector images. However, they belong to the highly sophisticated parametric space, which is a restriction for solving the task of synthesizing vector graphics by GANs. In this paper, we consider a specific application domain that softens this restriction dramatically allowing the usage of vector image synthesis.   Music cover images should meet the requirements of Internet streaming services and printing standards, which imply high resolution of graphic materials without any additional requirements on the content of such images. Existing music cover image generation services do not analyze tracks themselves; however, some services mostly consider only genre tags. To generate music covers as vector images that reflect the music and consist of simple geometric objects, we suggest a GAN-based algorithm called CoverGAN. The assessment of resulting images is based on their correspondence to the music compared with AttnGAN and DALL-E text-to-image generation according to title or lyrics. Moreover, the significance of the patterns found by CoverGAN has been evaluated in terms of the correspondence of the generated cover images to the musical tracks. Listeners evaluate the music covers generated by the proposed algorithm as quite satisfactory and corresponding to the tracks. Music cover images generation code and demo are available at https://github.com/IzhanVarsky/CoverGAN.



### Uncertainty estimation for Cross-dataset performance in Trajectory prediction
- **Arxiv ID**: http://arxiv.org/abs/2205.07310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07310v2)
- **Published**: 2022-05-15 15:28:02+00:00
- **Updated**: 2022-07-12 15:53:39+00:00
- **Authors**: Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, Fabien Moutarde
- **Comment**: Workshop on Fresh Perspectives on the Future of Autonomous Driving,
  ICRA 2022
- **Journal**: None
- **Summary**: While a lot of work has been carried on developing trajectory prediction methods, and various datasets have been proposed for benchmarking this task, little study has been done so far on the generalizability and the transferability of these methods across dataset. In this paper, we observe the performance of two of the latest state-of-the-art trajectory prediction methods across four different datasets (Argoverse, NuScenes, Interaction, Shifts). This analysis allows to gain some insights on the generalizability proprieties of most recent trajectory prediction models and to analyze which dataset is more representative of real driving scenes and therefore enables better transferability. Furthermore we present a novel method to estimate prediction uncertainty and show how it could be used to achieve better performance across datasets.



### Trucks Don't Mean Trump: Diagnosing Human Error in Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.07333v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07333v1)
- **Published**: 2022-05-15 17:06:51+00:00
- **Updated**: 2022-05-15 17:06:51+00:00
- **Authors**: J. D. Zamfirescu-Pereira, Jerry Chen, Emily Wen, Allison Koenecke, Nikhil Garg, Emma Pierson
- **Comment**: To be published in FAccT 2022
- **Journal**: None
- **Summary**: Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.



### Novel Multicolumn Kernel Extreme Learning Machine for Food Detection via Optimal Features from CNN
- **Arxiv ID**: http://arxiv.org/abs/2205.07348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07348v1)
- **Published**: 2022-05-15 18:07:43+00:00
- **Updated**: 2022-05-15 18:07:43+00:00
- **Authors**: Ghalib Ahmed, Tahir Chu, Kiong Loo
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic food detection is an emerging topic of interest due to its wide array of applications ranging from detecting food images on social media platforms to filtering non-food photos from the users in dietary assessment apps. Recently, during the COVID-19 pandemic, it has facilitated enforcing an eating ban by automatically detecting eating activities from cameras in public places. Therefore, to tackle the challenge of recognizing food images with high accuracy, we proposed the idea of a hybrid framework for extracting and selecting optimal features from an efficient neural network. There on, a nonlinear classifier is employed to discriminate between linearly inseparable feature vectors with great precision. In line with this idea, our method extracts features from MobileNetV3, selects an optimal subset of attributes by using Shapley Additive exPlanations (SHAP) values, and exploits kernel extreme learning machine (KELM) due to its nonlinear decision boundary and good generalization ability. However, KELM suffers from the 'curse of dimensionality problem' for large datasets due to the complex computation of kernel matrix with large numbers of hidden nodes. We solved this problem by proposing a novel multicolumn kernel extreme learning machine (MCKELM) which exploited the k-d tree algorithm to divide data into N subsets and trains separate KELM on each subset of data. Then, the method incorporates KELM classifiers into parallel structures and selects the top k nearest subsets during testing by using the k-d tree search for classifying input instead of the whole network. For evaluating a proposed framework large food/non-food dataset is prepared using nine publically available datasets. Experimental results showed the superiority of our method on an integrated set of measures while solving the problem of 'curse of dimensionality in KELM for large datasets.



### High-Resolution CMB Lensing Reconstruction with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07368v1
- **DOI**: None
- **Categories**: **astro-ph.CO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07368v1)
- **Published**: 2022-05-15 19:58:39+00:00
- **Updated**: 2022-05-15 19:58:39+00:00
- **Authors**: Peikai Li, Ipek Ilayda Onur, Scott Dodelson, Shreyas Chaudhari
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Next-generation cosmic microwave background (CMB) surveys are expected to provide valuable information about the primordial universe by creating maps of the mass along the line of sight. Traditional tools for creating these lensing convergence maps include the quadratic estimator and the maximum likelihood based iterative estimator. Here, we apply a generative adversarial network (GAN) to reconstruct the lensing convergence field. We compare our results with a previous deep learning approach -- Residual-UNet -- and discuss the pros and cons of each. In the process, we use training sets generated by a variety of power spectra, rather than the one used in testing the methods.



### SuperWarp: Supervised Learning and Warping on U-Net for Invariant Subvoxel-Precise Registration
- **Arxiv ID**: http://arxiv.org/abs/2205.07399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07399v1)
- **Published**: 2022-05-15 23:17:32+00:00
- **Updated**: 2022-05-15 23:17:32+00:00
- **Authors**: Sean I. Young, YaÃ«l Balbastre, Adrian V. Dalca, William M. Wells, Juan Eugenio Iglesias, Bruce Fischl
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, learning-based image registration methods have gradually moved away from direct supervision with target warps to instead use self-supervision, with excellent results in several registration benchmarks. These approaches utilize a loss function that penalizes the intensity differences between the fixed and moving images, along with a suitable regularizer on the deformation. In this paper, we argue that the relative failure of supervised registration approaches can in part be blamed on the use of regular U-Nets, which are jointly tasked with feature extraction, feature matching, and estimation of deformation. We introduce one simple but crucial modification to the U-Net that disentangles feature extraction and matching from deformation prediction, allowing the U-Net to warp the features, across levels, as the deformation field is evolved. With this modification, direct supervision using target warps begins to outperform self-supervision approaches that require segmentations, presenting new directions for registration when images do not have segmentations. We hope that our findings in this preliminary workshop paper will re-ignite research interest in supervised image registration techniques. Our code is publicly available from https://github.com/balbasty/superwarp.




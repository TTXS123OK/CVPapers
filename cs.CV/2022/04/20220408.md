# Arxiv Papers in cs.CV on 2022-04-08
### Semantic Representation and Dependency Learning for Multi-Label Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.03795v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03795v2)
- **Published**: 2022-04-08 00:55:15+00:00
- **Updated**: 2023-01-09 15:28:41+00:00
- **Authors**: Tao Pu, Mingzhan Sun, Hefeng Wu, Tianshui Chen, Ling Tian, Liang Lin
- **Comment**: accepted by Neurocomputing
- **Journal**: None
- **Summary**: Recently many multi-label image recognition (MLR) works have made significant progress by introducing pre-trained object detection models to generate lots of proposals or utilizing statistical label co-occurrence enhance the correlation among different categories. However, these works have some limitations: (1) the effectiveness of the network significantly depends on pre-trained object detection models that bring expensive and unaffordable computation; (2) the network performance degrades when there exist occasional co-occurrence objects in images, especially for the rare categories. To address these problems, we propose a novel and effective semantic representation and dependency learning (SRDL) framework to learn category-specific semantic representation for each category and capture semantic dependency among all categories. Specifically, we design a category-specific attentional regions (CAR) module to generate channel/spatial-wise attention matrices to guide model to focus on semantic-aware regions. We also design an object erasing (OE) module to implicitly learn semantic dependency among categories by erasing semantic-aware regions to regularize the network training. Extensive experiments and comparisons on two popular MLR benchmark datasets (i.e., MS-COCO and Pascal VOC 2007) demonstrate the effectiveness of the proposed framework over current state-of-the-art algorithms.



### A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.03804v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2204.03804v2)
- **Published**: 2022-04-08 01:35:19+00:00
- **Updated**: 2022-06-28 23:41:08+00:00
- **Authors**: Wanyu Bian, Qingchao Zhang, Xiaojing Ye, Yunmei Chen
- **Comment**: Provisional Accepted by MICCAI2022
- **Journal**: None
- **Summary**: Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic information but is limited in practice due to excessive data acquisition time. In this paper, we propose a novel deep-learning model for joint reconstruction and synthesis of multi-modal MRI using incomplete k-space data of several source modalities as inputs. The output of our model includes reconstructed images of the source modalities and high-quality image synthesized in the target modality. Our proposed model is formulated as a variational problem that leverages several learnable modality-specific feature extractors and a multimodal synthesis module. We propose a learnable optimization algorithm to solve this model, which induces a multi-phase network whose parameters can be trained using multi-modal MRI data. Moreover, a bilevel-optimization framework is employed for robust parameter training. We demonstrate the effectiveness of our approach using extensive numerical experiments.



### Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2204.04215v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04215v2)
- **Published**: 2022-04-08 01:56:51+00:00
- **Updated**: 2022-06-22 02:54:19+00:00
- **Authors**: Yefei He, Luoming Zhang, Weijia Wu, Hong Zhou
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Data-free quantization is a task that compresses the neural network to low bit-width without access to original training data. Most existing data-free quantization methods cause severe performance degradation due to inaccurate activation clipping range and quantization error, especially for low bit-width. In this paper, we present a simple yet effective data-free quantization method with accurate activation clipping and adaptive batch normalization. Accurate activation clipping (AAC) improves the model accuracy by exploiting accurate activation information from the full-precision model. Adaptive batch normalization firstly proposes to address the quantization error from distribution changes by updating the batch normalization layer adaptively. Extensive experiments demonstrate that the proposed data-free quantization method can yield surprisingly performance, achieving 64.33% top-1 accuracy of ResNet18 on ImageNet dataset, with 3.7% absolute improvement outperforming the existing state-of-the-art methods.



### Canonical Mean Filter for Almost Zero-Shot Multi-Task classification
- **Arxiv ID**: http://arxiv.org/abs/2204.03815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03815v1)
- **Published**: 2022-04-08 02:28:04+00:00
- **Updated**: 2022-04-08 02:28:04+00:00
- **Authors**: Yong Li, Heng Wang, Xiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: The support set is a key to providing conditional prior for fast adaption of the model in few-shot tasks. But the strict form of support set makes its construction actually difficult in practical application. Motivated by ANIL, we rethink the role of adaption in the feature extractor of CNAPs, which is a state-of-the-art representative few-shot method. To investigate the role, Almost Zero-Shot (AZS) task is designed by fixing the support set to replace the common scheme, which provides corresponding support sets for the different conditional prior of different tasks. The AZS experiment results infer that the adaptation works little in the feature extractor. However, CNAPs cannot be robust to randomly selected support sets and perform poorly on some datasets of Meta-Dataset because of its scattered mean embeddings responded by the simple mean operator. To enhance the robustness of CNAPs, Canonical Mean Filter (CMF) module is proposed to make the mean embeddings intensive and stable in feature space by mapping the support sets into a canonical form. CMFs make CNAPs robust to any fixed support sets even if they are random matrices. This attribution makes CNAPs be able to remove the mean encoder and the parameter adaptation network at the test stage, while CNAP-CMF on AZS tasks keeps the performance with one-shot tasks. It leads to a big parameter reduction. Precisely, 40.48\% parameters are dropped at the test stage. Also, CNAP-CMF outperforms CNAPs in one-shot tasks because it addresses inner-task unstable performance problems. Classification performance, visualized and clustering results verify that CMFs make CNAPs better and simpler.



### Learning Trajectory-Aware Transformer for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.04216v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04216v3)
- **Published**: 2022-04-08 03:37:39+00:00
- **Updated**: 2022-04-20 04:38:21+00:00
- **Authors**: Chengxu Liu, Huan Yang, Jianlong Fu, Xueming Qian
- **Comment**: CVPR 2022 Oral
- **Journal**: None
- **Summary**: Video super-resolution (VSR) aims to restore a sequence of high-resolution (HR) frames from their low-resolution (LR) counterparts. Although some progress has been made, there are grand challenges to effectively utilize temporal dependency in entire video sequences. Existing approaches usually align and aggregate video frames from limited adjacent frames (e.g., 5 or 7 frames), which prevents these approaches from satisfactory results. In this paper, we take one step further to enable effective spatio-temporal learning in videos. We propose a novel Trajectory-aware Transformer for Video Super-Resolution (TTVSR). In particular, we formulate video frames into several pre-aligned trajectories which consist of continuous visual tokens. For a query token, self-attention is only learned on relevant visual tokens along spatio-temporal trajectories. Compared with vanilla vision Transformers, such a design significantly reduces the computational cost and enables Transformers to model long-range features. We further propose a cross-scale feature tokenization module to overcome scale-changing problems that often occur in long-range videos. Experimental results demonstrate the superiority of the proposed TTVSR over state-of-the-art models, by extensive quantitative and qualitative evaluations in four widely-used video super-resolution benchmarks. Both code and pre-trained models can be downloaded at https://github.com/researchmm/TTVSR.



### Feature-enhanced Adversarial Semi-supervised Semantic Segmentation Network for Pulmonary Embolism Annotation
- **Arxiv ID**: http://arxiv.org/abs/2204.04217v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04217v1)
- **Published**: 2022-04-08 04:21:02+00:00
- **Updated**: 2022-04-08 04:21:02+00:00
- **Authors**: Ting-Wei Cheng, Jerry Chang, Ching-Chun Huang, Chin Kuo, Yun-Chien Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: This study established a feature-enhanced adversarial semi-supervised semantic segmentation model to automatically annotate pulmonary embolism lesion areas in computed tomography pulmonary angiogram (CTPA) images. In current studies, all of the PE CTPA image segmentation methods are trained by supervised learning. However, the supervised learning models need to be retrained and the images need to be relabeled when the CTPA images come from different hospitals. This study proposed a semi-supervised learning method to make the model applicable to different datasets by adding a small amount of unlabeled images. By training the model with both labeled and unlabeled images, the accuracy of unlabeled images can be improved and the labeling cost can be reduced. Our semi-supervised segmentation model includes a segmentation network and a discriminator network. We added feature information generated from the encoder of segmentation network to the discriminator so that it can learn the similarity between predicted mask and ground truth mask. This HRNet-based architecture can maintain a higher resolution for convolutional operations so the prediction of small PE lesion areas can be improved. We used the labeled open-source dataset and the unlabeled National Cheng Kung University Hospital (NCKUH) (IRB number: B-ER-108-380) dataset to train the semi-supervised learning model, and the resulting mean intersection over union (mIOU), dice score, and sensitivity achieved 0.3510, 0.4854, and 0.4253, respectively on the NCKUH dataset. Then, we fine-tuned and tested the model with a small amount of unlabeled PE CTPA images from China Medical University Hospital (CMUH) (IRB number: CMUH110-REC3-173) dataset. Comparing the results of our semi-supervised model with the supervised model, the mIOU, dice score, and sensitivity improved from 0.2344, 0.3325, and 0.3151 to 0.3721, 0.5113, and 0.4967, respectively.



### Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.03838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03838v1)
- **Published**: 2022-04-08 04:40:18+00:00
- **Updated**: 2022-04-08 04:40:18+00:00
- **Authors**: Lin Chen, Huaian Chen, Zhixiang Wei, Xin Jin, Xiao Tan, Yi Jin, Enhong Chen
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Adversarial learning has achieved remarkable performances for unsupervised domain adaptation (UDA). Existing adversarial UDA methods typically adopt an additional discriminator to play the min-max game with a feature extractor. However, most of these methods failed to effectively leverage the predicted discriminative information, and thus cause mode collapse for generator. In this work, we address this problem from a different perspective and design a simple yet effective adversarial paradigm in the form of a discriminator-free adversarial learning network (DALN), wherein the category classifier is reused as a discriminator, which achieves explicit domain alignment and category distinguishment through a unified objective, enabling the DALN to leverage the predicted discriminative information for sufficient feature alignment. Basically, we introduce a Nuclear-norm Wasserstein discrepancy (NWD) that has definite guidance meaning for performing discrimination. Such NWD can be coupled with the classifier to serve as a discriminator satisfying the K-Lipschitz constraint without the requirements of additional weight clipping or gradient penalty strategy. Without bells and whistles, DALN compares favorably against the existing state-of-the-art (SOTA) methods on a variety of public datasets. Moreover, as a plug-and-play technique, NWD can be directly used as a generic regularizer to benefit existing UDA algorithms. Code is available at https://github.com/xiaoachen98/DALN.



### From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion
- **Arxiv ID**: http://arxiv.org/abs/2204.03842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03842v2)
- **Published**: 2022-04-08 05:11:04+00:00
- **Updated**: 2022-07-06 11:55:06+00:00
- **Authors**: Weiguang Zhao, Chaolong Yang, Jianan Ye, Yuyao Yan, Xi Yang, Kaizhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of Multi-view 3D Face Reconstruction (MVR) with weakly supervised learning that leverages a limited number of 2D face images (e.g. 3) to generate a high-quality 3D face model with very light annotation. Despite their encouraging performance, present MVR methods simply concatenate multi-view image features and pay less attention to critical areas (e.g. eye, brow, nose, and mouth). To this end, we propose a novel model called Deep Fusion MVR (DF-MVR) and design a multi-view encoding to a single decoding framework with skip connections, able to extract, integrate, and compensate deep features with attention from multi-view images. In addition, we develop a multi-view face parse network to learn, identify, and emphasize the critical common face area. Finally, though our model is trained with a few 2D images, it can reconstruct an accurate 3D model even if one single 2D image is input. We conduct extensive experiments to evaluate various multi-view 3D face reconstruction methods. Experiments on Pixel-Face and Bosphorus datasets indicate the superiority of our model. Without 3D landmarks annotation, DF-MVR achieves 5.2% and 3.0% RMSE improvements over the existing best weakly supervised MVRs respectively on Pixel-Face and Bosphorus datasets; with 3D landmarks annotation, DF-MVR attains superior performance particularly on Pixel-Face dataset, leading to 13.4% RMSE improvement over the best weakly supervised MVR model.



### Prediction of COVID-19 using chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2204.03849v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.03849v1)
- **Published**: 2022-04-08 05:23:24+00:00
- **Updated**: 2022-04-08 05:23:24+00:00
- **Authors**: Narayana Darapaneni, Suma Maram, Harpreet Singh, Syed Subhani, Mandeep Kour, Sathish Nagam, Anwesh Reddy Paduri
- **Comment**: None
- **Journal**: None
- **Summary**: COVID-19, also known as Novel Coronavirus Disease, is a highly contagious disease that first surfaced in China in late 2019. SARS-CoV-2 is a coronavirus that belongs to the vast family of coronaviruses that causes this disease. The sickness originally appeared in Wuhan, China in December 2019 and quickly spread to over 213 nations, becoming a global pandemic. Fever, dry cough, and tiredness are the most typical COVID-19 symptoms. Aches, pains, and difficulty breathing are some of the other symptoms that patients may face. The majority of these symptoms are indicators of respiratory infections and lung abnormalities, which radiologists can identify. Chest x-rays of COVID-19 patients seem similar, with patchy and hazy lungs rather than clear and healthy lungs. On x-rays, however, pneumonia and other chronic lung disorders can resemble COVID-19. Trained radiologists must be able to distinguish between COVID-19 and an illness that is less contagious. Our AI algorithm seeks to give doctors a quantitative estimate of the risk of deterioration. So that patients at high risk of deterioration can be triaged and treated efficiently. The method could be particularly useful in pandemic hotspots when screening upon admission is important for allocating limited resources like hospital beds.



### Multi-scale temporal network for continuous sign language recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.03864v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03864v2)
- **Published**: 2022-04-08 06:14:22+00:00
- **Updated**: 2022-08-16 06:36:09+00:00
- **Authors**: Qidan Zhu, Jing Li, Fei Yuan, Quan Gan
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Continuous Sign Language Recognition (CSLR) is a challenging research task due to the lack of accurate annotation on the temporal sequence of sign language data. The recent popular usage is a hybrid model based on "CNN + RNN" for CSLR. However, when extracting temporal features in these works, most of the methods using a fixed temporal receptive field and cannot extract the temporal features well for each sign language word. In order to obtain more accurate temporal features, this paper proposes a multi-scale temporal network (MSTNet). The network mainly consists of three parts. The Resnet and two fully connected (FC) layers constitute the frame-wise feature extraction part. The time-wise feature extraction part performs temporal feature learning by first extracting temporal receptive field features of different scales using the proposed multi-scale temporal block (MST-block) to improve the temporal modeling capability, and then further encoding the temporal features of different scales by the transformers module to obtain more accurate temporal features. Finally, the proposed multi-level Connectionist Temporal Classification (CTC) loss part is used for training to obtain recognition results. The multi-level CTC loss enables better learning and updating of the shallow network parameters in CNN, and the method has no parameter increase and can be flexibly embedded in other models. Experimental results on two publicly available datasets demonstrate that our method can effectively extract sign language features in an end-to-end manner without any prior knowledge, improving the accuracy of CSLR and achieving competitive results.



### Optical flow GNSS for navigation in the Indian subcontinent (NavIC)
- **Arxiv ID**: http://arxiv.org/abs/2204.05980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05980v1)
- **Published**: 2022-04-08 06:19:10+00:00
- **Updated**: 2022-04-08 06:19:10+00:00
- **Authors**: Sunit Shantanu Digamber Fulari, Harbinder Singh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reveals about global navigation satellite system GNSS in the indian subcontinent known as the navigation in the indian subcontinent(NavIC) We have tried to model a new technique in GNSS known as the optical flow tracking global navigation system (OF GNSS). This method using differential equations is very accurate for very small distances on the surface of the earth in the 1500km range of the Indian subcontinent satellite coverage. When we talk of accuracy of the GPS system it should be very accurate on the surface of the earth when used to show changes in coordinate of the moving body with respect to the ground by the satellite which is situated on the earths orbit. Optical flow is a method which uses movements with respect to x and y axis for infinitesimal changes in its coordinates and then uses this algorithm to use it in global positioning system to find accurate position of the body with respect to the satellite coordinates with respect to ground positioning. The modern method of differential frames is also very accurate as it involves infinitesimal frames which are modelled together from the satellite to find changes in the coordinates on the earths surface, so we have designed a new algorithm in this paper on the Optical flow GNSS system which is an alternative and can improve the study done in the design of these algorithms in this field of applications.



### Frequency Selective Augmentation for Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.03865v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03865v2)
- **Published**: 2022-04-08 06:19:32+00:00
- **Updated**: 2022-12-06 07:44:07+00:00
- **Authors**: Jinhyung Kim, Taeoh Kim, Minho Shim, Dongyoon Han, Dongyoon Wee, Junmo Kim
- **Comment**: AAAI23
- **Journal**: None
- **Summary**: Recent self-supervised video representation learning methods focus on maximizing the similarity between multiple augmented views from the same video and largely rely on the quality of generated views. However, most existing methods lack a mechanism to prevent representation learning from bias towards static information in the video. In this paper, we propose frequency augmentation (FreqAug), a spatio-temporal data augmentation method in the frequency domain for video representation learning. FreqAug stochastically removes specific frequency components from the video so that learned representation captures essential features more from the remaining information for various downstream tasks. Specifically, FreqAug pushes the model to focus more on dynamic features rather than static features in the video via dropping spatial or temporal low-frequency components. To verify the generality of the proposed method, we experiment with FreqAug on multiple self-supervised learning frameworks along with standard augmentations. Transferring the improved representation to five video action recognition and two temporal action localization downstream tasks shows consistent improvements over baselines.



### Controllable Missingness from Uncontrollable Missingness: Joint Learning Measurement Policy and Imputation
- **Arxiv ID**: http://arxiv.org/abs/2204.03872v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.03872v1)
- **Published**: 2022-04-08 06:51:37+00:00
- **Updated**: 2022-04-08 06:51:37+00:00
- **Authors**: Seongwook Yoon, Jaehyun Kim, Heejeong Lim, Sanghoon Sull
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the cost or interference of measurement, we need to control measurement system. Assuming that each variable can be measured sequentially, there exists optimal policy choosing next measurement for the former observations. Though optimal measurement policy is actually dependent on the goal of measurement, we mainly focus on retrieving complete data, so called as imputation. Also, we adapt the imputation method to missingness varying with measurement policy. However, learning measurement policy and imputation requires complete data which is impossible to be observed, unfortunately. To tackle this problem, we propose a data generation method and joint learning algorithm. The main idea is that 1) the data generation method is inherited by imputation method, and 2) the adaptation of imputation encourages measurement policy to learn more than individual learning. We implemented some variations of proposed algorithm for two different datasets and various missing rates. From the experimental results, we demonstrate that our algorithm is generally applicable and outperforms baseline methods.



### Spatial Transformer Network on Skeleton-based Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.03873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03873v1)
- **Published**: 2022-04-08 06:53:23+00:00
- **Updated**: 2022-04-08 06:53:23+00:00
- **Authors**: Cun Zhang, Xing-Peng Chen, Guo-Qiang Han, Xiang-Jie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based gait recognition models usually suffer from the robustness problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to 70\% in walking with coats cases. In this work, we propose a state-of-the-art robust skeleton-based gait recognition model called Gait-TR, which is based on the combination of spatial transformer frameworks and temporal convolutional networks. Gait-TR achieves substantial improvements over other skeleton-based gait models with higher accuracy and better robustness on the well-known gait dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\% Rank-1 gait recognition accuracy rate, which is higher than the best result of silhouette-based models, which usually have higher accuracy than the silhouette-based gait recognition models. Moreover, our experiment on CASIA-B shows that the spatial transformer can extract gait features from the human skeleton better than the widely used graph convolutional network.



### CD$^2$-pFed: Cyclic Distillation-guided Channel Decoupling for Model Personalization in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.03880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03880v1)
- **Published**: 2022-04-08 07:13:30+00:00
- **Updated**: 2022-04-08 07:13:30+00:00
- **Authors**: Yiqing Shen, Yuyin Zhou, Lequan Yu
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Federated learning (FL) is a distributed learning paradigm that enables multiple clients to collaboratively learn a shared global model. Despite the recent progress, it remains challenging to deal with heterogeneous data clients, as the discrepant data distributions usually prevent the global model from delivering good generalization ability on each participating client. In this paper, we propose CD^2-pFed, a novel Cyclic Distillation-guided Channel Decoupling framework, to personalize the global model in FL, under various settings of data heterogeneity. Different from previous works which establish layer-wise personalization to overcome the non-IID data across different clients, we make the first attempt at channel-wise assignment for model personalization, referred to as channel decoupling. To further facilitate the collaboration between private and shared weights, we propose a novel cyclic distillation scheme to impose a consistent regularization between the local and global model representations during the federation. Guided by the cyclical distillation, our channel decoupling framework can deliver more accurate and generalized results for different kinds of heterogeneity, such as feature skew, label distribution skew, and concept shift. Comprehensive experiments on four benchmarks, including natural image and medical image analysis tasks, demonstrate the consistent effectiveness of our method on both local and external validations.



### Vision Transformers for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2204.03883v1
- **DOI**: 10.1109/TIP.2023.3256763
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03883v1)
- **Published**: 2022-04-08 07:17:20+00:00
- **Updated**: 2022-04-08 07:17:20+00:00
- **Authors**: Yuda Song, Zhuqing He, Hui Qian, Xin Du
- **Comment**: None
- **Journal**: None
- **Summary**: Image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. In recent years, convolutional neural network-based methods have dominated image dehazing. However, vision Transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. We start with the popular Swin Transformer and find that several of its key designs are unsuitable for image dehazing. To this end, we propose DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. We train multiple variants of DehazeFormer on various datasets to demonstrate its effectiveness. Specifically, on the most frequently used SOTS indoor set, our small model outperforms FFA-Net with only 25% #Param and 5% computational cost. To the best of our knowledge, our large model is the first method with the PSNR over 40 dB on the SOTS indoor set, dramatically outperforming the previous state-of-the-art methods. We also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method's capability to remove highly non-homogeneous haze.



### Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.04218v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04218v3)
- **Published**: 2022-04-08 07:56:55+00:00
- **Updated**: 2022-10-12 08:40:06+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Andreea-Iuliana Miron, Olivian Savencu, Nicolae-Catalin Ristea, Nicolae Verga, Fahad Shahbaz Khan
- **Comment**: Accepted at WACV 2023 (main paper + supplementary)
- **Journal**: None
- **Summary**: Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques capture several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multimodal multi-head convolutional attention module to super-resolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention, and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for super-resolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads. Our code is freely available at https://github.com/lilygeorgescu/MHCA.



### Towards Reliable and Explainable AI Model for Solid Pulmonary Nodule Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2204.04219v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04219v1)
- **Published**: 2022-04-08 08:21:00+00:00
- **Updated**: 2022-04-08 08:21:00+00:00
- **Authors**: Chenglong Wang, Yun Liu, Fen Wang, Chengxiu Zhang, Yida Wang, Mei Yuan, Guang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Lung cancer has the highest mortality rate of deadly cancers in the world. Early detection is essential to treatment of lung cancer. However, detection and accurate diagnosis of pulmonary nodules depend heavily on the experiences of radiologists and can be a heavy workload for them. Computer-aided diagnosis (CAD) systems have been developed to assist radiologists in nodule detection and diagnosis, greatly easing the workload while increasing diagnosis accuracy. Recent development of deep learning, greatly improved the performance of CAD systems. However, lack of model reliability and interpretability remains a major obstacle for its large-scale clinical application. In this work, we proposed a multi-task explainable deep-learning model for pulmonary nodule diagnosis. Our neural model can not only predict lesion malignancy but also identify relevant manifestations. Further, the location of each manifestation can also be visualized for visual interpretability. Our proposed neural model achieved a test AUC of 0.992 on LIDC public dataset and a test AUC of 0.923 on our in-house dataset. Moreover, our experimental results proved that by incorporating manifestation identification tasks into the multi-task model, the accuracy of the malignancy classification can also be improved. This multi-task explainable model may provide a scheme for better interaction with the radiologists in a clinical environment.



### A Survey of Supernet Optimization and its Applications: Spatial and Temporal Optimization for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2204.03916v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03916v2)
- **Published**: 2022-04-08 08:29:52+00:00
- **Updated**: 2023-02-07 07:33:25+00:00
- **Authors**: Stephen Cha, Taehyeon Kim, Hayeon Lee, Se-Young Yun
- **Comment**: None
- **Journal**: None
- **Summary**: This survey focuses on categorizing and evaluating the methods of supernet optimization in the field of Neural Architecture Search (NAS). Supernet optimization involves training a single, over-parameterized network that encompasses the search space of all possible network architectures. The survey analyses supernet optimization methods based on their approaches to spatial and temporal optimization. Spatial optimization relates to optimizing the architecture and parameters of the supernet and its subnets, while temporal optimization deals with improving the efficiency of selecting architectures from the supernet. The benefits, limitations, and potential applications of these methods in various tasks and settings, including transferability, domain generalization, and Transformer models, are also discussed.



### Biometric identification by means of hand geometry and a neural net classifier
- **Arxiv ID**: http://arxiv.org/abs/2204.03925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03925v1)
- **Published**: 2022-04-08 08:40:45+00:00
- **Updated**: 2022-04-08 08:40:45+00:00
- **Authors**: Marcos Faundez-Zanuy, Guillermo Mar Navarro MÃ©rida
- **Comment**: 8 pages, published in In Proceedings of the 8th international
  conference on Artificial Neural Networks computational Intelligence and
  Bioinspired Systems (IWANN 2005). Springer Verlag, Berlin, Heidelberg, 1172
  1179
- **Journal**: International Work-Conference on Artificial Neural Networks
  (IWANN) 2005, Vilanova i la Geltr\'u (Spain)
- **Summary**: This Paper describes a hand geometry biometric identification system. We have acquired a database of 22 people using a conventional document scanner. The experimental section consists of a study about the discrimination capability of different extracted features, and the identification rate using different classifiers based on neural networks.



### Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection
- **Arxiv ID**: http://arxiv.org/abs/2204.03929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03929v1)
- **Published**: 2022-04-08 08:46:27+00:00
- **Updated**: 2022-04-08 08:46:27+00:00
- **Authors**: Chunyu Li, Yusuke Monno, Masatoshi Okutomi
- **Comment**: Accepted by CVPR 2022. Project homepage:
  http://www.ok.sc.e.titech.ac.jp/res/DHD/
- **Journal**: None
- **Summary**: Depth reconstruction and hyperspectral reflectance reconstruction are two active research topics in computer vision and image processing. Conventionally, these two topics have been studied separately using independent imaging setups and there is no existing method which can acquire depth and spectral reflectance simultaneously in one shot without using special hardware. In this paper, we propose a novel single-shot hyperspectral-depth reconstruction method using an off-the-shelf RGB camera and projector. Our method is based on a single color-dot projection, which simultaneously acts as structured light for depth reconstruction and spatially-varying color illuminations for hyperspectral reflectance reconstruction. To jointly reconstruct the depth and the hyperspectral reflectance from a single color-dot image, we propose a novel end-to-end network architecture that effectively incorporates a geometric color-dot pattern loss and a photometric hyperspectral reflectance loss. Through the experiments, we demonstrate that our hyperspectral-depth reconstruction method outperforms the combination of an existing state-of-the-art single-shot hyperspectral reflectance reconstruction method and depth reconstruction method.



### Does Robustness on ImageNet Transfer to Downstream Tasks?
- **Arxiv ID**: http://arxiv.org/abs/2204.03934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03934v1)
- **Published**: 2022-04-08 08:55:34+00:00
- **Updated**: 2022-04-08 08:55:34+00:00
- **Authors**: Yutaro Yamada, Mayu Otani
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classification. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classification from different domains. This raises a question: Can these robust image classifiers transfer robustness to downstream tasks? For object detection and semantic segmentation, we find that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classification, we find that models that are robustified for ImageNet do not retain robustness when fully fine-tuned. These findings suggest that current robustification techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.



### Study of a committee of neural networks for biometric hand-geometry recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.03935v1
- **DOI**: 10.1007/11494669_145
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03935v1)
- **Published**: 2022-04-08 08:56:42+00:00
- **Updated**: 2022-04-08 08:56:42+00:00
- **Authors**: Marcos Faundez-Zanuy
- **Comment**: 9 pages published in Proceedings of the 8th international conference
  on Artificial Neural Networks: computational Intelligence and Bioinspired
  Systems (IWANN'05). Springer Verlag, Berlin, Heidelberg, 1180 1187
- **Journal**: International Work-Conference on Artificial Neural Networks
  (Iwann) 2005, Vilanova i la Geltr\'u (Spain)
- **Summary**: This Paper studies different committees of neural networks for biometric pattern recognition. We use the neural nets as classifiers for identification and verification purposes. We show that a committee of nets can improve the recognition rates when compared with a multi-start initialization algo-rithm that just picks up the neural net which offers the best performance. On the other hand, we found that there is no strong correlation between identifi-cation and verification applications using the same classifier.



### On Distinctive Image Captioning via Comparing and Reweighting
- **Arxiv ID**: http://arxiv.org/abs/2204.03938v1
- **DOI**: 10.1109/TPAMI.2022.3159811
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.03938v1)
- **Published**: 2022-04-08 08:59:23+00:00
- **Updated**: 2022-04-08 08:59:23+00:00
- **Authors**: Jiuniu Wang, Wenjia Xu, Qingzhong Wang, Antoni B. Chan
- **Comment**: 20 pages. arXiv admin note: substantial text overlap with
  arXiv:2007.06877
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI 2022)
- **Summary**: Recent image captioning models are achieving impressive results based on popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most popular metrics that only consider the overlap between the generated captions and human annotation could result in using common words and phrases, which lacks distinctiveness, i.e., many similar images have the same caption. In this paper, we aim to improve the distinctiveness of image captions via comparing and reweighting with a set of similar images. First, we propose a distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric reveals that the human annotations of each image in the MSCOCO dataset are not equivalent based on distinctiveness; however, previous works normally treat the human annotations equally during training, which could be a reason for generating less distinctive captions. In contrast, we reweight each ground-truth caption according to its distinctiveness during training. We further integrate a long-tailed weight strategy to highlight the rare words that contain more information, and captions from the similar image set are sampled as negative examples to encourage the generated sentence to be unique. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study.



### Probabilistic Representations for Video Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.03946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03946v1)
- **Published**: 2022-04-08 09:09:30+00:00
- **Updated**: 2022-04-08 09:09:30+00:00
- **Authors**: Jungin Park, Jiyoung Lee, Ig-Jae Kim, Kwanghoon Sohn
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: This paper presents Probabilistic Video Contrastive Learning, a self-supervised representation learning method that bridges contrastive learning with probabilistic representation. We hypothesize that the clips composing the video have different distributions in short-term duration, but can represent the complicated and sophisticated video distribution through combination in a common embedding space. Thus, the proposed method represents video clips as normal distributions and combines them into a Mixture of Gaussians to model the whole video distribution. By sampling embeddings from the whole video distribution, we can circumvent the careful sampling strategy or transformations to generate augmented views of the clips, unlike previous deterministic methods that have mainly focused on such sample generation strategies for contrastive learning. We further propose a stochastic contrastive loss to learn proper video distributions and handle the inherent uncertainty from the nature of the raw video. Experimental results verify that our probabilistic embedding stands as a state-of-the-art video representation learning for action recognition and video retrieval on the most popular benchmarks, including UCF101 and HMDB51.



### Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.03957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03957v1)
- **Published**: 2022-04-08 09:31:24+00:00
- **Updated**: 2022-04-08 09:31:24+00:00
- **Authors**: Axel Berg, Magnus Oskarsson, Mark O'Connor
- **Comment**: Accepted to the 26th International Conference on Pattern Recognition
- **Journal**: None
- **Summary**: While the Transformer architecture has become ubiquitous in the machine learning field, its adaptation to 3D shape recognition is non-trivial. Due to its quadratic computational complexity, the self-attention operator quickly becomes inefficient as the set of input points grows larger. Furthermore, we find that the attention mechanism struggles to find useful connections between individual points on a global scale. In order to alleviate these problems, we propose a two-stage Point Transformer-in-Transformer (Point-TnT) approach which combines local and global attention mechanisms, enabling both individual points and patches of points to attend to each other effectively. Experiments on shape classification show that such an approach provides more useful features for downstream tasks than the baseline Transformer, while also being more computationally efficient. In addition, we also extend our method to feature matching for scene reconstruction, showing that it can be used in conjunction with existing scene reconstruction pipelines.



### SnapMode: An Intelligent and Distributed Large-Scale Fashion Image Retrieval Platform Based On Big Data and Deep Generative Adversarial Network Technologies
- **Arxiv ID**: http://arxiv.org/abs/2204.03998v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03998v1)
- **Published**: 2022-04-08 11:08:03+00:00
- **Updated**: 2022-04-08 11:08:03+00:00
- **Authors**: Narges Norouzi, Reza Azmi, Sara Saberi Tehrani Moghadam, Maral Zarvani
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion is now among the largest industries worldwide, for it represents human history and helps tell the worlds story. As a result of the Fourth Industrial Revolution, the Internet has become an increasingly important source of fashion information. However, with a growing number of web pages and social data, it is nearly impossible for humans to manually catch up with the ongoing evolution and the continuously variable content in this domain. The proper management and exploitation of big data can pave the way for the substantial growth of the global economy as well as citizen satisfaction. Therefore, computer scientists have found it challenging to handle e-commerce fashion websites by using big data and machine learning technologies. This paper first proposes a scalable focused Web Crawler engine based on the distributed computing platforms to extract and process fashion data on e-commerce websites. The role of the proposed platform is then described in developing a disentangled feature extraction method by employing deep convolutional generative adversarial networks (DCGANs) for content-based image indexing and retrieval. Finally, the state-of-the-art solutions are compared, and the results of the proposed approach are analyzed on a standard dataset. For the real-life implementation of the proposed solution, a Web-based application is developed on Apache Storm, Kafka, Solr, and Milvus platforms to create a fashion search engine called SnapMode.



### Multimodal Quasi-AutoRegression: Forecasting the visual popularity of new fashion products
- **Arxiv ID**: http://arxiv.org/abs/2204.04014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2204.04014v2)
- **Published**: 2022-04-08 11:53:54+00:00
- **Updated**: 2022-05-05 11:43:15+00:00
- **Authors**: Stefanos I. Papadopoulos, Christos Koutlis, Symeon Papadopoulos, Ioannis Kompatsiaris
- **Comment**: 14 pages, 4 figures
- **Journal**: None
- **Summary**: Estimating the preferences of consumers is of utmost importance for the fashion industry as appropriately leveraging this information can be beneficial in terms of profit. Trend detection in fashion is a challenging task due to the fast pace of change in the fashion industry. Moreover, forecasting the visual popularity of new garment designs is even more demanding due to lack of historical data. To this end, we propose MuQAR, a Multimodal Quasi-AutoRegressive deep learning architecture that combines two modules: (1) a multi-modal multi-layer perceptron processing categorical, visual and textual features of the product and (2) a quasi-autoregressive neural network modelling the "target" time series of the product's attributes along with the "exogenous" time series of all other attributes. We utilize computer vision, image classification and image captioning, for automatically extracting visual features and textual descriptions from the images of new products. Product design in fashion is initially expressed visually and these features represent the products' unique characteristics without interfering with the creative process of its designers by requiring additional inputs (e.g manually written texts). We employ the product's target attributes time series as a proxy of temporal popularity patterns, mitigating the lack of historical data, while exogenous time series help capture trends among interrelated attributes. We perform an extensive ablation analysis on two large scale image fashion datasets, Mallzee and SHIFT15m to assess the adequacy of MuQAR and also use the Amazon Reviews: Home and Kitchen dataset to assess generalisability to other domains. A comparative study on the VISUELLE dataset, shows that MuQAR is capable of competing and surpassing the domain's current state of the art by 4.65% and 4.8% in terms of WAPE and MAE respectively.



### Engagement Detection with Multi-Task Training in E-Learning Environments
- **Arxiv ID**: http://arxiv.org/abs/2204.04020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04020v1)
- **Published**: 2022-04-08 12:08:58+00:00
- **Updated**: 2022-04-08 12:08:58+00:00
- **Authors**: Onur Copur, Mert NakÄ±p, Simone Scardapane, JÃ¼rgen Slowack
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of user interaction, in particular engagement detection, became highly crucial for online working and learning environments, especially during the COVID-19 outbreak. Such recognition and detection systems significantly improve the user experience and efficiency by providing valuable feedback. In this paper, we propose a novel Engagement Detection with Multi-Task Training (ED-MTT) system which minimizes mean squared error and triplet loss together to determine the engagement level of students in an e-learning environment. The performance of this system is evaluated and compared against the state-of-the-art on a publicly available dataset as well as videos collected from real-life scenarios. The results show that ED-MTT achieves 6% lower MSE than the best state-of-the-art performance with highly acceptable training time and lightweight feature extraction.



### A Generic Image Retrieval Method for Date Estimation of Historical Document Collections
- **Arxiv ID**: http://arxiv.org/abs/2204.04028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2204.04028v1)
- **Published**: 2022-04-08 12:30:39+00:00
- **Updated**: 2022-04-08 12:30:39+00:00
- **Authors**: AdriÃ  Molina, Lluis Gomez, Oriol Ramos Terrades, Josep LladÃ³s
- **Comment**: Preprint of paper accepted at DAS2022
- **Journal**: None
- **Summary**: Date estimation of historical document images is a challenging problem, with several contributions in the literature that lack of the ability to generalize from one dataset to others. This paper presents a robust date estimation system based in a retrieval approach that generalizes well in front of heterogeneous collections. we use a ranking loss function named smooth-nDCG to train a Convolutional Neural Network that learns an ordination of documents for each problem. One of the main usages of the presented approach is as a tool for historical contextual retrieval. It means that scholars could perform comparative analysis of historical images from big datasets in terms of the period where they were produced. We provide experimental evaluation on different types of documents from real datasets of manuscript and newspaper images.



### Efficient tracking of team sport players with few game-specific annotations
- **Arxiv ID**: http://arxiv.org/abs/2204.04049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04049v1)
- **Published**: 2022-04-08 13:11:30+00:00
- **Updated**: 2022-04-08 13:11:30+00:00
- **Authors**: Adrien Maglo, Astrid Orcesi, Quoc-Cuong Pham
- **Comment**: Accepted to 2022 8th International Workshop on Computer Vision in
  Sports (CVsports 2022)
- **Journal**: None
- **Summary**: One of the requirements for team sports analysis is to track and recognize players. Many tracking and reidentification methods have been proposed in the context of video surveillance. They show very convincing results when tested on public datasets such as the MOT challenge. However, the performance of these methods are not as satisfactory when applied to player tracking. Indeed, in addition to moving very quickly and often being occluded, the players wear the same jersey, which makes the task of reidentification very complex. Some recent tracking methods have been developed more specifically for the team sport context. Due to the lack of public data, these methods use private datasets that make impossible a comparison with them. In this paper, we propose a new generic method to track team sport players during a full game thanks to few human annotations collected via a semi-interactive system. Non-ambiguous tracklets and their appearance features are automatically generated with a detection and a reidentification network both pre-trained on public datasets. Then an incremental learning mechanism trains a Transformer to classify identities using few game-specific human annotations. Finally, tracklets are linked by an association algorithm. We demonstrate the efficiency of our approach on a challenging rugby sevens dataset. To overcome the lack of public sports tracking dataset, we publicly release this dataset at https://kalisteo.cea.fr/index.php/free-resources/. We also show that our method is able to track rugby sevens players during a full match, if they are observable at a minimal resolution, with the annotation of only 6 few seconds length tracklets per player.



### Identifying Ambiguous Similarity Conditions via Semantic Matching
- **Arxiv ID**: http://arxiv.org/abs/2204.04053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04053v1)
- **Published**: 2022-04-08 13:15:55+00:00
- **Updated**: 2022-04-08 13:15:55+00:00
- **Authors**: Han-Jia Ye, Yi Shi, De-Chuan Zhan
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Rich semantics inside an image result in its ambiguous relationship with others, i.e., two images could be similar in one condition but dissimilar in another. Given triplets like "aircraft" is similar to "bird" than "train", Weakly Supervised Conditional Similarity Learning (WS-CSL) learns multiple embeddings to match semantic conditions without explicit condition labels such as "can fly". However, similarity relationships in a triplet are uncertain except providing a condition. For example, the previous comparison becomes invalid once the conditional label changes to "is vehicle". To this end, we introduce a novel evaluation criterion by predicting the comparison's correctness after assigning the learned embeddings to their optimal conditions, which measures how much WS-CSL could cover latent semantics as the supervised model. Furthermore, we propose the Distance Induced Semantic COndition VERification Network (DiscoverNet), which characterizes the instance-instance and triplets-condition relations in a "decompose-and-fuse" manner. To make the learned embeddings cover all semantics, DiscoverNet utilizes a set module or an additional regularizer over the correspondence between a triplet and a condition. DiscoverNet achieves state-of-the-art performance on benchmarks like UT-Zappos-50k and Celeb-A w.r.t. different criteria.



### Deep Learning-Based Intra Mode Derivation for Versatile Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2204.04059v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.04059v1)
- **Published**: 2022-04-08 13:23:59+00:00
- **Updated**: 2022-04-08 13:23:59+00:00
- **Authors**: Linwei Zhu, Yun Zhang, Na Li, Gangyi Jiang, Sam Kwong
- **Comment**: 19 pages, 7 figures, submitted to ACM TOMM
- **Journal**: None
- **Summary**: In intra coding, Rate Distortion Optimization (RDO) is performed to achieve the optimal intra mode from a pre-defined candidate list. The optimal intra mode is also required to be encoded and transmitted to the decoder side besides the residual signal, where lots of coding bits are consumed. To further improve the performance of intra coding in Versatile Video Coding (VVC), an intelligent intra mode derivation method is proposed in this paper, termed as Deep Learning based Intra Mode Derivation (DLIMD). In specific, the process of intra mode derivation is formulated as a multi-class classification task, which aims to skip the module of intra mode signaling for coding bits reduction. The architecture of DLIMD is developed to adapt to different quantization parameter settings and variable coding blocks including non-square ones, which are handled by one single trained model. Different from the existing deep learning based classification problems, the hand-crafted features are also fed into the intra mode derivation network besides the learned features from feature learning network. To compete with traditional method, one additional binary flag is utilized in the video codec to indicate the selected scheme with RDO. Extensive experimental results reveal that the proposed method can achieve 2.28%, 1.74%, and 2.18% bit rate reduction on average for Y, U, and V components on the platform of VVC test model, which outperforms the state-of-the-art works.



### Invariant Descriptors for Intrinsic Reflectance Optimization
- **Arxiv ID**: http://arxiv.org/abs/2204.04076v1
- **DOI**: 10.1364/JOSAA.414682
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04076v1)
- **Published**: 2022-04-08 13:52:13+00:00
- **Updated**: 2022-04-08 13:52:13+00:00
- **Authors**: Anil S. Baslamisli, Theo Gevers
- **Comment**: None
- **Journal**: Journal of the Optical Society of America A, Vol. 38, Issue 6, pp.
  887-896 (2021)
- **Summary**: Intrinsic image decomposition aims to factorize an image into albedo (reflectance) and shading (illumination) sub-components. Being ill-posed and under-constrained, it is a very challenging computer vision problem. There are infinite pairs of reflectance and shading images that can reconstruct the same input. To address the problem, Intrinsic Images in the Wild provides an optimization framework based on a dense conditional random field (CRF) formulation that considers long-range material relations. We improve upon their model by introducing illumination invariant image descriptors: color ratios. The color ratios and the reflectance intrinsic are both invariant to illumination and thus are highly correlated. Through detailed experiments, we provide ways to inject the color ratios into the dense CRF optimization. Our approach is physics-based, learning-free and leads to more accurate and robust reflectance decompositions.



### General Incremental Learning with Domain-aware Categorical Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.04078v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04078v2)
- **Published**: 2022-04-08 13:57:33+00:00
- **Updated**: 2022-10-13 14:12:53+00:00
- **Authors**: Jiangwei Xie, Shipeng Yan, Xuming He
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Continual learning is an important problem for achieving human-level intelligence in real-world applications as an agent must continuously accumulate knowledge in response to streaming data/tasks. In this work, we consider a general and yet under-explored incremental learning problem in which both the class distribution and class-specific domain distribution change over time. In addition to the typical challenges in class incremental learning, this setting also faces the intra-class stability-plasticity dilemma and intra-class domain imbalance problems. To address above issues, we develop a novel domain-aware continual learning method based on the EM framework. Specifically, we introduce a flexible class representation based on the von Mises-Fisher mixture model to capture the intra-class structure, using an expansion-and-reduction strategy to dynamically increase the number of components according to the class complexity. Moreover, we design a bi-level balanced memory to cope with data imbalances within and across classes, which combines with a distillation loss to achieve better inter- and intra-class stability-plasticity trade-off. We conduct exhaustive experiments on three benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our approach consistently outperforms previous methods by a significant margin, demonstrating its superiority.



### POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04083v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04083v2)
- **Published**: 2022-04-08 14:01:41+00:00
- **Updated**: 2023-08-13 20:49:39+00:00
- **Authors**: Ce Zheng, Matias Mendieta, Chen Chen
- **Comment**: ICCV Workshop (AMFG) 2023
- **Journal**: None
- **Summary**: Facial expression recognition (FER) is an important task in computer vision, having practical applications in areas such as human-computer interaction, education, healthcare, and online monitoring. In this challenging FER task, there are three key issues especially prevalent: inter-class similarity, intra-class discrepancy, and scale sensitivity. While existing works typically address some of these issues, none have fully addressed all three challenges in a unified framework. In this paper, we propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER), that aims to holistically solve all three issues. Specifically, we design a transformer-based cross-fusion method that enables effective collaboration of facial landmark features and image features to maximize proper attention to salient facial regions. Furthermore, POSTER employs a pyramid structure to promote scale invariance. Extensive experimental results demonstrate that our POSTER achieves new state-of-the-art results on RAF-DB (92.05%), FERPlus (91.62%), as well as AffectNet 7 class (67.31%) and 8 class (63.34%). The code is available at https://github.com/zczcwh/POSTER.



### Dynamic super-resolution in particle tracking problems
- **Arxiv ID**: http://arxiv.org/abs/2204.04092v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04092v1)
- **Published**: 2022-04-08 14:21:27+00:00
- **Updated**: 2022-04-08 14:21:27+00:00
- **Authors**: Ping Liu, Habib Ammari
- **Comment**: None
- **Journal**: None
- **Summary**: Particle tracking in biological imaging is concerned with reconstructing the trajectories, locations, or velocities of the targeting particles. The standard approach of particle tracking consists of two steps: first reconstructing statically the source locations in each time step, and second applying tracking techniques to obtain the trajectories and velocities. In contrast, the dynamic reconstruction seeks to simultaneously recover the source locations and velocities from all frames, which enjoys certain advantages. In this paper, we provide a rigorous mathematical analysis for the resolution limit of reconstructing source number, locations, and velocities by general dynamical reconstruction in particle tracking problems, by which we demonstrate the possibility of achieving super-resolution for the dynamic reconstruction. We show that when the location-velocity pairs of the particles are separated beyond certain distances (the resolution limits), the number of particles and the location-velocity pair can be stably recovered. The resolution limits are related to the cut-off frequency of the imaging system, signal-to-noise ratio, and the sparsity of the source. By these estimates, we also derive a stability result for a sparsity-promoting dynamic reconstruction. In addition, we further show that the reconstruction of velocities has a better resolution limit which improves constantly as the particles moving. This result is derived by an observation that the inherent cut-off frequency for the velocity recovery can be viewed as the total observation time multiplies the cut-off frequency of the imaging system, which may lead to a better resolution limit as compared to the one for each diffraction-limited frame. It is anticipated that this observation can inspire new reconstruction algorithms that improve the resolution of particle tracking in practice.



### Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline
- **Arxiv ID**: http://arxiv.org/abs/2204.04120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04120v1)
- **Published**: 2022-04-08 15:22:33+00:00
- **Updated**: 2022-04-08 15:22:33+00:00
- **Authors**: Pengyu Zhang, Jie Zhao, Dong Wang, Huchuan Lu, Xiang Ruan
- **Comment**: to be published in CVPR22. The project is available at
  https://zhang-pengyu.github.io/DUT-VTUAV/
- **Journal**: None
- **Summary**: With the popularity of multi-modal sensors, visible-thermal (RGB-T) object tracking is to achieve robust performance and wider application scenarios with the guidance of objects' temperature information. However, the lack of paired training samples is the main bottleneck for unlocking the power of RGB-T tracking. Since it is laborious to collect high-quality RGB-T sequences, recent benchmarks only provide test sequences. In this paper, we construct a large-scale benchmark with high diversity for visible-thermal UAV tracking (VTUAV), including 500 sequences with 1.7 million high-resolution (1920 $\times$ 1080 pixels) frame pairs. In addition, comprehensive applications (short-term tracking, long-term tracking and segmentation mask prediction) with diverse categories and scenes are considered for exhaustive evaluation. Moreover, we provide a coarse-to-fine attribute annotation, where frame-level attributes are provided to exploit the potential of challenge-specific trackers. In addition, we design a new RGB-T baseline, named Hierarchical Multi-modal Fusion Tracker (HMFT), which fuses RGB-T data in various levels. Numerous experiments on several datasets are conducted to reveal the effectiveness of HMFT and the complement of different fusion types. The project is available at here.



### Sat2lod2: A Software For Automated Lod-2 Modeling From Satellite-Derived Orthophoto And Digital Surface Model
- **Arxiv ID**: http://arxiv.org/abs/2204.04139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04139v1)
- **Published**: 2022-04-08 15:49:35+00:00
- **Updated**: 2022-04-08 15:49:35+00:00
- **Authors**: Shengxi Gui, Rongjun Qin, Yang Tang
- **Comment**: to be published in ISPRS congress 2022
- **Journal**: None
- **Summary**: Deriving LoD2 models from orthophoto and digital surface models (DSM) reconstructed from satellite images is a challenging task. Existing solutions are mostly system approaches that require complicated step-wise processes, including not only heuristic geometric operations, but also high-level steps such as machine learning-based semantic segmentation and building detection. Here in this paper, we describe an open-source tool, called SAT2LOD2, built based on a minorly modified version of our recently published work. SAT2LoD2 is a fully open-source and GUI (Graphics User Interface) based software, coded in Python, which takes an orthophoto and DSM as inputs, and outputs individual building models, and it can additionally take road network shapefiles, and customized classification maps to further improve the reconstruction results. We further improve the robustness of the method by 1) intergrading building segmentation based on HRNetV2 into our software; and 2) having implemented a decision strategy to identify complex buildings and directly generate mesh to avoid erroneous LoD2 reconstruction from a system point of view. The software can process a moderate level of data (around 5000*5000 size of orthophoto and DSM) using a PC with a graphics card supporting CUDA. Furthermore, the GUI is self-contained and stores the intermediate processing results facilitating researchers to learn the process easily and reuse intermediate files as needed. The updated codes and software are available under this GitHub page: https://github.com/GDAOSU/LOD2BuildingModel.



### Investigating Spherical Epipolar Rectification for Multi-View Stereo 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2204.04141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04141v1)
- **Published**: 2022-04-08 15:50:20+00:00
- **Updated**: 2022-04-08 15:50:20+00:00
- **Authors**: Mostafa Elhashash, Rongjun Qin
- **Comment**: to be published in ISPRS Congress 2022
- **Journal**: None
- **Summary**: Multi-view stereo (MVS) reconstruction is essential for creating 3D models. The approach involves applying epipolar rectification followed by dense matching for disparity estimation. However, existing approaches face challenges in applying dense matching for images with different viewpoints primarily due to large differences in object scale. In this paper, we propose a spherical model for epipolar rectification to minimize distortions caused by differences in principal rays. We evaluate the proposed approach using two aerial-based datasets consisting of multi-camera head systems. We show through qualitative and quantitative evaluation that the proposed approach performs better than frame-based epipolar correction by enhancing the completeness of point clouds by up to 4.05% while improving the accuracy by up to 10.23% using LiDAR data as ground truth.



### A Novel Intrinsic Image Decomposition Method to Recover Albedo for Aerial Images in Photogrammetry Processing
- **Arxiv ID**: http://arxiv.org/abs/2204.04142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04142v1)
- **Published**: 2022-04-08 15:50:52+00:00
- **Updated**: 2022-04-08 15:50:52+00:00
- **Authors**: Shuang Song, Rongjun Qin
- **Comment**: to be published in ISPRS Congress 2022
- **Journal**: None
- **Summary**: Recovering surface albedos from photogrammetric images for realistic rendering and synthetic environments can greatly facilitate its downstream applications in VR/AR/MR and digital twins. The textured 3D models from standard photogrammetric pipelines are suboptimal to these applications because these textures are directly derived from images, which intrinsically embedded the spatially and temporally variant environmental lighting information, such as the sun illumination, direction, causing different looks of the surface, making such models less realistic when used in 3D rendering under synthetic lightings. On the other hand, since albedo images are less variable by environmental lighting, it can, in turn, benefit basic photogrammetric processing. In this paper, we attack the problem of albedo recovery for aerial images for the photogrammetric process and demonstrate the benefit of albedo recovery for photogrammetry data processing through enhanced feature matching and dense matching. To this end, we proposed an image formation model with respect to outdoor aerial imagery under natural illumination conditions; we then, derived the inverse model to estimate the albedo by utilizing the typical photogrammetric products as an initial approximation of the geometry. The estimated albedo images are tested in intrinsic image decomposition, relighting, feature matching, and dense matching/point cloud generation results. Both synthetic and real-world experiments have demonstrated that our method outperforms existing methods and can enhance photogrammetric processing.



### Optical tracking in team sports
- **Arxiv ID**: http://arxiv.org/abs/2204.04143v1
- **DOI**: 10.1515/jqas-2020-0088
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04143v1)
- **Published**: 2022-04-08 15:51:35+00:00
- **Updated**: 2022-04-08 15:51:35+00:00
- **Authors**: Pegah Rahimian, Laszlo Toka
- **Comment**: None
- **Journal**: Journal of Quantitative Analysis in Sports 2022
- **Summary**: Sports analysis has gained paramount importance for coaches, scouts, and fans. Recently, computer vision researchers have taken on the challenge of collecting the necessary data by proposing several methods of automatic player and ball tracking. Building on the gathered tracking data, data miners are able to perform quantitative analysis on the performance of players and teams. With this survey, our goal is to provide a basic understanding for quantitative data analysts about the process of creating the input data and the characteristics thereof. Thus, we summarize the recent methods of optical tracking by providing a comprehensive taxonomy of conventional and deep learning methods, separately. Moreover, we discuss the preprocessing steps of tracking, the most common challenges in this domain, and the application of tracking data to sports teams. Finally, we compare the methods by their cost and limitations, and conclude the work by highlighting potential future research directions.



### Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems
- **Arxiv ID**: http://arxiv.org/abs/2204.04145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04145v1)
- **Published**: 2022-04-08 15:53:13+00:00
- **Updated**: 2022-04-08 15:53:13+00:00
- **Authors**: Debao Huang, Mostafa Elhashash, Rongjun Qin
- **Comment**: to be published in ISPRS Congress 2022
- **Journal**: None
- **Summary**: Structure from motion using uncalibrated multi-camera systems is a challenging task. This paper proposes a bundle adjustment solution that implements a baseline constraint respecting that these cameras are static to each other. We assume these cameras are mounted on a mobile platform, uncalibrated, and coarsely synchronized. To this end, we propose the baseline constraint that is formulated for the scenario in which the cameras have overlapping views. The constraint is incorporated in the bundle adjustment solution to keep the relative motion of different cameras static. Experiments were conducted using video frames of two collocated GoPro cameras mounted on a vehicle with no system calibration. These two cameras were placed capturing overlapping contents. We performed our bundle adjustment using the proposed constraint and then produced 3D dense point clouds. Evaluations were performed by comparing these dense point clouds against LiDAR reference data. We showed that, as compared to traditional bundle adjustment, our proposed method achieved an improvement of 29.38%.



### A Video Anomaly Detection Framework based on Appearance-Motion Semantics Representation Consistency
- **Arxiv ID**: http://arxiv.org/abs/2204.04151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04151v1)
- **Published**: 2022-04-08 15:59:57+00:00
- **Updated**: 2022-04-08 15:59:57+00:00
- **Authors**: Xiangyu Huang, Caidan Zhao, Yilin Wang, Zhiqiang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection refers to the identification of events that deviate from the expected behavior. Due to the lack of anomalous samples in training, video anomaly detection becomes a very challenging task. Existing methods almost follow a reconstruction or future frame prediction mode. However, these methods ignore the consistency between appearance and motion information of samples, which limits their anomaly detection performance. Anomalies only occur in the moving foreground of surveillance videos, so the semantics expressed by video frame sequences and optical flow without background information in anomaly detection should be highly consistent and significant for anomaly detection. Based on this idea, we propose Appearance-Motion Semantics Representation Consistency (AMSRC), a framework that uses normal data's appearance and motion semantic representation consistency to handle anomaly detection. Firstly, we design a two-stream encoder to encode the appearance and motion information representations of normal samples and introduce constraints to further enhance the consistency of the feature semantics between appearance and motion information of normal samples so that abnormal samples with low consistency appearance and motion feature representation can be identified. Moreover, the lower consistency of appearance and motion features of anomalous samples can be used to generate predicted frames with larger reconstruction error, which makes anomalies easier to spot. Experimental results demonstrate the effectiveness of the proposed method.



### Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2204.04153v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04153v2)
- **Published**: 2022-04-08 16:05:48+00:00
- **Updated**: 2022-07-25 17:50:31+00:00
- **Authors**: Adam W. Harley, Zhaoyuan Fang, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking pixels in videos is typically studied as an optical flow estimation problem, where every pixel is described with a displacement vector that locates it in the next frame. Even though wider temporal context is freely available, prior efforts to take this into account have yielded only small gains over 2-frame methods. In this paper, we revisit Sand and Teller's "particle video" approach, and study pixel tracking as a long-range motion estimation problem, where every pixel is described with a trajectory that locates it in multiple future frames. We re-build this classic approach using components that drive the current state-of-the-art in flow and object tracking, such as dense cost maps, iterative optimization, and learned appearance updates. We train our models using long-range amodal point trajectories mined from existing optical flow data that we synthetically augment with multi-frame occlusions. We test our approach in trajectory estimation benchmarks and in keypoint label propagation tasks, and compare favorably against state-of-the-art optical flow and feature tracking methods.



### Underwater Image Enhancement Using Pre-trained Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.04199v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04199v1)
- **Published**: 2022-04-08 17:50:32+00:00
- **Updated**: 2022-04-08 17:50:32+00:00
- **Authors**: Abderrahmene Boudiaf, Yuhang Guo, Adarsh Ghimire, Naoufel Werghi, Giulia De Masi, Sajid Javed, Jorge Dias
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to apply a denoising image transformer to remove the distortion from underwater images and compare it with other similar approaches. Automatic restoration of underwater images plays an important role since it allows to increase the quality of the images, without the need for more expensive equipment. This is a critical example of the important role of the machine learning algorithms to support marine exploration and monitoring, reducing the need for human intervention like the manual processing of the images, thus saving time, effort, and cost. This paper is the first application of the image transformer-based approach called "Pre-Trained Image Processing Transformer" to underwater images. This approach is tested on the UFO-120 dataset, containing 1500 images with the corresponding clean images.



### Dancing under the stars: video denoising in starlight
- **Arxiv ID**: http://arxiv.org/abs/2204.04210v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04210v1)
- **Published**: 2022-04-08 17:59:31+00:00
- **Updated**: 2022-04-08 17:59:31+00:00
- **Authors**: Kristina Monakhova, Stephan R. Richter, Laura Waller, Vladlen Koltun
- **Comment**: CVPR 2022. Project page:
  https://kristinamonakhova.com/starlight_denoising/
- **Journal**: None
- **Summary**: Imaging in low light is extremely challenging due to low photon counts. Using sensitive CMOS cameras, it is currently possible to take videos at night under moonlight (0.05-0.3 lux illumination). In this paper, we demonstrate photorealistic video under starlight (no moon present, $<$0.001 lux) for the first time. To enable this, we develop a GAN-tuned physics-based noise model to more accurately represent camera noise at the lowest light levels. Using this noise model, we train a video denoiser using a combination of simulated noisy video clips and real noisy still images. We capture a 5-10 fps video dataset with significant motion at approximately 0.6-0.7 millilux with no active illumination. Comparing against alternative methods, we achieve improved video quality at the lowest light levels, demonstrating photorealistic video denoising in starlight for the first time.



### Vision-Based American Sign Language Classification Approach via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04235v1
- **DOI**: 10.32473/flairs.v35i.130616
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04235v1)
- **Published**: 2022-04-08 18:01:27+00:00
- **Updated**: 2022-04-08 18:01:27+00:00
- **Authors**: Nelly Elsayed, Zag ElSayed, Anthony S. Maida
- **Comment**: 4 pages, Accepted in the The Florida AI Research Society (FLAIRS-35)
  2022
- **Journal**: None
- **Summary**: Hearing-impaired is the disability of partial or total hearing loss that causes a significant problem for communication with other people in society. American Sign Language (ASL) is one of the sign languages that most commonly used language used by Hearing impaired communities to communicate with each other. In this paper, we proposed a simple deep learning model that aims to classify the American Sign Language letters as a step in a path for removing communication barriers that are related to disabilities.



### ChildCI Framework: Analysis of Motor and Cognitive Development in Children-Computer Interaction for Age Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.04236v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04236v2)
- **Published**: 2022-04-08 18:03:39+00:00
- **Updated**: 2022-07-18 12:17:00+00:00
- **Authors**: Juan Carlos Ruiz-Garcia, Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Jaime Herreros-Rodriguez
- **Comment**: 11 pages, 3 figures, 6 tables
- **Journal**: None
- **Summary**: This article presents a comprehensive analysis of the different tests proposed in the recent ChildCI framework, proving its potential for generating a better understanding of children's neuromotor and cognitive development along time, as well as their possible application in other research areas such as e-Health and e-Learning. In particular, we propose a set of over 100 global features related to motor and cognitive aspects of the children interaction with mobile devices, some of them collected and adapted from the literature. Furthermore, we analyse the robustness and discriminative power of the proposed feature set including experimental results for the task of children age group detection based on their motor and cognitive behaviors. Two different scenarios are considered in this study: i) single-test scenario, and ii) multiple-test scenario. Results over 93% accuracy are achieved using the publicly available ChildCIdb_v1 database (over 400 children from 18 months to 8 years old), proving the high correlation of children's age with the way they interact with mobile devices.



### Elastic shape analysis of surfaces with second-order Sobolev metrics: a comprehensive numerical framework
- **Arxiv ID**: http://arxiv.org/abs/2204.04238v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U05, 49Q10, 58D10
- **Links**: [PDF](http://arxiv.org/pdf/2204.04238v3)
- **Published**: 2022-04-08 18:19:05+00:00
- **Updated**: 2022-12-05 21:23:10+00:00
- **Authors**: Emmanuel Hartman, Yashil Sukurdeep, Eric Klassen, Nicolas Charon, Martin Bauer
- **Comment**: 28 pages, 16 figures, 2 tables
- **Journal**: None
- **Summary**: This paper introduces a set of numerical methods for Riemannian shape analysis of 3D surfaces within the setting of invariant (elastic) second-order Sobolev metrics. More specifically, we address the computation of geodesics and geodesic distances between parametrized or unparametrized immersed surfaces represented as 3D meshes. Building on this, we develop tools for the statistical shape analysis of sets of surfaces, including methods for estimating Karcher means and performing tangent PCA on shape populations, and for computing parallel transport along paths of surfaces. Our proposed approach fundamentally relies on a relaxed variational formulation for the geodesic matching problem via the use of varifold fidelity terms, which enable us to enforce reparametrization independence when computing geodesics between unparametrized surfaces, while also yielding versatile algorithms that allow us to compare surfaces with varying sampling or mesh structures. Importantly, we demonstrate how our relaxed variational framework can be extended to tackle partially observed data. The different benefits of our numerical pipeline are illustrated over various examples, synthetic and real.



### Understanding the Influence of Receptive Field and Network Complexity in Neural-Network-Guided TEM Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2204.04250v1
- **DOI**: 10.1017/S1431927622012466
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04250v1)
- **Published**: 2022-04-08 18:45:15+00:00
- **Updated**: 2022-04-08 18:45:15+00:00
- **Authors**: Katherine Sytwu, Catherine Groschner, Mary C. Scott
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Trained neural networks are promising tools to analyze the ever-increasing amount of scientific image data, but it is unclear how to best customize these networks for the unique features in transmission electron micrographs. Here, we systematically examine how neural network architecture choices affect how neural networks segment, or pixel-wise separate, crystalline nanoparticles from amorphous background in transmission electron microscopy (TEM) images. We focus on decoupling the influence of receptive field, or the area of the input image that contributes to the output decision, from network complexity, which dictates the number of trainable parameters. We find that for low-resolution TEM images which rely on amplitude contrast to distinguish nanoparticles from background, the receptive field does not significantly influence segmentation performance. On the other hand, for high-resolution TEM images which rely on a combination of amplitude and phase contrast changes to identify nanoparticles, receptive field is a key parameter for increased performance, especially in images with minimal amplitude contrast. Our results provide insight and guidance as to how to adapt neural networks for applications with TEM datasets.



### On Improving Cross-dataset Generalization of Deepfake Detectors
- **Arxiv ID**: http://arxiv.org/abs/2204.04285v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04285v1)
- **Published**: 2022-04-08 20:34:53+00:00
- **Updated**: 2022-04-08 20:34:53+00:00
- **Authors**: Aakash Varma Nadimpalli, Ajita Rattani
- **Comment**: 2022 Conference on Computer Vision and Pattern Recognition Workshops
  | New Orleans, Louisiana
- **Journal**: None
- **Summary**: Facial manipulation by deep fake has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deep fake detection methods have been proposed recently. Most of them model deep fake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deep fake detection with the Area under the Curve (AUC) as high as 0.99. However, the performance of these methods degrades significantly when evaluated across datasets. In this paper, we formulate deep fake detection as a hybrid combination of supervised and reinforcement learning (RL) to improve its cross-dataset generalization performance. The proposed method chooses the top-k augmentations for each test sample by an RL agent in an image-specific manner. The classification scores, obtained using CNN, of all the augmentations of each test image are averaged together for final real or fake classification. Through extensive experimental validation, we demonstrate the superiority of our method over existing published research in cross-dataset generalization of deep fake detectors, thus obtaining state-of-the-art performance.



### Learning to modulate random weights can induce task-specific contexts for economical meta and continual learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04297v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04297v1)
- **Published**: 2022-04-08 21:12:13+00:00
- **Updated**: 2022-04-08 21:12:13+00:00
- **Authors**: Jinyung Hong, Theodore P. Pavlic
- **Comment**: 17 pages, 14 figures, 1 table
- **Journal**: None
- **Summary**: Neural networks are vulnerable to catastrophic forgetting when data distributions are non-stationary during continual online learning; learning of a later task often leads to forgetting of an earlier task. One solution approach is model-agnostic continual meta-learning, whereby both task-specific and meta parameters are trained. Here, we depart from this view and introduce a novel neural-network architecture inspired by neuromodulation in biological nervous systems. Neuromodulation is the biological mechanism that dynamically controls and fine-tunes synaptic dynamics to complement the behavioral context in real-time, which has received limited attention in machine learning. We introduce a single-hidden-layer network that learns only a relatively small context vector per task (task-specific parameters) that neuromodulates unchanging, randomized weights (meta parameters) that transform the input. We show that when task boundaries are available, this approach can eliminate catastrophic forgetting entirely while also drastically reducing the number of learnable parameters relative to other context-vector-based approaches. Furthermore, by combining this model with a simple meta-learning approach for inferring task identity, we demonstrate that the model can be generalized into a framework to perform continual learning without knowledge of task boundaries. Finally, we showcase the framework in a supervised continual online learning scenario and discuss the implications of the proposed formalism.




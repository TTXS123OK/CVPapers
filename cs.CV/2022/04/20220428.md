# Arxiv Papers in cs.CV on 2022-04-28
### Adversarial Fine-tune with Dynamically Regulated Adversary
- **Arxiv ID**: http://arxiv.org/abs/2204.13232v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13232v1)
- **Published**: 2022-04-28 00:07:15+00:00
- **Updated**: 2022-04-28 00:07:15+00:00
- **Authors**: Pengyue Hou, Ming Zhou, Jie Han, Petr Musilek, Xingyu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training is an effective method to boost model robustness to malicious, adversarial attacks. However, such improvement in model robustness often leads to a significant sacrifice of standard performance on clean images. In many real-world applications such as health diagnosis and autonomous surgical robotics, the standard performance is more valued over model robustness against such extremely malicious attacks. This leads to the question: To what extent we can boost model robustness without sacrificing standard performance? This work tackles this problem and proposes a simple yet effective transfer learning-based adversarial training strategy that disentangles the negative effects of adversarial samples on model's standard performance. In addition, we introduce a training-friendly adversarial attack algorithm, which facilitates the boost of adversarial robustness without introducing significant training complexity. Extensive experimentation indicates that the proposed method outperforms previous adversarial training algorithms towards the target: to improve model robustness while preserving model's standard performance on clean data.



### Automatic Detection and Classification of Symbols in Engineering Drawings
- **Arxiv ID**: http://arxiv.org/abs/2204.13277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13277v1)
- **Published**: 2022-04-28 04:14:14+00:00
- **Updated**: 2022-04-28 04:14:14+00:00
- **Authors**: Sourish Sarkar, Pranav Pandey, Sibsambhu Kar
- **Comment**: None
- **Journal**: None
- **Summary**: A method of finding and classifying various components and objects in a design diagram, drawing, or planning layout is proposed. The method automatically finds the objects present in a legend table and finds their position, count and related information with the help of multiple deep neural networks. The method is pre-trained on several drawings or design templates to learn the feature set that may help in representing the new templates. For a template not seen before, it does not require any training with template dataset. The proposed method may be useful in multiple industry applications such as design validation, object count, connectivity of components, etc. The method is generic and domain independent.



### Resource-efficient domain adaptive pre-training for medical images
- **Arxiv ID**: http://arxiv.org/abs/2204.13280v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2204.13280v1)
- **Published**: 2022-04-28 04:29:32+00:00
- **Updated**: 2022-04-28 04:29:32+00:00
- **Authors**: Yasar Mehmood, Usama Ijaz Bajwa, Xianfang Sun
- **Comment**: None
- **Journal**: None
- **Summary**: The deep learning-based analysis of medical images suffers from data scarcity because of high annotation costs and privacy concerns. Researchers in this domain have used transfer learning to avoid overfitting when using complex architectures. However, the domain differences between pre-training and downstream data hamper the performance of the downstream task. Some recent studies have successfully used domain-adaptive pre-training (DAPT) to address this issue. In DAPT, models are initialized with the generic dataset pre-trained weights, and further pre-training is performed using a moderately sized in-domain dataset (medical images). Although this technique achieved good results for the downstream tasks in terms of accuracy and robustness, it is computationally expensive even when the datasets for DAPT are moderately sized. These compute-intensive techniques and models impact the environment negatively and create an uneven playing field for researchers with limited resources. This study proposed computationally efficient DAPT without compromising the downstream accuracy and robustness. This study proposes three techniques for this purpose, where the first (partial DAPT) performs DAPT on a subset of layers. The second one adopts a hybrid strategy (hybrid DAPT) by performing partial DAPT for a few epochs and then full DAPT for the remaining epochs. The third technique performs DAPT on simplified variants of the base architecture. The results showed that compared to the standard DAPT (full DAPT), the hybrid DAPT technique achieved better performance on the development and external datasets. In contrast, simplified architectures (after DAPT) achieved the best robustness while achieving modest performance on the development dataset .



### Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.13286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13286v1)
- **Published**: 2022-04-28 04:43:22+00:00
- **Updated**: 2022-04-28 04:43:22+00:00
- **Authors**: Guangwei Gao, Zhengxue Wang, Juncheng Li, Wenjie Li, Yi Yu, Tieyong Zeng
- **Comment**: Accepted by IJCAI2022, short oral presentation
- **Journal**: None
- **Summary**: Single-image super-resolution (SISR) has achieved significant breakthroughs with the development of deep learning. However, these methods are difficult to be applied in real-world scenarios since they are inevitably accompanied by the problems of computational and memory costs caused by the complex operations. To solve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR. Specifically, an effective Symmetric CNN is designed for local feature extraction and coarse image reconstruction. Meanwhile, we propose a Recursive Transformer to fully learn the long-term dependence of images thus the global information can be fully used to further refine texture details. Studies show that the hybrid of CNN and Transformer can build a more efficient model. Extensive experiments have proved that our LBNet achieves more prominent performance than other state-of-the-art methods with a relatively low computational cost and memory consumption. The code is available at https://github.com/IVIPLab/LBNet.



### Region-level Contrastive and Consistency Learning for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.13314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13314v1)
- **Published**: 2022-04-28 07:22:47+00:00
- **Updated**: 2022-04-28 07:22:47+00:00
- **Authors**: Jianrong Zhang, Tianyi Wu, Chuanghao Ding, Hongwei Zhao, Guodong Guo
- **Comment**: Accepted by IJCAI 2022 (Long Oral)
- **Journal**: None
- **Summary**: Current semi-supervised semantic segmentation methods mainly focus on designing pixel-level consistency and contrastive regularization. However, pixel-level regularization is sensitive to noise from pixels with incorrect predictions, and pixel-level contrastive regularization has memory and computational cost with O(pixel_num^2). To address the issues, we propose a novel region-level contrastive and consistency learning framework (RC^2L) for semi-supervised semantic segmentation. Specifically, we first propose a Region Mask Contrastive (RMC) loss and a Region Feature Contrastive (RFC) loss to accomplish region-level contrastive property. Furthermore, Region Class Consistency (RCC) loss and Semantic Mask Consistency (SMC) loss are proposed for achieving region-level consistency. Based on the proposed region-level contrastive and consistency regularization, we develop a region-level contrastive and consistency learning framework (RC^2L) for semi-supervised semantic segmentation, and evaluate our RC$^2$L on two challenging benchmarks (PASCAL VOC 2012 and Cityscapes), outperforming the state-of-the-art.



### Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast
- **Arxiv ID**: http://arxiv.org/abs/2204.14057v3
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.14057v3)
- **Published**: 2022-04-28 07:28:56+00:00
- **Updated**: 2022-05-27 02:42:26+00:00
- **Authors**: Boqing Zhu, Kele Xu, Changjian Wang, Zheng Qin, Tao Sun, Huaimin Wang, Yuxing Peng
- **Comment**: 8 pages, 4 figures. Accepted by IJCAI-2022
- **Journal**: None
- **Summary**: We present an approach to learn voice-face representations from the talking face videos, without any identity labels. Previous works employ cross-modal instance discrimination tasks to establish the correlation of voice and face. These methods neglect the semantic content of different videos, introducing false-negative pairs as training noise. Furthermore, the positive pairs are constructed based on the natural correlation between audio clips and visual frames. However, this correlation might be weak or inaccurate in a large amount of real-world data, which leads to deviating positives into the contrastive paradigm. To address these issues, we propose the cross-modal prototype contrastive learning (CMPC), which takes advantage of contrastive methods and resists adverse effects of false negatives and deviate positives. On one hand, CMPC could learn the intra-class invariance by constructing semantic-wise positives via unsupervised clustering in different modalities. On the other hand, by comparing the similarities of cross-modal instances from that of cross-modal prototypes, we dynamically recalibrate the unlearnable instances' contribution to overall loss. Experiments show that the proposed approach outperforms state-of-the-art unsupervised methods on various voice-face association evaluation protocols. Additionally, in the low-shot supervision setting, our method also has a significant improvement compared to previous instance-wise contrastive learning.



### MMRotate: A Rotated Object Detection Benchmark using PyTorch
- **Arxiv ID**: http://arxiv.org/abs/2204.13317v4
- **DOI**: 10.1145/3503161.3548541
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.13317v4)
- **Published**: 2022-04-28 07:31:00+00:00
- **Updated**: 2022-07-19 08:05:58+00:00
- **Authors**: Yue Zhou, Xue Yang, Gefan Zhang, Jiabao Wang, Yanyi Liu, Liping Hou, Xue Jiang, Xingzhao Liu, Junchi Yan, Chengqi Lyu, Wenwei Zhang, Kai Chen
- **Comment**: 5 pages, 2 tables, MMRotate is accepted by ACM MM 2022 (OS Track).
  Yue Zhou and Xue Yang provided equal contribution. The code is publicly
  released at https://github.com/open-mmlab/mmrotate
- **Journal**: None
- **Summary**: We present an open-source toolbox, named MMRotate, which provides a coherent algorithm framework of training, inferring, and evaluation for the popular rotated object detection algorithm based on deep learning. MMRotate implements 18 state-of-the-art algorithms and supports the three most frequently used angle definition methods. To facilitate future research and industrial applications of rotated object detection-related problems, we also provide a large number of trained models and detailed benchmarks to give insights into the performance of rotated object detection. MMRotate is publicly released at https://github.com/open-mmlab/mmrotate.



### Two Decades of Colorization and Decolorization for Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.13322v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2204.13322v2)
- **Published**: 2022-04-28 07:43:52+00:00
- **Updated**: 2022-07-18 09:22:00+00:00
- **Authors**: Shiguang Liu
- **Comment**: 12 pages, 19 figures
- **Journal**: None
- **Summary**: Colorization is a computer-aided process, which aims to give color to a gray image or video. It can be used to enhance black-and-white images, including black-and-white photos, old-fashioned films, and scientific imaging results. On the contrary, decolorization is to convert a color image or video into a grayscale one. A grayscale image or video refers to an image or video with only brightness information without color information. It is the basis of some downstream image processing applications such as pattern recognition, image segmentation, and image enhancement. Different from image decolorization, video decolorization should not only consider the image contrast preservation in each video frame, but also respect the temporal and spatial consistency between video frames. Researchers were devoted to develop decolorization methods by balancing spatial-temporal consistency and algorithm efficiency. With the prevalance of the digital cameras and mobile phones, image and video colorization and decolorization have been paid more and more attention by researchers. This paper gives an overview of the progress of image and video colorization and decolorization methods in the last two decades.



### Discriminative-Region Attention and Orthogonal-View Generation Model for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2204.13323v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.13323v1)
- **Published**: 2022-04-28 07:46:03+00:00
- **Updated**: 2022-04-28 07:46:03+00:00
- **Authors**: Huadong Li, Yuefeng Wang, Ying Wei, Lin Wang, Li Ge
- **Comment**: 28pages,12 figures
- **Journal**: None
- **Summary**: Vehicle re-identification (Re-ID) is urgently demanded to alleviate thepressure caused by the increasingly onerous task of urban traffic management. Multiple challenges hamper the applications of vision-based vehicle Re-ID methods: (1) The appearances of different vehicles of the same brand/model are often similar; However, (2) the appearances of the same vehicle differ significantly from different viewpoints. Previous methods mainly use manually annotated multi-attribute datasets to assist the network in getting detailed cues and in inferencing multi-view to improve the vehicle Re-ID performance. However, finely labeled vehicle datasets are usually unattainable in real application scenarios. Hence, we propose a Discriminative-Region Attention and Orthogonal-View Generation (DRA-OVG) model, which only requires identity (ID) labels to conquer the multiple challenges of vehicle Re-ID.The proposed DRA model can automatically extract the discriminative region features, which can distinguish similar vehicles. And the OVG model can generate multi-view features based on the input view features to reduce the impact of viewpoint mismatches. Finally, the distance between vehicle appearances is presented by the discriminative region features and multi-view features together. Therefore, the significance of pairwise distance measure between vehicles is enhanced in acomplete feature space. Extensive experiments substantiate the effectiveness of each proposed ingredient, and experimental results indicate that our approach achieves remarkable improvements over the state- of-the-art vehicle Re-ID methods on VehicleID and VeRi-776 datasets.



### Controllable Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2204.13324v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13324v4)
- **Published**: 2022-04-28 07:47:49+00:00
- **Updated**: 2022-05-25 17:56:19+00:00
- **Authors**: Luka Maxwell
- **Comment**: This submission has been withdrawn by arXiv administrators because
  the identity of the submitter and author could not be verified
- **Journal**: None
- **Summary**: State-of-the-art image captioners can generate accurate sentences to describe images in a sequence to sequence manner without considering the controllability and interpretability. This, however, is far from making image captioning widely used as an image can be interpreted in infinite ways depending on the target and the context at hand. Achieving controllability is important especially when the image captioner is used by different people with different way of interpreting the images. In this paper, we introduce a novel framework for image captioning which can generate diverse descriptions by capturing the co-dependence between Part-Of-Speech tags and semantics. Our model decouples direct dependence between successive variables. In this way, it allows the decoder to exhaustively search through the latent Part-Of-Speech choices, while keeping decoding speed proportional to the size of the POS vocabulary. Given a control signal in the form of a sequence of Part-Of-Speech tags, we propose a method to generate captions through a Transformer network, which predicts words based on the input Part-Of-Speech tag sequences. Experiments on publicly available datasets show that our model significantly outperforms state-of-the-art methods on generating diverse image captions with high qualities.



### An Overview of Color Transfer and Style Transfer for Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.13339v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2204.13339v3)
- **Published**: 2022-04-28 08:20:54+00:00
- **Updated**: 2022-07-19 04:08:45+00:00
- **Authors**: Shiguang Liu
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: Image or video appearance features (e.g., color, texture, tone, illumination, and so on) reflect one's visual perception and direct impression of an image or video. Given a source image (video) and a target image (video), the image (video) color transfer technique aims to process the color of the source image or video (note that the source image or video is also referred to the reference image or video in some literature) to make it look like that of the target image or video, i.e., transferring the appearance of the target image or video to that of the source image or video, which can thereby change one's perception of the source image or video. As an extension of color transfer, style transfer refers to rendering the content of a target image or video in the style of an artist with either a style sample or a set of images through a style transfer model. As an emerging field, the study of style transfer has attracted the attention of a large number of researchers. After decades of development, it has become a highly interdisciplinary research with a variety of artistic expression styles can be achieved. This paper provides an overview of color transfer and style transfer methods over the past years.



### The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2204.13340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13340v2)
- **Published**: 2022-04-28 08:21:09+00:00
- **Updated**: 2023-04-01 07:37:37+00:00
- **Authors**: Alexandros Stergiou, Dima Damen
- **Comment**: Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023
- **Journal**: None
- **Summary**: Early action prediction deals with inferring the ongoing action from partially-observed videos, typically at the outset of the video. We propose a bottleneck-based attention model that captures the evolution of the action, through progressive sampling over fine-to-coarse scales. Our proposed Temporal Progressive (TemPr) model is composed of multiple attention towers, one for each scale. The predicted action label is based on the collective agreement considering confidences of these towers. Extensive experiments over four video datasets showcase state-of-the-art performance on the task of Early Action Prediction across a range of encoder architectures. We demonstrate the effectiveness and consistency of TemPr through detailed ablations.



### BAGNet: Bidirectional Aware Guidance Network for Malignant Breast lesions Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.13342v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13342v1)
- **Published**: 2022-04-28 08:28:06+00:00
- **Updated**: 2022-04-28 08:28:06+00:00
- **Authors**: Gongping Chen, Yuming Liu, Yu Dai, Jianxun Zhang, Liang Cui, Xiaotao Yin
- **Comment**: This paper has been accepted by 2022 7th Asia-Pacific Conference on
  Intelligent Robot Systems (ACIRS 2022)
- **Journal**: None
- **Summary**: Breast lesions segmentation is an important step of computer-aided diagnosis system, and it has attracted much attention. However, accurate segmentation of malignant breast lesions is a challenging task due to the effects of heterogeneous structure and similar intensity distributions. In this paper, a novel bidirectional aware guidance network (BAGNet) is proposed to segment the malignant lesion from breast ultrasound images. Specifically, the bidirectional aware guidance network is used to capture the context between global (low-level) and local (high-level) features from the input coarse saliency map. The introduction of the global feature map can reduce the interference of surrounding tissue (background) on the lesion regions. To evaluate the segmentation performance of the network, we compared with several state-of-the-art medical image segmentation methods on the public breast ultrasound dataset using six commonly used evaluation metrics. Extensive experimental results indicate that our method achieves the most competitive segmentation results on malignant breast ultrasound images.



### A Closer Look at Branch Classifiers of Multi-exit Architectures
- **Arxiv ID**: http://arxiv.org/abs/2204.13347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13347v2)
- **Published**: 2022-04-28 08:37:25+00:00
- **Updated**: 2022-07-13 09:07:05+00:00
- **Authors**: Shaohui Lin, Bo Ji, Rongrong Ji, Angela Yao
- **Comment**: under consideration at Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Multi-exit architectures consist of a backbone and branch classifiers that offer shortened inference pathways to reduce the run-time of deep neural networks. In this paper, we analyze different branching patterns that vary in their allocation of computational complexity for the branch classifiers. Constant-complexity branching keeps all branches the same, while complexity-increasing and complexity-decreasing branching place more complex branches later or earlier in the backbone respectively. Through extensive experimentation on multiple backbones and datasets, we find that complexity-decreasing branches are more effective than constant-complexity or complexity-increasing branches, which achieve the best accuracy-cost trade-off. We investigate a cause by using knowledge consistency to probe the effect of adding branches onto a backbone. Our findings show that complexity-decreasing branching yields the least disruption to the feature abstraction hierarchy of the backbone, which explains the effectiveness of the branching patterns.



### Deep Generalized Unfolding Networks for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2204.13348v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13348v1)
- **Published**: 2022-04-28 08:39:39+00:00
- **Updated**: 2022-04-28 08:39:39+00:00
- **Authors**: Chong Mou, Qian Wang, Jian Zhang
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Deep neural networks (DNN) have achieved great success in image restoration. However, most DNN methods are designed as a black box, lacking transparency and interpretability. Although some methods are proposed to combine traditional optimization algorithms with DNN, they usually demand pre-defined degradation processes or handcrafted assumptions, making it difficult to deal with complex and real-world applications. In this paper, we propose a Deep Generalized Unfolding Network (DGUNet) for image restoration. Concretely, without loss of interpretability, we integrate a gradient estimation strategy into the gradient descent step of the Proximal Gradient Descent (PGD) algorithm, driving it to deal with complex and real-world image degradation. In addition, we design inter-stage information pathways across proximal mapping in different PGD iterations to rectify the intrinsic information loss in most deep unfolding networks (DUN) through a multi-scale and spatial-adaptive way. By integrating the flexible gradient descent and informative proximal mapping, we unfold the iterative PGD algorithm into a trainable DNN. Extensive experiments on various image restoration tasks demonstrate the superiority of our method in terms of state-of-the-art performance, interpretability, and generalizability. The source code is available at https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration.



### Continual Learning with Bayesian Model based on a Fixed Pre-trained Feature Extractor
- **Arxiv ID**: http://arxiv.org/abs/2204.13349v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13349v1)
- **Published**: 2022-04-28 08:41:51+00:00
- **Updated**: 2022-04-28 08:41:51+00:00
- **Authors**: Yang Yang, Zhiying Cui, Junjie Xu, Changhong Zhong, Wei-Shi Zheng, Ruixuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has shown its human-level performance in various applications. However, current deep learning models are characterised by catastrophic forgetting of old knowledge when learning new classes. This poses a challenge particularly in intelligent diagnosis systems where initially only training data of a limited number of diseases are available. In this case, updating the intelligent system with data of new diseases would inevitably downgrade its performance on previously learned diseases. Inspired by the process of learning new knowledge in human brains, we propose a Bayesian generative model for continual learning built on a fixed pre-trained feature extractor. In this model, knowledge of each old class can be compactly represented by a collection of statistical distributions, e.g. with Gaussian mixture models, and naturally kept from forgetting in continual learning over time. Unlike existing class-incremental learning methods, the proposed approach is not sensitive to the continual learning process and can be additionally well applied to the data-incremental learning scenario. Experiments on multiple medical and natural image classification tasks showed that the proposed approach outperforms state-of-the-art approaches which even keep some images of old classes during continual learning of new classes.



### Poly-CAM: High resolution class activation map for convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2204.13359v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.13359v2)
- **Published**: 2022-04-28 09:06:19+00:00
- **Updated**: 2022-05-05 20:48:34+00:00
- **Authors**: Alexandre Englebert, Olivier Cornu, Christophe De Vleeschouwer
- **Comment**: The PolyCAM method was accepted at ICPR 2022 under the name "Backward
  recursive Class Activation Map refinement for high resolution saliency map"
- **Journal**: None
- **Summary**: The need for Explainable AI is increasing with the development of deep learning. The saliency maps derived from convolutional neural networks generally fail in localizing with accuracy the image features justifying the network prediction. This is because those maps are either low-resolution as for CAM [Zhou et al., 2016], or smooth as for perturbation-based methods [Zeiler and Fergus, 2014], or do correspond to a large number of widespread peaky spots as for gradient-based approaches [Sundararajan et al., 2017, Smilkov et al., 2017]. In contrast, our work proposes to combine the information from earlier network layers with the one from later layers to produce a high resolution Class Activation Map that is competitive with the previous art in term of insertion-deletion faithfulness metrics, while outperforming it in term of precision of class-specific features localization.



### On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning
- **Arxiv ID**: http://arxiv.org/abs/2204.13371v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13371v1)
- **Published**: 2022-04-28 09:26:10+00:00
- **Updated**: 2022-04-28 09:26:10+00:00
- **Authors**: Francis Seits, Indrajit Kurmi, Rakesh John Amala Arokia Nathan, Rudolf Ortner, Oliver Bimber
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Occlusion caused by vegetation is an essential problem for remote sensing applications in areas, such as search and rescue, wildfire detection, wildlife observation, surveillance, border control, and others. Airborne Optical Sectioning (AOS) is an optical, wavelength-independent synthetic aperture imaging technique that supports computational occlusion removal in real-time. It can be applied with manned or unmanned aircrafts, such as drones. In this article, we demonstrate a relationship between forest density and field of view (FOV) of applied imaging systems. This finding was made with the help of a simulated procedural forest model which offers the consideration of more realistic occlusion properties than our previous statistical model. While AOS has been explored with automatic and autonomous research prototypes in the past, we present a free AOS integration for DJI systems. It enables bluelight organizations and others to use and explore AOS with compatible, manually operated, off-the-shelf drones. The (digitally cropped) default FOV for this implementation was chosen based on our new finding.



### Morphing Attack Potential
- **Arxiv ID**: http://arxiv.org/abs/2204.13374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13374v1)
- **Published**: 2022-04-28 09:37:46+00:00
- **Updated**: 2022-04-28 09:37:46+00:00
- **Authors**: Matteo Ferrara, Annalisa Franco, Davide Maltoni, Christoph Busch
- **Comment**: This paper is a preprint of a paper accepted by IEEE International
  Workshop on Biometrics and Forensics (IWBF 2022). When the final version is
  published, the copy of record will be available at the IEEE Xplore
- **Journal**: None
- **Summary**: In security systems the risk assessment in the sense of common criteria testing is a very relevant topic; this requires quantifying the attack potential in terms of the expertise of the attacker, his knowledge about the target and access to equipment. Contrary to those attacks, the recently revealed morphing attacks against Face Recognition Systems (FRSs) can not be assessed by any of the above criteria. But not all morphing techniques pose the same risk for an operational face recognition system. This paper introduces with the Morphing Attack Potential (MAP) a consistent methodology, that can quantify the risk, which a certain morphing attack creates.



### Coupling Deep Imputation with Multitask Learning for Downstream Tasks on Genomics Data
- **Arxiv ID**: http://arxiv.org/abs/2204.13705v2
- **DOI**: None
- **Categories**: **q-bio.GN**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13705v2)
- **Published**: 2022-04-28 09:48:15+00:00
- **Updated**: 2022-05-25 09:28:19+00:00
- **Authors**: Sophie Peacock, Etai Jacob, Nikolay Burlutskiy
- **Comment**: Accepted as Oral presentation at The International Joint Conference
  on Neural Networks (IJCNN) 2022
- **Journal**: None
- **Summary**: Genomics data such as RNA gene expression, methylation and micro RNA expression are valuable sources of information for various clinical predictive tasks. For example, predicting survival outcomes, cancer histology type and other patients' related information is possible using not only clinical data but molecular data as well. Moreover, using these data sources together, for example in multitask learning, can boost the performance. However, in practice, there are many missing data points which leads to significantly lower patient numbers when analysing full cases, which in our setting refers to all modalities being present.   In this paper we investigate how imputing data with missing values using deep learning coupled with multitask learning can help to reach state-of-the-art performance results using combined genomics modalities, RNA, micro RNA and methylation. We propose a generalised deep imputation method to impute values where a patient has all modalities present except one. Interestingly enough, deep imputation alone outperforms multitask learning alone for the classification and regression tasks across most combinations of modalities. In contrast, when using all modalities for survival prediction we observe that multitask learning alone outperforms deep imputation alone with statistical significance (adjusted p-value 0.03). Thus, both approaches are complementary when optimising performance for downstream predictive tasks.



### Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.13382v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.13382v3)
- **Published**: 2022-04-28 09:55:28+00:00
- **Updated**: 2023-06-07 09:46:10+00:00
- **Authors**: Maurits Bleeker, Andrew Yates, Maarten de Rijke
- **Comment**: Published in Transactions on Machine Learning Research OpenReview:
  https://openreview.net/forum?id=T1XtOqrVKn Code:
  https://github.com/MauritsBleeker/reducing-predictive-feature-suppression.
  Video: https://www.youtube.com/watch?v=oxa5AbGrKCY
- **Journal**: None
- **Summary**: To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encoder from suppressing predictive features. We implement the LTD objective as an optimization constraint, to ensure that the reconstruction loss is below a bound value while primarily optimizing for the contrastive loss. Importantly, LTD does not depend on additional training data or expensive (hard) negative mining strategies. Our experiments show that, unlike reconstructing the input caption in the input space, LTD reduces predictive feature suppression, measured by obtaining higher recall@k, r-precision, and nDCG scores than a contrastive ICR baseline. Moreover, we show that LTD should be implemented as an optimization constraint instead of a dual optimization objective. Finally, we show that LTD can be used with different contrastive learning losses and a wide variety of resource-constrained ICR methods.



### Self-supervised Contrastive Learning for Audio-Visual Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.13386v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13386v2)
- **Published**: 2022-04-28 10:01:36+00:00
- **Updated**: 2023-03-20 07:33:09+00:00
- **Authors**: Yang Liu, Ying Tan, Haoyuan Lan
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: The underlying correlation between audio and visual modalities can be utilized to learn supervised information for unlabeled videos. In this paper, we propose an end-to-end self-supervised framework named Audio-Visual Contrastive Learning (AVCL), to learn discriminative audio-visual representations for action recognition. Specifically, we design an attention based multi-modal fusion module (AMFM) to fuse audio and visual modalities. To align heterogeneous audio-visual modalities, we construct a novel co-correlation guided representation alignment module (CGRA). To learn supervised information from unlabeled videos, we propose a novel self-supervised contrastive learning module (SelfCL). Furthermore, we build a new audio-visual action recognition dataset named Kinetics-Sounds100. Experimental results on Kinetics-Sounds32 and Kinetics-Sounds100 datasets demonstrate the superiority of our AVCL over the state-of-the-art methods on large-scale action recognition benchmark.



### List-Mode PET Image Reconstruction Using Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2204.13404v2
- **DOI**: 10.1109/TMI.2023.3239596
- **Categories**: **physics.med-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13404v2)
- **Published**: 2022-04-28 10:44:33+00:00
- **Updated**: 2022-11-04 19:17:55+00:00
- **Authors**: Kibo Ote, Fumio Hashimoto, Yuya Onishi, Takashi Isobe, Yasuomi Ouchi
- **Comment**: 13 pages, 10 figures
- **Journal**: IEEE Trans. Med. Imaging (2023)
- **Summary**: List-mode positron emission tomography (PET) image reconstruction is an important tool for PET scanners with many lines-of-response and additional information such as time-of-flight and depth-of-interaction. Deep learning is one possible solution to enhance the quality of PET image reconstruction. However, the application of deep learning techniques to list-mode PET image reconstruction has not been progressed because list data is a sequence of bit codes and unsuitable for processing by convolutional neural networks (CNN). In this study, we propose a novel list-mode PET image reconstruction method using an unsupervised CNN called deep image prior (DIP) which is the first trial to integrate list-mode PET image reconstruction and CNN. The proposed list-mode DIP reconstruction (LM-DIPRecon) method alternatively iterates the regularized list-mode dynamic row action maximum likelihood algorithm (LM-DRAMA) and magnetic resonance imaging conditioned DIP (MR-DIP) using an alternating direction method of multipliers. We evaluated LM-DIPRecon using both simulation and clinical data, and it achieved sharper images and better tradeoff curves between contrast and noise than the LM-DRAMA, MR-DIP and sinogram-based DIPRecon methods. These results indicated that the LM-DIPRecon is useful for quantitative PET imaging with limited events while keeping accurate raw data information. In addition, as list data has finer temporal information than dynamic sinograms, list-mode deep image prior reconstruction is expected to be useful for 4D PET imaging and motion correction.



### Semi-MoreGAN: A New Semi-supervised Generative Adversarial Network for Mixture of Rain Removal
- **Arxiv ID**: http://arxiv.org/abs/2204.13420v2
- **DOI**: 10.1111/cgf.14690
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13420v2)
- **Published**: 2022-04-28 11:35:26+00:00
- **Updated**: 2022-09-02 02:56:01+00:00
- **Authors**: Yiyang Shen, Yongzhen Wang, Mingqiang Wei, Honghua Chen, Haoran Xie, Gary Cheng, Fu Lee Wang
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Rain is one of the most common weather which can completely degrade the image quality and interfere with the performance of many computer vision tasks, especially under heavy rain conditions. We observe that: (i) rain is a mixture of rain streaks and rainy haze; (ii) the scene depth determines the intensity of rain streaks and the transformation into the rainy haze; (iii) most existing deraining methods are only trained on synthetic rainy images, and hence generalize poorly to the real-world scenes. Motivated by these observations, we propose a new SEMI-supervised Mixture Of rain REmoval Generative Adversarial Network (Semi-MoreGAN), which consists of four key modules: (I) a novel attentional depth prediction network to provide precise depth estimation; (ii) a context feature prediction network composed of several well-designed detailed residual blocks to produce detailed image context features; (iii) a pyramid depth-guided non-local network to effectively integrate the image context with the depth information, and produce the final rain-free images; and (iv) a comprehensive semi-supervised loss function to make the model not limited to synthetic datasets but generalize smoothly to real-world heavy rainy scenes. Extensive experiments show clear improvements of our approach over twenty representative state-of-the-arts on both synthetic and real-world rainy images.



### Hybrid Relation Guided Set Matching for Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.13423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13423v1)
- **Published**: 2022-04-28 11:43:41+00:00
- **Updated**: 2022-04-28 11:43:41+00:00
- **Authors**: Xiang Wang, Shiwei Zhang, Zhiwu Qing, Mingqian Tang, Zhengrong Zuo, Changxin Gao, Rong Jin, Nong Sang
- **Comment**: Accepted by CVPR-2022
- **Journal**: None
- **Summary**: Current few-shot action recognition methods reach impressive performance by learning discriminative features for each video via episodic training and designing various temporal alignment strategies. Nevertheless, they are limited in that (a) learning individual features without considering the entire task may lose the most relevant information in the current episode, and (b) these alignment strategies may fail in misaligned instances. To overcome the two limitations, we propose a novel Hybrid Relation guided Set Matching (HyRSM) approach that incorporates two key components: hybrid relation module and set matching metric. The purpose of the hybrid relation module is to learn task-specific embeddings by fully exploiting associated relations within and cross videos in an episode. Built upon the task-specific features, we reformulate distance measure between query and support videos as a set matching problem and further design a bidirectional Mean Hausdorff Metric to improve the resilience to misaligned instances. By this means, the proposed HyRSM can be highly informative and flexible to predict query categories under the few-shot settings. We evaluate HyRSM on six challenging benchmarks, and the experimental results show its superiority over the state-of-the-art methods by a convincing margin. Project page: https://hyrsm-cvpr2022.github.io/.



### AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2204.13426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13426v1)
- **Published**: 2022-04-28 11:50:18+00:00
- **Updated**: 2022-04-28 11:50:18+00:00
- **Authors**: Mira Kim, Jaehoon Ko, Kyusun Cho, Junmyeong Choi, Daewon Choi, Seungryong Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework for 3D-aware object manipulation, called Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D shape, appearance, and camera pose from an image, and a high-quality image is rendered from the attributes through disentangled generative Neural Radiance Fields (NeRF). To improve the disentanglement ability, we present two losses, global-local attribute consistency loss defined between input and output, and swapped-attribute classification loss. Since training such auto-encoding networks from scratch without ground-truth shape and appearance information is non-trivial, we present a stage-wise training scheme, which dramatically helps to boost the performance. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies.



### Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2204.13453v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DG, math.SP, stat.ML, I.2.10; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/2204.13453v1)
- **Published**: 2022-04-28 12:36:09+00:00
- **Updated**: 2022-04-28 12:36:09+00:00
- **Authors**: Nicolas Donati, Etienne Corman, Maks Ovsjanikov
- **Comment**: To appear in: IEEE, Conference on Computer Vision and Pattern
  Recognition, 2022 // Main Manuscript: 8 pages (without references), 3
  figures, 3 tables // Supplementary: 4 pages, 4 figures, 1 table //
- **Journal**: None
- **Summary**: State-of-the-art fully intrinsic networks for non-rigid shape matching often struggle to disambiguate the symmetries of the shapes leading to unstable correspondence predictions. Meanwhile, recent advances in the functional map framework allow to enforce orientation preservation using a functional representation for tangent vector field transfer, through so-called complex functional maps. Using this representation, we propose a new deep learning approach to learn orientation-aware features in a fully unsupervised setting. Our architecture is built on top of DiffusionNet, making it robust to discretization changes. Additionally, we introduce a vector field-based loss, which promotes orientation preservation without using (often unstable) extrinsic descriptors.



### Learning from Pixel-Level Noisy Label : A New Perspective for Light Field Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.13456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13456v1)
- **Published**: 2022-04-28 12:44:08+00:00
- **Updated**: 2022-04-28 12:44:08+00:00
- **Authors**: Mingtao Feng, Kendong Liu, Liang Zhang, Hongshan Yu, Yaonan Wang, Ajmal Mian
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Saliency detection with light field images is becoming attractive given the abundant cues available, however, this comes at the expense of large-scale pixel level annotated data which is expensive to generate. In this paper, we propose to learn light field saliency from pixel-level noisy labels obtained from unsupervised hand crafted featured based saliency methods. Given this goal, a natural question is: can we efficiently incorporate the relationships among light field cues while identifying clean labels in a unified framework? We address this question by formulating the learning as a joint optimization of intra light field features fusion stream and inter scenes correlation stream to generate the predictions. Specially, we first introduce a pixel forgetting guided fusion module to mutually enhance the light field features and exploit pixel consistency across iterations to identify noisy pixels. Next, we introduce a cross scene noise penalty loss for better reflecting latent structures of training data and enabling the learning to be invariant to noise. Extensive experiments on multiple benchmark datasets demonstrate the superiority of our framework showing that it learns saliency prediction comparable to state-of-the-art fully supervised light field saliency methods. Our code is available at https://github.com/OLobbCode/NoiseLF.



### TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2204.13483v3
- **DOI**: 10.1109/ITSC55140.2022.9922539
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.13483v3)
- **Published**: 2022-04-28 13:17:06+00:00
- **Updated**: 2022-07-27 09:46:06+00:00
- **Authors**: Lianqing Zheng, Zhixiong Ma, Xichan Zhu, Bin Tan, Sen Li, Kai Long, Weiqi Sun, Sihan Chen, Lu Zhang, Mengyue Wan, Libo Huang, Jie Bai
- **Comment**: 2022 IEEE International Intelligent Transportation Systems Conference
  (ITSC 2022)
- **Journal**: None
- **Summary**: The next-generation high-resolution automotive radar (4D radar) can provide additional elevation measurement and denser point clouds, which has great potential for 3D sensing in autonomous driving. In this paper, we introduce a dataset named TJ4DRadSet with 4D radar points for autonomous driving research. The dataset was collected in various driving scenarios, with a total of 7757 synchronized frames in 44 consecutive sequences, which are well annotated with 3D bounding boxes and track ids. We provide a 4D radar-based 3D object detection baseline for our dataset to demonstrate the effectiveness of deep learning methods for 4D radar point clouds. The dataset can be accessed via the following link: https://github.com/TJRadarLab/TJ4DRadSet.



### Streaming Multiscale Deep Equilibrium Models
- **Arxiv ID**: http://arxiv.org/abs/2204.13492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13492v3)
- **Published**: 2022-04-28 13:35:14+00:00
- **Updated**: 2022-08-09 11:35:31+00:00
- **Authors**: Can Ufuk Ertenli, Emre Akbas, Ramazan Gokberk Cinbis
- **Comment**: Accepted to ECCV 2022, 20 pages, 8 figures
- **Journal**: None
- **Summary**: We present StreamDEQ, a method that infers frame-wise representations on videos with minimal per-frame computation. In contrast to conventional methods where compute time grows at least linearly with the network depth, we aim to update the representations in a continuous manner. For this purpose, we leverage the recently emerging implicit layer models, which infer the representation of an image by solving a fixed-point problem. Our main insight is to leverage the slowly changing nature of videos and use the previous frame representation as an initial condition on each frame. This scheme effectively recycles the recent inference computations and greatly reduces the needed processing time. Through extensive experimental analysis, we show that StreamDEQ is able to recover near-optimal representations in a few frames time, and maintain an up-to-date representation throughout the video duration. Our experiments on video semantic segmentation and video object detection show that StreamDEQ achieves on par accuracy with the baseline (standard MDEQ) while being more than $3\times$ faster. Code and additional results are available at https://ufukertenli.github.io/streamdeq/.



### Unsupervised Spatial-spectral Hyperspectral Image Reconstruction and Clustering with Diffusion Geometry
- **Arxiv ID**: http://arxiv.org/abs/2204.13497v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2204.13497v1)
- **Published**: 2022-04-28 13:42:12+00:00
- **Updated**: 2022-04-28 13:42:12+00:00
- **Authors**: Kangning Cui, Ruoning Li, Sam L. Polk, James M. Murphy, Robert J. Plemmons, Raymond H. Chan
- **Comment**: 7 pages, 1 figure
- **Journal**: None
- **Summary**: Hyperspectral images, which store a hundred or more spectral bands of reflectance, have become an important data source in natural and social sciences. Hyperspectral images are often generated in large quantities at a relatively coarse spatial resolution. As such, unsupervised machine learning algorithms incorporating known structure in hyperspectral imagery are needed to analyze these images automatically. This work introduces the Spatial-Spectral Image Reconstruction and Clustering with Diffusion Geometry (DSIRC) algorithm for partitioning highly mixed hyperspectral images. DSIRC reduces measurement noise through a shape-adaptive reconstruction procedure. In particular, for each pixel, DSIRC locates spectrally correlated pixels within a data-adaptive spatial neighborhood and reconstructs that pixel's spectral signature using those of its neighbors. DSIRC then locates high-density, high-purity pixels far in diffusion distance (a data-dependent distance metric) from other high-density, high-purity pixels and treats these as cluster exemplars, giving each a unique label. Non-modal pixels are assigned the label of their diffusion distance-nearest neighbor of higher density and purity that is already labeled. Strong numerical results indicate that incorporating spatial information through image reconstruction substantially improves the performance of pixel-wise clustering.



### Inverse-Designed Meta-Optics with Spectral-Spatial Engineered Response to Mimic Color Perception
- **Arxiv ID**: http://arxiv.org/abs/2204.13520v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13520v1)
- **Published**: 2022-04-28 14:12:54+00:00
- **Updated**: 2022-04-28 14:12:54+00:00
- **Authors**: Chris Munley, Wenchao Ma, Johannes E. Frch, Quentin A. A. Tanguy, Elyas Bayati, Karl F. Bhringer, Zin Lin, Raphal Pestourie, Steven G. Johnson, Arka Majumdar
- **Comment**: None
- **Journal**: None
- **Summary**: Meta-optics have rapidly become a major research field within the optics and photonics community, strongly driven by the seemingly limitless opportunities made possible by controlling optical wavefronts through interaction with arrays of sub-wavelength scatterers. As more and more modalities are explored, the design strategies to achieve desired functionalities become increasingly demanding, necessitating more advanced design techniques. Herein, the inverse-design approach is utilized to create a set of single-layer meta-optics that simultaneously focus light and shape the spectra of focused light without using any filters. Thus, both spatial and spectral properties of the meta-optics are optimized, resulting in spectra that mimic the color matching functions of the CIE 1931 XYZ color space, which links the distributions of wavelengths in light and the color perception of a human eye. Experimental demonstrations of these meta-optics show qualitative agreement with the theoretical predictions and help elucidate the focusing mechanism of these devices.



### Tragedy Plus Time: Capturing Unintended Human Activities from Weakly-labeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.13548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13548v1)
- **Published**: 2022-04-28 14:56:43+00:00
- **Updated**: 2022-04-28 14:56:43+00:00
- **Authors**: Arnav Chakravarthy, Zhiyuan Fang, Yezhou Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In videos that contain actions performed unintentionally, agents do not achieve their desired goals. In such videos, it is challenging for computer vision systems to understand high-level concepts such as goal-directed behavior, an ability present in humans from a very early age. Inculcating this ability in artificially intelligent agents would make them better social learners by allowing them to evaluate human action under a teleological lens. To validate the ability of deep learning models to perform this task, we curate the W-Oops dataset, built upon the Oops dataset [15]. W-Oops consists of 2,100 unintentional human action videos, with 44 goal-directed and 30 unintentional video-level activity labels collected through human annotations. Due to the expensive segment annotation procedure, we propose a weakly supervised algorithm for localizing the goal-directed as well as unintentional temporal regions in the video leveraging solely video-level labels. In particular, we employ an attention mechanism-based strategy that predicts the temporal regions which contribute the most to a classification task. Meanwhile, our designed overlap regularization allows the model to focus on distinct portions of the video for inferring the goal-directed and unintentional activity while guaranteeing their temporal ordering. Extensive quantitative experiments verify the validity of our localization method. We further conduct a video captioning experiment which demonstrates that the proposed localization module does indeed assist teleological action understanding.



### Mixup-based Deep Metric Learning Approaches for Incomplete Supervision
- **Arxiv ID**: http://arxiv.org/abs/2204.13572v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13572v3)
- **Published**: 2022-04-28 15:36:16+00:00
- **Updated**: 2022-08-27 13:41:35+00:00
- **Authors**: Luiz H. Buris, Daniel C. G. Pedronette, Joao P. Papa, Jurandy Almeida, Gustavo Carneiro, Fabio A. Faria
- **Comment**: 5 pages, 1 figure, accepted for presentation at the ICIP2022
- **Journal**: None
- **Summary**: Deep learning architectures have achieved promising results in different areas (e.g., medicine, agriculture, and security). However, using those powerful techniques in many real applications becomes challenging due to the large labeled collections required during training. Several works have pursued solutions to overcome it by proposing strategies that can learn more for less, e.g., weakly and semi-supervised learning approaches. As these approaches do not usually address memorization and sensitivity to adversarial examples, this paper presents three deep metric learning approaches combined with Mixup for incomplete-supervision scenarios. We show that some state-of-the-art approaches in metric learning might not work well in such scenarios. Moreover, the proposed approaches outperform most of them in different datasets.



### Symmetric Transformer-based Network for Unsupervised Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2204.13575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13575v1)
- **Published**: 2022-04-28 15:45:09+00:00
- **Updated**: 2022-04-28 15:45:09+00:00
- **Authors**: Mingrui Ma, Lei Song, Yuanbo Xu, Guixia Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image registration is a fundamental and critical task in medical image analysis. With the rapid development of deep learning, convolutional neural networks (CNN) have dominated the medical image registration field. Due to the disadvantage of the local receptive field of CNN, some recent registration methods have focused on using transformers for non-local registration. However, the standard Transformer has a vast number of parameters and high computational complexity, which causes Transformer can only be applied at the bottom of the registration models. As a result, only coarse information is available at the lowest resolution, limiting the contribution of Transformer in their models. To address these challenges, we propose a convolution-based efficient multi-head self-attention (CEMSA) block, which reduces the parameters of the traditional Transformer and captures local spatial context information for reducing semantic ambiguity in the attention mechanism. Based on the proposed CEMSA, we present a novel Symmetric Transformer-based model (SymTrans). SymTrans employs the Transformer blocks in the encoder and the decoder respectively to model the long-range spatial cross-image relevance. We apply SymTrans to the displacement field and diffeomorphic registration. Experimental results show that our proposed method achieves state-of-the-art performance in image registration. Our code is publicly available at \url{https://github.com/MingR-Ma/SymTrans}.



### CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.14217v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.14217v2)
- **Published**: 2022-04-28 15:51:11+00:00
- **Updated**: 2022-05-27 14:40:07+00:00
- **Authors**: Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang
- **Comment**: None
- **Journal**: None
- **Summary**: The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.



### Computer Vision for Road Imaging and Pothole Detection: A State-of-the-Art Review of Systems and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2204.13590v1
- **DOI**: 10.1093/tse/tdac026
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.13590v1)
- **Published**: 2022-04-28 16:00:39+00:00
- **Updated**: 2022-04-28 16:00:39+00:00
- **Authors**: Nachuan Ma, Jiahe Fan, Wenshuo Wang, Jin Wu, Yu Jiang, Lihua Xie, Rui Fan
- **Comment**: accepted to Transportation Safety and Environment
- **Journal**: None
- **Summary**: Computer vision algorithms have been prevalently utilized for 3-D road imaging and pothole detection for over two decades. Nonetheless, there is a lack of systematic survey articles on state-of-the-art (SoTA) computer vision techniques, especially deep learning models, developed to tackle these problems. This article first introduces the sensing systems employed for 2-D and 3-D road data acquisition, including camera(s), laser scanners, and Microsoft Kinect. Afterward, it thoroughly and comprehensively reviews the SoTA computer vision algorithms, including (1) classical 2-D image processing, (2) 3-D point cloud modeling and segmentation, and (3) machine/deep learning, developed for road pothole detection. This article also discusses the existing challenges and future development trends of computer vision-based road pothole detection approaches: classical 2-D image processing-based and 3-D point cloud modeling and segmentation-based approaches have already become history; and Convolutional neural networks (CNNs) have demonstrated compelling road pothole detection results and are promising to break the bottleneck with the future advances in self/un-supervised learning for multi-modal semantic segmentation. We believe that this survey can serve as practical guidance for developing the next-generation road condition assessment systems.



### Generative Adversarial Networks for Image Super-Resolution: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2204.13620v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13620v2)
- **Published**: 2022-04-28 16:35:04+00:00
- **Updated**: 2022-10-03 08:41:40+00:00
- **Authors**: Chunwei Tian, Xuanyu Zhang, Jerry Chun-Wei Lin, Wangmeng Zuo, Yanning Zhang, Chia-Wen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images with small samples. However, there are little literatures summarizing different GANs in SISR. In this paper, we conduct a comparative study of GANs from different perspectives. We first take a look at developments of GANs. Second, we present popular architectures for GANs in big and small samples for image applications. Then, we analyze motivations, implementations and differences of GANs based optimization methods and discriminative learning for image super-resolution in terms of supervised, semi-supervised and unsupervised manners. Next, we compare performance of these popular GANs on public datasets via quantitative and qualitative analysis in SISR. Finally, we highlight challenges of GANs and potential research points for SISR.



### Oracle Guided Image Synthesis with Relative Queries
- **Arxiv ID**: http://arxiv.org/abs/2204.14189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.14189v1)
- **Published**: 2022-04-28 16:37:09+00:00
- **Updated**: 2022-04-28 16:37:09+00:00
- **Authors**: Alec Helbling, Christopher John Rozell, Matthew O'Shaughnessy, Kion Fallah
- **Comment**: Published at the International Conference on Learning Representations
  2022, Workshop on Deep Generative Models for Highly Structured Data
- **Journal**: None
- **Summary**: Isolating and controlling specific features in the outputs of generative models in a user-friendly way is a difficult and open-ended problem. We develop techniques that allow an oracle user to generate an image they are envisioning in their head by answering a sequence of relative queries of the form \textit{"do you prefer image $a$ or image $b$?"} Our framework consists of a Conditional VAE that uses the collected relative queries to partition the latent space into preference-relevant features and non-preference-relevant features. We then use the user's responses to relative queries to determine the preference-relevant features that correspond to their envisioned output image. Additionally, we develop techniques for modeling the uncertainty in images' predicted preference-relevant features, allowing our framework to generalize to scenarios in which the relative query training set contains noise.



### Rotationally Equivariant 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.13630v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13630v2)
- **Published**: 2022-04-28 16:48:50+00:00
- **Updated**: 2022-07-09 23:18:01+00:00
- **Authors**: Hong-Xing Yu, Jiajun Wu, Li Yi
- **Comment**: CVPR 2022, project website: https://kovenyu.com/eon/
- **Journal**: None
- **Summary**: Rotation equivariance has recently become a strongly desired property in the 3D deep learning community. Yet most existing methods focus on equivariance regarding a global input rotation while ignoring the fact that rotation symmetry has its own spatial support. Specifically, we consider the object detection problem in 3D scenes, where an object bounding box should be equivariant regarding the object pose, independent of the scene motion. This suggests a new desired property we call object-level rotation equivariance. To incorporate object-level rotation equivariance into 3D object detectors, we need a mechanism to extract equivariant features with local object-level spatial support while being able to model cross-object context information. To this end, we propose Equivariant Object detection Network (EON) with a rotation equivariance suspension design to achieve object-level equivariance. EON can be applied to modern point cloud object detectors, such as VoteNet and PointRCNN, enabling them to exploit object rotation symmetry in scene-scale inputs. Our experiments on both indoor scene and autonomous driving datasets show that significant improvements are obtained by plugging our EON design into existing state-of-the-art 3D object detectors.



### Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly
- **Arxiv ID**: http://arxiv.org/abs/2204.13631v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13631v3)
- **Published**: 2022-04-28 16:51:27+00:00
- **Updated**: 2022-10-20 17:36:51+00:00
- **Authors**: Spencer Whitehead, Suzanne Petryk, Vedaad Shakib, Joseph Gonzalez, Trevor Darrell, Anna Rohrbach, Marcus Rohrbach
- **Comment**: ECCV 2022. Code and models are available here:
  https://github.com/facebookresearch/reliable_vqa
- **Journal**: None
- **Summary**: Machine learning has advanced dramatically, narrowing the accuracy gap to humans in multimodal tasks like visual question answering (VQA). However, while humans can say "I don't know" when they are uncertain (i.e., abstain from answering a question), such ability has been largely neglected in multimodal research, despite the importance of this problem to the usage of VQA in real settings. In this work, we promote a problem formulation for reliable VQA, where we prefer abstention over providing an incorrect answer. We first enable abstention capabilities for several VQA models, and analyze both their coverage, the portion of questions answered, and risk, the error on that portion. For that, we explore several abstention approaches. We find that although the best performing models achieve over 70% accuracy on the VQA v2 dataset, introducing the option to abstain by directly using a model's softmax scores limits them to answering less than 7.5% of the questions to achieve a low risk of error (i.e., 1%). This motivates us to utilize a multimodal selection function to directly estimate the correctness of the predicted answers, which we show can increase the coverage by, for example, 2.3x from 6.8% to 15.6% at 1% risk. While it is important to analyze both coverage and risk, these metrics have a trade-off which makes comparing VQA models challenging. To address this, we also propose an Effective Reliability metric for VQA that places a larger cost on incorrect answers compared to abstentions. This new problem formulation, metric, and analysis for VQA provide the groundwork for building effective and reliable VQA models that have the self-awareness to abstain if and only if they don't know the answer.



### SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2204.13635v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13635v1)
- **Published**: 2022-04-28 16:53:25+00:00
- **Updated**: 2022-04-28 16:53:25+00:00
- **Authors**: Danish Nazir, Marcus Liwicki, Didier Stricker, Muhammad Zeshan Afzal
- **Comment**: None
- **Journal**: None
- **Summary**: Depth completion involves recovering a dense depth map from a sparse map and an RGB image. Recent approaches focus on utilizing color images as guidance images to recover depth at invalid pixels. However, color images alone are not enough to provide the necessary semantic understanding of the scene. Consequently, the depth completion task suffers from sudden illumination changes in RGB images (e.g., shadows). In this paper, we propose a novel three-branch backbone comprising color-guided, semantic-guided, and depth-guided branches. Specifically, the color-guided branch takes a sparse depth map and RGB image as an input and generates color depth which includes color cues (e.g., object boundaries) of the scene. The predicted dense depth map of color-guided branch along-with semantic image and sparse depth map is passed as input to semantic-guided branch for estimating semantic depth. The depth-guided branch takes sparse, color, and semantic depths to generate the dense depth map. The color depth, semantic depth, and guided depth are adaptively fused to produce the output of our proposed three-branch backbone. In addition, we also propose to apply semantic-aware multi-modal attention-based fusion block (SAMMAFB) to fuse features between all three branches. We further use CSPN++ with Atrous convolutions to refine the dense depth map produced by our three-branch backbone. Extensive experiments show that our model achieves state-of-the-art performance in the KITTI depth completion benchmark at the time of submission.



### Learning to Extract Building Footprints from Off-Nadir Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2204.13637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13637v1)
- **Published**: 2022-04-28 16:56:06+00:00
- **Updated**: 2022-04-28 16:56:06+00:00
- **Authors**: Jinwang Wang, Lingxuan Meng, Weijia Li, Wen Yang, Lei Yu, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting building footprints from aerial images is essential for precise urban mapping with photogrammetric computer vision technologies. Existing approaches mainly assume that the roof and footprint of a building are well overlapped, which may not hold in off-nadir aerial images as there is often a big offset between them. In this paper, we propose an offset vector learning scheme, which turns the building footprint extraction problem in off-nadir images into an instance-level joint prediction problem of the building roof and its corresponding "roof to footprint" offset vector. Thus the footprint can be estimated by translating the predicted roof mask according to the predicted offset vector. We further propose a simple but effective feature-level offset augmentation module, which can significantly refine the offset vector prediction by introducing little extra cost. Moreover, a new dataset, Buildings in Off-Nadir Aerial Images (BONAI), is created and released in this paper. It contains 268,958 building instances across 3,300 aerial images with fully annotated instance-level roof, footprint, and corresponding offset vector for each building. Experiments on the BONAI dataset demonstrate that our method achieves the state-of-the-art, outperforming other competitors by 3.37 to 7.39 points in F1-score. The codes, datasets, and trained models are available at https://github.com/jwwangchn/BONAI.git.



### Unlocking High-Accuracy Differentially Private Image Classification through Scale
- **Arxiv ID**: http://arxiv.org/abs/2204.13650v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.13650v2)
- **Published**: 2022-04-28 17:10:56+00:00
- **Updated**: 2022-06-16 17:47:42+00:00
- **Authors**: Soham De, Leonard Berrada, Jamie Hayes, Samuel L. Smith, Borja Balle
- **Comment**: None
- **Journal**: None
- **Summary**: Differential Privacy (DP) provides a formal privacy guarantee preventing adversaries with access to a machine learning model from extracting information about individual training points. Differentially Private Stochastic Gradient Descent (DP-SGD), the most popular DP training method for deep learning, realizes this protection by injecting noise during training. However previous works have found that DP-SGD often leads to a significant degradation in performance on standard image classification benchmarks. Furthermore, some authors have postulated that DP-SGD inherently performs poorly on large models, since the norm of the noise required to preserve privacy is proportional to the model dimension. In contrast, we demonstrate that DP-SGD on over-parameterized models can perform significantly better than previously thought. Combining careful hyper-parameter tuning with simple techniques to ensure signal propagation and improve the convergence rate, we obtain a new SOTA without extra data on CIFAR-10 of 81.4% under (8, 10^{-5})-DP using a 40-layer Wide-ResNet, improving over the previous SOTA of 71.7%. When fine-tuning a pre-trained NFNet-F3, we achieve a remarkable 83.8% top-1 accuracy on ImageNet under (0.5, 8*10^{-7})-DP. Additionally, we also achieve 86.7% top-1 accuracy under (8, 8 \cdot 10^{-7})-DP, which is just 4.3% below the current non-private SOTA for this task. We believe our results are a significant step towards closing the accuracy gap between private and non-private image classification.



### GRIT: General Robust Image Task Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2204.13653v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13653v2)
- **Published**: 2022-04-28 17:13:23+00:00
- **Updated**: 2022-05-02 19:26:41+00:00
- **Authors**: Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, Derek Hoiem
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision models excel at making predictions when the test distribution closely resembles the training distribution. Such models have yet to match the ability of biological vision to learn from multiple sources and generalize to new data sources and tasks. To facilitate the development and evaluation of more general vision systems, we introduce the General Robust Image Task (GRIT) benchmark. GRIT evaluates the performance, robustness, and calibration of a vision system across a variety of image prediction tasks, concepts, and data sources. The seven tasks in GRIT are selected to cover a range of visual skills: object categorization, object localization, referring expression grounding, visual question answering, segmentation, human keypoint detection, and surface normal estimation. GRIT is carefully designed to enable the evaluation of robustness under image perturbations, image source distribution shift, and concept distribution shift. By providing a unified platform for thorough assessment of skills and concepts learned by a vision model, we hope GRIT catalyzes the development of performant and robust general purpose vision systems.



### Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2204.13656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13656v1)
- **Published**: 2022-04-28 17:18:21+00:00
- **Updated**: 2022-04-28 17:18:21+00:00
- **Authors**: Zekang Chen, Jia Wei, Rui Li
- **Comment**: Accepted in IJCAI 2022
- **Journal**: None
- **Summary**: In clinical practice, well-aligned multi-modal images, such as Magnetic Resonance (MR) and Computed Tomography (CT), together can provide complementary information for image-guided therapies. Multi-modal image registration is essential for the accurate alignment of these multi-modal images. However, it remains a very challenging task due to complicated and unknown spatial correspondence between different modalities. In this paper, we propose a novel translation-based unsupervised deformable image registration approach to convert the multi-modal registration problem to a mono-modal one. Specifically, our approach incorporates a discriminator-free translation network to facilitate the training of the registration network and a patchwise contrastive loss to encourage the translation network to preserve object shapes. Furthermore, we propose to replace an adversarial loss, that is widely used in previous multi-modal image registration methods, with a pixel loss in order to integrate the output of translation into the target modality. This leads to an unsupervised method requiring no ground-truth deformation or pairs of aligned images for training. We evaluate four variants of our approach on the public Learn2Reg 2021 datasets \cite{hering2021learn2reg}. The experimental results demonstrate that the proposed architecture achieves state-of-the-art performance. Our code is available at https://github.com/heyblackC/DFMIR.



### ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2204.13662v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13662v3)
- **Published**: 2022-04-28 17:23:59+00:00
- **Updated**: 2023-04-23 13:11:57+00:00
- **Authors**: Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael J. Black, Otmar Hilliges
- **Comment**: Project page: https://arctic.is.tue.mpg.de
- **Journal**: None
- **Summary**: Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. Our code and data are available at https://arctic.is.tue.mpg.de.



### Unified Simulation, Perception, and Generation of Human Behavior
- **Arxiv ID**: http://arxiv.org/abs/2204.13678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.13678v1)
- **Published**: 2022-04-28 17:40:44+00:00
- **Updated**: 2022-04-28 17:40:44+00:00
- **Authors**: Ye Yuan
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: Understanding and modeling human behavior is fundamental to almost any computer vision and robotics applications that involve humans. In this thesis, we take a holistic approach to human behavior modeling and tackle its three essential aspects -- simulation, perception, and generation. Throughout the thesis, we show how the three aspects are deeply connected and how utilizing and improving one aspect can greatly benefit the other aspects. We also discuss the lessons learned and our vision for what is next for human behavior modeling.



### KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients
- **Arxiv ID**: http://arxiv.org/abs/2204.13683v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13683v1)
- **Published**: 2022-04-28 17:48:48+00:00
- **Updated**: 2022-04-28 17:48:48+00:00
- **Authors**: Niklas Hanselmann, Katrin Renz, Kashyap Chitta, Apratim Bhattacharyya, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: Simulators offer the possibility of safe, low-cost development of self-driving systems. However, current driving simulators exhibit na\"ive behavior models for background traffic. Hand-tuned scenarios are typically added during simulation to induce safety-critical situations. An alternative approach is to adversarially perturb the background traffic trajectories. In this paper, we study this approach to safety-critical driving scenario generation using the CARLA simulator. We use a kinematic bicycle model as a proxy to the simulator's true dynamics and observe that gradients through this proxy model are sufficient for optimizing the background traffic trajectories. Based on this finding, we propose KING, which generates safety-critical driving scenarios with a 20% higher success rate than black-box optimization. By solving the scenarios generated by KING using a privileged rule-based expert algorithm, we obtain training data for an imitation learning policy. After fine-tuning on this new data, we show that the policy becomes better at avoiding collisions. Importantly, our generated data leads to reduced collisions on both held-out scenarios generated via KING as well as traditional hand-crafted scenarios, demonstrating improved robustness.



### HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling
- **Arxiv ID**: http://arxiv.org/abs/2204.13686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13686v2)
- **Published**: 2022-04-28 17:54:25+00:00
- **Updated**: 2023-04-16 12:26:14+00:00
- **Authors**: Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, Lei Yang, Ziwei Liu
- **Comment**: Homepage: https://caizhongang.github.io/projects/HuMMan/
- **Journal**: None
- **Summary**: 4D human sensing and modeling are fundamental tasks in vision and graphics with numerous applications. With the advances of new sensors and algorithms, there is an increasing demand for more versatile datasets. In this work, we contribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human subjects, 400k sequences and 60M frames. HuMMan has several appealing properties: 1) multi-modal data and annotations including color images, point clouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile device is included in the sensor suite; 3) a set of 500 actions, designed to cover fundamental movements; 4) multiple tasks such as action recognition, pose estimation, parametric human recovery, and textured mesh reconstruction are supported and evaluated. Extensive experiments on HuMMan voice the need for further study on challenges such as fine-grained action recognition, dynamic human mesh reconstruction, point cloud-based parametric human recovery, and cross-device domain gaps.



### NeurMiPs: Neural Mixture of Planar Experts for View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.13696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13696v1)
- **Published**: 2022-04-28 17:59:41+00:00
- **Updated**: 2022-04-28 17:59:41+00:00
- **Authors**: Zhi-Hao Lin, Wei-Chiu Ma, Hao-Yu Hsu, Yu-Chiang Frank Wang, Shenlong Wang
- **Comment**: CVPR 2022. Project page: https://zhihao-lin.github.io/neurmips/
- **Journal**: None
- **Summary**: We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based scene representation for modeling geometry and appearance. NeurMiPs leverages a collection of local planar experts in 3D space as the scene representation. Each planar expert consists of the parameters of the local rectangular shape representing geometry and a neural radiance field modeling the color and opacity. We render novel views by calculating ray-plane intersections and composite output colors and densities at intersected points to the image. NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of the neural radiance field. Experiments demonstrate superior performance and speed of our proposed method, compared to other 3D representations in novel view synthesis.



### Learning cosmology and clustering with cosmic graphs
- **Arxiv ID**: http://arxiv.org/abs/2204.13713v2
- **DOI**: 10.3847/1538-4357/ac8930
- **Categories**: **astro-ph.CO**, astro-ph.GA, astro-ph.IM, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13713v2)
- **Published**: 2022-04-28 18:00:02+00:00
- **Updated**: 2023-02-08 20:32:15+00:00
- **Authors**: Pablo Villanueva-Domingo, Francisco Villaescusa-Navarro
- **Comment**: 21 pages, 8 figures, code publicly available at
  https://github.com/PabloVD/CosmoGraphNet
- **Journal**: ApJ 937 115 (2022)
- **Summary**: We train deep learning models on thousands of galaxy catalogues from the state-of-the-art hydrodynamic simulations of the CAMELS project to perform regression and inference. We employ Graph Neural Networks (GNNs), architectures designed to work with irregular and sparse data, like the distribution of galaxies in the Universe. We first show that GNNs can learn to compute the power spectrum of galaxy catalogues with a few percent accuracy. We then train GNNs to perform likelihood-free inference at the galaxy-field level. Our models are able to infer the value of $\Omega_{\rm m}$ with a $\sim12\%-13\%$ accuracy just from the positions of $\sim1000$ galaxies in a volume of $(25~h^{-1}{\rm Mpc})^3$ at $z=0$ while accounting for astrophysical uncertainties as modelled in CAMELS. Incorporating information from galaxy properties, such as stellar mass, stellar metallicity, and stellar radius, increases the accuracy to $4\%-8\%$. Our models are built to be translational and rotational invariant, and they can extract information from any scale larger than the minimum distance between two galaxies. However, our models are not completely robust: testing on simulations run with a different subgrid physics than the ones used for training does not yield as accurate results.



### One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation
- **Arxiv ID**: http://arxiv.org/abs/2204.13738v3
- **DOI**: 10.1109/TMI.2023.3261707
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13738v3)
- **Published**: 2022-04-28 18:49:27+00:00
- **Updated**: 2023-03-29 19:05:39+00:00
- **Authors**: Jiang Liu, Srivathsa Pasumarthi, Ben Duffy, Enhao Gong, Keshav Datta, Greg Zaharchuk
- **Comment**: IEEE TMI accepted final version
- **Journal**: None
- **Summary**: Multi-contrast magnetic resonance imaging (MRI) is widely used in clinical practice as each contrast provides complementary information. However, the availability of each imaging contrast may vary amongst patients, which poses challenges to radiologists and automated image analysis algorithms. A general approach for tackling this problem is missing data imputation, which aims to synthesize the missing contrasts from existing ones. While several convolutional neural networks (CNN) based algorithms have been proposed, they suffer from the fundamental limitations of CNN models, such as the requirement for fixed numbers of input and output channels, the inability to capture long-range dependencies, and the lack of interpretability. In this work, we formulate missing data imputation as a sequence-to-sequence learning problem and propose a multi-contrast multi-scale Transformer (MMT), which can take any subset of input contrasts and synthesize those that are missing. MMT consists of a multi-scale Transformer encoder that builds hierarchical representations of inputs combined with a multi-scale Transformer decoder that generates the outputs in a coarse-to-fine fashion. The proposed multi-contrast Swin Transformer blocks can efficiently capture intra- and inter-contrast dependencies for accurate image synthesis. Moreover, MMT is inherently interpretable as it allows us to understand the importance of each input contrast in different regions by analyzing the in-built attention maps of Transformer blocks in the decoder. Extensive experiments on two large-scale multi-contrast MRI datasets demonstrate that MMT outperforms the state-of-the-art methods quantitatively and qualitatively.



### Learning to Split for Automatic Bias Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.13749v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13749v2)
- **Published**: 2022-04-28 19:41:08+00:00
- **Updated**: 2022-07-20 23:44:01+00:00
- **Authors**: Yujia Bao, Regina Barzilay
- **Comment**: None
- **Journal**: None
- **Summary**: Classifiers are biased when trained on biased datasets. As a remedy, we propose Learning to Split (ls), an algorithm for automatic bias detection. Given a dataset with input-label pairs, ls learns to split this dataset so that predictors trained on the training split cannot generalize to the testing split. This performance gap suggests that the testing split is under-represented in the dataset, which is a signal of potential bias. Identifying non-generalizable splits is challenging since we have no annotations about the bias. In this work, we show that the prediction correctness of each example in the testing split can be used as a source of weak supervision: generalization performance will drop if we move examples that are predicted correctly away from the testing split, leaving only those that are mis-predicted. ls is task-agnostic and can be applied to any supervised learning problem, ranging from natural language understanding and image classification to molecular property prediction. Empirical results show that ls is able to generate astonishingly challenging splits that correlate with human-identified biases. Moreover, we demonstrate that combining robust learning algorithms (such as group DRO) with splits identified by ls enables automatic de-biasing. Compared to previous state-of-the-art, we substantially improve the worst-group performance (23.4% on average) when the source of biases is unknown during training and validation.



### Depth Estimation with Simplified Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.13791v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13791v3)
- **Published**: 2022-04-28 21:39:00+00:00
- **Updated**: 2022-05-27 23:14:52+00:00
- **Authors**: John Yang, Le An, Anurag Dixit, Jinkyu Koo, Su Inn Park
- **Comment**: Accepted for the CVPR 2022 Transformers For Vision (T4V) workshop
- **Journal**: None
- **Summary**: Transformer and its variants have shown state-of-the-art results in many vision tasks recently, ranging from image classification to dense prediction. Despite of their success, limited work has been reported on improving the model efficiency for deployment in latency-critical applications, such as autonomous driving and robotic navigation. In this paper, we aim at improving upon the existing transformers in vision, and propose a method for self-supervised monocular Depth Estimation with Simplified Transformer (DEST), which is efficient and particularly suitable for deployment on GPU-based platforms. Through strategic design choices, our model leads to significant reduction in model size, complexity, as well as inference latency, while achieving superior accuracy as compared to state-of-the-art. We also show that our design generalize well to other dense prediction task without bells and whistles.




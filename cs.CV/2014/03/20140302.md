# Arxiv Papers in cs.CV on 2014-03-02
### Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1403.0240v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE, cs.NA, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1403.0240v1)
- **Published**: 2014-03-02 16:58:29+00:00
- **Updated**: 2014-03-02 16:58:29+00:00
- **Authors**: Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale
- **Comment**: 21 pages, 10 figures
- **Journal**: None
- **Summary**: Bio-image analysis is challenging due to inhomogeneous intensity distributions and high levels of noise in the images. Bayesian inference provides a principled way for regularizing the problem using prior knowledge. A fundamental choice is how one measures "distances" between shapes in an image. It has been shown that the straightforward geometric L2 distance is degenerate and leads to pathological situations. This is avoided when using Sobolev gradients, rendering the segmentation problem less ill-posed. The high computational cost and implementation overhead of Sobolev gradients, however, have hampered practical applications. We show how particle methods as applied to image segmentation allow for a simple and computationally efficient implementation of Sobolev gradients. We show that the evaluation of Sobolev gradients amounts to particle-particle interactions along the contour in an image. We extend an existing particle-based segmentation algorithm to using Sobolev gradients. Using synthetic and real-world images, we benchmark the results for both 2D and 3D images using piecewise smooth and piecewise constant region models. The present particle approximation of Sobolev gradients is 2.8 to 10 times faster than the previous reference implementation, but retains the known favorable properties of Sobolev gradients. This speedup is achieved by using local particle-particle interactions instead of solving a global Poisson equation at each iteration. The computational time per iteration is higher for Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition the optimization problem, however, a smaller number of overall iterations may be necessary for the algorithm to converge, which can in some cases amortize the higher per-iteration cost.




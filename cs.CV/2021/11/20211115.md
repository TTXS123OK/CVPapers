# Arxiv Papers in cs.CV on 2021-11-15
### Human-error-potential Estimation based on Wearable Biometric Sensors
- **Arxiv ID**: http://arxiv.org/abs/2111.08502v1
- **DOI**: 10.5220/0010642400003064
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, cs.MM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2111.08502v1)
- **Published**: 2021-11-15 00:27:13+00:00
- **Updated**: 2021-11-15 00:27:13+00:00
- **Authors**: Hiroki Ohashi, Hiroto Nagayoshi
- **Comment**: Accepted by KDIR 2021 : 13th International Conference on Knowledge
  Discovery and Information Retrieval. (ISBN 978-989-758-533-3; ISSN 2184-3228,
  https://www.scitepress.org/PublicationsDetail.aspx?ID=X8eCT969CC0=&t=1)
- **Journal**: None
- **Summary**: This study tackles on a new problem of estimating human-error potential on a shop floor on the basis of wearable sensors. Unlike existing studies that utilize biometric sensing technology to estimate people's internal state such as fatigue and mental stress, we attempt to estimate the human-error potential in a situation where a target person does not stay calm, which is much more difficult as sensor noise significantly increases. We propose a novel formulation, in which the human-error-potential estimation problem is reduced to a classification problem, and introduce a new method that can be used for solving the classification problem even with noisy sensing data. The key ideas are to model the process of calculating biometric indices probabilistically so that the prior knowledge on the biometric indices can be integrated, and to utilize the features that represent the movement of target persons in combination with biometric features. The experimental analysis showed that our method effectively estimates the human-error potential.



### An Embarrassingly Pragmatic Introduction to Vision-based Autonomous Robots
- **Arxiv ID**: http://arxiv.org/abs/2112.05534v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.05534v2)
- **Published**: 2021-11-15 01:31:28+00:00
- **Updated**: 2021-12-14 05:19:00+00:00
- **Authors**: Marcos V. Conde
- **Comment**: CS Thesis. Lecture Notes in Computer Science
- **Journal**: None
- **Summary**: Autonomous robots are currently one of the most popular Artificial Intelligence problems, having experienced significant advances in the last decade, from Self-driving cars and humanoids to delivery robots and drones. Part of the problem is to get a robot to emulate the perception of human beings, our sense of sight, replacing the eyes with cameras and the brain with mathematical models such as Neural Networks. Developing an AI able to drive a car without human intervention and a small robot to deliver packages in the city may seem like different problems, nevertheless from the point of view of perception and vision, both problems have several similarities. The main solutions we currently find focus on the environment perception through visual information using Computer Vision techniques, Machine Learning, and various algorithms to make the robot understand the environment or scene, move, adapt its trajectory and perform its tasks (maintenance, exploration, etc.) without the need for human intervention. In this work, we develop a small-scale autonomous vehicle from scratch, capable of understanding the scene using only visual information, navigating through industrial environments, detecting people and obstacles, or performing simple maintenance tasks. We review the state-of-the-art of fundamental problems and demonstrate that many methods employed at small-scale are similar to the ones employed in real Self-driving cars from companies like Tesla or Lyft. Finally, we discuss the current state of Robotics and autonomous driving and the technological and ethical limitations that we can find in this field.



### Spatio-Temporal Split Learning for Autonomous Aerial Surveillance using Urban Air Mobility (UAM) Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.11856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2111.11856v1)
- **Published**: 2021-11-15 01:39:31+00:00
- **Updated**: 2021-11-15 01:39:31+00:00
- **Authors**: Yoo Jeong Ha, Soyi Jung, Jae-Hyun Kim, Marco Levorato, Joongheon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous surveillance unmanned aerial vehicles (UAVs) are deployed to observe the streets of the city for any suspicious activities. This paper utilizes surveillance UAVs for the purpose of detecting the presence of a fire in the streets. An extensive database is collected from UAV surveillance drones. With the aid of artificial intelligence (AI), fire stations can swiftly identify the presence of a fire emerging in the neighborhood. Spatio-temporal split learning is applied to this scenario to preserve privacy and globally train a fire classification model. Fires are hazardous natural disasters that can spread very quickly. Swift identification of fire is required to deploy firefighters to the scene. In order to do this, strong communication between the UAV and the central server where the deep learning process occurs is required. Improving communication resilience is integral to enhancing a safe experience on the roads. Therefore, this paper explores the adequate number of clients and data ratios for split learning in this UAV setting, as well as the required network infrastructure.



### Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks
- **Arxiv ID**: http://arxiv.org/abs/2111.07492v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2111.07492v5)
- **Published**: 2021-11-15 01:51:37+00:00
- **Updated**: 2022-02-27 15:12:25+00:00
- **Authors**: Chen Ma, Xiangyu Guo, Li Chen, Jun-Hai Yong, Yisen Wang
- **Comment**: Accepted at NeurIPS 2021. The missing square term in Eqn.(13), as
  well as many other mistakes of the previous version, have been fixed in the
  current version
- **Journal**: None
- **Summary**: One major problem in black-box adversarial attacks is the high query complexity in the hard-label attack setting, where only the top-1 predicted label is available. In this paper, we propose a novel geometric-based approach called Tangent Attack (TA), which identifies an optimal tangent point of a virtual hemisphere located on the decision boundary to reduce the distortion of the attack. Assuming the decision boundary is locally flat, we theoretically prove that the minimum $\ell_2$ distortion can be obtained by reaching the decision boundary along the tangent line passing through such tangent point in each iteration. To improve the robustness of our method, we further propose a generalized method which replaces the hemisphere with a semi-ellipsoid to adapt to curved decision boundaries. Our approach is free of pre-training. Extensive experiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that our approach can consume only a small number of queries to achieve the low-magnitude distortion. The implementation source code is released online at https://github.com/machanic/TangentAttack.



### Reinforcement Learning of Self Enhancing Camera Image and Signal Processing
- **Arxiv ID**: http://arxiv.org/abs/2111.07499v2
- **DOI**: 10.1007/978-981-99-0981-0_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07499v2)
- **Published**: 2021-11-15 02:23:40+00:00
- **Updated**: 2022-09-11 16:15:01+00:00
- **Authors**: Chandrajit Bajaj, Yi Wang, Yunhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Current camera image and signal processing pipelines (ISPs), including deep-trained versions, tend to apply a single filter that is uniformly applied to the entire image. This is despite the fact that most acquired camera images have spatially heterogeneous artifacts. This spatial heterogeneity manifests itself across the image space as varied Moire ringing, motion-blur, color-bleaching, or lens-based projection distortions. Moreover, combinations of these image artifacts can be present in small or large pixel neighborhoods, within an acquired image. Here, we present a deep reinforcement learning model that works in learned latent subspaces, and recursively improves camera image quality through a patch-based spatially adaptive artifact filtering and image enhancement. Our \textit{Recursive Self Enhancement Reinforcement Learning}(RSE-RL) model views the identification and correction of artifacts as a recursive self-learning and self-improvement exercise and consists of two major sub-modules: (i) The latent feature sub-space clustering/grouping obtained through variational auto-encoders enabling rapid identification of the correspondence and discrepancy between noisy and clean image patches. (ii) The adaptive learned transformation is controlled by a soft actor-critic agent that progressively filters and enhances the noisy patches using its closest feature distance neighbors of clean patches. Artificial artifacts that may be introduced in a patch-based ISP, are also removed through a reward-based de-blocking recovery and image enhancement. We demonstrate the self-improvement feature of our model by recursively training and testing on images, wherein the enhanced images resulting from each epoch provide a natural data augmentation and robustness to the RSE-RL training-filtering pipeline. Our method shows advantage for heterogeneous noise and artifact removal.



### Object Propagation via Inter-Frame Attentions for Temporally Stable Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.07529v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07529v3)
- **Published**: 2021-11-15 04:15:57+00:00
- **Updated**: 2021-12-14 03:48:10+00:00
- **Authors**: Anirudh S Chakravarthy, Won-Dong Jang, Zudi Lin, Donglai Wei, Song Bai, Hanspeter Pfister
- **Comment**: Accepted at CVPR RVSU Workshop 2021
- **Journal**: None
- **Summary**: Video instance segmentation aims to detect, segment, and track objects in a video. Current approaches extend image-level segmentation algorithms to the temporal domain. However, this results in temporally inconsistent masks. In this work, we identify the mask quality due to temporal stability as a performance bottleneck. Motivated by this, we propose a video instance segmentation method that alleviates the problem due to missing detections. Since this cannot be solved simply using spatial information, we leverage temporal context using inter-frame attentions. This allows our network to refocus on missing objects using box predictions from the neighbouring frame, thereby overcoming missing detections. Our method significantly outperforms previous state-of-the-art algorithms using the Mask R-CNN backbone, by achieving 36.0% mAP on the YouTube-VIS benchmark. Additionally, our method is completely online and requires no future frames. Our code is publicly available at https://github.com/anirudh-chakravarthy/ObjProp.



### A Probabilistic Hard Attention Model For Sequentially Observed Scenes
- **Arxiv ID**: http://arxiv.org/abs/2111.07534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07534v1)
- **Published**: 2021-11-15 04:47:47+00:00
- **Updated**: 2021-11-15 04:47:47+00:00
- **Authors**: Samrudhdhi B. Rangrej, James J. Clark
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: A visual hard attention model actively selects and observes a sequence of subregions in an image to make a prediction. The majority of hard attention models determine the attention-worthy regions by first analyzing a complete image. However, it may be the case that the entire image is not available initially but instead sensed gradually through a series of partial observations. In this paper, we design an efficient hard attention model for classifying such sequentially observed scenes. The presented model never observes an image completely. To select informative regions under partial observability, the model uses Bayesian Optimal Experiment Design. First, it synthesizes the features of the unobserved regions based on the already observed regions. Then, it uses the predicted features to estimate the expected information gain (EIG) attained, should various regions be attended. Finally, the model attends to the actual content on the location where the EIG mentioned above is maximum. The model uses a) a recurrent feature aggregator to maintain a recurrent state, b) a linear classifier to predict the class label, c) a Partial variational autoencoder to predict the features of unobserved regions. We use normalizing flows in Partial VAE to handle multi-modality in the feature-synthesis problem. We train our model using a differentiable objective and test it on five datasets. Our model gains 2-10% higher accuracy than the baseline models when both have seen only a couple of glimpses.



### T-AutoML: Automated Machine Learning for Lesion Segmentation using Transformers in 3D Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2111.07535v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07535v1)
- **Published**: 2021-11-15 04:57:53+00:00
- **Updated**: 2021-11-15 04:57:53+00:00
- **Authors**: Dong Yang, Andriy Myronenko, Xiaosong Wang, Ziyue Xu, Holger R. Roth, Daguang Xu
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: Lesion segmentation in medical imaging has been an important topic in clinical research. Researchers have proposed various detection and segmentation algorithms to address this task. Recently, deep learning-based approaches have significantly improved the performance over conventional methods. However, most state-of-the-art deep learning methods require the manual design of multiple network components and training strategies. In this paper, we propose a new automated machine learning algorithm, T-AutoML, which not only searches for the best neural architecture, but also finds the best combination of hyper-parameters and data augmentation strategies simultaneously. The proposed method utilizes the modern transformer model, which is introduced to adapt to the dynamic length of the search space embedding and can significantly improve the ability of the search. We validate T-AutoML on several large-scale public lesion segmentation data-sets and achieve state-of-the-art performance.



### Solving Inverse Problems in Medical Imaging with Score-Based Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2111.08005v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.08005v2)
- **Published**: 2021-11-15 05:41:12+00:00
- **Updated**: 2022-06-16 00:01:39+00:00
- **Authors**: Yang Song, Liyue Shen, Lei Xing, Stefano Ermon
- **Comment**: Published at ICLR 2022
- **Journal**: None
- **Summary**: Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.



### Searching for TrioNet: Combining Convolution with Local and Global Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.07547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07547v1)
- **Published**: 2021-11-15 05:45:10+00:00
- **Updated**: 2021-11-15 05:45:10+00:00
- **Authors**: Huaijin Pi, Huiyu Wang, Yingwei Li, Zizhang Li, Alan Yuille
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Recently, self-attention operators have shown superior performance as a stand-alone building block for vision models. However, existing self-attention models are often hand-designed, modified from CNNs, and obtained by stacking one operator only. A wider range of architecture space which combines different self-attention operators and convolution is rarely explored. In this paper, we explore this novel architecture space with weight-sharing Neural Architecture Search (NAS) algorithms. The result architecture is named TrioNet for combining convolution, local self-attention, and global (axial) self-attention operators. In order to effectively search in this huge architecture space, we propose Hierarchical Sampling for better training of the supernet. In addition, we propose a novel weight-sharing strategy, Multi-head Sharing, specifically for multi-head self-attention operators. Our searched TrioNet that combines self-attention and convolution outperforms all stand-alone models with fewer FLOPs on ImageNet classification where self-attention performs better than convolution. Furthermore, on various small datasets, we observe inferior performance for self-attention models, but our TrioNet is still able to match the best operator, convolution in this case. Our code is available at https://github.com/phj128/TrioNet.



### Unsupervised Lightweight Single Object Tracking with UHP-SOT++
- **Arxiv ID**: http://arxiv.org/abs/2111.07548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07548v2)
- **Published**: 2021-11-15 05:52:47+00:00
- **Updated**: 2022-04-07 00:28:51+00:00
- **Authors**: Zhiruo Zhou, Hongyu Fu, Suya You, C. -C. Jay Kuo
- **Comment**: updated content: comparison with state-of-the-art deep unsupervised
  methods
- **Journal**: None
- **Summary**: An unsupervised, lightweight and high-performance single object tracker, called UHP-SOT, was proposed by Zhou et al. recently. As an extension, we present an enhanced version and name it UHP-SOT++ in this work. Built upon the foundation of the discriminative-correlation-filters-based (DCF-based) tracker, two new ingredients are introduced in UHP-SOT and UHP-SOT++: 1) background motion modeling and 2) object box trajectory modeling. The main difference between UHP-SOT and UHP-SOT++ is the fusion strategy of proposals from three models (i.e., DCF, background motion and object box trajectory models). An improved fusion strategy is adopted by UHP-SOT++ for more robust tracking performance against large-scale tracking datasets. Our second contribution lies in an extensive evaluation of the performance of state-of-the-art supervised and unsupervised methods by testing them on four SOT benchmark datasets - OTB2015, TC128, UAV123 and LaSOT. Experiments show that UHP-SOT++ outperforms all previous unsupervised methods and several deep-learning (DL) methods in tracking accuracy. Since UHP-SOT++ has extremely small model size, high tracking performance, and low computational complexity (operating at a rate of 20 FPS on an i5 CPU even without code optimization), it is an ideal solution in real-time object tracking on resource-limited platforms. Based on the experimental results, we compare pros and cons of supervised and unsupervised trackers and provide a new perspective to understand the performance gap between supervised and unsupervised methods, which is the third contribution of this work.



### High-Quality Real Time Facial Capture Based on Single Camera
- **Arxiv ID**: http://arxiv.org/abs/2111.07556v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07556v1)
- **Published**: 2021-11-15 06:42:27+00:00
- **Updated**: 2021-11-15 06:42:27+00:00
- **Authors**: Hongwei Xu, Leijia Dai, Jianxing Fu, Xiangyuan Wang, Quanwei Wang
- **Comment**: arXiv admin note: text overlap with arXiv:1609.06536 by other authors
- **Journal**: None
- **Summary**: We propose a real time deep learning framework for video-based facial expression capture. Our process uses a high-end facial capture pipeline based on FACEGOOD to capture facial expression. We train a convolutional neural network to produce high-quality continuous blendshape weight output from video training. Since this facial capture is fully automated, our system can drastically reduce the amount of labor involved in the development of modern narrative-driven video games or films involving realistic digital doubles of actors and potentially hours of animated dialogue per character. We demonstrate compelling animation inference in challenging areas such as eyes and lips.



### Disparities in Dermatology AI: Assessments Using Diverse Clinical Images
- **Arxiv ID**: http://arxiv.org/abs/2111.08006v1
- **DOI**: 10.1126/sciadv.abq6147
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08006v1)
- **Published**: 2021-11-15 07:04:58+00:00
- **Updated**: 2021-11-15 07:04:58+00:00
- **Authors**: Roxana Daneshjou, Kailas Vodrahalli, Weixin Liang, Roberto A Novoa, Melissa Jenkins, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, Pritam Mukherjee, Michelle Phung, Kiana Yekrang, Bradley Fong, Rachna Sahasrabudhe, James Zou, Albert Chiou
- **Comment**: Machine Learning for Health (ML4H) - Extended Abstract
- **Journal**: None
- **Summary**: More than 3 billion people lack access to care for skin disease. AI diagnostic tools may aid in early skin cancer detection; however most models have not been assessed on images of diverse skin tones or uncommon diseases. To address this, we curated the Diverse Dermatology Images (DDI) dataset - the first publicly available, pathologically confirmed images featuring diverse skin tones. We show that state-of-the-art dermatology AI models perform substantially worse on DDI, with ROC-AUC dropping 29-40 percent compared to the models' original results. We find that dark skin tones and uncommon diseases, which are well represented in the DDI dataset, lead to performance drop-offs. Additionally, we show that state-of-the-art robust training methods cannot correct for these biases without diverse training data. Our findings identify important weaknesses and biases in dermatology AI that need to be addressed to ensure reliable application to diverse patients and across all disease.



### Weakly-Supervised Dense Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2111.07593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07593v1)
- **Published**: 2021-11-15 08:13:26+00:00
- **Updated**: 2021-11-15 08:13:26+00:00
- **Authors**: Haotong Zhang, Fuhai Chen, Angela Yao
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Dense anticipation aims to forecast future actions and their durations for long horizons. Existing approaches rely on fully-labelled data, i.e. sequences labelled with all future actions and their durations. We present a (semi-) weakly supervised method using only a small number of fully-labelled sequences and predominantly sequences in which only the (one) upcoming action is labelled. To this end, we propose a framework that generates pseudo-labels for future actions and their durations and adaptively refines them through a refinement module. Given only the upcoming action label as input, these pseudo-labels guide action/duration prediction for the future. We further design an attention mechanism to predict context-aware durations. Experiments on the Breakfast and 50Salads benchmarks verify our method's effectiveness; we are competitive even when compared to fully supervised state-of-the-art models. We will make our code available at: https://github.com/zhanghaotong1/WSLVideoDenseAnticipation.



### DFC: Deep Feature Consistency for Robust Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2111.07597v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07597v3)
- **Published**: 2021-11-15 08:27:21+00:00
- **Updated**: 2021-12-13 08:32:01+00:00
- **Authors**: Zhu Xu, Zhengyao Bai, Huijie Liu, Qianjie Lu, Shenglan Fan
- **Comment**: 12 pages, 7 figures, 6 tables
- **Journal**: None
- **Summary**: How to extract significant point cloud features and estimate the pose between them remains a challenging question, due to the inherent lack of structure and ambiguous order permutation of point clouds. Despite significant improvements in applying deep learning-based methods for most 3D computer vision tasks, such as object classification, object segmentation and point cloud registration, the consistency between features is still not attractive in existing learning-based pipelines. In this paper, we present a novel learning-based alignment network for complex alignment scenes, titled deep feature consistency and consisting of three main modules: a multiscale graph feature merging network for converting the geometric correspondence set into high-dimensional features, a correspondence weighting module for constructing multiple candidate inlier subsets, and a Procrustes approach named deep feature matching for giving a closed-form solution to estimate the relative pose. As the most important step of the deep feature matching module, the feature consistency matrix for each inlier subset is constructed to obtain its principal vectors as the inlier likelihoods of the corresponding subset. We comprehensively validate the robustness and effectiveness of our approach on both the 3DMatch dataset and the KITTI odometry dataset. For large indoor scenes, registration results on the 3DMatch dataset demonstrate that our method outperforms both the state-of-the-art traditional and learning-based methods. For KITTI outdoor scenes, our approach remains quite capable of lowering the transformation errors. We also explore its strong generalization capability over cross-datasets.



### FakeTransformer: Exposing Face Forgery From Spatial-Temporal Representation Modeled By Facial Pixel Variations
- **Arxiv ID**: http://arxiv.org/abs/2111.07601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07601v1)
- **Published**: 2021-11-15 08:44:52+00:00
- **Updated**: 2021-11-15 08:44:52+00:00
- **Authors**: Yuyang Sun, Zhiyong Zhang, Changzhen Qiu, Liang Wang, Zekai Wang
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of generation model, AI-based face manipulation technology, which called DeepFakes, has become more and more realistic. This means of face forgery can attack any target, which poses a new threat to personal privacy and property security. Moreover, the misuse of synthetic video shows potential dangers in many areas, such as identity harassment, pornography and news rumors. Inspired by the fact that the spatial coherence and temporal consistency of physiological signal are destroyed in the generated content, we attempt to find inconsistent patterns that can distinguish between real videos and synthetic videos from the variations of facial pixels, which are highly related to physiological information. Our approach first applies Eulerian Video Magnification (EVM) at multiple Gaussian scales to the original video to enlarge the physiological variations caused by the change of facial blood volume, and then transform the original video and magnified videos into a Multi-Scale Eulerian Magnified Spatial-Temporal map (MEMSTmap), which can represent time-varying physiological enhancement sequences on different octaves. Then, these maps are reshaped into frame patches in column units and sent to the vision Transformer to learn the spatio-time descriptors of frame levels. Finally, we sort out the feature embedding and output the probability of judging whether the video is real or fake. We validate our method on the FaceForensics++ and DeepFake Detection datasets. The results show that our model achieves excellent performance in forgery detection, and also show outstanding generalization capability in cross-data domain.



### Fingerprint Presentation Attack Detection by Channel-wise Feature Denoising
- **Arxiv ID**: http://arxiv.org/abs/2111.07620v2
- **DOI**: 10.1109/TIFS.2022.3197058
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07620v2)
- **Published**: 2021-11-15 09:13:21+00:00
- **Updated**: 2023-03-07 06:37:09+00:00
- **Authors**: Feng Liu, Zhe Kong, Haozhe Liu, Wentian Zhang, Linlin Shen
- **Comment**: 15 pages, 8 figures, Accepted by TIFS
- **Journal**: IEEE Transactions on Information Forensics and Security, vol. 17,
  pp. 2963-2976, 2022
- **Summary**: Due to the diversity of attack materials, fingerprint recognition systems (AFRSs) are vulnerable to malicious attacks. It is thus important to propose effective fingerprint presentation attack detection (PAD) methods for the safety and reliability of AFRSs. However, current PAD methods often exhibit poor robustness under new attack types settings. This paper thus proposes a novel channel-wise feature denoising fingerprint PAD (CFD-PAD) method by handling the redundant noise information ignored in previous studies. The proposed method learns important features of fingerprint images by weighing the importance of each channel and identifying discriminative channels and "noise" channels. Then, the propagation of "noise" channels is suppressed in the feature map to reduce interference. Specifically, a PA-Adaptation loss is designed to constrain the feature distribution to make the feature distribution of live fingerprints more aggregate and that of spoof fingerprints more disperse. Experimental results evaluated on the LivDet 2017 dataset showed that the proposed CFD-PAD can achieve a 2.53% average classification error (ACE) and a 93.83% true detection rate when the false detection rate equals 1.0% (TDR@FDR=1%). Also, the proposed method markedly outperforms the best single-model-based methods in terms of ACE (2.53% vs. 4.56%) and TDR@FDR=1%(93.83% vs. 73.32%), which demonstrates its effectiveness. Although we have achieved a comparable result with the state-of-the-art multiple-model-based methods, there still is an increase in TDR@FDR=1% from 91.19% to 93.83%. In addition, the proposed model is simpler, lighter and more efficient and has achieved a 74.76% reduction in computation time compared with the state-of-the-art multiple-model-based method. The source code is available at https://github.com/kongzhecn/cfd-pad.



### Attention Mechanisms in Computer Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2111.07624v1
- **DOI**: 10.1007/s41095-022-0271-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07624v1)
- **Published**: 2021-11-15 09:18:40+00:00
- **Updated**: 2021-11-15 09:18:40+00:00
- **Authors**: Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R. Martin, Ming-Ming Cheng, Shi-Min Hu
- **Comment**: 27 pages, 9 figures
- **Journal**: Computational Visual Media, 2022, Vol. 8, No. 3, 331-368
- **Summary**: Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video understanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention and branch attention; a related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research.



### On the validation of pansharpening methods
- **Arxiv ID**: http://arxiv.org/abs/2111.07625v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07625v1)
- **Published**: 2021-11-15 09:18:44+00:00
- **Updated**: 2021-11-15 09:18:44+00:00
- **Authors**: Gintautas Palubinskas
- **Comment**: 18 pages, 5 figures, 6 tables. arXiv admin note: substantial text
  overlap with arXiv:2103.03062
- **Journal**: None
- **Summary**: Validation of the quality of pansharpening methods is a difficult task because the reference is not directly available. In the meantime, two main approaches have been established: validation in reduced resolution and original resolution. In the former approach it is still not clear how the data are to be processed to a lower resolution. Other open issues are related to the question which resolution and measures should be used. In the latter approach the main problem is how the appropriate measure should be selected. In the most comparison studies the results of both approaches do not correspond, that means in each case other methods are selected as the best ones. Thus, the developers of the new pansharpening methods still stand in the front of dilemma: how to perform a correct or appropriate comparison/evaluation/validation. It should be noted, that the third approach is possible, that is to perform the comparison of methods in a particular application with the usage of their ground truth. But this is not always possible, because usually developers are not working with applications. Moreover, it can be an additional computational load for a researcher in a particular application. In this paper some of the questions/problems raised above are approached/discussed. The following component substitution (CS) and high pass filtering (HPF) pansharpening methods with additive and multiplicative models and their enhancements such as haze correction, histogram matching, usage of spectral response functions (SRF), modulation transfer function (MTF) based lowpass filtering are investigated on remote sensing data of WorldView-2 and WorldView-4 sensors.



### CoReS: Compatible Representations via Stationarity
- **Arxiv ID**: http://arxiv.org/abs/2111.07632v3
- **DOI**: 10.1109/TPAMI.2023.3259542
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07632v3)
- **Published**: 2021-11-15 09:35:54+00:00
- **Updated**: 2023-03-28 13:06:17+00:00
- **Authors**: Niccolo Biondi, Federico Pernici, Matteo Bruni, Alberto Del Bimbo
- **Comment**: in IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2023. Code: https://github.com/NiccoBiondi/cores-compatibility
- **Journal**: None
- **Summary**: Compatible features enable the direct comparison of old and new learned features allowing to use them interchangeably over time. In visual search systems, this eliminates the need to extract new features from the gallery-set when the representation model is upgraded with novel data. This has a big value in real applications as re-indexing the gallery-set can be computationally expensive when the gallery-set is large, or even infeasible due to privacy or other concerns of the application. In this paper, we propose CoReS, a new training procedure to learn representations that are \textit{compatible} with those previously learned, grounding on the stationarity of the features as provided by fixed classifiers based on polytopes. With this solution, classes are maximally separated in the representation space and maintain their spatial configuration stationary as new classes are added, so that there is no need to learn any mappings between representations nor to impose pairwise training with the previously learned model. We demonstrate that our training procedure largely outperforms the current state of the art and is particularly effective in the case of multiple upgrades of the training-set, which is the typical case in real applications.



### Pseudo-domains in imaging data improve prediction of future disease status in multi-center studies
- **Arxiv ID**: http://arxiv.org/abs/2111.07634v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07634v1)
- **Published**: 2021-11-15 09:40:54+00:00
- **Updated**: 2021-11-15 09:40:54+00:00
- **Authors**: Matthias Perkonigg, Peter Mesenbrink, Alexander Goehler, Miljen Martic, Ahmed Ba-Ssalamah, Georg Langs
- **Comment**: Accepted at Medical Imaging Meets NeurIPS 2021
- **Journal**: None
- **Summary**: In multi-center randomized clinical trials imaging data can be diverse due to acquisition technology or scanning protocols. Models predicting future outcome of patients are impaired by this data heterogeneity. Here, we propose a prediction method that can cope with a high number of different scanning sites and a low number of samples per site. We cluster sites into pseudo-domains based on visual appearance of scans, and train pseudo-domain specific models. Results show that they improve the prediction accuracy for steatosis after 48 weeks from imaging data acquired at an initial visit and 12-weeks follow-up in liver disease



### AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2111.07640v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07640v2)
- **Published**: 2021-11-15 10:00:06+00:00
- **Updated**: 2022-07-21 07:49:29+00:00
- **Authors**: Kangyeol Kim, Sunghyun Park, Jaeseong Lee, Sunghyo Chung, Junsoo Lee, Jaegul Choo
- **Comment**: 40 pages; Accepted to ECCV 2022; code and dataset URL added
- **Journal**: None
- **Summary**: We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an animation head reenactment. Different from previous animation head datasets, we utilize 3D animation models as the controllable image samplers, which can provide a large amount of head images with their corresponding detailed pose annotations. To facilitate a data creation process, we build a semi-automatic pipeline leveraging an open 3D computer graphics software with a developed annotation system. After training with the AnimeCeleb, recent head reenactment models produce high-quality animation head reenactment results, which are not achievable with existing datasets. Furthermore, motivated by metaverse application, we propose a novel pose mapping method and architecture to tackle a cross-domain head reenactment task. During inference, a user can easily transfer one's motion to an arbitrary animation head. Experiments demonstrate the usefulness of the AnimeCeleb to train animation head reenactment models, and the superiority of our cross-domain head reenactment model compared to state-of-the-art methods. Our dataset and code are available at https://github.com/kangyeolk/AnimeCeleb.



### Multimodal Generalized Zero Shot Learning for Gleason Grading using Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.07646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07646v1)
- **Published**: 2021-11-15 10:14:11+00:00
- **Updated**: 2021-11-15 10:14:11+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: Gleason grading from histopathology images is essential for accurate prostate cancer (PCa) diagnosis. Since such images are obtained after invasive tissue resection quick diagnosis is challenging under the existing paradigm. We propose a method to predict Gleason grades from magnetic resonance (MR) images which are non-interventional and easily acquired. We solve the problem in a generalized zero-shot learning (GZSL) setting since we may not access training images of every disease grade. Synthetic MRI feature vectors of unseen grades (classes) are generated by exploiting Gleason grades' ordered nature through a conditional variational autoencoder (CVAE) incorporating self-supervised learning. Corresponding histopathology features are generated using cycle GANs, and combined with MR features to predict Gleason grades of test images. Experimental results show our method outperforms competing feature generating approaches for GZSL, and comes close to performance of fully supervised methods.



### Fast Axiomatic Attribution for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.07668v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07668v1)
- **Published**: 2021-11-15 10:51:01+00:00
- **Updated**: 2021-11-15 10:51:01+00:00
- **Authors**: Robin Hesse, Simone Schaub-Meyer, Stefan Roth
- **Comment**: To appear at NeurIPS*2021. Project page and code:
  https://visinf.github.io/fast-axiomatic-attribution
- **Journal**: None
- **Summary**: Mitigating the dependence on spurious correlations present in the training dataset is a quickly emerging and important topic of deep learning. Recent approaches include priors on the feature attribution of a deep neural network (DNN) into the training process to reduce the dependence on unwanted features. However, until now one needed to trade off high-quality attributions, satisfying desirable axioms, against the time required to compute them. This in turn either led to long training times or ineffective attribution priors. In this work, we break this trade-off by considering a special class of efficiently axiomatically attributable DNNs for which an axiomatic feature attribution can be computed with only a single forward/backward pass. We formally prove that nonnegatively homogeneous DNNs, here termed $\mathcal{X}$-DNNs, are efficiently axiomatically attributable and show that they can be effortlessly constructed from a wide range of regular DNNs by simply removing the bias term of each layer. Various experiments demonstrate the advantages of $\mathcal{X}$-DNNs, beating state-of-the-art generic attribution methods on regular DNNs for training with attribution priors.



### FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2111.07677v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07677v2)
- **Published**: 2021-11-15 11:15:02+00:00
- **Updated**: 2021-11-16 06:33:39+00:00
- **Authors**: Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, Liwei Wu
- **Comment**: 11 pages,8 figures
- **Journal**: None
- **Summary**: Unsupervised anomaly detection and localization is crucial to the practical application when collecting and labeling sufficient anomaly data is infeasible. Most existing representation-based approaches extract normal image features with a deep convolutional neural network and characterize the corresponding distribution through non-parametric distribution estimation methods. The anomaly score is calculated by measuring the distance between the feature of the test image and the estimated distribution. However, current methods can not effectively map image features to a tractable base distribution and ignore the relationship between local and global features which are important to identify anomalies. To this end, we propose FastFlow implemented with 2D normalizing flows and use it as the probability distribution estimator. Our FastFlow can be used as a plug-in module with arbitrary deep feature extractors such as ResNet and vision transformer for unsupervised anomaly detection and localization. In training phase, FastFlow learns to transform the input visual feature into a tractable distribution and obtains the likelihood to recognize anomalies in inference phase. Extensive experimental results on the MVTec AD dataset show that FastFlow surpasses previous state-of-the-art methods in terms of accuracy and inference efficiency with various backbone networks. Our approach achieves 99.4% AUC in anomaly detection with high inference efficiency.



### 2nd Place Solution to Facebook AI Image Similarity Challenge Matching Track
- **Arxiv ID**: http://arxiv.org/abs/2111.09113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09113v1)
- **Published**: 2021-11-15 11:51:36+00:00
- **Updated**: 2021-11-15 11:51:36+00:00
- **Authors**: SeungKee Jeon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the 2nd place solution to the Facebook AI Image Similarity Challenge : Matching Track on DrivenData. The solution is based on self-supervised learning, and Vision Transformer(ViT). The main breaktrough comes from concatenating query and reference image to form as one image and asking ViT to directly predict from the image if query image used reference image. The solution scored 0.8291 Micro-average Precision on the private leaderboard.



### Casting graph isomorphism as a point set registration problem using a simplex embedding and sampling
- **Arxiv ID**: http://arxiv.org/abs/2111.09696v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09696v1)
- **Published**: 2021-11-15 12:16:21+00:00
- **Updated**: 2021-11-15 12:16:21+00:00
- **Authors**: Yigit Oktar
- **Comment**: None
- **Journal**: None
- **Summary**: Graph isomorphism is an important problem as its worst-case time complexity is not yet fully understood. In this study, we try to draw parallels between a related optimization problem called point set registration. A graph can be represented as a point set in enough dimensions using a simplex embedding and sampling. Given two graphs, the isomorphism of them corresponds to the existence of a perfect registration between the point set forms of the graphs. In the case of non-isomorphism, the point set form optimization result can be used as a distance measure between two graphs having the same number of vertices and edges. The related idea of equivalence classes suggests that graph canonization may be an important tool in tackling graph isomorphism problem and an orthogonal transformation invariant feature extraction based on this high dimensional point set representation may be fruitful. The concepts presented can also be extended to automorphism, and subgraph isomorphism problems and can also be applied on hypergraphs with certain modifications.



### Interactive Medical Image Segmentation with Self-Adaptive Confidence Calibration
- **Arxiv ID**: http://arxiv.org/abs/2111.07716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07716v1)
- **Published**: 2021-11-15 12:38:56+00:00
- **Updated**: 2021-11-15 12:38:56+00:00
- **Authors**: Wenhao Li, Qisen Xu, Chuyun Shen, Bin Hu, Fengping Zhu, Yuxin Li, Bo Jin, Xiangfeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation is one of the fundamental problems for artificial intelligence-based clinical decision systems. Current automatic medical image segmentation methods are often failed to meet clinical requirements. As such, a series of interactive segmentation algorithms are proposed to utilize expert correction information. However, existing methods suffer from some segmentation refining failure problems after long-term interactions and some cost problems from expert annotation, which hinder clinical applications. This paper proposes an interactive segmentation framework, called interactive MEdical segmentation with self-adaptive Confidence CAlibration (MECCA), by introducing the corrective action evaluation, which combines the action-based confidence learning and multi-agent reinforcement learning (MARL). The evaluation is established through a novel action-based confidence network, and the corrective actions are obtained from MARL. Based on the confidential information, a self-adaptive reward function is designed to provide more detailed feedback, and a simulated label generation mechanism is proposed on unsupervised data to reduce over-reliance on labeled data. Experimental results on various medical image datasets have shown the significant performance of the proposed algorithm.



### Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2111.07722v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07722v4)
- **Published**: 2021-11-15 12:49:27+00:00
- **Updated**: 2022-08-01 14:23:49+00:00
- **Authors**: Zixiang Ding, Yaran Chen, Nannan Li, Dongbin Zhao, C. L. Philip Chen
- **Comment**: 12 pages, 10 figures, 5 tables
- **Journal**: None
- **Summary**: Different from other deep scalable architecture-based NAS approaches, Broad Neural Architecture Search (BNAS) proposes a broad scalable architecture which consists of convolution and enhancement blocks, dubbed Broad Convolutional Neural Network (BCNN), as the search space for amazing efficiency improvement. BCNN reuses the topologies of cells in the convolution block so that BNAS can employ few cells for efficient search. Moreover, multi-scale feature fusion and knowledge embedding are proposed to improve the performance of BCNN with shallow topology. However, BNAS suffers some drawbacks: 1) insufficient representation diversity for feature fusion and enhancement and 2) time consumption of knowledge embedding design by human experts. This paper proposes Stacked BNAS, whose search space is a developed broad scalable architecture named Stacked BCNN, with better performance than BNAS. On the one hand, Stacked BCNN treats mini BCNN as a basic block to preserve comprehensive representation and deliver powerful feature extraction ability. For multi-scale feature enhancement, each mini BCNN feeds the outputs of deep and broad cells to the enhancement cell. For multi-scale feature fusion, each mini BCNN feeds the outputs of deep, broad and enhancement cells to the output node. On the other hand, Knowledge Embedding Search (KES) is proposed to learn appropriate knowledge embeddings in a differentiable way. Moreover, the basic unit of KES is an over-parameterized knowledge embedding module that consists of all possible candidate knowledge embeddings. Experimental results show that 1) Stacked BNAS obtains better performance than BNAS-v2 on both CIFAR-10 and ImageNet, 2) the proposed KES algorithm contributes to reducing the parameters of the learned architecture with satisfactory performance, and 3) Stacked BNAS delivers a state-of-the-art efficiency of 0.02 GPU days.



### Progress in Self-Certified Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.07737v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07737v3)
- **Published**: 2021-11-15 13:39:44+00:00
- **Updated**: 2021-12-10 11:28:10+00:00
- **Authors**: Maria Perez-Ortiz, Omar Rivasplata, Emilio Parrado-Hernandez, Benjamin Guedj, John Shawe-Taylor
- **Comment**: None
- **Journal**: Published at NeurIPS 2021 workshop: Bayesian Deep Learning
- **Summary**: A learning method is self-certified if it uses all available data to simultaneously learn a predictor and certify its quality with a tight statistical certificate that is valid on unseen data. Recent work has shown that neural network models trained by optimising PAC-Bayes bounds lead not only to accurate predictors, but also to tight risk certificates, bearing promise towards achieving self-certified learning. In this context, learning and certification strategies based on PAC-Bayes bounds are especially attractive due to their ability to leverage all data to learn a posterior and simultaneously certify its risk with a tight numerical certificate. In this paper, we assess the progress towards self-certification in probabilistic neural networks learnt by PAC-Bayes inspired objectives. We empirically compare (on 4 classification datasets) classical test set bounds for deterministic predictors and a PAC-Bayes bound for randomised self-certified predictors. We first show that both of these generalisation bounds are not too far from out-of-sample test set errors. We then show that in data starvation regimes, holding out data for the test set bounds adversely affects generalisation performance, while self-certified strategies based on PAC-Bayes bounds do not suffer from this drawback, proving that they might be a suitable choice for the small data regime. We also find that probabilistic neural networks learnt by PAC-Bayes inspired objectives lead to certificates that can be surprisingly competitive with commonly used test set bounds.



### Real-time Emotion and Gender Classification using Ensemble CNN
- **Arxiv ID**: http://arxiv.org/abs/2111.07746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07746v1)
- **Published**: 2021-11-15 13:51:35+00:00
- **Updated**: 2021-11-15 13:51:35+00:00
- **Authors**: Abhinav Lahariya, Varsha Singh, Uma Shanker Tiwary
- **Comment**: None
- **Journal**: None
- **Summary**: Analysing expressions on the person's face plays a very vital role in identifying emotions and behavior of a person. Recognizing these expressions automatically results in a crucial component of natural human-machine interfaces. Therefore research in this field has a wide range of applications in bio-metric authentication, surveillance systems , emotion to emoticons in various social media platforms. Another application includes conducting customer satisfaction surveys. As we know that the large corporations made huge investments to get feedback and do surveys but fail to get equitable responses. Emotion & Gender recognition through facial gestures is a technology that aims to improve product and services performance by monitoring customer behavior to specific products or service staff by their evaluation. In the past few years there have been a wide variety of advances performed in terms of feature extraction mechanisms , detection of face and also expression classification techniques. This paper is the implementation of an Ensemble CNN for building a real-time system that can detect emotion and gender of the person. The experimental results shows accuracy of 68% for Emotion classification into 7 classes (angry, fear , sad , happy , surprise , neutral , disgust) on FER-2013 dataset and 95% for Gender classification (Male or Female) on IMDB dataset. Our work can predict emotion and gender on single face images as well as multiple face images. Also when input is given through webcam our complete pipeline of this real-time system can take less than 0.5 seconds to generate results.



### D^2Conv3D: Dynamic Dilated Convolutions for Object Segmentation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.07774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07774v1)
- **Published**: 2021-11-15 14:15:28+00:00
- **Updated**: 2021-11-15 14:15:28+00:00
- **Authors**: Christian Schmidt, Ali Athar, Sabarinath Mahadevan, Bastian Leibe
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Despite receiving significant attention from the research community, the task of segmenting and tracking objects in monocular videos still has much room for improvement. Existing works have simultaneously justified the efficacy of dilated and deformable convolutions for various image-level segmentation tasks. This gives reason to believe that 3D extensions of such convolutions should also yield performance improvements for video-level segmentation tasks. However, this aspect has not yet been explored thoroughly in existing literature. In this paper, we propose Dynamic Dilated Convolutions (D^2Conv3D): a novel type of convolution which draws inspiration from dilated and deformable convolutions and extends them to the 3D (spatio-temporal) domain. We experimentally show that D^2Conv3D can be used to improve the performance of multiple 3D CNN architectures across multiple video segmentation related benchmarks by simply employing D^2Conv3D as a drop-in replacement for standard convolutions. We further show that D^2Conv3D out-performs trivial extensions of existing dilated and deformable convolutions to 3D. Lastly, we set a new state-of-the-art on the DAVIS 2016 Unsupervised Video Object Segmentation benchmark. Code is made publicly available at https://github.com/Schmiddo/d2conv3d .



### Learning Representations for Pixel-based Control: What Matters and Why?
- **Arxiv ID**: http://arxiv.org/abs/2111.07775v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07775v1)
- **Published**: 2021-11-15 14:16:28+00:00
- **Updated**: 2021-11-15 14:16:28+00:00
- **Authors**: Manan Tomar, Utkarsh A. Mishra, Amy Zhang, Matthew E. Taylor
- **Comment**: None
- **Journal**: None
- **Summary**: Learning representations for pixel-based control has garnered significant attention recently in reinforcement learning. A wide range of methods have been proposed to enable efficient learning, leading to sample complexities similar to those in the full state setting. However, moving beyond carefully curated pixel data sets (centered crop, appropriate lighting, clear background, etc.) remains challenging. In this paper, we adopt a more difficult setting, incorporating background distractors, as a first step towards addressing this challenge. We present a simple baseline approach that can learn meaningful representations with no metric-based learning, no data augmentations, no world-model learning, and no contrastive learning. We then analyze when and why previously proposed methods are likely to fail or reduce to the same performance as the baseline in this harder setting and why we should think carefully about extending such methods beyond the well curated environments. Our results show that finer categorization of benchmarks on the basis of characteristics like density of reward, planning horizon of the problem, presence of task-irrelevant components, etc., is crucial in evaluating algorithms. Based on these observations, we propose different metrics to consider when evaluating an algorithm on benchmark tasks. We hope such a data-centric view can motivate researchers to rethink representation learning when investigating how to best apply RL to real-world tasks.



### Spiking CapsNet: A Spiking Neural Network With A Biologically Plausible Routing Rule Between Capsules
- **Arxiv ID**: http://arxiv.org/abs/2111.07785v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07785v1)
- **Published**: 2021-11-15 14:23:15+00:00
- **Updated**: 2021-11-15 14:23:15+00:00
- **Authors**: Dongcheng Zhao, Yang Li, Yi Zeng, Jihang Wang, Qian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural network (SNN) has attracted much attention due to their powerful spatio-temporal information representation ability. Capsule Neural Network (CapsNet) does well in assembling and coupling features at different levels. Here, we propose Spiking CapsNet by introducing the capsules into the modelling of spiking neural networks. In addition, we propose a more biologically plausible Spike Timing Dependent Plasticity routing mechanism. By fully considering the spatio-temporal relationship between the low-level spiking capsules and the high-level spiking capsules, the coupling ability between them is further improved. We have verified experiments on the MNIST and FashionMNIST datasets. Compared with other excellent SNN models, our algorithm still achieves high performance. Our Spiking CapsNet fully combines the strengthens of SNN and CapsNet, and shows strong robustness to noise and affine transformation. By adding different Salt-Pepper and Gaussian noise to the test dataset, the experimental results demonstrate that our Spiking CapsNet shows a more robust performance when there is more noise, while the artificial neural network can not correctly clarify. As well, our Spiking CapsNet shows strong generalization to affine transformation on the AffNIST dataset.



### iBOT: Image BERT Pre-Training with Online Tokenizer
- **Arxiv ID**: http://arxiv.org/abs/2111.07832v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07832v3)
- **Published**: 2021-11-15 15:18:05+00:00
- **Updated**: 2022-01-27 09:20:49+00:00
- **Authors**: Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, Tao Kong
- **Comment**: None
- **Journal**: None
- **Summary**: The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, eg., object detection, instance segmentation, and semantic segmentation.



### Multi-View Motion Synthesis via Applying Rotated Dual-Pixel Blur Kernels
- **Arxiv ID**: http://arxiv.org/abs/2111.07837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07837v1)
- **Published**: 2021-11-15 15:23:55+00:00
- **Updated**: 2021-11-15 15:23:55+00:00
- **Authors**: Abdullah Abuolaim, Mahmoud Afifi, Michael S. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Portrait mode is widely available on smartphone cameras to provide an enhanced photographic experience. One of the primary effects applied to images captured in portrait mode is a synthetic shallow depth of field (DoF). The synthetic DoF (or bokeh effect) selectively blurs regions in the image to emulate the effect of using a large lens with a wide aperture. In addition, many applications now incorporate a new image motion attribute (NIMAT) to emulate background motion, where the motion is correlated with estimated depth at each pixel. In this work, we follow the trend of rendering the NIMAT effect by introducing a modification on the blur synthesis procedure in portrait mode. In particular, our modification enables a high-quality synthesis of multi-view bokeh from a single image by applying rotated blurring kernels. Given the synthesized multiple views, we can generate aesthetically realistic image motion similar to the NIMAT effect. We validate our approach qualitatively compared to the original NIMAT effect and other similar image motions, like Facebook 3D image. Our image motion demonstrates a smooth image view transition with fewer artifacts around the object boundary.



### Learnable Locality-Sensitive Hashing for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.07839v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07839v2)
- **Published**: 2021-11-15 15:25:45+00:00
- **Updated**: 2021-11-16 02:27:04+00:00
- **Authors**: Yue Lu, Congqi Cao, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection (VAD) mainly refers to identifying anomalous events that have not occurred in the training set where only normal samples are available. Existing works usually formulate VAD as a reconstruction or prediction problem. However, the adaptability and scalability of these methods are limited. In this paper, we propose a novel distance-based VAD method to take advantage of all the available normal data efficiently and flexibly. In our method, the smaller the distance between a testing sample and normal samples, the higher the probability that the testing sample is normal. Specifically, we propose to use locality-sensitive hashing (LSH) to map samples whose similarity exceeds a certain threshold into the same bucket in advance. In this manner, the complexity of near neighbor search is cut down significantly. To make the samples that are semantically similar get closer and samples not similar get further apart, we propose a novel learnable version of LSH that embeds LSH into a neural network and optimizes the hash functions with contrastive learning strategy. The proposed method is robust to data imbalance and can handle the large intra-class variations in normal data flexibly. Besides, it has a good ability of scalability. Extensive experiments demonstrate the superiority of our method, which achieves new state-of-the-art results on VAD benchmarks.



### Multi-Task Classification of Sewer Pipe Defects and Properties using a Cross-Task Graph Neural Network Decoder
- **Arxiv ID**: http://arxiv.org/abs/2111.07846v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07846v1)
- **Published**: 2021-11-15 15:36:50+00:00
- **Updated**: 2021-11-15 15:36:50+00:00
- **Authors**: Joakim Bruslund Haurum, Meysam Madadi, Sergio Escalera, Thomas B. Moeslund
- **Comment**: WACV 2022
- **Journal**: None
- **Summary**: The sewerage infrastructure is one of the most important and expensive infrastructures in modern society. In order to efficiently manage the sewerage infrastructure, automated sewer inspection has to be utilized. However, while sewer defect classification has been investigated for decades, little attention has been given to classifying sewer pipe properties such as water level, pipe material, and pipe shape, which are needed to evaluate the level of sewer pipe deterioration.   In this work we classify sewer pipe defects and properties concurrently and present a novel decoder-focused multi-task classification architecture Cross-Task Graph Neural Network (CT-GNN), which refines the disjointed per-task predictions using cross-task information. The CT-GNN architecture extends the traditional disjointed task-heads decoder, by utilizing a cross-task graph and unique class node embeddings. The cross-task graph can either be determined a priori based on the conditional probability between the task classes or determined dynamically using self-attention. CT-GNN can be added to any backbone and trained end-to-end at a small increase in the parameter count. We achieve state-of-the-art performance on all four classification tasks in the Sewer-ML dataset, improving defect classification and water level classification by 5.3 and 8.0 percentage points, respectively. We also outperform the single task methods as well as other multi-task classification approaches while introducing 50 times fewer parameters than previous model-focused approaches. The code and models are available at the project page http://vap.aau.dk/ctgnn



### Tracking People with 3D Representations
- **Arxiv ID**: http://arxiv.org/abs/2111.07868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07868v1)
- **Published**: 2021-11-15 16:15:21+00:00
- **Updated**: 2021-11-15 16:15:21+00:00
- **Authors**: Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for tracking multiple people in video. Unlike past approaches which employ 2D representations, we focus on using 3D representations of people, located in three-dimensional space. To this end, we develop a method, Human Mesh and Appearance Recovery (HMAR) which in addition to extracting the 3D geometry of the person as a SMPL mesh, also extracts appearance as a texture map on the triangles of the mesh. This serves as a 3D representation for appearance that is robust to viewpoint and pose changes. Given a video clip, we first detect bounding boxes corresponding to people, and for each one, we extract 3D appearance, pose, and location information using HMAR. These embedding vectors are then sent to a transformer, which performs spatio-temporal aggregation of the representations over the duration of the sequence. The similarity of the resulting representations is used to solve for associations that assigns each person to a tracklet. We evaluate our approach on the Posetrack, MuPoTs and AVA datasets. We find that 3D representations are more effective than 2D representations for tracking in these settings, and we obtain state-of-the-art performance. Code and results are available at: https://brjathu.github.io/T3DP.



### Oil and Gas Pipeline Monitoring during COVID-19 Pandemic via Unmanned Aerial Vehicle
- **Arxiv ID**: http://arxiv.org/abs/2111.09155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2111.09155v1)
- **Published**: 2021-11-15 16:44:16+00:00
- **Updated**: 2021-11-15 16:44:16+00:00
- **Authors**: Myssar Jabbar Hammood Al-Battbootti, Iuliana Marin, Nicolae Goga, Ramona Popa
- **Comment**: 14th International Conference of Education, Research and Innovation
  (ICERI2021)
- **Journal**: None
- **Summary**: The vast network of oil and gas transmission pipelines requires periodic monitoring for maintenance and hazard inspection to avoid equipment failure and potential accidents. The severe COVID-19 pandemic situation forced the companies to shrink the size of their teams. One risk which is faced on-site is represented by the uncontrolled release of flammable oil and gas. Among many inspection methods, the unmanned aerial vehicle system contains flexibility and stability. Unmanned aerial vehicles can transfer data in real-time, while they are doing their monitoring tasks. The current article focuses on unmanned aerial vehicles equipped with optical sensing and artificial intelligence, especially image recognition with deep learning techniques for pipeline surveillance. Unmanned aerial vehicles can be used for regular patrolling duties to identify and capture images and videos of the area of interest. Places that are hard to reach will be accessed faster, cheaper and with less risk. The current paper is based on the idea of capturing video and images of drone-based inspections, which can discover several potential hazardous problems before they become dangerous. Damage can emerge as a weakening of the cladding on the external pipe insulation. There can also be the case when the thickness of piping through external corrosion can occur. The paper describes a survey completed by experts from the oil and gas industry done for finding the functional and non-functional requirements of the proposed system.



### Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization
- **Arxiv ID**: http://arxiv.org/abs/2111.07898v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07898v2)
- **Published**: 2021-11-15 16:52:07+00:00
- **Updated**: 2022-05-10 17:36:28+00:00
- **Authors**: Sushrut Thorat, Giacomo Aldegheri, Tim C. Kietzmann
- **Comment**: 13 pages, 9 figures, peer-reviewed and accepted at the SVRHM 2021
  workshop at NeurIPS (+ 2 additional sections in the Appendix presenting newer
  supplementary results). SVRHM 2021 Workshop@ NeurIPS. 2021
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) have been shown to perform better than feedforward architectures in visual object categorization tasks, especially in challenging conditions such as cluttered images. However, little is known about the exact computational role of recurrent information flow in these conditions. Here we test RNNs trained for object categorization on the hypothesis that recurrence iteratively aids object categorization via the communication of category-orthogonal auxiliary variables (the location, orientation, and scale of the object). Using diagnostic linear readouts, we find that: (a) information about auxiliary variables increases across time in all network layers, (b) this information is indeed present in the recurrent information flow, and (c) its manipulation significantly affects task performance. These observations confirm the hypothesis that category-orthogonal auxiliary variable information is conveyed through recurrent connectivity and is used to optimize category inference in cluttered environments.



### Volumetric Parameterization of the Placenta to a Flattened Template
- **Arxiv ID**: http://arxiv.org/abs/2111.07900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07900v1)
- **Published**: 2021-11-15 16:53:08+00:00
- **Updated**: 2021-11-15 16:53:08+00:00
- **Authors**: S. Mazdak Abulnaga, Esra Abaci Turk, Mikhail Bessmeltsev, P. Ellen Grant, Justin Solomon, Polina Golland
- **Comment**: Accepted to IEEE TMI ( (c) IEEE). This manuscript expands the MICCAI
  2019 paper (arXiv:1903.05044) by developing additional template models and
  extensions to improve robustness, expanded evaluation on a significantly
  larger dataset, and experiments and discussion demonstrating utility for
  clinical research. Code is available at
  https://github.com/mabulnaga/placenta-flattening
- **Journal**: None
- **Summary**: We present a volumetric mesh-based algorithm for parameterizing the placenta to a flattened template to enable effective visualization of local anatomy and function. MRI shows potential as a research tool as it provides signals directly related to placental function. However, due to the curved and highly variable in vivo shape of the placenta, interpreting and visualizing these images is difficult. We address interpretation challenges by mapping the placenta so that it resembles the familiar ex vivo shape. We formulate the parameterization as an optimization problem for mapping the placental shape represented by a volumetric mesh to a flattened template. We employ the symmetric Dirichlet energy to control local distortion throughout the volume. Local injectivity in the mapping is enforced by a constrained line search during the gradient descent optimization. We validate our method using a research study of 111 placental shapes extracted from BOLD MRI images. Our mapping achieves sub-voxel accuracy in matching the template while maintaining low distortion throughout the volume. We demonstrate how the resulting flattening of the placenta improves visualization of anatomy and function. Our code is freely available at https://github.com/mabulnaga/placenta-flattening .



### Deep Semantic Manipulation of Facial Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.07902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07902v2)
- **Published**: 2021-11-15 16:55:16+00:00
- **Updated**: 2022-10-17 14:34:07+00:00
- **Authors**: Girish Kumar Solanki, Anastasios Roussos
- **Comment**: 4th Workshop and Competition on Affective Behavior Analysis
  in-the-wild (ABAW), European Conference on Computer Vision (ECCV), Tel Aviv,
  Israel, October 2022
- **Journal**: None
- **Summary**: Editing and manipulating facial features in videos is an interesting and important field of research with a plethora of applications, ranging from movie post-production and visual effects to realistic avatars for video games and virtual assistants. Our method supports semantic video manipulation based on neural rendering and 3D-based facial expression modelling. We focus on interactive manipulation of the videos by altering and controlling the facial expressions, achieving promising photorealistic results. The proposed method is based on a disentangled representation and estimation of the 3D facial shape and activity, providing the user with intuitive and easy-to-use control of the facial expressions in the input video. We also introduce a user-friendly, interactive AI tool that processes human-readable semantic labels about the desired expression manipulations in specific parts of the input video and synthesizes photorealistic manipulated videos. We achieve that by mapping the emotion labels to points on the Valence-Arousal space (where Valence quantifies how positive or negative is an emotion and Arousal quantifies the power of the emotion activation), which in turn are mapped to disentangled 3D facial expressions through an especially-designed and trained expression decoder network. The paper presents detailed qualitative and quantitative experiments, which demonstrate the effectiveness of our system and the promising results it achieves.



### Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2111.07910v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07910v2)
- **Published**: 2021-11-15 16:59:48+00:00
- **Updated**: 2022-03-21 12:56:00+00:00
- **Authors**: Yuanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, Luc Van Gool
- **Comment**: CVPR 2022; The first Transformer-based method for snapshot
  compressive imaging
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI restoration. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs. Code and pre-trained models are available at https://github.com/caiyuanhao1998/MST/



### Target Layer Regularization for Continual Learning Using Cramer-Wold Generator
- **Arxiv ID**: http://arxiv.org/abs/2111.07928v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07928v1)
- **Published**: 2021-11-15 17:32:54+00:00
- **Updated**: 2021-11-15 17:32:54+00:00
- **Authors**: Marcin Mazur, ukasz Pustelnik, Szymon Knop, Patryk Pagacz, Przemysaw Spurek
- **Comment**: The paper is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: We propose an effective regularization strategy (CW-TaLaR) for solving continual learning problems. It uses a penalizing term expressed by the Cramer-Wold distance between two probability distributions defined on a target layer of an underlying neural network that is shared by all tasks, and the simple architecture of the Cramer-Wold generator for modeling output data representation. Our strategy preserves target layer distribution while learning a new task but does not require remembering previous tasks' datasets. We perform experiments involving several common supervised frameworks, which prove the competitiveness of the CW-TaLaR method in comparison to a few existing state-of-the-art continual learning models.



### Fully Linear Graph Convolutional Networks for Semi-Supervised Learning and Clustering
- **Arxiv ID**: http://arxiv.org/abs/2111.07942v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07942v1)
- **Published**: 2021-11-15 17:45:09+00:00
- **Updated**: 2021-11-15 17:45:09+00:00
- **Authors**: Yaoming Cai, Zijia Zhang, Zhihua Cai, Xiaobo Liu, Yao Ding, Pedram Ghamisi
- **Comment**: Under review by IEEE Trans. xxx
- **Journal**: None
- **Summary**: This paper presents FLGC, a simple yet effective fully linear graph convolutional network for semi-supervised and unsupervised learning. Instead of using gradient descent, we train FLGC based on computing a global optimal closed-form solution with a decoupled procedure, resulting in a generalized linear framework and making it easier to implement, train, and apply. We show that (1) FLGC is powerful to deal with both graph-structured data and regular data, (2) training graph convolutional models with closed-form solutions improve computational efficiency without degrading performance, and (3) FLGC acts as a natural generalization of classic linear models in the non-Euclidean domain, e.g., ridge regression and subspace clustering. Furthermore, we implement a semi-supervised FLGC and an unsupervised FLGC by introducing an initial residual strategy, enabling FLGC to aggregate long-range neighborhoods and alleviate over-smoothing. We compare our semi-supervised and unsupervised FLGCs against many state-of-the-art methods on a variety of classification and clustering benchmarks, demonstrating that the proposed FLGC models consistently outperform previous methods in terms of accuracy, robustness, and learning efficiency. The core code of our FLGC is released at https://github.com/AngryCai/FLGC.



### Large-Scale Hyperspectral Image Clustering Using Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.07945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07945v1)
- **Published**: 2021-11-15 17:50:06+00:00
- **Updated**: 2021-11-15 17:50:06+00:00
- **Authors**: Yaoming Cai, Zijia Zhang, Yan Liu, Pedram Ghamisi, Kun Li, Xiaobo Liu, Zhihua Cai
- **Comment**: Under review by IEEE Trans. xxx
- **Journal**: None
- **Summary**: Clustering of hyperspectral images is a fundamental but challenging task. The recent development of hyperspectral image clustering has evolved from shallow models to deep and achieved promising results in many benchmark datasets. However, their poor scalability, robustness, and generalization ability, mainly resulting from their offline clustering scenarios, greatly limit their application to large-scale hyperspectral data. To circumvent these problems, we present a scalable deep online clustering model, named Spectral-Spatial Contrastive Clustering (SSCC), based on self-supervised learning. Specifically, we exploit a symmetric twin neural network comprised of a projection head with a dimensionality of the cluster number to conduct dual contrastive learning from a spectral-spatial augmentation pool. We define the objective function by implicitly encouraging within-cluster similarity and reducing between-cluster redundancy. The resulting approach is trained in an end-to-end fashion by batch-wise optimization, making it robust in large-scale data and resulting in good generalization ability for unseen data. Extensive experiments on three hyperspectral image benchmarks demonstrate the effectiveness of our approach and show that we advance the state-of-the-art approaches by large margins.



### Occluded Video Instance Segmentation: Dataset and ICCV 2021 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2111.07950v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2111.07950v1)
- **Published**: 2021-11-15 17:59:03+00:00
- **Updated**: 2021-11-15 17:59:03+00:00
- **Authors**: Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip H. S. Torr, Song Bai
- **Comment**: Accepted by NeurIPS 2021 Datasets and Benchmarks Track. arXiv admin
  note: text overlap with arXiv:2102.01558
- **Journal**: None
- **Summary**: Although deep learning methods have achieved advanced video object recognition performance in recent years, perceiving heavily occluded objects in a video is still a very challenging task. To promote the development of occlusion understanding, we collect a large-scale dataset called OVIS for video instance segmentation in the occluded scenario. OVIS consists of 296k high-quality instance masks and 901 occluded scenes. While our human vision systems can perceive those occluded objects by contextual reasoning and association, our experiments suggest that current video understanding systems cannot. On the OVIS dataset, all baseline methods encounter a significant performance degradation of about 80% in the heavily occluded object group, which demonstrates that there is still a long way to go in understanding obscured objects and videos in a complex real-world scenario. To facilitate the research on new paradigms for video understanding systems, we launched a challenge based on the OVIS dataset. The submitted top-performing algorithms have achieved much higher performance than our baselines. In this paper, we will introduce the OVIS dataset and further dissect it by analyzing the results of baselines and submitted methods. The OVIS dataset and challenge information can be found at http://songbai.site/ovis .



### QK Iteration: A Self-Supervised Representation Learning Algorithm for Image Similarity
- **Arxiv ID**: http://arxiv.org/abs/2111.07954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07954v1)
- **Published**: 2021-11-15 18:01:05+00:00
- **Updated**: 2021-11-15 18:01:05+00:00
- **Authors**: David Wu, Yunnan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised representation learning is a fundamental problem in computer vision with many useful applications (e.g., image search, instance level recognition, copy detection). In this paper we present a new contrastive self-supervised representation learning algorithm in the context of Copy Detection in the 2021 Image Similarity Challenge hosted by Facebook AI Research. Previous work in contrastive self-supervised learning has identified the importance of being able to optimize representations while ``pushing'' against a large number of negative examples. Representative previous solutions either use large batches enabled by modern distributed training systems or maintain queues or memory banks holding recently evaluated representations while relaxing some consistency properties. We approach this problem from a new angle: We directly learn a query model and a key model jointly and push representations against a very large number (e.g., 1 million) of negative representations in each SGD step. We achieve this by freezing the backbone on one side and by alternating between a Q-optimization step and a K-optimization step. During the competition timeframe, our algorithms achieved a micro-AP score of 0.3401 on the Phase 1 leaderboard, significantly improving over the baseline $\mu$AP of 0.1556. On the final Phase 2 leaderboard, our model scored 0.1919, while the baseline scored 0.0526. Continued training yielded further improvement. We conducted an empirical study to compare the proposed approach with a SimCLR style strategy where the negative examples are taken from the batch only. We found that our method ($\mu$AP of 0.3403) significantly outperforms this SimCLR-style baseline ($\mu$AP of 0.2001).



### Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation
- **Arxiv ID**: http://arxiv.org/abs/2111.07971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07971v1)
- **Published**: 2021-11-15 18:37:43+00:00
- **Updated**: 2021-11-15 18:37:43+00:00
- **Authors**: David Acuna, Jonah Philion, Sanja Fidler
- **Comment**: NeurIPS 2021; Project website:
  https://nv-tlabs.github.io/simulation-strategies/
- **Journal**: None
- **Summary**: Autonomous driving relies on a huge volume of real-world data to be labeled to high precision. Alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations. However, the domain gap between the synthetic and real data remains, raising the following important question: What are the best ways to utilize a self-driving simulator for perception tasks? In this work, we build on top of recent advances in domain-adaptation theory, and from this perspective, propose ways to minimize the reality gap. We primarily focus on the use of labels in the synthetic domain alone. Our approach introduces both a principled way to learn neural-invariant representations and a theoretically inspired view on how to sample the data from the simulator. Our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator. We showcase our approach on the bird's-eye-view vehicle segmentation task with multi-sensor data (cameras, lidar) using an open-source simulator (CARLA), and evaluate the entire framework on a real-world dataset (nuScenes). Last but not least, we show what types of variations (e.g. weather conditions, number of assets, map design, and color diversity) matter to perception networks when trained with driving simulators, and which ones can be compensated for with our domain adaptation technique.



### Semantically Grounded Object Matching for Robust Robotic Scene Rearrangement
- **Arxiv ID**: http://arxiv.org/abs/2111.07975v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07975v1)
- **Published**: 2021-11-15 18:39:43+00:00
- **Updated**: 2021-11-15 18:39:43+00:00
- **Authors**: Walter Goodwin, Sagar Vaze, Ioannis Havoutis, Ingmar Posner
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Object rearrangement has recently emerged as a key competency in robot manipulation, with practical solutions generally involving object detection, recognition, grasping and high-level planning. Goal-images describing a desired scene configuration are a promising and increasingly used mode of instruction. A key outstanding challenge is the accurate inference of matches between objects in front of a robot, and those seen in a provided goal image, where recent works have struggled in the absence of object-specific training data. In this work, we explore the deterioration of existing methods' ability to infer matches between objects as the visual shift between observed and goal scenes increases. We find that a fundamental limitation of the current setting is that source and target images must contain the same $\textit{instance}$ of every object, which restricts practical deployment. We present a novel approach to object matching that uses a large pre-trained vision-language model to match objects in a cross-instance setting by leveraging semantics together with visual features as a more robust, and much more general, measure of similarity. We demonstrate that this provides considerably improved matching performance in cross-instance settings, and can be used to guide multi-object rearrangement with a robot manipulator from an image that shares no object $\textit{instances}$ with the robot's scene.



### LiT: Zero-Shot Transfer with Locked-image text Tuning
- **Arxiv ID**: http://arxiv.org/abs/2111.07991v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07991v3)
- **Published**: 2021-11-15 18:53:48+00:00
- **Updated**: 2022-06-22 14:43:02+00:00
- **Authors**: Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer
- **Comment**: Xiaohua, Xiao, Basil, Andreas and Lucas contributed equally; CVPR
  2022
- **Journal**: None
- **Summary**: This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.



### Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.08046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2111.08046v1)
- **Published**: 2021-11-15 19:07:39+00:00
- **Updated**: 2021-11-15 19:07:39+00:00
- **Authors**: Kranti Kumar Parida, Siddharth Srivastava, Gaurav Sharma
- **Comment**: To appear in WACV 2022. arXiv admin note: text overlap with
  arXiv:2108.04906
- **Journal**: None
- **Summary**: Binaural audio gives the listener an immersive experience and can enhance augmented and virtual reality. However, recording binaural audio requires specialized setup with a dummy human head having microphones in left and right ears. Such a recording setup is difficult to build and setup, therefore mono audio has become the preferred choice in common devices. To obtain the same impact as binaural audio, recent efforts have been directed towards lifting mono audio to binaural audio conditioned on the visual input from the scene. Such approaches have not used an important cue for the task: the distance of different sound producing objects from the microphones. In this work, we argue that depth map of the scene can act as a proxy for inducing distance information of different objects in the scene, for the task of audio binauralization. We propose a novel encoder-decoder architecture with a hierarchical attention mechanism to encode image, depth and audio feature jointly. We design the network on top of state-of-the-art transformer networks for image and depth representation. We show empirically that the proposed method outperforms state-of-the-art methods comfortably for two challenging public datasets FAIR-Play and MUSIC-Stereo. We also demonstrate with qualitative results that the method is able to focus on the right information required for the task. The project details are available at \url{https://krantiparida.github.io/projects/bmonobinaural.html}



### Synthetic Unknown Class Learning for Learning Unknowns
- **Arxiv ID**: http://arxiv.org/abs/2111.08062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08062v1)
- **Published**: 2021-11-15 19:46:41+00:00
- **Updated**: 2021-11-15 19:46:41+00:00
- **Authors**: Jaeyeon Jang
- **Comment**: 11 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: This paper addresses the open set recognition (OSR) problem, where the goal is to correctly classify samples of known classes while detecting unknown samples to reject. In the OSR problem, "unknown" is assumed to have infinite possibilities because we have no knowledge about unknowns until they emerge. Intuitively, the more an OSR system explores the possibilities of unknowns, the more likely it is to detect unknowns. Thus, this paper proposes a novel synthetic unknown class learning method that generates unknown-like samples while maintaining diversity between the generated samples and learns these samples. In addition to this unknown sample generation process, knowledge distillation is introduced to provide room for learning synthetic unknowns. By learning the unknown-like samples and known samples in an alternating manner, the proposed method can not only experience diverse synthetic unknowns but also reduce overgeneralization with respect to known classes. Experiments on several benchmark datasets show that the proposed method significantly outperforms other state-of-the-art approaches. It is also shown that realistic unknown digits can be generated and learned via the proposed method after training on the MNIST dataset.



### Two-dimensional Deep Regression for Early Yield Prediction of Winter Wheat
- **Arxiv ID**: http://arxiv.org/abs/2111.08069v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08069v1)
- **Published**: 2021-11-15 20:40:15+00:00
- **Updated**: 2021-11-15 20:40:15+00:00
- **Authors**: Giorgio Morales, John W. Sheppard
- **Comment**: Accepted to appear in the SPIE Future Sensing Technologies 2021
- **Journal**: None
- **Summary**: Crop yield prediction is one of the tasks of Precision Agriculture that can be automated based on multi-source periodic observations of the fields. We tackle the yield prediction problem using a Convolutional Neural Network (CNN) trained on data that combines radar satellite imagery and on-ground information. We present a CNN architecture called Hyper3DNetReg that takes in a multi-channel input image and outputs a two-dimensional raster, where each pixel represents the predicted yield value of the corresponding input pixel. We utilize radar data acquired from the Sentinel-1 satellites, while the on-ground data correspond to a set of six raster features: nitrogen rate applied, precipitation, slope, elevation, topographic position index (TPI), and aspect. We use data collected during the early stage of the winter wheat growing season (March) to predict yield values during the harvest season (August). We present experiments over four fields of winter wheat and show that our proposed methodology yields better results than five compared methods, including multiple linear regression, an ensemble of feedforward networks using AdaBoost, a stacked autoencoder, and two other CNN architectures.



### LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations
- **Arxiv ID**: http://arxiv.org/abs/2111.08094v2
- **DOI**: 10.1007/s10994-022-06204-w
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08094v2)
- **Published**: 2021-11-15 21:40:34+00:00
- **Updated**: 2022-04-30 08:26:39+00:00
- **Authors**: Weronika Hryniewska, Adrianna Grudzie, Przemysaw Biecek
- **Comment**: None
- **Journal**: Machine Learning (2022) 1-18
- **Summary**: The increased interest in deep learning applications, and their hard-to-detect biases result in the need to validate and explain complex models. However, current explanation methods are limited as far as both the explanation of the reasoning process and prediction results are concerned. They usually only show the location in the image that was important for model prediction. The lack of possibility to interact with explanations makes it difficult to verify and understand exactly how the model works. This creates a significant risk when using the model. The risk is compounded by the fact that explanations do not take into account the semantic meaning of the explained objects. To escape from the trap of static and meaningless explanations, we propose a tool and a process called LIMEcraft. LIMEcraft enhances the process of explanation by allowing a user to interactively select semantically consistent areas and thoroughly examine the prediction for the image instance in case of many image features. Experiments on several models show that our tool improves model safety by inspecting model fairness for image pieces that may indicate model bias. The code is available at: http://github.com/MI2DataLab/LIMEcraft



### Error Diagnosis of Deep Monocular Depth Estimation Models
- **Arxiv ID**: http://arxiv.org/abs/2112.05533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.05533v1)
- **Published**: 2021-11-15 22:13:28+00:00
- **Updated**: 2021-11-15 22:13:28+00:00
- **Authors**: Jagpreet Chawla, Nikhil Thakurdesai, Anuj Godase, Md Reza, David Crandall, Soon-Heung Jung
- **Comment**: Presented at IROS'21
- **Journal**: None
- **Summary**: Estimating depth from a monocular image is an ill-posed problem: when the camera projects a 3D scene onto a 2D plane, depth information is inherently and permanently lost. Nevertheless, recent work has shown impressive results in estimating 3D structure from 2D images using deep learning. In this paper, we put on an introspective hat and analyze state-of-the-art monocular depth estimation models in indoor scenes to understand these models' limitations and error patterns. To address errors in depth estimation, we introduce a novel Depth Error Detection Network (DEDN) that spatially identifies erroneous depth predictions in the monocular depth estimation models. By experimenting with multiple state-of-the-art monocular indoor depth estimation models on multiple datasets, we show that our proposed depth error detection network can identify a significant number of errors in the predicted depth maps. Our module is flexible and can be readily plugged into any monocular depth prediction network to help diagnose its results. Additionally, we propose a simple yet effective Depth Error Correction Network (DECN) that iteratively corrects errors based on our initial error diagnosis.




# Arxiv Papers in cs.CV on 2021-11-09
### Lymph Node Detection in T2 MRI with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2111.04885v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.04885v1)
- **Published**: 2021-11-09 00:06:27+00:00
- **Updated**: 2021-11-09 00:06:27+00:00
- **Authors**: Tejas Sudharshan Mathai, Sungwon Lee, Daniel C. Elton, Thomas C. Shen, Yifan Peng, Zhiyong Lu, Ronald M. Summers
- **Comment**: Accepted at SPIE 2022
- **Journal**: None
- **Summary**: Identification of lymph nodes (LN) in T2 Magnetic Resonance Imaging (MRI) is an important step performed by radiologists during the assessment of lymphoproliferative diseases. The size of the nodes play a crucial role in their staging, and radiologists sometimes use an additional contrast sequence such as diffusion weighted imaging (DWI) for confirmation. However, lymph nodes have diverse appearances in T2 MRI scans, making it tough to stage for metastasis. Furthermore, radiologists often miss smaller metastatic lymph nodes over the course of a busy day. To deal with these issues, we propose to use the DEtection TRansformer (DETR) network to localize suspicious metastatic lymph nodes for staging in challenging T2 MRI scans acquired by different scanners and exam protocols. False positives (FP) were reduced through a bounding box fusion technique, and a precision of 65.41\% and sensitivity of 91.66\% at 4 FP per image was achieved. To the best of our knowledge, our results improve upon the current state-of-the-art for lymph node detection in T2 MRI scans.



### Universal Lesion Detection in CT Scans using Neural Network Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2111.04886v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04886v2)
- **Published**: 2021-11-09 00:11:01+00:00
- **Updated**: 2021-11-10 19:33:02+00:00
- **Authors**: Tarun Mattikalli, Tejas Sudharshan Mathai, Ronald M. Summers
- **Comment**: Accepted at SPIE 2022
- **Journal**: None
- **Summary**: In clinical practice, radiologists are reliant on the lesion size when distinguishing metastatic from non-metastatic lesions. A prerequisite for lesion sizing is their detection, as it promotes the downstream assessment of tumor spread. However, lesions vary in their size and appearance in CT scans, and radiologists often miss small lesions during a busy clinical day. To overcome these challenges, we propose the use of state-of-the-art detection neural networks to flag suspicious lesions present in the NIH DeepLesion dataset for sizing. Additionally, we incorporate a bounding box fusion technique to minimize false positives (FP) and improve detection accuracy. Finally, to resemble clinical usage, we constructed an ensemble of the best detection models to localize lesions for sizing with a precision of 65.17% and sensitivity of 91.67% at 4 FP per image. Our results improve upon or maintain the performance of current state-of-the-art methods for lesion detection in challenging CT scans.



### Mitigating domain shift in AI-based tuberculosis screening with unsupervised domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.04893v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04893v1)
- **Published**: 2021-11-09 00:45:52+00:00
- **Updated**: 2021-11-09 00:45:52+00:00
- **Authors**: Nishanjan Ravin, Sourajit Saha, Alan Schweitzer, Ameena Elahi, Farouk Dako, Daniel Mollura, David Chapman
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate that Domain Invariant Feature Learning (DIFL) can improve the out-of-domain generalizability of a deep learning Tuberculosis screening algorithm. It is well known that state of the art deep learning algorithms often have difficulty generalizing to unseen data distributions due to "domain shift". In the context of medical imaging, this could lead to unintended biases such as the inability to generalize from one patient population to another. We analyze the performance of a ResNet-50 classifier for the purposes of Tuberculosis screening using the four most popular public datasets with geographically diverse sources of imagery. We show that without domain adaptation, ResNet-50 has difficulty in generalizing between imaging distributions from a number of public Tuberculosis screening datasets with imagery from geographically distributed regions. However, with the incorporation of DIFL, the out-of-domain performance is greatly enhanced. Analysis criteria includes a comparison of accuracy, sensitivity, specificity and AUC over both the baseline, as well as the DIFL enhanced algorithms. We conclude that DIFL improves generalizability of Tuberculosis screening while maintaining acceptable accuracy over the source domain imagery when applied across a variety of public datasets.



### Label-Aware Distribution Calibration for Long-tailed Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.04901v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04901v1)
- **Published**: 2021-11-09 01:38:35+00:00
- **Updated**: 2021-11-09 01:38:35+00:00
- **Authors**: Chaozheng Wang, Shuzheng Gao, Cuiyun Gao, Pengyun Wang, Wenjie Pei, Lujia Pan, Zenglin Xu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Real-world data usually present long-tailed distributions. Training on imbalanced data tends to render neural networks perform well on head classes while much worse on tail classes. The severe sparseness of training instances for the tail classes is the main challenge, which results in biased distribution estimation during training. Plenty of efforts have been devoted to ameliorating the challenge, including data re-sampling and synthesizing new training instances for tail classes. However, no prior research has exploited the transferable knowledge from head classes to tail classes for calibrating the distribution of tail classes. In this paper, we suppose that tail classes can be enriched by similar head classes and propose a novel distribution calibration approach named as label-Aware Distribution Calibration LADC. LADC transfers the statistics from relevant head classes to infer the distribution of tail classes. Sampling from calibrated distribution further facilitates re-balancing the classifier. Experiments on both image and text long-tailed datasets demonstrate that LADC significantly outperforms existing methods.The visualization also shows that LADC provides a more accurate distribution estimation.



### Real-time Instance Segmentation of Surgical Instruments using Attention and Multi-scale Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2111.04911v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04911v2)
- **Published**: 2021-11-09 02:22:01+00:00
- **Updated**: 2021-11-10 03:08:59+00:00
- **Authors**: Juan Carlos Angeles-Ceron, Gilberto Ochoa-Ruiz, Leonardo Chang, Sharib Ali
- **Comment**: None
- **Journal**: None
- **Summary**: Precise instrument segmentation aid surgeons to navigate the body more easily and increase patient safety. While accurate tracking of surgical instruments in real-time plays a crucial role in minimally invasive computer-assisted surgeries, it is a challenging task to achieve, mainly due to 1) complex surgical environment, and 2) model design with both optimal accuracy and speed. Deep learning gives us the opportunity to learn complex environment from large surgery scene environments and placements of these instruments in real world scenarios. The Robust Medical Instrument Segmentation 2019 challenge (ROBUST-MIS) provides more than 10,000 frames with surgical tools in different clinical settings. In this paper, we use a light-weight single stage instance segmentation model complemented with a convolutional block attention module for achieving both faster and accurate inference. We further improve accuracy through data augmentation and optimal anchor localisation strategies. To our knowledge, this is the first work that explicitly focuses on both real-time performance and improved accuracy. Our approach out-performed top team performances in the ROBUST-MIS challenge with over 44% improvement on both area-based metric MI_DSC and distance-based metric MI_NSD. We also demonstrate real-time performance (> 60 frames-per-second) with different but competitive variants of our final approach.



### Self-Interpretable Model with TransformationEquivariant Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2111.04927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.04927v1)
- **Published**: 2021-11-09 03:21:25+00:00
- **Updated**: 2021-11-09 03:21:25+00:00
- **Authors**: Yipei Wang, Xiaoqian Wang
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: In this paper, we propose a self-interpretable model SITE with transformation-equivariant interpretations. We focus on the robustness and self-consistency of the interpretations of geometric transformations. Apart from the transformation equivariance, as a self-interpretable model, SITE has comparable expressive power as the benchmark black-box classifiers, while being able to present faithful and robust interpretations with high quality. It is worth noticing that although applied in most of the CNN visualization methods, the bilinear upsampling approximation is a rough approximation, which can only provide interpretations in the form of heatmaps (instead of pixel-wise). It remains an open question whether such interpretations can be direct to the input space (as shown in the MNIST experiments). Besides, we consider the translation and rotation transformations in our model. In future work, we will explore the robust interpretations under more complex transformations such as scaling and distortion. Moreover, we clarify that SITE is not limited to geometric transformation (that we used in the computer vision domain), and will explore SITEin other domains in future work.



### SAFA: Structure Aware Face Animation
- **Arxiv ID**: http://arxiv.org/abs/2111.04928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04928v1)
- **Published**: 2021-11-09 03:22:38+00:00
- **Updated**: 2021-11-09 03:22:38+00:00
- **Authors**: Qiulin Wang, Lu Zhang, Bo Li
- **Comment**: Accepted at 3DV2021
- **Journal**: None
- **Summary**: Recent success of generative adversarial networks (GAN) has made great progress on the face animation task. However, the complex scene structure of a face image still makes it a challenge to generate videos with face poses significantly deviating from the source image. On one hand, without knowing the facial geometric structure, generated face images might be improperly distorted. On the other hand, some area of the generated image might be occluded in the source image, which makes it difficult for GAN to generate realistic appearance. To address these problems, we propose a structure aware face animation (SAFA) method which constructs specific geometric structures to model different components of a face image. Following the well recognized motion based face animation technique, we use a 3D morphable model (3DMM) to model the face, multiple affine transforms to model the other foreground components like hair and beard, and an identity transform to model the background. The 3DMM geometric embedding not only helps generate realistic structure for the driving scene, but also contributes to better perception of occluded area in the generated image. Besides, we further propose to exploit the widely studied inpainting technique to faithfully recover the occluded image area. Both quantitative and qualitative experiment results have shown the superiority of our method. Code is available at https://github.com/Qiulin-W/SAFA.



### PREMA: Part-based REcurrent Multi-view Aggregation Network for 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.04945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.04945v1)
- **Published**: 2021-11-09 04:01:35+00:00
- **Updated**: 2021-11-09 04:01:35+00:00
- **Authors**: Jiongchao Jin, Huanqiang Xu, Pengliang Ji, Zehao Tang, Zhang Xiong
- **Comment**: Accepted by ICCSMT 2021
- **Journal**: None
- **Summary**: We propose the Part-based Recurrent Multi-view Aggregation network(PREMA) to eliminate the detrimental effects of the practical view defects, such as insufficient view numbers, occlusions or background clutters, and also enhance the discriminative ability of shape representations. Inspired by the fact that human recognize an object mainly by its discriminant parts, we define the multi-view coherent part(MCP), a discriminant part reoccurring in different views. Our PREMA can reliably locate and effectively utilize MCPs to build robust shape representations. Comprehensively, we design a novel Regional Attention Unit(RAU) in PREMA to compute the confidence map for each view, and extract MCPs by applying those maps to view features. PREMA accentuates MCPs via correlating features of different views, and aggregates the part-aware features for shape representation.



### Graph-Based Depth Denoising & Dequantization for Point Cloud Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2111.04946v2
- **DOI**: 10.1109/TIP.2022.3214077
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04946v2)
- **Published**: 2021-11-09 04:17:35+00:00
- **Updated**: 2022-10-06 04:58:40+00:00
- **Authors**: Xue Zhang, Gene Cheung, Jiahao Pang, Yash Sanghvi, Abhiram Gnanasambandam, Stanley H. Chan
- **Comment**: 16 pages,14 figures
- **Journal**: None
- **Summary**: A 3D point cloud is typically constructed from depth measurements acquired by sensors at one or more viewpoints. The measurements suffer from both quantization and noise corruption. To improve quality, previous works denoise a point cloud \textit{a posteriori} after projecting the imperfect depth data onto 3D space. Instead, we enhance depth measurements directly on the sensed images \textit{a priori}, before synthesizing a 3D point cloud. By enhancing near the physical sensing process, we tailor our optimization to our depth formation model before subsequent processing steps that obscure measurement errors.   Specifically, we model depth formation as a combined process of signal-dependent noise addition and non-uniform log-based quantization. The designed model is validated (with parameters fitted) using collected empirical data from a representative depth sensor. To enhance each pixel row in a depth image, we first encode intra-view similarities between available row pixels as edge weights via feature graph learning. We next establish inter-view similarities with another rectified depth image via viewpoint mapping and sparse linear interpolation. This leads to a maximum a posteriori (MAP) graph filtering objective that is convex and differentiable. We minimize the objective efficiently using accelerated gradient descent (AGD), where the optimal step size is approximated via Gershgorin circle theorem (GCT). Experiments show that our method significantly outperformed recent point cloud denoising schemes and state-of-the-art image denoising schemes in two established point cloud quality metrics.



### Dual Prototypical Contrastive Learning for Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.04982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04982v1)
- **Published**: 2021-11-09 08:14:50+00:00
- **Updated**: 2021-11-09 08:14:50+00:00
- **Authors**: Hyeongjun Kwon, Somi Jeong, Sunok Kim, Kwanghoon Sohn
- **Comment**: 8 pages, 7 figures, https://github.com/kwonjunn01/DPCL
- **Journal**: None
- **Summary**: We address the problem of few-shot semantic segmentation (FSS), which aims to segment novel class objects in a target image with a few annotated samples. Though recent advances have been made by incorporating prototype-based metric learning, existing methods still show limited performance under extreme intra-class object variations and semantically similar inter-class objects due to their poor feature representation. To tackle this problem, we propose a dual prototypical contrastive learning approach tailored to the FSS task to capture the representative semanticfeatures effectively. The main idea is to encourage the prototypes more discriminative by increasing inter-class distance while reducing intra-class distance in prototype feature space. To this end, we first present a class-specific contrastive loss with a dynamic prototype dictionary that stores the class-aware prototypes during training, thus enabling the same class prototypes similar and the different class prototypes to be dissimilar. Furthermore, we introduce a class-agnostic contrastive loss to enhance the generalization ability to unseen classes by compressing the feature distribution of semantic class within each episode. We demonstrate that the proposed dual prototypical contrastive learning approach outperforms state-of-the-art FSS methods on PASCAL-5i and COCO-20i datasets. The code is available at:https://github.com/kwonjunn01/DPCL1.



### Bilinear pooling and metric learning network for early Alzheimer's disease identification with FDG-PET images
- **Arxiv ID**: http://arxiv.org/abs/2111.04985v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04985v1)
- **Published**: 2021-11-09 08:17:55+00:00
- **Updated**: 2021-11-09 08:17:55+00:00
- **Authors**: Wenju Cui, Caiying Yan, Zhuangzhi Yan, Yunsong Peng, Yilin Leng, Chenlu Liu, Shuangqing Chen, Xi Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: FDG-PET reveals altered brain metabolism in individuals with mild cognitive impairment (MCI) and Alzheimer's disease (AD). Some biomarkers derived from FDG-PET by computer-aided-diagnosis (CAD) technologies have been proved that they can accurately diagnosis normal control (NC), MCI, and AD. However, the studies of identification of early MCI (EMCI) and late MCI (LMCI) with FDG-PET images are still insufficient. Compared with studies based on fMRI and DTI images, the researches of the inter-region representation features in FDG-PET images are insufficient. Moreover, considering the variability in different individuals, some hard samples which are very similar with both two classes limit the classification performance. To tackle these problems, in this paper, we propose a novel bilinear pooling and metric learning network (BMNet), which can extract the inter-region representation features and distinguish hard samples by constructing embedding space. To validate the proposed method, we collect 998 FDG-PET images from ADNI. Following the common preprocessing steps, 90 features are extracted from each FDG-PET image according to the automatic anatomical landmark (AAL) template and then sent into the proposed network. Extensive 5-fold cross-validation experiments are performed for multiple two-class classifications. Experiments show that most metrics are improved after adding the bilinear pooling module and metric losses to the Baseline model respectively. Specifically, in the classification task between EMCI and LMCI, the specificity improves 6.38% after adding the triple metric loss, and the negative predictive value (NPV) improves 3.45% after using the bilinear pooling module.



### Video Text Tracking With a Spatio-Temporal Complementary Model
- **Arxiv ID**: http://arxiv.org/abs/2111.04987v2
- **DOI**: 10.1109/TIP.2021.3124313
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04987v2)
- **Published**: 2021-11-09 08:23:06+00:00
- **Updated**: 2021-12-29 06:36:08+00:00
- **Authors**: Yuzhe Gao, Xing Li, Jiajian Zhang, Yu Zhou, Dian Jin, Jing Wang, Shenggao Zhu, Xiang Bai
- **Comment**: update Fig.7, in the third row of part (c), the second and third
  frame is wrong and we update the right pictures
- **Journal**: [J]. IEEE Transactions on Image Processing, 2021, 30: 9321-9331
- **Summary**: Text tracking is to track multiple texts in a video,and construct a trajectory for each text. Existing methodstackle this task by utilizing the tracking-by-detection frame-work, i.e., detecting the text instances in each frame andassociating the corresponding text instances in consecutiveframes. We argue that the tracking accuracy of this paradigmis severely limited in more complex scenarios, e.g., owing tomotion blur, etc., the missed detection of text instances causesthe break of the text trajectory. In addition, different textinstances with similar appearance are easily confused, leadingto the incorrect association of the text instances. To this end,a novel spatio-temporal complementary text tracking model isproposed in this paper. We leverage a Siamese ComplementaryModule to fully exploit the continuity characteristic of the textinstances in the temporal dimension, which effectively alleviatesthe missed detection of the text instances, and hence ensuresthe completeness of each text trajectory. We further integratethe semantic cues and the visual cues of the text instance intoa unified representation via a text similarity learning network,which supplies a high discriminative power in the presence oftext instances with similar appearance, and thus avoids the mis-association between them. Our method achieves state-of-the-art performance on several public benchmarks. The source codeis available at https://github.com/lsabrinax/VideoTextSCM.



### Incremental Meta-Learning via Episodic Replay Distillation for Few-Shot Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.04993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04993v2)
- **Published**: 2021-11-09 08:32:05+00:00
- **Updated**: 2021-11-11 16:54:45+00:00
- **Authors**: Kai Wang, Xialei Liu, Andy Bagdanov, Luis Herranz, Shangling Jui, Joost van de Weijer
- **Comment**: None
- **Journal**: None
- **Summary**: Most meta-learning approaches assume the existence of a very large set of labeled data available for episodic meta-learning of base knowledge. This contrasts with the more realistic continual learning paradigm in which data arrives incrementally in the form of tasks containing disjoint classes. In this paper we consider this problem of Incremental Meta-Learning (IML) in which classes are presented incrementally in discrete tasks. We propose an approach to IML, which we call Episodic Replay Distillation (ERD), that mixes classes from the current task with class exemplars from previous tasks when sampling episodes for meta-learning. These episodes are then used for knowledge distillation to minimize catastrophic forgetting. Experiments on four datasets demonstrate that ERD surpasses the state-of-the-art. In particular, on the more challenging one-shot, long task sequence incremental meta-learning scenarios, we reduce the gap between IML and the joint-training upper bound from 3.5% / 10.1% / 13.4% with the current state-of-the-art to 2.6% / 2.9% / 5.0% with our method on Tiered-ImageNet / Mini-ImageNet / CIFAR100, respectively.



### GDCA: GAN-based single image super resolution with Dual discriminators and Channel Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.05014v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05014v1)
- **Published**: 2021-11-09 09:11:59+00:00
- **Updated**: 2021-11-09 09:11:59+00:00
- **Authors**: Thanh Nguyen, Hieu Hoang, Chang D. Yoo
- **Comment**: None
- **Journal**: Korean Association of Artificial Intelligence 2019
- **Summary**: Single Image Super-Resolution (SISR) is a very active research field. This paper addresses SISR by using a GAN-based approach with dual discriminators and incorporating it with an attention mechanism. The experimental results show that GDCA can generate sharper and high pleasing images compare to other conventional methods.



### PIMIP: An Open Source Platform for Pathology Information Management and Integration
- **Arxiv ID**: http://arxiv.org/abs/2111.05794v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05794v1)
- **Published**: 2021-11-09 10:00:59+00:00
- **Updated**: 2021-11-09 10:00:59+00:00
- **Authors**: Jialun Wu, Anyu Mao, Xinrui Bao, Haichuan Zhang, Zeyu Gao, Chunbao Wang, Tieliang Gong, Chen Li
- **Comment**: BIBM 2021 accepted, including 8 pages, 8 figures
- **Journal**: None
- **Summary**: Digital pathology plays a crucial role in the development of artificial intelligence in the medical field. The digital pathology platform can make the pathological resources digital and networked, and realize the permanent storage of visual data and the synchronous browsing processing without the limitation of time and space. It has been widely used in various fields of pathology. However, there is still a lack of an open and universal digital pathology platform to assist doctors in the management and analysis of digital pathological sections, as well as the management and structured description of relevant patient information. Most platforms cannot integrate image viewing, annotation and analysis, and text information management. To solve the above problems, we propose a comprehensive and extensible platform PIMIP. Our PIMIP has developed the image annotation functions based on the visualization of digital pathological sections. Our annotation functions support multi-user collaborative annotation and multi-device annotation, and realize the automation of some annotation tasks. In the annotation task, we invited a professional pathologist for guidance. We introduce a machine learning module for image analysis. The data we collected included public data from local hospitals and clinical examples. Our platform is more clinical and suitable for clinical use. In addition to image data, we also structured the management and display of text information. So our platform is comprehensive. The platform framework is built in a modular way to support users to add machine learning modules independently, which makes our platform extensible.



### MAC-ReconNet: A Multiple Acquisition Context based Convolutional Neural Network for MR Image Reconstruction using Dynamic Weight Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.05055v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05055v2)
- **Published**: 2021-11-09 11:14:14+00:00
- **Updated**: 2022-03-10 07:35:34+00:00
- **Authors**: Sriprabha Ramanarayanan, Balamurali Murugesan, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: None
- **Journal**: Proceedings of the Third Conference on Medical Imaging with Deep
  Learning, PMLR 121:696-708, 2020
- **Summary**: Convolutional Neural network-based MR reconstruction methods have shown to provide fast and high quality reconstructions. A primary drawback with a CNN-based model is that it lacks flexibility and can effectively operate only for a specific acquisition context limiting practical applicability. By acquisition context, we mean a specific combination of three input settings considered namely, the anatomy under study, undersampling mask pattern and acceleration factor for undersampling. The model could be trained jointly on images combining multiple contexts. However the model does not meet the performance of context specific models nor extensible to contexts unseen at train time. This necessitates a modification to the existing architecture in generating context specific weights so as to incorporate flexibility to multiple contexts. We propose a multiple acquisition context based network, called MAC-ReconNet for MRI reconstruction, flexible to multiple acquisition contexts and generalizable to unseen contexts for applicability in real scenarios. The proposed network has an MRI reconstruction module and a dynamic weight prediction (DWP) module. The DWP module takes the corresponding acquisition context information as input and learns the context-specific weights of the reconstruction module which changes dynamically with context at run time. We show that the proposed approach can handle multiple contexts based on cardiac and brain datasets, Gaussian and Cartesian undersampling patterns and five acceleration factors. The proposed network outperforms the naive jointly trained model and gives competitive results with the context-specific models both quantitatively and qualitatively. We also demonstrate the generalizability of our model by testing on contexts unseen at train time.



### Data privacy protection in microscopic image analysis for material data mining
- **Arxiv ID**: http://arxiv.org/abs/2111.07892v1
- **DOI**: None
- **Categories**: **eess.IV**, cond-mat.mtrl-sci, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07892v1)
- **Published**: 2021-11-09 11:16:33+00:00
- **Updated**: 2021-11-09 11:16:33+00:00
- **Authors**: Boyuan Ma, Xiang Yin, Xiaojuan Ban, Haiyou Huang, Neng Zhang, Hao Wang, Weihua Xue
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Recent progress in material data mining has been driven by high-capacity models trained on large datasets. However, collecting experimental data has been extremely costly owing to the amount of human effort and expertise required. Therefore, material researchers are often reluctant to easily disclose their private data, which leads to the problem of data island, and it is difficult to collect a large amount of data to train high-quality models. In this study, a material microstructure image feature extraction algorithm FedTransfer based on data privacy protection is proposed. The core contributions are as follows: 1) the federated learning algorithm is introduced into the polycrystalline microstructure image segmentation task to make full use of different user data to carry out machine learning, break the data island and improve the model generalization ability under the condition of ensuring the privacy and security of user data; 2) A data sharing strategy based on style transfer is proposed. By sharing style information of images that is not urgent for user confidentiality, it can reduce the performance penalty caused by the distribution difference of data among different users.



### MMD-ReID: A Simple but Effective Solution for Visible-Thermal Person ReID
- **Arxiv ID**: http://arxiv.org/abs/2111.05059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05059v1)
- **Published**: 2021-11-09 11:33:32+00:00
- **Updated**: 2021-11-09 11:33:32+00:00
- **Authors**: Chaitra Jambigi, Ruchit Rawal, Anirban Chakraborty
- **Comment**: Accepted in BMVC 2021 (Oral)
- **Journal**: None
- **Summary**: Learning modality invariant features is central to the problem of Visible-Thermal cross-modal Person Reidentification (VT-ReID), where query and gallery images come from different modalities. Existing works implicitly align the modalities in pixel and feature spaces by either using adversarial learning or carefully designing feature extraction modules that heavily rely on domain knowledge. We propose a simple but effective framework, MMD-ReID, that reduces the modality gap by an explicit discrepancy reduction constraint. MMD-ReID takes inspiration from Maximum Mean Discrepancy (MMD), a widely used statistical tool for hypothesis testing that determines the distance between two distributions. MMD-ReID uses a novel margin-based formulation to match class-conditional feature distributions of visible and thermal samples to minimize intra-class distances while maintaining feature discriminability. MMD-ReID is a simple framework in terms of architecture and loss formulation. We conduct extensive experiments to demonstrate both qualitatively and quantitatively the effectiveness of MMD-ReID in aligning the marginal and class conditional distributions, thus learning both modality-independent and identity-consistent features. The proposed framework significantly outperforms the state-of-the-art methods on SYSU-MM01 and RegDB datasets. Code will be released at https://github.com/vcl-iisc/MMD-ReID



### View Birdification in the Crowd: Ground-Plane Localization from Perceived Movements
- **Arxiv ID**: http://arxiv.org/abs/2111.05060v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05060v2)
- **Published**: 2021-11-09 11:34:19+00:00
- **Updated**: 2022-10-25 08:54:25+00:00
- **Authors**: Mai Nishimura, Shohei Nobuhara, Ko Nishino
- **Comment**: Extended journal version of the original paper at BMVC 2021
- **Journal**: None
- **Summary**: We introduce view birdification, the problem of recovering ground-plane movements of people in a crowd from an ego-centric video captured from an observer (e.g., a person or a vehicle) also moving in the crowd. Recovered ground-plane movements would provide a sound basis for situational understanding and benefit downstream applications in computer vision and robotics. In this paper, we formulate view birdification as a geometric trajectory reconstruction problem and derive a cascaded optimization method from a Bayesian perspective. The method first estimates the observer's movement and then localizes surrounding pedestrians for each frame while taking into account the local interactions between them. We introduce three datasets by leveraging synthetic and real trajectories of people in crowds and evaluate the effectiveness of our method. The results demonstrate the accuracy of our method and set the ground for further studies of view birdification as an important but challenging visual understanding problem.



### Deep Convolution Network Based Emotion Analysis for Automatic Detection of Mild Cognitive Impairment in the Elderly
- **Arxiv ID**: http://arxiv.org/abs/2111.05066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05066v1)
- **Published**: 2021-11-09 11:51:33+00:00
- **Updated**: 2021-11-09 11:51:33+00:00
- **Authors**: Zixiang Fei, Erfu Yang, Leijian Yu, Xia Li, Huiyu Zhou, Wenju Zhou
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: A significant number of people are suffering from cognitive impairment all over the world. Early detection of cognitive impairment is of great importance to both patients and caregivers. However, existing approaches have their shortages, such as time consumption and financial expenses involved in clinics and the neuroimaging stage. It has been found that patients with cognitive impairment show abnormal emotion patterns. In this paper, we present a novel deep convolution network-based system to detect the cognitive impairment through the analysis of the evolution of facial emotions while participants are watching designed video stimuli. In our proposed system, a novel facial expression recognition algorithm is developed using layers from MobileNet and Support Vector Machine (SVM), which showed satisfactory performance in 3 datasets. To verify the proposed system in detecting cognitive impairment, 61 elderly people including patients with cognitive impairment and healthy people as a control group have been invited to participate in the experiments and a dataset was built accordingly. With this dataset, the proposed system has successfully achieved the detection accuracy of 73.3%.



### MixACM: Mixup-Based Robustness Transfer via Distillation of Activated Channel Maps
- **Arxiv ID**: http://arxiv.org/abs/2111.05073v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05073v1)
- **Published**: 2021-11-09 12:03:20+00:00
- **Updated**: 2021-11-09 12:03:20+00:00
- **Authors**: Muhammad Awais, Fengwei Zhou, Chuanlong Xie, Jiawei Li, Sung-Ho Bae, Zhenguo Li
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Deep neural networks are susceptible to adversarially crafted, small and imperceptible changes in the natural inputs. The most effective defense mechanism against these examples is adversarial training which constructs adversarial examples during training by iterative maximization of loss. The model is then trained to minimize the loss on these constructed examples. This min-max optimization requires more data, larger capacity models, and additional computing resources. It also degrades the standard generalization performance of a model. Can we achieve robustness more efficiently? In this work, we explore this question from the perspective of knowledge transfer. First, we theoretically show the transferability of robustness from an adversarially trained teacher model to a student model with the help of mixup augmentation. Second, we propose a novel robustness transfer method called Mixup-Based Activated Channel Maps (MixACM) Transfer. MixACM transfers robustness from a robust teacher to a student by matching activated channel maps generated without expensive adversarial perturbations. Finally, extensive experiments on multiple datasets and different learning scenarios show our method can transfer robustness while also improving generalization on natural images.



### Residual Quantity in Percentage of Factory Machines Using Computer Vision and Mathematical Methods
- **Arxiv ID**: http://arxiv.org/abs/2111.05080v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05080v3)
- **Published**: 2021-11-09 12:26:57+00:00
- **Updated**: 2022-02-14 01:28:45+00:00
- **Authors**: Seunghyeon Kim, Jihoon Ryoo, Dongyeob Lee, Youngho Kim
- **Comment**: 4 pages, 13 figures
- **Journal**: None
- **Summary**: Computer vision has been thriving since AI development was gaining thrust. Using deep learning techniques has been the most popular way which computer scientists thought the solution of. However, deep learning techniques tend to show lower performance than manual processing. Using deep learning is not always the answer to a problem related to computer vision.



### Approaching the Limit of Image Rescaling via Flow Guidance
- **Arxiv ID**: http://arxiv.org/abs/2111.05133v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05133v2)
- **Published**: 2021-11-09 13:17:38+00:00
- **Updated**: 2023-01-08 11:07:48+00:00
- **Authors**: Shang Li, Guixuan Zhang, Zhengxiong Luo, Jie Liu, Zhi Zeng, Shuwu Zhang
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Image downscaling and upscaling are two basic rescaling operations. Once the image is downscaled, it is difficult to be reconstructed via upscaling due to the loss of information. To make these two processes more compatible and improve the reconstruction performance, some efforts model them as a joint encoding-decoding task, with the constraint that the downscaled (i.e. encoded) low-resolution (LR) image must preserve the original visual appearance. To implement this constraint, most methods guide the downscaling module by supervising it with the bicubically downscaled LR version of the original high-resolution (HR) image. However, this bicubic LR guidance may be suboptimal for the subsequent upscaling (i.e. decoding) and restrict the final reconstruction performance. In this paper, instead of directly applying the LR guidance, we propose an additional invertible flow guidance module (FGM), which can transform the downscaled representation to the visually plausible image during downscaling and transform it back during upscaling. Benefiting from the invertibility of FGM, the downscaled representation could get rid of the LR guidance and would not disturb the downscaling-upscaling process. It allows us to remove the restrictions on the downscaling module and optimize the downscaling and upscaling modules in an end-to-end manner. In this way, these two modules could cooperate to maximize the HR reconstruction performance. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SotA) performance on both downscaled and reconstructed images.



### Ethically aligned Deep Learning: Unbiased Facial Aesthetic Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.05149v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05149v1)
- **Published**: 2021-11-09 13:53:32+00:00
- **Updated**: 2021-11-09 13:53:32+00:00
- **Authors**: Michael Danner, Thomas Weber, Leping Peng, Tobias Gerlach, Xueping Su, Matthias RÃ¤tsch
- **Comment**: Peer reviewed and accepted at CEPE/IACAP 2021 as Extended Abstract
- **Journal**: None
- **Summary**: Facial beauty prediction (FBP) aims to develop a machine that automatically makes facial attractiveness assessment. In the past those results were highly correlated with human ratings, therefore also with their bias in annotating. As artificial intelligence can have racist and discriminatory tendencies, the cause of skews in the data must be identified. Development of training data and AI algorithms that are robust against biased information is a new challenge for scientists. As aesthetic judgement usually is biased, we want to take it one step further and propose an Unbiased Convolutional Neural Network for FBP. While it is possible to create network models that can rate attractiveness of faces on a high level, from an ethical point of view, it is equally important to make sure the model is unbiased. In this work, we introduce AestheticNet, a state-of-the-art attractiveness prediction network, which significantly outperforms competitors with a Pearson Correlation of 0.9601. Additionally, we propose a new approach for generating a bias-free CNN to improve fairness in machine learning.



### RapidRead: Global Deployment of State-of-the-art Radiology AI for a Large Veterinary Teleradiology Practice
- **Arxiv ID**: http://arxiv.org/abs/2111.08165v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08165v1)
- **Published**: 2021-11-09 14:05:16+00:00
- **Updated**: 2021-11-09 14:05:16+00:00
- **Authors**: Michael Fitzke, Conrad Stack, Andre Dourson, Rodrigo M. B. Santana, Diane Wilson, Lisa Ziemer, Arjun Soin, Matthew P. Lungren, Paul Fisher, Mark Parkinson
- **Comment**: None
- **Journal**: None
- **Summary**: This work describes the development and real-world deployment of a deep learning-based AI system for evaluating canine and feline radiographs across a broad range of findings and abnormalities. We describe a new semi-supervised learning approach that combines NLP-derived labels with self-supervised training leveraging more than 2.5 million x-ray images. Finally we describe the clinical deployment of the model including system architecture, real-time performance evaluation and data drift detection.



### Exploiting Robust Unsupervised Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2111.05170v3
- **DOI**: 10.1049/ipr2.12380
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05170v3)
- **Published**: 2021-11-09 14:29:30+00:00
- **Updated**: 2022-02-12 08:32:47+00:00
- **Authors**: Xianghao Zang, Ge Li, Wei Gao, Xiujun Shu
- **Comment**: Preprint version; Accepted by IET Image Processing
- **Journal**: IET Image Processing 2022
- **Summary**: Unsupervised video person re-identification (reID) methods usually depend on global-level features. And many supervised reID methods employed local-level features and achieved significant performance improvements. However, applying local-level features to unsupervised methods may introduce an unstable performance. To improve the performance stability for unsupervised video reID, this paper introduces a general scheme fusing part models and unsupervised learning. In this scheme, the global-level feature is divided into equal local-level feature. A local-aware module is employed to explore the poentials of local-level feature for unsupervised learning. A global-aware module is proposed to overcome the disadvantages of local-level features. Features from these two modules are fused to form a robust feature representation for each input image. This feature representation has the advantages of local-level feature without suffering from its disadvantages. Comprehensive experiments are conducted on three benchmarks, including PRID2011, iLIDS-VID, and DukeMTMC-VideoReID, and the results demonstrate that the proposed approach achieves state-of-the-art performance. Extensive ablation studies demonstrate the effectiveness and robustness of proposed scheme, local-aware module and global-aware module. The code and generated features are available at https://github.com/deropty/uPMnet.



### Does Thermal data make the detection systems more reliable?
- **Arxiv ID**: http://arxiv.org/abs/2111.05191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05191v1)
- **Published**: 2021-11-09 15:04:34+00:00
- **Updated**: 2021-11-09 15:04:34+00:00
- **Authors**: Shruthi Gowda, Bahram Zonooz, Elahe Arani
- **Comment**: Accepted at NeurIPS 2021 - ML4AD workshop (The code for this research
  is available at: https://github.com/NeurAI-Lab/MMC)
- **Journal**: None
- **Summary**: Deep learning-based detection networks have made remarkable progress in autonomous driving systems (ADS). ADS should have reliable performance across a variety of ambient lighting and adverse weather conditions. However, luminance degradation and visual obstructions (such as glare, fog) result in poor quality images by the visual camera which leads to performance decline. To overcome these challenges, we explore the idea of leveraging a different data modality that is disparate yet complementary to the visual data. We propose a comprehensive detection system based on a multimodal-collaborative framework that learns from both RGB (from visual cameras) and thermal (from Infrared cameras) data. This framework trains two networks collaboratively and provides flexibility in learning optimal features of its own modality while also incorporating the complementary knowledge of the other. Our extensive empirical results show that while the improvement in accuracy is nominal, the value lies in challenging and extremely difficult edge cases which is crucial in safety-critical applications such as AD. We provide a holistic view of both merits and limitations of using a thermal imaging system in detection.



### Early Myocardial Infarction Detection over Multi-view Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2111.05790v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.05790v3)
- **Published**: 2021-11-09 15:36:10+00:00
- **Updated**: 2023-02-26 13:32:36+00:00
- **Authors**: Aysen Degerli, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Myocardial infarction (MI) is the leading cause of mortality in the world that occurs due to a blockage of the coronary arteries feeding the myocardium. An early diagnosis of MI and its localization can mitigate the extent of myocardial damage by facilitating early therapeutic interventions. Following the blockage of a coronary artery, the regional wall motion abnormality (RWMA) of the ischemic myocardial segments is the earliest change to set in. Echocardiography is the fundamental tool to assess any RWMA. Assessing the motion of the left ventricle (LV) wall only from a single echocardiography view may lead to missing the diagnosis of MI as the RWMA may not be visible on that specific view. Therefore, in this study, we propose to fuse apical 4-chamber (A4C) and apical 2-chamber (A2C) views in which a total of 12 myocardial segments can be analyzed for MI detection. The proposed method first estimates the motion of the LV wall by Active Polynomials (APs), which extract and track the endocardial boundary to compute myocardial segment displacements. The features are extracted from the A4C and A2C view displacements, which are concatenated and fed into the classifiers to detect MI. The main contributions of this study are 1) creation of a new benchmark dataset by including both A4C and A2C views in a total of 260 echocardiography recordings, which is publicly shared with the research community, 2) improving the performance of the prior work of threshold-based APs by a Machine Learning based approach, and 3) a pioneer MI detection approach via multi-view echocardiography by fusing the information of A4C and A2C views. Experimental results show that the proposed method achieves 90.91% sensitivity and 86.36% precision for MI detection over multi-view echocardiography. The software implementation is shared at https://github.com/degerliaysen/MultiEchoAI.



### Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.05222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05222v1)
- **Published**: 2021-11-09 16:01:56+00:00
- **Updated**: 2021-11-09 16:01:56+00:00
- **Authors**: Gnana Praveen R, Eric Granger, Patrick Cardinal
- **Comment**: Accepted in FG2021
- **Journal**: None
- **Summary**: Multimodal analysis has recently drawn much interest in affective computing, since it can improve the overall accuracy of emotion recognition over isolated uni-modal approaches. The most effective techniques for multimodal emotion recognition efficiently leverage diverse and complimentary sources of information, such as facial, vocal, and physiological modalities, to provide comprehensive feature representations. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos, where complex spatiotemporal relationships may be captured. Most of the existing fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complimentary nature of audio-visual (A-V) modalities. We introduce a cross-attentional fusion approach to extract the salient features across A-V modalities, allowing for accurate prediction of continuous values of valence and arousal. Our new cross-attentional A-V fusion model efficiently leverages the inter-modal relationships. In particular, it computes cross-attention weights to focus on the more contributive features across individual modalities, and thereby combine contributive feature representations, which are then fed to fully connected layers for the prediction of valence and arousal. The effectiveness of the proposed approach is validated experimentally on videos from the RECOLA and Fatigue (private) data-sets. Results indicate that our cross-attentional A-V fusion model is a cost-effective approach that outperforms state-of-the-art fusion approaches. Code is available: \url{https://github.com/praveena2j/Cross-Attentional-AV-Fusion}



### Leveraging blur information for plenoptic camera calibration
- **Arxiv ID**: http://arxiv.org/abs/2111.05226v1
- **DOI**: 10.1007/s11263-022-01582-z
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05226v1)
- **Published**: 2021-11-09 16:07:07+00:00
- **Updated**: 2021-11-09 16:07:07+00:00
- **Authors**: Mathieu LabussiÃ¨re, CÃ©line TeuliÃ¨re, FrÃ©dÃ©ric Bernardin, Omar Ait-Aider
- **Comment**: arXiv admin note: text overlap with arXiv:2004.07745
- **Journal**: None
- **Summary**: This paper presents a novel calibration algorithm for plenoptic cameras, especially the multi-focus configuration, where several types of micro-lenses are used, using raw images only. Current calibration methods rely on simplified projection models, use features from reconstructed images, or require separated calibrations for each type of micro-lens. In the multi-focus configuration, the same part of a scene will demonstrate different amounts of blur according to the micro-lens focal length. Usually, only micro-images with the smallest amount of blur are used. In order to exploit all available data, we propose to explicitly model the defocus blur in a new camera model with the help of our newly introduced Blur Aware Plenoptic (BAP) feature. First, it is used in a pre-calibration step that retrieves initial camera parameters, and second, to express a new cost function to be minimized in our single optimization process. Third, it is exploited to calibrate the relative blur between micro-images. It links the geometric blur, i.e., the blur circle, to the physical blur, i.e., the point spread function. Finally, we use the resulting blur profile to characterize the camera's depth of field. Quantitative evaluations in controlled environment on real-world data demonstrate the effectiveness of our calibrations.



### FILIP: Fine-grained Interactive Language-Image Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2111.07783v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07783v1)
- **Published**: 2021-11-09 17:15:38+00:00
- **Updated**: 2021-11-09 17:15:38+00:00
- **Authors**: Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.



### Unsupervised Spiking Instance Segmentation on Event Data using STDP
- **Arxiv ID**: http://arxiv.org/abs/2111.05283v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05283v2)
- **Published**: 2021-11-09 17:26:17+00:00
- **Updated**: 2021-11-12 16:16:55+00:00
- **Authors**: Paul Kirkland, Davide L. Manna, Alex Vicente-Sola, Gaetano Di Caterina
- **Comment**: 20 Pages, 13 Figures
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNN) and the field of Neuromorphic Engineering has brought about a paradigm shift in how to approach Machine Learning (ML) and Computer Vision (CV) problem. This paradigm shift comes from the adaption of event-based sensing and processing. An event-based vision sensor allows for sparse and asynchronous events to be produced that are dynamically related to the scene. Allowing not only the spatial information but a high-fidelity of temporal information to be captured. Meanwhile avoiding the extra overhead and redundancy of conventional high frame rate approaches. However, with this change in paradigm, many techniques from traditional CV and ML are not applicable to these event-based spatial-temporal visual streams. As such a limited number of recognition, detection and segmentation approaches exist. In this paper, we present a novel approach that can perform instance segmentation using just the weights of a Spike Time Dependent Plasticity trained Spiking Convolutional Neural Network that was trained for object recognition. This exploits the spatial and temporal aspects of the network's internal feature representations adding this new discriminative capability. We highlight the new capability by successfully transforming a single class unsupervised network for face detection into a multi-person face recognition and instance segmentation network.



### Sliced Recursive Transformer
- **Arxiv ID**: http://arxiv.org/abs/2111.05297v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05297v3)
- **Published**: 2021-11-09 17:59:14+00:00
- **Updated**: 2022-07-26 06:19:42+00:00
- **Authors**: Zhiqiang Shen, Zechun Liu, Eric Xing
- **Comment**: ECCV 2022, 31 pages with Appendix. Code and models are available at
  https://github.com/szq0214/SReT (v3: update license and fix arxiv timestamp)
- **Journal**: None
- **Summary**: We present a neat yet effective recursive operation on vision transformers that can improve parameter utilization without involving additional parameters. This is achieved by sharing weights across the depth of transformer networks. The proposed method can obtain a substantial gain (~2%) simply using naive recursive operation, requires no special or sophisticated knowledge for designing principles of networks, and introduces minimal computational overhead to the training procedure. To reduce the additional computation caused by recursive operation while maintaining the superior accuracy, we propose an approximating method through multiple sliced group self-attentions across recursive layers which can reduce the cost consumption by 10~30% with minimal performance loss. We call our model Sliced Recursive Transformer (SReT), a novel and parameter-efficient vision transformer design that is compatible with a broad range of other designs for efficient ViT architectures. Our best model establishes significant improvement on ImageNet-1K over state-of-the-art methods while containing fewer parameters. The proposed weight sharing mechanism by sliced recursion structure allows us to build a transformer with more than 100 or even 1000 shared layers with ease while keeping a compact size (13~15M), to avoid optimization difficulties when the model is too large. The flexible scalability has shown great potential for scaling up models and constructing extremely deep vision transformers. Code is available at https://github.com/szq0214/SReT.



### Using The Feedback of Dynamic Active-Pixel Vision Sensor (Davis) to Prevent Slip in Real Time
- **Arxiv ID**: http://arxiv.org/abs/2111.05308v1
- **DOI**: 10.1109/ICMRE49073.2020.9065017
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05308v1)
- **Published**: 2021-11-09 18:25:39+00:00
- **Updated**: 2021-11-09 18:25:39+00:00
- **Authors**: Armin Masoumian, Pezhman kazemi, Mohammad Chehreghani Montazer, Hatem A. Rashwan, Domenec Puig Valls
- **Comment**: 5 pages, Accepted for The 6th International Conference on
  Mechatronics and Robotics Engineering (ICMRE 2020)
- **Journal**: None
- **Summary**: The objective of this paper is to describe an approach to detect the slip and contact force in real-time feedback. In this novel approach, the DAVIS camera is used as a vision tactile sensor due to its fast process speed and high resolution. Two hundred experiments were performed on four objects with different shapes, sizes, weights, and materials to compare the accuracy and response of the Baxter robot grippers to avoid slipping. The advanced approach is validated by using a force-sensitive resistor (FSR402). The events captured with the DAVIS camera are processed with specific algorithms to provide feedback to the Baxter robot aiding it to detect the slip.



### Designing and Analyzing the PID and Fuzzy Control System for an Inverted Pendulum
- **Arxiv ID**: http://arxiv.org/abs/2111.05309v1
- **DOI**: 10.1109/ICMRE49073.2020.9065161
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05309v1)
- **Published**: 2021-11-09 18:26:23+00:00
- **Updated**: 2021-11-09 18:26:23+00:00
- **Authors**: Armin Masoumian, Pezhman kazemi, Mohammad Chehreghani Montazer, Hatem A. Rashwan, Domenec Puig Valls
- **Comment**: 5 pages, Accepted for The 6th International Conference on
  Mechatronics and Robotics Engineering (ICMRE 2020)
- **Journal**: None
- **Summary**: The inverted pendulum is a non-linear unbalanced system that needs to be controlled using motors to achieve stability and equilibrium. The inverted pendulum is constructed with Lego and using the Lego Mindstorm NXT, which is a programmable robot capable of completing many different functions. In this paper, an initial design of the inverted pendulum is proposed and the performance of different sensors, which are compatible with the Lego Mindstorm NXT was studied. Furthermore, the ability of computer vision to achieve the stability required to maintain the system is also investigated. The inverted pendulum is a conventional cart that can be controlled using a Fuzzy Logic controller that produces a self-tuning PID control for the cart to move on. The fuzzy logic and PID are simulated in MATLAB and Simulink, and the program for the robot is developed in the LabVIEW software.



### Monocular Human Shape and Pose with Dense Mesh-borne Local Image Features
- **Arxiv ID**: http://arxiv.org/abs/2111.05319v3
- **DOI**: 10.1109/FG52635.2021.9666993
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05319v3)
- **Published**: 2021-11-09 18:43:18+00:00
- **Updated**: 2021-11-11 08:38:08+00:00
- **Authors**: Shubhendu Jena, Franck Multon, Adnane Boukhayma
- **Comment**: FG 2021
- **Journal**: None
- **Summary**: We propose to improve on graph convolution based approaches for human shape and pose estimation from monocular input, using pixel-aligned local image features. Given a single input color image, existing graph convolutional network (GCN) based techniques for human shape and pose estimation use a single convolutional neural network (CNN) generated global image feature appended to all mesh vertices equally to initialize the GCN stage, which transforms a template T-posed mesh into the target pose. In contrast, we propose for the first time the idea of using local image features per vertex. These features are sampled from the CNN image feature maps by utilizing pixel-to-mesh correspondences generated with DensePose. Our quantitative and qualitative results on standard benchmarks show that using local features improves on global ones and leads to competitive performances with respect to the state-of-the-art.



### Data Augmentation Can Improve Robustness
- **Arxiv ID**: http://arxiv.org/abs/2111.05328v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.05328v1)
- **Published**: 2021-11-09 18:57:00+00:00
- **Updated**: 2021-11-09 18:57:00+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, Timothy Mann
- **Comment**: Accepted at NeurIPS 2021. arXiv admin note: substantial text overlap
  with arXiv:2103.01946; text overlap with arXiv:2110.09468
- **Journal**: None
- **Summary**: Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Furthermore, we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$, our model reaches 60.07% robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.



### Object-Centric Representation Learning with Generative Spatial-Temporal Factorization
- **Arxiv ID**: http://arxiv.org/abs/2111.05393v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05393v1)
- **Published**: 2021-11-09 20:04:16+00:00
- **Updated**: 2021-11-09 20:04:16+00:00
- **Authors**: Li Nanbo, Muhammad Ahmed Raza, Hu Wenbin, Zhaole Sun, Robert B. Fisher
- **Comment**: Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: Learning object-centric scene representations is essential for attaining structural understanding and abstraction of complex scenes. Yet, as current approaches for unsupervised object-centric representation learning are built upon either a stationary observer assumption or a static scene assumption, they often: i) suffer single-view spatial ambiguities, or ii) infer incorrectly or inaccurately object representations from dynamic scenes. To address this, we propose Dynamics-aware Multi-Object Network (DyMON), a method that broadens the scope of multi-view object-centric representation learning to dynamic scenes. We train DyMON on multi-view-dynamic-scene data and show that DyMON learns -- without supervision -- to factorize the entangled effects of observer motions and scene object dynamics from a sequence of observations, and constructs scene object spatial representations suitable for rendering at arbitrary times (querying across time) and from arbitrary viewpoints (querying across space). We also show that the factorized scene representations (w.r.t. objects) support querying about a single object by space and time independently.



### Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity
- **Arxiv ID**: http://arxiv.org/abs/2111.05329v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05329v5)
- **Published**: 2021-11-09 20:24:19+00:00
- **Updated**: 2022-11-25 04:41:38+00:00
- **Authors**: Pritam Sarkar, Ali Etemad
- **Comment**: Accepted in AAAI 2023
- **Journal**: None
- **Summary**: We present CrissCross, a self-supervised framework for learning audio-visual representations. A novel notion is introduced in our framework whereby in addition to learning the intra-modal and standard 'synchronous' cross-modal relations, CrissCross also learns 'asynchronous' cross-modal relationships. We perform in-depth studies showing that by relaxing the temporal synchronicity between the audio and visual modalities, the network learns strong generalized representations useful for a variety of downstream tasks. To pretrain our proposed solution, we use 3 different datasets with varying sizes, Kinetics-Sound, Kinetics400, and AudioSet. The learned representations are evaluated on a number of downstream tasks namely action recognition, sound classification, and action retrieval. Our experiments show that CrissCross either outperforms or achieves performances on par with the current state-of-the-art self-supervised methods on action recognition and action retrieval with UCF101 and HMDB51, as well as sound classification with ESC50 and DCASE. Moreover, CrissCross outperforms fully-supervised pretraining while pretrained on Kinetics-Sound. The codes and pretrained models are available on the project website.



### Robust deep learning-based semantic organ segmentation in hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2111.05408v2
- **DOI**: 10.1016/j.media.2022.102488
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.10; I.4.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2111.05408v2)
- **Published**: 2021-11-09 20:37:38+00:00
- **Updated**: 2022-07-10 09:41:44+00:00
- **Authors**: Silvia Seidlitz, Jan Sellner, Jan Odenthal, Berkin Ãzdemir, Alexander Studier-Fischer, Samuel KnÃ¶dler, Leonardo Ayala, Tim J. Adler, Hannes G. Kenngott, Minu Tizabi, Martin Wagner, Felix Nickel, Beat P. MÃ¼ller-Stich, Lena Maier-Hein
- **Comment**: The first two authors (Silvia Seidlitz and Jan Sellner) contributed
  equally to this paper
- **Journal**: Medical Image Analysis, Volume 80, 2022, 102488, ISSN 1361-8415
- **Summary**: Semantic image segmentation is an important prerequisite for context-awareness and autonomous robotics in surgery. The state of the art has focused on conventional RGB video data acquired during minimally invasive surgery, but full-scene semantic segmentation based on spectral imaging data and obtained during open surgery has received almost no attention to date. To address this gap in the literature, we are investigating the following research questions based on hyperspectral imaging (HSI) data of pigs acquired in an open surgery setting: (1) What is an adequate representation of HSI data for neural network-based fully automated organ segmentation, especially with respect to the spatial granularity of the data (pixels vs. superpixels vs. patches vs. full images)? (2) Is there a benefit of using HSI data compared to other modalities, namely RGB data and processed HSI data (e.g. tissue parameters like oxygenation), when performing semantic organ segmentation? According to a comprehensive validation study based on 506 HSI images from 20 pigs, annotated with a total of 19 classes, deep learning-based segmentation performance increases, consistently across modalities, with the spatial context of the input data. Unprocessed HSI data offers an advantage over RGB data or processed data from the camera provider, with the advantage increasing with decreasing size of the input to the neural network. Maximum performance (HSI applied to whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04), which is in the range of the inter-rater variability (DSC of 0.89 ((standard deviation (SD)) 0.07)). We conclude that HSI could become a powerful image modality for fully-automatic surgical scene understanding with many advantages over traditional imaging, including the ability to recover additional functional tissue information. Code and pre-trained models: https://github.com/IMSY-DKFZ/htc.



### Pipeline for 3D reconstruction of the human body from AR/VR headset mounted egocentric cameras
- **Arxiv ID**: http://arxiv.org/abs/2111.05409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.05409v1)
- **Published**: 2021-11-09 20:38:32+00:00
- **Updated**: 2021-11-09 20:38:32+00:00
- **Authors**: Shivam Grover, Kshitij Sidana, Vanita Jain
- **Comment**: 11 pages, 12 figures and 2 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel pipeline for the 3D reconstruction of the full body from egocentric viewpoints. 3-D reconstruction of the human body from egocentric viewpoints is a challenging task as the view is skewed and the body parts farther from the cameras are occluded. One such example is the view from cameras installed below VR headsets. To achieve this task, we first make use of conditional GANs to translate the egocentric views to full body third-person views. This increases the comprehensibility of the image and caters to occlusions. The generated third-person view is further sent through the 3D reconstruction module that generates a 3D mesh of the body. We also train a network that can take the third person full-body view of the subject and generate the texture maps for applying on the mesh. The generated mesh has fairly realistic body proportions and is fully rigged allowing for further applications such as real-time animation and pose transfer in games. This approach can be key to a new domain of mobile human telepresence.



### Efficient Data Compression for 3D Sparse TPC via Bicephalous Convolutional Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2111.05423v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05423v1)
- **Published**: 2021-11-09 21:26:37+00:00
- **Updated**: 2021-11-09 21:26:37+00:00
- **Authors**: Yi Huang, Yihui Ren, Shinjae Yoo, Jin Huang
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Real-time data collection and analysis in large experimental facilities present a great challenge across multiple domains, including high energy physics, nuclear physics, and cosmology. To address this, machine learning (ML)-based methods for real-time data compression have drawn significant attention. However, unlike natural image data, such as CIFAR and ImageNet that are relatively small-sized and continuous, scientific data often come in as three-dimensional data volumes at high rates with high sparsity (many zeros) and non-Gaussian value distribution. This makes direct application of popular ML compression methods, as well as conventional data compression methods, suboptimal. To address these obstacles, this work introduces a dual-head autoencoder to resolve sparsity and regression simultaneously, called \textit{Bicephalous Convolutional AutoEncoder} (BCAE). This method shows advantages both in compression fidelity and ratio compared to traditional data compression methods, such as MGARD, SZ, and ZFP. To achieve similar fidelity, the best performer among the traditional methods can reach only half the compression ratio of BCAE. Moreover, a thorough ablation study of the BCAE method shows that a dedicated segmentation decoder improves the reconstruction.



### Towards Active Vision for Action Localization with Reactive Control and Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.05448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05448v1)
- **Published**: 2021-11-09 23:16:55+00:00
- **Updated**: 2021-11-09 23:16:55+00:00
- **Authors**: Shubham Trehan, Sathyanarayanan N. Aakur
- **Comment**: To appear at WACV 2022
- **Journal**: None
- **Summary**: Visual event perception tasks such as action localization have primarily focused on supervised learning settings under a static observer, i.e., the camera is static and cannot be controlled by an algorithm. They are often restricted by the quality, quantity, and diversity of \textit{annotated} training data and do not often generalize to out-of-domain samples. In this work, we tackle the problem of active action localization where the goal is to localize an action while controlling the geometric and physical parameters of an active camera to keep the action in the field of view without training data. We formulate an energy-based mechanism that combines predictive learning and reactive control to perform active action localization without rewards, which can be sparse or non-existent in real-world environments. We perform extensive experiments in both simulated and real-world environments on two tasks - active object tracking and active action localization. We demonstrate that the proposed approach can generalize to different tasks and environments in a streaming fashion, without explicit rewards or training. We show that the proposed approach outperforms unsupervised baselines and obtains competitive performance compared to those trained with reinforcement learning.




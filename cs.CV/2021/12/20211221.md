# Arxiv Papers in cs.CV on 2021-12-21
### Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion
- **Arxiv ID**: http://arxiv.org/abs/2112.10936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.CR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.10936v2)
- **Published**: 2021-12-21 01:57:04+00:00
- **Updated**: 2022-12-02 04:08:28+00:00
- **Authors**: Shruti Agarwal, Liwen Hu, Evonne Ng, Trevor Darrell, Hao Li, Anna Rohrbach
- **Comment**: Accepted in WACV 2023
- **Journal**: None
- **Summary**: In today's era of digital misinformation, we are increasingly faced with new threats posed by video falsification techniques. Such falsifications range from cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g., sophisticated AI media synthesis methods), which are becoming perceptually indistinguishable from real videos. To tackle this challenge, we propose a multi-modal semantic forensic approach to discover clues that go beyond detecting discrepancies in visual quality, thereby handling both simpler cheapfakes and visually persuasive deepfakes. In this work, our goal is to verify that the purported person seen in the video is indeed themselves by detecting anomalous facial movements corresponding to the spoken words. We leverage the idea of attribution to learn person-specific biometric patterns that distinguish a given speaker from others. We use interpretable Action Units (AUs) to capture a person's face and head movement as opposed to deep CNN features, and we are the first to use word-conditioned facial motion analysis. We further demonstrate our method's effectiveness on a range of fakes not seen in training including those without video manipulation, that were not addressed in prior work.



### Structured Semantic Transfer for Multi-Label Recognition with Partial Labels
- **Arxiv ID**: http://arxiv.org/abs/2112.10941v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10941v3)
- **Published**: 2021-12-21 02:15:01+00:00
- **Updated**: 2022-03-04 07:58:47+00:00
- **Authors**: Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, Liang Lin
- **Comment**: Accepted by AAAI'22
- **Journal**: None
- **Summary**: Multi-label image recognition is a fundamental yet practical task because real-world images inherently possess multiple semantic labels. However, it is difficult to collect large-scale multi-label annotations due to the complexity of both the input images and output label spaces. To reduce the annotation cost, we propose a structured semantic transfer (SST) framework that enables training multi-label recognition models with partial labels, i.e., merely some labels are known while other labels are missing (also called unknown labels) per image. The framework consists of two complementary transfer modules that explore within-image and cross-image semantic correlations to transfer knowledge of known labels to generate pseudo labels for unknown labels. Specifically, an intra-image semantic transfer module learns image-specific label co-occurrence matrix and maps the known labels to complement unknown labels based on this matrix. Meanwhile, a cross-image transfer module learns category-specific feature similarities and helps complement unknown labels with high similarities. Finally, both known and generated labels are used to train the multi-label recognition models. Extensive experiments on the Microsoft COCO, Visual Genome and Pascal VOC datasets show that the proposed SST framework obtains superior performance over current state-of-the-art algorithms. Codes are available at https://github.com/HCPLab-SYSU/HCP-MLR-PL.



### Pixel-Stega: Generative Image Steganography Based on Autoregressive Models
- **Arxiv ID**: http://arxiv.org/abs/2112.10945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10945v1)
- **Published**: 2021-12-21 02:34:33+00:00
- **Updated**: 2021-12-21 02:34:33+00:00
- **Authors**: Siyu Zhang, Zhongliang Yang, Haoqin Tu, Jinshuai Yang, Yongfeng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In this letter, we explored generative image steganography based on autoregressive models. We proposed Pixel-Stega, which implements pixel-level information hiding with autoregressive models and arithmetic coding algorithm. Firstly, one of the autoregressive models, PixelCNN++, is utilized to produce explicit conditional probability distribution of each pixel. Secondly, secret messages are encoded to the selection of pixels through steganographic sampling (stegosampling) based on arithmetic coding. We carried out qualitative and quantitative assessment on gray-scale and colour image datasets. Experimental results show that Pixel-Stega is able to embed secret messages adaptively according to the entropy of the pixels to achieve both high embedding capacity (up to 4.3 bpp) and nearly perfect imperceptibility (about 50% detection accuracy).



### Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems
- **Arxiv ID**: http://arxiv.org/abs/2112.10948v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.10948v1)
- **Published**: 2021-12-21 02:44:49+00:00
- **Updated**: 2021-12-21 02:44:49+00:00
- **Authors**: Xu Kang, Bin Song, Jie Guo, Zhijin Qin, F. Richard Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The vigorous developments of Internet of Things make it possible to extend its computing and storage capabilities to computing tasks in the aerial system with collaboration of cloud and edge, especially for artificial intelligence (AI) tasks based on deep learning (DL). Collecting a large amount of image/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent analysis tasks to the back-end mobile edge computing (MEC) server due to their limited storage and computing capabilities. How to efficiently transmit the most correlated information for the AI model is a challenging topic. Inspired by the task-oriented communication in recent years, we propose a new aerial image transmission paradigm for the scene classification task. A lightweight model is developed on the front-end UAV for semantic blocks transmission with perception of images and channel conditions. In order to achieve the tradeoff between transmission latency and classification accuracy, deep reinforcement learning (DRL) is used to explore the semantic blocks which have the best contribution to the back-end classifier under various channel conditions. Experimental results show that the proposed method can significantly improve classification accuracy compared to the fixed transmission strategy and traditional content perception methods.



### Continuous-Time Video Generation via Learning Motion Dynamics with Neural ODE
- **Arxiv ID**: http://arxiv.org/abs/2112.10960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.10960v1)
- **Published**: 2021-12-21 03:30:38+00:00
- **Updated**: 2021-12-21 03:30:38+00:00
- **Authors**: Kangyeol Kim, Sunghyun Park, Junsoo Lee, Joonseok Lee, Sookyung Kim, Jaegul Choo, Edward Choi
- **Comment**: 24 pages; Accepted to BMVC 2021
- **Journal**: None
- **Summary**: In order to perform unconditional video generation, we must learn the distribution of the real-world videos. In an effort to synthesize high-quality videos, various studies attempted to learn a mapping function between noise and videos, including recent efforts to separate motion distribution and appearance distribution. Previous methods, however, learn motion dynamics in discretized, fixed-interval timesteps, which is contrary to the continuous nature of motion of a physical body. In this paper, we propose a novel video generation approach that learns separate distributions for motion and appearance, the former modeled by neural ODE to learn natural motion dynamics. Specifically, we employ a two-stage approach where the first stage converts a noise vector to a sequence of keypoints in arbitrary frame rates, and the second stage synthesizes videos based on the given keypoints sequence and the appearance noise vector. Our model not only quantitatively outperforms recent baselines for video generation, but also demonstrates versatile functionality such as dynamic frame rate manipulation and motion transfer between two datasets, thus opening new doors to diverse video generation applications.



### Nonlinear Transform Source-Channel Coding for Semantic Communications
- **Arxiv ID**: http://arxiv.org/abs/2112.10961v3
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2112.10961v3)
- **Published**: 2021-12-21 03:30:46+00:00
- **Updated**: 2022-11-02 06:34:05+00:00
- **Authors**: Jincheng Dai, Sixian Wang, Kailin Tan, Zhongwei Si, Xiaoqi Qin, Kai Niu, Ping Zhang
- **Comment**: published in IEEE JSAC
- **Journal**: None
- **Summary**: In this paper, we propose a class of high-efficiency deep joint source-channel coding methods that can closely adapt to the source distribution under the nonlinear transform, it can be collected under the name nonlinear transform source-channel coding (NTSCC). In the considered model, the transmitter first learns a nonlinear analysis transform to map the source data into latent space, then transmits the latent representation to the receiver via deep joint source-channel coding. Our model incorporates the nonlinear transform as a strong prior to effectively extract the source semantic features and provide side information for source-channel coding. Unlike existing conventional deep joint source-channel coding methods, the proposed NTSCC essentially learns both the source latent representation and an entropy model as the prior on the latent representation. Accordingly, novel adaptive rate transmission and hyperprior-aided codec refinement mechanisms are developed to upgrade deep joint source-channel coding. The whole system design is formulated as an optimization problem whose goal is to minimize the end-to-end transmission rate-distortion performance under established perceptual quality metrics. Across test image sources with various resolutions, we find that the proposed NTSCC transmission method generally outperforms both the analog transmission using the standard deep joint source-channel coding and the classical separation-based digital transmission. Notably, the proposed NTSCC method can potentially support future semantic communications due to its content-aware ability and perceptual optimization goal.



### DRPN: Making CNN Dynamically Handle Scale Variation
- **Arxiv ID**: http://arxiv.org/abs/2112.10963v2
- **DOI**: 10.1016/j.dsp.2022.103844
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10963v2)
- **Published**: 2021-12-21 03:38:58+00:00
- **Updated**: 2022-09-02 07:23:22+00:00
- **Authors**: Jingchao Peng, Haitao Zhao, Zhengwei Hu, Kaijie Zhao, Zhongze Wang
- **Comment**: None
- **Journal**: Digit. Signal Process, 133 (2023), pp. 103844
- **Summary**: Based on our observations of infrared targets, serious scale variation along within sequence frames has high-frequently occurred. In this paper, we propose a dynamic re-parameterization network (DRPN) to deal with the scale variation and balance the detection precision between small targets and large targets in infrared datasets. DRPN adopts the multiple branches with different sizes of convolution kernels and the dynamic convolution strategy. Multiple branches with different sizes of convolution kernels have different sizes of receptive fields. Dynamic convolution strategy makes DRPN adaptively weight multiple branches. DRPN can dynamically adjust the receptive field according to the scale variation of the target. Besides, in order to maintain effective inference in the test phase, the multi-branch structure is further converted to a single-branch structure via the re-parameterization technique after training. Extensive experiments on FLIR, KAIST, and InfraPlane datasets demonstrate the effectiveness of our proposed DRPN. The experimental results show that detectors using the proposed DRPN as the basic structure rather than SKNet or TridentNet obtained the best performances.



### Generalizing Interactive Backpropagating Refinement for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.10969v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.10969v2)
- **Published**: 2021-12-21 03:52:08+00:00
- **Updated**: 2021-12-22 11:07:46+00:00
- **Authors**: Fanqing Lin, Brian Price, Tony Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: As deep neural networks become the state-of-the-art approach in the field of computer vision for dense prediction tasks, many methods have been developed for automatic estimation of the target outputs given the visual inputs. Although the estimation accuracy of the proposed automatic methods continues to improve, interactive refinement is oftentimes necessary for further correction. Recently, feature backpropagating refinement scheme (f-BRS) has been proposed for the task of interactive segmentation, which enables efficient optimization of a small set of auxiliary variables inserted into the pretrained network to produce object segmentation that better aligns with user inputs. However, the proposed auxiliary variables only contain channel-wise scale and bias, limiting the optimization to global refinement only. In this work, in order to generalize backpropagating refinement for a wide range of dense prediction tasks, we introduce a set of G-BRS (Generalized Backpropagating Refinement Scheme) layers that enable both global and localized refinement for the following tasks: interactive segmentation, semantic segmentation, image matting and monocular depth estimation. Experiments on SBD, Cityscapes, Mapillary Vista, Composition-1k and NYU-Depth-V2 show that our method can successfully generalize and significantly improve performance of existing pretrained state-of-the-art models with only a few clicks.



### ACGNet: Action Complement Graph Network for Weakly-supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2112.10977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10977v1)
- **Published**: 2021-12-21 04:18:44+00:00
- **Updated**: 2021-12-21 04:18:44+00:00
- **Authors**: Zichen Yang, Jie Qin, Di Huang
- **Comment**: Accepted to AAAI 2022
- **Journal**: None
- **Summary**: Weakly-supervised temporal action localization (WTAL) in untrimmed videos has emerged as a practical but challenging task since only video-level labels are available. Existing approaches typically leverage off-the-shelf segment-level features, which suffer from spatial incompleteness and temporal incoherence, thus limiting their performance. In this paper, we tackle this problem from a new perspective by enhancing segment-level representations with a simple yet effective graph convolutional network, namely action complement graph network (ACGNet). It facilitates the current video segment to perceive spatial-temporal dependencies from others that potentially convey complementary clues, implicitly mitigating the negative effects caused by the two issues above. By this means, the segment-level features are more discriminative and robust to spatial-temporal variations, contributing to higher localization accuracies. More importantly, the proposed ACGNet works as a universal module that can be flexibly plugged into different WTAL frameworks, while maintaining the end-to-end training fashion. Extensive experiments are conducted on the THUMOS'14 and ActivityNet1.2 benchmarks, where the state-of-the-art results clearly demonstrate the superiority of the proposed approach.



### Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/2112.10982v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10982v3)
- **Published**: 2021-12-21 04:44:57+00:00
- **Updated**: 2022-03-24 15:49:07+00:00
- **Authors**: Josh Myers-Dean, Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari
- **Comment**: Includes supplementary materials
- **Journal**: None
- **Summary**: Generalized few-shot semantic segmentation was introduced to move beyond only evaluating few-shot segmentation models on novel classes to include testing their ability to remember base classes. While the current state-of-the-art approach is based on meta-learning, it performs poorly and saturates in learning after observing only a few shots. We propose the first fine-tuning solution, and demonstrate that it addresses the saturation problem while achieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We also show that it outperforms existing methods, whether fine-tuning multiple final layers or only the final layer. Finally, we present a triplet loss regularization that shows how to redistribute the balance of performance between novel and base categories so that there is a smaller gap between them.



### Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2112.10985v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.10985v1)
- **Published**: 2021-12-21 05:07:54+00:00
- **Updated**: 2021-12-21 05:07:54+00:00
- **Authors**: Ziang Li, Kailun Wu, Yiwen Guo, Changshui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The learned iterative shrinkage thresholding algorithm (LISTA) introduces deep unfolding models with learnable thresholds in some shrinkage functions for sparse coding. Drawing on some theoretical insights, we advocate an error-based thresholding (EBT) mechanism for LISTA, which leverages a function of the layer-wise reconstruction error to suggest an appropriate threshold value for each observation on each layer. We show that the EBT mechanism well disentangles the learnable parameters in the shrinkage functions from the reconstruction errors, making them more adaptive to the various observations. With rigorous theoretical analyses, we show that the proposed EBT can lead to a faster convergence on the basis of LISTA and its variants, in addition to its higher adaptivity. Extensive experimental results confirm our theoretical analyses and verify the effectiveness of our methods.



### Mapping industrial poultry operations at scale with deep learning and aerial imagery
- **Arxiv ID**: http://arxiv.org/abs/2112.10988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.10988v1)
- **Published**: 2021-12-21 05:11:39+00:00
- **Updated**: 2021-12-21 05:11:39+00:00
- **Authors**: Caleb Robinson, Ben Chugg, Brandon Anderson, Juan M. Lavista Ferres, Daniel E. Ho
- **Comment**: None
- **Journal**: None
- **Summary**: Concentrated Animal Feeding Operations (CAFOs) pose serious risks to air, water, and public health, but have proven to be challenging to regulate. The U.S. Government Accountability Office notes that a basic challenge is the lack of comprehensive location information on CAFOs. We use the USDA's National Agricultural Imagery Program (NAIP) 1m/pixel aerial imagery to detect poultry CAFOs across the continental United States. We train convolutional neural network (CNN) models to identify individual poultry barns and apply the best performing model to over 42 TB of imagery to create the first national, open-source dataset of poultry CAFOs. We validate the model predictions against held-out validation set on poultry CAFO facility locations from 10 hand-labeled counties in California and demonstrate that this approach has significant potential to fill gaps in environmental monitoring.



### Explainable Medical Imaging AI Needs Human-Centered Design: Guidelines and Evidence from a Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2112.12596v4
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.12596v4)
- **Published**: 2021-12-21 05:14:44+00:00
- **Updated**: 2022-09-29 19:26:31+00:00
- **Authors**: Haomin Chen, Catalina Gomez, Chien-Ming Huang, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: Transparency in Machine Learning (ML), attempts to reveal the working mechanisms of complex models. Transparent ML promises to advance human factors engineering goals of human-centered AI in the target users. From a human-centered design perspective, transparency is not a property of the ML model but an affordance, i.e. a relationship between algorithm and user; as a result, iterative prototyping and evaluation with users is critical to attaining adequate solutions that afford transparency. However, following human-centered design principles in healthcare and medical image analysis is challenging due to the limited availability of and access to end users. To investigate the state of transparent ML in medical image analysis, we conducted a systematic review of the literature. Our review reveals multiple severe shortcomings in the design and validation of transparent ML for medical image analysis applications. We find that most studies to date approach transparency as a property of the model itself, similar to task performance, without considering end users during neither development nor evaluation. Additionally, the lack of user research, and the sporadic validation of transparency claims put contemporary research on transparent ML for medical image analysis at risk of being incomprehensible to users, and thus, clinically irrelevant. To alleviate these shortcomings in forthcoming research while acknowledging the challenges of human-centered design in healthcare, we introduce the INTRPRT guideline, a systematic design directive for transparent ML systems in medical image analysis. The INTRPRT guideline suggests formative user research as the first step of transparent model design to understand user needs and domain requirements. Following this process produces evidence to support design choices, and ultimately, increases the likelihood that the algorithms afford transparency.



### Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.10992v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2112.10992v2)
- **Published**: 2021-12-21 05:31:51+00:00
- **Updated**: 2022-04-24 09:18:20+00:00
- **Authors**: Xiangbo Shu, Jiawen Yang, Rui Yan, Yan Song
- **Comment**: None
- **Journal**: None
- **Summary**: This work focuses on the task of elderly activity recognition, which is a challenging task due to the existence of individual actions and human-object interactions in elderly activities. Thus, we attempt to effectively aggregate the discriminative information of actions and interactions from both RGB videos and skeleton sequences by attentively fusing multi-modal features. Recently, some nonlinear multi-modal fusion approaches are proposed by utilizing nonlinear attention mechanism that is extended from Squeeze-and-Excitation Networks (SENet). Inspired by this, we propose a novel Expansion-Squeeze-Excitation Fusion Network (ESE-FN) to effectively address the problem of elderly activity recognition, which learns modal and channel-wise Expansion-Squeeze-Excitation (ESE) attentions for attentively fusing the multi-modal features in the modal and channel-wise ways. Furthermore, we design a new Multi-modal Loss (ML) to keep the consistency between the single-modal features and the fused multi-modal features by adding the penalty of difference between the minimum prediction losses on single modalities and the prediction loss on the fused modality. Finally, we conduct experiments on a largest-scale elderly activity dataset, i.e., ETRI-Activity3D (including 110,000+ videos, and 50+ categories), to demonstrate that the proposed ESE-FN achieves the best accuracy compared with the state-of-the-art methods. In addition, more extensive experimental results show that the proposed ESE-FN is also comparable to the other methods in terms of normal action recognition task.



### Point spread function estimation for blind image deblurring problems based on framelet transform
- **Arxiv ID**: http://arxiv.org/abs/2112.11004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, eess.IV, math.IT, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2112.11004v1)
- **Published**: 2021-12-21 06:15:37+00:00
- **Updated**: 2021-12-21 06:15:37+00:00
- **Authors**: Reza Parvaz
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most important issues in the image processing is the approximation of the image that has been lost due to the blurring process. These types of matters are divided into non-blind and blind problems. The second type of problem is more complex in terms of calculations than the first problems due to the unknown of original image and point spread function estimation. In the present paper, an algorithm based on coarse-to-fine iterative by $l_0-\alpha l_1$ regularization and framelet transform is introduced to approximate the spread function estimation. Framelet transfer improves the restored kernel due to the decomposition of the kernel to different frequencies. Also in the proposed model fraction gradient operator is used instead of ordinary gradient operator. The proposed method is investigated on different kinds of images such as text, face, natural. The output of the proposed method reflects the effectiveness of the proposed algorithm in restoring the images from blind problems.



### MPViT: Multi-Path Vision Transformer for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.11010v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11010v2)
- **Published**: 2021-12-21 06:34:50+00:00
- **Updated**: 2021-12-27 02:46:40+00:00
- **Authors**: Youngwan Lee, Jonghee Kim, Jeff Willette, Sung Ju Hwang
- **Comment**: technical report
- **Journal**: None
- **Summary**: Dense computer vision tasks such as object detection and segmentation require effective multi-scale feature representation for detecting or classifying objects or regions with varying sizes. While Convolutional Neural Networks (CNNs) have been the dominant architectures for such tasks, recently introduced Vision Transformers (ViTs) aim to replace them as a backbone. Similar to CNNs, ViTs build a simple multi-stage structure (i.e., fine-to-coarse) for multi-scale representation with single-scale patches. In this work, with a different perspective from existing Transformers, we explore multi-scale patch embedding and multi-path structure, constructing the Multi-Path Vision Transformer (MPViT). MPViT embeds features of the same size~(i.e., sequence length) with patches of different scales simultaneously by using overlapping convolutional patch embedding. Tokens of different scales are then independently fed into the Transformer encoders via multiple paths and the resulting features are aggregated, enabling both fine and coarse feature representations at the same feature level. Thanks to the diverse, multi-scale feature representations, our MPViTs scaling from tiny~(5M) to base~(73M) consistently achieve superior performance over state-of-the-art Vision Transformers on ImageNet classification, object detection, instance segmentation, and semantic segmentation. These extensive results demonstrate that MPViT can serve as a versatile backbone network for various vision tasks. Code will be made publicly available at \url{https://git.io/MPViT}.



### fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits
- **Arxiv ID**: http://arxiv.org/abs/2112.11014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11014v2)
- **Published**: 2021-12-21 06:52:48+00:00
- **Updated**: 2022-06-29 19:07:15+00:00
- **Authors**: Rotem Leibovitz, Jhonathan Osin, Lior Wolf, Guy Gurevitch, Talma Hendler
- **Comment**: None
- **Journal**: MICCAI 2022
- **Summary**: We obtain a personal signature of a person's learning progress in a self-neuromodulation task, guided by functional MRI (fMRI). The signature is based on predicting the activity of the Amygdala in a second neurofeedback session, given a similar fMRI-derived brain state in the first session. The prediction is made by a deep neural network, which is trained on the entire training cohort of patients. This signal, which is indicative of a person's progress in performing the task of Amygdala modulation, is aggregated across multiple prototypical brain states and then classified by a linear classifier to various personal and clinical indications. The predictive power of the obtained signature is stronger than previous approaches for obtaining a personal signature from fMRI neurofeedback and provides an indication that a person's learning pattern may be used as a diagnostic tool. Our code has been made available, and data would be shared, subject to ethical approvals.



### A Theoretical View of Linear Backpropagation and Its Convergence
- **Arxiv ID**: http://arxiv.org/abs/2112.11018v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2112.11018v1)
- **Published**: 2021-12-21 07:18:00+00:00
- **Updated**: 2021-12-21 07:18:00+00:00
- **Authors**: Ziang Li, Yiwen Guo, Haodi Liu, Changshui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Backpropagation is widely used for calculating gradients in deep neural networks (DNNs). Applied often along with stochastic gradient descent (SGD) or its variants, backpropagation is considered as a de-facto choice in a variety of machine learning tasks including DNN training and adversarial attack/defense. Recently, a linear variant of BP named LinBP was introduced for generating more transferable adversarial examples for black-box adversarial attacks, by Guo et al. Yet, it has not been theoretically studied and the convergence analysis of such a method is lacking. This paper serves as a complement and somewhat an extension to Guo et al.'s paper, by providing theoretical analyses on LinBP in neural-network-involved learning tasks including adversarial attack and model training. We demonstrate that, somewhat surprisingly, LinBP can lead to faster convergence in these tasks in the same hyper-parameter settings, compared to BP. We confirm our theoretical results with extensive experiments.



### SOIT: Segmenting Objects with Instance-Aware Transformers
- **Arxiv ID**: http://arxiv.org/abs/2112.11037v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11037v2)
- **Published**: 2021-12-21 08:23:22+00:00
- **Updated**: 2021-12-23 15:28:12+00:00
- **Authors**: Xiaodong Yu, Dahu Shi, Xing Wei, Ye Ren, Tingqun Ye, Wenming Tan
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: This paper presents an end-to-end instance segmentation framework, termed SOIT, that Segments Objects with Instance-aware Transformers. Inspired by DETR \cite{carion2020end}, our method views instance segmentation as a direct set prediction problem and effectively removes the need for many hand-crafted components like RoI cropping, one-to-many label assignment, and non-maximum suppression (NMS). In SOIT, multiple queries are learned to directly reason a set of object embeddings of semantic category, bounding-box location, and pixel-wise mask in parallel under the global image context. The class and bounding-box can be easily embedded by a fixed-length vector. The pixel-wise mask, especially, is embedded by a group of parameters to construct a lightweight instance-aware transformer. Afterward, a full-resolution mask is produced by the instance-aware transformer without involving any RoI-based operation. Overall, SOIT introduces a simple single-stage instance segmentation framework that is both RoI- and NMS-free. Experimental results on the MS COCO dataset demonstrate that SOIT outperforms state-of-the-art instance segmentation approaches significantly. Moreover, the joint learning of multiple tasks in a unified query embedding can also substantially improve the detection performance. Code is available at \url{https://github.com/yuxiaodongHRI/SOIT}.



### Geometry-Aware Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2112.11041v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11041v1)
- **Published**: 2021-12-21 08:45:42+00:00
- **Updated**: 2021-12-21 08:45:42+00:00
- **Authors**: You-Wei Luo, Chuan-Xian Ren, Zi-Ying Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from the labeled source domain to the unlabeled target domain in the presence of dataset shift. Most existing methods cannot address the domain alignment and class discrimination well, which may distort the intrinsic data structure for downstream tasks (e.g., classification). To this end, we propose a novel geometry-aware model to learn the transferability and discriminability simultaneously via nuclear norm optimization. We introduce the domain coherence and class orthogonality for UDA from the perspective of subspace geometry. The domain coherence will ensure the model has a larger capacity for learning separable representations, and class orthogonality will minimize the correlation between clusters to alleviate the misalignment. So, they are consistent and can benefit from each other. Besides, we provide a theoretical insight into the norm-based learning literature in UDA, which ensures the interpretability of our model. We show that the norms of domains and clusters are expected to be larger and smaller to enhance the transferability and discriminability, respectively. Extensive experimental results on standard UDA datasets demonstrate the effectiveness of our theory and model.



### Leveraging Image Complexity in Macro-Level Neural Network Design for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.11065v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11065v1)
- **Published**: 2021-12-21 09:49:47+00:00
- **Updated**: 2021-12-21 09:49:47+00:00
- **Authors**: Tariq M. Khan, Syed S. Naqvi, Erik Meijering
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progress in encoder-decoder neural network architecture design has led to significant performance improvements in a wide range of medical image segmentation tasks. However, state-of-the-art networks for a given task may be too computationally demanding to run on affordable hardware, and thus users often resort to practical workarounds by modifying various macro-level design aspects. Two common examples are downsampling of the input images and reducing the network depth to meet computer memory constraints. In this paper we investigate the effects of these changes on segmentation performance and show that image complexity can be used as a guideline in choosing what is best for a given dataset. We consider four statistical measures to quantify image complexity and evaluate their suitability on ten different public datasets. For the purpose of our experiments we also propose two new encoder-decoder architectures representing shallow and deep networks that are more memory efficient than currently popular networks. Our results suggest that median frequency is the best complexity measure in deciding about an acceptable input downsampling factor and network depth. For high-complexity datasets, a shallow network running on the original images may yield better segmentation results than a deep network running on downsampled images, whereas the opposite may be the case for low-complexity images.



### RC-Net: A Convolutional Neural Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.11078v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11078v1)
- **Published**: 2021-12-21 10:24:01+00:00
- **Updated**: 2021-12-21 10:24:01+00:00
- **Authors**: Tariq M Khan, Antonio Robles-Kelly, Syed S. Naqvi
- **Comment**: None
- **Journal**: None
- **Summary**: Over recent years, increasingly complex approaches based on sophisticated convolutional neural network architectures have been slowly pushing performance on well-established benchmark datasets. In this paper, we take a step back to examine the real need for such complexity. We present RC-Net, a fully convolutional network, where the number of filters per layer is optimized to reduce feature overlapping and complexity. We also used skip connections to keep spatial information loss to a minimum by keeping the number of pooling operations in the network to a minimum. Two publicly available retinal vessel segmentation datasets were used in our experiments. In our experiments, RC-Net is quite competitive, outperforming alternatives vessels segmentation methods with two or even three orders of magnitude less trainable parameters.



### RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality
- **Arxiv ID**: http://arxiv.org/abs/2112.11081v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11081v2)
- **Published**: 2021-12-21 10:28:17+00:00
- **Updated**: 2022-03-30 13:25:56+00:00
- **Authors**: Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Jungong Han, Guiguang Ding
- **Comment**: Accepted by CVPR-2022. This is the latest version
- **Journal**: None
- **Summary**: Compared to convolutional layers, fully-connected (FC) layers are better at modeling the long-range dependencies but worse at capturing the local patterns, hence usually less favored for image recognition. In this paper, we propose a methodology, Locality Injection, to incorporate local priors into an FC layer via merging the trained parameters of a parallel conv kernel into the FC kernel. Locality Injection can be viewed as a novel Structural Re-parameterization method since it equivalently converts the structures via transforming the parameters. Based on that, we propose a multi-layer-perceptron (MLP) block named RepMLP Block, which uses three FC layers to extract features, and a novel architecture named RepMLPNet. The hierarchical design distinguishes RepMLPNet from the other concurrently proposed vision MLPs. As it produces feature maps of different levels, it qualifies as a backbone model for downstream tasks like semantic segmentation. Our results reveal that 1) Locality Injection is a general methodology for MLP models; 2) RepMLPNet has favorable accuracy-efficiency trade-off compared to the other MLPs; 3) RepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic segmentation. The code and models are available at https://github.com/DingXiaoH/RepMLP.



### Can We Use Neural Regularization to Solve Depth Super-Resolution?
- **Arxiv ID**: http://arxiv.org/abs/2112.11085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11085v1)
- **Published**: 2021-12-21 10:40:04+00:00
- **Updated**: 2021-12-21 10:40:04+00:00
- **Authors**: Milena Gazdieva, Oleg Voynov, Alexey Artemov, Youyi Zheng, Luiz Velho, Evgeny Burnaev
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Depth maps captured with commodity sensors often require super-resolution to be used in applications. In this work we study a super-resolution approach based on a variational problem statement with Tikhonov regularization where the regularizer is parametrized with a deep neural network. This approach was previously applied successfully in photoacoustic tomography. We experimentally show that its application to depth map super-resolution is difficult, and provide suggestions about the reasons for that.



### EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.11088v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11088v4)
- **Published**: 2021-12-21 10:48:34+00:00
- **Updated**: 2022-12-20 15:02:09+00:00
- **Authors**: Zhe Liu, Tengteng Huang, Bingling Li, Xiwu Chen, Xi Wang, Xiang Bai
- **Comment**: Accepted by TPAMI-2022
- **Journal**: None
- **Summary**: Recently, fusing the LiDAR point cloud and camera image to improve the performance and robustness of 3D object detection has received more and more attention, as these two modalities naturally possess strong complementarity. In this paper, we propose EPNet++ for multi-modal 3D object detection by introducing a novel Cascade Bi-directional Fusion~(CB-Fusion) module and a Multi-Modal Consistency~(MC) loss. More concretely, the proposed CB-Fusion module enhances point features with plentiful semantic information absorbed from the image features in a cascade bi-directional interaction fusion manner, leading to more powerful and discriminative feature representations. The MC loss explicitly guarantees the consistency between predicted scores from two modalities to obtain more comprehensive and reliable confidence scores. The experimental results on the KITTI, JRDB and SUN-RGBD datasets demonstrate the superiority of EPNet++ over the state-of-the-art methods. Besides, we emphasize a critical but easily overlooked problem, which is to explore the performance and robustness of a 3D detector in a sparser scene. Extensive experiments present that EPNet++ outperforms the existing SOTA methods with remarkable margins in highly sparse point cloud cases, which might be an available direction to reduce the expensive cost of LiDAR sensors. Code is available at: https://github.com/happinesslz/EPNetV2.



### GlobalMatch: Registration of Forest Terrestrial Point Clouds by Global Matching of Relative Stem Positions
- **Arxiv ID**: http://arxiv.org/abs/2112.11121v3
- **DOI**: 10.1016/j.isprsjprs.2023.01.013
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.11121v3)
- **Published**: 2021-12-21 11:47:51+00:00
- **Updated**: 2023-04-01 19:13:46+00:00
- **Authors**: Xufei Wang, Zexin Yang, Xiaojun Cheng, Jantien Stoter, Wenbing Xu, Zhenlun Wu, Liangliang Nan
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing. Vol. 197,
  71-86, 2023
- **Summary**: Registering point clouds of forest environments is an essential prerequisite for LiDAR applications in precision forestry. State-of-the-art methods for forest point cloud registration require the extraction of individual tree attributes, and they have an efficiency bottleneck when dealing with point clouds of real-world forests with dense trees. We propose an automatic, robust, and efficient method for the registration of forest point clouds. Our approach first locates tree stems from raw point clouds and then matches the stems based on their relative spatial relationship to determine the registration transformation. The algorithm requires no extra individual tree attributes and has quadratic complexity to the number of trees in the environment, allowing it to align point clouds of large forest environments. Extensive experiments on forest terrestrial point clouds have revealed that our method inherits the effectiveness and robustness of the stem-based registration strategy while exceedingly increasing its efficiency. Besides, we introduce a new benchmark dataset that complements the very few existing open datasets for the development and evaluation of registration methods for forest point clouds. The source code of our method and the dataset are available at https://github.com/zexinyang/GlobalMatch.



### Learning Human Motion Prediction via Stochastic Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2112.11124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11124v1)
- **Published**: 2021-12-21 11:55:13+00:00
- **Updated**: 2021-12-21 11:55:13+00:00
- **Authors**: Kedi Lyu, Zhenguang Liu, Shuang Wu, Haipeng Chen, Xuhong Zhang, Yuyu Yin
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Human motion understanding and prediction is an integral aspect in our pursuit of machine intelligence and human-machine interaction systems. Current methods typically pursue a kinematics modeling approach, relying heavily upon prior anatomical knowledge and constraints. However, such an approach is hard to generalize to different skeletal model representations, and also tends to be inadequate in accounting for the dynamic range and complexity of motion, thus hindering predictive accuracy. In this work, we propose a novel approach in modeling the motion prediction problem based on stochastic differential equations and path integrals. The motion profile of each skeletal joint is formulated as a basic stochastic variable and modeled with the Langevin equation. We develop a strategy of employing GANs to simulate path integrals that amounts to optimizing over possible future paths. We conduct experiments in two large benchmark datasets, Human 3.6M and CMU MoCap. It is highlighted that our approach achieves a 12.48% accuracy improvement over current state-of-the-art methods in average.



### Cloud Sphere: A 3D Shape Representation via Progressive Deformation
- **Arxiv ID**: http://arxiv.org/abs/2112.11133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11133v1)
- **Published**: 2021-12-21 12:10:23+00:00
- **Updated**: 2021-12-21 12:10:23+00:00
- **Authors**: Zongji Wang, Yunfei Liu, Feng Lu
- **Comment**: This paper was submitted first in CVPR 2021 (paper id: 2255), and
  then was submitted in CVM 2022 (id: 160)
- **Journal**: None
- **Summary**: In the area of 3D shape analysis, the geometric properties of a shape have long been studied. Instead of directly extracting representative features using expert-designed descriptors or end-to-end deep neural networks, this paper is dedicated to discovering distinctive information from the shape formation process. Concretely, a spherical point cloud served as the template is progressively deformed to fit the target shape in a coarse-to-fine manner. During the shape formation process, several checkpoints are inserted to facilitate recording and investigating the intermediate stages. For each stage, the offset field is evaluated as a stage-aware description. The summation of the offsets throughout the shape formation process can completely define the target shape in terms of geometry. In this perspective, one can derive the point-wise shape correspondence from the template inexpensively, which benefits various graphic applications. In this paper, the Progressive Deformation-based Auto-Encoder (PDAE) is proposed to learn the stage-aware description through a coarse-to-fine shape fitting task. Experimental results show that the proposed PDAE has the ability to reconstruct 3D shapes with high fidelity, and consistent topology is preserved in the multi-stage deformation process. Additional applications based on the stage-aware description are performed, demonstrating its universality.



### Input-Specific Robustness Certification for Randomized Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2112.12084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.12084v1)
- **Published**: 2021-12-21 12:16:03+00:00
- **Updated**: 2021-12-21 12:16:03+00:00
- **Authors**: Ruoxin Chen, Jie Li, Junchi Yan, Ping Li, Bin Sheng
- **Comment**: Accepted by AAAI22
- **Journal**: None
- **Summary**: Although randomized smoothing has demonstrated high certified robustness and superior scalability to other certified defenses, the high computational overhead of the robustness certification bottlenecks the practical applicability, as it depends heavily on the large sample approximation for estimating the confidence interval. In existing works, the sample size for the confidence interval is universally set and agnostic to the input for prediction. This Input-Agnostic Sampling (IAS) scheme may yield a poor Average Certified Radius (ACR)-runtime trade-off which calls for improvement. In this paper, we propose Input-Specific Sampling (ISS) acceleration to achieve the cost-effectiveness for robustness certification, in an adaptive way of reducing the sampling size based on the input characteristic. Furthermore, our method universally controls the certified radius decline from the ISS sample size reduction. The empirical results on CIFAR-10 and ImageNet show that ISS can speed up the certification by more than three times at a limited cost of 0.05 certified radius. Meanwhile, ISS surpasses IAS on the average certified radius across the extensive hyperparameter settings. Specifically, ISS achieves ACR=0.958 on ImageNet ($\sigma=1.0$) in 250 minutes, compared to ACR=0.917 by IAS under the same condition. We release our code in \url{https://github.com/roy-ch/Input-Specific-Certification}.



### PONet: Robust 3D Human Pose Estimation via Learning Orientations Only
- **Arxiv ID**: http://arxiv.org/abs/2112.11153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11153v1)
- **Published**: 2021-12-21 12:48:48+00:00
- **Updated**: 2021-12-21 12:48:48+00:00
- **Authors**: Jue Wang, Shaoli Huang, Xinchao Wang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional 3D human pose estimation relies on first detecting 2D body keypoints and then solving the 2D to 3D correspondence problem.Despite the promising results, this learning paradigm is highly dependent on the quality of the 2D keypoint detector, which is inevitably fragile to occlusions and out-of-image absences.In this paper,we propose a novel Pose Orientation Net (PONet) that is able to robustly estimate 3D pose by learning orientations only, hence bypassing the error-prone keypoint detector in the absence of image evidence. For images with partially invisible limbs, PONet estimates the 3D orientation of these limbs by taking advantage of the local image evidence to recover the 3D pose.Moreover, PONet is competent to infer full 3D poses even from images with completely invisible limbs, by exploiting the orientation correlation between visible limbs to complement the estimated poses,further improving the robustness of 3D pose estimation.We evaluate our method on multiple datasets, including Human3.6M, MPII, MPI-INF-3DHP, and 3DPW. Our method achieves results on par with state-of-the-art techniques in ideal settings, yet significantly eliminates the dependency on keypoint detectors and the corresponding computation burden. In highly challenging scenarios, such as truncation and erasing, our method performs very robustly and yields much superior results as compared to state of the art,demonstrating its potential for real-world applications.



### Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization
- **Arxiv ID**: http://arxiv.org/abs/2112.11177v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11177v3)
- **Published**: 2021-12-21 13:18:46+00:00
- **Updated**: 2022-03-28 07:59:29+00:00
- **Authors**: Ziqi Zhou, Lei Qi, Xin Yang, Dong Ni, Yinghuan Shi
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: For medical image segmentation, imagine if a model was only trained using MR images in source domain, how about its performance to directly segment CT images in target domain? This setting, namely generalizable cross-modality segmentation, owning its clinical potential, is much more challenging than other related settings, e.g., domain adaptation. To achieve this goal, we in this paper propose a novel dual-normalization model by leveraging the augmented source-similar and source-dissimilar images during our generalizable segmentation. To be specific, given a single source domain, aiming to simulate the possible appearance change in unseen target domains, we first utilize a nonlinear transformation to augment source-similar and source-dissimilar images. Then, to sufficiently exploit these two types of augmentations, our proposed dual-normalization based model employs a shared backbone yet independent batch normalization layer for separate normalization. Afterward, we put forward a style-based selection scheme to automatically choose the appropriate path in the test stage. Extensive experiments on three publicly available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal Multi-Organ datasets, have demonstrated that our method outperforms other state-of-the-art domain generalization methods. Code is available at https://github.com/zzzqzhou/Dual-Normalization.



### Improving Robustness with Image Filtering
- **Arxiv ID**: http://arxiv.org/abs/2112.11235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11235v1)
- **Published**: 2021-12-21 14:04:25+00:00
- **Updated**: 2021-12-21 14:04:25+00:00
- **Authors**: Matteo Terzi, Mattia Carletti, Gian Antonio Susto
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial robustness is one of the most challenging problems in Deep Learning and Computer Vision research. All the state-of-the-art techniques require a time-consuming procedure that creates cleverly perturbed images. Due to its cost, many solutions have been proposed to avoid Adversarial Training. However, all these attempts proved ineffective as the attacker manages to exploit spurious correlations among pixels to trigger brittle features implicitly learned by the model. This paper first introduces a new image filtering scheme called Image-Graph Extractor (IGE) that extracts the fundamental nodes of an image and their connections through a graph structure. By leveraging the IGE representation, we build a new defense method, Filtering As a Defense, that does not allow the attacker to entangle pixels to create malicious patterns. Moreover, we show that data augmentation with filtered images effectively improves the model's robustness to data corruption. We validate our techniques on CIFAR-10, CIFAR-100, and ImageNet.



### PointCaps: Raw Point Cloud Processing using Capsule Networks with Euclidean Distance Routing
- **Arxiv ID**: http://arxiv.org/abs/2112.11258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11258v2)
- **Published**: 2021-12-21 14:34:39+00:00
- **Updated**: 2022-08-21 02:44:48+00:00
- **Authors**: Dishanika Denipitiyage, Vinoj Jayasundara, Ranga Rodrigo, Chamira U. S. Edussooriya
- **Comment**: Accepted to be published in Journal of Visual Communication and Image
  Representation (Elsevier), 16 Pages, 4 Figures, 5 Tables
- **Journal**: None
- **Summary**: Raw point cloud processing using capsule networks is widely adopted in classification, reconstruction, and segmentation due to its ability to preserve spatial agreement of the input data. However, most of the existing capsule based network approaches are computationally heavy and fail at representing the entire point cloud as a single capsule. We address these limitations in existing capsule network based approaches by proposing PointCaps, a novel convolutional capsule architecture with parameter sharing. Along with PointCaps, we propose a novel Euclidean distance routing algorithm and a class-independent latent representation. The latent representation captures physically interpretable geometric parameters of the point cloud, with dynamic Euclidean routing, PointCaps well-represents the spatial (point-to-part) relationships of points. PointCaps has a significantly lower number of parameters and requires a significantly lower number of FLOPs while achieving better reconstruction with comparable classification and segmentation accuracy for raw point clouds compared to state-of-the-art capsule networks.



### High-Fidelity Point Cloud Completion with Low-Resolution Recovery and Noise-Aware Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2112.11271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11271v2)
- **Published**: 2021-12-21 14:51:52+00:00
- **Updated**: 2021-12-22 04:01:15+00:00
- **Authors**: Ren-Wu Li, Bo Wang, Chun-Peng Li, Ling-Xiao Zhang, Lin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Completing an unordered partial point cloud is a challenging task. Existing approaches that rely on decoding a latent feature to recover the complete shape, often lead to the completed point cloud being over-smoothing, losing details, and noisy. Instead of decoding a whole shape, we propose to decode and refine a low-resolution (low-res) point cloud first, and then performs a patch-wise noise-aware upsampling rather than interpolating the whole sparse point cloud at once, which tends to lose details. Regarding the possibility of lacking details of the initially decoded low-res point cloud, we propose an iterative refinement to recover the geometric details and a symmetrization process to preserve the trustworthy information from the input partial point cloud. After obtaining a sparse and complete point cloud, we propose a patch-wise upsampling strategy. Patch-based upsampling allows to better recover fine details unlike decoding a whole shape, however, the existing upsampling methods are not applicable to completion task due to the data discrepancy (i.e., input sparse data here is not from ground-truth). Therefore, we propose a patch extraction approach to generate training patch pairs between the sparse and ground-truth point clouds, and an outlier removal step to suppress the noisy points from the sparse point cloud. Together with the low-res recovery, our whole method is able to achieve high-fidelity point cloud completion. Comprehensive evaluations are provided to demonstrate the effectiveness of the proposed method and its individual components.



### Multimodal Entity Tagging with Multimodal Knowledge Base
- **Arxiv ID**: http://arxiv.org/abs/2201.00693v2
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00693v2)
- **Published**: 2021-12-21 15:04:57+00:00
- **Updated**: 2022-07-28 07:56:08+00:00
- **Authors**: Hao Peng, Hang Li, Lei Hou, Juanzi Li, Chao Qiao
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: To enhance research on multimodal knowledge base and multimodal information processing, we propose a new task called multimodal entity tagging (MET) with a multimodal knowledge base (MKB). We also develop a dataset for the problem using an existing MKB. In an MKB, there are entities and their associated texts and images. In MET, given a text-image pair, one uses the information in the MKB to automatically identify the related entity in the text-image pair. We solve the task by using the information retrieval paradigm and implement several baselines using state-of-the-art methods in NLP and CV. We conduct extensive experiments and make analyses on the experimental results. The results show that the task is challenging, but current technologies can achieve relatively high performance. We will release the dataset, code, and models for future research.



### Review of Face Presentation Attack Detection Competitions
- **Arxiv ID**: http://arxiv.org/abs/2112.11290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11290v1)
- **Published**: 2021-12-21 15:20:10+00:00
- **Updated**: 2021-12-21 15:20:10+00:00
- **Authors**: Zitong Yu, Jukka Komulainen, Xiaobai Li, Guoying Zhao
- **Comment**: Handbook of Biometric Anti-Spoofing (3rd Ed.)
- **Journal**: None
- **Summary**: Face presentation attack detection (PAD) has received increasing attention ever since the vulnerabilities to spoofing have been widely recognized. The state of the art in unimodal and multi-modal face anti-spoofing has been assessed in eight international competitions organized in conjunction with major biometrics and computer vision conferences in 2011, 2013, 2017, 2019, 2020 and 2021, each introducing new challenges to the research community. In this chapter, we present the design and results of the five latest competitions from 2019 until 2021. The first two challenges aimed to evaluate the effectiveness of face PAD in multi-modal setup introducing near-infrared (NIR) and depth modalities in addition to colour camera data, while the latest three competitions focused on evaluating domain and attack type generalization abilities of face PAD algorithms operating on conventional colour images and videos. We also discuss the lessons learnt from the competitions and future challenges in the field in general.



### Implicit Neural Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2112.11312v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11312v1)
- **Published**: 2021-12-21 15:59:00+00:00
- **Updated**: 2021-12-21 15:59:00+00:00
- **Authors**: Yunfan Zhang, Ties van Rozendaal, Johann Brehmer, Markus Nagel, Taco Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to compress full-resolution video sequences with implicit neural representations. Each frame is represented as a neural network that maps coordinate positions to pixel values. We use a separate implicit network to modulate the coordinate inputs, which enables efficient motion compensation between frames. Together with a small residual network, this allows us to efficiently compress P-frames relative to the previous frame. We further lower the bitrate by storing the network weights with learned integer quantization. Our method, which we call implicit pixel flow (IPF), offers several simplifications over established neural video codecs: it does not require the receiver to have access to a pretrained neural network, does not use expensive interpolation-based warping operations, and does not require a separate training dataset. We demonstrate the feasibility of neural implicit compression on image and video data.



### iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images
- **Arxiv ID**: http://arxiv.org/abs/2112.11325v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11325v6)
- **Published**: 2021-12-21 16:16:30+00:00
- **Updated**: 2022-07-17 13:20:09+00:00
- **Authors**: Qin Liu, Zhenlin Xu, Yining Jiao, Marc Niethammer
- **Comment**: MICCAI'22 camera-ready
- **Journal**: None
- **Summary**: We propose iSegFormer, a memory-efficient transformer that combines a Swin transformer with a lightweight multilayer perceptron (MLP) decoder. With the efficient Swin transformer blocks for hierarchical self-attention and the simple MLP decoder for aggregating both local and global attention, iSegFormer learns powerful representations while achieving high computational efficiencies. Specifically, we apply iSegFormer to interactive 3D medical image segmentation.



### Multispectral image fusion based on super pixel segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.11329v3
- **DOI**: 10.1109/ICASSP49357.2023.10095874
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11329v3)
- **Published**: 2021-12-21 16:19:10+00:00
- **Updated**: 2023-01-03 14:50:28+00:00
- **Authors**: Nati Ofir, Jean-Christophe Nebel
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral image fusion is a computer vision process that is essential to remote sensing. For applications such as dehazing and object detection, there is a need to offer solutions that can perform in real-time on any type of scene. Unfortunately, current state-of-the-art approaches do not meet these criteria as they need to be trained on domain-specific data and have high computational complexity. This paper focuses on the task of fusing color (RGB) and near-infrared (NIR) images as this the typical RGBT sensors, as in multispectral cameras for detection, fusion, and dehazing. Indeed, the NIR channel has the ability to capture details not visible in RGB and see beyond haze, fog, and clouds. To combine this information, a novel approach based on superpixel segmentation is designed so that multispectral image fusion is performed according to the specific local content of the images to be fused. Therefore, the proposed method produces a fusion that contains the most relevant content of each spectrum. The experiments reported in this manuscript show that the novel approach better preserve details than alternative fusion methods.



### PrimSeq: a deep learning-based pipeline to quantitate rehabilitation training
- **Arxiv ID**: http://arxiv.org/abs/2112.11330v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11330v2)
- **Published**: 2021-12-21 16:19:14+00:00
- **Updated**: 2021-12-22 13:22:39+00:00
- **Authors**: Avinash Parnandi, Aakash Kaku, Anita Venkatesan, Natasha Pandit, Audre Wirtanen, Haresh Rajamohan, Kannan Venkataramanan, Dawn Nilsen, Carlos Fernandez-Granda, Heidi Schambra
- **Comment**: None
- **Journal**: None
- **Summary**: Stroke rehabilitation seeks to increase neuroplasticity through the repeated practice of functional motions, but may have minimal impact on recovery because of insufficient repetitions. The optimal training content and quantity are currently unknown because no practical tools exist to measure them. Here, we present PrimSeq, a pipeline to classify and count functional motions trained in stroke rehabilitation. Our approach integrates wearable sensors to capture upper-body motion, a deep learning model to predict motion sequences, and an algorithm to tally motions. The trained model accurately decomposes rehabilitation activities into component functional motions, outperforming competitive machine learning methods. PrimSeq furthermore quantifies these motions at a fraction of the time and labor costs of human experts. We demonstrate the capabilities of PrimSeq in previously unseen stroke patients with a range of upper extremity motor impairment. We expect that these advances will support the rigorous measurement required for quantitative dosing trials in stroke rehabilitation.



### Deep Learning Based 3D Point Cloud Regression for Estimating Forest Biomass
- **Arxiv ID**: http://arxiv.org/abs/2112.11335v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, I.2.10; I.2.1; J.0
- **Links**: [PDF](http://arxiv.org/pdf/2112.11335v3)
- **Published**: 2021-12-21 16:26:13+00:00
- **Updated**: 2023-02-21 19:20:27+00:00
- **Authors**: Stefan Oehmcke, Lei Li, Katerina Trepekli, Jaime Revenga, Thomas Nord-Larsen, Fabian Gieseke, Christian Igel
- **Comment**: 31 pages, 14 figures, 4 tables
- **Journal**: None
- **Summary**: Quantification of forest biomass stocks and their dynamics is important for implementing effective climate change mitigation measures. The knowledge is needed, e.g., for local forest management, studying the processes driving af-, re-, and deforestation, and can improve the accuracy of carbon-accounting. Remote sensing using airborne LiDAR can be used to perform these measurements of vegetation structure at large scale. We present deep learning systems for predicting wood volume, above-ground biomass (AGB), and subsequently above-ground carbon stocks directly from airborne LiDAR point clouds. We devise different neural network architectures for point cloud regression and evaluate them on remote sensing data of areas for which AGB estimates have been obtained from field measurements in the Danish national forest inventory. Our adaptation of Minkowski convolutional neural networks for regression gave the best results. The deep neural networks produced significantly more accurate wood volume, AGB, and carbon stock estimates compared to state-of-the-art approaches operating on basic statistics of the point clouds. In contrast to other methods, the proposed deep learning approach does not require a digital terrain model. We expect this finding to have a strong impact on LiDAR-based analyses of biomass dynamics.



### Transferable End-to-end Room Layout Estimation via Implicit Encoding
- **Arxiv ID**: http://arxiv.org/abs/2112.11340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11340v1)
- **Published**: 2021-12-21 16:33:14+00:00
- **Updated**: 2021-12-21 16:33:14+00:00
- **Authors**: Hao Zhao, Rene Ranftl, Yurong Chen, Hongbin Zha
- **Comment**: Project: https://sites.google.com/view/transferrl/
- **Journal**: None
- **Summary**: We study the problem of estimating room layouts from a single panorama image. Most former works have two stages: feature extraction and parametric model fitting. Here we propose an end-to-end method that directly predicts parametric layouts from an input panorama image. It exploits an implicit encoding procedure that embeds parametric layouts into a latent space. Then learning a mapping from images to this latent space makes end-to-end room layout estimation possible. However end-to-end methods have several notorious drawbacks despite many intriguing properties. A widely raised criticism is that they are troubled with dataset bias and do not transfer to unfamiliar domains. Our study echos this common belief. To this end, we propose to use semantic boundary prediction maps as an intermediate domain. It brings significant performance boost on four benchmarks (Structured3D, PanoContext, S3DIS, and Matterport3D), notably in the zero-shot transfer setting. Code, data, and models will be released.



### Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2112.11347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11347v2)
- **Published**: 2021-12-21 16:37:48+00:00
- **Updated**: 2022-04-06 17:38:12+00:00
- **Authors**: Atsuhiro Noguchi, Umar Iqbal, Jonathan Tremblay, Tatsuya Harada, Orazio Gallo
- **Comment**: CVPR2022, 16 pages, Project page:
  https://nvlabs.github.io/watch-it-move
- **Journal**: None
- **Summary**: Rendering articulated objects while controlling their poses is critical to applications such as virtual reality or animation for movies. Manipulating the pose of an object, however, requires the understanding of its underlying structure, that is, its joints and how they interact with each other. Unfortunately, assuming the structure to be known, as existing methods do, precludes the ability to work on new object categories. We propose to learn both the appearance and the structure of previously unseen articulated objects by observing them move from multiple views, with no joints annotation supervision, or information about the structure. We observe that 3D points that are static relative to one another should belong to the same part, and that adjacent parts that move relative to each other must be connected by a joint. To leverage this insight, we model the object parts in 3D as ellipsoids, which allows us to identify joints. We combine this explicit representation with an implicit one that compensates for the approximation introduced. We show that our method works for different structures, from quadrupeds, to single-arm robots, to humans.



### Contrastive Object Detection Using Knowledge Graph Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2112.11366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11366v1)
- **Published**: 2021-12-21 17:10:21+00:00
- **Updated**: 2021-12-21 17:10:21+00:00
- **Authors**: Christopher Lang, Alexander Braun, Abhinav Valada
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition for the most part has been approached as a one-hot problem that treats classes to be discrete and unrelated. Each image region has to be assigned to one member of a set of objects, including a background class, disregarding any similarities in the object types. In this work, we compare the error statistics of the class embeddings learned from a one-hot approach with semantically structured embeddings from natural language processing or knowledge graphs that are widely applied in open world object detection. Extensive experimental results on multiple knowledge-embeddings as well as distance metrics indicate that knowledge-based class representations result in more semantically grounded misclassifications while performing on par compared to one-hot methods on the challenging COCO and Cityscapes object detection benchmarks. We generalize our findings to multiple object detection architectures by proposing a knowledge-embedded design for keypoint-based and transformer-based object detection architectures.



### Shape from Polarization for Complex Scenes in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2112.11377v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11377v3)
- **Published**: 2021-12-21 17:30:23+00:00
- **Updated**: 2022-04-20 11:58:41+00:00
- **Authors**: Chenyang Lei, Chenyang Qi, Jiaxin Xie, Na Fan, Vladlen Koltun, Qifeng Chen
- **Comment**: Accepted to CVPR 2022; Github link:
  https://github.com/ChenyangLEI/sfp-wild ;Project website:
  https://chenyanglei.github.io/sfpwild/index.html
- **Journal**: None
- **Summary**: We present a new data-driven approach with physics-based priors to scene-level normal estimation from a single polarization image. Existing shape from polarization (SfP) works mainly focus on estimating the normal of a single object rather than complex scenes in the wild. A key barrier to high-quality scene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we contribute the first real-world scene-level SfP dataset with paired input polarization images and ground-truth normal maps. Then we propose a learning-based framework with a multi-head self-attention module and viewing encoding, which is designed to handle increasing polarization ambiguities caused by complex materials and non-orthographic projection in scene-level SfP. Our trained model can be generalized to far-field outdoor scenes as the relationship between polarized light and surface normals is not affected by distance. Experimental results demonstrate that our approach significantly outperforms existing SfP models on two datasets. Our dataset and source code will be publicly available at https://github.com/ChenyangLEI/sfp-wild



### A novel approach for the automated segmentation and volume quantification of cardiac fats on computed tomography
- **Arxiv ID**: http://arxiv.org/abs/2112.11381v1
- **DOI**: 10.1016/j.cmpb.2015.09.017
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11381v1)
- **Published**: 2021-12-21 17:38:06+00:00
- **Updated**: 2021-12-21 17:38:06+00:00
- **Authors**: rick Oliveira Rodrigues, FFC Morais, NAOS Morais, LS Conci, LV Neto, Aura Conci
- **Comment**: Computer methods and programs in biomedicine, 2016
- **Journal**: None
- **Summary**: The deposits of fat on the surroundings of the heart are correlated to several health risk factors such as atherosclerosis, carotid stiffness, coronary artery calcification, atrial fibrillation and many others. These deposits vary unrelated to obesity, which reinforces its direct segmentation for further quantification. However, manual segmentation of these fats has not been widely deployed in clinical practice due to the required human workload and consequential high cost of physicians and technicians. In this work, we propose a unified method for an autonomous segmentation and quantification of two types of cardiac fats. The segmented fats are termed epicardial and mediastinal, and stand apart from each other by the pericardium. Much effort was devoted to achieve minimal user intervention. The proposed methodology mainly comprises registration and classification algorithms to perform the desired segmentation. We compare the performance of several classification algorithms on this task, including neural networks, probabilistic models and decision tree algorithms. Experimental results of the proposed methodology have shown that the mean accuracy regarding both epicardial and mediastinal fats is 98.5% (99.5% if the features are normalized), with a mean true positive rate of 98.0%. In average, the Dice similarity index was equal to 97.6%.



### ADJUST: A Dictionary-Based Joint Reconstruction and Unmixing Method for Spectral Tomography
- **Arxiv ID**: http://arxiv.org/abs/2112.11406v2
- **DOI**: 10.1088/1361-6420/ac932e
- **Categories**: **cs.CV**, cs.CE, cs.NA, math.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2112.11406v2)
- **Published**: 2021-12-21 18:09:39+00:00
- **Updated**: 2022-10-04 14:38:49+00:00
- **Authors**: Math T. Zeegers, Ajinkya Kadu, Tristan van Leeuwen, Kees Joost Batenburg
- **Comment**: This paper is under consideration at Inverse Problems with minor
  revisions. 33 pages, 24 figures
- **Journal**: Inverse Problems 38 12 (2022) 125002
- **Summary**: Advances in multi-spectral detectors are causing a paradigm shift in X-ray Computed Tomography (CT). Spectral information acquired from these detectors can be used to extract volumetric material composition maps of the object of interest. If the materials and their spectral responses are known a priori, the image reconstruction step is rather straightforward. If they are not known, however, the maps as well as the responses need to be estimated jointly. A conventional workflow in spectral CT involves performing volume reconstruction followed by material decomposition, or vice versa. However, these methods inherently suffer from the ill-posedness of the joint reconstruction problem. To resolve this issue, we propose 'A Dictionary-based Joint reconstruction and Unmixing method for Spectral Tomography' (ADJUST). Our formulation relies on forming a dictionary of spectral signatures of materials common in CT and prior knowledge of the number of materials present in an object. In particular, we decompose the spectral volume linearly in terms of spatial material maps, a spectral dictionary, and the indicator of materials for the dictionary elements. We propose a memory-efficient accelerated alternating proximal gradient method to find an approximate solution to the resulting bi-convex problem. From numerical demonstrations on several synthetic phantoms, we observe that ADJUST performs exceedingly well compared to other state-of-the-art methods. Additionally, we address the robustness of ADJUST against limited and noisy measurement patterns. The demonstration of the proposed approach on a spectral micro-CT dataset shows its potential for real-world applications. Code is available at https://github.com/mzeegers/ADJUST.



### StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.11427v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11427v2)
- **Published**: 2021-12-21 18:45:45+00:00
- **Updated**: 2022-03-30 01:00:39+00:00
- **Authors**: Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman
- **Comment**: Camera-Ready version. Paper was accepted as oral to CVPR 2022. Added
  discussions and figures from the rebuttal to the supplementary material
  (sections C & F). Project Webpage: https://stylesdf.github.io/
- **Journal**: None
- **Summary**: We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024x1024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.



### Learned Queries for Efficient Local Attention
- **Arxiv ID**: http://arxiv.org/abs/2112.11435v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11435v2)
- **Published**: 2021-12-21 18:52:33+00:00
- **Updated**: 2022-04-19 17:49:50+00:00
- **Authors**: Moab Arar, Ariel Shamir, Amit H. Bermano
- **Comment**: CVPR 2022 - Oral
- **Journal**: None
- **Summary**: Vision Transformers (ViT) serve as powerful vision models. Unlike convolutional neural networks, which dominated vision research in previous years, vision transformers enjoy the ability to capture long-range dependencies in the data. Nonetheless, an integral part of any transformer architecture, the self-attention mechanism, suffers from high latency and inefficient memory utilization, making it less suitable for high-resolution input images. To alleviate these shortcomings, hierarchical vision models locally employ self-attention on non-interleaving windows. This relaxation reduces the complexity to be linear in the input size; however, it limits the cross-window interaction, hurting the model performance. In this paper, we propose a new shift-invariant local attention layer, called query and attend (QnA), that aggregates the input locally in an overlapping manner, much like convolutions. The key idea behind QnA is to introduce learned queries, which allow fast and efficient implementation. We verify the effectiveness of our layer by incorporating it into a hierarchical vision transformer model. We show improvements in speed and memory complexity while achieving comparable accuracy with state-of-the-art models. Finally, our layer scales especially well with window size, requiring up-to x10 less memory while being up-to x5 faster than existing methods. The code is publicly available at \url{https://github.com/moabarar/qna}.



### Multi-Modality Distillation via Learning the teacher's modality-level Gram Matrix
- **Arxiv ID**: http://arxiv.org/abs/2112.11447v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11447v1)
- **Published**: 2021-12-21 18:53:58+00:00
- **Updated**: 2021-12-21 18:53:58+00:00
- **Authors**: Peng Liu
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In the context of multi-modality knowledge distillation research, the existing methods was mainly focus on the problem of only learning teacher final output. Thus, there are still deep differences between the teacher network and the student network. It is necessary to force the student network to learn the modality relationship information of the teacher network. To effectively exploit transfering knowledge from teachers to students, a novel modality relation distillation paradigm by modeling the relationship information among different modality are adopted, that is learning the teacher modality-level Gram Matrix.



### Max-Margin Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.11450v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.11450v1)
- **Published**: 2021-12-21 18:56:54+00:00
- **Updated**: 2021-12-21 18:56:54+00:00
- **Authors**: Anshul Shah, Suvrit Sra, Rama Chellappa, Anoop Cherian
- **Comment**: Accepted at AAAI 2022
- **Journal**: None
- **Summary**: Standard contrastive learning approaches usually require a large number of negatives for effective unsupervised learning and often exhibit slow convergence. We suspect this behavior is due to the suboptimal selection of negatives used for offering contrast to the positives. We counter this difficulty by taking inspiration from support vector machines (SVMs) to present max-margin contrastive learning (MMCL). Our approach selects negatives as the sparse support vectors obtained via a quadratic optimization problem, and contrastiveness is enforced by maximizing the decision margin. As SVM optimization can be computationally demanding, especially in an end-to-end setting, we present simplifications that alleviate the computational burden. We validate our approach on standard vision benchmark datasets, demonstrating better performance in unsupervised representation learning over state-of-the-art, while having better empirical convergence properties.



### GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping
- **Arxiv ID**: http://arxiv.org/abs/2112.11454v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11454v2)
- **Published**: 2021-12-21 18:59:34+00:00
- **Updated**: 2023-03-16 20:59:14+00:00
- **Authors**: Omid Taheri, Vasileios Choutas, Michael J. Black, Dimitrios Tzionas
- **Comment**: None
- **Journal**: None
- **Summary**: Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied, but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this, the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. GOAL takes a step towards synthesizing realistic full-body object grasping.



### Teacher-Student Architecture for Mixed Supervised Lung Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.11541v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.6, I.4.6; I.4.9; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2112.11541v1)
- **Published**: 2021-12-21 22:02:34+00:00
- **Updated**: 2021-12-21 22:02:34+00:00
- **Authors**: Vemund Fredriksen, Svein Ole M. Svele, Andr Pedersen, Thomas Lang, Gabriel Kiss, Frank Lindseth
- **Comment**: 17 pages, 3 figures, 5 tables, submitted to journal
- **Journal**: None
- **Summary**: Purpose: Automating tasks such as lung tumor localization and segmentation in radiological images can free valuable time for radiologists and other clinical personnel. Convolutional neural networks may be suited for such tasks, but require substantial amounts of labeled data to train. Obtaining labeled data is a challenge, especially in the medical domain. Methods: This paper investigates the use of a teacher-student design to utilize datasets with different types of supervision to train an automatic model performing pulmonary tumor segmentation on computed tomography images. The framework consists of two models: the student that performs end-to-end automatic tumor segmentation and the teacher that supplies the student additional pseudo-annotated data during training. Results: Using only a small proportion of semantically labeled data and a large number of bounding box annotated data, we achieved competitive performance using a teacher-student design. Models trained on larger amounts of semantic annotations did not perform better than those trained on teacher-annotated data. Conclusions: Our results demonstrate the potential of utilizing teacher-student designs to reduce the annotation load, as less supervised annotation schemes may be performed, without any real degradation in segmentation accuracy.



### MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2112.11542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.11542v1)
- **Published**: 2021-12-21 22:06:24+00:00
- **Updated**: 2021-12-21 22:06:24+00:00
- **Authors**: Zhongzhi Yu, Yonggan Fu, Sicheng Li, Chaojian Li, Yingyan Lin
- **Comment**: None
- **Journal**: None
- **Summary**: ViTs are often too computationally expensive to be fitted onto real-world resource-constrained devices, due to (1) their quadratically increased complexity with the number of input tokens and (2) their overparameterized self-attention heads and model depth. In parallel, different images are of varied complexity and their different regions can contain various levels of visual information, indicating that treating all regions/tokens equally in terms of model complexity is unnecessary while such opportunities for trimming down ViTs' complexity have not been fully explored. To this end, we propose a Multi-grained Input-adaptive Vision Transformer framework dubbed MIA-Former that can input-adaptively adjust the structure of ViTs at three coarse-to-fine-grained granularities (i.e., model depth and the number of model heads/tokens). In particular, our MIA-Former adopts a low-cost network trained with a hybrid supervised and reinforcement training method to skip unnecessary layers, heads, and tokens in an input adaptive manner, reducing the overall computational cost. Furthermore, an interesting side effect of our MIA-Former is that its resulting ViTs are naturally equipped with improved robustness against adversarial attacks over their static counterparts, because MIA-Former's multi-grained dynamic control improves the model diversity similar to the effect of ensemble and thus increases the difficulty of adversarial attacks against all its sub-models. Extensive experiments and ablation studies validate that the proposed MIA-Former framework can effectively allocate computation budgets adaptive to the difficulty of input images meanwhile increase robustness, achieving state-of-the-art (SOTA) accuracy-efficiency trade-offs, e.g., 20% computation savings with the same or even a higher accuracy compared with SOTA dynamic transformer models.



### Real-time Street Human Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/2112.11543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11543v1)
- **Published**: 2021-12-21 22:11:19+00:00
- **Updated**: 2021-12-21 22:11:19+00:00
- **Authors**: Yanquan Chen, Fei Yang, Tianyu Lang, Guanfang Dong, Anup Basu
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: In recent years, motion capture technology using computers has developed rapidly. Because of its high efficiency and excellent performance, it replaces many traditional methods and is being widely used in many fields. Our project is about street scene video human motion capturing and analysis. The primary goal of the project is to capture the human motion in a video and use the motion information for 3D animation (human) in real-time. We applied a neural network for motion capture and implement it in the unity under a street view scene. By analyzing the motion data, we will have a better estimation of the street condition, which is useful for other high-tech applications such as self-driving cars.



### Decompose the Sounds and Pixels, Recompose the Events
- **Arxiv ID**: http://arxiv.org/abs/2112.11547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.11547v1)
- **Published**: 2021-12-21 22:22:46+00:00
- **Updated**: 2021-12-21 22:22:46+00:00
- **Authors**: Varshanth R. Rao, Md Ibrahim Khalil, Haoda Li, Peng Dai, Juwei Lu
- **Comment**: Accepted at AAAI 2022
- **Journal**: None
- **Summary**: In this paper, we propose a framework centering around a novel architecture called the Event Decomposition Recomposition Network (EDRNet) to tackle the Audio-Visual Event (AVE) localization problem in the supervised and weakly supervised settings. AVEs in the real world exhibit common unravelling patterns (termed as Event Progress Checkpoints (EPC)), which humans can perceive through the cooperation of their auditory and visual senses. Unlike earlier methods which attempt to recognize entire event sequences, the EDRNet models EPCs and inter-EPC relationships using stacked temporal convolutions. Based on the postulation that EPC representations are theoretically consistent for an event category, we introduce the State Machine Based Video Fusion, a novel augmentation technique that blends source videos using different EPC template sequences. Additionally, we design a new loss function called the Land-Shore-Sea loss to compactify continuous foreground and background representations. Lastly, to alleviate the issue of confusing events during weak supervision, we propose a prediction stabilization method called Bag to Instance Label Correction. Experiments on the AVE dataset show that our collective framework outperforms the state-of-the-art by a sizable margin.



### Distribution-aware Margin Calibration for Semantic Segmentation in Images
- **Arxiv ID**: http://arxiv.org/abs/2112.11554v1
- **DOI**: 10.1007/s11263-021-01533-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11554v1)
- **Published**: 2021-12-21 22:38:25+00:00
- **Updated**: 2021-12-21 22:38:25+00:00
- **Authors**: Litao Yu, Zhibin Li, Min Xu, Yongsheng Gao, Jiebo Luo, Jian Zhang
- **Comment**: This paper has been accepted by International Journal of Computer
  Vision (IJCV), and published on 09 November 2021. arXiv admin note: text
  overlap with arXiv:2011.01462
- **Journal**: None
- **Summary**: The Jaccard index, also known as Intersection-over-Union (IoU), is one of the most critical evaluation metrics in image semantic segmentation. However, direct optimization of IoU score is very difficult because the learning objective is neither differentiable nor decomposable. Although some algorithms have been proposed to optimize its surrogates, there is no guarantee provided for the generalization ability. In this paper, we propose a margin calibration method, which can be directly used as a learning objective, for an improved generalization of IoU over the data-distribution, underpinned by a rigid lower bound. This scheme theoretically ensures a better segmentation performance in terms of IoU score. We evaluated the effectiveness of the proposed margin calibration method on seven image datasets, showing substantial improvements in IoU score over other learning objectives using deep segmentation models.



### Anomaly Clustering: Grouping Images into Coherent Clusters of Anomaly Types
- **Arxiv ID**: http://arxiv.org/abs/2112.11573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.11573v2)
- **Published**: 2021-12-21 23:11:33+00:00
- **Updated**: 2022-10-14 22:34:25+00:00
- **Authors**: Kihyuk Sohn, Jinsung Yoon, Chun-Liang Li, Chen-Yu Lee, Tomas Pfister
- **Comment**: WACV2023
- **Journal**: None
- **Summary**: We study anomaly clustering, grouping data into coherent clusters of anomaly types. This is different from anomaly detection that aims to divide anomalies from normal data. Unlike object-centered image clustering, anomaly clustering is particularly challenging as anomalous patterns are subtle and local. We present a simple yet effective clustering framework using a patch-based pretrained deep embeddings and off-the-shelf clustering methods. We define a distance function between images, each of which is represented as a bag of embeddings, by the Euclidean distance between weighted averaged embeddings. The weight defines the importance of instances (i.e., patch embeddings) in the bag, which may highlight defective regions. We compute weights in an unsupervised way or in a semi-supervised way when labeled normal data is available. Extensive experimental studies show the effectiveness of the proposed clustering framework along with a novel distance function upon exist-ing multiple instance or deep clustering frameworks. Over-all, our framework achieves 0.451 and 0.674 normalized mutual information scores on MVTec object and texture categories and further improve with a few labeled normal data (0.577, 0.669), far exceeding the baselines (0.244, 0.273) or state-of-the-art deep clustering methods (0.176, 0.277).




# Arxiv Papers in cs.CV on 2021-12-01
### Is the use of Deep Learning and Artificial Intelligence an appropriate means to locate debris in the ocean without harming aquatic wildlife?
- **Arxiv ID**: http://arxiv.org/abs/2112.00190v1
- **DOI**: 10.1016/j.marpolbul.2022.113853
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00190v1)
- **Published**: 2021-12-01 00:12:04+00:00
- **Updated**: 2021-12-01 00:12:04+00:00
- **Authors**: Zoe Moorton, Zeyneb Kurt, Wai Lok Woo
- **Comment**: reference list is added/updated; sorry for causing any
  inconveniences. 3681 words, 14 pages
- **Journal**: None
- **Summary**: With the global issue of plastic debris ever expanding, it is about time that the technology industry stepped in. This study aims to assess whether deep learning can successfully distinguish between marine life and man-made debris underwater. The aim is to find if we are safely able to clean up our oceans with Artificial Intelligence without disrupting the delicate balance of the aquatic ecosystems. The research explores the use of Convolutional Neural Networks from the perspective of protecting the ecosystem, rather than primarily collecting rubbish. We did this by building a custom-built, deep learning model, with an original database including 1,644 underwater images and used a binary classification to sort synthesised material from aquatic life. We concluded that although it is possible to safely distinguish between debris and life, further exploration with a larger database and stronger CNN structure has the potential for much more promising results.



### Detecting Extratropical Cyclones of the Northern Hemisphere with Single Shot Detector
- **Arxiv ID**: http://arxiv.org/abs/2112.01283v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.01283v1)
- **Published**: 2021-12-01 00:46:37+00:00
- **Updated**: 2021-12-01 00:46:37+00:00
- **Authors**: Minjing Shi, Pengfei He, Yuli Shi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a deep learning-based model to detect extratropical cyclones (ETCs) of northern hemisphere, while developing a novel workflow of processing images and generating labels for ETCs. We first label the cyclone center by adapting an approach from Bonfanti et.al. [1] and set up criteria of labeling ETCs of three categories: developing, mature, and declining stages. We then propose a framework of labeling and preprocessing the images in our dataset. Once the images and labels are ready to serve as inputs, we create our object detection model named Single Shot Detector (SSD) to fit the format of our dataset. We train and evaluate our model with our labeled dataset on two settings (binary and multiclass classifications), while keeping a record of the results. Finally, we achieved relatively high performance with detecting ETCs of mature stage (mean Average Precision is 86.64%), and an acceptable result for detecting ETCs of all three categories (mean Average Precision 79.34%). We conclude that the single-shot detector model can succeed in detecting ETCs of different stages, and it has demonstrated great potential in the future applications of ETC detection in other relevant settings.



### 3DVNet: Multi-View Depth Prediction and Volumetric Refinement
- **Arxiv ID**: http://arxiv.org/abs/2112.00202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00202v1)
- **Published**: 2021-12-01 00:52:42+00:00
- **Updated**: 2021-12-01 00:52:42+00:00
- **Authors**: Alexander Rich, Noah Stier, Pradeep Sen, Tobias Höllerer
- **Comment**: 10 pages, 6 figures, 3 tables. Accepted to 3DV 2021
- **Journal**: None
- **Summary**: We present 3DVNet, a novel multi-view stereo (MVS) depth-prediction method that combines the advantages of previous depth-based and volumetric MVS approaches. Our key idea is the use of a 3D scene-modeling network that iteratively updates a set of coarse depth predictions, resulting in highly accurate predictions which agree on the underlying scene geometry. Unlike existing depth-prediction techniques, our method uses a volumetric 3D convolutional neural network (CNN) that operates in world space on all depth maps jointly. The network can therefore learn meaningful scene-level priors. Furthermore, unlike existing volumetric MVS techniques, our 3D CNN operates on a feature-augmented point cloud, allowing for effective aggregation of multi-view information and flexible iterative refinement of depth maps. Experimental results show our method exceeds state-of-the-art accuracy in both depth prediction and 3D reconstruction metrics on the ScanNet dataset, as well as a selection of scenes from the TUM-RGBD and ICL-NUIM datasets. This shows that our method is both effective and generalizes to new settings.



### Querying Labelled Data with Scenario Programs for Sim-to-Real Validation
- **Arxiv ID**: http://arxiv.org/abs/2112.00206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.PL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.00206v1)
- **Published**: 2021-12-01 01:04:13+00:00
- **Updated**: 2021-12-01 01:04:13+00:00
- **Authors**: Edward Kim, Jay Shenoy, Sebastian Junges, Daniel Fremont, Alberto Sangiovanni-Vincentelli, Sanjit Seshia
- **Comment**: pre-print
- **Journal**: None
- **Summary**: Simulation-based testing of autonomous vehicles (AVs) has become an essential complement to road testing to ensure safety. Consequently, substantial research has focused on searching for failure scenarios in simulation. However, a fundamental question remains: are AV failure scenarios identified in simulation meaningful in reality, i.e., are they reproducible on the real system? Due to the sim-to-real gap arising from discrepancies between simulated and real sensor data, a failure scenario identified in simulation can be either a spurious artifact of the synthetic sensor data or an actual failure that persists with real sensor data. An approach to validate simulated failure scenarios is to identify instances of the scenario in a corpus of real data, and check if the failure persists on the real data. To this end, we propose a formal definition of what it means for a labelled data item to match an abstract scenario, encoded as a scenario program using the SCENIC probabilistic programming language. Using this definition, we develop a querying algorithm which, given a scenario program and a labelled dataset, finds the subset of data matching the scenario. Experiments demonstrate that our algorithm is accurate and efficient on a variety of realistic traffic scenarios, and scales to a reasonable number of agents.



### Improved sparse PCA method for face and image recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.00207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00207v1)
- **Published**: 2021-12-01 01:11:04+00:00
- **Updated**: 2021-12-01 01:11:04+00:00
- **Authors**: Loc Hoang Tran, Tuan Tran, An Mai
- **Comment**: 11 pages. arXiv admin note: substantial text overlap with
  arXiv:1904.08496
- **Journal**: None
- **Summary**: Face recognition is the very significant field in pattern recognition area. It has multiple applications in military and finance, to name a few. In this paper, the combination of the sparse PCA with the nearest-neighbor method (and with the kernel ridge regression method) will be proposed and will be applied to solve the face recognition problem. Experimental results illustrate that the accuracy of the combination of the sparse PCA method (using the proximal gradient method and the FISTA method) and one specific classification system may be lower than the accuracy of the combination of the PCA method and one specific classification system but sometimes the combination of the sparse PCA method (using the proximal gradient method or the FISTA method) and one specific classification system leads to better accuracy. Moreover, we recognize that the process computing the sparse PCA algorithm using the FISTA method is always faster than the process computing the sparse PCA algorithm using the proximal gradient method.



### PoseKernelLifter: Metric Lifting of 3D Human Pose using Sound
- **Arxiv ID**: http://arxiv.org/abs/2112.00216v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2112.00216v2)
- **Published**: 2021-12-01 01:34:56+00:00
- **Updated**: 2021-12-03 00:26:50+00:00
- **Authors**: Zhijian Yang, Xiaoran Fan, Volkan Isler, Hyun Soo Park
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing the 3D pose of a person in metric scale from a single view image is a geometrically ill-posed problem. For example, we can not measure the exact distance of a person to the camera from a single view image without additional scene assumptions (e.g., known height). Existing learning based approaches circumvent this issue by reconstructing the 3D pose up to scale. However, there are many applications such as virtual telepresence, robotics, and augmented reality that require metric scale reconstruction. In this paper, we show that audio signals recorded along with an image, provide complementary information to reconstruct the metric 3D pose of the person.   The key insight is that as the audio signals traverse across the 3D space, their interactions with the body provide metric information about the body's pose. Based on this insight, we introduce a time-invariant transfer function called pose kernel -- the impulse response of audio signals induced by the body pose. The main properties of the pose kernel are that (1) its envelope highly correlates with 3D pose, (2) the time response corresponds to arrival time, indicating the metric distance to the microphone, and (3) it is invariant to changes in the scene geometry configurations. Therefore, it is readily generalizable to unseen scenes. We design a multi-stage 3D CNN that fuses audio and visual signals and learns to reconstruct 3D pose in a metric scale. We show that our multi-modal method produces accurate metric reconstruction in real world scenes, which is not possible with state-of-the-art lifting approaches including parametric mesh regression and depth regression.



### Scalable Primitives for Generalized Sensor Fusion in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2112.00219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.00219v1)
- **Published**: 2021-12-01 01:43:15+00:00
- **Updated**: 2021-12-01 01:43:15+00:00
- **Authors**: Sammy Sidhu, Linda Wang, Tayyab Naseer, Ashish Malhotra, Jay Chia, Aayush Ahuja, Ella Rasmussen, Qiangui Huang, Ray Gao
- **Comment**: Presented in Machine Learning for Autonomous Driving Workshop at the
  35th Conference on Neural Information Processing Systems (NeurIPS 2021),
  Sydney, Australia. 11 pages, 8 figures
- **Journal**: None
- **Summary**: In autonomous driving, there has been an explosion in the use of deep neural networks for perception, prediction and planning tasks. As autonomous vehicles (AVs) move closer to production, multi-modal sensor inputs and heterogeneous vehicle fleets with different sets of sensor platforms are becoming increasingly common in the industry. However, neural network architectures typically target specific sensor platforms and are not robust to changes in input, making the problem of scaling and model deployment particularly difficult. Furthermore, most players still treat the problem of optimizing software and hardware as entirely independent problems. We propose a new end to end architecture, Generalized Sensor Fusion (GSF), which is designed in such a way that both sensor inputs and target tasks are modular and modifiable. This enables AV system designers to easily experiment with different sensor configurations and methods and opens up the ability to deploy on heterogeneous fleets using the same models that are shared across a large engineering organization. Using this system, we report experimental results where we demonstrate near-parity of an expensive high-density (HD) LiDAR sensor with a cheap low-density (LD) LiDAR plus camera setup in the 3D object detection task. This paves the way for the industry to jointly design hardware and software architectures as well as large fleets with heterogeneous configurations.



### MC-Blur: A Comprehensive Benchmark for Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2112.00234v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00234v2)
- **Published**: 2021-12-01 02:10:42+00:00
- **Updated**: 2022-06-10 03:59:04+00:00
- **Authors**: Kaihao Zhang, Tao Wang, Wenhan Luo, Boheng Chen, Wenqi Ren, Bjorn Stenger, Wei Liu, Hongdong Li, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Blur artifacts can seriously degrade the visual quality of images, and numerous deblurring methods have been proposed for specific scenarios. However, in most real-world images, blur is caused by different factors, e.g., motion and defocus. In this paper, we address how different deblurring methods perform in the case of multiple types of blur. For in-depth performance evaluation, we construct a new large-scale multi-cause image deblurring dataset (called MC-Blur), including real-world and synthesized blurry images with mixed factors of blurs. The images in the proposed MC-Blur dataset are collected using different techniques: averaging sharp images captured by a 1000-fps high-speed camera, convolving Ultra-High-Definition (UHD) sharp images with large-size kernels, adding defocus to images, and real-world blurry images captured by various camera models. Based on the MC-Blur dataset, we conduct extensive benchmarking studies to compare SOTA methods in different scenarios, analyze their efficiency, and investigate the built dataset's capacity. These benchmarking results provide a comprehensive overview of the advantages and limitations of current deblurring methods, and reveal the advances of our dataset.



### VoRTX: Volumetric 3D Reconstruction With Transformers for Voxelwise View Selection and Fusion
- **Arxiv ID**: http://arxiv.org/abs/2112.00236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00236v1)
- **Published**: 2021-12-01 02:18:11+00:00
- **Updated**: 2021-12-01 02:18:11+00:00
- **Authors**: Noah Stier, Alexander Rich, Pradeep Sen, Tobias Höllerer
- **Comment**: 3DV 2021
- **Journal**: None
- **Summary**: Recent volumetric 3D reconstruction methods can produce very accurate results, with plausible geometry even for unobserved surfaces. However, they face an undesirable trade-off when it comes to multi-view fusion. They can fuse all available view information by global averaging, thus losing fine detail, or they can heuristically cluster views for local fusion, thus restricting their ability to consider all views jointly. Our key insight is that greater detail can be retained without restricting view diversity by learning a view-fusion function conditioned on camera pose and image content. We propose to learn this multi-view fusion using a transformer. To this end, we introduce VoRTX, an end-to-end volumetric 3D reconstruction network using transformers for wide-baseline, multi-view feature fusion. Our model is occlusion-aware, leveraging the transformer architecture to predict an initial, projective scene geometry estimate. This estimate is used to avoid backprojecting image features through surfaces into occluded regions. We train our model on ScanNet and show that it produces better reconstructions than state-of-the-art methods. We also demonstrate generalization without any fine-tuning, outperforming the same state-of-the-art methods on two other datasets, TUM-RGBD and ICL-NUIM.



### AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions
- **Arxiv ID**: http://arxiv.org/abs/2112.00246v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.00246v6)
- **Published**: 2021-12-01 03:00:05+00:00
- **Updated**: 2023-05-04 14:47:16+00:00
- **Authors**: Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas Guibas, Hao Dong
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Perceiving and interacting with 3D articulated objects, such as cabinets, doors, and faucets, pose particular challenges for future home-assistant robots performing daily tasks in human environments. Besides parsing the articulated parts and joint parameters, researchers recently advocate learning manipulation affordance over the input shape geometry which is more task-aware and geometrically fine-grained. However, taking only passive observations as inputs, these methods ignore many hidden but important kinematic constraints (e.g., joint location and limits) and dynamic factors (e.g., joint friction and restitution), therefore losing significant accuracy for test cases with such uncertainties. In this paper, we propose a novel framework, named AdaAfford, that learns to perform very few test-time interactions for quickly adapting the affordance priors to more accurate instance-specific posteriors. We conduct large-scale experiments using the PartNet-Mobility dataset and prove that our system performs better than baselines.



### Shallow Network Based on Depthwise Over-Parameterized Convolution for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.00250v1
- **DOI**: 10.1109/LGRS.2021.3133598
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00250v1)
- **Published**: 2021-12-01 03:10:02+00:00
- **Updated**: 2021-12-01 03:10:02+00:00
- **Authors**: Hongmin Gao, Zhonghao Chen, Chenming Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural network (CNN) techniques have gained popularity as a tool for hyperspectral image classification (HSIC). To improve the feature extraction efficiency of HSIC under the condition of limited samples, the current methods generally use deep models with plenty of layers. However, deep network models are prone to overfitting and gradient vanishing problems when samples are limited. In addition, the spatial resolution decreases severely with deeper depth, which is very detrimental to spatial edge feature extraction. Therefore, this letter proposes a shallow model for HSIC, which is called depthwise over-parameterized convolutional neural network (DOCNN). To ensure the effective extraction of the shallow model, the depthwise over-parameterized convolution (DO-Conv) kernel is introduced to extract the discriminative features. The depthwise over-parameterized Convolution kernel is composed of a standard convolution kernel and a depthwise convolution kernel, which can extract the spatial feature of the different channels individually and fuse the spatial features of the whole channels simultaneously. Moreover, to further reduce the loss of spatial edge features due to the convolution operation, a dense residual connection (DRC) structure is proposed to apply to the feature extraction part of the whole network. Experimental results obtained from three benchmark data sets show that the proposed method outperforms other state-of-the-art methods in terms of classification accuracy and computational efficiency.



### Ranking Distance Calibration for Cross-Domain Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00260v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00260v2)
- **Published**: 2021-12-01 03:36:58+00:00
- **Updated**: 2022-03-23 04:25:48+00:00
- **Authors**: Pan Li, Shaogang Gong, Chengjie Wang, Yanwei Fu
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: Recent progress in few-shot learning promotes a more realistic cross-domain setting, where the source and target datasets are from different domains. Due to the domain gap and disjoint label spaces between source and target datasets, their shared knowledge is extremely limited. This encourages us to explore more information in the target domain rather than to overly elaborate training strategies on the source domain as in many existing methods. Hence, we start from a generic representation pre-trained by a cross-entropy loss and a conventional distance-based classifier, along with an image retrieval view, to employ a re-ranking process for calibrating a target distance matrix by discovering the reciprocal k-nearest neighbours within the task. Assuming the pre-trained representation is biased towards the source, we construct a non-linear subspace to minimise task-irrelevant features therewithin while keep more transferrable discriminative information by a hyperbolic tangent transformation. The calibrated distance in this target-aware non-linear subspace is complementary to that in the pre-trained representation. To impose such distance calibration information onto the pre-trained representation, a Kullback-Leibler divergence loss is employed to gradually guide the model towards the calibrated distance-based distribution. Extensive evaluations on eight target domains show that this target ranking calibration process can improve conventional distance-based classifiers in few-shot learning.



### GLocal: Global Graph Reasoning and Local Structure Transfer for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2112.00263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00263v1)
- **Published**: 2021-12-01 03:54:30+00:00
- **Updated**: 2021-12-01 03:54:30+00:00
- **Authors**: Liyuan Ma, Kejie Huang, Dongxu Wei, Haibin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on person image generation, namely, generating person image under various conditions, e.g., corrupted texture or different pose. To address texture occlusion and large pose misalignment in this task, previous works just use the corresponding region's style to infer the occluded area and rely on point-wise alignment to reorganize the context texture information, lacking the ability to globally correlate the region-wise style codes and preserve the local structure of the source. To tackle these problems, we present a GLocal framework to improve the occlusion-aware texture estimation by globally reasoning the style inter-correlations among different semantic regions, which can also be employed to recover the corrupted images in texture inpainting. For local structural information preservation, we further extract the local structure of the source image and regain it in the generated image via local structure transfer. We benchmark our method to fully characterize its performance on DeepFashion dataset and present extensive ablation studies that highlight the novelty of our method.



### Training BatchNorm Only in Neural Architecture Search and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2112.00265v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00265v1)
- **Published**: 2021-12-01 04:09:09+00:00
- **Updated**: 2021-12-01 04:09:09+00:00
- **Authors**: Yichen Zhu, Jie Du, Yuqin Zhu, Yi Wang, Zhicai Ou, Feifei Feng, Jian Tang
- **Comment**: 11 pages Technical report
- **Journal**: None
- **Summary**: This work investigates the usage of batch normalization in neural architecture search (NAS). Specifically, Frankle et al. find that training BatchNorm only can achieve nontrivial performance. Furthermore, Chen et al. claim that training BatchNorm only can speed up the training of the one-shot NAS supernet over ten times. Critically, there is no effort to understand 1) why training BatchNorm only can find the perform-well architectures with the reduced supernet-training time, and 2) what is the difference between the train-BN-only supernet and the standard-train supernet. We begin by showing that the train-BN-only networks converge to the neural tangent kernel regime, obtain the same training dynamics as train all parameters theoretically. Our proof supports the claim to train BatchNorm only on supernet with less training time. Then, we empirically disclose that train-BN-only supernet provides an advantage on convolutions over other operators, cause unfair competition between architectures. This is due to only the convolution operator being attached with BatchNorm. Through experiments, we show that such unfairness makes the search algorithm prone to select models with convolutions. To solve this issue, we introduce fairness in the search space by placing a BatchNorm layer on every operator. However, we observe that the performance predictor in Chen et al. is inapplicable on the new search space. To this end, we propose a novel composite performance indicator to evaluate networks from three perspectives: expressivity, trainability, and uncertainty, derived from the theoretical property of BatchNorm. We demonstrate the effectiveness of our approach on multiple NAS-benchmarks (NAS-Bench101, NAS-Bench-201) and search spaces (DARTS search space and MobileNet search space).



### FDA-GAN: Flow-based Dual Attention GAN for Human Pose Transfer
- **Arxiv ID**: http://arxiv.org/abs/2112.00281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00281v1)
- **Published**: 2021-12-01 05:10:37+00:00
- **Updated**: 2021-12-01 05:10:37+00:00
- **Authors**: Liyuan Ma, Kejie Huang, Dongxu Wei, Zhaoyan Ming, Haibin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose transfer aims at transferring the appearance of the source person to the target pose. Existing methods utilizing flow-based warping for non-rigid human image generation have achieved great success. However, they fail to preserve the appearance details in synthesized images since the spatial correlation between the source and target is not fully exploited. To this end, we propose the Flow-based Dual Attention GAN (FDA-GAN) to apply occlusion- and deformation-aware feature fusion for higher generation quality. Specifically, deformable local attention and flow similarity attention, constituting the dual attention mechanism, can derive the output features responsible for deformable- and occlusion-aware fusion, respectively. Besides, to maintain the pose and global position consistency in transferring, we design a pose normalization network for learning adaptive normalization from the target pose to the source person. Both qualitative and quantitative results show that our method outperforms state-of-the-art models in public iPER and DeepFashion datasets.



### Point Cloud Segmentation Using Sparse Temporal Local Attention
- **Arxiv ID**: http://arxiv.org/abs/2112.00289v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.00289v2)
- **Published**: 2021-12-01 06:00:50+00:00
- **Updated**: 2021-12-02 06:16:52+00:00
- **Authors**: Joshua Knights, Peyman Moghadam, Clinton Fookes, Sridha Sridharan
- **Comment**: 8 pages, 3 figures Published at the Australasian Conference on
  Robotics and Automation (ACRA) 2021
- **Journal**: None
- **Summary**: Point clouds are a key modality used for perception in autonomous vehicles, providing the means for a robust geometric understanding of the surrounding environment. However despite the sensor outputs from autonomous vehicles being naturally temporal in nature, there is still limited exploration of exploiting point cloud sequences for 3D seman-tic segmentation. In this paper we propose a novel Sparse Temporal Local Attention (STELA) module which aggregates intermediate features from a local neighbourhood in previous point cloud frames to provide a rich temporal context to the decoder. Using the sparse local neighbourhood enables our approach to gather features more flexibly than those which directly match point features, and more efficiently than those which perform expensive global attention over the whole point cloud frame. We achieve a competitive mIoU of 64.3% on the SemanticKitti dataset, and demonstrate significant improvement over the single-frame baseline in our ablation studies.



### Unsupervised Statistical Learning for Die Analysis in Ancient Numismatics
- **Arxiv ID**: http://arxiv.org/abs/2112.00290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00290v1)
- **Published**: 2021-12-01 06:02:07+00:00
- **Updated**: 2021-12-01 06:02:07+00:00
- **Authors**: Andreas Heinecke, Emanuel Mayer, Abhinav Natarajan, Yoonju Jung
- **Comment**: None
- **Journal**: None
- **Summary**: Die analysis is an essential numismatic method, and an important tool of ancient economic history. Yet, manual die studies are too labor-intensive to comprehensively study large coinages such as those of the Roman Empire. We address this problem by proposing a model for unsupervised computational die analysis, which can reduce the time investment necessary for large-scale die studies by several orders of magnitude, in many cases from years to weeks. From a computer vision viewpoint, die studies present a challenging unsupervised clustering problem, because they involve an unknown and large number of highly similar semantic classes of imbalanced sizes. We address these issues through determining dissimilarities between coin faces derived from specifically devised Gaussian process-based keypoint features in a Bayesian distance clustering framework. The efficacy of our method is demonstrated through an analysis of 1135 Roman silver coins struck between 64-66 C.E..



### Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2112.00295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00295v1)
- **Published**: 2021-12-01 06:11:43+00:00
- **Updated**: 2021-12-01 06:11:43+00:00
- **Authors**: Kai Zhang, Yifan Sun, Rui Wang, Haichang Li, Xiaohui Hu
- **Comment**: 13 pages, 2 figures, submitted to BMVC2021
- **Journal**: None
- **Summary**: This paper challenges the cross-domain semantic segmentation task, aiming to improve the segmentation accuracy on the unlabeled target domain without incurring additional annotation. Using the pseudo-label-based unsupervised domain adaptation (UDA) pipeline, we propose a novel and effective Multiple Fusion Adaptation (MFA) method. MFA basically considers three parallel information fusion strategies, i.e., the cross-model fusion, temporal fusion and a novel online-offline pseudo label fusion. Specifically, the online-offline pseudo label fusion encourages the adaptive training to pay additional attention to difficult regions that are easily ignored by offline pseudo labels, therefore retaining more informative details. While the other two fusion strategies may look standard, MFA pays significant efforts to raise the efficiency and effectiveness for integration, and succeeds in injecting all the three strategies into a unified framework. Experiments on two widely used benchmarks, i.e., GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes, show that our method significantly improves the semantic segmentation adaptation, and sets up new state of the art (58.2% and 62.5% mIoU, respectively). The code will be available at https://github.com/KaiiZhang/MFA.



### Graph Convolutional Module for Temporal Action Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2112.00302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00302v1)
- **Published**: 2021-12-01 06:36:59+00:00
- **Updated**: 2021-12-01 06:36:59+00:00
- **Authors**: Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, Chuang Gan
- **Comment**: Accepted by T-PAMI
- **Journal**: None
- **Summary**: Temporal action localization has long been researched in computer vision. Existing state-of-the-art action localization methods divide each video into multiple action units (i.e., proposals in two-stage methods and segments in one-stage methods) and then perform action recognition/regression on each of them individually, without explicitly exploiting their relations during learning. In this paper, we claim that the relations between action units play an important role in action localization, and a more powerful action detector should not only capture the local content of each action unit but also allow a wider field of view on the context related to it. To this end, we propose a general graph convolutional module (GCM) that can be easily plugged into existing action localization methods, including two-stage and one-stage paradigms. To be specific, we first construct a graph, where each action unit is represented as a node and their relations between two action units as an edge. Here, we use two types of relations, one for capturing the temporal connections between different action units, and the other one for characterizing their semantic relationship. Particularly for the temporal connections in two-stage methods, we further explore two different kinds of edges, one connecting the overlapping action units and the other one connecting surrounding but disjointed units. Upon the graph we built, we then apply graph convolutional networks (GCNs) to model the relations among different action units, which is able to learn more informative representations to enhance action localization. Experimental results show that our GCM consistently improves the performance of existing action localization methods, including two-stage methods (e.g., CBR and R-C3D) and one-stage methods (e.g., D-SSAD), verifying the generality and effectiveness of our GCM.



### Total-Body Low-Dose CT Image Denoising using Prior Knowledge Transfer Technique with Contrastive Regularization Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2112.00729v2
- **DOI**: 10.1002/mp.16163
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2112.00729v2)
- **Published**: 2021-12-01 06:46:38+00:00
- **Updated**: 2021-12-06 02:17:16+00:00
- **Authors**: Minghan Fu, Yanhua Duan, Zhaoping Cheng, Wenjian Qin, Ying Wang, Dong Liang, Zhanli Hu
- **Comment**: Want to improve the methodology
- **Journal**: None
- **Summary**: Reducing the radiation exposure for patients in Total-body CT scans has attracted extensive attention in the medical imaging community. Given the fact that low radiation dose may result in increased noise and artifacts, which greatly affected the clinical diagnosis. To obtain high-quality Total-body Low-dose CT (LDCT) images, previous deep-learning-based research work has introduced various network architectures. However, most of these methods only adopt Normal-dose CT (NDCT) images as ground truths to guide the training of the denoising network. Such simple restriction leads the model to less effectiveness and makes the reconstructed images suffer from over-smoothing effects. In this paper, we propose a novel intra-task knowledge transfer method that leverages the distilled knowledge from NDCT images to assist the training process on LDCT images. The derived architecture is referred to as the Teacher-Student Consistency Network (TSC-Net), which consists of the teacher network and the student network with identical architecture. Through the supervision between intermediate features, the student network is encouraged to imitate the teacher network and gain abundant texture details. Moreover, to further exploit the information contained in CT scans, a contrastive regularization mechanism (CRM) built upon contrastive learning is introduced.CRM performs to pull the restored CT images closer to the NDCT samples and push far away from the LDCT samples in the latent space. In addition, based on the attention and deformable convolution mechanism, we design a Dynamic Enhancement Module (DEM) to improve the network transformation capability.



### Forward Operator Estimation in Generative Models with Kernel Transfer Operators
- **Arxiv ID**: http://arxiv.org/abs/2112.00305v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2112.00305v1)
- **Published**: 2021-12-01 06:54:31+00:00
- **Updated**: 2021-12-01 06:54:31+00:00
- **Authors**: Zhichun Huang, Rudrasis Chakraborty, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models which use explicit density modeling (e.g., variational autoencoders, flow-based generative models) involve finding a mapping from a known distribution, e.g. Gaussian, to the unknown input distribution. This often requires searching over a class of non-linear functions (e.g., representable by a deep neural network). While effective in practice, the associated runtime/memory costs can increase rapidly, usually as a function of the performance desired in an application. We propose a much cheaper (and simpler) strategy to estimate this mapping based on adapting known results in kernel transfer operators. We show that our formulation enables highly efficient distribution approximation and sampling, and offers surprisingly good empirical performance that compares favorably with powerful baselines, but with significant runtime savings. We show that the algorithm also performs well in small sample size settings (in brain imaging).



### Unleashing the Potential of Unsupervised Pre-Training with Intra-Identity Regularization for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2112.00317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2112.00317v1)
- **Published**: 2021-12-01 07:16:37+00:00
- **Updated**: 2021-12-01 07:16:37+00:00
- **Authors**: Zizheng Yang, Xin Jin, Kecheng Zheng, Feng Zhao
- **Comment**: Technical report, code: https://github.com/Frost-Yang-99/UP-ReID
- **Journal**: None
- **Summary**: Existing person re-identification (ReID) methods typically directly load the pre-trained ImageNet weights for initialization. However, as a fine-grained classification task, ReID is more challenging and exists a large domain gap between ImageNet classification. Inspired by the great success of self-supervised representation learning with contrastive objectives, in this paper, we design an Unsupervised Pre-training framework for ReID based on the contrastive learning (CL) pipeline, dubbed UP-ReID. During the pre-training, we attempt to address two critical issues for learning fine-grained ReID features: (1) the augmentations in CL pipeline may distort the discriminative clues in person images. (2) the fine-grained local features of person images are not fully-explored. Therefore, we introduce an intra-identity (I$^2$-)regularization in the UP-ReID, which is instantiated as two constraints coming from global image aspect and local patch aspect: a global consistency is enforced between augmented and original person images to increase robustness to augmentation, while an intrinsic contrastive constraint among local patches of each image is employed to fully explore the local discriminative clues. Extensive experiments on multiple popular Re-ID datasets, including PersonX, Market1501, CUHK03, and MSMT17, demonstrate that our UP-ReID pre-trained model can significantly benefit the downstream ReID fine-tuning and achieve state-of-the-art performance. Codes and models will be released to https://github.com/Frost-Yang-99/UP-ReID.



### Object-Aware Cropping for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00319v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00319v2)
- **Published**: 2021-12-01 07:23:37+00:00
- **Updated**: 2023-04-06 20:05:35+00:00
- **Authors**: Shlok Mishra, Anshul Shah, Ankan Bansal, Abhyuday Jagannatha, Janit Anjaria, Abhishek Sharma, David Jacobs, Dilip Krishnan
- **Comment**: None
- **Journal**: Transactions on Machine Learning Research 2022
- **Summary**: A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both object and scene level semantic representations. Using this approach, which we call object-aware cropping, results in significant improvements over scene cropping on classification and object detection benchmarks. For example, on OpenImages, our approach achieves an improvement of 8.8% mAP over random scene-level cropping using MoCo-v2 based pre-training. We also show significant improvements on COCO and PASCAL-VOC object detection and segmentation tasks over the state-of-the-art self-supervised learning approaches. Our approach is efficient, simple and general, and can be used in most existing contrastive and non-contrastive self-supervised learning frameworks.



### FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.00322v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00322v2)
- **Published**: 2021-12-01 07:28:52+00:00
- **Updated**: 2022-03-24 06:12:39+00:00
- **Authors**: Danila Rukhovich, Anna Vorontsova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, promising applications in robotics and augmented reality have attracted considerable attention to 3D object detection from point clouds. In this paper, we present FCAF3D - a first-in-class fully convolutional anchor-free indoor 3D object detection method. It is a simple yet effective method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions. FCAF3D can handle large-scale scenes with minimal runtime through a single fully convolutional feed-forward pass. Existing 3D object detection methods make prior assumptions on the geometry of objects, and we argue that it limits their generalization ability. To get rid of any prior assumptions, we propose a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. The proposed method achieves state-of-the-art 3D object detection results in terms of mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The code and models are available at https://github.com/samsunglabs/fcaf3d.



### Highly accelerated MR parametric mapping by undersampling the k-space and reducing the contrast number simultaneously with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00730v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00730v1)
- **Published**: 2021-12-01 07:29:29+00:00
- **Updated**: 2021-12-01 07:29:29+00:00
- **Authors**: Yanjie Zhu, Haoxiang Li, Yuanyuan Liu, Muzi Guo, Guanxun Cheng, Gang Yang, Haifeng Wang, Dong Liang
- **Comment**: 27 pages,11 figures. Submitted to Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Purpose: To propose a novel deep learning-based method called RG-Net (reconstruction and generation network) for highly accelerated MR parametric mapping by undersampling k-space and reducing the acquired contrast number simultaneously.   Methods: The proposed framework consists of a reconstruction module and a generative module. The reconstruction module reconstructs MR images from the acquired few undersampled k-space data with the help of a data prior. The generative module then synthesizes the remaining multi-contrast images from the reconstructed images, where the exponential model is implicitly incorporated into the image generation through the supervision of fully sampled labels. The RG-Net was evaluated on the T1\r{ho} mapping data of knee and brain at different acceleration rates. Regional T1\r{ho} analysis for cartilage and the brain was performed to access the performance of RG-Net.   Results: RG-Net yields a high-quality T1\r{ho} map at a high acceleration rate of 17. Compared with the competing methods that only undersample k-space, our framework achieves better performance in T1\r{ho} value analysis. Our method also improves quality of T1\r{ho} maps on patient with glioma.   Conclusion: The proposed RG-Net that adopted a new strategy by undersampling k-space and reducing the contrast number simultaneously for fast MR parametric mapping, can achieve a high acceleration rate while maintaining good reconstruction quality. The generative module of our framework can also be used as an insert module in other fast MR parametric mapping methods.   Keywords: Deep learning, convolutional neural network, fast MR parametric mapping



### Push Stricter to Decide Better: A Class-Conditional Feature Adaptive Framework for Improving Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2112.00323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00323v1)
- **Published**: 2021-12-01 07:37:56+00:00
- **Updated**: 2021-12-01 07:37:56+00:00
- **Authors**: Jia-Li Yin, Lehui Xie, Wanqing Zhu, Ximeng Liu, Bo-Hao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In response to the threat of adversarial examples, adversarial training provides an attractive option for enhancing the model robustness by training models on online-augmented adversarial examples. However, most of the existing adversarial training methods focus on improving the robust accuracy by strengthening the adversarial examples but neglecting the increasing shift between natural data and adversarial examples, leading to a dramatic decrease in natural accuracy. To maintain the trade-off between natural and robust accuracy, we alleviate the shift from the perspective of feature adaption and propose a Feature Adaptive Adversarial Training (FAAT) optimizing the class-conditional feature adaption across natural data and adversarial examples. Specifically, we propose to incorporate a class-conditional discriminator to encourage the features become (1) class-discriminative and (2) invariant to the change of adversarial attacks. The novel FAAT framework enables the trade-off between natural and robust accuracy by generating features with similar distribution across natural and adversarial data, and achieve higher overall robustness benefited from the class-discriminative feature characteristics. Experiments on various datasets demonstrate that FAAT produces more discriminative features and performs favorably against state-of-the-art methods. Codes are available at https://github.com/VisionFlow/FAAT.



### Optimizing for In-memory Deep Learning with Emerging Memory Technology
- **Arxiv ID**: http://arxiv.org/abs/2112.00324v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.ET
- **Links**: [PDF](http://arxiv.org/pdf/2112.00324v1)
- **Published**: 2021-12-01 07:39:18+00:00
- **Updated**: 2021-12-01 07:39:18+00:00
- **Authors**: Zhehui Wang, Tao Luo, Rick Siow Mong Goh, Wei Zhang, Weng-Fai Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In-memory deep learning computes neural network models where they are stored, thus avoiding long distance communication between memory and computation units, resulting in considerable savings in energy and time. In-memory deep learning has already demonstrated orders of magnitude higher performance density and energy efficiency. The use of emerging memory technology promises to increase the gains in density, energy, and performance even further. However, emerging memory technology is intrinsically unstable, resulting in random fluctuations of data reads. This can translate to non-negligible accuracy loss, potentially nullifying the gains. In this paper, we propose three optimization techniques that can mathematically overcome the instability problem of emerging memory technology. They can improve the accuracy of the in-memory deep learning model while maximizing its energy efficiency. Experiments show that our solution can fully recover most models' state-of-the-art accuracy, and achieves at least an order of magnitude higher energy efficiency than the state-of-the-art.



### Multi-View Stereo with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2112.00336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00336v1)
- **Published**: 2021-12-01 08:06:59+00:00
- **Updated**: 2021-12-01 08:06:59+00:00
- **Authors**: Jie Zhu, Bo Peng, Wanqing Li, Haifeng Shen, Zhe Zhang, Jianjun Lei
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a network, referred to as MVSTR, for Multi-View Stereo (MVS). It is built upon Transformer and is capable of extracting dense features with global context and 3D consistency, which are crucial to achieving reliable matching for MVS. Specifically, to tackle the problem of the limited receptive field of existing CNN-based MVS methods, a global-context Transformer module is first proposed to explore intra-view global context. In addition, to further enable dense features to be 3D-consistent, a 3D-geometry Transformer module is built with a well-designed cross-view attention mechanism to facilitate inter-view information interaction. Experimental results show that the proposed MVSTR achieves the best overall performance on the DTU dataset and strong generalization on the Tanks & Temples benchmark dataset.



### A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.00337v2
- **DOI**: 10.1016/j.eswa.2023.120461
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00337v2)
- **Published**: 2021-12-01 08:07:01+00:00
- **Updated**: 2023-08-01 03:45:57+00:00
- **Authors**: Jihyo Kim, Jiin Koo, Sangheum Hwang
- **Comment**: Published in ESWA
  (https://www.sciencedirect.com/science/article/pii/S0957417423009636)
- **Journal**: Expert Systems with Applications (2023), Vol. 229, Part A, 120461
- **Summary**: Deep neural networks have achieved outstanding performance over various tasks, but they have a critical issue: over-confident predictions even for completely unknown samples. Many studies have been proposed to successfully filter out these unknown samples, but they only considered narrow and specific tasks, referred to as misclassification detection, open-set recognition, or out-of-distribution detection. In this work, we argue that these tasks should be treated as fundamentally an identical problem because an ideal model should possess detection capability for all those tasks. Therefore, we introduce the unknown detection task, an integration of previous individual tasks, for a rigorous examination of the detection capability of deep neural networks on a wide spectrum of unknown samples. To this end, unified benchmark datasets on different scales were constructed and the unknown detection capabilities of existing popular methods were subject to comparison. We found that Deep Ensemble consistently outperforms the other approaches in detecting unknowns; however, all methods are only successful for a specific type of unknown. The reproducible code and benchmark datasets are available at https://github.com/daintlab/unknown-detection-benchmarks .



### Confidence Propagation Cluster: Unleash Full Potential of Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2112.00342v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00342v4)
- **Published**: 2021-12-01 08:22:00+00:00
- **Updated**: 2022-03-20 14:06:59+00:00
- **Authors**: Yichun Shen, Wanli Jiang, Zhen Xu, Rundong Li, Junghyun Kwon, Siyi Li
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: It has been a long history that most object detection methods obtain objects by using the non-maximum suppression (NMS) and its improved versions like Soft-NMS to remove redundant bounding boxes. We challenge those NMS-based methods from three aspects: 1) The bounding box with highest confidence value may not be the true positive having the biggest overlap with the ground-truth box. 2) Not only suppression is required for redundant boxes, but also confidence enhancement is needed for those true positives. 3) Sorting candidate boxes by confidence values is not necessary so that full parallelism is achievable.   In this paper, inspired by belief propagation (BP), we propose the Confidence Propagation Cluster (CP-Cluster) to replace NMS-based methods, which is fully parallelizable as well as better in accuracy. In CP-Cluster, we borrow the message passing mechanism from BP to penalize redundant boxes and enhance true positives simultaneously in an iterative way until convergence. We verified the effectiveness of CP-Cluster by applying it to various mainstream detectors such as FasterRCNN, SSD, FCOS, YOLOv3, YOLOv5, Centernet etc. Experiments on MS COCO show that our plug and play method, without retraining detectors, is able to steadily improve average mAP of all those state-of-the-art models with a clear margin from 0.3 to 1.9 respectively when compared with NMS-based methods.



### Camera Motion Agnostic 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.00343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00343v1)
- **Published**: 2021-12-01 08:22:50+00:00
- **Updated**: 2021-12-01 08:22:50+00:00
- **Authors**: Seong Hyun Kim, Sunwon Jeong, Sungbum Park, Ju Yong Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Although the performance of 3D human pose and shape estimation methods has improved significantly in recent years, existing approaches typically generate 3D poses defined in camera or human-centered coordinate system. This makes it difficult to estimate a person's pure pose and motion in world coordinate system for a video captured using a moving camera. To address this issue, this paper presents a camera motion agnostic approach for predicting 3D human pose and mesh defined in the world coordinate system. The core idea of the proposed approach is to estimate the difference between two adjacent global poses (i.e., global motion) that is invariant to selecting the coordinate system, instead of the global pose coupled to the camera motion. To this end, we propose a network based on bidirectional gated recurrent units (GRUs) that predicts the global motion sequence from the local pose sequence consisting of relative rotations of joints called global motion regressor (GMR). We use 3DPW and synthetic datasets, which are constructed in a moving-camera environment, for evaluation. We conduct extensive experiments and prove the effectiveness of the proposed method empirically. Code and datasets are available at https://github.com/seonghyunkim1212/GMR



### Automatic travel pattern extraction from visa page stamps using CNN models
- **Arxiv ID**: http://arxiv.org/abs/2112.00348v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00348v2)
- **Published**: 2021-12-01 08:54:29+00:00
- **Updated**: 2022-03-03 09:27:17+00:00
- **Authors**: Eimantas Ledinauskas, Julius Ruseckas, Julius Marozas, Kasparas Karlauskas, Justas Terentjevas, Augustas Mačijauskas, Alfonsas Juršėnas
- **Comment**: 15 pages, 13 figures, 4 tables, submitted for peer review
- **Journal**: None
- **Summary**: Manual travel pattern inference from visa page stamps is a time consuming activity and constitutes an important bottleneck in the efficiency of traveler inspection at border crossings. Despite efforts to digitize and record the border crossing information into databases, travel pattern inference from stamps will remain a problem until every country in the world is incorporated into such a unified system. This could take decades. We propose an automated document analysis system that processes scanned visa pages and automatically extracts the travel pattern from detected stamps. The system processes the page via the following pipeline: stamp detection in the visa page; general stamp country and entry/exit recognition; Schengen area stamp country and entry/exit recognition; Schengen area stamp date extraction. For each stage of the proposed pipeline we construct neural network models and train then on a mixture of real and synthetic data. We integrated Schengen area stamp detection and date, country, entry/exit recognition models together with a graphical user interface into a prototype of an automatic travel pattern extraction tool. We find that by combining simple neural network models into our proposed pipeline a useful tool can be created which can speed up the travel pattern extraction significantly.



### CLIPstyler: Image Style Transfer with a Single Text Condition
- **Arxiv ID**: http://arxiv.org/abs/2112.00374v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00374v3)
- **Published**: 2021-12-01 09:48:53+00:00
- **Updated**: 2022-03-19 11:35:18+00:00
- **Authors**: Gihyun Kwon, Jong Chul Ye
- **Comment**: CVPR 2022 camera ready
- **Journal**: None
- **Summary**: Existing neural style transfer methods require reference style images to transfer texture information of style images to content images. However, in many practical situations, users may not have reference style images but still be interested in transferring styles by just imagining them. In order to deal with such applications, we propose a new framework that enables a style transfer `without' a style image, but only with a text description of the desired style. Using the pre-trained text-image embedding model of CLIP, we demonstrate the modulation of the style of content images only with a single text condition. Specifically, we propose a patch-wise text-image matching loss with multiview augmentations for realistic texture transfer. Extensive experimental results confirmed the successful image style transfer with realistic textures that reflect semantic query texts.



### $\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2112.00378v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00378v2)
- **Published**: 2021-12-01 09:55:01+00:00
- **Updated**: 2022-07-19 05:07:05+00:00
- **Authors**: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
- **Comment**: Accepted to the 17th European Conference on Computer Vision (ECCV
  2022)
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches in training robust models against such attacks. However, it is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration, hampering its effectiveness. Recently, Fast Adversarial Training (FAT) was proposed that can obtain robust models efficiently. However, the reasons behind its success are not fully understood, and more importantly, it can only train robust models for $\ell_\infty$-bounded attacks as it uses FGSM during training. In this paper, by leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a general, more principled approach toward reducing the time complexity of robust training. Unlike existing methods, our approach can be adapted to a wide variety of training objectives, including TRADES, $\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental results indicate that our approach speeds up adversarial training by 2-3 times while experiencing a slight reduction in the clean and robust accuracy.



### Deep Measurement Updates for Bayes Filters
- **Arxiv ID**: http://arxiv.org/abs/2112.00380v1
- **DOI**: 10.1109/LRA.2021.3128687
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.00380v1)
- **Published**: 2021-12-01 10:00:37+00:00
- **Updated**: 2021-12-01 10:00:37+00:00
- **Authors**: Johannes Pankert, Maria Vittoria Minniti, Lorenz Wellhausen, Marco Hutter
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters, vol. 7, no. 1, pp. 414-421,
  Jan. 2022
- **Summary**: Measurement update rules for Bayes filters often contain hand-crafted heuristics to compute observation probabilities for high-dimensional sensor data, like images. In this work, we propose the novel approach Deep Measurement Update (DMU) as a general update rule for a wide range of systems. DMU has a conditional encoder-decoder neural network structure to process depth images as raw inputs. Even though the network is trained only on synthetic data, the model shows good performance at evaluation time on real-world data. With our proposed training scheme primed data training , we demonstrate how the DMU models can be trained efficiently to be sensitive to condition variables without having to rely on a stochastic information bottleneck. We validate the proposed methods in multiple scenarios of increasing complexity, beginning with the pose estimation of a single object to the joint estimation of the pose and the internal state of an articulated system. Moreover, we provide a benchmark against Articulated Signed Distance Functions(A-SDF) on the RBO dataset as a baseline comparison for articulation state estimation.



### Exploration into Translation-Equivariant Image Quantization
- **Arxiv ID**: http://arxiv.org/abs/2112.00384v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00384v3)
- **Published**: 2021-12-01 10:08:24+00:00
- **Updated**: 2023-02-26 12:28:02+00:00
- **Authors**: Woncheol Shin, Gyubok Lee, Jiyoung Lee, Eunyi Lyou, Joonseok Lee, Edward Choi
- **Comment**: ICASSP 2023
- **Journal**: None
- **Summary**: This is an exploratory study that discovers the current image quantization (vector quantization) do not satisfy translation equivariance in the quantized space due to aliasing. Instead of focusing on anti-aliasing, we propose a simple yet effective way to achieve translation-equivariant image quantization by enforcing orthogonality among the codebook embeddings. To explore the advantages of translation-equivariant image quantization, we conduct three proof-of-concept experiments with a carefully controlled dataset: (1) text-to-image generation, where the quantized image indices are the target to predict, (2) image-to-text generation, where the quantized image indices are given as a condition, (3) using a smaller training set to analyze sample efficiency. From the strictly controlled experiments, we empirically verify that the translation-equivariant image quantizer improves not only sample efficiency but also the accuracy over VQGAN up to +11.9% in text-to-image generation and +3.9% in image-to-text generation.



### SegDiff: Image Segmentation with Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2112.00390v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00390v3)
- **Published**: 2021-12-01 10:17:25+00:00
- **Updated**: 2022-09-07 04:14:48+00:00
- **Authors**: Tomer Amit, Tal Shaharbany, Eliya Nachmani, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion Probabilistic Methods are employed for state-of-the-art image generation. In this work, we present a method for extending such models for performing image segmentation. The method learns end-to-end, without relying on a pre-trained backbone. The information in the input image and in the current estimation of the segmentation map is merged by summing the output of two encoders. Additional encoding layers and a decoder are then used to iteratively refine the segmentation map, using a diffusion model. Since the diffusion model is probabilistic, it is applied multiple times, and the results are merged into a final segmentation map. The new method produces state-of-the-art results on the Cityscapes validation set, the Vaihingen building segmentation benchmark, and the MoNuSeg dataset.



### Dyadic Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2112.00396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00396v2)
- **Published**: 2021-12-01 10:30:40+00:00
- **Updated**: 2022-01-13 11:43:25+00:00
- **Authors**: Isinsu Katircioglu, Costa Georgantas, Mathieu Salzmann, Pascal Fua
- **Comment**: added reference for section 2
- **Journal**: None
- **Summary**: Prior work on human motion forecasting has mostly focused on predicting the future motion of single subjects in isolation from their past pose sequence. In the presence of closely interacting people, however, this strategy fails to account for the dependencies between the different subject's motions. In this paper, we therefore introduce a motion prediction framework that explicitly reasons about the interactions of two observed subjects. Specifically, we achieve this by introducing a pairwise attention mechanism that models the mutual dependencies in the motion history of the two subjects. This allows us to preserve the long-term motion dynamics in a more realistic way and more robustly predict unusual and fast-paced movements, such as the ones occurring in a dance scenario. To evaluate this, and because no existing motion prediction datasets depict two closely-interacting subjects, we introduce the LindyHop600K dance dataset. Our results evidence that our approach outperforms the state-of-the-art single person motion prediction techniques.



### Rethink, Revisit, Revise: A Spiral Reinforced Self-Revised Network for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00410v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00410v1)
- **Published**: 2021-12-01 10:51:57+00:00
- **Updated**: 2021-12-01 10:51:57+00:00
- **Authors**: Zhe Liu, Yun Li, Lina Yao, Julian McAuley, Sam Dixon
- **Comment**: None
- **Journal**: None
- **Summary**: Current approaches to Zero-Shot Learning (ZSL) struggle to learn generalizable semantic knowledge capable of capturing complex correlations. Inspired by \emph{Spiral Curriculum}, which enhances learning processes by revisiting knowledge, we propose a form of spiral learning which revisits visual representations based on a sequence of attribute groups (e.g., a combined group of \emph{color} and \emph{shape}). Spiral learning aims to learn generalized local correlations, enabling models to gradually enhance global learning and thus understand complex correlations. Our implementation is based on a 2-stage \emph{Reinforced Self-Revised (RSR)} framework: \emph{preview} and \emph{review}. RSR first previews visual information to construct diverse attribute groups in a weakly-supervised manner. Then, it spirally learns refined localities based on attribute groups and uses localities to revise global semantic correlations. Our framework outperforms state-of-the-art algorithms on four benchmark datasets in both zero-shot and generalized zero-shot settings, which demonstrates the effectiveness of spiral learning in learning generalizable and complex correlations. We also conduct extensive analysis to show that attribute groups and reinforced decision processes can capture complementary semantic information to improve predictions and aid explainability.



### The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification
- **Arxiv ID**: http://arxiv.org/abs/2112.00412v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.00412v3)
- **Published**: 2021-12-01 10:58:30+00:00
- **Updated**: 2022-03-28 02:53:48+00:00
- **Authors**: Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, Jin Young Choi
- **Comment**: Accepted by CVPR 2022, 14 pages
- **Journal**: None
- **Summary**: The problem of class imbalanced data is that the generalization performance of the classifier deteriorates due to the lack of data from minority classes. In this paper, we propose a novel minority over-sampling method to augment diversified minority samples by leveraging the rich context of the majority classes as background images. To diversify the minority samples, our key idea is to paste an image from a minority class onto rich-context images from a majority class, using them as background images. Our method is simple and can be easily combined with the existing long-tailed recognition methods. We empirically prove the effectiveness of the proposed oversampling method through extensive experiments and ablation studies. Without any architectural changes or complex algorithms, our method achieves state-of-the-art performance on various long-tailed classification benchmarks. Our code is made available at https://github.com/naver-ai/cmo.



### Personalized Federated Learning with Adaptive Batchnorm for Healthcare
- **Arxiv ID**: http://arxiv.org/abs/2112.00734v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00734v3)
- **Published**: 2021-12-01 11:36:56+00:00
- **Updated**: 2022-05-20 02:13:58+00:00
- **Authors**: Wang Lu, Jindong Wang, Yiqiang Chen, Xin Qin, Renjun Xu, Dimitrios Dimitriadis, Tao Qin
- **Comment**: Accepted by IEEE Transactions on Big Data; code:
  https://github.com/microsoft/personalizedfl. arXiv admin note: substantial
  text overlap with arXiv:2106.01009
- **Journal**: None
- **Summary**: There is a growing interest in applying machine learning techniques to healthcare. Recently, federated learning (FL) is gaining popularity since it allows researchers to train powerful models without compromising data privacy and security. However, the performance of existing FL approaches often deteriorates when encountering non-iid situations where there exist distribution gaps among clients, and few previous efforts focus on personalization in healthcare. In this article, we propose FedAP to tackle domain shifts and then obtain personalized models for local clients. FedAP learns the similarity between clients based on the statistics of the batch normalization layers while preserving the specificity of each client with different local batch normalization. Comprehensive experiments on five healthcare benchmarks demonstrate that FedAP achieves better accuracy compared to state-of-the-art methods (e.g., 10% accuracy improvement for PAMAP2) with faster convergence speed.



### Adv-4-Adv: Thwarting Changing Adversarial Perturbations via Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2112.00428v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.00428v2)
- **Published**: 2021-12-01 11:37:26+00:00
- **Updated**: 2021-12-04 09:32:02+00:00
- **Authors**: Tianyue Zheng, Zhe Chen, Shuya Ding, Chao Cai, Jun Luo
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Whereas adversarial training can be useful against specific adversarial perturbations, they have also proven ineffective in generalizing towards attacks deviating from those used for training. However, we observe that this ineffectiveness is intrinsically connected to domain adaptability, another crucial issue in deep learning for which adversarial domain adaptation appears to be a promising solution. Consequently, we proposed Adv-4-Adv as a novel adversarial training method that aims to retain robustness against unseen adversarial perturbations. Essentially, Adv-4-Adv treats attacks incurring different perturbations as distinct domains, and by leveraging the power of adversarial domain adaptation, it aims to remove the domain/attack-specific features. This forces a trained model to learn a robust domain-invariant representation, which in turn enhances its generalization ability. Extensive evaluations on Fashion-MNIST, SVHN, CIFAR-10, and CIFAR-100 demonstrate that a model trained by Adv-4-Adv based on samples crafted by simple attacks (e.g., FGSM) can be generalized to more advanced attacks (e.g., PGD), and the performance exceeds state-of-the-art proposals on these datasets.



### MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2112.00431v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2112.00431v2)
- **Published**: 2021-12-01 11:47:09+00:00
- **Updated**: 2022-03-28 16:35:52+00:00
- **Authors**: Mattia Soldan, Alejandro Pardo, Juan León Alcázar, Fabian Caba Heilbron, Chen Zhao, Silvio Giancola, Bernard Ghanem
- **Comment**: 12 Pages, 6 Figures, 7 Tables
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition CVPR 2022
- **Summary**: The recent and increasing interest in video-language research has driven the development of large-scale datasets that enable data-intensive machine learning techniques. In comparison, limited effort has been made at assessing the fitness of these datasets for the video-language grounding task. Recent works have begun to discover significant limitations in these datasets, suggesting that state-of-the-art techniques commonly overfit to hidden dataset biases. In this work, we present MAD (Movie Audio Descriptions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annotations and focuses on crawling and aligning available audio descriptions of mainstream movies. MAD contains over 384,000 natural language sentences grounded in over 1,200 hours of videos and exhibits a significant reduction in the currently diagnosed biases for video-language grounding datasets. MAD's collection strategy enables a novel and more challenging version of video-language grounding, where short temporal moments (typically seconds long) must be accurately grounded in diverse long-form videos that can last up to three hours. We have released MAD's data and baselines code at https://github.com/Soldelli/MAD.



### A benchmark with decomposed distribution shifts for 360 monocular depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.00432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00432v1)
- **Published**: 2021-12-01 11:48:23+00:00
- **Updated**: 2021-12-01 11:48:23+00:00
- **Authors**: Georgios Albanis, Nikolaos Zioulis, Petros Drakoulis, Federico Alvarez, Dimitrios Zarpalas, Petros Daras
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we contribute a distribution shift benchmark for a computer vision task; monocular depth estimation. Our differentiation is the decomposition of the wider distribution shift of uncontrolled testing on in-the-wild data, to three distinct distribution shifts. Specifically, we generate data via synthesis and analyze them to produce covariate (color input), prior (depth output) and concept (their relationship) distribution shifts. We also synthesize combinations and show how each one is indeed a different challenge to address, as stacking them produces increased performance drops and cannot be addressed horizontally using standard approaches.



### On-Device Spatial Attention based Sequence Learning Approach for Scene Text Script Identification
- **Arxiv ID**: http://arxiv.org/abs/2112.00448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00448v1)
- **Published**: 2021-12-01 12:16:02+00:00
- **Updated**: 2021-12-01 12:16:02+00:00
- **Authors**: Rutika Moharir, Arun D Prabhu, Sukumar Moharana, Gopi Ramena, Rachit S Munjal
- **Comment**: Accepted for publication in CVIP 2021
- **Journal**: None
- **Summary**: Automatic identification of script is an essential component of a multilingual OCR engine. In this paper, we present an efficient, lightweight, real-time and on-device spatial attention based CNN-LSTM network for scene text script identification, feasible for deployment on resource constrained mobile devices. Our network consists of a CNN, equipped with a spatial attention module which helps reduce the spatial distortions present in natural images. This allows the feature extractor to generate rich image representations while ignoring the deformities and thereby, enhancing the performance of this fine grained classification task. The network also employs residue convolutional blocks to build a deep network to focus on the discriminative features of a script. The CNN learns the text feature representation by identifying each character as belonging to a particular script and the long term spatial dependencies within the text are captured using the sequence learning capabilities of the LSTM layers. Combining the spatial attention mechanism with the residue convolutional blocks, we are able to enhance the performance of the baseline CNN to build an end-to-end trainable network for script identification. The experimental results on several standard benchmarks demonstrate the effectiveness of our method. The network achieves competitive accuracy with state-of-the-art methods and is superior in terms of network size, with a total of just 1.1 million parameters and inference time of 2.7 milliseconds.



### Reference-guided Pseudo-Label Generation for Medical Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2112.00735v1
- **DOI**: 10.1609/aaai.v36i2.20114
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, 68T07, 68T45, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2112.00735v1)
- **Published**: 2021-12-01 12:21:24+00:00
- **Updated**: 2021-12-01 12:21:24+00:00
- **Authors**: Constantin Seibold, Simon Reiß, Jens Kleesiek, Rainer Stiefelhagen
- **Comment**: 36th AAAI Conference on Artificial Intelligence 2022
- **Journal**: None
- **Summary**: Producing densely annotated data is a difficult and tedious task for medical imaging applications. To address this problem, we propose a novel approach to generate supervision for semi-supervised semantic segmentation. We argue that visually similar regions between labeled and unlabeled images likely contain the same semantics and therefore should share their label. Following this thought, we use a small number of labeled images as reference material and match pixels in an unlabeled image to the semantics of the best fitting pixel in a reference set. This way, we avoid pitfalls such as confirmation bias, common in purely prediction-based pseudo-labeling. Since our method does not require any architectural changes or accompanying networks, one can easily insert it into existing frameworks. We achieve the same performance as a standard fully supervised model on X-ray anatomy segmentation, albeit 95% fewer labeled images. Aside from an in-depth analysis of different aspects of our proposed method, we further demonstrate the effectiveness of our reference-guided learning paradigm by comparing our approach against existing methods for retinal fluid segmentation with competitive performance as we improve upon recent work by up to 15% mean IoU.



### Information Theoretic Representation Distillation
- **Arxiv ID**: http://arxiv.org/abs/2112.00459v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00459v3)
- **Published**: 2021-12-01 12:39:50+00:00
- **Updated**: 2022-10-07 16:05:00+00:00
- **Authors**: Roy Miles, Adrian Lopez Rodriguez, Krystian Mikolajczyk
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Despite the empirical success of knowledge distillation, current state-of-the-art methods are computationally expensive to train, which makes them difficult to adopt in practice. To address this problem, we introduce two distinct complementary losses inspired by a cheap entropy-like estimator. These losses aim to maximise the correlation and mutual information between the student and teacher representations. Our method incurs significantly less training overheads than other approaches and achieves competitive performance to the state-of-the-art on the knowledge distillation and cross-model transfer tasks. We further demonstrate the effectiveness of our method on a binary distillation task, whereby it leads to a new state-of-the-art for binary quantisation and approaches the performance of a full precision model. Code: www.github.com/roymiles/ITRD



### The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization
- **Arxiv ID**: http://arxiv.org/abs/2112.00463v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00463v2)
- **Published**: 2021-12-01 12:43:41+00:00
- **Updated**: 2022-04-04 18:59:28+00:00
- **Authors**: M. Jehanzeb Mirza, Jakub Micorek, Horst Possegger, Horst Bischof
- **Comment**: Accepted to CVPR 2022 - Camera Ready Version - Code:
  https://github.com/jmiemirza/DUA
- **Journal**: None
- **Summary**: Domain adaptation is crucial to adapt a learned model to new scenarios, such as domain shifts or changing data distributions. Current approaches usually require a large amount of labeled or unlabeled data from the shifted domain. This can be a hurdle in fields which require continuous dynamic adaptation or suffer from scarcity of data, e.g. autonomous driving in challenging weather conditions. To address this problem of continuous adaptation to distribution shifts, we propose Dynamic Unsupervised Adaptation (DUA). By continuously adapting the statistics of the batch normalization layers we modify the feature representations of the model. We show that by sequentially adapting a model with only a fraction of unlabeled data, a strong performance gain can be achieved. With even less than 1% of unlabeled data from the target domain, DUA already achieves competitive results to strong baselines. In addition, the computational overhead is minimal in contrast to previous approaches. Our approach is simple, yet effective and can be applied to any architecture which uses batch normalization as one of its components. We show the utility of DUA by evaluating it on a variety of domain adaptation datasets and tasks including object recognition, digit recognition and object detection.



### Weakly-Supervised Video Object Grounding via Causal Intervention
- **Arxiv ID**: http://arxiv.org/abs/2112.00475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2112.00475v1)
- **Published**: 2021-12-01 13:13:03+00:00
- **Updated**: 2021-12-01 13:13:03+00:00
- **Authors**: Wei Wang, Junyu Gao, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We target at the task of weakly-supervised video object grounding (WSVOG), where only video-sentence annotations are available during model learning. It aims to localize objects described in the sentence to visual regions in the video, which is a fundamental capability needed in pattern analysis and machine learning. Despite the recent progress, existing methods all suffer from the severe problem of spurious association, which will harm the grounding performance. In this paper, we start from the definition of WSVOG and pinpoint the spurious association from two aspects: (1) the association itself is not object-relevant but extremely ambiguous due to weak supervision, and (2) the association is unavoidably confounded by the observational bias when taking the statistics-based matching strategy in existing methods. With this in mind, we design a unified causal framework to learn the deconfounded object-relevant association for more accurate and robust video object grounding. Specifically, we learn the object-relevant association by causal intervention from the perspective of video data generation process. To overcome the problems of lacking fine-grained supervision in terms of intervention, we propose a novel spatial-temporal adversarial contrastive learning paradigm. To further remove the accompanying confounding effect within the object-relevant association, we pursue the true causality by conducting causal intervention via backdoor adjustment. Finally, the deconfounded object-relevant association is learned and optimized under a unified causal framework in an end-to-end manner. Extensive experiments on both IID and OOD testing sets of three benchmarks demonstrate its accurate and robust grounding performance against state-of-the-arts.



### Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2112.00484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00484v1)
- **Published**: 2021-12-01 13:21:20+00:00
- **Updated**: 2021-12-01 13:21:20+00:00
- **Authors**: Xianzheng Ma, Zhixiang Wang, Yacheng Zhan, Yinqiang Zheng, Zheng Wang, Dengxin Dai, Chia-Wen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Although considerable progress has been made in semantic scene understanding under clear weather, it is still a tough problem under adverse weather conditions, such as dense fog, due to the uncertainty caused by imperfect observations. Besides, difficulties in collecting and labeling foggy images hinder the progress of this field. Considering the success in semantic scene understanding under clear weather, we think it is reasonable to transfer knowledge learned from clear images to the foggy domain. As such, the problem becomes to bridge the domain gap between clear images and foggy images. Unlike previous methods that mainly focus on closing the domain gap caused by fog -- defogging the foggy images or fogging the clear images, we propose to alleviate the domain gap by considering fog influence and style variation simultaneously. The motivation is based on our finding that the style-related gap and the fog-related gap can be divided and closed respectively, by adding an intermediate domain. Thus, we propose a new pipeline to cumulatively adapt style, fog and the dual-factor (style and fog). Specifically, we devise a unified framework to disentangle the style factor and the fog factor separately, and then the dual-factor from images in different domains. Furthermore, we collaborate the disentanglement of three factors with a novel cumulative loss to thoroughly disentangle these three factors. Our method achieves the state-of-the-art performance on three benchmarks and shows generalization ability in rainy and snowy scenes.



### Learning Transformer Features for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2112.00485v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00485v2)
- **Published**: 2021-12-01 13:23:00+00:00
- **Updated**: 2022-03-23 08:41:26+00:00
- **Authors**: Chao Zeng, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: Objective image quality evaluation is a challenging task, which aims to measure the quality of a given image automatically. According to the availability of the reference images, there are Full-Reference and No-Reference IQA tasks, respectively. Most deep learning approaches use regression from deep features extracted by Convolutional Neural Networks. For the FR task, another option is conducting a statistical comparison on deep features. For all these methods, non-local information is usually neglected. In addition, the relationship between FR and NR tasks is less explored. Motivated by the recent success of transformers in modeling contextual information, we propose a unified IQA framework that utilizes CNN backbone and transformer encoder to extract features. The proposed framework is compatible with both FR and NR modes and allows for a joint training scheme. Evaluation experiments on three standard IQA datasets, i.e., LIVE, CSIQ and TID2013, and KONIQ-10K, show that our proposed model can achieve state-of-the-art FR performance. In addition, comparable NR performance is achieved in extensive experiments, and the results show that the NR performance can be leveraged by the joint training scheme.



### Human-Object Interaction Detection via Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2112.00492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00492v1)
- **Published**: 2021-12-01 13:36:06+00:00
- **Updated**: 2021-12-01 13:36:06+00:00
- **Authors**: Mert Kilickaya, Arnold Smeulders
- **Comment**: Accepted at BMVC'21
- **Journal**: None
- **Summary**: The goal of this paper is Human-object Interaction (HO-I) detection. HO-I detection aims to find interacting human-objects regions and classify their interaction from an image. Researchers obtain significant improvement in recent years by relying on strong HO-I alignment supervision from [5]. HO-I alignment supervision pairs humans with their interacted objects, and then aligns human-object pair(s) with their interaction categories. Since collecting such annotation is expensive, in this paper, we propose to detect HO-I without alignment supervision. We instead rely on image-level supervision that only enumerates existing interactions within the image without pointing where they happen. Our paper makes three contributions: i) We propose Align-Former, a visual-transformer based CNN that can detect HO-I with only image-level supervision. ii) Align-Former is equipped with HO-I align layer, that can learn to select appropriate targets to allow detector supervision. iii) We evaluate Align-Former on HICO-DET [5] and V-COCO [13], and show that Align-Former outperforms existing image-level supervised HO-I detectors by a large margin (4.71% mAP improvement from 16.14% to 20.85% on HICO-DET [5]).



### Revisiting the Transferability of Supervised Pretraining: an MLP Perspective
- **Arxiv ID**: http://arxiv.org/abs/2112.00496v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00496v3)
- **Published**: 2021-12-01 13:47:30+00:00
- **Updated**: 2022-03-28 15:17:28+00:00
- **Authors**: Yizhou Wang, Shixiang Tang, Feng Zhu, Lei Bai, Rui Zhao, Donglian Qi, Wanli Ouyang
- **Comment**: Accepted by CVPR 2022. [camera ready with supplement]
- **Journal**: None
- **Summary**: The pretrain-finetune paradigm is a classical pipeline in visual learning. Recent progress on unsupervised pretraining methods shows superior transfer performance to their supervised counterparts. This paper revisits this phenomenon and sheds new light on understanding the transferability gap between unsupervised and supervised pretraining from a multilayer perceptron (MLP) perspective. While previous works focus on the effectiveness of MLP on unsupervised image classification where pretraining and evaluation are conducted on the same dataset, we reveal that the MLP projector is also the key factor to better transferability of unsupervised pretraining methods than supervised pretraining methods. Based on this observation, we attempt to close the transferability gap between supervised and unsupervised pretraining by adding an MLP projector before the classifier in supervised pretraining. Our analysis indicates that the MLP projector can help retain intra-class variation of visual features, decrease the feature distribution distance between pretraining and evaluation datasets, and reduce feature redundancy. Extensive experiments on public benchmarks demonstrate that the added MLP projector significantly boosts the transferability of supervised pretraining, e.g. +7.2% top-1 accuracy on the concept generalization task, +5.8% top-1 accuracy for linear evaluation on 12-domain classification tasks, and +0.8% AP on COCO object detection task, making supervised pretraining comparable or even better than unsupervised pretraining.



### Multi-task fusion for improving mammography screening data classification
- **Arxiv ID**: http://arxiv.org/abs/2112.01320v1
- **DOI**: 10.1109/TMI.2021.3129068
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.01320v1)
- **Published**: 2021-12-01 13:56:27+00:00
- **Updated**: 2021-12-01 13:56:27+00:00
- **Authors**: Maria Wimmer, Gert Sluiter, David Major, Dimitrios Lenis, Astrid Berg, Theresa Neubauer, Katja Bühler
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Machine learning and deep learning methods have become essential for computer-assisted prediction in medicine, with a growing number of applications also in the field of mammography. Typically these algorithms are trained for a specific task, e.g., the classification of lesions or the prediction of a mammogram's pathology status. To obtain a comprehensive view of a patient, models which were all trained for the same task(s) are subsequently ensembled or combined. In this work, we propose a pipeline approach, where we first train a set of individual, task-specific models and subsequently investigate the fusion thereof, which is in contrast to the standard model ensembling strategy. We fuse model predictions and high-level features from deep learning models with hybrid patient models to build stronger predictors on patient level. To this end, we propose a multi-branch deep learning model which efficiently fuses features across different tasks and mammograms to obtain a comprehensive patient-level prediction. We train and evaluate our full pipeline on public mammography data, i.e., DDSM and its curated version CBIS-DDSM, and report an AUC score of 0.962 for predicting the presence of any lesion and 0.791 for predicting the presence of malignant lesions on patient level. Overall, our fusion approaches improve AUC scores significantly by up to 0.04 compared to standard model ensembling. Moreover, by providing not only global patient-level predictions but also task-specific model results that are related to radiological features, our pipeline aims to closely support the reading workflow of radiologists.



### Learning Oriented Remote Sensing Object Detection via Naive Geometric Computing
- **Arxiv ID**: http://arxiv.org/abs/2112.00504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00504v1)
- **Published**: 2021-12-01 13:58:42+00:00
- **Updated**: 2021-12-01 13:58:42+00:00
- **Authors**: Yanjie Wang, Xu Zou, Zhijun Zhang, Wenhui Xu, Liqun Chen, Sheng Zhong, Luxin Yan, Guodong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting oriented objects along with estimating their rotation information is one crucial step for analyzing remote sensing images. Despite that many methods proposed recently have achieved remarkable performance, most of them directly learn to predict object directions under the supervision of only one (e.g. the rotation angle) or a few (e.g. several coordinates) groundtruth values individually. Oriented object detection would be more accurate and robust if extra constraints, with respect to proposal and rotation information regression, are adopted for joint supervision during training. To this end, we innovatively propose a mechanism that simultaneously learns the regression of horizontal proposals, oriented proposals, and rotation angles of objects in a consistent manner, via naive geometric computing, as one additional steady constraint (see Figure 1). An oriented center prior guided label assignment strategy is proposed for further enhancing the quality of proposals, yielding better performance. Extensive experiments demonstrate the model equipped with our idea significantly outperforms the baseline by a large margin to achieve a new state-of-the-art result without any extra computational burden during inference. Our proposed idea is simple and intuitive that can be readily implemented. Source codes and trained models are involved in supplementary files.



### Trimap-guided Feature Mining and Fusion Network for Natural Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2112.00510v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00510v3)
- **Published**: 2021-12-01 14:13:11+00:00
- **Updated**: 2022-05-30 00:34:34+00:00
- **Authors**: Weihao Jiang, Dongdong Yu, Zhaozhi Xie, Yaoyi Li, Zehuan Yuan, Hongtao Lu
- **Comment**: submitted to Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Utilizing trimap guidance and fusing multi-level features are two important issues for trimap-based matting with pixel-level prediction. To utilize trimap guidance, most existing approaches simply concatenate trimaps and images together to feed a deep network or apply an extra network to extract more trimap guidance, which meets the conflict between efficiency and effectiveness. For emerging content-based feature fusion, most existing matting methods only focus on local features which lack the guidance of a global feature with strong semantic information related to the interesting object. In this paper, we propose a trimap-guided feature mining and fusion network consisting of our trimap-guided non-background multi-scale pooling (TMP) module and global-local context-aware fusion (GLF) modules. Considering that trimap provides strong semantic guidance, our TMP module focuses effective feature mining on interesting objects under the guidance of trimap without extra parameters. Furthermore, our GLF modules use global semantic information of interesting objects mined by our TMP module to guide an effective global-local context-aware multi-level feature fusion. In addition, we build a common interesting object matting (CIOM) dataset to advance high-quality image matting. Particularly, results on the Composition-1k and our CIOM show that our TMFNet achieves 13% and 25% relative improvement on SAD, respectively, against a strong baseline with fewer parameters and 14% fewer FLOPs. Experimental results on the Composition-1k test set, Alphamatting benchmark, and our CIOM test set demonstrate that our method outperforms state-of-the-art approaches.



### Incomplete Multi-view Clustering via Cross-view Relation Transfer
- **Arxiv ID**: http://arxiv.org/abs/2112.00739v1
- **DOI**: 10.1109/TCSVT.2022.3201822
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00739v1)
- **Published**: 2021-12-01 14:28:15+00:00
- **Updated**: 2021-12-01 14:28:15+00:00
- **Authors**: Yiming Wang, Dongxia Chang, Zhiqiang Fu, Yao Zhao
- **Comment**: None
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2022
- **Summary**: In this paper, we consider the problem of multi-view clustering on incomplete views. Compared with complete multi-view clustering, the view-missing problem increases the difficulty of learning common representations from different views. To address the challenge, we propose a novel incomplete multi-view clustering framework, which incorporates cross-view relation transfer and multi-view fusion learning. Specifically, based on the consistency existing in multi-view data, we devise a cross-view relation transfer-based completion module, which transfers known similar inter-instance relationships to the missing view and recovers the missing data via graph networks based on the transferred relationship graph. Then the view-specific encoders are designed to extract the recovered multi-view data, and an attention-based fusion layer is introduced to obtain the common representation. Moreover, to reduce the impact of the error caused by the inconsistency between views and obtain a better clustering structure, a joint clustering layer is introduced to optimize recovery and clustering simultaneously. Extensive experiments conducted on several real datasets demonstrate the effectiveness of the proposed method.



### Subtask-dominated Transfer Learning for Long-tail Person Search
- **Arxiv ID**: http://arxiv.org/abs/2112.00527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00527v1)
- **Published**: 2021-12-01 14:34:48+00:00
- **Updated**: 2021-12-01 14:34:48+00:00
- **Authors**: Chuang Liu, Hua Yang, Qin Zhou, Shibao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Person search unifies person detection and person re-identification (Re-ID) to locate query persons from the panoramic gallery images. One major challenge comes from the imbalanced long-tail person identity distributions, which prevents the one-step person search model from learning discriminative person features for the final re-identification. However, it is under-explored how to solve the heavy imbalanced identity distributions for the one-step person search. Techniques designed for the long-tail classification task, for example, image-level re-sampling strategies, are hard to be effectively applied to the one-step person search which jointly solves person detection and Re-ID subtasks with a detection-based multi-task framework. To tackle this problem, we propose a Subtask-dominated Transfer Learning (STL) method. The STL method solves the long-tail problem in the pretraining stage of the dominated Re-ID subtask and improves the one-step person search by transfer learning of the pretrained model. We further design a Multi-level RoI Fusion Pooling layer to enhance the discrimination ability of person features for the one-step person search. Extensive experiments on CUHK-SYSU and PRW datasets demonstrate the superiority and effectiveness of the proposed method.



### FaceTuneGAN: Face Autoencoder for Convolutional Expression Transfer Using Neural Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.00532v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.00532v1)
- **Published**: 2021-12-01 14:42:03+00:00
- **Updated**: 2021-12-01 14:42:03+00:00
- **Authors**: Nicolas Olivier, Kelian Baert, Fabien Danieau, Franck Multon, Quentin Avril
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present FaceTuneGAN, a new 3D face model representation decomposing and encoding separately facial identity and facial expression. We propose a first adaptation of image-to-image translation networks, that have successfully been used in the 2D domain, to 3D face geometry. Leveraging recently released large face scan databases, a neural network has been trained to decouple factors of variations with a better knowledge of the face, enabling facial expressions transfer and neutralization of expressive faces. Specifically, we design an adversarial architecture adapting the base architecture of FUNIT and using SpiralNet++ for our convolutional and sampling operations. Using two publicly available datasets (FaceScape and CoMA), FaceTuneGAN has a better identity decomposition and face neutralization than state-of-the-art techniques. It also outperforms classical deformation transfer approach by predicting blendshapes closer to ground-truth data and with less of undesired artifacts due to too different facial morphologies between source and target.



### Semi-Supervised Surface Anomaly Detection of Composite Wind Turbine Blades From Drone Imagery
- **Arxiv ID**: http://arxiv.org/abs/2112.00556v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00556v1)
- **Published**: 2021-12-01 15:20:12+00:00
- **Updated**: 2021-12-01 15:20:12+00:00
- **Authors**: Jack. W. Barker, Neelanjan Bhowmik, Toby. P. Breckon
- **Comment**: In-proceedings at 2022 17th International Conference on Computer
  Vision Theory and Applications (VISAPP)
- **Journal**: None
- **Summary**: Within commercial wind energy generation, the monitoring and predictive maintenance of wind turbine blades in-situ is a crucial task, for which remote monitoring via aerial survey from an Unmanned Aerial Vehicle (UAV) is commonplace. Turbine blades are susceptible to both operational and weather-based damage over time, reducing the energy efficiency output of turbines. In this study, we address automating the otherwise time-consuming task of both blade detection and extraction, together with fault detection within UAV-captured turbine blade inspection imagery. We propose BladeNet, an application-based, robust dual architecture to perform both unsupervised turbine blade detection and extraction, followed by super-pixel generation using the Simple Linear Iterative Clustering (SLIC) method to produce regional clusters. These clusters are then processed by a suite of semi-supervised detection methods. Our dual architecture detects surface faults of glass fibre composite material blades with high aptitude while requiring minimal prior manual image annotation. BladeNet produces an Average Precision (AP) of 0.995 across our {\O}rsted blade inspection dataset for offshore wind turbines and 0.223 across the Danish Technical University (DTU) NordTank turbine blade inspection dataset. BladeNet also obtains an AUC of 0.639 for surface anomaly detection across the {\O}rsted blade inspection dataset.



### 3D Reconstruction Using a Linear Laser Scanner and a Camera
- **Arxiv ID**: http://arxiv.org/abs/2112.00557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00557v1)
- **Published**: 2021-12-01 15:20:24+00:00
- **Updated**: 2021-12-01 15:20:24+00:00
- **Authors**: Rui Wang
- **Comment**: 8 pages, 16 figures, published in The 2nd International Conference on
  Artificial Intelligence and Computer Engineering (ICAICE2021)
- **Journal**: None
- **Summary**: With the rapid development of computer graphics and vision, several three-dimensional (3D) reconstruction techniques have been proposed and used to obtain the 3D representation of objects in the form of point cloud models, mesh models, and geometric models. The cost of 3D reconstruction is declining due to the maturing of this technology, however, the inexpensive 3D reconstruction scanners on the market may not be able to generate a clear point cloud model as expected. This study systematically reviews some basic types of 3D reconstruction technology and introduces an easy implementation using a linear laser scanner, a camera, and a turntable. The implementation is based on the monovision with laser and has tested several objects like wiki and mug. The accuracy and resolution of the point cloud result are quite satisfying. It turns out everyone can build such a 3D reconstruction system with appropriate procedures.



### Attribute Artifacts Removal for Geometry-based Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/2112.00560v2
- **DOI**: 10.1109/TIP.2022.3170722
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00560v2)
- **Published**: 2021-12-01 15:21:06+00:00
- **Updated**: 2022-03-01 02:35:56+00:00
- **Authors**: Xihua Sheng, Li Li, Dong Liu, Zhiwei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Geometry-based point cloud compression (G-PCC) can achieve remarkable compression efficiency for point clouds. However, it still leads to serious attribute compression artifacts, especially under low bitrate scenarios. In this paper, we propose a Multi-Scale Graph Attention Network (MS-GAT) to remove the artifacts of point cloud attributes compressed by G-PCC. We first construct a graph based on point cloud geometry coordinates and then use the Chebyshev graph convolutions to extract features of point cloud attributes. Considering that one point may be correlated with points both near and far away from it, we propose a multi-scale scheme to capture the short- and long-range correlations between the current point and its neighboring and distant points. To address the problem that various points may have different degrees of artifacts caused by adaptive quantization, we introduce the quantization step per point as an extra input to the proposed network. We also incorporate a weighted graph attentional layer into the network to pay special attention to the points with more attribute artifacts. To the best of our knowledge, this is the first attribute artifacts removal method for G-PCC. We validate the effectiveness of our method over various point clouds. Objective comparison results show that our proposed method achieves an average of 9.74% BD-rate reduction compared with Predlift and 10.13% BD-rate reduction compared with RAHT. Subjective comparison results present that visual artifacts such as color shifting, blurring, and quantization noise are reduced.



### Dual Spoof Disentanglement Generation for Face Anti-spoofing with Depth Uncertainty Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00568v1)
- **Published**: 2021-12-01 15:36:59+00:00
- **Updated**: 2021-12-01 15:36:59+00:00
- **Authors**: Hangtong Wu, Dan Zen, Yibo Hu, Hailin Shi, Tao Mei
- **Comment**: Accepted to TCSVT, arXiv version. The codes are available at
  https://github.com/JDAI-CV/FaceX-Zoo/tree/main/addition_module/DSDG
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in preventing face recognition systems from presentation attacks. Existing face anti-spoofing datasets lack diversity due to the insufficient identity and insignificant variance, which limits the generalization ability of FAS model. In this paper, we propose Dual Spoof Disentanglement Generation (DSDG) framework to tackle this challenge by "anti-spoofing via generation". Depending on the interpretable factorized latent disentanglement in Variational Autoencoder (VAE), DSDG learns a joint distribution of the identity representation and the spoofing pattern representation in the latent space. Then, large-scale paired live and spoofing images can be generated from random noise to boost the diversity of the training set. However, some generated face images are partially distorted due to the inherent defect of VAE. Such noisy samples are hard to predict precise depth values, thus may obstruct the widely-used depth supervised optimization. To tackle this issue, we further introduce a lightweight Depth Uncertainty Module (DUM), which alleviates the adverse effects of noisy samples by depth uncertainty learning. DUM is developed without extra-dependency, thus can be flexibly integrated with any depth supervised network for face anti-spoofing. We evaluate the effectiveness of the proposed method on five popular benchmarks and achieve state-of-the-art results under both intra- and inter- test settings. The codes are available at https://github.com/JDAI-CV/FaceX-Zoo/tree/main/addition_module/DSDG.



### Background Activation Suppression for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2112.00580v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00580v2)
- **Published**: 2021-12-01 15:53:40+00:00
- **Updated**: 2022-04-02 16:07:53+00:00
- **Authors**: Pingyu Wu, Wei Zhai, Yang Cao
- **Comment**: Accepted by CVPR 2022. Code: https://github.com/wpy1999/BAS
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) aims to localize objects using only image-level labels. Recently a new paradigm has emerged by generating a foreground prediction map (FPM) to achieve localization task. Existing FPM-based methods use cross-entropy (CE) to evaluate the foreground prediction map and to guide the learning of generator. We argue for using activation value to achieve more efficient learning. It is based on the experimental observation that, for a trained network, CE converges to zero when the foreground mask covers only part of the object region. While activation value increases until the mask expands to the object boundary, which indicates that more object areas can be learned by using activation value. In this paper, we propose a Background Activation Suppression (BAS) method. Specifically, an Activation Map Constraint module (AMC) is designed to facilitate the learning of generator by suppressing the background activation value. Meanwhile, by using the foreground region guidance and the area constraint, BAS can learn the whole region of the object. In the inference phase, we consider the prediction maps of different categories together to obtain the final localization results. Extensive experiments show that BAS achieves significant and consistent improvement over the baseline methods on the CUB-200-2011 and ILSVRC datasets. Code and models are available at https://github.com/wpy1999/BAS.



### Transformer-based Network for RGB-D Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2112.00582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00582v1)
- **Published**: 2021-12-01 15:53:58+00:00
- **Updated**: 2021-12-01 15:53:58+00:00
- **Authors**: Yue Wang, Xu Jia, Lu Zhang, Yuke Li, James Elder, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-D saliency detection integrates information from both RGB images and depth maps to improve prediction of salient regions under challenging conditions. The key to RGB-D saliency detection is to fully mine and fuse information at multiple scales across the two modalities. Previous approaches tend to apply the multi-scale and multi-modal fusion separately via local operations, which fails to capture long-range dependencies. Here we propose a transformer-based network to address this issue. Our proposed architecture is composed of two modules: a transformer-based within-modality feature enhancement module (TWFEM) and a transformer-based feature fusion module (TFFM). TFFM conducts a sufficient feature fusion by integrating features from multiple scales and two modalities over all positions simultaneously. TWFEM enhances feature on each scale by selecting and integrating complementary information from other scales within the same modality before TFFM. We show that transformer is a uniform operation which presents great efficacy in both feature fusion and feature enhancement, and simplifies the model design. Extensive experimental results on six benchmark datasets demonstrate that our proposed network performs favorably against state-of-the-art RGB-D saliency detection methods.



### The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D Shapes from Parts
- **Arxiv ID**: http://arxiv.org/abs/2112.00584v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00584v2)
- **Published**: 2021-12-01 15:54:54+00:00
- **Updated**: 2022-07-21 22:56:38+00:00
- **Authors**: Kai Wang, Paul Guerrero, Vladimir Kim, Siddhartha Chaudhuri, Minhyuk Sung, Daniel Ritchie
- **Comment**: European Conference on Computer Vision (ECCV) 2022
- **Journal**: None
- **Summary**: We present the Shape Part Slot Machine, a new method for assembling novel 3D shapes from existing parts by performing contact-based reasoning. Our method represents each shape as a graph of ``slots,'' where each slot is a region of contact between two shape parts. Based on this representation, we design a graph-neural-network-based model for generating new slot graphs and retrieving compatible parts, as well as a gradient-descent-based optimization scheme for assembling the retrieved parts into a complete shape that respects the generated slot graph. This approach does not require any semantic part labels; interestingly, it also does not require complete part geometries -- reasoning about the slots proves sufficient to generate novel, high-quality 3D shapes. We demonstrate that our method generates shapes that outperform existing modeling-by-assembly approaches regarding quality, diversity, and structural complexity.



### Neural Emotion Director: Speech-preserving semantic control of facial expressions in "in-the-wild" videos
- **Arxiv ID**: http://arxiv.org/abs/2112.00585v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00585v2)
- **Published**: 2021-12-01 15:55:04+00:00
- **Updated**: 2022-03-30 10:58:03+00:00
- **Authors**: Foivos Paraperas Papantoniou, Panagiotis P. Filntisis, Petros Maragos, Anastasios Roussos
- **Comment**: CVPR 2022 (oral). Project page: https://foivospar.github.io/NED/
- **Journal**: None
- **Summary**: In this paper, we introduce a novel deep learning method for photo-realistic manipulation of the emotional state of actors in "in-the-wild" videos. The proposed method is based on a parametric 3D face representation of the actor in the input scene that offers a reliable disentanglement of the facial identity from the head pose and facial expressions. It then uses a novel deep domain translation framework that alters the facial expressions in a consistent and plausible manner, taking into account their dynamics. Finally, the altered facial expressions are used to photo-realistically manipulate the facial region in the input scene based on an especially-designed neural face renderer. To the best of our knowledge, our method is the first to be capable of controlling the actor's facial expressions by even using as a sole input the semantic labels of the manipulated emotions, while at the same time preserving the speech-related lip movements. We conduct extensive qualitative and quantitative evaluations and comparisons, which demonstrate the effectiveness of our approach and the especially promising results that we obtain. Our method opens a plethora of new possibilities for useful applications of neural rendering technologies, ranging from movie post-production and video games to photo-realistic affective avatars.



### DeepSportLab: a Unified Framework for Ball Detection, Player Instance Segmentation and Pose Estimation in Team Sports Scenes
- **Arxiv ID**: http://arxiv.org/abs/2112.00627v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, 68T45 (Primary) (Secondary), I.4.6; I.4.8; I.5.4; I.5.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2112.00627v1)
- **Published**: 2021-12-01 16:30:51+00:00
- **Updated**: 2021-12-01 16:30:51+00:00
- **Authors**: Seyed Abolfazl Ghasemzadeh, Gabriel Van Zandycke, Maxime Istasse, Niels Sayez, Amirafshar Moshtaghpour, Christophe De Vleeschouwer
- **Comment**: 13 pages, 5 figures, BMVC, BMVC2021
- **Journal**: None
- **Summary**: This paper presents a unified framework to (i) locate the ball, (ii) predict the pose, and (iii) segment the instance mask of players in team sports scenes. Those problems are of high interest in automated sports analytics, production, and broadcast. A common practice is to individually solve each problem by exploiting universal state-of-the-art models, \eg, Panoptic-DeepLab for player segmentation. In addition to the increased complexity resulting from the multiplication of single-task models, the use of the off-the-shelf models also impedes the performance due to the complexity and specificity of the team sports scenes, such as strong occlusion and motion blur. To circumvent those limitations, our paper proposes to train a single model that simultaneously predicts the ball and the player mask and pose by combining the part intensity fields and the spatial embeddings principles. Part intensity fields provide the ball and player location, as well as player joints location. Spatial embeddings are then exploited to associate player instance pixels to their respective player center, but also to group player joints into skeletons. We demonstrate the effectiveness of the proposed model on the DeepSport basketball dataset, achieving comparable performance to the SoA models addressing each individual task separately.



### A Systematic Review of Robustness in Deep Learning for Computer Vision: Mind the gap?
- **Arxiv ID**: http://arxiv.org/abs/2112.00639v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00639v2)
- **Published**: 2021-12-01 16:42:38+00:00
- **Updated**: 2022-11-28 00:27:22+00:00
- **Authors**: Nathan Drenkow, Numair Sani, Ilya Shpitser, Mathias Unberath
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks for computer vision are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, refers here to preserving model performance under naturally-induced image corruptions or alterations.   We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find this area of research has received disproportionately less attention relative to adversarial machine learning, yet a significant robustness gap exists that manifests in performance degradation similar in magnitude to adversarial conditions.   Toward developing a more transparent definition of robustness, we provide a conceptual framework based on a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model's behavior on corrupted images corresponding to low-probability samples from the unaltered data distribution. We identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This robustness perspective reveals that common practices in the literature correspond to causal concepts. We offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.



### D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2112.03028v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.03028v2)
- **Published**: 2021-12-01 17:04:39+00:00
- **Updated**: 2022-03-30 17:43:20+00:00
- **Authors**: Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin Hwangbo, Jie Song, Otmar Hilliges
- **Comment**: CVPR-2022 camera ready. Project page at
  https://eth-ait.github.io/d-grasp/
- **Journal**: None
- **Summary**: We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and generates a wide range of motions. Furthermore, even imperfect labels can be corrected by our method to generate dynamic interaction sequences.



### Object-aware Video-language Pre-training for Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2112.00656v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2112.00656v6)
- **Published**: 2021-12-01 17:06:39+00:00
- **Updated**: 2022-05-18 09:15:43+00:00
- **Authors**: Alex Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, Mike Zheng Shou
- **Comment**: CVPR2022; Code: https://github.com/FingerRec/OA-Transformer
- **Journal**: None
- **Summary**: Recently, by introducing large-scale dataset and strong transformer network, video-language pre-training has shown great success especially for retrieval. Yet, existing video-language transformer models do not explicitly fine-grained semantic align. In this work, we present Object-aware Transformers, an object-centric approach that extends video-language transformer to incorporate object representations. The key idea is to leverage the bounding boxes and object tags to guide the training process. We evaluate our model on three standard sub-tasks of video-text matching on four widely used benchmarks. We also provide deep analysis and detailed ablation about the proposed method. We show clear improvement in performance across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a video-language architecture. The code will be released at \url{https://github.com/FingerRec/OA-Transformer}.



### Iterative Saliency Enhancement using Superpixel Similarity
- **Arxiv ID**: http://arxiv.org/abs/2112.00665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00665v2)
- **Published**: 2021-12-01 17:22:54+00:00
- **Updated**: 2022-03-04 18:40:31+00:00
- **Authors**: Leonardo de Melo Joao, Alexandre Xavier Falcao
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency Object Detection (SOD) has several applications in image analysis. The methods have evolved from image-intrinsic to object-inspired (deep-learning-based) models. When a model fail, however, there is no alternative to enhance its saliency map. We fill this gap by introducing a hybrid approach, named \textit{Iterative Saliency Enhancement over Superpixel Similarity} (ISESS), that iteratively generates enhanced saliency maps by executing two operations alternately: object-based superpixel segmentation and superpixel-based saliency estimation -- cycling operations never exploited. ISESS estimates seeds for superpixel delineation from a given saliency map and defines superpixel queries in the foreground and background. A new saliency map results from color similarities between queries and superpixels at each iteration. The process repeats and, after a given number of iterations, the generated saliency maps are combined into one by cellular automata. Finally, the resulting map is merged with the initial one by the maximum bewteen their average values per superpixel. We demonstrate that our hybrid model can consistently outperform three state-of-the-art deep-learning-based methods on five image datasets.



### CYBORG: Blending Human Saliency Into the Loss Improves Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00686v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00686v3)
- **Published**: 2021-12-01 18:04:15+00:00
- **Updated**: 2022-08-17 20:54:54+00:00
- **Authors**: Aidan Boyd, Patrick Tinsley, Kevin Bowyer, Adam Czajka
- **Comment**: None
- **Journal**: None
- **Summary**: Can deep learning models achieve greater generalization if their training is guided by reference to human perceptual abilities? And how can we implement this in a practical manner? This paper proposes a training strategy to ConveY Brain Oversight to Raise Generalization (CYBORG). This new approach incorporates human-annotated saliency maps into a loss function that guides the model's learning to focus on image regions that humans deem salient for the task. The Class Activation Mapping (CAM) mechanism is used to probe the model's current saliency in each training batch, juxtapose this model saliency with human saliency, and penalize large differences. Results on the task of synthetic face detection, selected to illustrate the effectiveness of the approach, show that CYBORG leads to significant improvement in accuracy on unseen samples consisting of face images generated from six Generative Adversarial Networks across multiple classification network architectures. We also show that scaling to even seven times the training data, or using non-human-saliency auxiliary information, such as segmentation masks, and standard loss cannot beat the performance of CYBORG-trained models. As a side effect of this work, we observe that the addition of explicit region annotation to the task of synthetic face detection increased human classification accuracy. This work opens a new area of research on how to incorporate human visual saliency into loss functions in practice. All data, code and pre-trained models used in this work are offered with this paper.



### MDFM: Multi-Decision Fusing Model for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2112.00690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00690v2)
- **Published**: 2021-12-01 18:13:09+00:00
- **Updated**: 2021-12-03 08:41:40+00:00
- **Authors**: Shuai Shao, Lei Xing, Rui Xu, Weifeng Liu, Yan-Jiang Wang, Bao-Di Liu
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT). arXiv admin note: text overlap with arXiv:2109.07785
- **Journal**: None
- **Summary**: In recent years, researchers pay growing attention to the few-shot learning (FSL) task to address the data-scarce problem. A standard FSL framework is composed of two components: i) Pre-train. Employ the base data to generate a CNN-based feature extraction model (FEM). ii) Meta-test. Apply the trained FEM to the novel data (category is different from base data) to acquire the feature embeddings and recognize them. Although researchers have made remarkable breakthroughs in FSL, there still exists a fundamental problem. Since the trained FEM with base data usually cannot adapt to the novel class flawlessly, the novel data's feature may lead to the distribution shift problem. To address this challenge, we hypothesize that even if most of the decisions based on different FEMs are viewed as weak decisions, which are not available for all classes, they still perform decently in some specific categories. Inspired by this assumption, we propose a novel method Multi-Decision Fusing Model (MDFM), which comprehensively considers the decisions based on multiple FEMs to enhance the efficacy and robustness of the model. MDFM is a simple, flexible, non-parametric method that can directly apply to the existing FEMs. Besides, we extend the proposed MDFM to two FSL settings (i.e., supervised and semi-supervised settings). We evaluate the proposed method on five benchmark datasets and achieve significant improvements of 3.4%-7.3% compared with state-of-the-arts.



### Label-Free Model Evaluation with Semi-Structured Dataset Representations
- **Arxiv ID**: http://arxiv.org/abs/2112.00694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00694v1)
- **Published**: 2021-12-01 18:15:58+00:00
- **Updated**: 2021-12-01 18:15:58+00:00
- **Authors**: Xiaoxiao Sun, Yunzhong Hou, Hongdong Li, Liang Zheng
- **Comment**: 10 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Label-free model evaluation, or AutoEval, estimates model accuracy on unlabeled test sets, and is critical for understanding model behaviors in various unseen environments. In the absence of image labels, based on dataset representations, we estimate model performance for AutoEval with regression. On the one hand, image feature is a straightforward choice for such representations, but it hampers regression learning due to being unstructured (\ie no specific meanings for component at certain location) and of large-scale. On the other hand, previous methods adopt simple structured representations (like average confidence or average feature), but insufficient to capture the data characteristics given their limited dimensions. In this work, we take the best of both worlds and propose a new semi-structured dataset representation that is manageable for regression learning while containing rich information for AutoEval. Based on image features, we integrate distribution shapes, clusters, and representative samples for a semi-structured dataset representation. Besides the structured overall description with distribution shapes, the unstructured description with clusters and representative samples include additional fine-grained information facilitating the AutoEval task. On three existing datasets and 25 newly introduced ones, we experimentally show that the proposed representation achieves competitive results. Code and dataset are available at https://github.com/sxzrt/Semi-Structured-Dataset-Representations.



### CondenseNeXt: An Ultra-Efficient Deep Neural Network for Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/2112.00698v1
- **DOI**: 10.1109/CCWC51732.2021.9375950
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00698v1)
- **Published**: 2021-12-01 18:20:52+00:00
- **Updated**: 2021-12-01 18:20:52+00:00
- **Authors**: Priyank Kalgaonkar, Mohamed El-Sharkawy
- **Comment**: 5 pages, 3 figures, published in an IEEE Conference
- **Journal**: None
- **Summary**: Due to the advent of modern embedded systems and mobile devices with constrained resources, there is a great demand for incredibly efficient deep neural networks for machine learning purposes. There is also a growing concern of privacy and confidentiality of user data within the general public when their data is processed and stored in an external server which has further fueled the need for developing such efficient neural networks for real-time inference on local embedded systems. The scope of our work presented in this paper is limited to image classification using a convolutional neural network. A Convolutional Neural Network (CNN) is a class of Deep Neural Network (DNN) widely used in the analysis of visual images captured by an image sensor, designed to extract information and convert it into meaningful representations for real-time inference of the input data. In this paper, we propose a neoteric variant of deep convolutional neural network architecture to ameliorate the performance of existing CNN architectures for real-time inference on embedded systems. We show that this architecture, dubbed CondenseNeXt, is remarkably efficient in comparison to the baseline neural network architecture, CondenseNet, by reducing trainable parameters and FLOPs required to train the network whilst maintaining a balance between the trained model size of less than 3.0 MB and accuracy trade-off resulting in an unprecedented computational efficiency.



### A Novel Gaussian Process Based Ground Segmentation Algorithm with Local-Smoothness Estimation
- **Arxiv ID**: http://arxiv.org/abs/2112.05847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.05847v1)
- **Published**: 2021-12-01 18:42:08+00:00
- **Updated**: 2021-12-01 18:42:08+00:00
- **Authors**: Pouria Mehrabi, Hamid D. Taghirad
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2111.10638
- **Journal**: None
- **Summary**: Autonomous Land Vehicles (ALV) shall efficiently recognize the ground in unknown environments. A novel $\mathcal{GP}$-based method is proposed for the ground segmentation task in rough driving scenarios. A non-stationary covariance function is utilized as the kernel for the $\mathcal{GP}$. The ground surface behavior is assumed to only demonstrate local-smoothness. Thus, point estimates of the kernel's length-scales are obtained. Thus, two Gaussian processes are introduced to separately model the observation and local characteristics of the data. While, the \textit{observation process} is used to model the ground, the \textit{latent process} is put on length-scale values to estimate point values of length-scales at each input location. Input locations for this latent process are chosen in a physically-motivated procedure to represent an intuition about ground condition. Furthermore, an intuitive guess of length-scale value is represented by assuming the existence of hypothetical surfaces in the environment that every bunch of data points may be assumed to be resulted from measurements from this surfaces. Bayesian inference is implemented using \textit{maximum a Posteriori} criterion. The log-marginal likelihood function is assumed to be a multi-task objective function, to represent a whole-frame unbiased view of the ground at each frame. Simulation results shows the effectiveness of the proposed method even in an uneven, rough scene which outperforms similar Gaussian process based ground segmentation methods. While adjacent segments do not have similar ground structure in an uneven scene, the proposed method gives an efficient ground estimation based on a whole-frame viewpoint instead of just estimating segment-wise probable ground surfaces.



### Improving GAN Equilibrium by Raising Spatial Awareness
- **Arxiv ID**: http://arxiv.org/abs/2112.00718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00718v2)
- **Published**: 2021-12-01 18:55:51+00:00
- **Updated**: 2022-04-18 10:58:51+00:00
- **Authors**: Jianyuan Wang, Ceyuan Yang, Yinghao Xu, Yujun Shen, Hongdong Li, Bolei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The success of Generative Adversarial Networks (GANs) is largely built upon the adversarial training between a generator (G) and a discriminator (D). They are expected to reach a certain equilibrium where D cannot distinguish the generated images from the real ones. However, such an equilibrium is rarely achieved in practical GAN training, instead, D almost always surpasses G. We attribute one of its sources to the information asymmetry between D and G. We observe that D learns its own visual attention when determining whether an image is real or fake, but G has no explicit clue on which regions to focus on for a particular synthesis. To alleviate the issue of D dominating the competition in GANs, we aim to raise the spatial awareness of G. Randomly sampled multi-level heatmaps are encoded into the intermediate layers of G as an inductive bias. Thus G can purposefully improve the synthesis of certain image regions. We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance. As a byproduct, the introduced spatial awareness facilitates interactive editing over the output synthesis. Demo video and code are available at https://genforce.github.io/eqgan-sa/.



### HyperInverter: Improving StyleGAN Inversion via Hypernetwork
- **Arxiv ID**: http://arxiv.org/abs/2112.00719v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00719v2)
- **Published**: 2021-12-01 18:56:05+00:00
- **Updated**: 2022-04-04 17:39:19+00:00
- **Authors**: Tan M. Dinh, Anh Tuan Tran, Rang Nguyen, Binh-Son Hua
- **Comment**: Accepted to CVPR 2022; Project page is located at
  https://di-mi-ta.github.io/HyperInverter/
- **Journal**: None
- **Summary**: Real-world image manipulation has achieved fantastic progress in recent years as a result of the exploration and utilization of GAN latent spaces. GAN inversion is the first step in this pipeline, which aims to map the real image to the latent code faithfully. Unfortunately, the majority of existing GAN inversion methods fail to meet at least one of the three requirements listed below: high reconstruction quality, editability, and fast inference. We present a novel two-phase strategy in this research that fits all requirements at the same time. In the first phase, we train an encoder to map the input image to StyleGAN2 $\mathcal{W}$-space, which was proven to have excellent editability but lower reconstruction quality. In the second phase, we supplement the reconstruction ability in the initial phase by leveraging a series of hypernetworks to recover the missing information during inversion. These two steps complement each other to yield high reconstruction quality thanks to the hypernetwork branch and excellent editability due to the inversion done in the $\mathcal{W}$-space. Our method is entirely encoder-based, resulting in extremely fast inference. Extensive experiments on two challenging datasets demonstrate the superiority of our method.



### RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs
- **Arxiv ID**: http://arxiv.org/abs/2112.00724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2112.00724v1)
- **Published**: 2021-12-01 18:59:46+00:00
- **Updated**: 2021-12-01 18:59:46+00:00
- **Authors**: Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan
- **Comment**: Project page available at
  https://m-niemeyer.github.io/regnerf/index.html
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.



### The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2112.00725v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00725v4)
- **Published**: 2021-12-01 18:59:54+00:00
- **Updated**: 2023-01-24 16:17:47+00:00
- **Authors**: Yuki M. Asano, Aaqib Saeed
- **Comment**: Accepted at ICLR'23. Webpage:
  https://single-image-distill.github.io/, code:
  https://github.com/yukimasano/single-img-extrapolating
- **Journal**: None
- **Summary**: What can neural networks learn about the visual world when provided with only a single image as input? While any image obviously cannot contain the multitudes of all existing objects, scenes and lighting conditions - within the space of all 256^(3x224x224) possible 224-sized square images, it might still provide a strong prior for natural images. To analyze this `augmented image prior' hypothesis, we develop a simple framework for training neural networks from scratch using a single image and augmentations using knowledge distillation from a supervised pretrained teacher. With this, we find the answer to the above question to be: `surprisingly, a lot'. In quantitative terms, we find accuracies of 94%/74% on CIFAR-10/100, 69% on ImageNet, and by extending this method to video and audio, 51% on Kinetics-400 and 84% on SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the effect of augmentations, choice of data and network architectures and also provide qualitative evaluations that include lucid `panda neurons' in networks that have never even seen one.



### MonoScene: Monocular 3D Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2112.00726v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2112.00726v2)
- **Published**: 2021-12-01 18:59:57+00:00
- **Updated**: 2022-03-29 17:59:46+00:00
- **Authors**: Anh-Quan Cao, Raoul de Charette
- **Comment**: Accepted at CVPR 2022. Project page:
  https://cv-rits.github.io/MonoScene/
- **Journal**: None
- **Summary**: MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the dense geometry and semantics of a scene are inferred from a single monocular RGB image. Different from the SSC literature, relying on 2.5 or 3D input, we solve the complex problem of 2D to 3D scene reconstruction while jointly inferring its semantics. Our framework relies on successive 2D and 3D UNets bridged by a novel 2D-3D features projection inspiring from optics and introduces a 3D context relation prior to enforce spatio-semantic consistency. Along with architectural contributions, we introduce novel global scene and local frustums losses. Experiments show we outperform the literature on all metrics and datasets while hallucinating plausible scenery even beyond the camera field of view. Our code and trained models are available at https://github.com/cv-rits/MonoScene.



### Routing with Self-Attention for Multimodal Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2112.00775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00775v1)
- **Published**: 2021-12-01 19:01:26+00:00
- **Updated**: 2021-12-01 19:01:26+00:00
- **Authors**: Kevin Duarte, Brian Chen, Nina Shvetsova, Andrew Rouditchenko, Samuel Thomas, Alexander Liu, David Harwath, James Glass, Hilde Kuehne, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: The task of multimodal learning has seen a growing interest recently as it allows for training neural architectures based on different modalities such as vision, text, and audio. One challenge in training such models is that they need to jointly learn semantic concepts and their relationships across different input representations. Capsule networks have been shown to perform well in context of capturing the relation between low-level input features and higher-level concepts. However, capsules have so far mainly been used only in small-scale fully supervised settings due to the resource demand of conventional routing algorithms. We present a new multimodal capsule network that allows us to leverage the strength of capsules in the context of a multimodal learning framework on large amounts of video data. To adapt the capsules to large-scale input data, we propose a novel routing by self-attention mechanism that selects relevant capsules which are then used to generate a final joint multimodal feature representation. This allows not only for robust training with noisy video data, but also to scale up the size of the capsule network compared to traditional routing methods while still being computationally efficient. We evaluate the proposed architecture by pretraining it on a large-scale multimodal video dataset and applying it on four datasets in two challenging downstream tasks. Results show that the proposed multimodal capsule network is not only able to improve results compared to other routing techniques, but also achieves competitive performance on the task of multimodal learning.



### Using Deep Image Prior to Assist Variational Selective Segmentation Deep Learning Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2112.00793v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00793v1)
- **Published**: 2021-12-01 19:31:55+00:00
- **Updated**: 2021-12-01 19:31:55+00:00
- **Authors**: Liam Burrows, Ke Chen, Francesco Torella
- **Comment**: Presented at SIPAIM 2021
- **Journal**: None
- **Summary**: Variational segmentation algorithms require a prior imposed in the form of a regularisation term to enforce smoothness of the solution. Recently, it was shown in the Deep Image Prior work that the explicit regularisation in a model can be removed and replaced by the implicit regularisation captured by the architecture of a neural network. The Deep Image Prior approach is competitive, but is only tailored to one specific image and does not allow us to predict future images. We propose to incorporate the ideas from Deep Image Prior into a more traditional learning algorithm to allow us to use the implicit regularisation offered by the Deep Image Prior, but still be able to predict future images.



### DFTS2: Simulating Deep Feature Transmission Over Packet Loss Channels
- **Arxiv ID**: http://arxiv.org/abs/2112.00794v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00794v1)
- **Published**: 2021-12-01 19:34:49+00:00
- **Updated**: 2021-12-01 19:34:49+00:00
- **Authors**: Ashiv Dhondea, Robert A. Cohen, Ivan V. Bajić
- **Comment**: 6 pages, 4 figures, IEEE Conference on Visual Communications and
  Image Processing (VCIP) 2021
- **Journal**: None
- **Summary**: In edge-cloud collaborative intelligence (CI), an unreliable transmission channel exists in the information path of the AI model performing the inference. It is important to be able to simulate the performance of the CI system across an imperfect channel in order to understand system behavior and develop appropriate error control strategies. In this paper we present a simulation framework called DFTS2, which enables researchers to define the components of the CI system in TensorFlow~2, select a packet-based channel model with various parameters, and simulate system behavior under various channel conditions and error/loss control strategies. Using DFTS2, we also present the most comprehensive study to date of the packet loss concealment methods for collaborative image classification models.



### PreViTS: Contrastive Pretraining with Video Tracking Supervision
- **Arxiv ID**: http://arxiv.org/abs/2112.00804v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00804v2)
- **Published**: 2021-12-01 19:49:57+00:00
- **Updated**: 2022-09-27 18:35:14+00:00
- **Authors**: Brian Chen, Ramprasaath R. Selvaraju, Shih-Fu Chang, Juan Carlos Niebles, Nikhil Naik
- **Comment**: To be presented at WACV 2023
- **Journal**: None
- **Summary**: Videos are a rich source for self-supervised learning (SSL) of visual representations due to the presence of natural temporal transformations of objects. However, current methods typically randomly sample video clips for learning, which results in an imperfect supervisory signal. In this work, we propose PreViTS, an SSL framework that utilizes an unsupervised tracking signal for selecting clips containing the same object, which helps better utilize temporal transformations of objects. PreViTS further uses the tracking signal to spatially constrain the frame regions to learn from and trains the model to locate meaningful objects by providing supervision on Grad-CAM attention maps. To evaluate our approach, we train a momentum contrastive (MoCo) encoder on VGG-Sound and Kinetics-400 datasets with PreViTS. Training with PreViTS outperforms representations learnt by contrastive strategy alone on video downstream tasks, obtaining state-of-the-art performance on action classification. PreViTS helps learn feature representations that are more robust to changes in background and context, as seen by experiments on datasets with background changes. Learning from large-scale videos with PreViTS could lead to more accurate and robust visual feature representations.



### FaSS-MVS -- Fast Multi-View Stereo with Surface-Aware Semi-Global Matching from UAV-borne Monocular Imagery
- **Arxiv ID**: http://arxiv.org/abs/2112.00821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00821v1)
- **Published**: 2021-12-01 20:43:33+00:00
- **Updated**: 2021-12-01 20:43:33+00:00
- **Authors**: Boitumelo Ruf, Martin Weinmann, Stefan Hinz
- **Comment**: None
- **Journal**: None
- **Summary**: With FaSS-MVS, we present an approach for fast multi-view stereo with surface-aware Semi-Global Matching that allows for rapid depth and normal map estimation from monocular aerial video data captured by UAVs. The data estimated by FaSS-MVS, in turn, facilitates online 3D mapping, meaning that a 3D map of the scene is immediately and incrementally generated while the image data is acquired or being received. FaSS-MVS is comprised of a hierarchical processing scheme in which depth and normal data, as well as corresponding confidence scores, are estimated in a coarse-to-fine manner, allowing to efficiently process large scene depths which are inherent to oblique imagery captured by low-flying UAVs. The actual depth estimation employs a plane-sweep algorithm for dense multi-image matching to produce depth hypotheses from which the actual depth map is extracted by means of a surface-aware semi-global optimization, reducing the fronto-parallel bias of SGM. Given the estimated depth map, the pixel-wise surface normal information is then computed by reprojecting the depth map into a point cloud and calculating the normal vectors within a confined local neighborhood. In a thorough quantitative and ablative study we show that the accuracies of the 3D information calculated by FaSS-MVS is close to that of state-of-the-art approaches for offline multi-view stereo, with the error not even being one magnitude higher than that of COLMAP. At the same time, however, the average run-time of FaSS-MVS to estimate a single depth and normal map is less than 14 % of that of COLMAP, allowing to perform an online and incremental processing of Full-HD imagery at 1-2 Hz.



### Pose2Room: Understanding 3D Scenes from Human Activities
- **Arxiv ID**: http://arxiv.org/abs/2112.03030v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.03030v2)
- **Published**: 2021-12-01 20:54:36+00:00
- **Updated**: 2022-07-14 16:20:50+00:00
- **Authors**: Yinyu Nie, Angela Dai, Xiaoguang Han, Matthias Nießner
- **Comment**: Accepted by ECCV'2022; Project page:
  https://yinyunie.github.io/pose2room-page/ Video:
  https://www.youtube.com/watch?v=MFfKTcvbM5o
- **Journal**: None
- **Summary**: With wearable IMU sensors, one can estimate human poses from wearable devices without requiring visual input~\cite{von2017sparse}. In this work, we pose the question: Can we reason about object structure in real-world environments solely from human trajectory information? Crucially, we observe that human motion and interactions tend to give strong information about the objects in a scene -- for instance a person sitting indicates the likely presence of a chair or sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of the objects in a scene characterized by their class categories and oriented 3D bounding boxes, based on an input observed human trajectory in the environment. P2R-Net models the probability distribution of object class as well as a deep Gaussian mixture model for object boxes, enabling sampling of multiple, diverse, likely modes of object configurations from an observed human trajectory. In our experiments we show that P2R-Net can effectively learn multi-modal distributions of likely objects for human motions, and produce a variety of plausible object structures of the environment, even without any visual information. The results demonstrate that P2R-Net consistently outperforms the baselines on the PROX dataset and the VirtualHome platform.



### CLAWS: Contrastive Learning with hard Attention and Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2112.00847v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.00847v2)
- **Published**: 2021-12-01 21:45:58+00:00
- **Updated**: 2022-01-31 22:11:02+00:00
- **Authors**: Jansel Herrera-Gerena, Ramakrishnan Sundareswaran, John Just, Matthew Darr, Ali Jannesari
- **Comment**: None
- **Journal**: None
- **Summary**: Learning effective visual representations without human supervision is a long-standing problem in computer vision. Recent advances in self-supervised learning algorithms have utilized contrastive learning, with methods such as SimCLR, which applies a composition of augmentations to an image, and minimizes a contrastive loss between the two augmented images. In this paper, we present CLAWS, an annotation-efficient learning framework, addressing the problem of manually labeling large-scale agricultural datasets along with potential applications such as anomaly detection and plant growth analytics. CLAWS uses a network backbone inspired by SimCLR and weak supervision to investigate the effect of contrastive learning within class clusters. In addition, we inject a hard attention mask to the cropped input image before maximizing agreement between the image pairs using a contrastive loss function. This mask forces the network to focus on pertinent object features and ignore background features. We compare results between a supervised SimCLR and CLAWS using an agricultural dataset with 227,060 samples consisting of 11 different crop classes. Our experiments and extensive evaluations show that CLAWS achieves a competitive NMI score of 0.7325. Furthermore, CLAWS engenders the creation of low dimensional representations of very large datasets with minimal parameter tuning and forming well-defined clusters, which lends themselves to using efficient, transparent, and highly interpretable clustering methods such as Gaussian Mixture Models.



### Interpretable Deep Learning-Based Forensic Iris Segmentation and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2112.00849v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2112.00849v2)
- **Published**: 2021-12-01 21:59:16+00:00
- **Updated**: 2021-12-20 14:59:21+00:00
- **Authors**: Andrey Kuehlkamp, Aidan Boyd, Adam Czajka, Kevin Bowyer, Patrick Flynn, Dennis Chute, Eric Benjamin
- **Comment**: None
- **Journal**: None
- **Summary**: Iris recognition of living individuals is a mature biometric modality that has been adopted globally from governmental ID programs, border crossing, voter registration and de-duplication, to unlocking mobile phones. On the other hand, the possibility of recognizing deceased subjects with their iris patterns has emerged recently. In this paper, we present an end-to-end deep learning-based method for postmortem iris segmentation and recognition with a special visualization technique intended to support forensic human examiners in their efforts. The proposed postmortem iris segmentation approach outperforms the state of the art and in addition to iris annulus, as in case of classical iris segmentation methods - detects abnormal regions caused by eye decomposition processes, such as furrows or irregular specular highlights present on the drying and wrinkling cornea. The method was trained and validated with data acquired from 171 cadavers, kept in mortuary conditions, and tested on subject-disjoint data acquired from 259 deceased subjects. To our knowledge, this is the largest corpus of data used in postmortem iris recognition research to date. The source code of the proposed method are offered with the paper. The test data will be available through the National Archive of Criminal Justice Data (NACJD) archives.



### Automatic tumour segmentation in H&E-stained whole-slide images of the pancreas
- **Arxiv ID**: http://arxiv.org/abs/2112.01533v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2112.01533v2)
- **Published**: 2021-12-01 22:05:15+00:00
- **Updated**: 2022-01-24 22:19:06+00:00
- **Authors**: Pierpaolo Vendittelli, Esther M. M. Smeets, Geert Litjens
- **Comment**: None
- **Journal**: None
- **Summary**: Pancreatic cancer will soon be the second leading cause of cancer-related death in Western society. Imaging techniques such as CT, MRI and ultrasound typically help providing the initial diagnosis, but histopathological assessment is still the gold standard for final confirmation of disease presence and prognosis. In recent years machine learning approaches and pathomics pipelines have shown potential in improving diagnostics and prognostics in other cancerous entities, such as breast and prostate cancer. A crucial first step in these pipelines is typically identification and segmentation of the tumour area. Ideally this step is done automatically to prevent time consuming manual annotation. We propose a multi-task convolutional neural network to balance disease detection and segmentation accuracy. We validated our approach on a dataset of 29 patients (for a total of 58 slides) at different resolutions. The best single task segmentation network achieved a median Dice of 0.885 (0.122) IQR at a resolution of 15.56 $\mu$m. Our multi-task network improved on that with a median Dice score of 0.934 (0.077) IQR.



### GANORCON: Are Generative Models Useful for Few-shot Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2112.00854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00854v2)
- **Published**: 2021-12-01 22:06:20+00:00
- **Updated**: 2022-04-28 22:12:33+00:00
- **Authors**: Oindrila Saha, Zezhou Cheng, Subhransu Maji
- **Comment**: CVPR 2022 Camera Ready Version
- **Journal**: None
- **Summary**: Advances in generative modeling based on GANs has motivated the community to find their use beyond image generation and editing tasks. In particular, several recent works have shown that GAN representations can be re-purposed for discriminative tasks such as part segmentation, especially when training data is limited. But how do these improvements stack-up against recent advances in self-supervised learning? Motivated by this we present an alternative approach based on contrastive learning and compare their performance on standard few-shot part segmentation benchmarks. Our experiments reveal that not only do the GAN-based approach offer no significant performance advantage, their multi-step training is complex, nearly an order-of-magnitude slower, and can introduce additional bias. These experiments suggest that the inductive biases of generative models, such as their ability to disentangle shape and texture, are well captured by standard feed-forward networks trained using contrastive learning. These experiments suggest that the inductive biases present in current generative models, such as their ability to disentangle shape and texture, are well captured by standard feed-forward networks trained using contrastive learning.



### Learning to automate cryo-electron microscopy data collection with Ptolemy
- **Arxiv ID**: http://arxiv.org/abs/2112.01534v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2112.01534v2)
- **Published**: 2021-12-01 22:39:28+00:00
- **Updated**: 2022-01-14 21:48:09+00:00
- **Authors**: Paul T. Kim, Alex J. Noble, Anchi Cheng, Tristan Bepler
- **Comment**: Main: 12 pages, 11 figures. Appendix: 2 pages, 1 figure
- **Journal**: None
- **Summary**: Over the past decade, cryogenic electron microscopy (cryo-EM) has emerged as a primary method for determining near-native, near-atomic resolution 3D structures of biological macromolecules. In order to meet increasing demand for cryo-EM, automated methods to improve throughput and efficiency while lowering costs are needed. Currently, all high-magnification cryo-EM data collection softwares require human input and manual tuning of parameters. Expert operators must navigate low- and medium-magnification images to find good high-magnification collection locations. Automating this is non-trivial: the images suffer from low signal-to-noise ratio and are affected by a range of experimental parameters that can differ for each collection session. Here, we use various computer vision algorithms, including mixture models, convolutional neural networks, and U-Nets to develop the first pipeline to automate low- and medium-magnification targeting. Learned models in this pipeline are trained on a large internal dataset of images from real world cryo-EM data collection sessions, labeled with locations that were selected by operators. Using these models, we show that we can effectively detect and classify regions of interest in low- and medium-magnification images, and can generalize to unseen sessions, as well as to images captured using different microscopes from external facilities. We expect our open-source pipeline, Ptolemy, will be both immediately useful as a tool for automation of cryo-EM data collection, and serve as a foundation for future advanced methods for efficient and automated cryo-EM microscopy.



### Generating Diverse 3D Reconstructions from a Single Occluded Face Image
- **Arxiv ID**: http://arxiv.org/abs/2112.00879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2112.00879v2)
- **Published**: 2021-12-01 23:13:49+00:00
- **Updated**: 2022-03-31 19:30:14+00:00
- **Authors**: Rahul Dey, Vishnu Naresh Boddeti
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Occlusions are a common occurrence in unconstrained face images. Single image 3D reconstruction from such face images often suffers from corruption due to the presence of occlusions. Furthermore, while a plurality of 3D reconstructions is plausible in the occluded regions, existing approaches are limited to generating only a single solution. To address both of these challenges, we present Diverse3DFace, which is specifically designed to simultaneously generate a diverse and realistic set of 3D reconstructions from a single occluded face image. It consists of three components: a global+local shape fitting process, a graph neural network-based mesh VAE, and a Determinantal Point Process based diversity promoting iterative optimization procedure. Quantitative and qualitative comparisons of 3D reconstruction on occluded faces show that Diverse3DFace can estimate 3D shapes that are consistent with the visible regions in the target image while exhibiting high, yet realistic, levels of diversity on the occluded regions. On face images occluded by masks, glasses, and other random objects, Diverse3DFace generates a distribution of 3D shapes having ~50% higher diversity on the occluded regions compared to the baselines. Moreover, our closest sample to the ground truth has ~40% lower MSE than the singular reconstructions by existing approaches.



